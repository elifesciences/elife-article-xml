<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56212</article-id><article-id pub-id-type="doi">10.7554/eLife.56212</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Developmental Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The development of active binocular vision under normal and alternate rearing conditions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-176072"><name><surname>Klimmasch</surname><given-names>Lukas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9923-3052</contrib-id><email>klimmasch@fias.uni-frankfurt.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177941"><name><surname>Schneider</surname><given-names>Johann</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177910"><name><surname>Lelais</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-242089"><name><surname>Fronius</surname><given-names>Maria</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177909"><name><surname>Shi</surname><given-names>Bertram Emil</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-111472"><name><surname>Triesch</surname><given-names>Jochen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8166-2441</contrib-id><email>triesch@fias.uni-frankfurt.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Frankfurt Institute for Advanced Studies (FIAS)</institution><addr-line><named-content content-type="city">Frankfurt am Main</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Department of Ophthalmology, Child Vision Research Unit, Goethe University</institution><addr-line><named-content content-type="city">Frankfurt am Main</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology</institution><addr-line><named-content content-type="city">Hong Kong</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name><role>Reviewing Editor</role><aff><institution>University of Nottingham</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>08</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e56212</elocation-id><history><date date-type="received" iso-8601-date="2020-02-20"><day>20</day><month>02</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-08-04"><day>04</day><month>08</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-02-21"><day>21</day><month>02</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.02.20.957449"/></event></pub-history><permissions><copyright-statement>Â© 2021, Klimmasch et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Klimmasch et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56212-v2.pdf"/><abstract><p>The development of binocular vision is an active learning process comprising the development of disparity tuned neurons in visual cortex and the establishment of precise vergence control of the eyes. We present a computational model for the learning and self-calibration of active binocular vision based on the Active Efficient Coding framework, an extension of classic efficient coding ideas to active perception. Under normal rearing conditions with naturalistic input, the model develops disparity tuned neurons and precise vergence control, allowing it to correctly interpret random dot stereograms. Under altered rearing conditions modeled after neurophysiological experiments, the model qualitatively reproduces key experimental findings on changes in binocularity and disparity tuning. Furthermore, the model makes testable predictions regarding how altered rearing conditions impede the learning of precise vergence control. Finally, the model predicts a surprising new effect that impaired vergence control affects the statistics of orientation tuning in visual cortical neurons.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>binocular vision</kwd><kwd>depth perception</kwd><kwd>eye movements</kwd><kwd>vergence</kwd><kwd>perception</kwd><kwd>action</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Federal Ministry of Education and Research</institution></institution-wrap></funding-source><award-id>01GQ1414</award-id><principal-award-recipient><name><surname>Klimmasch</surname><given-names>Lukas</given-names></name><name><surname>Lelais</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Federal Ministry of Education and Research</institution></institution-wrap></funding-source><award-id>01EW1603A</award-id><principal-award-recipient><name><surname>Klimmasch</surname><given-names>Lukas</given-names></name><name><surname>Schneider</surname><given-names>Johann</given-names></name><name><surname>Triesch</surname><given-names>Jochen</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>713010</award-id><principal-award-recipient><name><surname>Lelais</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002920</institution-id><institution>Research Grants Council, University Grants Committee</institution></institution-wrap></funding-source><award-id>16244416</award-id><principal-award-recipient><name><surname>Shi</surname><given-names>Bertram Emil</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Quandt Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Triesch</surname><given-names>Jochen</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Federal Ministry of Education and Research</institution></institution-wrap></funding-source><award-id>01EW1603B</award-id><principal-award-recipient><name><surname>Fronius</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new computational model explains how alternate rearing conditions affect the development of binocular vision.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans and other species learn to perceive the world largely autonomously. This is in sharp contrast to todayâs machine learning approaches (<xref ref-type="bibr" rid="bib57">Kotsiantis et al., 2007</xref>; <xref ref-type="bibr" rid="bib49">Jordan and Mitchell, 2015</xref>), which typically use millions of carefully labeled training images in order to learn to, say, recognize an object or perceive its three-dimensional structure. How can biological vision systems learn so much more autonomously? The development of binocular vision presents a paradigmatic case for studying this question. This development is an active process that includes the learning of appropriate sensory representations and the learning of precise motor behavior. Species with two forward facing eyes learn to register small differences between the images projected onto the left and right retinas. These differences are called binocular disparities and are detected by populations of neurons in visual cortex (<xref ref-type="bibr" rid="bib52">Kandel et al., 2000</xref>; <xref ref-type="bibr" rid="bib7">Blake and Wilson, 2011</xref>) that have receptive subfields in both eyes. Frequently, they are modeled using separate Gabor-shaped filters for each eye, where the disparity is encoded by a shift in the centers of the filters, a difference between their phases, or by a combination of both (<xref ref-type="bibr" rid="bib28">Fleet et al., 1996</xref>; <xref ref-type="bibr" rid="bib13">Chen and Qian, 2004</xref>). The responses of such disparity tuned neurons can be used to infer the three-dimensional structure of the world. At the same time, such species also learn to align their eyes such that the optical axes of the two eyes converge on the same point of interest. These so-called vergence eye movements are also learned and fine-tuned during development (<xref ref-type="bibr" rid="bib40">Held et al., 1980</xref>; <xref ref-type="bibr" rid="bib29">Fox et al., 1980</xref>; <xref ref-type="bibr" rid="bib88">Stidwill and Fletcher, 2017</xref>). Again, this learning does not require any supervision from outside, but must rely on some form of self-calibration.</p><p>Various computational models have been employed to explain the development of binocular disparity tuning in the context of efficient coding ideas (<xref ref-type="bibr" rid="bib63">Li and Atick, 1994</xref>; <xref ref-type="bibr" rid="bib46">Hunt et al., 2013</xref>), independent component analysis (ICA) (<xref ref-type="bibr" rid="bib42">Hoyer and HyvÃ¤rinen, 2000</xref>), Bayesian inference (<xref ref-type="bibr" rid="bib9">Burge and Geisler, 2014</xref>), or nonlinear Hebbian learning (<xref ref-type="bibr" rid="bib11">Chauhan et al., 2018</xref>) (see <xref ref-type="bibr" rid="bib12">Chauhan et al., 2020</xref> for a review). A critical limitation of these studies is that they ignore the importance of behavior in shaping the statistics of the sensory input and in particular the role of vergence eye movements in determining the statistics of disparities. Indeed, while it has long been argued that the development of disparity tuning and vergence eye movements are interdependent (<xref ref-type="bibr" rid="bib44">Hubel and Wiesel, 1965</xref>; <xref ref-type="bibr" rid="bib10">Candy, 2019</xref>), it has been only recently that computational models have tried to explain how the learning of disparity tuning and vergence eye movements are coupled and allow the visual system to self-calibrate (<xref ref-type="bibr" rid="bib106">Zhao et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Klimmasch et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Eckmann et al., 2019</xref>). These models have been developed in the framework of Active Efficient Coding (AEC), which is an extension of Barlowâs classic efficient coding hypothesis to active perception (<xref ref-type="bibr" rid="bib6">Barlow, 1961</xref>). In a nutshell, classic efficient coding argues that sensory systems should use representations that remove redundancies from sensory signals to encode them more efficiently. Therefore, sensory representations should be adapted to the statistics of sensory signals. Based on this idea, a wide range of data on tuning properties of sensory neurons in different modalities have been explained from a unified theoretical framework (<xref ref-type="bibr" rid="bib19">Dan et al., 1996</xref>; <xref ref-type="bibr" rid="bib99">Vinje and Gallant, 2000</xref>; <xref ref-type="bibr" rid="bib82">Simoncelli, 2003</xref>; <xref ref-type="bibr" rid="bib83">Smith and Lewicki, 2006</xref>; <xref ref-type="bibr" rid="bib25">Doi et al., 2012</xref>). AEC goes beyond classic efficient coding by acknowledging that developing sensory systems shape the statistics of sensory signals through their own behavior. This gives them a second route for optimizing the encoding of sensory signals by adapting their behavior. In the case of binocular vision, for example, the control of vergence eye movements is shaping the statistics of binocular disparities. By simultaneously optimizing neural tuning properties and behavior, AEC models have provided the first comprehensive account of how humans and other binocular species may self-calibrate their binocular vision through the simultaneous learning of disparity tuning and vergence control.</p><p>A generic AEC model has two components. The first component is an efficient coding model that learns to encode sensory signals by adapting the tuning properties of a population of simulated sensory neurons (<xref ref-type="bibr" rid="bib72">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib73">Olshausen and Field, 1997</xref>). In the case of binocular vision, this is a population of visual cortical neurons receiving input from the two eyes that learns to encode the visual signals via an efficient code. The second component is a reinforcement learning (RL) model that learns to control the behavior. In the case of binocular vision, this component will learn to control eye vergence. For this, it receives as input the population activity of the visual neurons and learns to map it onto vergence commands. This learning is guided by an internally generated reward signal, which reinforces movements that lead to a more efficient encoding of the current visual scene. For example, when the eyes are aligned on the same point, the left and right images become largely redundant. The efficient coding model can exploit this redundant structure in both eyes, by developing neurons tuned to small or zero disparities. Conversely, such binocular neurons tuned to small disparities will represent any remaining misalignments of the eyes, providing informative input for vergence control. In this way, learning of vergence control supports the development of neurons tuned to small disparities and this developing population of neurons in turn facilitates the learning of fine vergence control (<xref ref-type="bibr" rid="bib106">Zhao et al., 2012</xref>).</p><p>Importantly, however, this normal development of binocular vision is impaired in a range of alternate rearing conditions. In fact, already since the days of Hubel and Wiesel, alternate rearing conditions have been used to improve our understanding of visual cortex plasticity and function. Manipulating the input to the visual system during development and observing how the system reacts to such manipulations has shaped our understanding of visual development until today. For example, artificially inducing a strabismus (squint) leads to drastic changes in the tuning properties of neurons in visual cortex (<xref ref-type="bibr" rid="bib44">Hubel and Wiesel, 1965</xref>). A comprehensive theoretical account of the development of binocular vision must therefore also be able to explain the experimentally observed differences in alternate rearing conditions. Our recent work (<xref ref-type="bibr" rid="bib26">Eckmann et al., 2019</xref>) took a step in this direction by modeling the development of amblyopia in response to anisometropic rearing (introducing differences between the refractive power of the two eyes). In the present study, in contrast, we aim to demonstrate the generality of the AEC approach by reproducing and explaining a large range of neurophysiological findings from different alternate rearing conditions: changing the orientation distribution in the visual input (horizontal, vertical, or orthogonal rearing), monocular rearing, strabismic rearing, and aniseikonia. We also utilize a sophisticated biomechanical model of the oculomotor system, opening the door to simulating the effects of both optical and motor abberations on visual development.</p><p>Our results show that the model qualitatively captures findings on how different alternate rearing conditions alter the statistics of disparity tuning and binocularity. Furthermore, the model makes specific novel and testable predictions about differences in vergence behavior under the different rearing conditions. Surprisingly, it also predicts systematic differences in the statistics of orientation tuning of visual cortical neurons depending on the fidelity of vergence eye movements. Overall, our results support AEC as a parsimonious account of the emergence of binocular vision, highlighting the active nature of this development.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A model for the development of active binocular vision</title><p>The model comprises a virtual agent situated in a simulated environment. The agent looks at a textured plane that is positioned in front of it at variable distances between 0.5Â m and 6Â m (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We use planar images instead of a full 3D environment to (i) allow us to uniquely define the correct vergence angle for the current visual scene and (ii) make sure that the visual input follows natural image statistics. Note, that previous AEC models have already demonstrated the approach in full 3-D environments (<xref ref-type="bibr" rid="bib109">Zhu et al., 2017a</xref>; <xref ref-type="bibr" rid="bib110">Zhu et al., 2017b</xref>; <xref ref-type="bibr" rid="bib59">Lelais et al., 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Model overview.</title><p>(<bold>A</bold>) The agent looks at the object plane in the simulated environment. (<bold>B</bold>) Architecture of the model. Input images are filtered to simulate alternate rearing conditions (N: normal, V: vertical, H: horizontal, O: orthogonal, M: monocular). Binocular patches are extracted at a coarse and a fine scale (turquoiseÂ and orange boxes) with different resolutions. These patches are encoded via sparse coding and combined with the muscle activations to form a state vector for actor critic reinforcement learning. The reconstruction error of sparse coding indicates coding efficiency and serves as a reward signal (purple arrow) to train the critic. The actor generates changes in muscle activations, which result in differential rotations of the eyeballs and a new iteration of the perception-action cycle.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig1-v2.tif"/></fig><p>An image is rendered for the left eye and a second image is rendered for the right eye. Binocular patches are extracted from these images and encoded by a sparse coding algorithm. The activation levels of the learned binocular basis functions (BFs) can be thought of as firing rates of binocular simple cells in primary visual cortex. The basis functions themselves roughly describe their receptive fields (RFs) and are adapted through learning (<xref ref-type="bibr" rid="bib73">Olshausen and Field, 1997</xref>). These activations are then squared and pooled across the image to obtain a more position-invariant representation mimicking the behavior of complex cells. From this state representation a reinforcement learner generates vergence commands that symmetrically rotate the eyeballs inwards or outwards. This results in two new images being rendered and a new simulation iteration starts. The complete process is depicted in <xref ref-type="fig" rid="fig1">Figure 1B</xref> (see MaterialsÂ andÂ methods for details).</p><p>In the human retina, the RF size of ganglion cells increases towards the periphery (<xref ref-type="bibr" rid="bib18">Curcio et al., 1990</xref>). We incorporate this idea by extracting patches from an input image at two different spatial scales: A high-resolution fine scale is extracted from the central part and a low-resolution coarse scale is extracted from a larger area (orange and turquoise boxes in <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig2">Figure 2</xref>). The overlap between the coarse and fine scale does not depict the biological reality, but simplifies the implementation and analysis of the model. Covering a visual angle of 8.3Â° in total, the fine scale corresponds to the central/para-central region (including the fovea) and the coarse scale to the near-peripheral region with a diameter of 26.6Â°. On the one hand, this two-scale architecture is more biologically plausible than using just a single scale, on the other hand it also increases the resulting verging performance (<xref ref-type="bibr" rid="bib64">Lonini et al., 2013</xref>). One input patch (or subfield) in the coarse scale can detect a disparity of up to 8.8Â° while one patch in the fine scale covers 1.6Â°. The coarse scale can therefore be used to detect large disparities, while the fine scale detects small disparities.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual input and learned receptive fields under different rearing conditions.</title><p>Left: Illustration of visual inputs under the different rearing conditions. Except for the <italic>normal</italic> condition, the images are convolved with different Gaussian filters to blur out certain orientations or simulate monocular deprivation. To simulate strabismus the right eye is rotated inward by 10Â°, so that neurons receive non-corresponding inputs to their left and right eye receptive fields. The structures behind the object plane depict a background image in the simulator. Right: Examples of binocular RFs for the fine and coarse scale learned under the different rearing conditions after 0.5 million iterations. For each RF, the left eye and right eye patches are aligned vertically. In each case, the 10 RFs most frequently selected by the sparse coding algorithm are shown.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2âsource data 1.</label><caption><title>Coarse and fine scale RFs in vectorized form for all rearing conditions.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>All learned RFs for the six rearing conditions.</title><p>Complete set of all RFs that are learned during training for all different rearing conditions.Â .</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig2-figsupp1-v2.tif"/></fig></fig-group><p>We simulate altered rearing conditions by convolving the input images for the two eyes with two-dimensional Gaussian kernels to blur certain oriented edges, or to simulate monocular deprivation. To mimic strabismus, the right eyeball is rotated inwards while the left eye remains unchanged to enforce non-overlapping input to corresponding positions of the left and right retina (see MaterialsÂ andÂ methods for details).</p><p>The adaptation of the neural representation and the learning of appropriate motor commands occur simultaneously: While the sparse coder updates the initially random RFs to minimize the reconstruction error, the RL agent generates vergence eye movements to minimize the reconstruction error of the sparse coder. Since the sparse coder has a fixed capacity, minimizing its reconstruction error is equivalent to maximizing its coding efficiency. Thus, both the sparse coder and the reinforcement learner aim to maximize the overall coding efficiency of the model. The learning of the two components (sparse coder and RL agent) happens roughly at the same timescale. Our model is robust to variations in the learning rates, as long as the reinforcement learnerâs critic converges faster than the actor (<xref ref-type="bibr" rid="bib98">Van Hasselt and Wiering, 2007</xref>).</p></sec><sec id="s2-2"><title>Normal rearing conditions lead to the autonomous learning of accurate vergence control for natural input and random dot stereograms</title><p>A first critical test of a model of the development of binocular vision is whether the model produces plausible behavior. Indeed, under normal rearing conditions the joint learning of the neural representation and motor behavior results in an agent that accurately verges the eyes on the plane in front of it (<xref ref-type="bibr" rid="bib55">Klimmasch et al., 2017</xref>). <xref ref-type="video" rid="video1">Video 1</xref> illustrates the learned behavior.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-56212-video1.mp4"><label>Video 1.</label><caption><title>Vergence performance for normal visual input.</title><p>The sizes of the scales and the according patch sizes are indicated in blue for the coarse scale and red for the fine scale.</p></caption></media><p>To quantify vergence behavior in the model, we define the absolute vergence error. It measures by how much the vergence angle between the eyes deviates from the ideal position at the end of a fixation (see MaterialsÂ andÂ methods for details). The model obtains an accuracy of vergence eye movements of 0.12Â Â±Â 0.17Â° or 455.40Â Â± 613.75 arc sec. Note, however, that the model as described above has a much lower visual resolution compared to human foveal vision. One pixel in the model corresponds toÂ 802Â arcÂ sec, while the spacing between photoreceptors in the fovea corresponds to 28Â arcÂ sec. When we correct for the modelâs lower visual resolution (see MaterialsÂ andÂ methods), the corrected vergence accuracy is 15.9Â Â±Â 21.44Â arcÂ sec. This falls within the range of human performance under natural viewing conditions, which is typically better than 60Â arcÂ sec (1Â arcÂ min) for stimuli not closer than 0.5Â m (<xref ref-type="bibr" rid="bib47">Jaschinski, 1997</xref>; <xref ref-type="bibr" rid="bib48">Jaschinski, 2001</xref>).</p><p>A second critical test of a model of the development of stereoscopic vision is whether it can handle <italic>random-dot stereograms</italic> (RDSs), which represent the most challenging stimuli for stereopsis (<xref ref-type="bibr" rid="bib58">Lee and Olshausen, 1996</xref>; <xref ref-type="bibr" rid="bib13">Chen and Qian, 2004</xref>; <xref ref-type="bibr" rid="bib11">Chauhan et al., 2018</xref>). Since their introduction by <xref ref-type="bibr" rid="bib50">Julesz, 1971</xref> RDSs have been used extensively to investigate the human ability for stereoscopic vision. Nowadays, they are used in opthalmological examinations to asses stereo acuity as well as to detect malfunctions in the visual system, such as strabismus or amblyopia (<xref ref-type="bibr" rid="bib100">Walraven, 1975</xref>; <xref ref-type="bibr" rid="bib69">Okuda et al., 1977</xref>; <xref ref-type="bibr" rid="bib80">Ruttum, 1988</xref>). In these experiments, participants view a grid of random dots through a stereoscope or another form of dichoptic presentation. Typically, the central part is shifted in one of the two images which results in the perception of stereoscopic depth in healthy subjects. The advantage of this form of examination is that there are no monocular depth cues (such as occlusion, relative size, or perspective). The impression of depths arises solely because of the brainâs ability to integrate information coming from the two eyes.</p><p>To show that our model is able to perceive depth in RDS, although not having been trained on them, we generate various RDS and render the shifted images for the left and right eye separately. We expose the model that was trained on natural input stimuli to a range of RDS with different spatial frequencies, window sizes, disparities, and object distances. The model is able to exploit the differences in the images and align the eyes on the virtual plane that will appear in front or behind the actual object plane in the RDS. Averaged over all trials, the model achieves an absolute vergence error of 0.21Â Â±Â 0.22Â° at the end of a fixation. This corresponds to a corrected vergence accuracy of 26.8Â Â±Â 28.8Â arcÂ sec. This is only slightly worse than the modelâs performance on natural images (see Figure 6) and demonstrates that the model generalizes to artificial images it has never seen before. A video of the performance can be found in <xref ref-type="video" rid="video2">Video 2</xref>.</p><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-56212-video2.mp4"><label>Video 2.</label><caption><title>Vergence performance on a randomly generated set of RDS.</title></caption></media></sec><sec id="s2-3"><title>Altered rearing conditions cause qualitative changes in neural representations</title><p>A third critical test of any model of the development of binocular vision is whether it can account for the effects of alternate rearing conditions observed in biological experiments. We simulate such alternate rearing conditions by filtering the input images for the left and right eyes with Gaussian filters. The amount of blur was chosen to simulate experiments where animals where exposed to just one single orientation during development (<xref ref-type="bibr" rid="bib90">Stryker et al., 1978</xref>; <xref ref-type="bibr" rid="bib94">Tanaka et al., 2006</xref>). <xref ref-type="fig" rid="fig2">Figure 2</xref> shows illustrative examples of the filtered images that were used to train our model and the respective learned RFs. Here, we only depict the 10 RFs that have been selected most often during training for each scale. The full set of all RFs can be found in <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>.</p><p>When the model is trained with unaltered natural visual input, the resulting RFs resemble Gabor wavelets (<xref ref-type="bibr" rid="bib20">Daugman, 1985</xref>), as shown in the first row in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The changes that are applied to the input images in the alternate rearing conditions are reflected in the RFs that are learned: Among the 10 most often selected RFs there are no vertically (horizontally) oriented RFs, when the model is trained on images that are deprived of vertical (horizontal) edges. Orthogonal RFs emerge as a result of training on orthogonal input. When one eye is deprived of input, the RFs will become <italic>monocular</italic> and encode information coming from the âhealthyâ eye only. Strabismic rearing results in the development of monocular RFs without a preference for one or the other eye (<xref ref-type="bibr" rid="bib46">Hunt et al., 2013</xref>). In the following sections, we will quantify neuronsâ tuning properties for different rearing conditions and compare them to neurophysiological findings.</p></sec><sec id="s2-4"><title>Neuronsâ orientation tuning reflects input statistics</title><p>To analyze the statistics of the developing RFs in greater detail, we fit oriented two-dimensional Gabor wavelets to each RF (see MaterialsÂ andÂ methods for details). For this part of the analysis, the left and right parts of the binocular RFs are studied separately, thatÂ is, we consider the <italic>monocular</italic> RF fits only. We combine the results from coarse and fine scale, since a two-sample Kolmogorov-Smirnov test (<xref ref-type="bibr" rid="bib104">Young, 1977</xref>) did not reveal a statistically significant difference between the distributions of orientation preferences (p-values &gt; 0.18 for all rearing conditions). Only those RFs which met a criterion for a sufficiently good Gabor fit are considered for further analysis (98% of all bases, see Materials and methods for details).</p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows how the altered input changes the distribution of preferred orientations of the RFs. The <italic>normal</italic> case exhibits a clear over-representation of vertically (0Â°) and horizontally (90Â°) tuned RFs. This over-representation has been observed in different animals (<xref ref-type="bibr" rid="bib4">Appelle, 1972</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2003</xref>) and humans (<xref ref-type="bibr" rid="bib33">Furmanski and Engel, 2000</xref>) and is considered a neural correlate of the <italic>oblique effect</italic>. This phenomenon describes the relative deficiency in participantsâ performance in perceptual tasks on oblique contours as compared to the performance on horizontal or vertical contours (<xref ref-type="bibr" rid="bib4">Appelle, 1972</xref>). It has been argued that it stems from the over-representation of vertical and horizontal edges in natural images (<xref ref-type="bibr" rid="bib15">Coppola et al., 1998</xref>) and reflects the internal model of orientation distribution in the world (<xref ref-type="bibr" rid="bib35">Girshick et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Burge and Geisler, 2014</xref>). Furthermore, it may reflect aspects of the imaging geometry (<xref ref-type="bibr" rid="bib79">Rothkopf et al., 2009</xref>; <xref ref-type="bibr" rid="bib89">Straub and Rothkopf, 2021</xref>). Additionally, we cannot exclude the possibility that it is related to the rectangular pixel grid representing the input to our model.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Distributions of RFs' orientation preference for different rearing conditions.</title><p>Displayed are the preferred orientations resulting from fitting Gabor wavelets to the learned RFs of the left eye. Coarse and fine scale RFs have been combined (800 in total). The error bars indicate the standard deviation over five different simulations. <inline-formula><mml:math id="inf1"><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:math></inline-formula> describes the average number of RFs that passed the selection criterion for the quality of Gabor fitting (see Materials and methods).</p><p><supplementary-material id="fig3sdata1"><label>Figure 3âsource data 1.</label><caption><title>Orientation tuning for all rearing conditions.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig3-v2.tif"/></fig><p>While the distribution of orientations does not change much in the <italic>monocular</italic> and <italic>strabismic</italic> rearing case, we observe a marked difference to the normal case when certain orientations are attenuated in the input. The models trained on <italic>vertical</italic> input are missing the peak at horizontal orientations and vice versa for the <italic>horizontal</italic> case. Additionally, we find an increased number of neurons tuned to the dominant orientation in the input. These observations are consistent with animal studies (<xref ref-type="bibr" rid="bib90">Stryker et al., 1978</xref>; <xref ref-type="bibr" rid="bib94">Tanaka et al., 2006</xref>).</p><p>The separate analysis of the RFs in the left and right eye for the models that were trained on <italic>orthogonal</italic> input reveals the adaptation of each eye to its input statistics. Furthermore, we find that orthogonal RFs developed (also see fourth row in <xref ref-type="fig" rid="fig2">Figure 2</xref>) that have been observed in an orthogonal rearing study in cats (<xref ref-type="bibr" rid="bib61">Leventhal and Hirsch, 1975</xref>).</p></sec><sec id="s2-5"><title>The development of binocular receptive fields requires congruent binocular input</title><p>Another interesting feature of the neural representation that has been studied extensively in the context of alternate rearing is the <italic>binocularity</italic>. The binocularity index (BI) is used to assess how responsive a neuron is to inputs from the two eyes. A <italic>binocular</italic> neuron requires input from both eyes to respond maximally, while a <italic>monocular</italic> neuron is mostly driven by just one eye. To determine the binocularity indices for the neurons in our model, we use an adaptation of the original method from <xref ref-type="bibr" rid="bib43">Hubel and Wiesel, 1962</xref> (see MaterialsÂ andÂ methods for details). The binocularity index can vary from â1 (monocular left) over 0 (binocular) to +1 (monocular right).</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> depicts the binocularity distributions for the coarse and the fine scale for all rearing conditions. The models that were trained on input that is coherent between the left and right eye (top row) exhibit the majority of neurons falling in the bin with binocularity index 0. Neurons in this category receive about the same drive from the left and the right eye. In the <italic>normal</italic> case, more neurons fall into that bin than in the <italic>vertical</italic> and <italic>horizontal</italic> case. This is due to the ability of the model to perform precise vergence control: Since left and right image are almost identical most of the time, the great majority of basis functions will develop to encode the exact same input from both eyes. This, in turn, will result in the cells being completely binocular with a binocularity index of 0. This effect is even more pronounced at the coarse scale, where small residual disparities can no longer be resolved. In the vertical and horizontal rearing case, we observe a reduction in the number of cells that have a binocularity index around 0. We attribute this to the limited vergence performance in these cases, that we will analyse in the next sections.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Binocularity distributions for different rearing conditions.</title><p>The binocularity index ranges from â1 (monocular left) over 0 (binocular) to 1 (monocular right). Error bars indicate the standard deviation over five different simulations. <inline-formula><mml:math id="inf2"><mml:msub><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf3"><mml:msub><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula> are the average number of basis functions (out of a total of 400) that pass the selection criterion for Gabor fitting (see Materials and methods).</p><p><supplementary-material id="fig4sdata1"><label>Figure 4âsource data 1.</label><caption><title>Binocularity values for all rearing conditions for coarse and fine scale.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig4-v2.tif"/></fig><p>If, however, the input differs qualitatively for the two eyes (<xref ref-type="fig" rid="fig4">Figure 4</xref>, bottom row) the receptive fields will also differ in their monocular sub-parts. This can also be observed in <xref ref-type="fig" rid="fig2">Figure 2</xref> for the orthogonal, monocular, and strabismic case. Most cells become monocular, with a symmetric distribution for orthogonal and strabismic rearing. Monocular deprivation of the right eye leads to a distribution of binocularity indices that is biased toward the left eye.</p><p>Comparing our model to biological data, the modelâs pronounced peak of bincularity indices close to 0 in the normal case matches experimental findings (Figure 1 in <xref ref-type="bibr" rid="bib102">Wiesel and Hubel, 1963</xref> and Figure 5 in <xref ref-type="bibr" rid="bib44">Hubel and Wiesel, 1965</xref>). Animals trained on inputs deprived of certain orientations (Figure 6B in <xref ref-type="bibr" rid="bib90">Stryker et al., 1978</xref>) develop more monocular neurons, but most neurons remain binocular. This is in good agreement with our model.</p><p><xref ref-type="bibr" rid="bib90">Stryker et al., 1978</xref> also reared kittens on orthogonal input and report an increase in monocular neurons (their Figure 6A) when compared to the normal rearing data from Hubel and Wiesel. In comparison to the rearing on horizontal or vertical stripes, there are fewer binocular cells. The loss of binocular neurons that we see in our data is also reported in <xref ref-type="bibr" rid="bib41">Hirsch and Spinelli, 1970</xref>, who reared kittens on orthogonal stripes.</p><p>Finally, monocular rearing and the analysis of binocularity was performed in <xref ref-type="bibr" rid="bib102">Wiesel and Hubel, 1963</xref>. In their Figures 3 and 5, we see the development of completely monocular cells after visual deprivation of the other eye. The strabismic case was studied a few years later in <xref ref-type="bibr" rid="bib44">Hubel and Wiesel, 1965</xref> (their Figure 5A) and revealed a division of the neural population in monocular neurons for either left or right eye, in agreement with our model.</p></sec><sec id="s2-6"><title>Alternate rearing conditions reduce the number of disparity tuned neurons</title><p>A central aspect of the development of binocular vision is the emergence of neurons which are tuned to binocular, horizontal disparities. We therefore investigate how alternate rearing affects the number of neurons with disparity tuning and the distribution of their preferred disparities. We estimate horizontal disparity tuning by considering phase shifts between left and right RFs in the following way: We fit binocular Gabor wavelets to the RFs, where all parameters, except for the phase shift, are enforced to be identical for the left and right monocular RF. The disparity for one neuron can then be calculated as described in MaterialsÂ andÂ methods. The distribution of disparity tuning of the coarse scale neurons is shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> for the different rearing conditions. Results for the fine scale are comparable and presented in <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>. First, there is a noticeable difference in the number of cells that are disparity tuned between the different rearing conditions: In the normal case, we find the highest number of disparity tuned cells, rearing in a striped environment reduces the number, and uncorrelated input results in the smallest number of disparity tuned cells. In every case, the distribution of preferred disparities is peaked at zero. The height of this peak is reduced for rearing conditions with in-congruent input to the two eyes.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Distributions of neuronsâ preferred disparities for different rearing conditions.</title><p>The neuronsâ preferred disparities are extracted from the binocular Gabor fits. Presented are the averaged data for the coarse scale from five simulations. <inline-formula><mml:math id="inf4"><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:math></inline-formula> describes the average number of neurons meeting the selection criteria (see Materials and methods).</p><p><supplementary-material id="fig5sdata1"><label>Figure 5âsource data 1.</label><caption><title>Coarse scale disparity tuning for all rearing conditions.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig5-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5âsource data 2.</label><caption><title>Fine scale disparity tuning for all rearing conditions.Â .</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig5-data2-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig5sdata3"><label>Figure 5âsource data 3.</label><caption><title>Coarse scale disparity tuning for training with a constant angle of 3Â°.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig5-data3-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Disparity tuning for the fine scale.</title><p>Disparity tuning of the fine scale of models that were trained under different rearing conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Disparity tuning for training with a constant strabismic angle of 3Â°.</title><p>Disparity tuning of a model that was trained with a constant strabismic angle of 3Â°. Note the marked similarity to Figure 2 in <xref ref-type="bibr" rid="bib81">Shlaer, 1971</xref>. Depicted are the results from the coarse scale only.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig5-figsupp2-v2.tif"/></fig></fig-group><p>Comparing the normal with the vertical and horizontal case, there is an increase in the number of cells that are tuned to non-zero disparities. This indicates that under these rearing conditions, the agents are exposed to non-zero disparities more often. This is in good agreement with the results from the next section (also see <xref ref-type="fig" rid="fig6">Figure 6</xref>), where we will see that those models perform less accurate vergence movements compared to the normal case.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Vergence performance of models trained under different rearing conditions.</title><p>(<bold>A</bold>) Moving average of the vergence error during training. The vergence error is defined as the absolute difference between the actual vergence angle and the vergence angle required to correctly fixate the object. Shaded areas indicate the standard deviation over five different simulations. The curves for <italic>orthogonal</italic>, <italic>monocular</italic>, and <italic>strabismic</italic> conditions are overlapping, see text for details. (<bold>B</bold>) Vergence errors at the end of training after correction of any visual aberrations. Shown is the distribution of vergence errors at the end of a fixation (20 iterations) for previously unseen stimuli. Outliers have been removed. The gray shaded area indicates vergence errors below 0.22Â°, which corresponds to the modelâs resolution limit. The second y-axis shows values corrected to match human resolution (see MaterialsÂ andÂ methods for details).</p><p><supplementary-material id="fig6sdata1"><label>Figure 6âsource data 1.</label><caption><title>Training performance for all rearing conditions recorded every 10 iterations.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig6-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig6sdata2"><label>Figure 6âsource data 2.</label><caption><title>Performance at testing for all rearing conditions.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig6-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig6-v2.tif"/></fig><p>In the strabismic case, a neuronâs receptive fields in left and right eye are driven by un-corresponding input. This results in very few disparity tuned cells that exhibit a much broader distribution of preferred disparities.</p><p>To investigate the effect of a less severe strabismus we conduct an additional experiment similar to <xref ref-type="bibr" rid="bib81">Shlaer, 1971</xref> (see their FigureÂ 2). Here, we fix the strabismic angle to 3Â°, which results in a corresponding image in the two eyes because one input patch in the coarse scale covers an angle of 6.4Â°. <xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref> shows that this leads to an increased amount of disparity tuned cells and a shift of their preferred disparity to 3Â°. Exactly as in <xref ref-type="bibr" rid="bib81">Shlaer, 1971</xref>, the constant exposure to a certain disparity leads to a preference for that disparity for the majority of cells.</p></sec><sec id="s2-7"><title>Model predicts how alternate rearing conditions affect vergence learning</title><p>While the effect of alternate rearing conditions on receptive fields of visual cortical neurons is well studied, there has been little research on the effect of alternate rearing conditions on vergence behavior.</p><p><xref ref-type="fig" rid="fig6">Figure 6A</xref> shows the evolution of the absolute vergence error, that we interpret as the modelsâ stereo acuity, over the training time for the different rearing conditions. The models with <italic>normal</italic> or <italic>vertical</italic> rearing learn to verge the eyes on the same point on the object, resulting in the reduction of the vergence error to small values of around 0.3Â°. The model that learns on images without vertical edges (<italic>horizontal</italic> case) also learns vergence behavior, but does not reach the accuracy of the former models. The <italic>orthogonally, monocularly,</italic> and <italic>strabismically</italic> reared models show only random vergence movements and do not improve throughout the training period. Since we use the same random seeds for all simulations, including the initial weights and order of input images, the only difference between these models is the filtering applied to the images (the different rearing conditions). That difference alone is not sufficient to influence the behavior significantly. That is why the results for these three models overlap completely in <xref ref-type="fig" rid="fig6">Figure 6A</xref>.</p><p>The main difference to the models that were able to learn vergence is that under these conditions the left and right eye are provided with incongruent input. The orthogonal model receives two monocular images that retain different orientations. The right monocular image of the monocularly deprived model contains little information at all, and the two eyes are physically prevented from looking at the same object in the strabismic case. In these cases, very few neurons with disparity tuning emerge (compare previous section) that could drive accurate vergence eye movements.</p></sec><sec id="s2-8"><title>Vision remains impaired if input is corrected after the critical period</title><p>In biological vision systems, alterations of the visual input during a critical period of visual development (e.g. due toÂ astigmatism or cataract or experimentally induced alternate rearing conditions) lead to lasting visual deficits that can remain after visual input has been corrected. To test if a similar phenomenon arises in the model, we first train the model with alternate rearing conditions as described above. Then, we freeze all its synaptic weights and study its behavior for <italic>normal</italic> visual input. Specifically, objects are presented at distances <inline-formula><mml:math id="inf5"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¦</mml:mi><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> m, the initial vergence error is chosen randomly between â2 and 2Â°, and 40 stimuli that were not seen during training are applied on the object plane. From these initial conditions, we simulate fixations of 20 iterations and record the vergence error at the end.</p><p>The results of this testing are displayed in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. Here, the gray-shaded area indicates a vergence error of 1 pixel. The <italic>normally</italic> trained model exhibits the best performance and actually achieves sub-pixel accuracy in the great majority of trials. The model is more accurate here than in the training phase, because there is no exploration noise during action selection in this testing procedure. Performance declines somewhat for the <italic>vertical</italic>/<italic>horizontal</italic> models, which were trained on input without horizontal/vertical edges, respectively. Finally, performance for the <italic>orthogonal</italic>, <italic>monocular</italic> and <italic>strabismic</italic> models is very poor. This is due to their incongruent input to both eyes during training, which impairs the development of binocular neurons tuned to different disparities. Since this is the first study to investigate the quality of learned vergence movements after exposure to alternate rearing conditions (to the best of our knowledge), the differences in performance are a genuine prediction of our model.</p><p>To gain a deeper insight into the underlying mechanisms, we consider the modelâs <italic>reward landscape</italic> after training under the different rearing conditions. The modelâs reward is the <italic>negative reconstruction error</italic> of the sparse coders. This means that vergence angles that result in a low reconstruction error will be preferred. <xref ref-type="fig" rid="fig7">Figure 7</xref> shows the averaged reconstruction error over three different object distances and ten stimuli for the different rearing conditions. In the normal, vertical, and horizontal case, there is a pronounced minimum at zero disparity, which drives the model to fixate on the same point with both eyes. This is in contrast to the orthogonal, monocular, and strabismic conditions, where the reward landscape is flat, thatÂ is, there is no incentive to align the two eyes onto the same point.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Reward landscape at the end of training for the different rearing conditions.</title><p>Shown is the logarithm of the sparse coderâs reconstruction error as a function of disparity. The negative reconstruction error is used as the reward for learning vergence movements. Data are averaged over 10 stimuli not encountered during training, three different object distances (0.5, 3, and 6Â m), and five simulations for every condition. The shaded area represents one standard error over the five simulations. Only those models that receive corresponding input to left and right eye display a reconstruction error that is minimal at zero disparity. These are the only models that learn to verge the eyes.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7âsource data 1.</label><caption><title>Rewards for 5 random seeds, 3 object distances, 11 different disparity values, 10 different input images and 2 scales for all six rearing conditions.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig7-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig7-v2.tif"/></fig></sec><sec id="s2-9"><title>Model captures stereo vision deficits in aniseikonia and predicts increased number of neurons tuned to vertical disparities</title><p>Aniseikonia is a condition characterized by a perceived difference in the sizes of left and right eye images. It can occur naturally as result of anatomical or refraction differences of the two eyes, different spacing of photoreceptors in the retinas, or other neurological causes (<xref ref-type="bibr" rid="bib84">South et al., 2019</xref>). Aniseikonia can also be induced as result of the treatment of anisometropia (different refractive powers of the eyes) (<xref ref-type="bibr" rid="bib1">Achiron et al., 1997</xref>). In this scenario, spectacles or artificial lenses are used to correct the refractive power of one or both eyes to create a sharp image in both eyes. However, due to optical magnification this also leads to a difference in the image sizes. When this difference remains small (typically lower than 3%), it can be tolerated by the visual system. Larger values on the other hand lead to problems in fusing the images and a loss of stereopsis (<xref ref-type="bibr" rid="bib53">Katsumi et al., 1986</xref>; <xref ref-type="bibr" rid="bib68">Oguchi and Mashima, 1989</xref>; <xref ref-type="bibr" rid="bib1">Achiron et al., 1997</xref>). Aniseikonia may also occur in anisometropic patients after cataract surgery with implanted intraocular lenses (<xref ref-type="bibr" rid="bib54">Katsumi et al., 1992</xref>; <xref ref-type="bibr" rid="bib36">Gobin et al., 2008</xref>). A recent study reported 7.8% measured aniseikonia in an outpatient clinic cohort (<xref ref-type="bibr" rid="bib34">Furr, 2019</xref>).</p><p>Since little is known about the effects of aniseikonia on visual development or the potential benefits of correction (<xref ref-type="bibr" rid="bib85">South et al., 2020</xref>), we conducted a series of experiments to simulate the effects of aniseikonia. We achieve this by simply scaling up the right image by a certain factor and cutting the edges so left and right images retain the same size. The rest of the training procedure remains unchanged.</p><p><xref ref-type="fig" rid="fig8">Figure 8A</xref> shows the improvement of the stereoscopic acuity as measured by the absolute vergence error as a function of training time for four values of aniseikonia: 0%, 10%, 15% and 25%, where 0% aniseikonia corresponds to the <italic>normal</italic> model from previous sections. TenÂ percent of aniseikonia leads to slower learning and a slightly reduced vergence performance. While an improvement in vergence performance is still present for 15%, it completely fails for 25%. The increased size of the right image leads to partly incongruent input to the two eyes. As a result, an increased number of monocular RFs develops (<xref ref-type="fig" rid="fig8s1">Figure 8âfigure supplement 1A</xref>, see <xref ref-type="fig" rid="fig8s2">Figure 8âfigure supplement 2</xref> for a full set of RFs). The lack of congruent input to both eyes and the resulting lack of binocular receptive fields impairs the development of correct image fusion.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The effect of unequal image size (aniseikonia) on the development of binocular vision.</title><p>(<bold>A</bold>) Vergence error as a function of time for different degrees of aniseikonia. (<bold>B</bold>) Number of RFs tuned to <italic>vertical disparities</italic> for different degrees of aniseikonia during learning. (<bold>C</bold>) Stereo acuity when different degrees of aniseikonia are introduced after normal rearing. The solid line depicts a quadratic fit to the data. The <italic>corrected stereo acuity</italic> on the right y-axis corrects for the lower visual resolution of the model compared to humans.</p><p><supplementary-material id="fig8sdata1"><label>Figure 8âsource data 1.</label><caption><title>Vergence error over training time measured every 10 iterations for all degrees of aniseikonia (five random seeds).Â .</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig8-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig8sdata2"><label>Figure 8âsource data 2.</label><caption><title>Tuning to vertical disparities for all degrees of aniseikonia (five random seeds).</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig8-data2-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig8sdata3"><label>Figure 8âsource data 3.</label><caption><title>Vergence performance of <italic>normal</italic> models tested under different degrees of aniseikonia (five random seeds).</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig8-data3-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig8sdata4"><label>Figure 8âsource data 4.</label><caption><title>Binocularity values for all degrees of aniseikonia.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig8-data4-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig8sdata5"><label>Figure 8âsource data 5.</label><caption><title>Orientation tuning for all degrees of aniseikonia.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig8-data5-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig8sdata6"><label>Figure 8âsource data 6.</label><caption><title>Alls RFs for all degrees of aniseikonia.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig8-data6-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8âfigure supplement 1.</label><caption><title>Binocularity, orientation tuning, and the number of RFs tuned to the vertical orientation for models trained under different degrees of aniseikonia.</title><p>Effect of different degrees of induced aniseikonia on (<bold>A</bold>) binocularity and (<bold>B</bold>) orientation tuning (over five random seeds). (<bold>C</bold>) depicts the number of cells tuned to the vertical orientation (0Â°). The difference between 0% and 10% aniseikonia is significant with a p-value of <inline-formula><mml:math id="inf6"><mml:mrow><mml:mn>1.7</mml:mn><mml:mo>â¢</mml:mo><mml:mi>x</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8âfigure supplement 2.</label><caption><title>All RFs for different degrees of aniseikonia.</title><p>Full set of RFs from models trained under different degrees of aniseikonia.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig8-figsupp2-v2.tif"/></fig></fig-group><p>The different object sizes in the left and right image also lead to <italic>vertical disparities</italic>. For example, when fixating the center of a square, the upper edge of the square will be projected to different vertical positions for the two eyes due to the different sizes of the square in the two eyes. We can measure these vertical disparities in a similar way as we measured the horizontal disparities before (see Analysis of receptive fields). <xref ref-type="fig" rid="fig8">Figure 8B</xref> shows that the number of neurons tuned to vertical disparities initially increases with growing aniseikonia but then reduces again for 25% of aniseikonia. The key to understanding this phenomenon is considering binocularity (<xref ref-type="fig" rid="fig8s1">Figure 8âfigure supplement 1A</xref>) and vergence performance (<xref ref-type="fig" rid="fig8">Figure 8A</xref>): at 25% aniseikonia, vergence behavior does not develop and many neurons assume monocular RFs. This reduces the total number of neurons which are tuned to vertical disparites. The inverted U-shaped amount of RFs tuned to vertical disparities for increasing amounts of aniseikonia is a testable prediction of the model.</p><p>Interestingly, when looking at the orientation tuning in <xref ref-type="fig" rid="fig8s1">Figure 8âfigure supplement 1B</xref>, we observe that the number of cells tuned to the <italic>vertical orientation</italic> decreases with increasing aniseikonia (see <xref ref-type="fig" rid="fig8s1">Figure 8âfigure supplement 1C</xref> for a direct comparison). Since the distribution of orientations in the input images does not change by changing image size, we attribute this change in orientation preference to the modelâs (in-)ability to perform accurate vergence movements. As we will elaborate in the next section, the ability to detect different ranges of horizontal disparities results in an abundance of vertically tuned cells. When the visual system looses the ability to detect horizontal disparities and to verge, the number of vertical RFs decreases.</p><p>We also test the effects of a suddenly induced aniseikonia on a fully developed healthy visual system. <xref ref-type="bibr" rid="bib65">Lovasik and Szymkiw, 1985</xref> induced aniseikonia in healthy subjects and let them perform the Randot and Titmus stereo acuity tests. They found that the stereo acuity diminishes roughly quadratically with the level of aniseikonia. We simulate their experiments by taking a normally trained, healthy model, induce aniseikonia, and test it under the same conditions as before: frozen synaptic weights, novel test stimuli, and a whole range of different object distances and initial vergence errors. We interpret the mean absolute vergence error as the stereo acuity of that model. <xref ref-type="fig" rid="fig8">Figure 8C</xref> shows that the stereo acuity declines approximately quadratically with increasing aniseikonia as observed by <xref ref-type="bibr" rid="bib65">Lovasik and Szymkiw, 1985</xref>. When we correct for the modelâs lower visual resolution compared to humans (see Materials and methods), we find that the stereo acuity achieved by the model falls in the typical range of human stereo acuity (<xref ref-type="bibr" rid="bib16">Coutant and Westheimer, 1993</xref>; <xref ref-type="bibr" rid="bib8">Bohr and Read, 2013</xref>). In fact, our model appears to be somewhat more robust against larger values of aniseikonia than human subjects (<xref ref-type="bibr" rid="bib65">Lovasik and Szymkiw, 1985</xref>; <xref ref-type="bibr" rid="bib5">Atchison et al., 2020</xref>). We speculate that this is due to the absence of an interocular suppression mechanism in our model that may accentuate the effects of aniseikonia on stereo vision in humans.</p></sec><sec id="s2-10"><title>Model predicts how vergence influences the statistics of orientation preference</title><p>Our model also allows us to investigate, for the first time, how the quality of the vergence control influences the neural representation. As a baseline, we consider the orientation tuning of a reference model which is trained on normal visual input and learns an appropriate vergence policy. For simplicity, we only consider the fine scale in the following. We compare this model to a version that is trained on the same input images, but could not verge the eyes. Specifically, the sparse coder is trained normally, but the RL part is disabled. This model sees different disparities during training by looking at objects at different depths, but is not able to change this distribution of disparities to facilitate the encoding. We refer to this model as the ârandom disparityâ model. In another version of the model, we artificially always set the vergence angle to correctly fixate the objects. In this way, this model is never exposed to non-zero disparities (except for very small ones in the periphery that arise because of slightly different perspectives in the left and right eye). We refer to this version as the âzero disparityâ model.</p><p><xref ref-type="fig" rid="fig9">Figure 9A</xref> shows the fraction of neurons that are tuned to vertical orientations (0Â Â±Â 7.5Â°) for these three models. When the influence of the RL agent is removed, we observe a significant decrease in the number of vertically tuned neurons. This change must be caused by the different distributions of disparities that the models experience due to their different motor behavior, because all other parameters remain unchanged. In the model that was trained without disparities, we find the least amount of neurons tuned to vertical edges.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>The effect of vergence learning on the number of neurons tuned to vertical orientations.</title><p>(<bold>A</bold>) Fraction of RFs tuned to vertical orientations for different versions of the model (see text for details). Asterisks indicate a statistically significant difference between the samples as revealed by a students t-test (p-values are <inline-formula><mml:math id="inf7"><mml:mrow><mml:mn>7</mml:mn><mml:mo>â¢</mml:mo><mml:mi>x</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:mrow><mml:mn>1</mml:mn><mml:mo>â¢</mml:mo><mml:mi>x</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Fractions of fine scale RFs tuned to vertical orientations for models trained with (truncated) Laplacian disparity distributions of different standard deviations <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>. The value <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> corresponds to 0 disparity all the time, while <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> corresponds to an almost uniform disparity distribution. Error bars indicate the standard deviation over five different simulations. The black dotted line indicates the fraction of vertically tuned RFs in the <italic>normal</italic> model.</p><p><supplementary-material id="fig9sdata1"><label>Figure 9âsource data 1.</label><caption><title>Orientation tuning of the three different models.Â .</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig9-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig9sdata2"><label>Figure 9âsource data 2.</label><caption><title>Orientation tuning of models trained under different Laplacian disparity policies for coarse and fine scale.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig9-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9âfigure supplement 1.</label><caption><title>Number of RFs tuned to vertical orientations for coarse and fine scale combined.</title><p>Fractions of RFs tuned to vertical orientations for models trained with Laplacian disparity distributions of different standard deviations <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> for coarse and fine scale combined.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig9-figsupp1-v2.tif"/></fig></fig-group><p>To study the role of the distribution of experienced disparities more systematically, we train the sparse coder on different truncated Laplacian distributions of disparities. The distributions are heavy-tailed and centered around zero. The spread in this distribution is determined by <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>, the standard deviation. <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> means zero disparity all the time (corresponding to the zero disparity case), while the distribution becomes almost uniform for big values of <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>. <xref ref-type="fig" rid="fig9">Figure 9B</xref> shows how the number of vertically tuned neurons changes in response to different values of <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>. We find the smallest number of vertically tuned cells when the disparity is zero throughout the whole training. For very large <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> there are more vertical cells, but not as many as for smaller values which are different from zero. In fact for <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, which corresponds to a standard deviation of one pixel in the input image, the number of vertically tuned neurons is maximized.</p><p>An intuitive explanation for this over-representation of cells tuned to vertical orientations is given in <xref ref-type="fig" rid="fig10">Figure 10</xref>. Here, we depict a part of an input image at three different disparities. While the horizontal edge can be encoded by the same RF for all disparity values, the vertical edge demands three different RFs to represent the input pattern faithfully. A system that experiences these disparities in its inputs, needs to devote neural resources to represent them all. If the distribution of disparities becomes too wide, however, individual neurons will receive close to independent input from both eyes and disparities that lie in the range that can be represented by a single RF will be rare. This explains the reduction of the number of vertically tuned RFs for very wide disparity distributions (<xref ref-type="fig" rid="fig9">Figure 9B</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Intuition for the over-representation of vertical edges when different disparities have to be encoded.</title><p>Top: Input scence with marked input patch. Middle: Anaglyph rendering (left: green, right: magenta) of the patch for three different disparities. Two RF locations are highlighted (yellow and cyan circles). Bottom: RFs selected by the sparse coder to encode the inputs. While the RF that encodes the input in the cyan circle is the same for all disparities, the input inside the yellow circle can best be encoded by RFs that are tuned to the corresponding disparities.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig10-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>A major goal of Computational Neuroscience is the development of models that explain how the tuning properties of neurons develop and how they contribute to the behavior of the organism. Over the last decades, the dominant theoretical framework for understanding the development of tuning properties of sensory neurons has been the <italic>efficient coding hypothesis</italic>. It states that sensory tuning properties adapt to the statistics of the sensory signals. In this framework, the behavior of the organism has been largely neglected, however. Specifically, there has been hardly any work on how developing neural tuning properties shape behavior, how the developing behavior affects the statistics of sensory signals, and how these changing statistics feed back on neural tuning properties. We argue that understanding the development of sensory systems requires understanding this feedback cycle between the statistics of sensory signals, neural tuning properties and behavior. The <italic>active efficient coding</italic> (AEC) approach offered here extends classic theories of efficient coding by a behavior component to study this feedback cycle in detail. In AEC, both sensory coding and behavior are adapted to improve the systemâs coding efficiency. This coupling of perception and action is also a feature of the general framework of <italic>Active Inference</italic> (<xref ref-type="bibr" rid="bib2">Adams et al., 2013</xref>; <xref ref-type="bibr" rid="bib75">Parr and Friston, 2018</xref>). There, motor commands are generated to fulfill sensory predictions, while AEC offers a mechanism to adapt motor commands to improve sensory coding efficiency via reinforcement learning. Interestingly, the presented model does not make use of any efference copy of motor commands to help predict the next sensory input. Evidently, such feedback is not required to learn accurate stereoscopic vision. However, extending our model to incorporate efference copies of motor commands may still be interesting, forÂ example, for the case of calibrating pursuit eye movements, and is left for future work. Interestingly, both AEC and Active Inference have also been linked to higher level cognitive phenomena such as imitation (<xref ref-type="bibr" rid="bib32">Friston and Frith, 2015</xref>; <xref ref-type="bibr" rid="bib96">Triesch, 2013</xref>).</p><p>In the present study, we have focused on active binocular vision, where a simulated agent autonomously learns to fixate a target object with both eyes via vergence eye movements. All parts of our model self-organize in tandem to optimize overall coding efficiency. We have identified three critical tests that a model of the development of stereoscopic vision should pass and we have demonstrated that the proposed model passes all of them. Specifically, we have shown that (1) our model autonomously self-calibrates, reaching human-like vergence performance when correcting for differences in visual resolution. Second, it handles random dot stereograms, despite having never been exposed to such stimuli. Third, the model reproduces a wide range of findings from animal studies on alternate rearing conditions, which often show dramatic effects on neural representations and behavior. Beyond explaining the experimental findings, our model also predicts systematic changes in the learned vergence behavior in response to altered rearing conditions. In addition, the model predicts that the learning of accurate vergence behavior systematically influences the neural representation and offers a novel explanation for why vertical orientations tend to be over-represented in visual cortex compared to horizontal ones, at least in primates (<xref ref-type="bibr" rid="bib23">De Valois et al., 1982b</xref>) and humans (<xref ref-type="bibr" rid="bib103">Yacoub et al., 2008</xref>; <xref ref-type="bibr" rid="bib91">Sun et al., 2013</xref>). These predictions should be tested in future experiments. For example, in unilaterally enucleated animals, a bias in favor of vertical orientations over horizontal ones may be reduced or completely absent (<xref ref-type="bibr" rid="bib31">FrÃ©gnac et al., 1981</xref>).</p><p>By freezing the neural network after the training period, we also simulated the state of the brain after the critical period. Even after correcting the optical aberrations present during training we observed a reduced vergence performance for all alternate rearing regimes. This finding is in line with a large body of evidence suggesting that optical aberrations should be corrected as early as possible to facilitate healthy development of binocular vision (e.g. <xref ref-type="bibr" rid="bib21">Daw, 1998</xref>; <xref ref-type="bibr" rid="bib27">Fawcett et al., 2005</xref>, but also see <xref ref-type="bibr" rid="bib24">Ding and Levi, 2011</xref>).</p><p>While our results qualitatively match experimental findings, there are some quantitative differences. In particular, while the distribution of binocularity indices (<xref ref-type="bibr" rid="bib102">Wiesel and Hubel, 1963</xref>) and disparities (<xref ref-type="bibr" rid="bib86">Sprague et al., 2015</xref>) in healthy animals are relatively broad (<xref ref-type="bibr" rid="bib22">De Valois et al., 1982a</xref>; <xref ref-type="bibr" rid="bib87">Stevenson et al., 1992</xref>; <xref ref-type="bibr" rid="bib78">Ringach et al., 1997</xref>), we find more narrow ones in our model. These differences are likely due to a number of simplifications present in our model. In the brain, inputs from both eyes into primary visual cortex are organized into ocular dominance bands such that individual cortical neurons may receive input which is already biased toward one or the other eye (<xref ref-type="bibr" rid="bib60">LeVay et al., 1980</xref>; <xref ref-type="bibr" rid="bib17">Crowley and Katz, 2000</xref>). In contrast, in our model all neurons receive similar amounts of input from both eyes and are therefore already predisposed for becoming binocular cells. This might explain the modelâs narrower distribution of binocularity indices. Regarding the distribution of preferred disparities, animals raised under natural conditions will experience a broad range of disparities in different parts of the visual field, since objects in different locations will be at different distances. In the model, the visual input is quite impoverished, as it is dominated by a single large frontoparallel textured plane. Once this plane is accurately fixated, most parts of the visual field will appear at close to zero disparity. This may explain the narrower distribution of preferred disparities observed in the model.</p><p>Similarly, the distribution of preferred orientations in our model shows a very strong preference for horizontal and vertical, that is accentuated in comparison to biological data (<xref ref-type="bibr" rid="bib62">Li et al., 2003</xref>; <xref ref-type="bibr" rid="bib23">De Valois et al., 1982b</xref>). Possible reasons for this include the discrete, rectangular pixel grid with which visual inputs are sampled, the choice of our image data base (<xref ref-type="bibr" rid="bib70">Olmos and Kingdom, 2004a</xref>), which contains mostly man-made structures including buildings, etc., for which it is known that they contain an abundance of horizontal and vertical edges (<xref ref-type="bibr" rid="bib15">Coppola et al., 1998</xref>), and the modelâs restriction to the central portion of the visual field, where the oblique effect is more pronounced (<xref ref-type="bibr" rid="bib79">Rothkopf et al., 2009</xref>). To clarify the role of the input images, we repeated the main findings with a random selection of all sections from the McGill Database (Appendix 1) and indeed found that the over-representation of vertical and horizontal orientations is reduced.</p><p>Next to addressing the above limitations, an interesting topic for future work is to use the model to study the development of amblyopia. For this, we have recently incorporated an interocular suppression mechanism, since suppression is considered a central mechanism in the development of amblyopia (<xref ref-type="bibr" rid="bib26">Eckmann et al., 2019</xref>). Such models could be a useful tool for predicting the effectiveness of novel treatment methods (<xref ref-type="bibr" rid="bib74">Papageorgiou et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Gopal et al., 2019</xref>).</p><p>In conclusion, we have presented a computational model that sheds new light on the central role of behavior in the development of binocular vision. The model highlights how stimulus statistics, sensory representation and behavior are all inter-dependent and influence one another and how alternate rearing conditions affect every aspect of this system. The Active Efficient Coding approach pursued here may be suitable for studying various other sensory modalities across species.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>In the following, we describe the different components of the model, the experimental setup, and the analysis techniques. The implementation is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Klimmasch/AEC/">https://github.com/Klimmasch/AEC/</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e31b960da868232561e2456ea5bb4e4c55d3c24d;origin=https://github.com/Klimmasch/AEC;visit=swh:1:snp:6f2ac0c23a53a916cd90e63e4ef267e7f6a502b0;anchor=swh:1:rev:96e9ae2336937469a8f1602c178ea5e0cb8564b6">swh:1:rev:96e9ae2336937469a8f1602c178ea5e0cb8564b6</ext-link>;Â <xref ref-type="bibr" rid="bib56">Klimmasch, 2021</xref>).</p><sec id="s4-1"><title>Image processing</title><p>We use OpenEyeSim (<xref ref-type="bibr" rid="bib77">Priamikov and Triesch, 2014</xref>; <xref ref-type="bibr" rid="bib76">Priamikov et al., 2016</xref>) to render the left and right eye image. It comprises a detailed biomechanical model of the human oculomotor system and simulates a 3-dimensional environment. A rectangular plane is moved in front of the learning agent (perpendicular to the gaze direction). On it we apply greyscale textures from the McGill Calibrated Color Image Database (<xref ref-type="bibr" rid="bib71">Olmos and Kingdom, 2004b</xref>) to simulate objects at different depths. Specifically, we chose the man-made section from the McGill Database (<xref ref-type="bibr" rid="bib70">Olmos and Kingdom, 2004a</xref>), because its statistics may resemble the statistics of the indoor environments that a majority of infants grow up in. As a comparison, we repeated our main analysis with a random set of images across all sections of the McGill data base (see Appendix 1). Behind the textured plane there is a large background image, simulating a natural background behind objects of interest. This background image also prevents the agent from receiving trivial input.</p><p>Even tough it is possible to place three-dimensional objects inside OpenEyeSim, we opted for rendering natural input stimuli on a flat plane at different depths. On the one hand, this ensures natural input statistics, and on the other hand it enables us to uniquely define the correct vergence angle and measure the modelâs vergence performance (see Measuring the vergence error).</p><p>The two monocular images rendered by OpenEyeSim cover a vertical field of view of 50Â° and have 320Â pxÂ ÃÂ 240Â px (focal length <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>257.34</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). We use Matlab to extract single patches in different resolutions and combine corresponding patches from the left and right image. These binocular patches will be jointly encoded by the sparse coder. The <italic>coarse scale</italic> corresponds to 128Â pxÂ ÃÂ 128Â px in the original image (corresponds to 26.6Â°Â ÃÂ 26.6Â°) and is down-sampled by a factor of 4 to 32Â pxÂ ÃÂ 32Â px. The <italic>fine scale</italic> image corresponds to 40Â pxÂ ÃÂ 40Â px (8.3Â°Â ÃÂ 8.3Â°) and is not down-sampled. From coarse and fine scale we extract 8Â pxÂ ÃÂ 8Â px patches with a stride of 4 px and combine corresponding left and right patches to 16Â pxÂ ÃÂ 8Â px binocular patches (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). One patch in the coarse scale covers a visual angle of 6.6Â° and in the fine scale one patch covers 1.6Â°. In total, we generate 81 fine scale and 49 coarse scale patches that are subsequently normalized to have zero mean and unit norm.</p></sec><sec id="s4-2"><title>Sparse coding</title><p>The patches from coarse and fine scale are used in the sparse coding step to construct a neural representation of the visual input and to generate a reward signal that indicates the efficiency of this encoding. Each scale <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>S</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> comprises a dictionary of binocular basis functions (BFs) <inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">â¬</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We refer to them as receptive fields (RFs) for simplicity. We choose <inline-formula><mml:math id="inf22"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">â¬</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math></inline-formula> because less would result in a decline in vergence performance and more are computationally more expensive and do not improve performance (<xref ref-type="bibr" rid="bib59">Lelais et al., 2019</xref>).</p><p>Each input patch <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is reconstructed by a sparse linear combination of 10 BFs:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">â¬</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>Îº</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>â¢</mml:mo><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the vector of activations <inline-formula><mml:math id="inf24"><mml:msubsup><mml:mi>Îº</mml:mi><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> is allowed to have only 10 non-zero entries. The <inline-formula><mml:math id="inf25"><mml:msubsup><mml:mi>Îº</mml:mi><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> are chosen by <italic>matching pursuit</italic> (<xref ref-type="bibr" rid="bib66">Mallat and Zhang, 1993</xref>). This greedy algorithm selects the 10 BF from the respective dictionary that yield the best approximation <inline-formula><mml:math id="inf26"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of a patch and was chosen for computational efficiency (<xref ref-type="bibr" rid="bib105">Zhang et al., 2015</xref>). Using 10 BFs to encode the input leads to a qualitatively good reconstruction (<xref ref-type="bibr" rid="bib59">Lelais et al., 2019</xref>) and more would be computationally more expensive.</p><p>The total reconstruction error <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>E</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>S</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the scale, is calculated as the sum over all squared differences between all patches and their approximations:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover><mml:mpadded width="+2.8pt"><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The total reconstruction errors combined from both scales, <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is used as the <italic>negative reward</italic> during reinforcement learning (see following section). The average reconstruction errors per patch for each scale are also used to update the BFs via <italic>Gradient descent</italic>. This adaptation is achieved by a simple Hebbian learning rule (<xref ref-type="bibr" rid="bib72">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib106">Zhao et al., 2012</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>Î·</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:msubsup><mml:mi>Îº</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This formula implements a simple form of activity-dependent learning between a population of encoding neurons <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>Îº</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> and an error-detection population. Î· is the sparse coderâs learning rate and set to 0.2 throughout our simulations when learning was active. Varying this parameter (while <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>Î·</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) just influences the convergence speed of the RFs but does not influence tuning properties. After each update with <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> the weight vector of a RF is divided by its L2-norm to normalize it to unit length.</p><p>In the beginning of training, analogous to the state just before eye opening (<xref ref-type="bibr" rid="bib45">Huberman et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Hagihara et al., 2015</xref>), we initialize the RFs with random Gabor functions. Specifically, both the left eye and the right eye component of a binocular basis function have the shape of a Gabor function, but the two Gabor functions have independently drawn random orientations. We have verified that the results can also be achieved when RFs are initialized as Gaussian white noise. The use of random Gabors makes the vergence learning more stable and is biologically more plausible (<xref ref-type="bibr" rid="bib3">Albert et al., 2008</xref>).</p><p>For the next step (reinforcement learning), we generate a state representation in the form of a feature vector, where every entry describes the mean squared activation of one BF over the whole input image:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover><mml:mpadded width="+2.8pt"><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Îº</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Taken together, this feature vector <inline-formula><mml:math id="inf32"><mml:mi>F</mml:mi></mml:math></inline-formula> has <inline-formula><mml:math id="inf33"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">â¬</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> entries for both scales combined.</p><p>With this pooling procedure we simulate the activity of complex cells that integrate the firing rates of multiple simple cells that are distributed over the whole visual space (<xref ref-type="bibr" rid="bib30">Freeman and Ohzawa, 1990</xref>). In that sense, we achieve a marginalization over all positions and estimate what disparities are present in the input image. This is in line with approaches that utilize feature histograms to extract position-invariant features, for example to classify objects (<xref ref-type="bibr" rid="bib93">Swain and Ballard, 1991</xref>; <xref ref-type="bibr" rid="bib67">Mel, 1997</xref>). In these studies, it is common to normalize the coefficients/features in the histograms to make up for different sampling rates, different lighting conditions, etc. We do not need to normalize the pooled values, because in our case, there is a fixed number of active features (10) per image patch.</p></sec><sec id="s4-3"><title>Generation of motor commands</title><p>The angular position of the eyes are controlled by two extra-ocular eye muscles responsible for rotations around the vertical axis. This <italic>medial</italic> and <italic>lateral rectus</italic> are simulated utilizing an elaborate muscle model (<xref ref-type="bibr" rid="bib97">Umberger et al., 2003</xref>) inside OpenEyeSim (<xref ref-type="bibr" rid="bib77">Priamikov and Triesch, 2014</xref>; <xref ref-type="bibr" rid="bib76">Priamikov et al., 2016</xref>). Since we are interested in vergence movements only, we assume symmetrical eye movements so that the activities of the two muscles are mirrored for both eyes.</p><p>In contrast to recent models of <italic>active inference</italic> where a prediction of proprioceptive feedback is send to the muscles (<xref ref-type="bibr" rid="bib2">Adams et al., 2013</xref>; <xref ref-type="bibr" rid="bib75">Parr and Friston, 2018</xref>), we simply add a differential change in muscle innervation to the current muscle innervation. To generate those innervations (between [0,Â 1] in arbitrary units), we use reinforcement learning (<xref ref-type="bibr" rid="bib92">Sutton and Barto, 1998</xref>). Specifically, the model employs the CACLA+VAR algorithm from <xref ref-type="bibr" rid="bib98">Van Hasselt and Wiering, 2007</xref> that generates outputs in continuous action space. In short, it uses an actor-critic architecture (<xref ref-type="bibr" rid="bib38">Grondman et al., 2012</xref>), where the actor and critic use neural networks as function approximators. These neural networks receive the state vector <italic>s</italic><sub><italic>t</italic></sub> that is the concatenation of the BF activations from both scales (see previous section) and the current muscle innervations. The entries in <italic>s</italic><sub><italic>t</italic></sub> are scaled by Welfordâs algorithm (<xref ref-type="bibr" rid="bib101">Welford, 1962</xref>) to have zero mean and a fixed standard deviation.</p><p>The critic is a one-layer network that aims to learn the value of a state. From the state vector it approximates the discounted sum of all future rewards<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">â</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>Î³</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>r</italic><sub><italic>t</italic></sub> represents the reward achieved at time <inline-formula><mml:math id="inf34"><mml:mi>t</mml:mi></mml:math></inline-formula> and Î³ is the discount factor. To update this value network, we calculate the <italic>Temporal Difference Error</italic> (<xref ref-type="bibr" rid="bib95">Tesauro, 1995</xref>; <xref ref-type="bibr" rid="bib92">Sutton and Barto, 1998</xref>) as <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>Î´</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The parameters of the critic, <inline-formula><mml:math id="inf36"><mml:msup><mml:mi>Î¸</mml:mi><mml:mi>V</mml:mi></mml:msup></mml:math></inline-formula>, are initialized randomly and updated by<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:msubsup><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Î±</mml:mi><mml:msub><mml:mi>Î´</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where Î± represents the learning rate for updating the critic.</p><p>The actor is an artificial neural network with one hidden layer with <inline-formula><mml:math id="inf37"><mml:mi>tanh</mml:mi></mml:math></inline-formula> activation functions and a two-dimensional output that depicts changes in muscle innervation for the two relevant eye muscles (lateral and medial rectus). The generated motor outputs are random in the beginning and the network is updated whenever the given reward was higher than estimated by the critic:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>IF</mml:mtext><mml:mspace width="5.690551pt"/><mml:msub><mml:mi>Î´</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:msubsup><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Î²</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:msub><mml:mi>Î´</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mfrac><mml:mo>â</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where Î² is the actorâs learning rate, <inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the action selected by the actor at time <inline-formula><mml:math id="inf39"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the action that is actually executed. Adding Gaussian noise to the actorâs output to discover more favorable actions is called <italic>Gaussian exploration</italic>. The last term scales the update depending on how much better the action was than expected with respect to its standard deviation.</p><p>To keep the actorâs weights in check, we use a weight regularizer <inline-formula><mml:math id="inf41"><mml:mi>g</mml:mi></mml:math></inline-formula> in a scaling operation:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:msubsup><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mi/><mml:mpadded width="+2.8pt"><mml:mover accent="true"><mml:mo>â</mml:mo><mml:mi/></mml:mover></mml:mpadded><mml:mrow><mml:msubsup><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mi>Î²</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The convergence of the RL algorithm (<xref ref-type="bibr" rid="bib98">Van Hasselt and Wiering, 2007</xref>) is only guaranteed when the critic learns to represent the reward landscape on a faster timescale than the actor learns to find appropriate actions. Including this constraint, we conducted an exhaustive grid-based search for parameters that would minimize the <italic>root mean square error</italic> (RMSE) of the <italic>vergence error</italic> (see Measuring the vergence error) after 0.5 million iterations while ensuring the median being close to 0. The critic learning rate Î±, the actor learning rate Î², and the discount factor Î³ were varied between 0 and 1. The results were more stable when Î² decayed to 0. The number of hidden units in the actor <inline-formula><mml:math id="inf42"><mml:mi>L</mml:mi></mml:math></inline-formula> was varied between 5 and 500, explorative noise <inline-formula><mml:math id="inf43"><mml:msup><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> and weight regularization <inline-formula><mml:math id="inf44"><mml:mi>g</mml:mi></mml:math></inline-formula> between 10<sup>â4</sup> and 10<sup>â6</sup>, and the standard deviation in the feature vector <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between 10<sup>â1</sup> and 10<sup>â3</sup>. The following parameters were found to be optimal and are used throughout all experiments in this paper: <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>, Î² starts at 0.5 and linearly decreases to 0, <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf49"><mml:mrow><mml:msup><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf51"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>x</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-4"><title>Simulation of alternate rearing conditions</title><p>The deprivation of oriented edges is simulated by convolving the input images with elongated Gaussian kernels defined by:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represent the standard deviation in the horizontal/vertical direction.</p><p>Kernels with a large <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf54"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>) will blur out vertical (horizontal) edges. Specifically, to simulate the deprivation of horizontal orientations, <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> is set to 33Â px (to cover one patch in the coarse scale completely) and <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> to a small value of 0.1Â px. The numbers are reversed for the deprivation of vertical orientations. In the case of orthogonal rearing, the left eye receives an image deprived of horizontal orientations while the right eye receives one without vertical orientations. To make up for the small standard deviation of 0.1 in the direction that should not be impaired, the images in the <italic>normal</italic> case are convolved with a Gaussian kernel with <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The resulting filters are displayed in <xref ref-type="fig" rid="fig11">Figure 11</xref>.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>The filters used for the normal, vertical, horizontal, othogonal, and monocular models.</title><p><supplementary-material id="fig11sdata1"><label>Figure 11âsource data 1.</label><caption><title>The actual filters used during training.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-56212-fig11-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-fig11-v2.tif"/></fig><p>To simulate monocular deprivation (MD) we set <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>240</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the right input image only. The small patches that we extract from this strongly blurred image contain almost no high spatial frequencies.</p><p>A strabismus is artificially induced by rotating the right eye ball inwards as it is commonly done in biological experiments by fixating a prism in front of the eye or by cutting part of the lateral rectus muscle. In our Open-Eye-Simulator, however, we can rotate the eye by a specific angle. One input patch in the coarse scale covers 6.6Â°. When we set the strabismic angle to 3Â° there is still an overlap in the input images that will be reflected in the neural code. In contrast, when the strabismic angle is set to 10Â°, the input patches become completely uncorrelated. Examples of the changes done to the input images are displayed in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></sec><sec id="s4-5"><title>Analysis of receptive fields</title><sec id="s4-5-1"><title>Gabor fitting</title><p>To determine the orientations of the RFs we use MATLABâs implementation of the <italic>trust region reflective algorithm</italic> for non-linear curve fitting (<xref ref-type="bibr" rid="bib14">Coleman and Li, 1996</xref>) to fit them to two-dimensional Gabor functions as defined by:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Î¾</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo mathsize="142%" stretchy="false">â²</mml:mo><mml:mo>â£</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>Î¾</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>â¢</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo mathsize="142%" stretchy="false">â²</mml:mo><mml:mo>â£</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mtext>cos</mml:mtext><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo rspace="5.3pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Â with <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mi>x</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Here, <inline-formula><mml:math id="inf61"><mml:mi>f</mml:mi></mml:math></inline-formula> denotes the frequency, Ï the phase offset, Ï the standard deviation of the Gaussian envelope, Î¾ the spatial aspect ratio and Î¸ the orientation, where <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>â</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> corresponds to a vertically oriented Gabor function. We initialize the parameters randomly 150 times and fit the function either to the left or right RFs (or to both, see below). To evaluate the quality of the fits, we record the difference between the actual RFs and the Gabor fit. More specifically, the <italic>residual</italic> is defined as the sum of the squared differences in single pixel values between RFs and the fit. To compare the fits across the different experimental conditions, we only took those fits where this residual was less than or equal to 0.2. This accounts for more than 96% of all RFs in the healthy case. Another interpretation for these fits is a stimulus that maximally activates the particular neuron.</p></sec><sec id="s4-5-2"><title>Binocularity index</title><p>To assess the extent to wich a neuron is responsive to inputs from one vs. the other eye, <xref ref-type="bibr" rid="bib43">Hubel and Wiesel, 1962</xref> introduced the binocularity index. They determined a stimulus that maximizes the monocular response, and applied this stimulus separately in left or right eye to get the (monocular) neural responses <inline-formula><mml:math id="inf63"><mml:mi>L</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mi>R</mml:mi></mml:math></inline-formula>. Hubel and Wiesel then compared the responses and sorted each cell into one of 7 bins. The first bin contained all cells responsive only to the contralateral eye, the 7th contained all cells responsive only to the ipsilateral eye, the 4th contained all binocular cells and the rest was distributed between the other bins. To investigate the binocularity of a cell in the model, we compare their monocular response to the left and right Gabor fit. The eye with the greater response is the dominant eye for this neuron. Similar as in <xref ref-type="bibr" rid="bib43">Hubel and Wiesel, 1962</xref> we show the best stimulus (here the Gabor fit) to the dominant eye and the same stimulus to the non-dominant eye and record the responses <inline-formula><mml:math id="inf65"><mml:mi>L</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mi>R</mml:mi></mml:math></inline-formula>. We then calculate the binocularity index <inline-formula><mml:math id="inf67"><mml:mi>b</mml:mi></mml:math></inline-formula> as:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>such that the resulting binocularity index lies between â1 (monocular left) and +1 (monocular right), and 0 indicates a perfectly binocular neuron.</p></sec><sec id="s4-5-3"><title>Disparity tuning</title><p>To establish the (horizontal) disparity tuning of a binocular model neuron, we fit coupled Gabor functions to the left and right receptive fields. In doing so, we assume that all parameters are equal for the left and right monocular sub-region of the RFs except for the phase offset Ï, that can be different for left and right eye. Following the assumption that the disparity tuning in a binocular cell is encoded by means of this phase shift, we can calculate the preferred (horizontal) disparity <inline-formula><mml:math id="inf68"><mml:mi>d</mml:mi></mml:math></inline-formula> of a neuron by:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Î¸</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The maximally detectable disparity is given by the RF size, that is, the visual angle one binocular patch covers. RFs with a disparity preference bigger than the RF size are excluded from the analysis.</p><p>For calculating the preferred <italic>vertical</italic> disparity, we adapt <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> in the following way:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>f</mml:mi><mml:mi>sin</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For the number of neurons tuned to vertical disparites in <xref ref-type="fig" rid="fig8">Figure 8B</xref>, we consider only neurons with horizontal orientation preference (90Â°Â Â±Â 7.5Â°) and simply count all neurons in the population that do not fall into the zero disparity bin (0Â°Â Â±Â 0.6Â°).</p></sec></sec><sec id="s4-6"><title>Measuring the vergence error</title><p>Given the exact distance to an object (<inline-formula><mml:math id="inf69"><mml:msub><mml:mi>d</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>) and the inter-pupillary distance (<inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5.6</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) we can calculate the vergence angle desired for perfectly fixating this object as:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>arctan</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The absolute difference between this angle and the actual angle between the eyes, <inline-formula><mml:math id="inf71"><mml:mi>z</mml:mi></mml:math></inline-formula>, is called the vergence error and is used in our experiments to track the modelâs ability to use active binocular vision:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In our experiments, we use a textured plane with varying distances instead of a 3D environment. This provides us with an unambiguous measure of the distance to the objects and we can easily calculate <inline-formula><mml:math id="inf72"><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf73"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. While the vergence error can be defined at every time step, we only analyze it at the end of a fixation (corresponding to the last of 10 time steps), to give the model sufficient time to fixate the object.</p><p>When we look at the influence of the vergence movements on the neural representation (<xref ref-type="fig" rid="fig9">Figure 9B</xref>), we artificially set the vergence angle to simulate different disparity distributions. We use Laplacian distributions, centered around 0, with different standard deviations.</p><p>The probability density distribution of a Laplacian distributed random variable <inline-formula><mml:math id="inf74"><mml:mi>X</mml:mi></mml:math></inline-formula> is defined as<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>b</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>â</mml:mo><mml:mi>Î¼</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mo>â</mml:mo><mml:mi mathvariant="normal">â</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">â</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac></mml:mrow></mml:math></inline-formula> is the scaling parameter. We limit the vergence angle to lie between 0 (looking parallel) and 11.4 (looking at 0.28Â m). To simulate the disparity distribution, we set Î¼ to the angle that is desired to fixate an object at a certain distance <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>d</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î¼</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>arctan</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>and draw from the distribution. The data shown in <xref ref-type="fig" rid="fig9">Figure 9B</xref> depict the fine scale only. The results from the two-scale model can be found in <xref ref-type="fig" rid="fig9s1">Figure 9âfigure supplement 1</xref>.</p></sec><sec id="s4-7"><title>Correcting for lower visual resolution of the model compared to humans</title><p>Visual resolution in humans is (amongst other factors) constrained by the distance of photoreceptors on the central retina, which is around 2.5Â Âµm (<xref ref-type="bibr" rid="bib18">Curcio et al., 1990</xref>). Translated to visual angle, this corresponds to a resolution of <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>28</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mtext>Â </mml:mtext><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib51">Kalloniatis and Luu, 2007</xref>). In the model, visual resolution is constrained by the discrete sampling of the pixel array. Given the focal length <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>257.34</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from above, the angular resolution corresponds to <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>arctan</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mi>F</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mn>0.22</mml:mn><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>802</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mtext>Â </mml:mtext><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To convert measurements of the modelâs stereo acuity or vergence accuracy to human values, we therefore apply a conversion factor of <inline-formula><mml:math id="inf80"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>conversion</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mi>human</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>model</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.035</mml:mn></mml:mrow></mml:math></inline-formula> to both kinds of values. Note that doing this for vergence accuracy assumes that vergence performance is ultimately limited by visual processing constraints rather than motor constraints. Since our model neglects any motor noise and uses a continuous action space, this assumption is reasonable. We therefore equate vergence error with stereo acuity in the model.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Alexander Lichtenstein for providing access to the OpenEyeSim simulation environment. We also thank the NEURO-Dream consortium for stimulating discussions that inspired part of this work.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Investigation</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-56212-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. Source data files have been provided for all Figures displaying our own generated data.</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Olmos</surname><given-names>A</given-names></name><name><surname>Kingdom</surname><given-names>FAA</given-names></name></person-group><year iso-8601-date="2004">2004</year><data-title>McGill Calibrated Colour Image Database</data-title><source>McGill</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="http://tabby.vision.mcgill.ca/">tabby.vision.mcgill.ca/</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achiron</surname> <given-names>LR</given-names></name><name><surname>Witkin</surname> <given-names>N</given-names></name><name><surname>Primo</surname> <given-names>S</given-names></name><name><surname>Broocker</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Contemporary management of aniseikonia</article-title><source>Survey of Ophthalmology</source><volume>41</volume><fpage>321</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1016/S0039-6257(96)00005-7</pub-id><pub-id pub-id-type="pmid">9104769</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname> <given-names>RA</given-names></name><name><surname>Shipp</surname> <given-names>S</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictions not commands: active inference in the motor system</article-title><source>Brain Structure and Function</source><volume>218</volume><fpage>611</fpage><lpage>643</lpage><pub-id pub-id-type="doi">10.1007/s00429-012-0475-5</pub-id><pub-id pub-id-type="pmid">23129312</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname> <given-names>MV</given-names></name><name><surname>Schnabel</surname> <given-names>A</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Innate visual learning through spontaneous activity patterns</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000137</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000137</pub-id><pub-id pub-id-type="pmid">18670593</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appelle</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Perception and discrimination as a function of stimulus orientation: the &quot;oblique effect&quot; in man and animals</article-title><source>Psychological Bulletin</source><volume>78</volume><fpage>266</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1037/h0033117</pub-id><pub-id pub-id-type="pmid">4562947</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atchison</surname> <given-names>DA</given-names></name><name><surname>Lee</surname> <given-names>J</given-names></name><name><surname>Lu</surname> <given-names>J</given-names></name><name><surname>Webber</surname> <given-names>AL</given-names></name><name><surname>Hess</surname> <given-names>RF</given-names></name><name><surname>Baldwin</surname> <given-names>AS</given-names></name><name><surname>Schmid</surname> <given-names>KL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Effects of simulated anisometropia and aniseikonia on Stereopsis</article-title><source>Ophthalmic and Physiological Optics</source><volume>40</volume><fpage>323</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1111/opo.12680</pub-id><pub-id pub-id-type="pmid">32128857</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible principles underlying the transformations of sensory messages</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblith</surname> <given-names>W. A</given-names></name></person-group><source>Sensory Communication</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>217</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.001.0001</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname> <given-names>R</given-names></name><name><surname>Wilson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Binocular vision</article-title><source>Vision Research</source><volume>51</volume><fpage>754</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.10.009</pub-id><pub-id pub-id-type="pmid">20951722</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bohr</surname> <given-names>I</given-names></name><name><surname>Read</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stereoacuity with Frisby and revised FD2 stereo tests</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e82999</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0082999</pub-id><pub-id pub-id-type="pmid">24349416</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal disparity estimation in natural stereo images</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/14.2.1</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Candy</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The importance of the interaction between ocular motor function and vision during human infancy</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>201</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014741</pub-id><pub-id pub-id-type="pmid">31525140</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chauhan</surname> <given-names>T</given-names></name><name><surname>Masquelier</surname> <given-names>T</given-names></name><name><surname>Montlibert</surname> <given-names>A</given-names></name><name><surname>Cottereau</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of binocular disparity selectivity through hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9563</fpage><lpage>9578</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1259-18.2018</pub-id><pub-id pub-id-type="pmid">30242050</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chauhan</surname> <given-names>T</given-names></name><name><surname>HÃ©jja-Brichard</surname> <given-names>Y</given-names></name><name><surname>Cottereau</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modelling binocular disparity processing from statistics in natural scenes</article-title><source>Vision Research</source><volume>176</volume><fpage>27</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2020.07.009</pub-id><pub-id pub-id-type="pmid">32771554</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Qian</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A coarse-to-fine disparity energy model with both phase-shift and position-shift receptive field mechanisms</article-title><source>Neural Computation</source><volume>16</volume><fpage>1545</fpage><lpage>1577</lpage><pub-id pub-id-type="doi">10.1162/089976604774201596</pub-id><pub-id pub-id-type="pmid">15228745</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coleman</surname> <given-names>TF</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>An interior trust region approach for nonlinear minimization subject to bounds</article-title><source>SIAM Journal on Optimization</source><volume>6</volume><fpage>418</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1137/0806023</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coppola</surname> <given-names>DM</given-names></name><name><surname>Purves</surname> <given-names>HR</given-names></name><name><surname>McCoy</surname> <given-names>AN</given-names></name><name><surname>Purves</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The distribution of oriented contours in the real world</article-title><source>PNAS</source><volume>95</volume><fpage>4002</fpage><lpage>4006</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.7.4002</pub-id><pub-id pub-id-type="pmid">9520482</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutant</surname> <given-names>BE</given-names></name><name><surname>Westheimer</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Population distribution of stereoscopic ability</article-title><source>Ophthalmic and Physiological Optics</source><volume>13</volume><fpage>3</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1111/j.1475-1313.1993.tb00419.x</pub-id><pub-id pub-id-type="pmid">8510945</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowley</surname> <given-names>JC</given-names></name><name><surname>Katz</surname> <given-names>LC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Early development of ocular dominance columns</article-title><source>Science</source><volume>290</volume><fpage>1321</fpage><lpage>1324</lpage><pub-id pub-id-type="doi">10.1126/science.290.5495.1321</pub-id><pub-id pub-id-type="pmid">11082053</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname> <given-names>CA</given-names></name><name><surname>Sloan</surname> <given-names>KR</given-names></name><name><surname>Kalina</surname> <given-names>RE</given-names></name><name><surname>Hendrickson</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Human photoreceptor topography</article-title><source>The Journal of Comparative Neurology</source><volume>292</volume><fpage>497</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1002/cne.902920402</pub-id><pub-id pub-id-type="pmid">2324310</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dan</surname> <given-names>Y</given-names></name><name><surname>Atick</surname> <given-names>JJ</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>3351</fpage><lpage>3362</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-10-03351.1996</pub-id><pub-id pub-id-type="pmid">8627371</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daugman</surname> <given-names>JG</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><fpage>1160</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.001160</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>NW</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Critical periods and amblyopia</article-title><source>Archives of Ophthalmology</source><volume>116</volume><fpage>502</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1001/archopht.116.4.502</pub-id><pub-id pub-id-type="pmid">9565050</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname> <given-names>RL</given-names></name><name><surname>Albrecht</surname> <given-names>DG</given-names></name><name><surname>Thorell</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="1982">1982a</year><article-title>Spatial frequency selectivity of cells in macaque visual cortex</article-title><source>Vision Research</source><volume>22</volume><fpage>545</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(82)90113-4</pub-id><pub-id pub-id-type="pmid">7112954</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname> <given-names>RL</given-names></name><name><surname>Yund</surname> <given-names>EW</given-names></name><name><surname>Hepler</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1982">1982b</year><article-title>The orientation and direction selectivity of cells in macaque visual cortex</article-title><source>Vision Research</source><volume>22</volume><fpage>531</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(82)90112-2</pub-id><pub-id pub-id-type="pmid">7112953</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>J</given-names></name><name><surname>Levi</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recovery of Stereopsis through perceptual learning in human adults with abnormal binocular vision</article-title><source>PNAS</source><volume>108</volume><fpage>E733</fpage><lpage>E741</lpage><pub-id pub-id-type="doi">10.1073/pnas.1105183108</pub-id><pub-id pub-id-type="pmid">21896742</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doi</surname> <given-names>E</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Machado</surname> <given-names>TA</given-names></name><name><surname>Jepson</surname> <given-names>LH</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Gunning</surname> <given-names>DE</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient coding of spatial information in the primate retina</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>16256</fpage><lpage>16264</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4036-12.2012</pub-id><pub-id pub-id-type="pmid">23152609</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Eckmann</surname> <given-names>S</given-names></name><name><surname>Klimmasch</surname> <given-names>L</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Active efficient coding explains the development of binocular vision and its failure in amblyopia</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/571802</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fawcett</surname> <given-names>SL</given-names></name><name><surname>Wang</surname> <given-names>Y-Z</given-names></name><name><surname>Birch</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The critical period for susceptibility of human Stereopsis</article-title><source>Investigative Opthalmology &amp; Visual Science</source><volume>46</volume><fpage>521</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1167/iovs.04-0175</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleet</surname> <given-names>DJ</given-names></name><name><surname>Wagner</surname> <given-names>H</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural encoding of binocular disparity: energy models, position shifts and phase shifts</article-title><source>Vision Research</source><volume>36</volume><fpage>1839</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00313-4</pub-id><pub-id pub-id-type="pmid">8759452</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>R</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Shea</surname> <given-names>SL</given-names></name><name><surname>Dumais</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Stereopsis in human infants</article-title><source>Science</source><volume>207</volume><fpage>323</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1126/science.7350666</pub-id><pub-id pub-id-type="pmid">7350666</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname> <given-names>RD</given-names></name><name><surname>Ohzawa</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>On the neurophysiological organization of binocular vision</article-title><source>Vision Research</source><volume>30</volume><fpage>1661</fpage><lpage>1676</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90151-A</pub-id><pub-id pub-id-type="pmid">2288082</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FrÃ©gnac</surname> <given-names>Y</given-names></name><name><surname>Trotter</surname> <given-names>Y</given-names></name><name><surname>Bienenstock</surname> <given-names>E</given-names></name><name><surname>Buisseret</surname> <given-names>P</given-names></name><name><surname>Gary-Bobo</surname> <given-names>E</given-names></name><name><surname>Imbert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Effect of neonatal unilateral enucleation on the development of orientation selectivity in the primary visual cortex of normally and dark-reared kittens</article-title><source>Experimental Brain Research</source><volume>42</volume><fpage>453</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1007/BF00237510</pub-id><pub-id pub-id-type="pmid">7238684</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active inference, communication and hermeneutics</article-title><source>Cortex</source><volume>68</volume><fpage>129</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.025</pub-id><pub-id pub-id-type="pmid">25957007</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furmanski</surname> <given-names>CS</given-names></name><name><surname>Engel</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>An oblique effect in human primary visual cortex</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>535</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/75702</pub-id><pub-id pub-id-type="pmid">10816307</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furr</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Aniseikonia: a 21st century look</article-title><source>Journal of Binocular Vision and Ocular Motility</source><volume>69</volume><fpage>43</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1080/2576117X.2019.1603793</pub-id><pub-id pub-id-type="pmid">31058577</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname> <given-names>AR</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobin</surname> <given-names>L</given-names></name><name><surname>Rozema</surname> <given-names>JJ</given-names></name><name><surname>Tassignon</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Predicting refractive aniseikonia after cataract surgery in anisometropia</article-title><source>Journal of Cataract and Refractive Surgery</source><volume>34</volume><fpage>1353</fpage><lpage>1361</lpage><pub-id pub-id-type="doi">10.1016/j.jcrs.2008.04.023</pub-id><pub-id pub-id-type="pmid">18655987</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gopal</surname> <given-names>SK</given-names></name><name><surname>Kelkar</surname> <given-names>J</given-names></name><name><surname>Kelkar</surname> <given-names>A</given-names></name><name><surname>Pandit</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simplified updates on the pathophysiology and recent developments in the treatment of amblyopia: a review</article-title><source>Indian J OphthalmolÂ </source><volume>67</volume><fpage>1392</fpage><lpage>1399</lpage><pub-id pub-id-type="doi">10.4103/ijo.IJO_11_19</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grondman</surname> <given-names>I</given-names></name><name><surname>Busoniu</surname> <given-names>L</given-names></name><name><surname>Lopes</surname> <given-names>GAD</given-names></name><name><surname>Babuska</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A survey of Actor-Critic reinforcement learning: standard and natural policy gradients</article-title><source>IEEE Transactions on Systems, Man, and Cybernetics, Part C</source><volume>42</volume><fpage>1291</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1109/TSMCC.2012.2218595</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagihara</surname> <given-names>KM</given-names></name><name><surname>Murakami</surname> <given-names>T</given-names></name><name><surname>Yoshida</surname> <given-names>T</given-names></name><name><surname>Tagawa</surname> <given-names>Y</given-names></name><name><surname>Ohki</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal activity is not required for the initial formation and maturation of visual selectivity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1780</fpage><lpage>1788</lpage><pub-id pub-id-type="doi">10.1038/nn.4155</pub-id><pub-id pub-id-type="pmid">26523644</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Held</surname> <given-names>R</given-names></name><name><surname>Birch</surname> <given-names>E</given-names></name><name><surname>Gwiazda</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Stereoacuity of human infants</article-title><source>PNAS</source><volume>77</volume><fpage>5572</fpage><lpage>5574</lpage><pub-id pub-id-type="doi">10.1073/pnas.77.9.5572</pub-id><pub-id pub-id-type="pmid">6933571</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirsch</surname> <given-names>HV</given-names></name><name><surname>Spinelli</surname> <given-names>DN</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Visual experience modifies distribution of horizontally and vertically oriented receptive fields in cats</article-title><source>Science</source><volume>168</volume><fpage>869</fpage><lpage>871</lpage><pub-id pub-id-type="doi">10.1126/science.168.3933.869</pub-id><pub-id pub-id-type="pmid">5444065</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoyer</surname> <given-names>PO</given-names></name><name><surname>HyvÃ¤rinen</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Independent component analysis applied to feature extraction from colour and stereo images</article-title><source>Network: Computation in Neural Systems</source><volume>11</volume><fpage>191</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_11_3_302</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>Binocular interaction in striate cortex of kittens reared with artificial squint</article-title><source>Journal of Neurophysiology</source><volume>28</volume><fpage>1041</fpage><lpage>1059</lpage><pub-id pub-id-type="doi">10.1152/jn.1965.28.6.1041</pub-id><pub-id pub-id-type="pmid">5883731</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberman</surname> <given-names>AD</given-names></name><name><surname>Feller</surname> <given-names>MB</given-names></name><name><surname>Chapman</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms underlying development of visual maps and receptive fields</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>479</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125533</pub-id><pub-id pub-id-type="pmid">18558864</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname> <given-names>JJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Goodhill</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sparse coding can predict primary visual cortex receptive field changes induced by abnormal visual input</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003005</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003005</pub-id><pub-id pub-id-type="pmid">23675290</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaschinski</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Fixation disparity and accommodation as a function of viewing distance and prism load</article-title><source>Ophthalmic and Physiological Optics</source><volume>17</volume><fpage>324</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1111/j.1475-1313.1997.tb00064.x</pub-id><pub-id pub-id-type="pmid">9390377</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaschinski</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Fixation disparity and accommodation for stimuli closer and more distant than oculomotor tonic positions</article-title><source>Vision Research</source><volume>41</volume><fpage>923</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(00)00322-9</pub-id><pub-id pub-id-type="pmid">11248277</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname> <given-names>MI</given-names></name><name><surname>Mitchell</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Machine learning: trends, perspectives, and prospects</article-title><source>Science</source><volume>349</volume><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1126/science.aaa8415</pub-id><pub-id pub-id-type="pmid">26185243</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Julesz</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1971">1971</year><source>Foundations of Cyclopean Perception</source><publisher-loc>Chicago</publisher-loc><publisher-name>University of Chicago Press</publisher-name><pub-id pub-id-type="doi">10.1002/bs.3830170307</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kalloniatis</surname> <given-names>M</given-names></name><name><surname>Luu</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Webvision: The Organization of the Retina and Visual System [Internet]</source><publisher-name>University of Utah Health Sciences Center</publisher-name></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kandel</surname> <given-names>ER</given-names></name><name><surname>Schwartz</surname> <given-names>JH</given-names></name><name><surname>Jessell</surname> <given-names>TM</given-names></name><name><surname>of Biochemistry</surname> <given-names>D</given-names></name><name><surname>Jessell</surname> <given-names>MBT</given-names></name><name><surname>Siegelbaum</surname> <given-names>S</given-names></name><name><surname>Hudspeth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Principles of Neural Science</source><publisher-loc>New York</publisher-loc><publisher-name>McGraw-hill</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katsumi</surname> <given-names>O</given-names></name><name><surname>Tanino</surname> <given-names>T</given-names></name><name><surname>Hirose</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Effect of aniseikonia on binocular function</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>27</volume><fpage>601</fpage><lpage>604</lpage><pub-id pub-id-type="pmid">3957580</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katsumi</surname> <given-names>O</given-names></name><name><surname>Miyajima</surname> <given-names>H</given-names></name><name><surname>Ogawa</surname> <given-names>T</given-names></name><name><surname>Hirose</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Aniseikonia and stereoacuity in pseudophakic patients. Unilateral and bilateral cases</article-title><source>Ophthalmology</source><volume>99</volume><fpage>1270</fpage><lpage>1277</lpage><pub-id pub-id-type="doi">10.1016/s0161-6420(92)31813-5</pub-id><pub-id pub-id-type="pmid">1513582</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Klimmasch</surname> <given-names>L</given-names></name><name><surname>Lelais</surname> <given-names>A</given-names></name><name><surname>Lichtenstein</surname> <given-names>A</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning of active binocular vision in a biomechanical model of the oculomotor system</article-title><conf-name>2017 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics</conf-name><pub-id pub-id-type="doi">10.1109/DEVLRN.2017.8329782</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Klimmasch</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>AEC</data-title><source>Github</source><version designator="96e9ae2">96e9ae2</version><ext-link ext-link-type="uri" xlink:href="https://github.com/Klimmasch/AEC/">https://github.com/Klimmasch/AEC/</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kotsiantis</surname> <given-names>SB</given-names></name><name><surname>Zaharakis</surname> <given-names>I</given-names></name><name><surname>Pintelas</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Supervised machine learning: a review of classification techniques</article-title><source>Informatica</source><volume>31</volume><fpage>249</fpage><lpage>268</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>CW</given-names></name><name><surname>Olshausen</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A nonlinear hebbian network that learns to detect disparity in random-dot stereograms</article-title><source>Neural Computation</source><volume>8</volume><fpage>545</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.3.545</pub-id><pub-id pub-id-type="pmid">8868567</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lelais</surname> <given-names>A</given-names></name><name><surname>Mahn</surname> <given-names>J</given-names></name><name><surname>Narayan</surname> <given-names>V</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Autonomous development of active binocular and motion vision through active efficient coding</article-title><source>Frontiers in Neurorobotics</source><volume>13</volume><elocation-id>49</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2019.00049</pub-id><pub-id pub-id-type="pmid">31379548</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeVay</surname> <given-names>S</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name><name><surname>Hubel</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>The development of ocular dominance columns in normal and visually deprived monkeys</article-title><source>The Journal of Comparative Neurology</source><volume>191</volume><fpage>1</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1002/cne.901910102</pub-id><pub-id pub-id-type="pmid">6772696</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leventhal</surname> <given-names>AG</given-names></name><name><surname>Hirsch</surname> <given-names>HV</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Cortical effect of early selective exposure to diagonal lines</article-title><source>Science</source><volume>190</volume><fpage>902</fpage><lpage>904</lpage><pub-id pub-id-type="doi">10.1126/science.1188371</pub-id><pub-id pub-id-type="pmid">1188371</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>B</given-names></name><name><surname>Peterson</surname> <given-names>MR</given-names></name><name><surname>Freeman</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Oblique effect: a neural basis in the visual cortex</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>204</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1152/jn.00954.2002</pub-id><pub-id pub-id-type="pmid">12611956</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>Z</given-names></name><name><surname>Atick</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Efficient stereo coding in the multiscale representation</article-title><source>Network: Computation in Neural Systems</source><volume>5</volume><fpage>157</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_5_2_003</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lonini</surname> <given-names>L</given-names></name><name><surname>Forestier</surname> <given-names>S</given-names></name><name><surname>TeuliÃ¨re</surname> <given-names>C</given-names></name><name><surname>Zhao</surname> <given-names>Y</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robust active binocular vision through intrinsically motivated learning</article-title><source>Frontiers in Neurorobotics</source><volume>7</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2013.00020</pub-id><pub-id pub-id-type="pmid">24223552</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lovasik</surname> <given-names>JV</given-names></name><name><surname>Szymkiw</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Effects of Aniseikonia, anisometropia, accommodation, retinal illuminance, and pupil size on Stereopsis</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>26</volume><fpage>741</fpage><lpage>750</lpage><pub-id pub-id-type="pmid">3997423</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mallat</surname> <given-names>SG</given-names></name><name><surname>Zhang</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Matching pursuits with time-frequency dictionaries</article-title><source>IEEE Transactions on Signal Processing</source><volume>41</volume><fpage>3397</fpage><lpage>3415</lpage><pub-id pub-id-type="doi">10.1109/78.258082</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mel</surname> <given-names>BW</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>SEEMORE: combining color, shape, and texture histogramming in a neurally inspired approach to visual object recognition</article-title><source>Neural Computation</source><volume>9</volume><fpage>777</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.4.777</pub-id><pub-id pub-id-type="pmid">9161022</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oguchi</surname> <given-names>Y</given-names></name><name><surname>Mashima</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The influence of aniseikonia on the VEP by random-dot stereogram</article-title><source>Acta Ophthalmologica</source><volume>67</volume><fpage>127</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1111/j.1755-3768.1989.tb00740.x</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okuda</surname> <given-names>FC</given-names></name><name><surname>Apt</surname> <given-names>L</given-names></name><name><surname>Wanter</surname> <given-names>BS</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Evaluation of the TNO Random-Dot stereogram test</article-title><source>American Orthoptic Journal</source><volume>27</volume><fpage>124</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1080/0065955X.1977.11982436</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Olmos</surname> <given-names>A</given-names></name><name><surname>Kingdom</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2004">2004a</year><source>McGill Calibrated Colour Image Database</source><publisher-name>McGill Press</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olmos</surname> <given-names>A</given-names></name><name><surname>Kingdom</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2004">2004b</year><article-title>A biologically inspired algorithm for the recovery of shading and reflectance images</article-title><source>Perception</source><volume>33</volume><fpage>1463</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1068/p5321</pub-id><pub-id pub-id-type="pmid">15729913</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papageorgiou</surname> <given-names>E</given-names></name><name><surname>Asproudis</surname> <given-names>I</given-names></name><name><surname>Maconachie</surname> <given-names>G</given-names></name><name><surname>Tsironi</surname> <given-names>EE</given-names></name><name><surname>Gottlob</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The treatment of amblyopia: current practice and emerging trends</article-title><source>Graefe's Archive for Clinical and Experimental Ophthalmology</source><volume>257</volume><fpage>1061</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1007/s00417-019-04254-w</pub-id><pub-id pub-id-type="pmid">30706134</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname> <given-names>T</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Active inference and the anatomy of oculomotion</article-title><source>Neuropsychologia</source><volume>111</volume><fpage>334</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.01.041</pub-id><pub-id pub-id-type="pmid">29407941</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priamikov</surname> <given-names>A</given-names></name><name><surname>Fronius</surname> <given-names>M</given-names></name><name><surname>Shi</surname> <given-names>B</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>OpenEyeSim: a biomechanical model for simulation of closed-loop visual perception</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.1167/16.15.25</pub-id><pub-id pub-id-type="pmid">28006074</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Priamikov</surname> <given-names>A</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Openeyesim-a platform for biomechanical modeling of oculomotor control</article-title><conf-name>IEEE International Conference on Development and Learning and on Epigenetic Robotics</conf-name><pub-id pub-id-type="doi">10.1109/DEVLRN.2014.6983013</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname> <given-names>DL</given-names></name><name><surname>Hawken</surname> <given-names>MJ</given-names></name><name><surname>Shapley</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Dynamics of orientation tuning in macaque primary visual cortex</article-title><source>Nature</source><volume>387</volume><fpage>281</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1038/387281a0</pub-id><pub-id pub-id-type="pmid">9153392</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rothkopf</surname> <given-names>CA</given-names></name><name><surname>Weisswange</surname> <given-names>TH</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Learning independent causes in natural images explains the spacevariant oblique effect</article-title><conf-name>2009 IEEE 8th International Conference on Development and Learning</conf-name><pub-id pub-id-type="doi">10.1109/DEVLRN.2009.5175534</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruttum</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Visual screening with random dot stereograms</article-title><source>Seminars in Ophthalmology</source><volume>3</volume><fpage>175</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.3109/08820538809064577</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shlaer</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Shift in binocular disparity causes compensatory change in the cortical structure of kittens</article-title><source>Science</source><volume>173</volume><fpage>638</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1126/science.173.3997.638</pub-id><pub-id pub-id-type="pmid">5564596</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Vision and the statistics of the visual environment</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>144</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(03)00047-3</pub-id><pub-id pub-id-type="pmid">12744966</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>EC</given-names></name><name><surname>Lewicki</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient auditory coding</article-title><source>Nature</source><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>South</surname> <given-names>J</given-names></name><name><surname>Gao</surname> <given-names>T</given-names></name><name><surname>Collins</surname> <given-names>A</given-names></name><name><surname>Turuwhenua</surname> <given-names>J</given-names></name><name><surname>Robertson</surname> <given-names>K</given-names></name><name><surname>Black</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Aniseikonia and anisometropia: implications for suppression and amblyopia</article-title><source>Clinical and Experimental Optometry</source><volume>102</volume><fpage>556</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1111/cxo.12881</pub-id><pub-id pub-id-type="pmid">30791133</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>South</surname> <given-names>J</given-names></name><name><surname>Gao</surname> <given-names>T</given-names></name><name><surname>Collins</surname> <given-names>A</given-names></name><name><surname>Lee</surname> <given-names>A</given-names></name><name><surname>Turuwhenua</surname> <given-names>J</given-names></name><name><surname>Black</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Clinical aniseikonia in Anisometropia and amblyopia</article-title><source>British and Irish Orthoptic Journal</source><volume>16</volume><elocation-id>44</elocation-id><pub-id pub-id-type="doi">10.22599/bioj.154</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname> <given-names>WW</given-names></name><name><surname>Cooper</surname> <given-names>EA</given-names></name><name><surname>ToÅ¡iÄ</surname> <given-names>I</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Stereopsis is adaptive for the natural environment</article-title><source>Science Advances</source><volume>1</volume><elocation-id>e1400254</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.1400254</pub-id><pub-id pub-id-type="pmid">26207262</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname> <given-names>SB</given-names></name><name><surname>Cormack</surname> <given-names>LK</given-names></name><name><surname>Schor</surname> <given-names>CM</given-names></name><name><surname>Tyler</surname> <given-names>CW</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Disparity tuning in mechanisms of human Stereopsis</article-title><source>Vision Research</source><volume>32</volume><fpage>1685</fpage><lpage>1694</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90161-B</pub-id><pub-id pub-id-type="pmid">1455740</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stidwill</surname> <given-names>D</given-names></name><name><surname>Fletcher</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Normal Binocular Vision: Theory, Investigation and Practical Aspects</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/9781118788684</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Straub</surname> <given-names>D</given-names></name><name><surname>Rothkopf</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Looking for image statistics: active vision with avatars in a naturalistic virtual environment</article-title><source>Frontiers in Psychology</source><volume>12</volume><elocation-id>431</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2021.641471</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stryker</surname> <given-names>MP</given-names></name><name><surname>Sherk</surname> <given-names>H</given-names></name><name><surname>Leventhal</surname> <given-names>AG</given-names></name><name><surname>Hirsch</surname> <given-names>HV</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Physiological consequences for the cat's visual cortex of effectively restricting early visual experience with oriented contours</article-title><source>Journal of Neurophysiology</source><volume>41</volume><fpage>896</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.1152/jn.1978.41.4.896</pub-id><pub-id pub-id-type="pmid">681993</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>P</given-names></name><name><surname>Gardner</surname> <given-names>JL</given-names></name><name><surname>Costagli</surname> <given-names>M</given-names></name><name><surname>Ueno</surname> <given-names>K</given-names></name><name><surname>Waggoner</surname> <given-names>RA</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Cheng</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Demonstration of tuning to stimulus orientation in the human visual cortex: a high-resolution fMRI study with a novel continuous and periodic stimulation paradigm</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1618</fpage><lpage>1629</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs149</pub-id><pub-id pub-id-type="pmid">22661413</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swain</surname> <given-names>MJ</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Color indexing</article-title><source>International Journal of Computer Vision</source><volume>7</volume><fpage>11</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1007/BF00130487</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname> <given-names>S</given-names></name><name><surname>Ribot</surname> <given-names>J</given-names></name><name><surname>Imamura</surname> <given-names>K</given-names></name><name><surname>Tani</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Orientation-restricted continuous visual exposure induces marked reorganization of orientation maps in early life</article-title><source>NeuroImage</source><volume>30</volume><fpage>462</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.09.056</pub-id><pub-id pub-id-type="pmid">16275019</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tesauro</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Temporal difference learning and TD-Gammon</article-title><source>Communications of the ACM</source><volume>38</volume><fpage>58</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1145/203330.203343</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Triesch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Imitation learning based on an intrinsic motivation mechanism for efficient coding</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>800</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00800</pub-id><pub-id pub-id-type="pmid">24204350</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Umberger</surname> <given-names>BR</given-names></name><name><surname>Gerritsen</surname> <given-names>KG</given-names></name><name><surname>Martin</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A model of human muscle energy expenditure</article-title><source>Computer Methods in Biomechanics and Biomedical Engineering</source><volume>6</volume><fpage>99</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1080/1025584031000091678</pub-id><pub-id pub-id-type="pmid">12745424</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Van Hasselt</surname> <given-names>H</given-names></name><name><surname>Wiering</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reinforcement learning in continuous action spaces</article-title><conf-name>IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning</conf-name><pub-id pub-id-type="doi">10.1109/ADPRL.2007.368199</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinje</surname> <given-names>WE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title><source>Science</source><volume>287</volume><fpage>1273</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1126/science.287.5456.1273</pub-id><pub-id pub-id-type="pmid">10678835</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walraven</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Amblyopia screening with random-dot stereograms</article-title><source>American Journal of Ophthalmology</source><volume>80</volume><fpage>893</fpage><lpage>900</lpage><pub-id pub-id-type="doi">10.1016/0002-9394(75)90286-X</pub-id><pub-id pub-id-type="pmid">1190281</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welford</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Note on a method for calculating corrected sums of squares and products</article-title><source>Technometrics</source><volume>4</volume><fpage>419</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1080/00401706.1962.10490022</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesel</surname> <given-names>TN</given-names></name><name><surname>Hubel</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Single-cell responses in striate cortex of kittens deprived of vision in one eye</article-title><source>Journal of Neurophysiology</source><volume>26</volume><fpage>1003</fpage><lpage>1017</lpage><pub-id pub-id-type="doi">10.1152/jn.1963.26.6.1003</pub-id><pub-id pub-id-type="pmid">14084161</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Harel</surname> <given-names>N</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High-field fMRI unveils orientation columns in humans</article-title><source>PNAS</source><volume>105</volume><fpage>10607</fpage><lpage>10612</lpage><pub-id pub-id-type="doi">10.1073/pnas.0804110105</pub-id><pub-id pub-id-type="pmid">18641121</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname> <given-names>IT</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Proof without prejudice: use of the Kolmogorov-Smirnov test for the analysis of histograms from flow systems and other sources</article-title><source>Journal of Histochemistry &amp; Cytochemistry</source><volume>25</volume><fpage>935</fpage><lpage>941</lpage><pub-id pub-id-type="doi">10.1177/25.7.894009</pub-id><pub-id pub-id-type="pmid">894009</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Q</given-names></name><name><surname>Hu</surname> <given-names>X</given-names></name><name><surname>Zhang</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Comparison of lâ-Norm SVR and sparse coding algorithms for linear regression</article-title><source>IEEE Transactions on Neural Networks and Learning Systems</source><volume>26</volume><fpage>1828</fpage><lpage>1833</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2014.2377245</pub-id><pub-id pub-id-type="pmid">25532195</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>Y</given-names></name><name><surname>Rothkopf</surname> <given-names>CA</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A unified model of the joint development of disparity selectivity and vergence control</article-title><conf-name>2012 IEEE International Conference on Development and Learning and Epigenetic Robotics</conf-name><pub-id pub-id-type="doi">10.1109/DevLrn.2012.6400876</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>J</given-names></name><name><surname>Reynaud</surname> <given-names>A</given-names></name><name><surname>Yao</surname> <given-names>Z</given-names></name><name><surname>Liu</surname> <given-names>R</given-names></name><name><surname>Feng</surname> <given-names>L</given-names></name><name><surname>Zhou</surname> <given-names>Y</given-names></name><name><surname>Hess</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Amblyopic suppression: passive attenuation, enhanced dichoptic masking by the fellow eye or reduced dichoptic masking by the amblyopic eye?</article-title><source>Investigative Opthalmology &amp; Visual Science</source><volume>59</volume><fpage>4190</fpage><lpage>4197</lpage><pub-id pub-id-type="doi">10.1167/iovs.18-24206</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>J</given-names></name><name><surname>He</surname> <given-names>Z</given-names></name><name><surname>Wu</surname> <given-names>Y</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Liang</surname> <given-names>Y</given-names></name><name><surname>Mao</surname> <given-names>Y</given-names></name><name><surname>Yao</surname> <given-names>Z</given-names></name><name><surname>Lu</surname> <given-names>F</given-names></name><name><surname>Qu</surname> <given-names>J</given-names></name><name><surname>Hess</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Inverse occlusion: a binocularly motivated treatment for amblyopia</article-title><source>Neural Plasticity</source><volume>2019</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1155/2019/5157628</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname> <given-names>Q</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Autonomous, self-calibrating binocular vision based on learned attention and active efficient coding</article-title><conf-name>2017 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics</conf-name><pub-id pub-id-type="doi">10.1109/DEVLRN.2017.8329783</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname> <given-names>Q</given-names></name><name><surname>Triesch</surname> <given-names>J</given-names></name><name><surname>Shi</surname> <given-names>BE</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Joint Learning of Binocularly Driven Saccades and Vergence by Active Efficient Coding</article-title><source>Frontiers in Neurorobotics</source><volume>11</volume><elocation-id>58</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2017.00058</pub-id><pub-id pub-id-type="pmid">29163121</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Influence of the choice of input images</title><p>Here we compare our main results that were achieved using the <italic>man-made</italic> section of the McGill database (<xref ref-type="bibr" rid="bib70">Olmos and Kingdom, 2004a</xref>) to those obtained with input images from a random collection of images from this database that also contains the sections <italic>animals, foliage, flowers, fruits, landscapes, winter,</italic> and <italic>shadows</italic>.</p><p>Overall the results are very similar, except the reduction of RFs tuned to the vertical and horizontal orientation. This also results in a reduced effect of changing <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Since man-made environments typically contain many vertical and horizontal structures it is not surprising that this feature is accentuated in the RFsâ statistics as compared to those trained on a random sample of images.</p><fig id="app1fig1" position="float"><label>Appendix 1âfigure 1.</label><caption><title>Orientation tuning for five models trained with the man-made section or a random sample from the McGill database.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1âfigure 2.</label><caption><title>Binocularity values for five models trained with the man-made section or a random sample from the McGill database.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-app1-fig2-v2.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1âfigure 3.</label><caption><title>Disparity tuning for five models trained with the man-made section or a random sample from the McGill database.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-app1-fig3-v2.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1âfigure 4.</label><caption><title>Vergence acuity over training time (A) and at testing (B) for five models trained with the man-made section or a random sample from the McGill database.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-app1-fig4-v2.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1âfigure 5.</label><caption><title>Number of RFs tuned to the vertical orientation for different values of <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>, the standard deviation of the truncated Laplacian disparity input distribution, for five models trained with a random sample from the McGill database.</title><p>Compared to the man-made section in <xref ref-type="fig" rid="fig9">Figure 9B</xref>, we observe an increase in the number of RFs for small but non-zero values of <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> and a decrease for bigger values of <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>. However, the magnitude of the effect is reduced.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-app1-fig5-v2.tif"/></fig></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><fig id="app2fig1" position="float"><label>Appendix 2âfigure 1.</label><caption><title>In response to the reviewersâ comments, we tested the effect that patching the weak eye would have on the recovery from monocular deprivation (<xref ref-type="bibr" rid="bib108">Zhou et al., 2019</xref>).</title><p>To that end a model was trained under monocular deprivation, then normal visual input was reinstated and the weak eye received twice as much contrast as the other eye. This model, <italic>MD constrast adaptation</italic>, is compared to the <italic>normal</italic> and a reference model that did not receive an increased contrast (<italic>MD normal recovery</italic>), during training (<bold>A</bold>) and testing (<bold>B</bold>). All models trained under monocular deprivation can recover, when the RFs are still plastic. We do not observe a significant difference between the <italic>normal recovery</italic> and the <italic>contrast adaptation</italic>, probably because our model does not incorporate an interocular suppression mechanism that has been used to explain the effects of amblyopia on visual function (<xref ref-type="bibr" rid="bib107">Zhou et al., 2018</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56212-app2-fig1-v2.tif"/></fig></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56212.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name><role>Reviewing Editor</role><aff><institution>University of Nottingham</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Perrinet</surname><given-names>Laurent</given-names> </name><role>Reviewer</role><aff><institution>AMU CNRS</institution><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>While traditional models of the development of the sensory cortices have relied on efficient coding principles and passive information processing, more recently active models have started to appear that minimize reconstruction error. In this paper Klimmasch et al., show that such model reproduce a number of experimental phenomena when applied to binocular vision.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;The development of active binocular vision under normal and alternate rearing conditions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Laurent Perrinet (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This paper demonstrates that the Active Efficient Coding (AEC) model that Shi and Triesch have presented in several previous papers is also able to reproduce standard results regarding the effects of abnormal visual experience.</p><p>Essential revisions:</p><p>1. There seems to be quite a bit of overlap in this study and your recent PNAS paper (&quot;Active efficient coding explains the development of binocular vision and its failure in amblyopia&quot;). It is critical for you to provide a strong, detailed explanation of the key differences between the two studies and how the present work represents substantially new insights.</p><p>2. As we understand, the action perception loop is &quot;closed&quot; by enacting the differential change in eye movements to the simulator. It is therefore different from other strategies like Active Inference for which a feedback prediction signal may be sent from motor to sensory systems. This difference should be highlighted in the text. Also, we are surprised that the study uses a very elaborate simulation system but that you simply show a <italic>planar image of a natural image</italic> and not a three-dimensional scene. Do you observe vergence movements if you scan the image? Finally, concerning the experimental procedure it is specified line 100 that &quot;The plane is positioned in front the agent at variable distances&quot; but no detail is given until Figure 8B (or we missed something).</p><p>3. A simple mathematical treatment could likely predict some results you have found, for instance the property that the network detects the right vergence for RDS. As a binary white noise, one can predict the distribution of activated coefficients for different disparities, especially given the kind of fits you perform on the basis functions. Also some elements on the bibliography of modeling emergence of binocular disparity (e.g., doi: 10.1523/JNEUROSCI.1259-18.2018) are lacking. These points should be clarified, including possibly de-emphasizing the RDS results if their novelty and importance to the overall message cannot be better established.</p><p>4. Some details lack in the presentation of the model, in particular concerning sparse coding (lines 474-488 page 15). Indeed, you explain the sparse coding scheme, but not how you implement sparse Hebbian learning. Either more details (e.g., equations) or references to existing work are needed. Also, how do you control the equalization of the coefficients? The pooling procedure generates some form of histogram (marginalization over all positions) and it is lacking in figure 1 and the generality of this procedure should be illustrated, for instance by comparing it to other schemes using histograms of features to classify images.</p><p>5. A nice extension of this model would be to predict how one could recover disparity acuity after abnormal rearing such as strabismus. For instance, it has been shown that patching the weak eye could be more effective that the usual strategy of patching the dominant eye (see Zhou, J, Reynaud, A., Yao, Z., Liu, R., Feng, L., Zhou, Y. and Hess, R. F. (2018) Amblyopic suppression: passive attenuation, enhanced dichoptic masking by the fellow eye or reduced dichoptic masking by the amblyopic eye?. Investigative Ophthalmology and Visual Science, 59, 4190-4197. for instance). Could you predict such a methodology? Predict another one?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56212.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions</p><p>1. There seems to be quite a bit of overlap in this study and your recent PNAS paper (&quot;Active efficient coding explains the development of binocular vision and its failure in amblyopia&quot;). It is critical for you to provide a strong, detailed explanation of the key differences between the two studies and how the present work represents substantially new insights.</p></disp-quote><p>The PNAS paper [Eckmann et al., (2019)] extends previous Active Efficient Coding (AEC) models that learn disparity tuning and vergence control to the learning of accommodation control. It also includes an interocular suppression mechanism to capture the role of interocular suppression for the development of amblyopia in children suffering from anisometropia. In the present paper, in contrast, we simulate a large range of alternate rearing conditions used in animal experiments. This allows us to test the AEC approach against a large body of neurophysiological work. For example, we compare neuronal tuning properties observed experimentally with those developing in the model. Specifically, we provide a detailed analysis of the role of behavior for the development of the statistics of orientation and disparity tuning as well as binocularity. In terms of the model itself, a major difference lies in the utilized reinforcement learning model. While Eckmann et al., (2019) used a very simplistic approach with small numbers of discrete actions for vergence and accommodation control, we here use a more sophisticated continuous action reinforcement learning model and a biomechanical model of the extra-ocular eye muscles, enhancing the realism of the model. Finally, in the current work we also test the model on random dot stereograms (RDSs), the most challenging kind of stereoscopic input and demonstrate that the model can âsolve&quot; RDSs, despite never being trained on them. Finally, we also study the phenomenon of aniseikonia and how it interferes with stereoscopic vision. We have brought another expert on board, Maria Fronius, to advise us on various aspects of the development of binocular vision from biological and clinical perspectives. We have clarified all these differences in the Introduction.</p><disp-quote content-type="editor-comment"><p>2. As we understand, the action perception loop is &quot;closed&quot; by enacting the differential change in eye movements to the simulator. It is therefore different from other strategies like Active Inference for which a feedback prediction signal may be sent from motor to sensory systems. This difference should be highlighted in the text.</p></disp-quote><p>Yes, it is correct that the action-perception loop is closed through motor actions (vergence eye movements) changing the next sensory input. The model does not use an explicit prediction of the next sensory input based on an efference copy of the motor command. It turns out that this is not necessary for the system to learn to perform at subpixel accuracy. It is an open question if such a mechanism might speed up learning. In active inference, an agents acts to make sensory inputs match prior expectations. But it is not always clear where these prior expectations come from. We have addressed these points in the Discussion.</p><disp-quote content-type="editor-comment"><p>Also, we are surprised that the study uses a very elaborate simulation system but that you simply show a planar image of a natural image and not a three-dimensional scene. Do you observe vergence movements if you scan the image? Finally, concerning the experimental procedure it is specified line 100 that &quot;The plane is positioned in front the agent at variable distances&quot; but no detail is given until Figure 8B (or we missed something).</p></disp-quote><p>The planar object surface allows us to precisely define the ground-truth for the vergence angle desired to fixate an object. If different points on a presented object have different distances from the observer, then the correct vergence angle in no longer uniquely defined and we wanted to avoid this. It is important to point out, however, that the model does not require planar objects and we have shown in previous work that AEC models can work well in more complex naturalistic 3D scences [Zhu et al., (2017), Lelais et al., (2019)].</p><p>More detail was added to the Results and Methods.</p><disp-quote content-type="editor-comment"><p>3. A simple mathematical treatment could likely predict some results you have found, for instance the property that the network detects the right vergence for RDS. As a binary white noise, one can predict the distribution of activated coefficients for different disparities, especially given the kind of fits you perform on the basis functions.</p></disp-quote><p>Indeed, given Gabor-fitted receptive fields and white noise inputs it should be possible to analytically calculate the average activation of our simulated simple cells. However, we do not think that this would provide any deeper insights into the learning processes, on which we are focusing here. Our primary concern is how the receptive fields develop their binocular tuning properties for natural input, how this input and the emerging vergence behavior determine the population statistics of receptive fields and how the neural activities get mapped to appropriate behavior (vergence commands), which in turn changes the input statistics and so forth. Therefore, we think it is better not to clutter the (already long) manuscript with an additional analysis like this, although it could certainly be done if required.</p><disp-quote content-type="editor-comment"><p>Also some elements on the bibliography of modeling emergence of binocular disparity (e.g., doi: 10.1523/JNEUROSCI.1259-18.2018) are lacking. These points should be clarified, including possibly de-emphasizing the RDS results if their novelty and importance to the overall message cannot be better established.</p></disp-quote><p>More references were added to the Introduction describing previous work on the development of binocular disparities in the context of different computational modeling frameworks. The RDS section was also adapted.</p><disp-quote content-type="editor-comment"><p>4. Some details lack in the presentation of the model, in particular concerning sparse coding (lines 474-488 page 15). Indeed, you explain the sparse coding scheme, but not how you implement sparse Hebbian learning. Either more details (e.g., equations) or references to existing work are needed. Also, how do you control the equalization of the coefficients? The pooling procedure generates some form of histogram (marginalization over all positions) and it is lacking in figure 1 and the generality of this procedure should be illustrated, for instance by comparing it to other schemes using histograms of features to classify images.</p></disp-quote><p>We added a new section to our Methods, containing formulae and additional references. Our pooling procedure is analogous to the operation of complex cells that pool responses of multiple simple cells across some region of visual space. This is also similar to approaches that utilize feature histograms to extract position-invariant features, for example to classify objects [Swain and Ballard (1991), Mel (1997)]. In these studies, it is common to normalize the coefficients/features in the histograms to make up for different sampling rates, different lighting conditions, etc. Here, we do not need to normalize the pooled values, because in our case, there is a fixed number of active features (10) per image patch. To clarify all this, we added an explanatory section to the Methods.</p><p>To keep the simplicity of Figure 1, we did not include the pooling procedure here.</p><disp-quote content-type="editor-comment"><p>5. A nice extension of this model would be to predict how one could recover disparity acuity after abnormal rearing such as strabismus. For instance, it has been shown that patching the weak eye could be more effective that the usual strategy of patching the dominant eye (see Zhou, J, Reynaud, A., Yao, Z., Liu, R., Feng, L., Zhou, Y. and Hess, R. F. (2018) Amblyopic suppression: passive attenuation, enhanced dichoptic masking by the fellow eye or reduced dichoptic masking by the amblyopic eye?. Investigative Ophthalmology and Visual Science, 59, 4190-4197. for instance). Could you predict such a methodology? Predict another one?</p></disp-quote><p>Thank you for this suggestion, which motivated us to do an additional analysis. We have assumed that patching the weak eye results in a boost of the contrast sensitivity after removing the patch. To capture this in our model, we increased the contrast in the deprived eye of a monocularly reared model after normal visual input was reinstated. However, we saw no difference to a model that learned without this contrast adaptation. We can not rule out, however, that this could change if we included an interocular suppression mechanism to the model. We added these new results to the manuscript (see Appendix 2).</p><p>As another clinically relevant aspect, we also added experiments on a common side effect of the treatment of anisometropia: Correction of the refraction differences between the eyes can lead to different magnifications of the input images on the patients' retinas, a condition called aniseikonia. We show that aniseikonia impedes visual development and can lead to a complete loss of binocular function. Interestingly, we also find that the number of cells tuned to vertical disparities increases with increasing aniseikonia (until some degree), which represents another testable prediction of our model.</p></body></sub-article></article>