<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">94296</article-id><article-id pub-id-type="doi">10.7554/eLife.94296</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94296.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Decoding contextual influences on auditory perception from primary auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9106-0356</contrib-id><email>englitz@science.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Akram</surname><given-names>Sahar</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Elhilali</surname><given-names>Mounya</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2597-738X</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Shamma</surname><given-names>Shihab</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>Institute for Systems Research, University of Maryland</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Computational Neuroscience Lab, Donders Institute for Brain Cognition and Behavior</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zbnvs85</institution-id><institution>Research Data Science, Meta Platforms</institution></institution-wrap><addr-line><named-content content-type="city">Menlo Park</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Department of Electrical and Computer Engineering, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01dp7jr64</institution-id><institution>Equipe Audition, Ecole Normale Supérieure</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>09</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP94296</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-01-14"><day>14</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-12-24"><day>24</day><month>12</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.24.573229"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-04-26"><day>26</day><month>04</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94296.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-22"><day>22</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94296.2"/></event></pub-history><permissions><copyright-statement>© 2024, Englitz et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Englitz et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-94296-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-94296-figures-v1.pdf"/><abstract><p>Perception can be highly dependent on stimulus context, but whether and how sensory areas encode the context remains uncertain. We used an ambiguous auditory stimulus – a tritone pair – to investigate the neural activity associated with a preceding contextual stimulus that strongly influenced the tritone pair’s perception: either as an ascending or a descending step in pitch. We recorded single-unit responses from a population of auditory cortical cells in awake ferrets listening to the tritone pairs preceded by the contextual stimulus. We find that the responses adapt locally to the contextual stimulus, consistent with human MEG recordings from the auditory cortex under the same conditions. Decoding the population responses demonstrates that cells responding to pitch-changes are able to predict well the context-sensitive percept of the tritone pairs. Conversely, decoding the individual pitch representations and taking their distance in the circular Shepard tone space predicts the <italic>opposite</italic> of the percept. The various percepts can be readily captured and explained by a neural model of cortical activity based on populations of adapting, pitch and pitch-direction cells, aligned with the neurophysiological responses. Together, these decoding and model results suggest that contextual influences on perception may well be already encoded at the level of the primary sensory cortices, reflecting basic neural response properties commonly found in these areas.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ferret</kwd><kwd>human</kwd><kwd>decoding</kwd><kwd>auditory cortex</kwd><kwd>ambiguous percept</kwd><kwd>adaptation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.VIDI.189.052</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U01 AG058532</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name><name><surname>Shamma</surname><given-names>Shihab</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>EN919/1-1</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Neume</award-id><principal-award-recipient><name><surname>Shamma</surname><given-names>Shihab</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The perception of ambiguous steps in relative tone height is predicted by direction-selective cells in the auditory cortex, rather than the brain's represented distance between the tone heights.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In real world scenarios, the elements of the sensory environment do not occur independently (<xref ref-type="bibr" rid="bib41">Lewicki, 2002</xref>; <xref ref-type="bibr" rid="bib64">Smith and Lewicki, 2006</xref>). Temporal, spatial and informational predictability exists within and across modalities, already as a consequence of the basic physical properties, such as spatial and temporal continuity (<xref ref-type="bibr" rid="bib53">Rao and Ballard, 1999</xref>). Neural systems make efficient use of this inherent predictability of the environment in the form of expectations (<xref ref-type="bibr" rid="bib65">Sohn et al., 2019</xref>). Expectations are valuable, because they provide an internal mechanism to recognize stimuli faster (<xref ref-type="bibr" rid="bib63">Simon and Craft, 1989</xref>) and more reliably under noisy conditions (<xref ref-type="bibr" rid="bib40">Lawrance et al., 2014</xref>) with speech being of specific relevance for humans (<xref ref-type="bibr" rid="bib48">Norris et al., 2016</xref>). As a consequence, the same stimulus can be perceived differently, depending on the context it occurs in, for example which stimuli it is preceded by or which stimuli co-occur with it (<xref ref-type="bibr" rid="bib51">Phillips et al., 2017</xref>). We can thus study the expectation underlying a percept by studying the nature of the contextual influence.</p><p>Within audition, several forms of contextual influences have been found to shape perception. They range from spatial (e.g. localization in different contexts), to grouping (e.g. ABA sequences, <xref ref-type="bibr" rid="bib4">Bregman, 1994</xref>) and phonetic (<xref ref-type="bibr" rid="bib33">Holt, 2005</xref>) contextual influences. A striking example in human communication concerns the perception of certain syllable sequences, such as an ambiguous syllable between /ga/ and /da/ preceded by either /al/ or /ar/. In both, the second syllable is physically identical, but is heard as <italic>/da/</italic> or <italic>/ga/</italic> depending on the preceding syllable (<xref ref-type="bibr" rid="bib43">Lotto and Holt, 2006</xref>). Subsequent psychoacoustic investigations have revealed that this effect still occurs if the preceding syllable was replaced by appropriately chosen tone sequences, that it persists with substantial silent gaps between the two syllables, and that only very few tones are in fact necessary to bias the percepts one way or another (<xref ref-type="bibr" rid="bib33">Holt, 2005</xref>). Hence, these contextual effects are likely not linguistic in nature, but reflect more basic adaptive neural mechanisms. Different interpretations have been provided to interpret these findings, such as the enhancement of contrasts (<xref ref-type="bibr" rid="bib33">Holt, 2005</xref>; <xref ref-type="bibr" rid="bib66">Ulanovsky et al., 2003</xref>).</p><p>Here, we investigate the neural correlates of these contextual effects using a simplified paradigm, in which the context reliably biases the percept of an <italic>ambiguous</italic> acoustic stimulus: A sequence of two <italic>Shepard</italic> tones (<xref ref-type="bibr" rid="bib62">Shepard, 1964</xref>), differing by half an octave in the frequencies of its constituent tones, can be perceived as ascending or descending in pitch. Shepard tones are complex tones with octave spaced constituent tones (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This percept can be reliably manipulated by presenting a suitably chosen sequence of Shepard tones before, setting up different contexts (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). This contextual influence is highly effective, rapidly established and can last for multiple seconds (<xref ref-type="bibr" rid="bib55">Repp, 1997</xref>; <xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>). Importantly, the ability to determine the changes in pitch has relevance for a wide spectrum of real-world tasks, ranging from distinguishing an approaching from a departing vehicle to distinguishing different emotions in human communication (<xref ref-type="bibr" rid="bib10">Chebbi and Ben Jebara, 2018</xref>; <xref ref-type="bibr" rid="bib24">Ethofer et al., 2006</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimulus design and recording techniques.</title><p>(<bold>A</bold>) Shepard tones are acoustic complexes, composed of octave-spaced pure-tones (top). Each Shepard tone is uniquely characterized by its base frequency f<sub>base</sub>, and the difference between two Shepard tones by the difference of their base frequencies f<sub>diff</sub>, usually given in semitones. A Shepard tone shifted by a full octave ‘projects’ onto itself, and is therefore the physically same stimulus. The space of Shepard tones therefore forms a circle (bottom), in which each stimulus can be characterized as a so-called pitch class in semitones, which runs from 0 to 12, corresponding to a full octave. The base pitch class 0 was here chosen to correspond to f<sub>base</sub> = 440 Hz. We use the displayed color mapping (hue) throughout the paper. (<bold>B</bold>) We used the tritone paradox – a sequence of two Shepard tones – to investigate how ambiguous percepts are resolved, for example by preceding stimuli. In the tritone paradox, two Shepard tones are presented, which are separated by half an octave (6 semitones). Listeners, asked to judge the relative pitch between the two Shepard tones, are ambiguous as to their percept of an ascending or a descending step. If the ambiguous Shepard pair is preceded with a sequence of Shepard tones with pitch classes above the first but below the second tone (red area), listeners report an ascending percept. Conversely, if the ambiguous Shepard pair is preceded by a sequence of Shepard tones with pitch classes below the first, but above the second tone (blue area), listeners perceive a descending step. The neural representation of this contextual influence is not known, and we conducted a series of physiological and psychophysical experiments to elucidate the neural basis. (<bold>C</bold>) Neural responses from individual neurons were collected in awake ferrets from the auditory cortex (left). Individual neurons modulated their firing rate during the presentation of the stimulus sequence and exhibited tuned responses (see <bold>Fig. S1</bold>). Using MEG recordings, we also collected neural responses from populations of neurons in auditory cortex from human subjects, performing the up/down discrimination task. The amplitude of the magnetic field was modulated as a function of time during the stimulus presentation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig1-v1.tif"/></fig><p>We presented various Shepard tone sequences to awake ferrets and humans while simultaneously performing single-unit population recordings using chronically implanted electrode arrays in the left primary auditory cortex, and MEG (magnetoencephalographic) recordings, respectively. Exposure to the contextual sequence resulted in localized adaptation that faded over the time-course of ~1 s, consistent with stimulus specific adaptation (<xref ref-type="bibr" rid="bib66">Ulanovsky et al., 2003</xref>) and with findings from a related human MEG study (<xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>). A straight-forward decoding approach demonstrates that the perceived pitch-change direction can be directly related to the contextually adapted activity of direction selective cells, that is cells that have a preference for sounds with ascending or descending frequency over time (<xref ref-type="bibr" rid="bib7">Brosch and Schreiner, 2000</xref>; <xref ref-type="bibr" rid="bib6">Brosch et al., 1999</xref>). Conversely, decoding the represented Shepard tone pitches and their respective differences, predicts a repulsive effect, opposite to the perceived direction of pitch change. These underlying neuronal adaptation dynamics are consistent with changes in neural activity in the auditory cortex estimated from human MEG recordings collected for the same sounds.</p><p>We can account for these effects in a simplified model of the cortical representation based on known properties of pitch-change selective cells, which matches both the results from the directional- and the distance-decoding analysis. Further, the model is consistent with multiple observed properties of the neural representation, including tuning changes and directional tuning of individual neurons as well as the build-up of the contextual effect in humans.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We collected neural recordings from 7 awake ferrets (662 responsive, tuned single units) and from 16 humans (MEG recordings) in response to sequences of Shepard tones (the ‘Bias’) followed by an ambiguous, semi-octave separated test pair (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The human participants performed a two-alternative forced choice task, selecting between hearing an ascending or descending step in the test pair, while ferrets listened passively. It has been previously shown (<xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>; <xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>) that the presence of the Bias reliably influences human perception, towards hearing a pitch step ‘bridging’ the location of the Bias, that is if the Bias is located above the first tone an ascending step is heard (<xref ref-type="fig" rid="fig1">Figure 1B</xref> middle), and a descending one, if it is below (<xref ref-type="fig" rid="fig1">Figure 1B</xref> bottom). The present study investigates the neural representation underlying these modified percepts.</p><p>In the following, we first show that the bias sequence induces a local adaptation in the neural population activity in both passive (ferret) and active (human) condition (<xref ref-type="fig" rid="fig2">Figure 2</xref>, see Discussion for more details). Next, we demonstrate the effect of this adaptation on the stimulus representation by decoding the neural population response. We find a repulsive influence of the Bias sequence on the pitch classes in the pair, that is the pitch class of each Shepard tone shifts away from the Bias. This increases their distance in pitch class along the perceived direction (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and is thus not compatible with a simple, pitch (class)-distance based decoder as an explanation of the directionality percept (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We then provide a simplified model of neuronal activity in auditory cortex that captures both the population representation and the adaptive changes in tuning of individual cells (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Finally, based on this model, we provide an alternative explanation for the directionality percept of the ambiguous pair by showing that the adaptation pattern of directional cells predicts the percept (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural responses adapt locally during the Bias sequence for awake ferrets (top) and behaving humans (bottom).</title><p>(<bold>A1</bold>) During the presentation of the Bias sequence (10 tones, black bars), the neural response adapts over time (individual cell). This adaptation occurs for all parts of the response, shown here is the onset part (0–50ms, black). See <xref ref-type="fig" rid="fig1">Figure 1</xref> for more details on the different response types. Errorbars denote 2 SEM across trials. (<bold>A2</bold>) On the population level, the response reaches an adapted plateau 13% below the initial response after about 5 stimuli (<italic>τ</italic>=3.9 stimuli, also for the onset response). This rate of reduction is similar to the rate of build-up of perceptual influence in human behavior (<xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>; <xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>). Errorbars denote 2 SEM across neurons. (<bold>A3</bold>) After the Bias the activity of the cells is significantly more reduced (Δ=33%, p=0.001) around the center or the Bias (&lt;2.5 semitones from the center) compared to the edges (2.5–3 semitones from the center). Errorbars denote 2 SEM across neurons. (<bold>A4</bold>) Recovery of spontaneous neural firing rates from the adaptation due to the Bias sequence progressed over multiple seconds with an exponential recovery time-constant of 1.2 s. (<bold>B1</bold>) In human auditory cortex the Bias sequences also evoked an adapting sequence of responses, here shown is the activity for a single subject (#8). Errorbars denote 2 SEM across trials. (<bold>B2</bold>) On average, the adaptation of the neural response proceeded with a similar time course as the single-unit response (<bold>A2</bold>), and plateaus after about 3–4 stimuli. Errorbars denote 2 SEM across subjects. (<bold>B3</bold>) Following the Bias, the activity state of the cortex is probed with a sequence of brief stimuli (35ms Shepard tones, after 0.5 s silence). Responses to probe tones in the same (red) semi-octave are significantly reduced (21% for the first time window, signed ranks test, p&lt;0.0001) compared to the corresponding response in the opposite semi-octave (blue), indicating a local effect of adaptation. Errorbars denote one SEM across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Auditory cortex neurons respond tuned to Shepard tones.</title><p>Three representative neurons are shown, which span the range of observed response types. Neurons differed in their pitch class tuning and their temporal response profiles. The top row depicts the response pattern for 10 repetitions of a fixed Bias sequence, in the bottom row the responses to all presented Shepard tones during the Bias sequences have been sorted by pitch class, much like in a classical pure tone based tuning. (<bold>A</bold>) Most cells exhibited an onset type tuning, that is a brief response within the first 50ms (orange) each after stimulus onset, but no sustained (red, 50–100ms) or offset response (black, 0–50ms in the silence after the stimulus). These cells typically showed broad/complex tuning w.r.t. to the pitch class of the stimulus. The errorbars indicate one SEM based on the repetitions of the stimulus. (<bold>B</bold>) A considerable fraction of cells had a surprisingly sharp pitch class tuning, with tuning half-widths of only 2–3 semitones, leading to comparably sparse response patterns. These cells often showed co-tuned onset and sustained responses. (<bold>C</bold>) The remaining cells exhibited predominant offset pitch class tunings, which showed similarly broad/complex tunings as the onset tuned cells. (<bold>D</bold>) Distribution of response types over the entire set of cells. (<bold>E</bold>) Tuning halfwidth (f<sub>50%</sub>) for individual cells as a function of pitch class (angle) and response type (colors as in <bold>D</bold>). The radius indicates the halfwidth of the tuning in octaves (see Methods). More precisely tuned cells lie in the center of the circle. This indicates that while many cells were tuned narrowly enough to only be excited by a single constituent tone, many others were excited and/or inhibited by multiple constituent tones of the Shepard tone.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig2-figsupp1-v1.tif"/></fig></fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Repulsive shifts are not consistent with minimal distance hypothesis.</title><p>(<bold>A</bold>) In the case of an unbiased Shepard pair, steps of less than 6 pitch classes lead to unambiguous percepts, e.g. a 0st to 3st steps leads to an ascending percept (red) and a 0st to 9st (i.e. 0st = &gt;-3st) step to a descending percept (blue). Semi-octave (0st to 6st, blue/red) steps lead to an ambiguous percept, which strongly depends on the stimulus history (<xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>; <xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>) and even the individual’s specifics, such as language experience and vocal range (<xref ref-type="bibr" rid="bib18">Deutsch, 1986</xref>). This suggests the <italic>minimal distance hypothesis</italic> which predicts the percept to follow the smaller of the two distances along the circle between the two pitch classes (<bold>B</bold>) In the case of an ambiguous Shepard pair (0st to 6st), preceded by a Bias sequence (red bar, right), here an <italic>UP</italic>-Bias, the ascending percept together with the <italic>minimal distance hypothesis</italic> would predict the distance between the Shepard tones to be reduced on the side of the <italic>UP</italic>-Bias (red dots). However, the population decoding shows that the distance between the tones is indeed increased on the side of the <italic>UP</italic>-Bias, challenging the <italic>minimal distance hypothesis</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig3-v1.tif"/></fig><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Population decoding predicts a Bias-induced, repulsive shift in pitch class.</title><p>(<bold>A</bold>) We decoded the represented stimulus using dimensionality reduction techniques (see <xref ref-type="fig" rid="fig2">Figure 2</xref> for population-vector decoding). The stimulus identity (top) is reflected in the joint activity of all neurons (middle). If the neurons are considered as dimensions of a high-dimensional response space, the circular stimulus space of Shepard tones induces a circular manifold of responses, which lies in a lower dimensional space (light red plane). Colors represent a Shepard tone’s pitch class, also in the following graphs. (<bold>B</bold>) The entire set of responses to the 240 distinct Shepard tones (from the various Bias sequences) is projected by the decoding into a low dimensional space (dots, hue = true pitch class), in which neighboring stimuli fall close to each other and the stimuli overall form a circle. The thick, colored line is computed from local averages along the range of pitch classes and emphasizes the circular structure. The Shepard tones in the ambiguous pairs are projected using the same decoder (denoted by the different triangles, hue = true pitch class), and roughly fall into their expected locations. However, if a stimulus was relatively above the Bias sequence (△, bright, upward triangles), their representation is shifted to higher pitch classes, compared to the same stimulus when located relatively below the Bias sequence (▼, dark, downward triangles). Hence, the preceding Bias repulsed the stimuli in the ambiguous pair in their represented pitch class. Both stimuli of the pair are treated equally here.(<bold>C</bold>) To demonstrate that decoding in this way is reliable, we compare real and estimated pitch classes (by taking the circular position in B) for each stimulus, which exhibits a reliable relation (<italic>r</italic>=0.995). (<bold>D</bold>) The influence of the Bias can be compared quantitatively by centering the represented test stimuli around their actual pitch class and inspecting the difference between the two different Bias conditions. After the Bias the decoded pitch class is shifted from their actual pitch class away from the biased pitch class range with high significance (p&lt;10<sup>–43</sup>, Wilcoxon-test). (<bold>E</bold>) The size of the shift is influenced by the length of the Bias sequence (5 tones = red, 10 tones = black) and the time between the Bias and the test tones (<italic>τ</italic>=1.1 s). The errorbars indicate 1 SEM based on the number of number of test tones located above (N=24, upward triangles) and below (N=24, downward triangles).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Populationvector-based decoding also predicts a repulsive shift in pitch class.</title><p>(<bold>A</bold>) Individual stimuli (top) can also be decoded, by assigning each neuron its preferred pitch class (a complex number, gray vectors, bottom) and weighting these by the neural response (small purple dots, bottom) for the given stimulus. The angle of the average of these vectors (black vectors, bottom) then corresponds to the decoded stimulus (big purple dot, bottom). (<bold>B</bold>) As in <xref ref-type="fig" rid="fig3">Figure 3</xref>, the entire set of 240 distinct Shepard tones (from the various Bias sequences) is represented in a lower dimensional space (dots, hue = true pitch class), in which neighboring stimuli fall close to each other and the stimuli overall form a circle. The Shepard tones in the ambiguous pairs are projected using the same decoder (denoted by the different triangles, hue = true pitch class), and roughly fall into their expected locations. As before, if a stimulus was relatively above the Bias sequence (△, bright, upward triangles), their representation is shifted to higher pitch classes, compared to the same stimulus when located relatively below the Bias sequence (▼, dark, downward triangles). Hence, the preceding Bias repulsed the stimuli in the ambiguous pair in their represented pitch class. Both stimuli of the pair are treated equally here. (<bold>C</bold>) As before for the dimensionality reduction decoding we compare real and estimated pitch classes (by taking the circular position in B) for each stimulus, which explains &gt;99% of the variance. (<bold>D</bold>) The influence of the Bias can be compared quantitatively, by centering the represented test stimuli around their actual pitch class and inspecting the difference between the two different Bias conditions. The Bias shifts them away with high significance (p&lt;10^–43, Wilcoxon-test). (<bold>E</bold>) As before, the size of the shift is influenced by the length of the Bias sequence (5 tones = red, 10 tones = black) and the time between the Bias and the test tones (<italic>τ</italic>=0.92 s). The errorbars indicate 1 SEM based on the number of number of test tones located above (N=24, upward triangles) and below (N=24, downward triangles).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig4-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Distributed activity and local adaptation predict tuning changes and repulsive shifts.</title><p>(<bold>A</bold>) We model the encoding by a simplified model, which starts from the cochlea, includes only one intermediate station (e.g. the MGB), and then projects to cortical neurons. The model is general in the sense that a cascaded version would lead to the same response, as long as similar mechanisms act on each level. A stimulus elicits an activity distribution along the cochlea (bottom) which is retained in shape on the intermediate level (2nd from below). In the native state, the stimulus is transferred to the cortical level without adaptation (2nd from top, black) and integrated by the cortical neuron (top, black). After a stimulus presentation, an adapted trough is left behind in the connections leading up to the cortical level (2nd from top, red), which reduces the cortical tuning curve locally. Since tuning curves closer to the center adapt more strongly, the stimulus representation in the neural population shifts away from the region of adaptation. (<bold>B</bold>) Applying the same analysis as above (<xref ref-type="fig" rid="fig3">Figure 3</xref>) for the real data leads again to a circular decoding (<bold>B1</bold>), with the estimated pitch classes of the tones the Shepard pair shifted repulsively by the preceding Bias (B2, for more details see the description of <xref ref-type="fig" rid="fig4">Figure 4</xref>). (<bold>C</bold>) Single cells show adaptation of their responses colocalized (different colors) with the biased region (colored bars, bottom). The Bias was presented in four different regions in separate trials, and the tuning of the cell probed in between the biasing stimuli. The left side shows a model example and the right side a representative, neural example. (<bold>D</bold>) Centered on the Bias, neurons in the auditory cortex adapt their response colocal with the Bias. The curves represent the difference in response rate between the unadapted tuning and the adapted tuning, again for model cells on the left and actual data on the right. (<bold>E</bold>) The presence of the Bias reduces the firing rate relative to the initial discharge rate, by ~40% (red), while the rate stays the same or is slightly elevated outside of the Bias regions (green) (see <xref ref-type="fig" rid="fig3">Figure 3</xref> for the decoding results and two related, incompatible models, which demonstrate noteworthy subtleties of the decoding process). The errorbars indicate 1 SEM across the set of neurons (see D for Ns).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Local and global (postsynaptic) adaptation cannot explain both influences of the Bias on the tuning (<bold>A1–C1</bold>) and the represented stimuli (<bold>A2–C2</bold>).</title><p>(<bold>A</bold>) The observed tuning curves could also be obtained if the adaptation was assumed to be strictly local, and no distributed activity in response to a stimulus was considered. In this case, however, no change in stimulus representation is expected (<bold>A2</bold>), below (red) and above (blue) curves are identical, since the local adaptation would reduce the response of each neuron (either multiplicatively or subtractively) and, hence, not changing the center of the represented stimulus. (<bold>B</bold>) The repulsive shifts in the represented stimulus could also be obtained, if adaptation was assumed to be global across all inputs, that is only on the postsynaptic side, determined by the total postsynaptic activity. In this case, however, tuning curves are predicted to scale, rather than change in shape or position (<bold>B1</bold>). This is inconsistent with the observed changes in tuning curve shape (<xref ref-type="fig" rid="fig5">Figure 5C–E</xref>). (<bold>C</bold>) If local adaptation (either on the synaptic level or in the cells projecting on the present cell) is combined with a distributed activity in the ascending auditory system, sharply adapted tuning curves are obtained as well as a repulsive influence of a preceding sequence of Bias stimuli. In response to a stimulus, both local changes in tuning curves (<bold>C1</bold>) and Bias-repulsed stimulus representation are obtained (<bold>C2</bold>). It should be noted that other models exist which would produce similar results, yet, the present model makes only a limited set of well accepted assumptions and is consistent with the data. (<bold>A3–C3</bold>) The decoding curves are obtained from the encoding-decoding matrix, which relates the stimulus (abscissa) to the response for each neuron (ordinate). Decoding consists of reading off the population response at the test stimulus (blue vertical line). Here, the Bias below condition is illustrated (red curves in the other plots).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig5-figsupp1-v1.tif"/></fig></fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Decoding based on the directionality of the individual cells predicts the directional percept.</title><p>(<bold>A</bold>) The predicted directional percept <italic>D(S</italic>) for a stimulus <italic>S</italic> is computed by the average over the cells activity <italic>f<sub>i</sub>(S</italic>), weighted by their directionality <italic>D<sub>i</sub></italic> and their distance to the stimulus <italic>w<sub>i</sub></italic>. The inset images show prototypical <italic>SSTRF</italic>s of cells with the ascending (top, <italic>up</italic>), descending (bottom, <italic>down</italic>) and undirected (middle) directional preference. (<bold>B</bold>) Examples of three directional cells based on the Shepard-tone-based spectrotemporal receptive fields (<inline-formula><mml:math id="inf1"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula><bold>s</bold>). Directionality was determined by the asymmetry of the 2nd column of the <inline-formula><mml:math id="inf2"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> (response to previous stimulus), centered at the maximum (BF) of the first column (response to current stimulus, see Methods for details). As usual, time on the abscissa runs into the past. The middle cell for example is a down-cell, since it responds more strongly to a stimulus sequence 10st = &gt;7 st, than 4st = &gt;7 st based on the <inline-formula><mml:math id="inf3"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula>. (<bold>C</bold>) The prediction of the decoding (ordinate) compared to the usually perceived direction for the two sequences (abscissa). Predictions depended on the length of the sequence (o=5 tones, <bold>•</bold>=10 tones) and the predicted tone (red = 1 st tone, blue = 2 nd tone). The dashed red line corresponds to a flat prediction. (<bold>D</bold>) Predictive performance increased as a function of Bias length and distance to the Bias, reflected as 1st (red) or 2nd (blue) tone after the Bias. Both dependencies are consistent with human performance and the build-up and recovery of adaptive processes. (<bold>E</bold>) The basis for the directional decoding can be analyzed by considering the entire set of Bias-induced differences in response, arranged by the directional preference of each cell (abscissa), and the location in BF relative to each stimulus in the Shepard pair (ordinate). Applying the analysis to the neural data, the obtained pattern of activity (top) is composed of two angled stripes of positive (red) and negative (blue) differential activity. For cells with BFs close to the pitch class of the test tones, the relative activities are significantly different (p=0.03, one-way ANOVA) between ascending and descending preferring cells, thus predicting the percept of these tones. Gray boxes indicate combinations of directionality and relative location which did not exist in the cell population. The errorbars indicate 1 SEM across the cells in each directionality bin. (<bold>F</bold>) Applied to a population of model neurons (as in <xref ref-type="fig" rid="fig5">Figure 5</xref>, see Methods for details) subjected to the same stimulus as the real neurons, in the absence of adaptation (left) no significant pattern emerges. If no directional cells are present (middle), adaptation leads to a distinct pattern for different relative spectral locations, but the lack of directional cells prevents a directional judgment. Finally, with adaptation and directional cells a pattern of differential activation is obtained, similar to the pattern in the neural data. T Cells located close to the target tone (near 0 on the ordinate) show a differential activity, predictive of the percept, which was used in the direct decoding above (shown separately in the lower plots). While these activities exhibit no significant dependence in the absence of adaptation or directional cells, the dependence becomes significantly characteristic with adaptation (p&lt;0.001, one-way ANOVA, bottom right). (<bold>G</bold>) The above results can be summarized as a symmetric imbalance in the activities of directional cells after the Bias around it (right), which when decoded predict steps consistent with the percept, that is both are judged in their relative position to the Bias. Hence the percept of the pitch change direction is determined by the local activity, rather than by the circular distance between Shepard tones (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94296-fig6-v1.tif"/></fig><p>For simplicity, the term <italic>pitch</italic> is used interchangeably with <italic>pitch class as</italic> only Shepard tones are considered in this study. Pitch (class) is here mainly used as a term to describe the neural responses to Shepard tones, as in previous literature on the topic, and the fact that Shepard tones are composite stimuli that lead to a pitch percept.</p><sec id="s2-1"><title>The contextual bias adapts the neural population locally</title><p>Adaptation to a stimulus is a ubiquitous phenomenon in neural systems (<xref ref-type="bibr" rid="bib25">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib67">Ulanovsky et al., 2004</xref>; <xref ref-type="bibr" rid="bib11">Clifford et al., 2007</xref>). Multiple kinds and roles of adaptation have been proposed, ranging from fatigue to adaptation in statistics (<xref ref-type="bibr" rid="bib14">Dean et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Dean et al., 2008</xref>; <xref ref-type="bibr" rid="bib70">Wen et al., 2012</xref>; <xref ref-type="bibr" rid="bib3">Benucci et al., 2013</xref>) to higher-order adaptation (<xref ref-type="bibr" rid="bib61">Shechter and Depireux, 2006</xref>; <xref ref-type="bibr" rid="bib52">Rabinowitz et al., 2011</xref>). Since adaptation has previously been implicated in affecting perception, for example in the tilt-after effect in vision (<xref ref-type="bibr" rid="bib59">Seriès et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Kohn and Movshon, 2004</xref>), we start out by characterizing the adaptation in neural response during and following the Bias under awake (ferrets, single units) and behaving (humans, MEG) conditions. The Bias was matched to the choice from a previous human study (<xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>, see <xref ref-type="fig" rid="fig1">Figure 1B</xref>), that is it consisted of a sequence of 5 or 10 Shepard tones with pitch classes randomly drawn from a range of 5 semitones, symmetrically arranged around a central pitch class, for example a Bias sequence centered at 3 semitones had individual tones drawn from 0.5 to 5.5 semitones. Relative to the ambiguous pairs, there were both an <italic>up</italic>- and a <italic>down</italic>-Bias, positioned above or below the first tone in the ambiguous pair, respectively.</p><p>In the single unit data, the average response strength decreases as a function of the position in the Bias sequence. Cells adapted their onset, sustained and offset response within a few tones in the biasing sequence (<xref ref-type="fig" rid="fig2">Figure 2 A1</xref>). This behavior was observable for the vast majority of cells (91%, p&lt;0.05, Kruskal-Wallis test), and is thus conserved in the grand average (<xref ref-type="fig" rid="fig2">Figure 2 A2</xref>). The adaptation plateaued after about 3–4 stimuli (corresponding to a time-constant of 0.59 s) on a level about 13% below the initial level.</p><p>The single-unit response strength is reduced locally by the Bias sequence, that is more strongly for the range of Shepard tones occuring in the Bias. The responses to the tones in the ambiguous pair (<xref ref-type="fig" rid="fig2">Figure 2 A3</xref>, blue) – which are at the edges of the Bias sequence – are significantly less reduced (33%, p=0.0011, 2 group t-test) compared to within the bias (<xref ref-type="fig" rid="fig2">Figure 2 A3</xref>, blue vs. red), relative to the first responses of the bias. The average response was here compared against the unadapted response of the neurons measured via their Shepard tone tuning curve, collected prior to the Bias experiment (see Fig. S1 for some examples). This difference is enhanced if longer sequences are used and the entire non-biased region is measured (see below) The response strength remains adapted on the order of one second for single cells. The average spontaneous activity recovered with a time constant of 1.2 s (<xref ref-type="fig" rid="fig2">Figure 2 A4</xref>). The initial buildup before the reduction is probably due to offset responses of some cells.</p><p>For the human recordings, we obtained quite similar time-courses and qualitative response progressions. The neural response adapted both for individuals (<xref ref-type="fig" rid="fig2">Figure 2 B1</xref>) and on average (<xref ref-type="fig" rid="fig2">Figure 2 B2</xref>), with slightly faster time-constants (0.69 s), which could stem from the lower repetition rate (4 Hz compared to 7 Hz) used in the human experiments, potentially leading to less adaptation. To the contrary though, the amount of adaptation under behaving conditions in humans appears to be more substantial (40%) than for the average single units under awake conditions. While this difference could be partly explained by desynchronization which is typically associated with active behavior or attention (<xref ref-type="bibr" rid="bib1">Alishbayli et al., 2019</xref>), general response adaptation to repeated stimuli is also typical in behaving humans (<xref ref-type="bibr" rid="bib47">Netser et al., 2011</xref>). However, comparisons between the level of adaptation in MEG and single neuron firing rates may be misleading, due to the differences in the signal measured and subsequent processing.</p><p>Similarly to the neuronal data, in the MEG data the responses to probe tones in the same semi-octave (<xref ref-type="fig" rid="fig2">Figure 2B3</xref>, red) are significantly reduced (21%, signed ranks test, p&lt;0.0001) compared to the corresponding response in the opposite semi-octave (blue). This local reduction is not surprising, given that single neurons in the auditory cortex can be well tuned to Shepard tones, with tuning widths of as little as 2–3 semitones (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E</xref>). The detailed effect adaptation has on individual cells is studied in detail further below. Note, that the term local here could mean that a neuron is adapted in multiple octaves, but this is collapsed into the Shepard tone space.</p><p>In summary, both under awake (ferret) and behaving (humans) conditions, we find that neural responses adapt with similar time courses. The adaptation is local in nature, despite the global, that is wide-band, nature of the Shepard tones. Together, this suggests that adaptation may play an important role in explaining the effect the Bias sequence has on perceiving the ambiguous Shepard pair. Below we investigate the specific influence of the Bias on the detailed neural representation using the neuronal recordings from the ferret auditory cortex, which cannot be achieved currently with the human MEG data.</p></sec><sec id="s2-2"><title>The contextual bias repels the ambiguous pair in pitch</title><p>Adaptation can have a variety of effects on the represented stimulus attributes (see <xref ref-type="bibr" rid="bib59">Seriès et al., 2009</xref>): stimulus properties can be attracted, repelled or left unchanged depending on the kind of adaptation. In the present paradigm, one hypothesis to explain the percept would be that <italic>the bias attracts subsequent tones, thus reducing the distance along this side of the pitch-circle</italic>, for example an <italic>UP</italic>-Bias would reduce an ambiguous 6 semitone step to a non-ambiguous 5 semitone step (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To test this hypothesis, we decoded the represented stimulus using various decoding techniques from the neural population.</p><p>In population decoding the goal is to estimate a mapping from neural activity to stimulus properties, which assigns a stimulus to the population response. In the present context, this amounts to predicting the pitch class for a given neural response. Several decoding techniques exist which apply different algorithms and start from different assumptions. We here present a dimensionality reduction technique, based on principal component analysis, however, other techniques gave very similar results (e.g. Stochastic Neighborhood Embedding tSNE, <xref ref-type="bibr" rid="bib44">Maaten and Hinton, 2008</xref>), or for a population vector decoding, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><p>Decoding techniques based on dimensionality reduction attempt to discover a new coordinate system, which accounts for a substantial portion of the variance within much fewer dimensions (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In other words, they estimate a new representation adapted to the intrinsic geometry of the set of neural responses. In the case of Shepard tones, we predict this geometry to be circular (assuming the neural representation is not degenerate), given the circular nature of the Shepard tones (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). As a circular variable can be represented (embedded) in a 2D Euclidean space, we only consider two dimensions of the decoding, typically the first two, if sorted by explained variance.</p><p>The projection of the neural data onto the first two principal components, forms indeed a circular arrangement (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Each point corresponds to a Shepard tone, with its actual pitch given by its color. The dimensionality reduction was based on the neural responses of 662 neurons to 240 distinct Shepard tones, compiled from the 32 biasing sequences (16 for both sequence lengths, 5 and 10), which covered the octave evenly. The orderly progression of colors indicates that a proximity in stimulus space leads to a proximity in neural response space.</p><p>To reassign a pitch class to each point, we estimated a continuous pitch-circle (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, colorful polygon), by computing the local average of each set of 10 adjacent points (w.r.t. actual pitch) and interpolating in between these points. This decoder produces an excellent mapping between actual and estimated pitch classes on the training set (<italic>r</italic>=0.995, Pearson correlation, <xref ref-type="fig" rid="fig4">Figure 4C</xref>). Based on the responses to the Bias sequences, we have thus constructed a decoder of high accuracy.</p><p>Next, we apply the decoder to the responses of the Shepard tones in the ambiguous pair to estimate their represented pitch class, and check whether they are represented at their expected pitch class. We find that their pitch classes are shifted <italic>away</italic> from the Bias, that is tones in the pair that occur above the Bias are shifted further above (<xref ref-type="fig" rid="fig4">Figure 4B/D/E</xref>, △, bright, upward triangles), and vice versa (<xref ref-type="fig" rid="fig4">Figure 4B/D/E</xref>, ▼, dark, downward triangles). This result is highly significant (p=10<sup>–41</sup>, exact ranks test, MATLAB <italic>signrank</italic>) and holds for all tested pitch classes in the pair ([0,3,6,9], <xref ref-type="fig" rid="fig4">Figure 4D</xref>). The size of the shift increases with the length of the Bias sequence (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, red: L=5, black: L=10) and decreases with the temporal separation between Bias and ambiguous pair (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, <italic>τ</italic>=1.1 s). This time constant agrees well with the time course of recovery from adaptation (<xref ref-type="fig" rid="fig2">Figure 2A4</xref>). The effect size – ranging up to a total of 0.8 semitones – much greater than the human threshold of ~0.2 semitones for distinguishing two Shepard tones (internal pilot experiment, data not shown). Practically the same result is obtained using population vector decoding instead (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Consequently, the presence of the Bias has a <italic>repulsive</italic> effect on the tones in the pair. Therefore, their distance increases (when measured on the Shepard pitch circle) on the side of the pitch circle where the Bias was presented, for example for a 6 semitone step an <italic>UP</italic>-Bias leads to a represented 7 semitone (or correspondingly –5 semitone) step (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Hence, the population decoding suggests that a decoder based on circular distance in pitch <italic>cannot</italic> account for the effect of the Bias on the percept, since this would have predicted the distance to shrink on the side of the Bias.</p></sec><sec id="s2-3"><title>An SSA-like model accounts for repulsion and local adaptation</title><p>Before we can propose an alternative decoder for the directionality percept, we devise a basic neural model which is consistent with both the local nature of the adaptation (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and the repulsion in pitch (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This will serve to highlight a few boundary conditions coming from the neural activity and tuning properties that a complete model of the perceptual effect will have to obey. Generally, the model is a two-layer, tonotopically organized model with spectral integration for both layers and adaptation occurring in the transition from the first to the second layer (see <xref ref-type="fig" rid="fig5">Figure 5A</xref> for a depiction and Methods for more details).</p><p>As detailed below, the Bias not only leads to 'local' adaptation in the sense of 'cells close to the location of the Bias', but the adaptation even acts locally within the tuning curve of a given cell (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, left). Hence, while global, postsynaptic adaptation (fatigue) has been shown to be sufficient in producing a repulsion from the adaptor in decoded stimuli (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="bibr" rid="bib59">Seriès et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Jin et al., 2005</xref>), the local reduction of individual tuning curves actually observed in our data requires us to consider non-global adaptation in the model. Adaptation of this type is not unheard of in the auditory cortex, given that another cortical property of stimulus representation – stimulus specific adaptation (SSA, <xref ref-type="bibr" rid="bib66">Ulanovsky et al., 2003</xref>; <xref ref-type="bibr" rid="bib49">Parras et al., 2017</xref>) – is likely to rest on similar mechanisms.</p><p>The simplest possibility along these lines is very local adaptation, that is specific and limited to each stimulus. While this adaptation could account for the local changes in tuning-curves, it would not predict a repulsion to occur in decoding (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>, see Methods for details on the model implementation). Since this adaptation is assumed to be specific for each stimulus, the population activity for this decoded stimulus would simply be scaled, which would leave the mean and all other moments the same (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>).</p><p>A more biological variant is adaptation that is based on the <italic>internal</italic>, <italic>neural representation of the stimulus</italic> (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As the auditory system has non-zero filter-bandwidths, every stimulus elicits a distributed, rather than a perfectly localized activity. At least up to the primary auditory cortex, acoustic stimuli are therefore represented in a distributed manner, in particular in the medial geniculate body (MGB) of the thalamus. If this distribution of activity adapts the corresponding channels locally, the tuning curves of cells in field AI of the primary auditory cortex – which receive forward input from the MGB – will be locally reduced, however, less local than in the point-like representation due to the width of the distribution in its inputs (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C1</xref>). On the other hand, decoding of pitches will be repulsive after an adaptor, because cells closer in BF to the adapting stimulus will integrate more adaptation (<xref ref-type="fig" rid="fig5">Figure 5A</xref> top right), and thus contribute less of their stimulus preference to the decoding. This imbalance shifts the average in the decoded pitch further away (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C3</xref>). For simplicity, the adaptation here is attributed to the incoming synaptic connections to AI, yet, it could equally be localized at an earlier or multiple levels.</p><p>The models discussed above (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) were implemented non-dynamically to illustrate the interaction between context and different types of adaptation. Based on the aforementioned considerations, we implemented a dynamical rate model including local adaptation and distributed representation (the latter two as in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>), which receives the identical stimulus sequences as were presented to the real neurons. The dynamical model provides a quantitative match to the adaptation of single cells and the repulsive representation of Shepard tones after the Bias and further allows us to estimate parameters of the underlying processing (<xref ref-type="fig" rid="fig5">Figure 5B–E</xref>).</p><p>First, when subjected to population decoding analysis as for the real data before, the model exhibits a very similar circular representation of the space of Shepard tones (<xref ref-type="fig" rid="fig5">Figure 5B1</xref>) and repulsive shifts in represented pitch class (<xref ref-type="fig" rid="fig5">Figure 5B2</xref>). The basis for this representation is analyzed in the following.</p><p>Local adaptation in the tuning of individual cells is retained in the model (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Individual cells showed adaptation patterns matched in location to the region where the Bias was presented (different colors represent different Bias regions), in comparison to the unbiased tuning curve (gray). Similarly, in the model, the locally implemented adaptation together with the distributed activity in the middle level leads to similarly adapted individual tuning curves.</p><p>To study the adaptation in a more standardized way, we computed the pointwise difference between adapted and unadapted tuning curve. These differences were then cocentered with the Bias before averaging (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Both for the onset (brown) and the sustained response (red), the reduction is highly local, with steep flanks at the boundaries of the Bias for both the model and the actual data. The offset data appears to show some local reduction but not as sharply defined as the other two.</p><p>In order to find the relative reduction, the response rates inside (red lines) and outside (green lines) the biased regions are plotted against the corresponding regions in the unbiased case (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). While small firing rates appear to be less influenced, a reduction of ~40% stabilized for higher firing rates, largely independent of the response bin. This analysis can illustrate dependencies on firing rate and prevents degenerate divisions by small firing rates.</p><p>The adaptation encapsulated in the model makes only few assumptions, yet provides a qualitatively matched description of the neural behavior in response to the stimulus sequences (w.r.t. the present level of analysis). The model can now be extended to directional cells, to provide a novel explanation for the directionality percept of the biased Shepard pairs.</p></sec><sec id="s2-4"><title>The contextual Bias differentially adapts directional cells predicting the percept</title><p>The decoding techniques used in the previous sections relied only on the cells’ tunings to the <italic>currently</italic> presented Shepard tone. However, cells in the auditory cortex also possess preferences for the <italic>succession of two</italic> (or more) stimuli, for example differing in frequency (<xref ref-type="bibr" rid="bib7">Brosch and Schreiner, 2000</xref>; <xref ref-type="bibr" rid="bib6">Brosch et al., 1999</xref>; <xref ref-type="bibr" rid="bib5">Brosch and Schreiner, 1997</xref>). We hypothesized that perception of pitch steps could rely on the relative activities of cells preferring ascending and descending steps in frequency, or presently pitch class (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), instead of the difference in decoded pitch class (as in the <italic>minimal distance hypothesis,</italic> disproven above).</p><p>We tested this <italic>directional hypothesis</italic> by decoding the perceived direction more directly by taking the directional preferences of each cell into account (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The directional percept is predicted by the population response weighted by each cell’s directionality index (DI) and the distance to the currently presented stimulus (see Methods for details). The DI is computed from the cells' <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which are estimated from the sequences of Shepard tones. This approach avoids estimating receptive fields from stimuli with different statistics. Directional cells have asymmetric spectro-temporal receptive fields (<inline-formula><mml:math id="inf5"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig6">Figure 6B</xref> for some examples): down-selective cells have <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with active zones (red) angled down (from past to future, current time being on the left), up-selective cells the opposite. In both cases, the <inline-formula><mml:math id="inf7"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> is dominated by the activity in response to the current stimulus, that is the recent past.</p><p>The directional decoding successfully predicts the percept for both stimuli in the ambiguous pair. Predictions are performed for each stimulus in the test pair separately. The step direction of the first stimulus is defined based on its relative position to the center of the Bias. For the analysis of the second Shepard tone in the pair, the step is assumed to be perceived on the side of the Bias, that is as previously shown in human perception (<xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>). Predictions (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) for the first tone (red) were generally more reliable than for the second tone (blue), and predictions also improved for both tones with the length of the Bias sequence (5 tones = o, 10 tones = •). This dependence of prediction reliability is consistent with the certainty of judgment in human psychophysics (<xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>). On average, the prediction was correct in 88% and 95% for the first tone, for a Bias length of 5 and 10 tones, respectively, and 67% and 88% for the second tone, respectively (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). In summary, the Bias seems to influence the relative activities of up- and down-preferring cells differentially above and below the Bias, such that responses from down-preferring cells prevail below the Bias, and up-preferring cells prevail above the Bias, predicting the human percept correctly.</p><p>We next investigate in what way the activities of directional cells are modified by the Bias to generate these perception-matched decodings. In the previous sections, we have seen that rather local adaptation occurs during the Bias and modifies the response properties of a cell. How does this adaptation affect a cell that has a directional-preference? To address this question, we distinguish the differential response between the <italic>Up</italic> and <italic>Down</italic> Bias as a function of a cell’s directionality and pitch class separation from the test tone, that is Shepard tones in the ambiguous pair. Hence, the analysis (<xref ref-type="fig" rid="fig6">Figure 6E/F</xref>) plots the difference in response between a preceding <italic>Up</italic>- and <italic>Down</italic>-Bias (specifically: Bias-is-locally-above-tone minus Bias-is-locally-below-tone, color scale), as a function of the cells directional selectivity (abscissa) and the cell’s best pitch class location with respect to each tone (ordinate). For the present analysis, the responses to the first and second tone in the pair were analyzed together. For the second tone, a cell thus contributes also to a second relative-to-tone bin (ordinate) at the same directionality, however, with a different set of responses. Also, each cell contributed for each tone in multiple locations, since multiple target tones (4) were tested in the paradigm.</p><p>For the neural data, the differential responses exhibit an angled stripe pattern, formed by a positive and a negative stripe (<xref ref-type="fig" rid="fig6">Figure 6E</xref> top). The stripes are connected at the top and bottom ends, due to the circularity of the Shepard space. The pattern of differential responses conforms to the <italic>directional hypothesis</italic>, if <italic>down</italic>-cells (left half) are more active than <italic>up</italic>-cells (right half) close to the pair tones (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, ordinate around 0). This central region was considered here, since these are the cells that will respond most strongly to the tone. For the neural data, this differential activity is significantly dependent on the directionality of the cells (<xref ref-type="fig" rid="fig6">Figure 6E</xref> bottom, ANOVA, p&lt;0.005).</p></sec><sec id="s2-5"><title>Extending the neuronal model to directionally sensitive cells</title><p>In order to better understand the mechanisms shaping this biasing pattern, the same analysis was applied to neural models including different properties (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). The same model as in the previous section was used, with the only difference that the tuning of the cells extended in time to include two stimuli instead of only one, comparable to the <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of the actual cells (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). To illustrate the effect of adaptation, three models are compared: without adaptation (<xref ref-type="fig" rid="fig6">Figure 6F</xref> left), without directionally tuned cells (<xref ref-type="fig" rid="fig6">Figure 6F</xref> middle), and with adaptation and directionality tuned cells (<xref ref-type="fig" rid="fig6">Figure 6F</xref> right).</p><p>Without adaptation, the cells do not show a differential response (<xref ref-type="fig" rid="fig6">Figure 6F</xref> left), since the Bias does not affect the responses in the test pair (Note, that there is a 200ms pause between the end of the Bias and the test pair, such that the directionality itself cannot explain the pattern of responses). Here, the difference in activity around the current tone is not significant (ANOVA, p=0.5; <xref ref-type="fig" rid="fig6">Figure 6F</xref> left bottom).</p><p>Without directional cells, the pattern reflects only the difference in activity generated by the interaction of the Bias with the adaptation. The lack of directional cells limits the pattern to a small range of directionalities, generated by estimation inaccuracy. Hence, the local pattern of differential response around the test tone is not significantly modulated, due to the lack of directional cells to span the range (ANOVA, p=0.7; <xref ref-type="fig" rid="fig6">Figure 6F</xref> middle bottom).</p><p>In the model with adapting and directional cells, the pattern resembles the angled double stripe pattern from the neural data. The stripes in the pattern are generated by the adaptation, whereas the directionality of the cells leads to the angle of those stripes. Locally around the test tone, this difference shows a statistically significant dependence on the directionality of the cells (ANOVA, p&lt;0.0001; <xref ref-type="fig" rid="fig6">Figure 6F</xref> right bottom).</p><p>The resulting distribution of activities in their relation to the Bias is, hence, symmetric around the Bias (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). Without prior stimulation, the population of cells is unadapted and thus exhibits balanced activity in response to a stimulus. After a sequence of stimuli, the population is partially adapted (<xref ref-type="fig" rid="fig6">Figure 6G</xref> right), such that a subsequent stimulus now elicits an imbalanced activity. Translated concretely to the present paradigm, the Bias will locally adapt cells. The degree of adaptation will be stronger, if their tuning curve overlaps more with the biased region. Adaptation in this region should therefore most strongly influence a cell’s response. For example, if one considers two directional cells, an <italic>up-</italic> and a <italic>down-</italic>selective cell, cocentered in the same spectral location below the Bias, then the Bias will more strongly adapt the <italic>up</italic>-cell, which has its dominant, recent part of the <inline-formula><mml:math id="inf9"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> more inside the region of the Bias (<xref ref-type="fig" rid="fig6">Figure 6G</xref> right). Consistent with the percept, this imbalance predicts the tone to be perceived as a descending step relative to the Bias. Conversely, for the second stimulus in the pair, located above the Bias, the <italic>down</italic>-selective cells will be more adapted, thus predicting an ascending step relative to the previous tone.</p><p><italic>In summary</italic>, taking into account the directional selectivities of the population of cells and local neural adaptation, the changes in the directional percept induced by the Bias sequences can be predicted from the neural data. Specifically, the local adaptation of specifically the directionally selective cells caused by the Bias underlies the imbalance in their responses, and thus is likely to be the underlying mechanism of the biase Shepard tones percept.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have investigated the physiological basis underlying the influence of stimulus history on the perception of pitch-direction, using a bistable acoustic stimulus, pairs of Shepard tones. Stimulus history is found to persist as spectrally localized adaptation in animal and human recordings, which specifically shapes the activity of direction-selective cells in agreement with the percept. The adaptation’s spectral and temporal properties suggest a common origin with previously described mechanisms, such as stimulus specific adaptation (SSA). Conversely, the typically assumed (<xref ref-type="bibr" rid="bib62">Shepard, 1964</xref>; <xref ref-type="bibr" rid="bib55">Repp, 1997</xref>), but rarely explicitly discussed, circle-distance hypothesis in Shepard tone judgements is in conflict with the repulsive effect on cortically represented pitch revealed presently using different types of population decoding. While our entire study was based on Shepard tones, we hypothesize that the underlying mechanisms will influence many other stimuli as well, although the perceptual salience will depend on the level of ambiguity.</p><sec id="s3-1"><title>Relation to previous studies of Shepard tone percepts and their underlying physiology</title><p>While context-dependent auditory perception of Shepard tones has been studied previously in humans, we here provide a first account of the underlying neurophysiological representation. Previous studies have considered how the stimulus context influences various judgements, for example whether subsequent tones influence each other in frequency (<xref ref-type="bibr" rid="bib58">Ronken, 1972</xref>; <xref ref-type="bibr" rid="bib54">Raviv et al., 2012</xref>), whether a sound is continuous (<xref ref-type="bibr" rid="bib56">Riecke et al., 2011</xref>) or which of two related phonemes is perceived (<xref ref-type="bibr" rid="bib33">Holt, 2005</xref>). In the present study, we chose directional judgements, due to the fundamental role frequency-modulations play in the perception of natural stimuli and language. We find the preceding stimulus to locally bias directional cells, such that on a population level the first Shepard tone is perceived as a step downward, and the second tone as a step upward for an UP Bias, and conversely for a DOWN Bias. While the present study cannot directly rule out that the local adaptation occurs before the thalamo-cortical junction, both physiology (<xref ref-type="bibr" rid="bib69">Wehr and Zador, 2005</xref>) and psychophysical (binaural fusion, <xref ref-type="bibr" rid="bib19">Deutsch, 1992</xref>) results suggest a location beyond the olivary nuclei.</p><p>The success of the directional decoder in linking the cellular activity in A1 to the human percept under the same stimulus conditions is of remarkable accuracy, ~90%. We would like to emphasize that the use of the directional decoder is equally plausible as the use of a preferred frequency based decoder. In both cases, it is assumed that a downstream region in the brain pools the neural responses from A1 assuming either only a frequency preference, or a combination of frequency and direction preference. The elegance of the directional decoder is that it makes a direction connection to well-known directional response characteristics of auditory cortical neurons (see below), while avoiding any mechanisms specific to Shepard tones, such as the computation of the circular distance.</p><p>Are these results compatible with the cellular mechanisms that give rise to direction selectivity? The cellular mechanisms underlying the emergence of directional selectivity in the auditory system have been elucidated in recent years using in vivo intracellular recordings (<xref ref-type="bibr" rid="bib71">Ye et al., 2010</xref>; <xref ref-type="bibr" rid="bib72">Zhang et al., 2003</xref>; <xref ref-type="bibr" rid="bib39">Kuo and Wu, 2012</xref>). Two mechanisms have been identified in the auditory cortex, (i) excitatory inputs with different timing and spectral location and (ii) excitatory and inhibitory inputs with different spectral location. In both mechanisms local adaptation by prior stimulation would tend to equalize direction selectivity, by diminishing (i) one excitatory channel or (ii) either the inhibitory or the excitatory channel. The observed changes in response properties under local stimulation are thus compatible with the network mechanisms underlying direction selectivity. This makes the prediction that the presentation of FM-sweeps of one direction should bias subsequent perception to the opposite direction. The timescales tested in these studies (<xref ref-type="bibr" rid="bib71">Ye et al., 2010</xref>; <xref ref-type="bibr" rid="bib60">Shamma et al., 1993</xref>) are similar to what was presently used, for example <xref ref-type="bibr" rid="bib71">Ye et al., 2010</xref> used FM sweeps that lasted for up to 200ms, which is quite comparable to our SOA of 150ms. Psychophysical evidence in this respect has been observed previously in terms of threshold shifts of directional perception, which are in agreement with a local bias influencing the directional percept of subsequent stimuli (<xref ref-type="bibr" rid="bib28">Gardner and Wilson, 1979</xref>; <xref ref-type="bibr" rid="bib13">Dawe et al., 1998</xref>). More specific adaptation paradigms are required to resolve some of the more detailed effects for example local differences across the octave (<xref ref-type="bibr" rid="bib13">Dawe et al., 1998</xref>).</p><p>Conversely, we could disprove the hypothesis that directional judgements are based on the distance between the tones on the circular Shepard space. Earlier studies on directional judgements of Shepard pairs have – implicitly or explicitly – used the circular nature of the Shepard space to predict the percept (<xref ref-type="bibr" rid="bib55">Repp, 1997</xref>; <xref ref-type="bibr" rid="bib13">Dawe et al., 1998</xref>; <xref ref-type="bibr" rid="bib18">Deutsch, 1986</xref>), starting from the fundamental work of <xref ref-type="bibr" rid="bib62">Shepard, 1964</xref> The original idea was to construct a stimulus with tonal character but ambiguous pitch, and as such it has interesting applications in the study of pitch perception. However, as presently shown, the percept of directionality does not rest on the circular construction. This conclusion is obtained by decoding the represented pitch of the Shepard tones in the context of different biasing sequences. This analysis demonstrated that the biasing sequence exerts a repulsive rather than an attractive effect on the pitch of following stimuli. Repulsive effects of this kind have been widely investigated in the visual literature, in particular the <italic>tilt after-effect</italic> (<xref ref-type="bibr" rid="bib59">Seriès et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Kohn and Movshon, 2004</xref>; <xref ref-type="bibr" rid="bib36">Jin et al., 2005</xref>; <xref ref-type="bibr" rid="bib29">Gibson and Radner, 1937</xref>; <xref ref-type="bibr" rid="bib38">Kohn, 2007</xref>), where exposure to a single oriented grating perceptually repels subsequently presented gratings of similar orientation. Repulsive effects have also been described in the auditory perception (<xref ref-type="bibr" rid="bib33">Holt, 2005</xref>; <xref ref-type="bibr" rid="bib58">Ronken, 1972</xref>; <xref ref-type="bibr" rid="bib56">Riecke et al., 2011</xref>), but not in auditory physiology. In conclusion, we find the percept to be inconsistent with the increase in the circular distance in the Shepard tone space.</p><p>An interesting approach would be to provide a Bayesian interpretation for the effect of the Bias on the cortical representation. Typically an increase in activity is considered as a representation of the prior occurrence probability of stimuli (<xref ref-type="bibr" rid="bib34">Huys et al., 2007</xref>). Given the local reduction in activity described above, this interpretation would, however, not predict the percept. Alternatively, one could propose to interpret the <italic>negative</italic> deviation, that is local adaptation, as the local magnitude of the prior, which could be consistently interpreted with the percept in this paradigm, as has been proposed before (<xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>). Recordings from different areas in the auditory cortex might, however, show different characteristics, including a sign inversion.</p></sec><sec id="s3-2"><title>Relation to other principles of adaptation in audition</title><p>Adaptation has been attributed with several functions in sensory processing, ranging from fatigue (adaptation in excitability of spiking), representation of stimulus statistics (<xref ref-type="bibr" rid="bib25">Fairhall et al., 2001</xref>), compensation for stimulus statistics (<xref ref-type="bibr" rid="bib3">Benucci et al., 2013</xref>), sensitization for novel stimuli (<xref ref-type="bibr" rid="bib66">Ulanovsky et al., 2003</xref>; <xref ref-type="bibr" rid="bib50">Pérez-González et al., 2005</xref>) and sensory memory (<xref ref-type="bibr" rid="bib67">Ulanovsky et al., 2004</xref>; <xref ref-type="bibr" rid="bib35">Jääskeläinen et al., 2007</xref>). Adaptation is also present on multiple time-scales, ranging from milliseconds to minutes (<xref ref-type="bibr" rid="bib25">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib67">Ulanovsky et al., 2004</xref>; <xref ref-type="bibr" rid="bib61">Shechter and Depireux, 2006</xref>; <xref ref-type="bibr" rid="bib30">Gourévitch et al., 2009</xref>). Based on the time-scales of the stimulus and the task-design, the present experiments mainly revealed adaptation in the range of fractions of a second. Adaptation can be global – in the sense that a neuron responds less to all stimuli – or local – in the sense that adaptation is specific to certain, usually the previously presented stimuli, as in SSA (<xref ref-type="bibr" rid="bib66">Ulanovsky et al., 2003</xref>; <xref ref-type="bibr" rid="bib12">Condon and Weinberger, 1991</xref>). Here, adaptation was well confined to the set of stimuli presented before. Hence, the adaptation identified presently is temporally and spectrally well matched to SSA described before. In recent years, the research on SSA has focussed on the aspect of stimulus novelty (<xref ref-type="bibr" rid="bib46">Nelken and Ulanovsky, 2007</xref>; <xref ref-type="bibr" rid="bib68">von der Behrens et al., 2009</xref>; <xref ref-type="bibr" rid="bib2">Bäuerle et al., 2011</xref>), as a potential single-cell correlate of mismatch-negativity (MMN) recorded in human EEG and MEG tasks. While the connection between SSA and MMN appears convincing when it comes to some properties, for example stimulus frequency, it appears to not transfer in a similar way to other, still primary properties, such as stimulus level or duration, which elicit robust MMN (<xref ref-type="bibr" rid="bib26">Farley et al., 2010</xref>). The present results reemphasize another putative role of SSA, namely sensory memory. Naturally, adaptation – if it is local – constitutes a ‘negative afterimage’ of the preceding stimulus history. Recent studies in humans suggest a functional role for this adapted state in representing properties of the task. This was recently demonstrated in an auditory delayed match-to-sample task, where a frequency-specific reduction in activity was maintained between the sample and the match (<xref ref-type="bibr" rid="bib42">Linke et al., 2011</xref>), see also <xref ref-type="bibr" rid="bib57">Rinne et al., 2005</xref>. Localized adaptation as described presently provides a likely substrate for such a sensory memory trace.</p></sec><sec id="s3-3"><title>Future directions</title><p>While in human perception task engagement is not necessary to be influenced by the biasing sequence, a natural continuation of the present work would be to record from behaving animals. This would allow us to investigate potential differences in neural activity depending on the activity state, and how individual neurons contribute to the decision on a trial-by-trial basis (<xref ref-type="bibr" rid="bib32">Haefner et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Haefner et al., 2013</xref>). Furthermore, the current study was limited to the primary auditory cortex of the ferret, but secondary areas as well as parietal and frontal areas could also be involved and should be explored in subsequent research. Switching to mice as an experimental species would allow us to differentiate the roles of different cell types better (<xref ref-type="bibr" rid="bib45">Natan et al., 2017</xref>). On the paradigm level, an extension of the time between the end of the Bias sequence and the test pair would be of particular interest in the active condition, where human research suggests that the Bias can persist for more extended times than suggested by the decay properties of the adaptation in the present data set.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental procedures</title><p>We collected single unit recordings from 7 adult, female ferrets (age: 6–12 months, <italic>Mustela putorius furo</italic>) in the awake condition. The animal experiments were performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (Neurobiology of Auditory Cognition in Ferrets R-APR-23–24) of the University of Maryland, College Park.</p><p>We collected MEG recordings from 16 human subjects (9 female, average age: 26y, range 23–34 y) and psychophysical recordings from 10 human subjects (5 female, average age: 28y, range 25–32 y). All human experiments were performed in accordance with the ethical guidelines of the University of Maryland. The human experiments were approved by the Institutional Review Board of the University of Maryland, College Park under the project number 1467378 (&quot;The Adaptive Auditory Mind&quot;). Listeners read and signed a consent form regarding data use and processing before data collection.</p></sec><sec id="s4-2"><title>Surgical procedures</title><p>A dental cement cap and a headpost were surgically implanted on the animal’s head using sterile procedures, as described previously (<xref ref-type="bibr" rid="bib27">Fritz et al., 2003</xref>). Microelectrode arrays (Microprobes Inc, 32–96 channels, 2.5 MOhm, shaft ø=125 μm, various planar layouts with 0.5 mm interelectrode spacing) were surgically implanted in the primary auditory cortex AI at a depth of ~500 μm, for two animals sequentially on both hemispheres. A custom-designed, chronic drive system (see <ext-link ext-link-type="uri" xlink:href="https://code.google.com/archive/p/edds-array-drive/">https://code.google.com/archive/p/edds-array-drive/</ext-link>) was used in some recordings to change the depth of the electrode array.</p></sec><sec id="s4-3"><title>Physiology : stimulation and recording</title><p>Acoustic stimuli were generated at 80 kHz using custom written software (available in the data repository <xref ref-type="bibr" rid="bib23">Englitz et al., 2024</xref>, see below) in MATLAB (The Mathworks, Natick, USA) and presented via a flat calibrated (within +/-5 dB in the range 0.1–32 kHz using the inverse impulse response) sound system (amplifier: Crown D75A; speaker: Manger, flat within 0.08–35 kHz). Animals were head-restrained in a standard position in a tube inside a soundproof chamber (mac3, Industrial Acoustics Corporation). The speaker was positioned centrally above the animal’s head and calibration was performed for the animal head’s position during recordings.</p><p>Signals were pre-amplified directly on the head (1 x or 2 x, Blackrock/TBSI) and further amplified (1000 x, Plexon Inc) and bandpass-filtered (0.1–8000 Hz, Plexon Inc) before digitization ([–5,5]V, 16 bits, 25 kHz, M-series cards, National Instruments) and storage/display using an open-source DAQ system (<xref ref-type="bibr" rid="bib22">Englitz et al., 2013</xref>). Single units were identified using custom written software for spike sorting (for details see <xref ref-type="bibr" rid="bib21">Englitz et al., 2009</xref>). All subsequent analyses were performed in Matlab.</p></sec><sec id="s4-4"><title>Magnetoencephalography and psychophysics : stimulation, recording, and data analysis</title><p>Acoustic stimuli were generated at 44.1 kHz using custom written software in MATLAB and presented via a flat calibrated (within +/-5 dB in the range 40–3000 Hz) sound system. During MEG experiments, the sound was delivered to the ear via sound tubing (ER-3A, Etymotic), inserted with foam plugs (ER-3–14) into the ear canal, while during psychophysical experiments an over-the-ear headphone (Sony MDR-V700) was used. While the limited calibration range (due to the sound tubing) is not optimal, it still encompasses &gt;6 octaves/constituent tones for every Shepard tone. Sound stimuli were presented at 70 dBSPL. Magnetoencephalographic (MEG) signals were recorded in a magnetically shielded room (Yokogawa Corp.) using a 160 channel, whole-head system (Kanazawa Institute of Technology, Kanazawa, Japan), with the detection coils (ø=15.5 mm) arranged uniformly (~25 mm center-to-center spacing) around the top part of the head. Sensors are configured as first-order axial gradiometers with a baseline of 50 mm, with field sensitivities of &gt;5 fT/Hz in the white noise region. Three of the 160 channels were used as reference channels in noise-filtering methods (<xref ref-type="bibr" rid="bib16">de Cheveigné and Simon, 2007</xref>). The magnetic signals were band-passed between 1 Hz and 200 Hz, notch filtered at 60 Hz, and sampled at 1 kHz. Finally, the power spectrum was computed and the amplitude at the target rate of 4 Hz was extracted (as in <xref ref-type="bibr" rid="bib20">Elhilali et al., 2009</xref>, all magnetic field amplitudes in <xref ref-type="fig" rid="fig2">Figure 2B</xref> represent this measure).</p><p>Subjects had to press one of two buttons (controller held in the right hand, away from the sensors) to indicate an ascending or a descending percept. Subjects listened to 120 stimuli in a block, and completed 3 blocks in a session, lasting ~1 hr.</p></sec><sec id="s4-5"><title>Acoustic stimuli</title><p>All stimuli were composed of sequences of Shepard tones. A Shepard tone is a complex tone built as the sum of octave-spaced pure-tones. To stimulate a wide range of neurons, we used a flat envelope, that is all constituent tones had the same amplitude. Phases of the constituents tones were randomized for each trial, to prevent any single, fixed phase relationship from influencing the pitch percept. Each Shepard tone was gated with 5ms sinusoidal ramps at the beginning and end.</p><p>A Shepard tone can be characterized by its position in an octave, termed pitch class (in units of semitones), w.r.t. a base-tone. In the present study, the Shepard tone based on 440 Hz was assigned pitch class 0. The Shepard tone with pitch class 1 is one semitone higher than pitch class 0 and pitch class 12 is identical to pitch class 0, since all constituent tones are shifted by an octave and range from inaudibly low to inaudibly high frequencies. Hence, the space of Shepard tones is circular (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Across the entire set of experiments the duration of the Shepard tones was 0.1 s (neural recordings) / 0.125 s (MEG recordings) and the amplitude 70 dB SPL (at the ear).</p><p>We used two different stimulus sequences to probe the neural representation of the ambiguous Shepard pairs and their spectral and temporal tuning properties, (i) the Biased Shepard Pair and (ii) the Biased Shepard Tuning:</p><sec id="s4-5-1"><title>Biased shepard pair</title><p>In this paradigm, an ambiguous Shepard pair (6 st separation) preceded by a longer sequence of Shepard tones, the Bias (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The Bias consists of a sequence of Shepard tones (lengths: 5 and 10 stimuli) which are within 6 semitones above or below the first Shepard tone in the pair. These biases are called ‘up’ and ‘down’ Bias respectively, as they bias the perception of the ambiguous pair to be ‘ascending’ or ‘descending’, respectively, in pitch (<xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref>; <xref ref-type="bibr" rid="bib9">Chambers et al., 2017</xref>). A pause of different length ([0.05,0.2,0.5] s) was inserted between the Bias and the pair, to study the temporal aspects of the neural representation. Altogether we presented 32 different Bias sequences 4 base pitch classes ([0,3,6,9] st), 2 randomization (pitch classes and position in sequence), 2 Bias lengths (<xref ref-type="bibr" rid="bib63">Simon and Craft, 1989</xref>; <xref ref-type="bibr" rid="bib33">Holt, 2005</xref> stimuli, ‘up’ and ‘down Bias), which in total contained 240 distinct Shepard tones. Their individual pitch classes in the Bias were drawn randomly and continuously from their respective range. Each stimulus was repeated 10 times. In all subsequent analyses, neural responses are averages over these repetitions, and all analyses are performed on the pooled data from all animals. For the neural data, these 240 different Shepard tones were also used to obtain a ‘Shepard tone tuning' for individual cells (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The stimulus described above was presented to both animals and humans. The human psychophysical data were only used to reproduce the previous findings by <xref ref-type="bibr" rid="bib8">Chambers and Pressnitzer, 2014</xref> with the current parameters. For the MEG recordings, a variation of the biased Shepard pair stimulus was used, which enabled the separate measurement of the activation state in the biased and the unbiased spectral regions. For this purpose a second sequence of Shepard tones (tone duration: 30ms; SOA: 250ms; pitch classes: 3 st above or below the tone of the pair) was inserted between the Bias sequence and the Shepard pair, with the time between the two adapted to include the duration of the sequence (2 s) and a pause after the Bias sequence ([0.5,1,2] s).</p></sec><sec id="s4-5-2"><title>Biased shepard tuning</title><p>For estimating the changes in the tuning curve of individual neurons, much longer sequences (154 Shepard tones) were presented to a subset of the neurons. The duration and stimulus onset asynchrony was matched to the Bias sequence. The Shepard tones in these sequences were chosen to maintain the influence of the Bias over the entire sequence, while intermittently probing the entire octave of semitones to estimate the overall influence of the Bias on the tuning of neurons. For this purpose, 5/6 (~83%) of the tones in the sequence were randomly drawn from one of the four Bias regions ([0–5], [3-8],[6-11],[9-2]st), while the 6th tone was randomly drawn from the entire octave, discretized to 24 steps (reminiscent of the studies of <xref ref-type="bibr" rid="bib14">Dean et al., 2005</xref>). The 6th tone could thus be used to measure each neurons 'Shepard tuning' at a resolution of 0.5 semitones, adapted to different Bias locations. To avoid onset effects, a lead-in sequence of 15 Bias tones preceded the first tuning estimation tone. Individual stimulus parameters (intensity, durations of tone and interstimulus interval) were chosen as above. Five pseudorandom sequences were presented for each of the four Bias regions, repeated 6 or more times, providing at least 30 repetitions for each location in the tuning curve (Results of these conditions are shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>). A randomly varied pause of ~5 s separated the trials.</p></sec></sec><sec id="s4-6"><title>Unit selection</title><p>Overall, we recorded from 1467 neurons across all ferrets, out of which 662 were selected for the decoding analysis based on their driven firing rate (i.e. whether they responded significantly to auditory stimulation) and whether they showed a differential response to different Shepard tones. The thresholds for auditory response and tuning to Shepard tones were not very critical: setting the threshold low led to qualitatively the same result, however, with more noise. Setting the thresholds very high, reduced the set of cells included in the analysis, and eventually made the results less stable, as the cells did not cover the entire range of preferences to Shepard tones.</p></sec><sec id="s4-7"><title>Response type analysis</title><p>Whether a cell was adapting or facilitating within the Bias sequence was assessed by averaging the responses across all Bias sequences for a given cell separately. The resulting PSTH was then split up into Onset, Sustained and Offset bins, each 50ms in time, for each stimulus in the Bias. The sequence of response rates was then fitted with an exponential function, and the direction of the adaptation assessed by comparing the initial rate and the asymptotic rate. If the asymptotic rate exceeded the initial rate, a cell was classified as facilitating, conversely as adapting. The three response bins showed similar proportions of adapting response and were thus averaged to assign a single response type to a given cell, as reported in the Results.</p></sec><sec id="s4-8"><title>Population decoding</title><p>The represented stimuli in the ambiguous pair were estimated from the neural responses by training a decoder on the biasing sequences and then applying the decoder to the neural response of the pair. We used two different decoders to compare their results, one based on dimensionality reduction (PCA, Principal Component Analysis) and one based on a weighted population-vector, which both gave very similar results (see <xref ref-type="fig" rid="fig4">Figure 4</xref> and S2). For both decoders, we first built a matrix of responses which had the (240) different Shepard tones occuring in all Bias sequences running along one dimension and the neurons along the other dimension.</p><p>The PCA decoder performed a linear dimensionality reduction, utilizing the stimuli as examples and the neurons as dimensions of the representation. The data were projected to the first three dimensions, which represented the pitch class as well as the position in the sequence of stimuli (see <xref ref-type="fig" rid="fig4">Figure 4A</xref> for a schematic). As the position in the Bias sequence was not relevant for the subsequent pitch class decoding, we only focussed on the two dimensions that spanned the pitch circle. A wide range of linear and non-linear dimensionality reduction techniques – for example tSNE (<xref ref-type="bibr" rid="bib44">Maaten and Hinton, 2008</xref>) – was tested leading to very similar results.</p><p>The weighted population decoder was computed by assigning each neuron its best pitch class (i.e. pitch class that evoked the highest response) and then evaluating the firing-rate weighted sum of all neurons' best pitch classes (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref> for a schematic). Since the stimulus space is circular, this weighted average was performed in the complex domain, where each neuron was represented by a unit vector in the complex plane, with an angle corresponding to the best pitch class. More precisely, this decoder is simply (omitting indices in the following)<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="normal">_</mml:mi></mml:munder><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>ϵ</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the preferred pitch class of a cell and <inline-formula><mml:math id="inf11"><mml:mi>C</mml:mi></mml:math></inline-formula> is the set of pitch classes, <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> the firing rate of the neuron <italic>i</italic> for stimulus <italic>S</italic>. In the decoding, firing rate is normalized to the maximal firing rate for each cell, and the preferred pitch class for the empirical frequency of occurrence <inline-formula><mml:math id="inf13"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, to compensate for uneven sampling of preferred pitch classes.</p><p>To assign a pitch class to the decoded stimuli of the test pair, we projected them onto the ‘pitch-circle’ formed by the decoded stimuli from the Bias sequences (<xref ref-type="fig" rid="fig4">Figure 4A/B</xref>). More precisely, we first estimated a coarse pitch circle with 24 steps at a resolution of 0.5 st, by averaging over bins of 10 neighboring pitch classes (partitioning the total of 240 Bias tones). Next, a more finely resolved trajectory through the set of Bias-tones at a resolution of 0.05 st was created by linear interpolation. Then, the pitch class of the test tone was set to the pitch class of the closest point on the trajectory.</p><p>For the present purpose the decoder was not cross-validated within the Bias sequence data, because its purpose was to provide a reference for the ambiguous pair stimuli, which were not part of the training set.</p></sec><sec id="s4-9"><title>Neural modeling</title><p>We used rate-based models of neural responses in the auditory cortex to investigate the link between the Bias-induced changes in response characteristic and the population decoding results. These are not trivially related, as different kinds of adaptation can lead to different – repulsive or attractive – effects (<xref ref-type="bibr" rid="bib59">Seriès et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Jin et al., 2005</xref>). Two types of models were investigated for this purpose:</p><p>(i) a <italic>non-dynamic tuning model</italic>, which serves to investigate generally the effect of different types of adaptation on the represented stimuli (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec><sec id="s4-10"><title>Non-dynamic neural models</title><p>In the non-dynamic model each neuron is represented as a <italic>von Mises</italic> distribution<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>12</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>12</mml:mn><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with two parameters, best pitch class <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, both measured in semitones, and <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> normalizing the response to an area of 1. We simulate the response of a population of N=100 cortical neurons with <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> equally spaced within [0,12] st. The models were run at the same sampling rate (20 Hz) as the data analysis for consistency.</p><p>The influence of the Bias is modeled assuming an idealized, continuous range of Biases, rather than individual tones. We consider three different models of adaptation: (a) local adaptation (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>) (b) global adaptation (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>), and (c) local adaptation with spreaded representation (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>):</p><p>a) <italic>Local adaptation</italic> refers to a multiplicative reduction of responses to individual stimuli, based on the local, recent stimulus history. The amount of local adaptation is taken as the prominence of this stimulus in the recent history, that is<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>S<sub>Bias</sub>(φ</italic>) is defined as a function over [0,12] st taking values in [0,1]. <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the maximal fraction of adaptation, set to 0.8 in Fig.S3. The cells adapted/biased response to a single Shepard tone is then given by<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the more general case of a complex stimulus <italic>S</italic>, one would replace <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> with <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mi>*</mml:mi><mml:mi>S</mml:mi></mml:math></inline-formula>, that is the convolution of response and stimulus distribution.</p><p>This form of local adaptation resembles a highly stimulus specific version of adaptation. Hence, the responses are adapted only to previously presented stimuli, but no transfer to other stimuli occurs (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>). This type of local adaptation leads to no adaptation, since neurons uniformly reduce their response to the test stimulus, which keeps the mean of the population response the same.</p><p>b) <italic>Global adaptation</italic> refers to a multiplicative reduction of the <italic>entire</italic> tuning curve, based on the recent response history, irrespective of which stimulus caused it. The amount of global adaptation is computed as the correlation between a cell’s tuning curve and the stimulus history <italic>S<sub>Bias</sub>(PC</italic>), that is<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:mn>12</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where * denotes convolution. By the normalization of both <italic>S<sub>Bias</sub></italic> and <italic>R<sub>i</sub></italic>, <italic>A<sub>i</sub></italic> will also be normalized within [0,A<sub>0</sub>]. The cells biased tuning curve to a single Shepard tone is then given by<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Global adaptation in this sense captures summarized adaptation effects that occur ‘globally’ for the postsynaptic cell (e.g. a change in excitability which changes the slope of the IF-curve; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). Global adaptation shifts subsequent stimuli away from the adapting stimulus, since neurons close to the adaptor adapt more strongly (<xref ref-type="bibr" rid="bib59">Seriès et al., 2009</xref>).</p><p>c) <italic>Local adaptation with input spread</italic> combines local adaptation with a distributed neural representation of pointlike stimuli (like a single tone or single Shepard tone), that is the stimulus is first represented on an intermediate level (e.g. the MGB) and then integrated on the cortical level, with adaptation occurring locally at the synapses connecting MGB-AI (see <xref ref-type="fig" rid="fig5">Figure 5A</xref> for an illustration of the architecture). Concretely, the cortical response is given as<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>J</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the intermediate representation <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is given as<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>S</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>12</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>that is the convolution of the stimulus with the intermediate response properties <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> assumed to also be given by <italic>von Mises</italic> distributions as well. The adaptation <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is equated with the midlevel activity induced by the bias<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This form of local adaptation is ‘less local’ than the purely local adaptation described above. Hence, a presentation of a given stimulus will adapt the neural response not only to this stimulus, but – via the distributed representation – also for neighboring stimuli (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>), which in the decoding leads to repulsive shifts, while reducing tuning curves locally. The resulting shape of the adaptation has been described as a shift in tuning curve combined with a global adaptation (<xref ref-type="bibr" rid="bib36">Jin et al., 2005</xref>). We propose that the adaptation proposed here provides a simpler explanation for this observed shape of tuning curve change.</p><p>Note that the differences in decoding emerge only at the boundaries of the Bias region, depicted by the encoding-decoding matrices in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> A3/B3/C3. If the distribution at the vertical line (at 0) has more weight above 0 on the abscissa, this corresponds to a repulsive shift.</p><p>In summary, purely local adaptation can account for the local changes in Shepard tunings of the real data, but fails to explain the repulsive decoding (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>). Global adaptation is consistent with the repulsive decoding results, but fails to explain the local tuning curve changes (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). The combination of local adaptation and distributed input on the intermediate level (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>) is consistent with both the encoding and decoding findings.</p><p>This model is detailed in the Supplementary Methods and results are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><p>(ii) a <italic>dynamic model</italic>, which serves to use the insights of the non-dynamic model to account in more detail for the neural data. We used the identical stimulus sequences and analyses as for the real data. The structure of the dynamic model corresponded to non-dynamic model (c) (see above), that is a distributed stimulus representation before cortex and local adaptation in the thalamo-cortical synapses and (see <xref ref-type="fig" rid="fig5">Figure 5A</xref> for a schematic representation of the model). A sampling rate of 20 Hz was used for the simulations to speed up computations. Stimuli were represented as spectrograms – that is time-frequency representations – with 'frequency' being encoded as Shepard tones, that is they ranged over one octave and wrapped at the spectral boundaries.</p><p>In the mid-level (e.g. MGB) neural representation of the stimulus, each cell’s response was modeled by a peak-normalized <italic>von Mises</italic> distribution for each time t of the filter, that is <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>12</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>12</mml:mn><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the stimulus, <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the mean of the distribution μ denotes the best pitch class and <inline-formula><mml:math id="inf27"><mml:mi>σ</mml:mi></mml:math></inline-formula> the standard deviation, all in semitones. The maximal rate R<sub>max</sub> was arbitrarily set to 1, after normalizing the height to 1 by division via <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Hence, the responses on the mid-level <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> of each neuron <italic>j</italic> were modeled as a weighted average of the spectrogram at time <inline-formula><mml:math id="inf30"><mml:mi>t</mml:mi></mml:math></inline-formula> with the neuron’s tuning curve<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>12</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>On the top-level, corresponding to auditory cortex, the activity of each neuron was modeled as a spectrotemporal filter on the activity of the mid-level representation with local synaptic depression at the synapses<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>J</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the time-frequency filter for cortical neuron i, weighting the activity of the MGB neurons <italic>j</italic> at times  <inline-formula><mml:math id="inf32"><mml:mi>τ</mml:mi></mml:math></inline-formula>=0...T before the current time. <inline-formula><mml:math id="inf33"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> stands for Shepard Spectro-Temporal Receptive Field, which is equivalent to a classical STRF (<xref ref-type="bibr" rid="bib17">Depireux et al., 2001</xref>), just for Shepard tones. The state of synaptic depression between cortical neuron <italic>i</italic> and thalamic neuron <italic>j</italic> is given by <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The adaptation was determined by the activity locally present at each synapse and thus led to relatively local changes in the postsynaptic tuning curves. The dynamics of <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> are given by<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mspace width="thinmathspace"/><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a constant weighting factor, which scales the amount of adaptation. In both cases the response computed via the <inline-formula><mml:math id="inf37"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> is weighted with the adaptation coefficients <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and each coefficient recovers by a fraction <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in each step (leading to exponential recovery).</p><p>For the final simulations (<xref ref-type="fig" rid="fig6">Figure 6</xref>), the model was extended to contain a subset of directional cells, by extending the dependence of the <inline-formula><mml:math id="inf40"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> by another 150ms (3 timesteps at the SR). A directional preference was implemented by adding a von Mises distribution (see above for definition) at the time range 150–250ms with a peak size of 0.25, roughly matching the observed peak-sizes in the <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of real directional cells. For downward preferring cells the center of the von Mises was placed relatively higher than the best semitone of the cell, and vice versa for upward preferring cells, in each case wrapping at the edges to account for the circularity of the Shepard tone response. The simulated population of 500 cells was split into one third non-directional cells, one third upward selective cells and one third downward selective cells.</p></sec><sec id="s4-11"><title>Tuning curve adaptation analysis</title><p>We estimated the <italic>biased</italic> Shepard tunings from the long stimulus sequences (see Acoustic Stimuli: <italic>Biased Shepard Tuning</italic>) by averaging the test stimuli for each location in the octave (see <xref ref-type="fig" rid="fig4">Figure 4C</xref>, different colors indicate different locations of the Bias sequence). To get an estimate of the unadapted tuning curve, we collected the initial 5 stimuli from each condition and thus constructed a corresponding tuning curve at a resolution of 1st. To evaluate the influence of the Bias, the local difference (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) and fraction (<xref ref-type="fig" rid="fig5">Figure 5E</xref>) between the adapted and the unadapted tuning curve were analyzed. The same analysis was applied to model data generated from the identical stimuli using the same model as above (local adaptation, distributed input on the intermediate level, see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>).</p></sec><sec id="s4-12"><title>Directionality analysis</title><p>We investigated the effect of the Bias sequence on directionally selective cells. For this purpose, each cell’s directional selectivity was estimated from the steps contained in the biasing sequences. <inline-formula><mml:math id="inf42"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> were approximated by reverse correlating each neuron’s response with the Bias sequences of Shepard tones (using normalized linear regression, three examples are shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>). The <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> were discretized at 50ms, and include only the bins during the stimulus, not during the pause.</p><p>First, directional preference was assessed by computing the asymmetry in response strength in the second time bin <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, centered on the maximal response in the first time bin, that is <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Positive values of <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> indicate up-ward selective cells and vice versa. The <inline-formula><mml:math id="inf47"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> was first normalized to the maximal value to obtain comparable values between cells.</p><p>Second, a cell’s spectral location relative to the test stimulus was determined by computing the distance between a cell’s <inline-formula><mml:math id="inf48"><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:math></inline-formula> center-of-mass and the pitch class of the test tone. These first two steps, located a cell on the x- and y-axis of the following analysis (see <xref ref-type="fig" rid="fig6">Figure 6C/D</xref>, top).</p><p>Third, the difference in response for identical test stimuli with different preceding Bias locations (relative to each tone in the pair) was computed (‘above’ - ‘below’).</p><p>Finally, these differences were averaged for all cells with a given directionality and pitch-class relative to the target tone.</p><p>This analysis was also applied for the second test-stimulus, which means that each cell contributes to two locations, separated by the semi-octave distance between the two test-tones; however, the contribution was constituted by different (later) responses of the cell. This analysis was conducted both for the actual neural data, as well as for model data. These modeling results were obtained with the same model as above (local adaptation, distributed input on the intermediate level), although adaptation was set to 0 in one condition to demonstrate its role in generating the asymmetry of responses.</p></sec><sec id="s4-13"><title>Directional decoder</title><p>The decoder above first estimated the neurally represented pitch class of the stimulus and then evaluated the circular distance between the pitch classes for a given ambiguous pair to predict the percept. This approach implicitly assumes that the neural system organizes the neural representations correspondingly and can compute distances in this way. A more general and direct way of assessing whether a given stimulus is relatively higher or lower in pitch than a preceding stimulus may be to integrate the responses of neurons with regards to their directional preference (see previous section). We refer to this as the <italic>directional hypothesis</italic>. Quantitatively, this approach for decoding simply takes estimated direction selectivity <italic>DI</italic> of each cell, and weighs it by its activity, and then sums across all cells. Analogously to decoding based on preferred pitch class, it thus assumes that a downstream decoder in the brain 'knows' about one or multiple characteristic properties of the cell (e.g. spectral and/or direction selectivity) and combines the activity of many cells in a weighted manner to arrive at a single estimate. We assume that this directionality is evaluated at the location of the current stimulus, that is the contribution of each cell is therefore weighted by the distance in preferred pitch class to the pitch class of the currently presented stimulus (see <xref ref-type="fig" rid="fig6">Figure 6A</xref> for the mathematical structure of the decoding).</p></sec><sec id="s4-14"><title>Tuning halfwidth</title><p>A neuron’s tuning halfwidth with respect to Shepard tones was estimated using the range of Shepard tones that the firing rate was above f<sub>50%</sub> = (f<sub>Max</sub> - f<sub>Min</sub>)/2. We used a conservative estimation method by determining f<sub>Min</sub> and then computing the range between the closest crossing of f<sub>50%</sub> above and below f<sub>Min</sub>. In this way, neurons with a small difference between f<sub>Max</sub> and f<sub>Min</sub> were assigned comparatively large tuning halfwidths, corresponding to their less salient tuning.</p></sec><sec id="s4-15"><title>Statistical analysis</title><p>Non-parametric tests were used throughout the study to avoid assumptions regarding distributional shape. Single group medians were assessed with the Wilcoxon signed rank test, two group median comparisons with the Mann-Whitney U-test, multiple groups with the Kruskal-Wallis (one-way) and Friedman test (two-way), with post-hoc testing performed using Bonferroni-correction of p-values.ll tests are implemented in the Matlab Statistics Toolbox (The Mathworks, Natick).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The human experiments were approved by the Institutional Review Board of the University of Maryland, College Park under the project number 1467378 (&quot;The Adaptive Auditory Mind&quot;). Listeners read and signed a consent form regarding data use and processing before data collection.</p></fn><fn fn-type="other"><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (Neurobiology of Auditory Cognition in Ferrets R-APR-23-24) of the University of Maryland, College Park.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-94296-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and code for reproducing the current analyses and figures have been deposited in collection di.dcn.DSC_626840_0009_350 inside the open access data repository of the Radboud University and can be accessed under the DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/c0je-x552">https://doi.org/10.34973/c0je-x552</ext-link> .</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Englitz</surname><given-names>A</given-names></name><name><surname>Elhilali</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Decoding contextual influences on auditory perception from primary auditory cortex</data-title><source>Radboud Data Repository di.dcn.DSC_626840_0009_350</source><pub-id pub-id-type="doi">10.34973/c0je-x552</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors would like to thank Barak Shechter, John Rinzel and Romain Brette for interesting discussions and comments on the manuscript. Funding information: European Research Council (Neume to SS); National Institutes of Health (to MH and SS: U01 AG058532). BE acknowledges funding from an NWO VIDI grant (016.VIDI.189.052) and a NWO ALW Open (ALWOP.146).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alishbayli</surname><given-names>A</given-names></name><name><surname>Tichelaar</surname><given-names>JG</given-names></name><name><surname>Gorska</surname><given-names>U</given-names></name><name><surname>Cohen</surname><given-names>MX</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The asynchronous state’s relation to large-scale potentials in cortex</article-title><source>Journal of Neurophysiology</source><volume>122</volume><fpage>2206</fpage><lpage>2219</lpage><pub-id pub-id-type="doi">10.1152/jn.00013.2019</pub-id><pub-id pub-id-type="pmid">31642401</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bäuerle</surname><given-names>P</given-names></name><name><surname>von der Behrens</surname><given-names>W</given-names></name><name><surname>Kössl</surname><given-names>M</given-names></name><name><surname>Gaese</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Stimulus-specific adaptation in the gerbil primary auditory thalamus is the result of a fast frequency-specific habituation and is regulated by the corticofugal system</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>9708</fpage><lpage>9722</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5814-10.2011</pub-id><pub-id pub-id-type="pmid">21715636</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benucci</surname><given-names>A</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Adaptation maintains population homeostasis in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>724</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1038/nn.3382</pub-id><pub-id pub-id-type="pmid">23603708</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bregman</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Auditory Scene Analysis: The Perceptual Organization of Sound</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.1121/1.408434</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>M</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Time course of forward masking tuning curves in cat primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>923</fpage><lpage>943</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.2.923</pub-id><pub-id pub-id-type="pmid">9065859</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>M</given-names></name><name><surname>Schulz</surname><given-names>A</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Processing of sound sequences in macaque auditory cortex: response enhancement</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>1542</fpage><lpage>1559</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.3.1542</pub-id><pub-id pub-id-type="pmid">10482768</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>M</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Sequence sensitivity of neurons in cat primary auditory cortex</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>1155</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.12.1155</pub-id><pub-id pub-id-type="pmid">11073865</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>C</given-names></name><name><surname>Pressnitzer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Perceptual hysteresis in the judgment of auditory pitch shift</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>76</volume><fpage>1271</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.3758/s13414-014-0676-5</pub-id><pub-id pub-id-type="pmid">24874257</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>C</given-names></name><name><surname>Akram</surname><given-names>S</given-names></name><name><surname>Adam</surname><given-names>V</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Pressnitzer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prior context in audition informs binding and shapes simple features</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15027</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15027</pub-id><pub-id pub-id-type="pmid">28425433</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chebbi</surname><given-names>S</given-names></name><name><surname>Ben Jebara</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the use of pitch-based features for fear emotion detection from speech</article-title><conf-name>2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP</conf-name><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ATSIP.2018.8364512</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clifford</surname><given-names>CWG</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Stanley</surname><given-names>GB</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual adaptation: neural, psychological and computational aspects</article-title><source>Vision Research</source><volume>47</volume><fpage>3125</fpage><lpage>3131</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.08.023</pub-id><pub-id pub-id-type="pmid">17936871</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Condon</surname><given-names>CD</given-names></name><name><surname>Weinberger</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Habituation produces frequency-specific plasticity of receptive fields in the auditory cortex</article-title><source>Behavioral Neuroscience</source><volume>105</volume><fpage>416</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1037//0735-7044.105.3.416</pub-id><pub-id pub-id-type="pmid">1863363</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawe</surname><given-names>LA</given-names></name><name><surname>Platt</surname><given-names>JR</given-names></name><name><surname>Welsh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Spectral-motion aftereffects and the tritone paradox among Canadian subjects</article-title><source>Perception &amp; Psychophysics</source><volume>60</volume><fpage>209</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.3758/BF03206030</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>I</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural population coding of sound level adapts to stimulus statistics</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1684</fpage><lpage>1689</lpage><pub-id pub-id-type="doi">10.1038/nn1541</pub-id><pub-id pub-id-type="pmid">16286934</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>I</given-names></name><name><surname>Robinson</surname><given-names>BL</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Rapid neural adaptation to sound level statistics</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6430</fpage><lpage>6438</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0470-08.2008</pub-id><pub-id pub-id-type="pmid">18562614</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Denoising based on time-shift PCA</article-title><source>Journal of Neuroscience Methods</source><volume>165</volume><fpage>297</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.06.003</pub-id><pub-id pub-id-type="pmid">17624443</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Depireux</surname><given-names>DA</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Klein</surname><given-names>DJ</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>1220</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.85.3.1220</pub-id><pub-id pub-id-type="pmid">11247991</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deutsch</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>A musical paradox</article-title><source>Music Perception</source><volume>3</volume><fpage>275</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.2307/40285337</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deutsch</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Paradoxes of musical pitch</article-title><source>Scientific American</source><volume>267</volume><fpage>88</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1038/scientificamerican0892-88</pub-id><pub-id pub-id-type="pmid">1641627</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Xiang</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Interaction between attention and bottom-up saliency mediates the representation of foreground and background in an auditory scene</article-title><source>PLOS Biology</source><volume>7</volume><elocation-id>e1000129</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000129</pub-id><pub-id pub-id-type="pmid">19529760</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Tolnai</surname><given-names>S</given-names></name><name><surname>Typlt</surname><given-names>M</given-names></name><name><surname>Jost</surname><given-names>J</given-names></name><name><surname>Rübsamen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reliability of synaptic transmission at the synapses of Held in vivo under acoustic stimulation</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e7014</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0007014</pub-id><pub-id pub-id-type="pmid">19798414</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Sorenson</surname><given-names>MD</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MANTA--an open-source, high density electrophysiology recording suite for MATLAB</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00069</pub-id><pub-id pub-id-type="pmid">23653593</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Akram</surname><given-names>S</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Decoding contextual influences on auditory perception from primary auditory cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.12.24.573229</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Pourtois</surname><given-names>G</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Investigating audiovisual integration of emotional signals in the human brain</article-title><source>Understanding Emotions. Elsevier</source><volume>1</volume><fpage>345</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)56019-4</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1038/35090500</pub-id><pub-id pub-id-type="pmid">11518957</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farley</surname><given-names>BJ</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Doherty</surname><given-names>JJ</given-names></name><name><surname>Christian</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus-specific adaptation in auditory cortex is an NMDA-independent process distinct from the sensory novelty encoded by the mismatch negativity</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>16475</fpage><lpage>16484</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2793-10.2010</pub-id><pub-id pub-id-type="pmid">21147987</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Klein</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>1216</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1038/nn1141</pub-id><pub-id pub-id-type="pmid">14583754</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>RB</given-names></name><name><surname>Wilson</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Evidence for direction-specific channels in the processing of frequency modulation</article-title><source>The Journal of the Acoustical Society of America</source><volume>66</volume><fpage>704</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1121/1.383220</pub-id><pub-id pub-id-type="pmid">489842</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name><name><surname>Radner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>Adaptation, after-effect and contrast in the perception of tilted lines: I: Quantitative studies</article-title><source>Journal of Experimental Psychology</source><volume>20</volume><fpage>453</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1037/h0059826</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gourévitch</surname><given-names>B</given-names></name><name><surname>Noreña</surname><given-names>A</given-names></name><name><surname>Shaw</surname><given-names>G</given-names></name><name><surname>Eggermont</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spectrotemporal receptive fields in anesthetized cat primary auditory cortex are context dependent</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>1448</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn184</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Gerwinn</surname><given-names>S</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inferring decoding strategies from choice probabilities in the presence of correlated variability</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>235</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/nn.3309</pub-id><pub-id pub-id-type="pmid">23313912</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual decision-making as probabilistic inference by neural sampling</article-title><source>Neuron</source><volume>90</volume><fpage>649</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.020</pub-id><pub-id pub-id-type="pmid">27146267</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Temporally nonadjacent nonlinguistic sounds affect speech categorization</article-title><source>Psychological Science</source><volume>16</volume><fpage>305</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.01532.x</pub-id><pub-id pub-id-type="pmid">15828978</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Zemel</surname><given-names>RS</given-names></name><name><surname>Natarajan</surname><given-names>R</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Fast population coding</article-title><source>Neural Computation</source><volume>19</volume><fpage>404</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.2.404</pub-id><pub-id pub-id-type="pmid">17206870</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name><name><surname>Ahveninen</surname><given-names>J</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Raij</surname><given-names>T</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Short-term plasticity in auditory cognition</article-title><source>Trends in Neurosciences</source><volume>30</volume><fpage>653</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2007.09.003</pub-id><pub-id pub-id-type="pmid">17981345</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>DZ</given-names></name><name><surname>Dragoi</surname><given-names>V</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Tilt aftereffect and adaptation-induced changes in orientation tuning in visual cortex</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>4038</fpage><lpage>4050</lpage><pub-id pub-id-type="doi">10.1152/jn.00571.2004</pub-id><pub-id pub-id-type="pmid">16135549</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Adaptation changes the direction tuning of macaque MT neurons</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>764</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1038/nn1267</pub-id><pub-id pub-id-type="pmid">15195097</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual adaptation: physiology, mechanisms, and functional benefits</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>3155</fpage><lpage>3164</lpage><pub-id pub-id-type="doi">10.1152/jn.00086.2007</pub-id><pub-id pub-id-type="pmid">17344377</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>RI</given-names></name><name><surname>Wu</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The generation of direction selectivity in the auditory system</article-title><source>Neuron</source><volume>73</volume><fpage>1016</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.11.035</pub-id><pub-id pub-id-type="pmid">22405210</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrance</surname><given-names>ELA</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>Cooke</surname><given-names>JE</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal predictability enhances auditory detection</article-title><source>The Journal of the Acoustical Society of America</source><volume>135</volume><fpage>EL357</fpage><lpage>EL363</lpage><pub-id pub-id-type="doi">10.1121/1.4879667</pub-id><pub-id pub-id-type="pmid">24907846</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Efficient coding of natural sounds</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>356</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nn831</pub-id><pub-id pub-id-type="pmid">11896400</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linke</surname><given-names>AC</given-names></name><name><surname>Vicente-Grabovetsky</surname><given-names>A</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Stimulus-specific suppression preserves information in auditory short-term memory</article-title><source>PNAS</source><volume>108</volume><fpage>12961</fpage><lpage>12966</lpage><pub-id pub-id-type="doi">10.1073/pnas.1102118108</pub-id><pub-id pub-id-type="pmid">21768383</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lotto</surname><given-names>AJ</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Putting phonetic context effects into context: a commentary on Fowler (2006)</article-title><source>Perception &amp; Psychophysics</source><volume>68</volume><fpage>178</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.3758/bf03193667</pub-id><pub-id pub-id-type="pmid">16773891</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing high-dimensional data using t-SNE</article-title><source>Journal of Machine Learning Research</source><volume>1</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Natan</surname><given-names>RG</given-names></name><name><surname>Rao</surname><given-names>W</given-names></name><name><surname>Geffen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cortical interneurons differentially shape frequency tuning following adaptation</article-title><source>Cell Reports</source><volume>21</volume><fpage>878</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.10.012</pub-id><pub-id pub-id-type="pmid">29069595</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mismatch negativity and stimulus-specific adaptation in animal models</article-title><source>Journal of Psychophysiology</source><volume>21</volume><fpage>214</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1027/0269-8803.21.34.214</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Netser</surname><given-names>S</given-names></name><name><surname>Zahar</surname><given-names>Y</given-names></name><name><surname>Gutfreund</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Stimulus-specific adaptation: can it be a neural correlate of behavioral habituation?</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>17811</fpage><lpage>17820</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4790-11.2011</pub-id><pub-id pub-id-type="pmid">22159097</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction, Bayesian inference and feedback in speech recognition</article-title><source>Language, Cognition and Neuroscience</source><volume>31</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1081703</pub-id><pub-id pub-id-type="pmid">26740960</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parras</surname><given-names>GG</given-names></name><name><surname>Nieto-Diego</surname><given-names>J</given-names></name><name><surname>Carbajal</surname><given-names>GV</given-names></name><name><surname>Valdés-Baizabal</surname><given-names>C</given-names></name><name><surname>Escera</surname><given-names>C</given-names></name><name><surname>Malmierca</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neurons along the auditory pathway exhibit a hierarchical organization of prediction error</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>2148</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02038-6</pub-id><pub-id pub-id-type="pmid">29247159</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-González</surname><given-names>D</given-names></name><name><surname>Malmierca</surname><given-names>MS</given-names></name><name><surname>Covey</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Novelty detector neurons in the mammalian auditory midbrain</article-title><source>The European Journal of Neuroscience</source><volume>22</volume><fpage>2879</fpage><lpage>2885</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04472.x</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>EAK</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Diverse effects of stimulus history in waking mouse auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>1376</fpage><lpage>1393</lpage><pub-id pub-id-type="doi">10.1152/jn.00094.2017</pub-id><pub-id pub-id-type="pmid">28566458</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contrast gain control in auditory cortex</article-title><source>Neuron</source><volume>70</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.030</pub-id><pub-id pub-id-type="pmid">21689603</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raviv</surname><given-names>O</given-names></name><name><surname>Ahissar</surname><given-names>M</given-names></name><name><surname>Loewenstein</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How recent history affects perception: the normative approach and its heuristic approximation</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002731</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002731</pub-id><pub-id pub-id-type="pmid">23133343</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Spectral envelope and context effects in the tritone paradox</article-title><source>Perception</source><volume>26</volume><fpage>645</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1068/p260645</pub-id><pub-id pub-id-type="pmid">9488887</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riecke</surname><given-names>L</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Vanbussel</surname><given-names>M</given-names></name><name><surname>Schreiner</surname><given-names>CS</given-names></name><name><surname>Mendelsohn</surname><given-names>D</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recalibration of the auditory continuity illusion: sensory and decisional effects</article-title><source>Hearing Research</source><volume>277</volume><fpage>152</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2011.01.013</pub-id><pub-id pub-id-type="pmid">21276844</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Pekkola</surname><given-names>J</given-names></name><name><surname>Degerman</surname><given-names>A</given-names></name><name><surname>Autti</surname><given-names>T</given-names></name><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Modulation of auditory cortex activation by sound presentation rate and attention</article-title><source>Human Brain Mapping</source><volume>26</volume><fpage>94</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1002/hbm.20123</pub-id><pub-id pub-id-type="pmid">15852467</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ronken</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Changes in frequency discrimination caused by leading and trailing tones</article-title><source>The Journal of the Acoustical Society of America</source><volume>51</volume><fpage>1947</fpage><lpage>1950</lpage><pub-id pub-id-type="doi">10.1121/1.1913054</pub-id><pub-id pub-id-type="pmid">5045254</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname><given-names>P</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Is the homunculus “aware” of sensory adaptation?</article-title><source>Neural Computation</source><volume>21</volume><fpage>3271</fpage><lpage>3304</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.09-08-869</pub-id><pub-id pub-id-type="pmid">19686064</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Fleshman</surname><given-names>JW</given-names></name><name><surname>Wiser</surname><given-names>PR</given-names></name><name><surname>Versnel</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Organization of response areas in ferret primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>367</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.2.367</pub-id><pub-id pub-id-type="pmid">8459273</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shechter</surname><given-names>B</given-names></name><name><surname>Depireux</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Response adaptation to broadband sounds in primary auditory cortex of the awake ferret</article-title><source>Hearing Research</source><volume>221</volume><fpage>91</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2006.08.002</pub-id><pub-id pub-id-type="pmid">16982164</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Circularity in judgments of relative pitch</article-title><source>The Journal of the Acoustical Society of America</source><volume>36</volume><fpage>2346</fpage><lpage>2353</lpage><pub-id pub-id-type="doi">10.1121/1.1919362</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>JR</given-names></name><name><surname>Craft</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The effect of prediction accuracy on choice reaction time</article-title><source>Memory &amp; Cognition</source><volume>17</volume><fpage>503</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.3758/BF03202624</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>EC</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient auditory coding</article-title><source>Nature</source><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian computation through cortical latent dynamics</article-title><source>Neuron</source><volume>103</volume><fpage>934</fpage><lpage>947</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.012</pub-id><pub-id pub-id-type="pmid">31320220</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulanovsky</surname><given-names>N</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Processing of low-probability sounds by cortical neurons</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>391</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1038/nn1032</pub-id><pub-id pub-id-type="pmid">12652303</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulanovsky</surname><given-names>N</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Farkas</surname><given-names>D</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multiple time scales of adaptation in auditory cortex neurons</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>10440</fpage><lpage>10453</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1905-04.2004</pub-id><pub-id pub-id-type="pmid">15548659</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von der Behrens</surname><given-names>W</given-names></name><name><surname>Bäuerle</surname><given-names>P</given-names></name><name><surname>Kössl</surname><given-names>M</given-names></name><name><surname>Gaese</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Correlating stimulus-specific adaptation of cortical neurons and local field potentials in the awake rat</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13837</fpage><lpage>13849</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3475-09.2009</pub-id><pub-id pub-id-type="pmid">19889995</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Synaptic mechanisms of forward suppression in rat auditory cortex</article-title><source>Neuron</source><volume>47</volume><fpage>437</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.06.009</pub-id><pub-id pub-id-type="pmid">16055066</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>GI</given-names></name><name><surname>Dean</surname><given-names>I</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Time course of dynamic range adaptation in the auditory nerve</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>69</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1152/jn.00055.2012</pub-id><pub-id pub-id-type="pmid">22457465</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>C</given-names></name><name><surname>Poo</surname><given-names>M</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Synaptic mechanisms of direction selectivity in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>1861</fpage><lpage>1868</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3088-09.2010</pub-id><pub-id pub-id-type="pmid">20130195</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>LI</given-names></name><name><surname>Tan</surname><given-names>AYY</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Topography and synaptic shaping of direction selectivity in primary auditory cortex</article-title><source>Nature</source><volume>424</volume><fpage>201</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1038/nature01796</pub-id><pub-id pub-id-type="pmid">12853959</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94296.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>King</surname><given-names>Andrew J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study explores the neural basis for a well known auditory illusion, often utilized in movie soundtracks, in which a sequence of two complex tones can be perceived as either rising or falling in pitch depending on the context in which they are presented. <bold>Convincing</bold> single-neuron data and analyses are presented to show that correlates of these pitch-direction changes are found in the ferret primary auditory cortex. While these findings provide an interesting link between cortical activity and perception, the manuscript could be clearer on the wider implications of the failure of traditional decoding models to account for these results.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94296.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Previous work demonstrated a strong bias in the percept of an ambiguous Shepard tone as either ascending or descending in pitch, depending on the preceding contextual stimulus. The authors recorded human MEG and ferret A1 single-unit activity during presentation of stimuli identical to those used in the behavioral studies. They used multiple neural decoding methods to test if context-dependent neural responses to ambiguous stimulus replicated the behavioral results. Strikingly, a decoder trained to report stimulus pitch produced biases opposite to the perceptual reports. These biases could be explained robustly by a feed-forward adaptation model. Instead, a decoder that took into account direction selectivity of neurons in the population was able to replicate the change in perceptual bias.</p><p>Strengths:</p><p>This study explores an interesting and important link between neural activity and sensory percepts, and it demonstrates convincingly that traditional neural decoding models cannot explain percepts. Experimental design and data collection appear to have been executed carefully. Subsequent analysis and modeling appear rigorous. The conclusion that traditional decoding models cannot explain the contextual effects on percepts is quite strong.</p><p>Weaknesses:</p><p>Beyond the very convincing negative results, it is less clear exactly what the conclusion is or what readers should take away from this study. The presentation of the alternative, &quot;direction aware&quot; models is unclear, making it difficult to determine if they are presented as realistic possibilities or simply novel concepts. Does this study make predictions about how information from auditory cortex must be read out by downstream areas? There are several places where the thinking of the authors should be clarified, in particular, around how this idea of specialized readout of direction-selective neurons should be integrated with a broader understanding of auditory cortex.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94296.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This is an elegant study investigating possible mechanisms underlying the hysteresis effect in the perception of perceptually ambiguous Shepard tones. The authors make a fairly convincing case that the adaptation of pitch direction sensitive cells in auditory cortex is likely responsible for this phenomenon.</p><p>Strengths:</p><p>The manuscript is overall well written. My only slight criticism is that, in places, particularly for non-expert readers, it might be helpful to work a little bit more methods detail into the results section, so readers don't have to work quite so hard jumping from results to methods and back.</p><p>The methods seem sound and the conclusions warranted and carefully stated. Overall I would rate the quality of this study as very high, and I do not have any major issues to raise.</p><p>Weaknesses:</p><p>I think this study is about as good as it can be with the current state of the art. Generally speaking, one has to bear in mind that this is an observational, rather than an interventional study, and therefore only able to identify plausible candidate mechanisms rather than making definitive identifications. However, the study nevertheless represents a significant advance over the current state of knowledge, and about as good as it can be with the techniques that are currently widely available.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94296.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><role specific-use="author">Author</role><aff><institution>Computational Neuroscience Lab, Donders Center for Neuroscience, Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Akram</surname><given-names>Sahar</given-names></name><role specific-use="author">Author</role><aff><institution>University of Maryland, College Park</institution><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Elhilali</surname><given-names>Mounya</given-names></name><role specific-use="author">Author</role><aff><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Shamma</surname><given-names>Shihab</given-names></name><role specific-use="author">Author</role><aff><institution>University of Maryland</institution><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>Previous work demonstrated a strong bias in the percept of an ambiguous Shepard tone as either ascending or descending in pitch, depending on the preceding contextual stimulus. The authors recorded human MEG and ferret A1 single-unit activity during presentation of stimuli identical to those used in the behavioral studies. They used multiple neural decoding methods to test if context-dependent neural responses to ambiguous stimulus replicated the behavioral results. Strikingly, a decoder trained to report stimulus pitch produced biases opposite to the perceptual reports. These biases could be explained robustly by a feed-forward adaptation model. Instead, a decoder that took into account direction selectivity of neurons in the population was able to replicate the change in perceptual bias.</p><p>Strengths:</p><p>This study explores an interesting and important link between neural activity and sensory percepts, and it demonstrates convincingly that traditional neural decoding models cannot explain percepts. Experimental design and data collection appear to have been executed carefully. Subsequent analysis and modeling appear rigorous. The conclusion that traditional decoding models cannot explain the contextual effects on percepts is quite strong.</p><p>Weaknesses:</p><p>Beyond the very convincing negative results, it is less clear exactly what the conclusion is or what readers should take away from this study. The presentation of the alternative, &quot;direction aware&quot; models is unclear, making it difficult to determine if they are presented as realistic possibilities or simply novel concepts. Does this study make predictions about how information from auditory cortex must be read out by downstream areas? There are several places where the thinking of the authors should be clarified, in particular, around how this idea of specialized readout of direction-selective neurons should be integrated with a broader understanding of auditory cortex.</p></disp-quote><p>While we have not used the term &quot;direction aware&quot;, we think the reviewer refers generally to the capability of our model to use a cell's direction selectivity in the decoding. In accordance with the reviewer's interpretation, we did indeed mean that the decoder assumes that a neuron does not only have a preferred frequency, but also a preferred direction of change in frequency (ascending/descending), which is what we use to demonstrate that the decoding in this way aligns with the human percept. We have adapted the text in several places to clarify this, in particular expanding the description in the Methods substantially.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>The authors aim to better understand the neural responses to Shepard tones in auditory cortex. This is an interesting question as Shepard tones can evoke an ambiguous pitch that is manipulated by a proceeding adapting stimulus, therefore it nicely disentangles pitch perception from simple stimulus acoustics.</p><p>The authors use a combination of computational modelling, ferret A1 recordings of single neurons, and human EEG measurements.</p><p>Their results provide new insights into neural correlates of these stimuli. However, the manuscript submitted is poorly organized, to the point where it is near impossible to review. We have provided Major Concerns below. We will only be able to understand and critique the manuscript fully after these issues have been addressed to improve the readability of the manuscript. Therefore, we have not yet reviewed the Discussion section.</p><p>Major concerns</p><p>Organization/presentation</p><p>The manuscript is disorganized and therefore difficult to follow. The biggest issue is that in many figures, the figure subpanels often do not correspond to the legend, the main body, or both. Subpanels described in the text are missing in several cases.</p></disp-quote><p>We have gone linearly through the text and checked that all figure subpanels are referred to in the text and the legend. As far as we can tell, this was already the case for all panels, with the exception of two subpanels of Fig. 5.</p><disp-quote content-type="editor-comment"><p>Many figure axes are unlabelled.</p></disp-quote><p>We have carefully checked the axes of all panels and all but two (Fig. 5D) were labeled. As is customary, certain panels inherit the axis label from a neighboring panel, if the label is the same, e.g. subpanels in Fig. 6F or Fig. 5E, which helps to declutter the figure. We hope that with this clarification, the reviewer can understand the labels of each panel.</p><disp-quote content-type="editor-comment"><p>There is an inconsistent style of in-text citation between figures and the main text. The manuscript contains typos and grammatical errors. My suggestions for edits below therefore should not be taken as an exhaustive list. I ask the authors to consider the following only a &quot;first pass&quot; review, and I will hopefully be able to think more deeply about the science in the second round of revisions after the manuscript is better organized.</p></disp-quote><p>While we are puzzled by the severity of issues that R2 indicates (see above, and R3 qualifies it as &quot;well written&quot;, and R1 does not comment on the writing negatively), we have carefully gone through all specific issues mentioned by R2 and the other reviewers. We hope that the revised version of the paper with all corrections and clarifications made will resolve any remaining issues.</p><disp-quote content-type="editor-comment"><p>Frequency and pitch</p><p>The terms &quot;frequency&quot; and &quot;pitch&quot; seem to be used interchangeably at times, which can lead to major misconceptions in a manuscript on Shepard tones. It is possible that the authors confuse these concepts themselves at times (e.g. Fig 5), although this would be surprising given their expertise in this field. Please check through every use of &quot;frequency&quot; and &quot;pitch&quot; in this manuscript and make sure you are using the right term in the right place. In many places, &quot;frequency&quot; should actually be &quot;fundamental frequency&quot; to avoid misunderstanding.</p></disp-quote><p>Thanks for pointing this out. We have checked every occurrence and modified where necessary.</p><disp-quote content-type="editor-comment"><p>Insufficient detail or lack of clarity in descriptions</p><p>There seems to be insufficient information provided to evaluate parts of these analysis, most critically the final pitch-direction decoder (Fig 6), which is a major finding. Please clarify.</p></disp-quote><p>Thanks for pointing this out. We have extended the description of the pitch-direction decoder and highlighted its role for interpreting the results.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>This is an elegant study investigating possible mechanisms underlying the hysteresis effect in the perception of perceptually ambiguous Shepard tones. The authors make a fairly convincing case that the adaptation of pitch direction sensitive cells in auditory cortex is likely responsible for this phenomenon.</p><p>Strengths:</p><p>The manuscript is overall well written. My only slight criticism is that, in places, particularly for non-expert readers, it might be helpful to work a little bit more methods detail into the results section, so readers don't have to work quite so hard jumping from results to methods and back.</p></disp-quote><p>Following this excellent suggestion, we have added more brief method sketches to the Results section, hopefully addressing this concern.</p><disp-quote content-type="editor-comment"><p>The methods seem sound and the conclusions warranted and carefully stated. Overall I would rate the quality of this study as very high, and I do not have any major issues to raise.</p></disp-quote><p>Thanks for your encouraging evaluation of the work.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>I think this study is about as good as it can be with the current state of the art. Generally speaking, one has to bear in mind that this is an observational, rather than an interventional study, and therefore only able to identify plausible candidate mechanisms rather than making definitive identifications. However, the study nevertheless represents a significant advance over the current state of knowledge, and about as good as it can be with the techniques that are currently widely available.</p></disp-quote><p>Thanks for your encouraging evaluation of our work. The suggestion of an interventional study has also been on our minds, however, this appears rather difficult, as it would require a specific subset of cells to be inhibited. The most suitable approach would likely be 2p imaging with holographic inhibition of a subset of cells (using ArchT for example), that has a preference for one direction of pitch change, which should then bias the percept/behavior in the opposite direction.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>MAJOR CONCERNS</p><p>(1) What is the timescale used to compute direction selectivity in neural tuning? How does it compare to the timing of the Shepard tones? The basic idea of up versus down pitch is clear, the intuition for the role of direction tuning and its relation to stimulus dynamics could be laid out more clearly. Are the authors proposing that there are two &quot;special&quot; populations of A1 neurons that are treated differently to produce the biased percept? Or is there something specific about the dynamics of the Shepard stimuli and how direction selective neurons respond to them specifically? It would help if the authors could clarify if this result links to broader concepts of dynamic pitch coding in general or if the example reported here is specific (or idiosyncratic) to Shepard tones.</p></disp-quote><p>We propose that the findings here are not specific to Shepard tones. To the contrary, only basic properties of auditory cortex neurons, i.e. frequency preference, frequency-direction (i.e. ascending or descending) preference, and local adaptation in the tuning curve, suffice. Each of these properties have been demonstrated many times before and we only verified this in the lead-up to the results in Fig. 6. While the same effects should be observable with pure tones, the lack of ambiguity in the perception of direction of a frequency step for pure tone pairs, would make them less noticeable here. Regarding the time-scale of the directional selectivity, we relied on the sequencing of tones in our paradigm, i.e. 150 ms spacing. The SSTRFs were discretized at 50 ms, and include only the bins during the stimulus, not during the pause. The directional tuning, i.e. differences in the SSTRF above and below the preferred pitchclass for stimuli before the last stimulus, typically extended only one stimulus back in time. We have clarified this in more detail now, in particular in the added Methods section on the directional decoder.</p><disp-quote content-type="editor-comment"><p>(2) (p. 9) &quot;weighted by each cell's directionality index ... (see Methods for details)&quot; The direction-selective decoder is interesting and appears critical to the study. However, the details of its implementation are difficult to locate. Maybe Fig. 6A contains the key concepts? It would help greatly if the authors could describe it in parallel with the other decoders in the Methods.</p></disp-quote><p>We have expanded the description of the decoder in the Methods as the reviewer suggests.</p><disp-quote content-type="editor-comment"><p>LESSER CONCERNS</p><p>p. 1. (L 24) &quot;distances between the pitch representations....&quot; It's not obvious what &quot;distances&quot; means without reading the main paper. Can some other term or extra context be provided?</p></disp-quote><p>We have added a brief description here.</p><disp-quote content-type="editor-comment"><p>p. 2. (L 26) &quot;Shepard tones&quot; Can the authors provide a citation when they first introduce this class of stimuli?</p></disp-quote><p>Citation has been added.</p><disp-quote content-type="editor-comment"><p>p. 3 (L 4) &quot;direction selective cells&quot; Please define or provide context for what has a direction. Selective to pitch changes in time?</p></disp-quote><p>Yes, selective to pitch changes in time is what is meant. We have further clarified this in the text.</p><disp-quote content-type="editor-comment"><p>p. 4 (L 9-19). This paragraph seems like it belongs in the Introduction?</p></disp-quote><p>Given the concerns raised by R2 about the organization of the manuscript we prefer to keep this 'road-map' in the manuscript, as a guidance for the reader.</p><disp-quote content-type="editor-comment"><p>p. 4 (L 32) &quot;majority of cells&quot; One might imagine that the overlap of the bias band and the frequency tuning curve of individual neurons might vary substantially. Was there some criterion about the degree of overlap for including single units in the analysis? Does overlap matter?</p></disp-quote><p>We are not certain which analysis the reviewer is referring to. Generally, cells were not excluded based on their overlap between a particular Bias band and their (Shepard) tuning curve. There are several reasons for this: The bias was located in 4 different, overlapping Shepard tone regions, and all sounds were Shepard tones. Therefore, all cells overlapped with their (Shepard) tuning curve with one or multiple of the Biases. For decoding analysis, all cells were included as both a response and lack of a response is contributing to the decoding. If the reviewer is referring only to the analysis of whether a cell adapts, then the same argument applies as above, i.e. this was an average over all Bias sequences, and therefore every responding cell was driven to respond by the Bias, and therefore it was possible to also assess whether it adapted its response for different positions inside the Bias. We acknowledge that the limited randomness of the Bias sequences in combination with the specific tuning of the cells could in a few cases create response patterns over time that are not indicative of the actual behavior for repeated stimulation, however, since the results are rather clear with 91% of cells adapting, we do not think this would significantly change the conclusions.</p><disp-quote content-type="editor-comment"><p>p. 5 (L 17) &quot;desynchronization ... behaving conditions&quot; The logic here is not clear. Is less desynchronization expected during behavior? Typically, increased attention is associated with greater desynchronization.</p></disp-quote><p>Yes, we reformulated the sentence to: While this difference could be partly explained by desynchronization which is typically associated with active behavior or attention [30], general response adaptation to repeated stimuli is also typical in behaving humans [31].</p><disp-quote content-type="editor-comment"><p>p. 7 (L 5) &quot;separation&quot; is this a separation in time?</p></disp-quote><p>Yes, added.</p><disp-quote content-type="editor-comment"><p>p. 7 (L 33) &quot;local adaptation&quot; The idea of feedforward adaptation biasing encoding has been proposed before, and it might be worth citing previous work. This includes work from Nelken specifically related to SSA. Also, this model seems similar to the one described in Lopez Espejo et al (PLoS CB 2019).</p></disp-quote><p>Thanks for pointing this out. We think, however, that neither of these publications suggested this very narrow way of biasing, which we consider biologically implausible. We have therefore not added either of these citations.</p><disp-quote content-type="editor-comment"><p>p. 11 (L. 17) The cartoon in Fig. 6G may provide some intuition, but it is quite difficult to interpret. Is there a way to indicate which neuron &quot;votes&quot; for which percept?</p></disp-quote><p>This is an excellent idea, and we have added now the purported perceptual relation of each cell in the diagram.</p><disp-quote content-type="editor-comment"><p>p. 12 (L. 8). &quot;classically assumed&quot; This statement could benefit from a citation. Or maybe &quot;classically&quot; is not the right word?</p></disp-quote><p>We have changed 'classically' to 'typically', and now cite classical works from Deutsch and Repp. We think this description makes sense, as the whole concept of bistable percepts has been interpreted as being equidistant (in added or subtracted semitone steps) from the first tone, see e.g. Repp 1997, Fig.2.</p><disp-quote content-type="editor-comment"><p>p. 12 (L. 12) &quot;...previous studies&quot; of Shepard tone percepts? Of physiology?</p></disp-quote><p>We have modified it to 'Relation to previous studies of Shepard tone percepts and their underlying physiology&quot;, since this section deals with both.</p><disp-quote content-type="editor-comment"><p>p. 12 (L. 25) &quot;compatible with cellular mechanisms...&quot; This paragraph seems key to the study and to Major Concern 1, above. What are the dynamics of the task stimuli? How do they compare with the dynamics of neural FM tuning and previously reported studies of bias? And can the authors be more explicit in their interpretation - should direction selective neurons respond preferentially to the Shepard tone stimuli themselves? And/or is there a conceptual framework where the same neurons inform downstream percepts of both FM sweeps and both normal (unbiased) and biased Shepard tones?</p></disp-quote><p>The reviewer raises a number of different questions, which we address below:</p><p>- Dynamics of the task stimuli in relation to previously reported cellular biasing: The timescales tested in the studies mentioned are similar to what we used in our bias, e.g. Ye et al 2010 used FM sweeps that lasted for up to 200ms, which is quite comparable to our SOA of 150ms.</p><p>- Preferred responses to Shepard tones: no, we do not think that there should be preferred responses to Shepard tones, but rather that responses to Shepard tones can be thought of as the combined responses to the constituent tones.</p><p>- Conceptual framework where the same neurons inform about FM sweeps and both normal (unbiased) and biased Shepard tones: Our perspective on this question is as follows: To our knowledge, the classical approach to population decoding in the auditory system, i.e. weighted based on preferred frequency, has not been directly demonstrated to be read out inside the brain, and certainly not demonstrated to be read out in only this way in all areas of the brain that receive input from the auditory cortex. Rather it has achieved its credibility by being linked directly with animal performance or match with the presented stimuli. However, these approaches were usually geared towards a representation that can be estimated based on constituent frequencies. Additional response properties of neurons, such as directional selectivity have been documented and analyzed before, however, not been used for explaining the percept. We agree that our use of this cellular response preference in the decoding implicitly assumes that the brain could utilize this as well, however, this seems just as likely or unlikely as the use of the preferred frequency of a neuron. Therefore we do not think that this decoding is any more speculative than the classical decoding. In both cases, subsequent neurons would have to implicitly 'know' the preference of the input neuron, and weigh its input correspondingly.</p><p>We have added all the above considerations to the discussion in an abbreviated form.</p><disp-quote content-type="editor-comment"><p>p. 15 (L. 15). Is there a citation for the drive system?</p></disp-quote><p>There is no publication, but an old repository, where the files are available, which we cite now: <ext-link ext-link-type="uri" xlink:href="https://code.google.com/archive/p/edds-array-drive/">https://code.google.com/archive/p/edds-array-drive/</ext-link></p><disp-quote content-type="editor-comment"><p>p. 16 (L. 24) &quot;position in an octave&quot; It is implied but not explicitly stated that the Shepard tones don't contain the fundamental frequency. Can the authors clarify the relationship between the neural tuning band and the bands of the stimulus. Did a single stimulus band typically fall in a neuron's frequency tuning curve? If not 1, how many?</p></disp-quote><p>Yes, it is correct that the concept of fundamental frequency does not cleanly apply to Shepard tones, because it is composed of octave spaced pure tones, but the lowest tone is placed outside the hearing range of the animal and amplitude envelope (across frequencies). Therefore one or more constituent tones of the Shepard tone can fall into the tuning curve of a neuron and contribute to driving the neuron (or inhibiting it, if they fall within an inhibitory region of the tuning curve). The number of constituent tones that fall within the tuning curve depends on the tuning width of the neurons. The distribution of tuning widths to Shepard tones is shown in Fig. S1E, which indicated that a lot of neurons had rather narrow tuning (close to the center), but many were also tuned widely, indicated that they would be stimulated by multiple constituent tones of the Shepard tone. As the tuning bandwidth (Q30: 30dB above threshold) of most cortical neurons in the ferret auditory cortex (see e.g. Bizley et al. Cerebral Cortex, 2005, Fig.12) is below 1, this means that typically not more than 1 tone fell into the tuning curve of a neuron. However, we also observed multimodal tuning-curves w.r.t. to Shepard tones, which suggests that some neurons were stimulated by more than 2 or more constituent tones again consistent with the existence of more broadly tuned neurons (see same citation). We have added this information partly to the manuscript in the caption of Fig. S1E.</p><disp-quote content-type="editor-comment"><p>p. 17 (L. 32). &quot;Fig 4&quot; Correct figure ref? This figure appears to be a schematic rather than one displaying data.</p></disp-quote><p>Thanks for pointing this out, changed to Fig. 5.</p><disp-quote content-type="editor-comment"><p>p. 18 (L. 25). &quot;assign a pitchclass&quot; Can the authors refer to a figure illustrating this process?</p></disp-quote><p>Added.</p><disp-quote content-type="editor-comment"><p>p. 19 (L. 17). Is mu the correct symbol?</p></disp-quote><p>Thanks. We changed it to phi_i, as in the formula above.</p><disp-quote content-type="editor-comment"><p>p. 19 (L 19). &quot;convolution&quot; in time? Frequency?</p></disp-quote><p>Thanks for pointing this out, the term convolution was incorrect in this context. We have replaced it by &quot;weighted average&quot; and also adapted and simplified the formula.</p><disp-quote content-type="editor-comment"><p>p. 19 (L 25) &quot;SSTRF&quot; this term is introduced before it is defined. Also it appears that &quot;SSTRF&quot; and &quot;STRF&quot; are sometimes interchanged.</p></disp-quote><p>Apologies, we have added the definition, and also checked its usage in each location.</p><disp-quote content-type="editor-comment"><p>p. 23 (Fig 2) There is a mismatch between panel labels in the figure and in the legend. Bottom right panel (B3), what does time refer to here?</p></disp-quote><p>Thanks for pointing these out, both fixed.</p><disp-quote content-type="editor-comment"><p>p. 24 (L 23) &quot;shifts them away&quot; away from what?</p></disp-quote><p>We have expanded the sentence to: &quot;After the bias, the decoded pitchclass is shifted from their actual pitchclass away from the biased pitchclass range ... &quot;</p><disp-quote content-type="editor-comment"><p>p. 25 (L 7) &quot;individual properties&quot; properties of individual subjects?</p></disp-quote><p>Thanks for pointing this out, the corresponding sentence has been clarified and citations added.</p><disp-quote content-type="editor-comment"><p>p. 26 (L 20) What is plotted in panel D? The average for all cells? What is n?</p></disp-quote><p>Yes, this is an average over cells, the number of cells has now been added to each panel.</p><disp-quote content-type="editor-comment"><p>p. 28 (L 3) How to apply the terms &quot;right&quot; &quot;right&quot; &quot;middle&quot; to the panel is not clear. Generally, this figure is quite dense and difficult to interpret.</p></disp-quote><p>We have changed the caption of Panel A and replaced the location terms with the symbols, which helps to directly relate them to the figure. We have considered different approaches of adding or removing content from the figure to help make it less dense, but that all did not seem to help. For lack of better options we have left it in its current form.</p><disp-quote content-type="editor-comment"><p>MINOR/TYPOS</p><p>p. 3 (L 1) &quot;Stimulus Specific Adaptation&quot; Capitalization seems unnecessary</p></disp-quote><p>Changed.</p><disp-quote content-type="editor-comment"><p>p. 4 (L 14) &quot;Siple&quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>p. 9 (L 10) &quot;an quantitatively&quot;</p></disp-quote><p>Corrected</p><disp-quote content-type="editor-comment"><p>p. 9 (L 20) &quot;directional ... direction ... directly ... directional&quot; This is a bit confusing as directseems to mean several different things in its different usages.</p></disp-quote><p>We have gone through these sentences, and we think the terms are now more clearly used, especially since the term 'direction' occurs in several different forms, as it relates to different aspects (cells/percept/hypothesis). Unfortunately, some repetition is necessary to maintain clarity.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Detailed critique</p><p>Stimuli</p><p>It would be very useful if the authors could provide demos of their stimuli on a website. Many readers will not be familiar with Shepard tones and the perceptual result of the acoustical descriptions are not intuitive. I ended up coding the stimuli myself to get some intuition for them.</p></disp-quote><p>We have created some sample tones and sequences and uploaded them with the revision as supplementary documents.</p><disp-quote content-type="editor-comment"><p>Abstract</p><p>P1 L27 'pitch and...selective cells' - The authors haven't provided sufficient controls to demonstrate that these are &quot;pitch cells&quot; or &quot;selective&quot; to pitch direction. They have only shown that they are sensitive to these properties in their stimuli. Controls would need to be included to ensure that the cells aren't simply responding to one frequency component in the complex sound, for example. This is not really critical to the overall findings, but the claim about pitch &quot;selectivity&quot; is not accurate.</p></disp-quote><p>Fair point. We have removed the word 'selective' in both occurrences.</p><disp-quote content-type="editor-comment"><p>Introduction</p><p>P2 L14-17: I do not follow the phonetic example provided. The authors state that the second syllable of /alga/ and /arda/ are physically identical, but how is this possible that ga = da? The acoustics are clearly different. More explanation is needed, or a correction.</p></disp-quote><p>Apologies for the slightly misleading description, it has now been corrected to be in line with the original reference.</p><disp-quote content-type="editor-comment"><p>P2,L26-27: Should the two uses of &quot;frequency&quot; be &quot;F0&quot; and &quot;pitch&quot; here? The tones are not separated in frequency by half and octave, but &quot;separated in [F0]&quot; by half an octave, correct? Their frequency ranges are largely overlapping. And the second 'frequency', which refers to the percept, should presumably be &quot;pitch&quot;.</p></disp-quote><p>Indeed. This is now corrected.</p><disp-quote content-type="editor-comment"><p>P3 L2-6: Unclear at this point in the manuscript what is the difference between the 3 percepts mentioned: perceived pitch-change direction, Shepard tone pitches, and &quot;their respective differences&quot;. (It becomes clear later, but clarification is needed here).</p></disp-quote><p>We have tried a few reformulations, however, it tends to overload the introduction with details. We believe it is preferable to present the gist of the results here, and present the complete details later in the MS.</p><disp-quote content-type="editor-comment"><p>P3 L6-7 What does it mean that the MEG and single unit results &quot;align in direction and dynamics&quot;? These are very different signals, so clarification is needed.</p></disp-quote><p>We have phrased the corresponding sentence more clearly.</p><disp-quote content-type="editor-comment"><p>Results</p><p>Throughout: Choose one of 'pitch class', 'pitchclass', or 'pitch-class' and use it consistently.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>P4L12 - would be helpful at this point to define 'repulsive effect'</p></disp-quote><p>We have added another sentence to clarify this term.</p><disp-quote content-type="editor-comment"><p>P4, L14 &quot;simple&quot;</p></disp-quote><p>Done</p><disp-quote content-type="editor-comment"><p>P4, L12 - not clear here what &quot;repulsive influence&quot; means</p></disp-quote><p>See above.</p><disp-quote content-type="editor-comment"><p>P4, L17 - alternative to which explanation? Please clarify. In general, this paragraph is difficult to interpret because we do not yet have the details needed to understand the terms used and the results described. In my opinion, it would be better to omit this summary of the results at the very beginning, and instead reveal the findings as they come, when they can be fully explained to the Reader.</p></disp-quote><p>We agree, but we also believe that a rather general description here is useful for providing a roadmap to the results. However, we have added a half-sentence to clarify what is meant by alternative.</p><disp-quote content-type="editor-comment"><p>P4 L30 - text says that cells adapt in their onset, sustained and offset responses, but only data for onset responses are shown (I think - clarification needed for fig 2A2). Supp figure shows only 1 example cell of sustained and offset, and in fact there is no effect of adaptation in the sustained response shown there.</p></disp-quote><p>Regarding the effect of adaptation and whether it can be discerned from the supplementary figure: the shown responses are for 10 repetitions of one particular Bias sequence. Since the response of the cell will depend on its tuning and the specific sequence of the Shepard tones in this Bias, it is not possible to assess adaptation for a given cell. We assess the level of adaptation, by averaging all biases (similar to what is shown in Fig. 2A2) per cell, and then fit an exponential to it, separately by response type. The step direction of the exponential, relative to the spontaneous rate is then used to assess the kind of adaptation. The vast majority of cells show adaptation. We have added this information to the Methods of the manuscript.</p><disp-quote content-type="editor-comment"><p>P4, L32 - please state the statistical test and criterion (alpha) used to determine that 91% of cells decreased their responses throughout the Bias sequence. Was this specifically for onset responses?</p></disp-quote><p>Thanks for pointing this out, test and p-value added. Adaptation was observed for onset, sustained and offset responses, in all cases with the vast majority showing an adapting behavior, although the onset responses were adapting the most.</p><disp-quote content-type="editor-comment"><p>P4 L36 - &quot;response strength is reduced locally&quot;. What does &quot;locally&quot; mean here? Nearby frequencies?</p></disp-quote><p>We have added a sentence here to clarify this question.</p><disp-quote content-type="editor-comment"><p>Figure 1 - this appears to be the wrong version of the figure, as it doesn't match the caption or results text. It's not possible to assess this figure until these things are fixed. Figure 1A schematic of definition of f(diff) does not correspond to legend definition.</p></disp-quote><p>As far as we can tell, it is all correct, only the resolution of the figure appears to be rather low. This has been improved now.</p><disp-quote content-type="editor-comment"><p>Fig 2 A2 - is this also onset responses only?</p></disp-quote><p>Yes, added to the caption.</p><disp-quote content-type="editor-comment"><p>Fig 2 A3 - add y-axis label. The authors are comparing a very wide octave band (5.5 octaves) to a much narrower band (0.5 octaves). Could this matter? Is there something special about the cut-off of 2.5 octaves in the 2 bands, or was this an arbitrary choice?</p></disp-quote><p>Interesting question.... essentially our stimulus design left us only with this choice, i.e. comparing the internal region of the bias with the boundary region of the bias, i.e. the test tones. The internal region just corresponds to the bias, which is 5 st wide, and therefore the range is here given as 2.5 st relative to its center, while the test tones are at the boundary, as they are 3 st from the center. The axis for the bias was mislabelled, and has now been corrected. The y-axis label is matched with the panel to the left, but has now been added to avoid any confusion.</p><disp-quote content-type="editor-comment"><p>Fig 2A4 - does not refer to ferret single unit data, as stated in the text (p5L8). Nor does supp Fig2, as stated. Also, the figure caption does not match the figure.</p></disp-quote><p>Apologies, this was an error in the code that led to this mislabelling. We have corrected the labels, which also added back the recovery from the Bias sequence in the new Panel A4.</p><disp-quote content-type="editor-comment"><p>P5 l9 - Figure 3 is not understandable at this point in the text, and should not be referred to here. There is a lot going on in Fig 3, and it isn't clear what you are referring to.</p></disp-quote><p>Removed.</p><disp-quote content-type="editor-comment"><p>P5 L12 - by Fig 2 B1, I assume you mean A4? Also, F2B1 shows only 1 subject, not 2.</p></disp-quote><p>Yes, mislabeled by mistake, and corrected now.</p><disp-quote content-type="editor-comment"><p>Fig2B2 -What is the y-axis?</p></disp-quote><p>Same as in the panel to its left, added for clarity.</p><disp-quote content-type="editor-comment"><p>Stimuli: why are tones presented at a faster rate to ferrets than to humans?</p></disp-quote><p>The main reason is that the response analysis in MEG requires more spacing in time than the neuronal analysis in the ferret brain.</p><disp-quote content-type="editor-comment"><p>P5 L6 - there is no Fig 5 D2? I don't think it is a good idea to get the reader to skip so far ahead in the figures at this stage anyway, even if such a figure existed. It is confusing to jump around the manuscript</p></disp-quote><p>Changed to 'see below'</p><disp-quote content-type="editor-comment"><p>P5 L8 - There is no Figure 2A4, so I don't know whether this time constant is accurate.</p></disp-quote><p>This was in reference to a panel that had been removed before, but we have added it back now.</p><disp-quote content-type="editor-comment"><p>P5 L16: &quot;in humans appears to be more substantial (40%) than for the average single units under awake conditions&quot;. One cannot directly compare magnitude of effects in MEG and single unit signals in this way and assume it is due to behavioural state. You are comparing different measures of neural activity, averaged over vastly different numbers of numbers, and recorded from different species listening to different stimuli (presentation rates).</p></disp-quote><p>Yes, that's why the next sentence is: &quot;However, comparisons between the level of adaptation in MEG and single neuron firing rates may be misleading, due to the differences in the signal measured and subsequent processing.&quot;, and all statements in the preceding sentences are phrased as 'appears' and 'may'. We think we have formulated this comparison with an appropriate level of uncertainty. Further, the main message here is that adaptation is taking place in both active and passive conditions.</p><disp-quote content-type="editor-comment"><p>P5 L25 -I do not see any evidence regarding tuning widths in Fig s2, as stated in the text.</p></disp-quote><p>Corrected to Fig. S1.</p><disp-quote content-type="editor-comment"><p>P5 l26 - Do not skip ahead to Fig 5 here. We aren't ready to process that yet.</p></disp-quote><p>OK, reference removed.</p><disp-quote content-type="editor-comment"><p>P5 l27 - Do you mean because it could be tuning to pitch chroma, not height?</p></disp-quote><p>Yes, that is a possible interpretation, although it could also arise from a combination of excitatory and inhibitory contributions across multiple octaves.</p><disp-quote content-type="editor-comment"><p>P5 l33 - remove speculation about active vs passive for reasons given above.</p></disp-quote><p>Removed.</p><disp-quote content-type="editor-comment"><p>P6L2-6 'In the present...5 semitone step' - This is an incorrect interpretation of the minimal distance hypothesis in the context of the Shepard tone ambiguity. The percept is ambiguous because the 'true' F0 of the Shepard tones are imperceptibly low. Each constituent frequency of a single tone can therefore be perceived either as a harmonic of some lower fundamental frequency or as an independent tone. The dominant pitch of the second tone in the tritone pair may therefore be biased to be perceived at a lower constituent frequency (when the bias sequence is low) or at a higher constituent frequency (when the bias sequence is high). The text states that the minimal distance hypothesis would predict that an up-bias would make a tritone into a perfect fourth (5 semitones). This is incorrect. The MDH would predict that an up-bias would reduce the distance between the 1st tone in the ambiguous pair and the upper constituent frequency of the 2nd tone in the pair, hence making the upper constituent frequency the dominant pitch percept of the 2nd tone, causing an ascending percept.</p></disp-quote><p>The reviewer here refers to a “minimal distance hypothesis”, which without a literature reference,is hard for us to fully interpret. However, some responses are given below:</p><p>- &quot;The percept is ambiguous because the 'true' F0 of the Shepard tones are imperceptibly low.&quot; This statement appears to be based on some misconception: due to the octave spacing (rather than multiple/harmonics of a lowest frequency), the Shepard tones cannot be interpreted as usual harmonic tones would be. It is correct that the lowest tone in a Shepard tone is not audible, due to the envelope and the fact that it could in principle be arbitrarily small... hence, speaking about an F0 is really not well-defined in the case of a Shepard tone. The closest one could get to it would be to refer to the Shepard tone that is both in the audible range and in the non-zero amplitude envelope. But again, since the envelope is fading out the highest and lowest constituent tones, it is not as easy to refer to the lowest one as F0 as it might be much quieter than the next higher constituent.</p><p>- &quot;The dominant pitch of the second tone in the tritone pair may therefore be biased to be perceived at a lower constituent frequency (when the bias sequence is low) or at a higher constituent frequency (when the bias sequence is high).&quot; This may relate to some known psychophysics, but we are unable to interpret it with certainty.</p><p>- &quot;The text states that the minimal distance hypothesis would predict that an up-bias would make a tritone into a perfect fourth (5 semitones). This is incorrect.&quot; We are unsure how the reviewer reaches this conclusion.</p><p>- &quot;The MDH would predict that an up-bias would reduce the distance between the 1st tone in the ambiguous pair and the upper constituent frequency of the 2nd tone in the pair, hence making the upper constituent frequency the dominant pitch percept of the 2nd tone, causing an ascending percept.&quot; Again, in the absence of a reference to the MDH, we are unsure of the implied rationale. We agree that this is a possible interpretation of distance, however, we believe that our interpretation of distance (i.e. distances between constituent tones) is also a possible interpretation.</p><disp-quote content-type="editor-comment"><p>Fig 4: Given that it comes before Figure 3 in the results text, these should be switched in order in the paper.</p></disp-quote><p>Switched.</p><disp-quote content-type="editor-comment"><p>PCA decoder: The methods (p18) state that the PCA uses the first 3 dimensions, and that pitch classes are calculated from the closest 4 stimuli. The results (P6), however, state that the first 2 principal components are used, and classes are computed from the average of 10 adjacent points. Which is correct, or am I missing something?</p></disp-quote><p>Thanks for pointing this out, we have made this more concrete in the Methods to: &quot;The data were projected to the first three dimensions, which represented the pitch class as well as the position in the sequence of stimuli (see Fig. 43A for a schematic). As the position in the Bias sequence was not relevant for the subsequent pitch class decoding, we only focussed on the two dimensions that spanned the pitch circle.&quot; Regarding the number of stimuli that were averaged: this might be a slight misunderstanding: Each Shepard tone was decoded/projected without averaging. However, to then assign an estimated pitch class, we first had to establish an axis (here going around the circle), where each position along the axis was associated with a pitch class. This was done by stepping in 0.5 semitone steps, and finding the location in decoded space that corresponded to the median of the Shepard tones within +/- 0.25st. To increase the resolution, this circular 'axis' of 24 points was then linearly interpolated to a resolution of 0.05st. We have updated the text in the Methods accordingly. The mentioning of 10 points for averaging in the Results was correct, as there were 240 tones in all bias stimuli, and 24 bins in the pitch circle. The mentioning of an average over 4 tones in the Methods was a typo.</p><disp-quote content-type="editor-comment"><p>Fig 3A: axes of pink plane should be PC not PCA</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>Fig 3B: the circularity in the distribution of these points is indeed interesting! But what do the authors make of the gap in the circle between semitones 6-7? Is this showing an inherent bias in the way the ambiguous tone is represented?</p></disp-quote><p>While we cannot be certain, we think that this represents an inhomogeneous sampling from the overall set of neural tuning preferences, and that if we had recorded more/all neurons, the circle would be complete and uniformly sampled (which it already nearly is, see Fig.4C, which used to be Fig. 3C).</p><disp-quote content-type="editor-comment"><p>Fig 3B (lesser note): It'd be preferable to replace the tint (bright vs. dark) differentiation of the triangles to be filled vs. unfilled because such a subtle change in tint is not easily differentiable from a change in hue (indicating a different variable in this plot) with this particular colour palette</p></disp-quote><p>We have experimented with this suggestion, and it didn't seem to improve the clarity. However, we have changed the outline of the test-pair triangles to white, which now visually separates them better.</p><disp-quote content-type="editor-comment"><p>P6 l32 - Please indicate if cross-validation was used in this decoder, and if so, what sort. Ideally, the authors would test on a held-out data set, or at least take a leave-one-out approach. Otherwise, the classifier may be overfit to the data, and overfitting would explain the exceptional performance (r=.995) of the classifier.</p></disp-quote><p>Cross-validation was not used, as the purpose of the decoder is here to create a standard against which to compare the biased responses in the ambiguous pair, which were not used for training of the decoder. We agree that if we instead used a cross-validated decoder (which would only apply to the local average to establish the pitch class circle) the correlation would be somewhat lower, however, this is less relevant for the main question, i.e. the influence of the Bias sequence on the neural representation of the ambiguous pair. We have added this information to the corresponding section.</p><disp-quote content-type="editor-comment"><p>Fig 3D: I understood that these pitch classifications shown by the triangles were carried out on the final ambiguous pair of stimuli. I thought these were always presented at the edges of the range of other stimuli, so I do not follow how they have so many different pitchclass values on the x-axis here.</p></disp-quote><p>There were 4 Biases, centered at 0,3,6 or 9 semitones, and covering [-2.5,2.5]st relative to this center. Therefore the edges of the bias ranges (3st away from their centers) happen to be the same as the centers, e.g. for the Bias centered at 3, the ambiguous pair would be a 0-6 or 6-0 step. Therefore there are 4 locations for the ambiguous tones on the x-axis of Fig. 4D (previously 3D).</p><disp-quote content-type="editor-comment"><p>Figure 4: This demonstration of the ambiguity of Shepard pairs may be misleading. The actual musical interval is never ambiguous, as this figure suggests. Only the ascending vs descending percept is ambiguous. Therefore the predictions of the ferret A1 decoding (Fig 3D) and the model in Fig 5 are inconsistent with perception in two ways. One (which the authors mention) is the direction of the bias shift (up vs down). Another (not mentioned here) is that one never experiences a shift in the shepard tone at a fraction of a semitone - the musical note stays the same, and changes only in pitch height, not pitch chroma.</p></disp-quote><p>We are unsure of the reviewer’s direction with this question. In particular the second point is not clear to us: &quot;...one (who?) never (in this experiment? in real life?) experiences a bias shift in the Shepard tone at a fraction of a semitone&quot; (why is this relevant in the current experiment?). Pitch chrome would actually be a possible replacement for pitch class, but somehow, the previous Shepard tone literature has referred to it as pitch class.</p><disp-quote content-type="editor-comment"><p>P7 l12 - omit one 'consequently'</p></disp-quote><p>Changed to 'Therefore'.</p><disp-quote content-type="editor-comment"><p>P7 l24 - I encourage the authors to not use &quot;local&quot; and &quot;global&quot; without making it clear what space they refer to. One tends to automatically think of frequency space in the auditory system, but I think here they mean f0 space? What is a &quot;cell close to the location of the bias&quot;? Cells reside in the brain. The bias is in f0 space. The use of &quot;local&quot; and &quot;global&quot; throughout the manuscript is too vague.</p></disp-quote><p>Agreed, the reference here was actually to the cell's preferred pitch class, not its physical location (which one might arguably be able to disambiguate, given the context). We have changed the wording, and also checked the use of global/local throughout the manuscript. The main use of 'global/local' is now in reference to the range of adaptation, and is properly introduced on first mention.</p><disp-quote content-type="editor-comment"><p>P7 L26 -there is no Fig 5D1. Do you mean the left panel of 5D?</p></disp-quote><p>Thanks. Changed.</p><disp-quote content-type="editor-comment"><p>FigS3 is referred to a lot on p7-8. Should this be moved to the main text?</p></disp-quote><p>The main reason why we kept it in the supplement is that it is based on a more static model, which is intended to illustrate the consequences of different encoding schemes. In order to not confuse the reader about these two models, we prefer to keep it in the supplement, which - for an online journal - makes little difference since the reader can just jump ahead to this figure in the same way as any other figure.</p><disp-quote content-type="editor-comment"><p>Fig 5C, D - label x-axis.</p></disp-quote><p>Added.</p><disp-quote content-type="editor-comment"><p>Fig 5E - axis labels needed. I don't know what is plotted on x and y, and cannot see red and green lines in left plot</p></disp-quote><p>Thanks for noticing this, colors corrected, axes labeled.</p><disp-quote content-type="editor-comment"><p>Page 8 L3-15 - If I follow this correctly, I think the authors are confusing pitch and frequency here in a way that is fundamental to their model. They seem to equate tonotopic frequency tuning to pitch tuning, leading to confused implications of frequency adaptation on the F0 representation of complex sounds like Shepard tones. To my knowledge, the authors do not examine pure tone frequency tuning in their neurons in this study. Please clarify how you propose that frequency tuning like that shown in Fig 5A relates to representation of the F0 of Shepard tones. Or...are the authors suggesting these neural effects have little to do with pitch processing and instead are just the result of frequency tuning for a single harmonic of the Shepard tones?</p></disp-quote><p>We agree that it is not trivial to describe this well, while keeping the text uncluttered, in particular, because often tuning properties to stimulus frequency contribute to tuning properties of the same neuron for pitch class, although this can be more or less straightforward: specifically, for some narrowly tuned cells, the Shepard tuning is simply a reflection of their tuning to a single octave range of the constituent tones (see Fig. S1). For more broadly tuned cells, multiple constituent tones will contribute to the overall Shepard tuning, which can be additive, subtractive, or more complex. The assumption in our approach is that we can directly estimate the Shepard tuning to evaluate the consequence for the percept. While this may seem artificial, as Shepard tones do not typically occur in nature, the same argument could be made against pure tones, on which classical tuning curves and associated decodings are often based. Relating the Shepard tuning to the classical tuning would be an interesting study in itself, although arguably relating the tuning of one artificial stimulus to another. Regarding the terminology of pitch, pitch class and frequency: The term pitch class is commonly used in the field of Shepard tones, and - as we indicated in the beginning of the results: &quot;the term <italic>pitch</italic> is used interchangeably with <italic>pitch class</italic> as only Shepard tones are considered in this study&quot;. We agree that the term pitch, which describes the perceptual convergence/construction of a tone-height from a range of possible physical stimuli, needs to be separated from frequency as one contributor/basis for the perception of a pitch. However, we think that the term pitch can - despite its perceptual origin - also be associated with neuron/neural responses, in order to investigate the neural origin of the pitch percept. At the same time, the present study is not targeted to study pitch encoding per se, as this would require the use of a variety of stimuli leading to consistent pitch percepts. Therefore, pitch (class) is here mainly used as a term to describe the neural responses to Shepard tones, based on the previous literature, and the fact that Shepard tones are composite stimuli that lead to a pitch percept. The last sentence has been added to the manuscript for clarity.</p><disp-quote content-type="editor-comment"><p>P7-9: I wasn't left with a clear idea of how the model works from this text. I assume you have layers of neurons tuned to frequency or f0 (based on the real data?), which are connected in some way to produce some sort of output when you input a sound? More detail is needed here. How is the dynamic adaptation implemented?</p></disp-quote><p>The detailed description of the model can be found in the Methods section. We have gone through the corresponding paragraph and have tried to clarify the description of the model by introducing a high-level description and the reference to the corresponding Figure (Fig. 5A) in the Results.</p><disp-quote content-type="editor-comment"><p>Fig6A: Figure caption can't be correct. In any case, these equations cannot be understood unless you define the terms in them.</p></disp-quote><p>We have clarified the description in the caption.</p><disp-quote content-type="editor-comment"><p>Fig 6/directionality analysis: Assuming that the &quot;F&quot; in the STRFs here is Shepard tone f0, and not simple frequency?</p></disp-quote><p>We have changed the formula in the caption and the axis labels now.</p><disp-quote content-type="editor-comment"><p>Fig 6C - y-axis values</p></disp-quote><p>In the submission, these values were left out on purpose, as the result has an arbitrary scale, but only whether it is larger or smaller than 0 counts for the evaluation of the decoded directionality (at the current level of granularity). An interesting refinement would be to relate the decoded values to animal performance. We have now scaled the values arbitrarily to fit within [-1,1], but we would like to emphasize that only their relative scale matters here, not their absolute scale.</p><disp-quote content-type="editor-comment"><p>Fig 6E - can't both be abscissa (caption). I might be missing something here, but I don't see the &quot;two stripes&quot; in the data that are described in the caption.</p></disp-quote><p>Thank you. The typo is fixed. The stripes are most clearly visible in the right panel of Fig. 6E, red and blue, diagonally from top left to bottom right.</p><disp-quote content-type="editor-comment"><p>Fig 6G -I have no idea what this figure is illustrating.</p></disp-quote><p>This panel is described in the text as follows: &quot;The resulting distribution of activities in their relation to the Bias is, hence, symmetric around the Bias (Fig. 6G). Without prior stimulation, the population of cells is unadapted and thus exhibits balanced activity in response to a stimulus. After a sequence of stimuli, the population is partially adapted (Fig. 6G right), such that a subsequent stimulus now elicits an imbalanced activity. Translated concretely to the present paradigm, the Bias will locally adapt cells. The degree of adaptation will be stronger, if their tuning curve overlaps more with the biased region. Adaptation in this region should therefore most strongly influence a cell’s response. For example, if one considers two directional cells, an up- and a down-selective cell, cocentered in the same frequency location below the Bias, then the Bias will more strongly adapt the up-cell, which has its dominant, recent part of the SSTRF more inside the region of the Bias (Fig. 6G right). Consistent with the percept, this imbalance predicts the tone to be perceived as a descending step relative to the Bias. Conversely, for the second stimulus in the pair, located above the Bias, the down-selective cells will be more adapted, thus predicting an ascending step relative to the previous tone.&quot;</p><disp-quote content-type="editor-comment"><p>I might be just confused or losing steam at this point, but I do not follow what has been done or the results in Fig 6 and the accompanying text very well at all. Can this be explained more clearly? Perhaps the authors could show spike rate responses of an example up-direction and down-direction neuron? Explain how the decoder works, not just the results of it.</p></disp-quote><p>We agree that we are presenting something new here. However, it is conceptually not very different from decoding based on preferred frequencies. We have attempted to provide two illustrations of how the decoder works (Fig. 6A) and how it then leads to the percept using prototypical examples of cellular SSTRFs (Fig. 6G). We have added a complete, but accessible description to the Methods section. Showing firing rates of neurons would unfortunately not be very telling, given the usual variability in neural response and the fact that our paradigm did not have a lot of repetitions (but instead a lot of conditions), which would be able to average out the variability on a single neuron level.</p><disp-quote content-type="editor-comment"><p>Discussion - I do not feel I can adequately critique the author's interpretation of the results until I understand their results and methods better. I will therefore save my critique of the discussion section for the next round of revisions after they have addressed the above issues of disorganization and clarity in the manuscript.</p></disp-quote><p>We hope that the updated version of the manuscript provides the reviewer now with this possibility.</p><disp-quote content-type="editor-comment"><p>Methods</p><p>P15L7 - gender of human subjects? Age distribution? Age of ferrets?</p></disp-quote><p>We have added this information.</p><disp-quote content-type="editor-comment"><p>P16L21 - What is the justification for randomizing the phase of the constituent frequencies?</p></disp-quote><p>The purpose of the randomization was to prevent idiosyncratic phase relationships for particular Shepard tones, which would depend in an orderly fashion on the included base-frequencies if non-randomized, and could have contributed to shaping the percept for each Shepard tone in a way that was only partly determined by the pitch class of the Shepard tone. Added to the section.</p><disp-quote content-type="editor-comment"><p>P17L6 - what are the 2 randomizations? What is being randomized?</p></disp-quote><p>Pitch classes and position in the Bias sequence. Added to the section.</p><disp-quote content-type="editor-comment"><p>P16 Shepard Tuning section - What were the durations of the tones and the time between tones within a trial?</p></disp-quote><p>Thanks, added!</p><disp-quote content-type="editor-comment"><p>Equations - several undefined terms in the equations throughout the manuscript.</p></disp-quote><p>Thanks. We have gone through the manuscript and all equations and have introduced additional definitions where they had been missing.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>P3L10: &quot;passive&quot; and &quot;active&quot; conditions come totally out of the blue. Need introducing first. (Or cut. If adaptation is always seen, why mention the two conditions if the difference is not relevant here?)</p></disp-quote><p>We have added an additional sentence in the preceding paragraph, that should clarify this. The reason for mentioning it is that otherwise a possible counter-argument could be made that adaptation does not occur in the active condition, which was not tested in ferrets (but presents an interesting avenue for future research).</p><disp-quote content-type="editor-comment"><p>P3L14 &quot;siple&quot; typo</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>P4L1 &quot;behaving humans&quot; you should elaborate just a little here on what sort of behavior the participants engaged in.</p></disp-quote><p>Thanks for pointing this out. We have clarified this by adding an additional sentence directly thereafter.</p><disp-quote content-type="editor-comment"><p>P4 adaptation: I wonder whether it would be useful to describe the Bias condition a bit more here before going into the observations. The reader cannot know what to expect unless they jump ahead to get a sense of what the Bias looks like in the sense of how many stimuli are in it, and how similar they are to each other. Observations such as &quot;the average response strength decreases as a function of the position in the Bias sequence&quot; are entirely expected if the Bias is made up of highly repetitive material, but less expected if it is not. I appreciate that it can be awkward to have Methods after Results, but with a format like that, the broad brushstroke Methods should really be incorporated into the Results and only the tedious details should be reserved for the Methods to avoid readers having to jump back and forth.</p></disp-quote><p>Agreed, we have inserted a corresponding description before going into the details of the results.</p><disp-quote content-type="editor-comment"><p>Related to this (perhaps): Bottom of P4, top of P5: &quot;significantly less reduced (33%, p=0.0011, 2 group t-test) compared to within the bias (Fig. 2 A3, blue vs. red), relative to the first responses of the bias&quot; ... I am at a loss as to what the red and blue symbols in Fig 2 A3 really show, and I wonder whether the &quot;at the edges&quot; to &quot;within the Bias&quot; comparison were to make sense if at this stage I had been told more about the composition of the Bias sequence. Do the ambiguous ('target') tones also occur within the Bias? As I am unclear about what is compared against what I am also not sure how sound that comparison is.</p></disp-quote><p>We have added an extended description of the Bias to the beginning of this section of the manuscript. For your reference: the Shepard tones that made up the ambiguous tones were not part of the Bias sequence, as they are located at 3st distance from the center of the Bias (above and below), while the Bias has a range of only +/- 2.5st.</p><disp-quote content-type="editor-comment"><p>Fig 2: A4 B1 B2 labels should be B1 B2 B3</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>Fig 2 A2, A3: consider adjusting y-axis range to have less empty space above the data. In A3 in particular, the &quot;interesting bit&quot; is quite compressed.</p></disp-quote><p>Done, however, while still matching the axes of A2 and A3 for better comparability.</p><disp-quote content-type="editor-comment"><p>I am under the strong impression that the human data only made it into Fig 2 and that the data from Fig 3 onwards are animal data only. That is of course fine MEG may not give responses that are differentiated enough to perform the sort of analyses shown in the later figures. But I do think that somewhere this should be explicitly stated.</p></disp-quote><p>Yes, the reviewer's observation is correct. The decoding analyses could not be conducted on the human MEG data and was therefore not further pursued. Its inclusion in the paper has the purpose of demonstrating that even in humans and active conditions, the local adaptation is present, which is a key contributor to the two decoding models. We now state this explicitly when starting the decoding analysis.</p><disp-quote content-type="editor-comment"><p>P5L2 &quot;bias&quot; not capitalized. Be consistent.</p></disp-quote><p>All changed to capitalized.</p><disp-quote content-type="editor-comment"><p>P5L8 reference to Fig 2 A4: something is amiss here. From legend of Fig 2 it seems clear that panel A4 label is mislabeled B1. Maybe some panels are missing to show recovery rates?</p></disp-quote><p>Apologies for this residual text from a previous version of the manuscript. We have gone through all references and corrected them.</p><disp-quote content-type="editor-comment"><p>P6L7 comma after &quot;decoding&quot;.</p></disp-quote><p>Changed.</p><disp-quote content-type="editor-comment"><p>Fig 3, I like this analysis. What would be useful / needed here though is a little bit more information about how the data were preprocessed and pooled over animals. Did you do the PCA separately for each animal, then combine, or pool all units into a big matrix that went into the PCA? What about repeat, presentations? Was every trial a row in the matrix, or was there some averaging over repeats? (In fact, were there repeats??)</p></disp-quote><p>Thanks for bringing up these relevant aspects, which were partly insufficiently detailed in the manuscript. Briefly, cells were pooled across animals and we only used cells that could meaningfully contribute to the decoding analysis, i.e. had auditory responses and different responses to different Shepard tones. Regarding the responses, as stated in the Methods, &quot;Each stimulus was repeated 10 times&quot;, and we computed average responses across these repetitions. Single trials were not analyzed separately. We have added this information in the Methods, and refer to it in the Results.</p><disp-quote content-type="editor-comment"><p>Also, there doesn't appear to be a preselection of units. We would not necessarily expect all cortical neurons to have a meaningful &quot;best pitch&quot; as they may be coding for things other than pitch. Intuitively I suspect that, perhaps, the PCA may take care of that by simply not assigning much weight to units that don't contribute much to explained variance? In any event I think it should be possible, and would be of some interest, to pull out of this dataset some descriptive statistics on what proportion of units actually &quot;care about pitch&quot; in that they have a lot (or at least significantly more than zero) of response variance explained by pitch. Would it make sense to show a distribution of %VE by pitch? Would it make sense to only perform the analysis in Fig 3 on units that meet some criterion? Doing so is unlikely to change the conclusion, but I think it may be useful for other scientists who may want to build on this work to get a sense of how much VE_pitch to expect.</p></disp-quote><p>We fully agree with the reviewer, which is why this information is already presented in Supplementary Fig.1, which details the tuning properties of the recorded neurons. Overall, we recorded from 1467 neurons across all ferrets, out of which 662 were selected for the decoding analysis based on their driven firing rate (i.e. whether they responded significantly to auditory stimulation) and whether they showed a differential response to different Shepard tones The thresholds for auditory response and tuning to Shepard tones were not very critical: setting the threshold low, led to quantitatively the same result, however, with more noise. Setting the thresholds very high, reduced the set of cells included in the analysis, and eventually that made the results less stable, as the cells did not cover the entire range of preferences to Shepard tones. We agree that the PCA based preprocessing would also automatically exclude many of the cells that were already excluded with the more concrete criteria beforehand. We have added further information on this issue in the Methods section under the heading 'Unit selection'.</p><disp-quote content-type="editor-comment"><p>P9 &quot;tones This&quot; missing period.</p></disp-quote><p>Changed.</p><disp-quote content-type="editor-comment"><p>P10L17 comma after &quot;analysis&quot;</p></disp-quote><p>Changed.</p></body></sub-article></article>