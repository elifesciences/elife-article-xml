<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">57977</article-id><article-id pub-id-type="doi">10.7554/eLife.57977</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Momentary subjective well-being depends on learning and not reward</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-90031"><name><surname>Blain</surname><given-names>Bastien</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7735-6043</contrib-id><email>b.blain@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-87818"><name><surname>Rutledge</surname><given-names>Robb B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7337-5039</contrib-id><email>robb.rutledge@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Wellcome Centre for Human Neuroimaging, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Department of Psychology, Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Lee</surname><given-names>Daeyeol</given-names></name><role>Reviewing Editor</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>11</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e57977</elocation-id><history><date date-type="received" iso-8601-date="2020-04-17"><day>17</day><month>04</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-11-16"><day>16</day><month>11</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Blain and Rutledge</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Blain and Rutledge</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-57977-v3.pdf"/><abstract><p>Subjective well-being or happiness is often associated with wealth. Recent studies suggest that momentary happiness is associated with reward prediction error, the difference between experienced and predicted reward, a key component of adaptive behaviour. We tested subjects in a reinforcement learning task in which reward size and probability were uncorrelated, allowing us to dissociate between the contributions of reward and learning to happiness. Using computational modelling, we found convergent evidence across stable and volatile learning tasks that happiness, like behaviour, is sensitive to learning-relevant variables (i.e. probability prediction error). Unlike behaviour, happiness is not sensitive to learning-irrelevant variables (i.e. reward prediction error). Increasing volatility reduces how many past trials influence behaviour but not happiness. Finally, depressive symptoms reduce happiness more in volatile than stable environments. Our results suggest that how we learn about our world may be more important for how we feel than the rewards we actually receive.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Many people believe they would be happier if only they had more money. And events such as winning the lottery or receiving a large pay rise do make people happy, at least temporarily. But recent studies suggest that the main factor driving happiness on such occasions is not the size of the reward received. Instead, it is how well that reward matches up with expectations. Receiving a 10% pay rise when you were expecting 1% will make you feel happier than receiving 10% when you had been expecting 20%.</p><p>This difference between an expected and an actual reward is referred to as a reward prediction error. Reward prediction errors have a key role in learning. They motivate people to repeat behaviours that led to unexpectedly large rewards. But they also enable people to update their beliefs about the world, which is rewarding in itself. Could it be that reward prediction errors are associated with happiness mainly because they help us understand the world a little better than before?</p><p>To test this idea, Blain and Rutledge designed a task in which the likelihood of receiving a reward was unrelated to the size of the reward. This study design makes it possible to separate out the contributions of learning versus reward to moment-by-moment happiness.</p><p>In the task, volunteers had to decide which of two cars would win a race. In the ‘stable’ condition, one of the cars always had an 80% chance of winning. In the ‘volatile’ condition, one car had an 80% chance of winning for the first 20 trials. The other car then had an 80% chance of winning for the next 20 trials. The volunteers were not told these probabilities in advance, but had to work them out by playing the game. However, on every trial, the volunteers were shown the reward they would receive if they chose either of the cars and that car went on to win. The size of the rewards varied at random and was unrelated to the likelihood of a car winning.</p><p>Every few trials, the volunteers were asked to indicate their current level of happiness on a scale. The results showed that volunteers were happier after winning than after losing. On average they were also happier in the stable condition than in the volatile condition. This was especially true for volunteers with pre-existing symptoms of depression. Moreover, happiness after wins did not depend on how large the reward they got was, but instead simply on how surprised they were to win.</p><p>These results suggest that how we learn about the world around us can be more important for how we feel than rewards we receive directly. Measuring happiness in various types of environment could help us understand factors affecting mental health. The current results suggest, for example, that uncertain environments may be especially unpleasant for people with depression. Further research is needed to understand why this might be the case. In the real world, rewards are often uncertain and infrequent, but learning may nevertheless have the potential to boost happiness.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>subjective well-being</kwd><kwd>happiness</kwd><kwd>dopamine</kwd><kwd>prediction error</kwd><kwd>learning</kwd><kwd>decision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/N02401X/1</award-id><principal-award-recipient><name><surname>Rutledge</surname><given-names>Robb B</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000874</institution-id><institution>Brain and Behavior Research Foundation</institution></institution-wrap></funding-source><award-id>Young Investigator Grant (27674)</award-id><principal-award-recipient><name><surname>Rutledge</surname><given-names>Robb B</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>NIMH</institution></institution-wrap></funding-source><award-id>1R01MH124110</award-id><principal-award-recipient><name><surname>Rutledge</surname><given-names>Robb B</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max Planck Society</institution></institution-wrap></funding-source><award-id>Centre Grant</award-id><principal-award-recipient><name><surname>Rutledge</surname><given-names>Robb B</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>203147/Z/16/Z</award-id><principal-award-recipient><name><surname>Rutledge</surname><given-names>Robb B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Analyses show that happiness is not associated with the reward prediction errors resulting from decisions but instead with belief updating and learning the structure of the world.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Decisions are guided by beliefs about states of the world. Some states are directly observable, like the potential prize for a bet. Other states, like the probability of winning, may not be directly observable but can be inferred from past events. Thus, learning from experience is essential for adaptive behaviour. In the standard theoretical framework, learning is driven by how unexpected the outcome is (i.e. by the prediction error): the difference between outcome and prediction (<xref ref-type="bibr" rid="bib1">Barto, 1995</xref>). Sensitivity to the prediction error (i.e. the learning rate) flexibly adapts to environmental statistics. The decisions of both humans and non-human primates are consistent with a higher learning rate in more volatile environments, and subjects are more likely to stay on the same option after positive compared to negative feedback when reward probabilities change more frequently (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Mathys et al., 2011</xref>).</p><p>Emotions are widely believed to play a role in adaptive behaviour (<xref ref-type="bibr" rid="bib24">Fredrickson, 2004</xref>), but no computational framework exists to link them. Unexpected outcomes influence affective states, so that bad outcomes feel worse when unexpected than when expected, and good outcomes feel better when unexpected than when expected (<xref ref-type="bibr" rid="bib37">Mellers et al., 1997</xref>; <xref ref-type="bibr" rid="bib52">Shepperd and Mcnulty, 2002</xref>). It has recently been shown that reward expectations and reward prediction errors (RPEs), the difference between experienced and predicted rewards, can explain changes in affective state in the context of decision-making under uncertainty when learning is not required (<xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>). A number of studies have found results consistent with the idea that happiness is modulated by past prediction errors (<xref ref-type="bibr" rid="bib40">Otto et al., 2016</xref>) including a recent report showing in students that prediction errors due to exam performance influence real-world emotions (<xref ref-type="bibr" rid="bib58">Villano et al., 2020</xref>). Mood has been proposed to represent environmental momentum, whether an environment is getting better or worse, which could be a useful variable for adaptive behaviour (<xref ref-type="bibr" rid="bib20">Eldar et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Eldar and Niv, 2015</xref>).</p><p>Impairments in reward and emotion processing are associated with depression (<xref ref-type="bibr" rid="bib52">Shepperd and Mcnulty, 2002</xref>). Learning in depression has been extensively studied, but there is not consistent evidence for a specific deficit (<xref ref-type="bibr" rid="bib7">Blanco et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Cella et al., 2010</xref>; <xref ref-type="bibr" rid="bib14">Chase et al., 2010</xref>; <xref ref-type="bibr" rid="bib25">Gillan et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Herzallah et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Kunisato et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Mueller et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Pechtel et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Robinson et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Taylor Tavares et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">Thoma et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Vrieze et al., 2013</xref>), as reviewed in <xref ref-type="bibr" rid="bib29">Huys et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Scholl and Klein-Flügge, 2018</xref>. The ability of individuals to appropriately adjust learning rates to environmental volatility is associated with anxiety symptoms (<xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>). Individuals with high trait anxiety show reduced ability to adjust updating of outcome expectancies for aversive outcomes to the volatility of the environment compared to individuals with low trait anxiety. Failure to appropriately adjust learning to environmental volatility has not been established in depression. Affective states reflect subjective estimates of uncertainty, which predict the dynamics of subjective and physiological stress responses (<xref ref-type="bibr" rid="bib18">de Berker et al., 2016</xref>). Overall mood during risky decision-making tasks is reduced with increasing depression severity, both in the laboratory and using remote smartphone-based data collection (<xref ref-type="bibr" rid="bib48">Rutledge et al., 2017</xref>).</p><p>Here, our goal was to quantify the relationship between mood and adaptive behaviour in two common reinforcement learning tasks (<xref ref-type="fig" rid="fig1">Figure 1</xref>): one in which reward probabilities do not change (stable) and one in which reward probabilities periodically change (volatile). We addressed the following questions: (1) Is mood more sensitive to learning-relevant or learning-irrelevant variables in established reinforcement learning models? (2) Do mood dynamics adjust to environmental volatility? (3) Are mood dynamics affected by depression in the context of learning?</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design.</title><p>Subjects (n = 75) performed a one-armed bandit reinforcement learning task, choosing repeatedly between two cars. They were instructed to maximise their cumulative points. In the stable task (80 trials), the probability to win for the best car was 80%. In the volatile task (80 trials), reward probabilities switched between 80% for one car and 80% for the other car every 20 trials. Task order was counterbalanced across subjects (see Materials and methods). The reward available for each car was randomly determined on each trial and unrelated to the probability of winning. Every three to four trials, subjects were asked to report ‘How happy are you right now?’ by moving a cursor on a line. Each trial started with a fixation symbol in the centre of the screen. Then, the stimuli were displayed but choice was not permitted. The potential reward for each car was then displayed and participants were free to choose an option without any time constraints. The chosen option was outlined by a yellow frame. Finally, the outcome was displayed. Both the car and the reward magnitude frames were green if the chosen car won the race (example shown). The car frame was red and crossed out if the chosen car lost (example shown in inset).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig1-v3.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><p>To investigate the relationship between mood dynamics and adaptive behaviour, we adapted a task design employed in previous studies (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>). Participants repeatedly made decisions between two cars racing against each other (corresponding to a one-armed bandit because only one car can win each race). The reward magnitude associated with each car was explicit and assigned randomly on each trial, whereas the outcome probability was implicit and had to be inferred from the outcomes of previous trials. Participants (n = 75) completed two learning tasks with a break in between tasks (counterbalanced across participants). In the stable environment, the ‘best’ car won 80% of the races on average. In the volatile environment, the ‘best’ car had an 80% probability of winning and the ‘best’ car switched every 20 trials (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Participants were asked to report their current happiness after every three to four trials. Unlike in previous studies in humans using a similar task design (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>), participants were informed that they would complete stable and volatile tasks. Which environment they were in was indicated at the start of each task. Subjects were given no guidance as to how they should use this information, although we expected this manipulation to increase the difference in behavioural sensitivity between environments relative to previous studies.</p><sec id="s2-1"><title>Learning rates change with environmental volatility</title><p>Participants chose the option with the higher expected value (i.e. choice accuracy) more often than chance (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>) in the stable environment (82.1 ± 1.1% (mean ± SEM), z = 7.5, p &lt; 10<sup>−13</sup>) and in the volatile environment (73.3 ± 1.0%, z = 7.5, p &lt; 10<sup>−13</sup>). Participants chose the higher probability option more often than chance in the stable environment (80.8 ± 1.0%, mean ± SEM, z = 7.5, p &lt; 10<sup>−13</sup>) and in the volatile environment (62.6 ± 1.1%, z = 7.2, p &lt; 10<sup>−12</sup>). To ensure that participants incorporated information about the magnitudes of potential rewards into their decisions, we considered only trials where the car with the lower outcome probability had the higher expected value. Subjects chose the low probability car in these trials more often than chance (stable: 61.2 ± 3.0%, z = 3.5, p &lt; 0.001; volatile: 75.6 ± 2.7%, z = 6.4, p &lt; 10<sup>−9</sup>). These results are consistent with participants integrating both probability and reward to make their decisions.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Learning rate adapts to environmental volatility.</title><p>(<bold>A</bold>) Participants chose the option with the highest expected value 82% of the time in the stable environment (blue curve, left panel) and 73% of the time in the volatile environment (orange curve, right panel). The additive model containing three parameters (a learning rate determining the sensitivity to prediction error, an inverse temperature reflecting choice stochasticity, and a relative weight for probability and reward magnitude in choice) fitted choice data well (black dashed lines) in the stable environment (mean pseudo-r<sup>2</sup> = 0.62) and the volatile environment (mean pseudo-r<sup>2</sup> = 0.45). (<bold>B</bold>) Participants chose more often the option with the higher probability in the stable environment compared to the volatile environment. Critically, participants stayed on the same option more often if choosing that option resulted in the car winning (light orange) compared to the car losing (dark orange) in the volatile environment compared to the stable environment (light blue and dark blue represent staying after winning and losing, respectively). This suggests that participant behaviour was more sensitive to feedback in the volatile than stable environment, as an agent with a higher learning rate would be. Additive model predictions show a similar difference in feedback sensitivity across environments (purple). (<bold>C</bold>) Learning rates were higher in the volatile environment (orange) compared to the stable environment (blue). This was true for participants completed the stable learning task before (stable 1) or after (stable 2) the volatile learning task. Error bars represent SEM. *p &lt; 0.05, ***p &lt; 0.001.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig2-v3.tif"/></fig><p>Multiplicative and additive models that integrate probability and reward in different ways have been widely used to explain behaviour across stable and volatile environments (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>). Both types of models include the same learning component for updating the probability estimate for each car to win (<xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2)</xref>, based on the probability prediction error (PPE), the difference between the outcome (0 or 1) and the estimated probability of winning:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where P<sub>car1 wins</sub>(t) is the estimated probability for car 1 winning on trial t, and α is the learning rate. For choices to car 2, a similar equation applies. Multiplicative and additive models differ regarding implementation of choice predictions with probability and reward magnitude either integrated multiplicatively (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>) or additively (<xref ref-type="bibr" rid="bib19">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib22">Farashahi et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>).</p><p>The multiplicative selector resembles the maximisation of expected utility common to economic decision models (<xref ref-type="bibr" rid="bib30">Kahneman and Tversky, 1979</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where η is a free parameter related to the level of risk aversion and β is the inverse temperature (i.e. choice stochasticity or precision), and <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the reward magnitude if car 1 is chosen and wins. The multiplicative model captured choice data in both stable (pseudo-r<sup>2</sup> = 0.54 ± 0.02, mean ± SEM) and volatile environments (pseudo-r<sup>2</sup> = 0.41 ± 0.02).</p><p>In contrast, the additive selector is implemented as follows:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where ∆Probability corresponds to the difference in estimated probability between the options and ∆Reward corresponds to the difference in normalised reward magnitude between the options, and ϕ represents the relative weight of probability and magnitude on choice. The additive model captured choice data in the stable (pseudo-r<sup>2</sup> = 0.62 ± 0.02) and volatile environments (pseudo-r<sup>2</sup> = 0.45 ± 0.02, see <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>Model comparison demonstrated that the additive model better explained choice data with the same number of parameters as the multiplicative model in both stable (∆BIC = 690) and volatile environment (∆BIC = 321, see <xref ref-type="table" rid="table1">Table 1</xref>). Our results are consistent with similar findings obtained in highly trained non-human primates using a task design in which changes between stable and volatile environments were signalled (<xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>) as in the present study.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Choice model comparison results.</title><p>The ‘Additive’ model refers to a model implementing a weighted sum of probability difference and reward magnitude difference when making decisions (<xref ref-type="bibr" rid="bib19">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib23">Farashahi et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Farashahi et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Rouault et al., 2019</xref>). The ‘Multiplicative’ model refers to the model first used to describe behaviour in this task, which integrates reward and probability information multiplicatively (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>). The ‘Probability only’ model includes only the probability component of the additive model and the ‘Magnitude only’ model includes only the magnitude component of the additive model. ∆BIC refers to the Bayesian Information Criterion computed for each model compared to the additive model, the preferred model in both stable and volatile environments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Model</th><th valign="bottom">Number of parameters</th><th valign="bottom">Stable pseudo-r<sup>2</sup></th><th valign="bottom">Volatile pseudo-r<sup>2</sup></th><th valign="bottom">Stable BIC</th><th valign="bottom">Volatile BIC</th><th valign="bottom">Stable ΔBIC</th><th valign="bottom">Volatile ΔBIC</th></tr></thead><tbody><tr><td valign="bottom">Additive</td><td valign="bottom">3</td><td valign="bottom">0.62</td><td valign="bottom">0.45</td><td valign="bottom">4134</td><td valign="bottom">5605</td><td valign="bottom">0</td><td valign="bottom">0</td></tr><tr><td valign="bottom">Multiplicative</td><td valign="bottom">3</td><td valign="bottom">0.54</td><td valign="bottom">0.41</td><td valign="bottom">4824</td><td valign="bottom">5926</td><td valign="bottom">690</td><td valign="bottom">321</td></tr><tr><td valign="bottom">Probability only</td><td valign="bottom">2</td><td valign="bottom">0.35</td><td valign="bottom">0.16</td><td valign="bottom">6104</td><td valign="bottom">7635</td><td valign="bottom">1970</td><td valign="bottom">2030</td></tr><tr><td valign="bottom">Magnitude only</td><td valign="bottom">1</td><td valign="bottom">0.14</td><td valign="bottom">0.23</td><td valign="bottom">7473</td><td valign="bottom">6739</td><td valign="bottom">3338</td><td valign="bottom">1134</td></tr></tbody></table></table-wrap><p>We next examined whether model fits were consistent with subjects integrating both potential reward magnitudes and probabilities to make decisions. The relative weight of probability and reward magnitude in the additive model (ϕ) was balanced on average (stable: ϕ = 0.57 ± 0.017 [mean ± SEM]; volatile: ϕ = 0.44 ± 0.027), suggesting that subjects integrated both probabilities and reward magnitudes to make decisions. Omitting ϕ and evaluating simpler models that considered only probabilities (α and β) or reward magnitudes (β only) resulted in worse fits (see <xref ref-type="table" rid="table1">Table 1</xref>). Lower BIC for additive and multiplicate models compared to the simpler models confirmed that subjects considered both probability and reward magnitude when making decisions.</p><p>We then asked whether greater environmental volatility was associated with higher learning rates as observed in previous studies (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>). A simple prediction for standard reinforcement learning models is that subjects should stay more on the same option after winning than losing (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Subjects did not stay more on the same option after winning than losing for the stable environment (difference in choice proportion, high probability car: 4.1 ± 1.9%, mean ± SEM, z = 1.7, p = 0.098; low probability car: 1.3 ± 3.2%, z = -0.43, p = 0.67). Subjects stayed on the same option more after winning than losing in the volatile environment (difference in choice proportion, high probability car: 22.1 ± 2.3%, z = 6.6, p &lt; 10<sup>−10</sup>, low probability car: 21.4 ± 3.7%, z = 4.8, p &lt; 10<sup>−5</sup>). Subjects stayed on the same option after winning compared to losing more in volatile than stable environments (difference volatile – stable, high probability car: 18.1 ± 2.7%, z = 5.7, p &lt; 10<sup>−7</sup>, low probability car: 19.8 ± 4.1%, z = 4.0, p &lt; 10<sup>−4</sup>; see <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>We then checked that the predictions generated by a reinforcement learning model fit separately to each environment correspond to observed behavioural patterns described above in model-independent analyses (predicted difference in choice proportion after winning and losing in stable [high probability car: 8.8 ± 1.6%, z = 5.2, p &lt; 10<sup>−6</sup>, low probability car: 6.1 ± 2.6%, z = 1.5, p = 0.12] and volatile environments [high probability car: 21.5 ± 2.2%, z = 6.9, p &lt; 10<sup>−11</sup>, low probability car: 21.1 ± 2.7%, z = 5.9, p &lt; 10<sup>−8</sup>; see <xref ref-type="fig" rid="fig2">Figure 2B</xref>]). The model predictions were able to capture observed differences in behaviour following wins and losses and also the difference in in choice proportion after winning and losing between volatile and stable environments (high probability car: 12.8 ± 2.4%, z = 4.6, p &lt; 10<sup>−5</sup>; low probability car: 14.3 ± 3.6, z = 3.4, p &lt; 0.001). We found that learning rates (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) were substantially higher in volatile than stable environments (stable α = 0.16 ± 0.02, mean ± SEM; volatile α = 0.47 ± 0.03; difference volatile – stable: 0.31 ± 0.03, z = 6.9, p &lt; 10<sup>−11</sup>). Overall, these results demonstrate that the learning rate increases substantially in the volatile compared to the stable environment, in line with previous studies (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>).</p></sec><sec id="s2-2"><title>Happiness is more sensitive to learning-relevant than learning-irrelevant variables</title><p>We next examined how happiness changes over time during the tasks. Subjects varied their happiness ratings in both the stable (SD = 24.2 ± 1.1, mean ± SEM) and volatile (SD = 25.0 ± 1.1) environments. They were happier on average after winning than after losing (stable: 63.8 ± 1.9 vs 34.5 ± 2.0, z = 7.5, p &lt; 10<sup>−13</sup>; volatile: 61.9 ± 1.8 vs 33.6 ± 2.0, z = 7.5, p &lt; 10<sup>−13</sup>, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Participants were happier on average in the stable environment than in the volatile environment (stable: 55.0 ± 1.7, volatile: 49.5 ± 1.6, z = 3.7, p &lt; 0.001). This effect may be at least partly due to lower choice accuracy in volatile environments, and the difference in average happiness between environments was correlated between participants with the difference in choice accuracy in terms of EV maximisation (Spearman’s ρ(73) = 0.24, p &lt; 0.05).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Happiness is associated with probability and probability prediction error.</title><p>(<bold>A</bold>) Most participants were happier when their chosen car won compared to when their chosen car lost (97% of participants in the stable environment, 96% in the volatile environment, in the left and right panel, respectively). (<bold>B</bold>) Momentary happiness was best explained by a model (black dotted lines) including both the chosen probability estimate and the probability prediction error (PPE) derived from the additive choice model in addition to a forgetting factor and a baseline mood parameter, for both the stable (mean r<sup>2</sup> = 0.58) and the volatile (mean r<sup>2</sup> = 0.62) environments. Happiness ratings were z-scored for individual participants before model fitting. The shaded areas represent SEM. (<bold>C</bold>) The chosen probability (denoted P) and the PPE parameters were significantly different from 0 for both environments. Both variables are significantly associated with changes in affective state over time. PPE weight was significantly higher than P weight in both the stable and volatile environments. See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> related to the win loss model parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Loss weights on happiness are greater than win weights.</title><p>The win and the loss parameters were significantly different from 0 for both environments. The weight for loss was higher than the weight for win. Both variables are significantly associated with changes in affective state over time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig3-figsupp1-v3.tif"/></fig></fig-group><p>Previous studies have reported that momentary happiness in response to outcomes in a probabilistic reward task were explained by recent RPEs when maximising cumulative reward does not require learning (<xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>). In our task, maximising cumulative reward requires learning the outcome probability. In this context, PPEs (depending on whether the outcome was a win or a loss and the subjective probability of that outcome) are relevant to learning but RPEs (depending on the magnitude of the reward received and the expected value of the chosen option) are not relevant to learning or future behaviour. Reward information was choice relevant and choices were driven by the (additive) integration of the estimated outcome probability and the magnitude of potential rewards. Therefore, we tested whether happiness was more strongly associated with the PPEs used for learning or alternatively by RPEs that incorporate learning-irrelevant reward magnitudes. We compared two models which both use the subjective probability as estimated in the additive choice model to compute prediction errors:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> refers to the probability prediction error (PPE), defined as the difference between the outcome (one for win, 0 for loss) and the subjective probability estimated from the additive choice model, w<sub>0</sub> is a constant term, w<sub>PPE</sub> is a weight capturing the influence of past PPEs, and 0 ≤ γ ≤1 is a forgetting factor that makes events in more recent trials more influential than those in earlier trials;<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the difference between reward magnitude and the expected value of the chosen option computed based on the subjective probability estimated from the additive choice model. Reward magnitudes were rescaled from 0 to 1.</p><p>Mood fluctuations were better explained by a model including past PPEs than by a model including past RPEs, both in the stable (BIC<sub>PP̂E</sub> = −698, BIC<sub>RP̂E</sub> = −299, ∆BIC = 399) and volatile (BIC<sub>PP̂E</sub> = −559, BIC<sub>RP̂E</sub> = −319, ∆BIC = 240) environments (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig4">Figure 4A</xref>). This result holds for a broader model space including other definitions of the prediction error terms (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Happiness is more strongly associated with learning than choice.</title><p>(<bold>A</bold>) Comparison between the <inline-formula><mml:math id="inf4"><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for the happiness model including a PPE term (denoted <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) estimated in the additive choice model (y axis) and the r<sup>2</sup> for the happiness model including an RPE term instead (denoted <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). Both models had the same number of parameters. The <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model accounted for more variance in mood ratings on average in both stable (blue) and volatile (orange) learning tasks. Dots above the dashed line correspond to subjects for whom more variance in happiness is explained by the <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> compared to the <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model. (<bold>B</bold>) The <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model including the chosen estimated probability (denoted <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and estimated from the additive choice model) better explained happiness ratings than a <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model including expected value (denoted <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) for both the stable (blue) and volatile (orange) environments with both models having the same number of parameters. Dots above the dashed line correspond to subjects where more variance in happiness is explained by the <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> compared to the <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model. See <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for the estimated model frequency or each model and <xref ref-type="table" rid="table2">Table 2</xref> for other model comparison metrics.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Estimated model frequency.</title><p>Error bars correspond to the estimated standard deviation. Exceedance probability (<bold>A</bold>) stable: EP<sub>PP̂E</sub> = 0.87, EP<sub>PPEmodels</sub> = 1.0, volatile: EP<sub>PP̂E</sub> = 0.81, EP<sub>PPEmodels</sub> = 1.0; (<bold>B</bold>) stable: EP<sub>P̂+PP̂E</sub> = 0.97, volatile: EP<sub>P̂+PP̂E </sub>= 1.0; (<bold>C</bold>) stable: EP<sub>P̂+PP̂E</sub> = 1.0, volatile: EP<sub>P̂+PP̂E</sub> = 0.48.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig4-figsupp1-v3.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Happiness model comparison results.</title><p>PPE is probability prediction error, RPE is reward prediction error, P is the probability estimate, EV is the expected value, R is reward, <inline-formula><mml:math id="inf16"><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is the reward average, and RP is a free parameter corresponding to the reference point above and below which happiness would increase or decrease. The hat over a variable indicates that it incorporates trial-by-trial choice probability estimated from the additive choice model. ∆BIC refers to the comparison of the model scores using the Bayesian Information Criterion (BIC) compared to the <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model. Happiness ratings were z-scored within individuals and all models included a constant term and a forgetting factor γ in addition to the parameters indicated.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Model</th><th valign="bottom">Number of parameters</th><th valign="bottom">Stable mean r<sup>2</sup></th><th valign="bottom">Volatile mean r<sup>2</sup></th><th valign="bottom">Stable BIC</th><th valign="bottom">Volatile BIC</th><th valign="bottom">Stable ΔBIC</th><th valign="bottom">Volatile ΔBIC</th></tr></thead><tbody><tr><td><inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td valign="bottom">3</td><td valign="bottom">0.50</td><td valign="bottom">0.44</td><td valign="bottom">−698</td><td valign="bottom">−559</td><td valign="bottom">184</td><td valign="bottom">587</td></tr><tr><td><inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td valign="bottom">3</td><td valign="bottom">0.38</td><td valign="bottom">0.39</td><td valign="bottom">−299</td><td valign="bottom">−319</td><td valign="bottom">583</td><td valign="bottom">826</td></tr><tr><td valign="bottom">PPE</td><td valign="bottom">3</td><td valign="bottom">0.48</td><td valign="bottom">0.42</td><td valign="bottom">−640</td><td valign="bottom">−436</td><td valign="bottom">242</td><td valign="bottom">710</td></tr><tr><td valign="bottom">RPE</td><td valign="bottom">3</td><td valign="bottom">0.36</td><td valign="bottom">0.36</td><td valign="bottom">−223</td><td valign="bottom">−212</td><td valign="bottom">658</td><td valign="bottom">934</td></tr><tr><td><inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td valign="bottom">4</td><td valign="bottom">0.58</td><td valign="bottom">0.62</td><td valign="bottom">−882</td><td valign="bottom">−1146</td><td valign="bottom">0</td><td valign="bottom">0</td></tr><tr><td><inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td valign="bottom">4</td><td valign="bottom">0.56</td><td valign="bottom">0.53</td><td valign="bottom">−752</td><td valign="bottom">−691</td><td valign="bottom">130</td><td valign="bottom">454</td></tr><tr><td><inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td valign="bottom">4</td><td valign="bottom">0.43</td><td valign="bottom">0.48</td><td valign="bottom">−224</td><td valign="bottom">−370</td><td valign="bottom">657</td><td valign="bottom">775</td></tr><tr><td><inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td valign="bottom">4</td><td valign="bottom">0.47</td><td valign="bottom">0.51</td><td valign="bottom">−370</td><td valign="bottom">−504</td><td valign="bottom">511</td><td valign="bottom">641</td></tr><tr><td><inline-formula><mml:math id="inf24"><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula></td><td valign="bottom">3</td><td valign="bottom">0.36</td><td valign="bottom">0.44</td><td valign="bottom">−242</td><td valign="bottom">−471</td><td valign="bottom">640</td><td valign="bottom">675</td></tr><tr><td><inline-formula><mml:math id="inf25"><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:math></inline-formula></td><td valign="bottom">4</td><td valign="bottom">0.35</td><td valign="bottom">0.42</td><td valign="bottom">47</td><td valign="bottom">−193</td><td valign="bottom">929</td><td valign="bottom">952</td></tr><tr><td valign="bottom">Win −loss</td><td valign="bottom">4</td><td valign="bottom">0.57</td><td valign="bottom">0.63</td><td valign="bottom">−848</td><td valign="bottom">−1181</td><td valign="bottom">34</td><td valign="bottom">−35</td></tr></tbody></table></table-wrap><p>where PPE refers to the objective PPE defined as the difference between the outcome sign and the objective probability of the chosen option (0.2 or 0.8), and also:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where RPE is computed by taking the difference between the reward magnitude and the objective expected value (potential reward multiplied by the objective probability of the chosen option) as above.</p><p>We then tested whether mood fluctuations were additionally sensitive to current expectations, as shown in risky choice tasks that do not require learning (<xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>). Again, two types of expectations may explain mood fluctuations: a subjective probability relevant to learning, or expected values that incorporate learning-irrelevant reward magnitudes. We compared the following models:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf26"><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is the probability estimated with the additive choice model, and:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the product between <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the reward magnitude and this term is mean-centred.</p><p>The model including choice probability better explained happiness ratings (stable: mean r<sup>2</sup> = 0.58; volatile: mean r<sup>2</sup> = 0.62) than the model including the expected value in the stable (BIC<sub>P̂+PP̂E</sub> = −882, BIC<sub>EV+PP̂E</sub> = −752, ∆BIC = 130) and in the volatile (BIC<sub>P̂+PP̂E</sub> = −1147, BIC<sub>EV+PP̂E</sub> = −691, ∆BIC = 454) environments (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>). The probability and PPE weights were significantly different from 0 at the group level in both the stable (w<sub>P̂ </sub>= 0.74 ± 0.09, z = 6.2, p &lt; 10<sup>−9</sup>; w<sub>PP̂E</sub> = 1.32 ± 0.06, z = 7.5, p &lt; 10<sup>−14</sup>) and the volatile environments (w<sub>P̂ </sub>= 0.94 ± 0.09, z = 6.7, p &lt; 10<sup>−10</sup>; w<sub>PP̂E</sub> = 1.14 ± 0.05, p &lt; 10<sup>−14</sup>, <xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>We next extended the model space with plausible alternative models. We included two models incorporating the history of reward magnitude. In the first model, we centred the reward magnitude regressor for each participant. This model thus predicts that reward magnitudes larger than the averaged reward magnitude will increase happiness, and that the larger the reward magnitude, the greater the happiness.<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where R<sub>j</sub> is the reward magnitude at trial j and <inline-formula><mml:math id="inf29"><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is the average reward magnitude. Instead of assuming a reference point of the average reward, we also used a free parameter in a subsequent model above and below which reward magnitudes increase or decrease happiness, respectively:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>RP</italic> is a free parameter corresponding to the reference point in an individual subject. If this value is 0, receipt of rewards always increases happiness in proportion to the reward magnitude, so this model also provides a test of whether failing to obtain reward decreases happiness during reinforcement learning. The average reward magnitude was 25.5 ± 2.5 (mean ± SD) points in the stable environment and 22.6 ± 3.0 points in the volatile environment. The reference point <italic>RP</italic> estimated in model 14 was on average 14.2 ± 9.5 points and greater than 0 in the stable environment (z = 7.0, p &lt; 10<sup>−11</sup>) and 14.8 ± 7.3 points and greater than 0 in the volatile environment (z = 7.4, p &lt; 10<sup>−12</sup>). This result supports the idea that obtaining 0 points on a trial is aversive: failing to obtain reward decreases happiness in our tasks. We also included two additional models incorporating <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the expected value of the chosen option, corresponding to the weighted sum of probability and reward estimated based on each individual participant’s choices with the additive choice model, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the averaged expected reward, and <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the difference between the outcome reward magnitude and <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We also included a model combining the estimated probability with the reward prediction error.<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Besides the constant and the forgetting factor, the <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model includes two parameters, one for predictions (w<sub>P̂</sub>) and one for probability prediction error (w<sub>PP̂E</sub>).</p><p>We also asked whether the history of wins (excluding any information about reward magnitude) and losses could account for happiness by fitting the following model:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As reported in <xref ref-type="table" rid="table2">Table 2</xref>, the model evidence for this new model was similar to the <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model overall. We next used estimated model frequency to compare both models. The <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is preferred to the win-loss model in the stable environment (EF<sub>P̂+PP̂E</sub> = 0.65 ± 0.05, EF<sub>win-loss</sub> = 0.35 ± 0.05, exceedance probability = 0.99). However, both models performed similarly in the volatile environment (EF<sub>P̂+PP̂E</sub> = 0.50 ± 0.06, EF<sub>win-loss</sub> = 0.50 ± 0.06, exceedance probability = 0.48; see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>We next asked whether an alternative analysis could test whether happiness was influenced by trial-by-trial probability estimates, the key difference between the models. If the weights for the two terms of the <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> model are identical, the equation mathematically reduces to a constant plus an exponentially weighted average of previous wins. However, the w<sub>PP̂E</sub> parameter was larger than w<sub>P̂</sub> in both stable (z = 6.0, p &lt; 10<sup>−8</sup>) and volatile tasks (z = 2.35, p &lt; 0.05). The difference between w<sub>PP̂E</sub> and w<sub>P̂</sub> was significantly larger in the stable compared to the volatile task (z = 3.7, p &lt; 0.001). Comparison of weights across tasks therefore suggests a reduced impact of expectations on happiness as environmental volatility increases. We next computed the residuals of the win-loss model (which does not include probability estimates) and tested for a correlation with trial-by-trial probability estimates. Because prediction errors are equal to outcomes minus expectations and numerically w<sub>P̂</sub> is lower than w<sub>PP̂E</sub> in both environments, the overall influence of probability on happiness should be negative after accounting for the impact of wins and losses. In the stable environment, we found the expected negative correlation between the win-loss model residuals and trial-by-trial probability estimates (average Spearman’s ρ(73) = −0.06 ± 0.03, z = 2.2, p = 0.03). This relationship was not present in the volatile environment (average Spearman’s ρ(73)=−0.02 ± 0.03, z = 0.65, p = 0.51). A potential explanation for this pattern of results is that expectations cannot affect happiness when participants do not have strong predictions, as it is the case immediately after reversals in the volatile condition. This would be consistent with findings from the animal literature showing that dopamine early in training does not represent prediction errors (<xref ref-type="bibr" rid="bib15">Coddington and Dudman, 2018</xref>).</p><p>Finally, we focused on the win-loss model. We asked whether weights from the win-loss model were positively correlated, consistent with similar but opposite impacts. We found instead a negative correlation across participants between win and loss weights (stable: Spearman’s ρ(73) = −0.56, p &lt; 10<sup>−6</sup>; volatile: Spearman’s ρ(73) = −0.68, p &lt; 10<sup>−20</sup>), suggesting that individuals that respond to wins tend to respond less to losses and vice versa. Indeed, comparing the weight of wins and losses shows that participants reacted more strongly on average to losses than to wins (difference in stable: 0.69 ± 0.12, z = 5.2, p &lt; 10<sup>−6</sup>; difference in volatile: 0.31 ± 0.13, z = 2.7, p &lt; 0.01; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Given that participants received positive feedback on average in 81% of trials in the stable environment and 63% of trials in the volatile environment, asymmetric responses to wins and losses are consistent with happiness reflecting knowledge of the underlying structure of both environments. Interestingly, the difference between win and loss weights was not correlated across participants with overall performance including the percentage of trials with positive feedback (stable: Spearman’s ρ(73) = −0.05, p = 0.65; volatile: Spearman’s ρ(73) = −0.18, p = 0.12) or the percentage of trials where the higher expected value option was chosen (stable: Spearman’s ρ(73) = 0.04, p = 0.74; volatile: Spearman’s ρ(73) = −0.17, p = 0.14).</p><p>Our results suggest that although reward information influences choice, contrary to what would be predicted from the literature, RPEs and reward magnitudes do not explain happiness when this information is not necessary for participants to learn the structure of the environment. Happiness reflects knowledge of the underlying structure of the environment in a way that cannot be explained by simple performance metrics. RPEs are relevant to learning in many paradigms, and happiness should relate to RPEs in such tasks because of their value for learning the structure of environment. Learning and reward are dissociable in our paradigm, and we find in this context that RPEs and reward magnitudes do not explain happiness.</p></sec><sec id="s2-3"><title>Sensitivity of mood dynamics to learning variables depends on volatility but not on learning rate</title><p>We found that the learning rate (i.e., behavioural sensitivity to PPE) was approximately three times higher in the volatile compared to the stable environment. Furthermore, mood dynamics were highly sensitive to PPE. However, PPE weights were actually higher in stable than volatile environments (∆w<sub>PP̂E</sub> = 0.18 ± 0.05, z = 3.5, p &lt; 0.001; see <xref ref-type="fig" rid="fig3">Figure 3C</xref>). Futhermore, the difference between w<sub>PP̂E</sub> and w<sub>P̂</sub> was greater in stable than volatile environments (stable – volatile: 0.39 ± 0.11, z = 3.7, p &lt; 0.001), consistent with a greater influence of trial-by-trial probability estimates on happiness in stable environments (see previous section). PPE weights were not correlated between participants with the learning rate in the stable (Spearman’s ρ(73) = −0.10, p = 0.40) and volatile (Spearman’s ρ(73) = −0.1, p = 0.37) environments nor was the difference of PPE weights across environments related to the difference in learning rate (Spearman’s ρ(73) = −0.02, p = 0.84). Instead, PPE weights were highly consistent across environments (Spearman’s ρ(73) = 0.44, p &lt; 0.001, see <xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Forgetting factors are consistent across stable and volatile learning tasks.</title><p>(<bold>A</bold>) Weights for PPEs in determining happiness were consistent across environments. (<bold>B</bold>) The happiness forgetting factor did not change between stable (blue) and volatile (orange) environments, regardless of testing order. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for an analysis without any assumption regarding the shape of the influence decay. (<bold>C</bold>) Happiness forgetting factors were consistent across environments. Error bars represent SEM. ***p &lt; 0.001.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Happiness is influenced by multiple past probability prediction errors.</title><p>Each bar corresponds to the influence of the past trials on the current happiness rating in the stable (blue) and volatile (orange) environments. (<bold>A</bold>) The left panel shows the influence of probability prediction error estimated from the additive learning model (<inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). (<bold>B</bold>) The right panel shows the influence of the objective probability prediction error (PPE). Error bars represent SEM. *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig5-figsupp1-v3.tif"/></fig></fig-group><p>The happiness model forgetting factor γ determines how many previous trials influence current affective state. When γ is equal to 1, mood is equally influenced by all previous trials, when γ is equal to 0, mood is influenced by only the most recent trial. If the change in forgetting factor mirrors behaviour, forgetting factors should be lower in volatile than stable environments, reflecting integration over fewer trials and consistent with the higher learning rates observed in volatile compared to stable environments. Instead, the forgetting factor was slightly higher on average in the volatile environment (stable: γ = 0.59 ± 0.04, volatile γ = 0.63 ± 0.03, corresponding to current happiness being influenced by 6–7 previous trials on average in both environments, stable – volatile: ∆γ = 0.05 ± 0.03, z = 1.9, p = 0.064; see <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Higher values for γ in volatile environments are not consistent with happiness integrating over fewer trials as behaviour would predict. Furthermore, the change in happiness forgetting factor was not correlated across participants with the learning rate difference between environments (Spearman’s ρ(73) = −0.08, p = 0.50). A linear regression with ten previous probability prediction errors as independent variables confirmed this model-based result (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). To further test for a relationship between the forgetting factor and the learning rate, we switched the learning rates estimated from stable and volatile conditions in each individual before re-fitting happiness data (i.e., we used the ‘wrong’ learning rate to estimate probabilities and PPEs before fitting the happiness model and estimating a forgetting factor). This did not substantially affect estimates of the happiness forgetting factor (stable γ = 0.58 ± 0.03, volatile γ = 0.62 ± 0.03, stable – volatile: ∆γ = 0.032 ± 0.026, z = 1.4, p = 0.16). The resulting forgetting factor estimates were highly correlated with forgetting factors estimated using the actual learning rates (stable: Spearman’s ρ(73) = 0.63, p &lt; 10<sup>−8</sup>; volatile: Spearman’s ρ(73) = 0.56, p &lt; 10<sup>−6</sup>). The happiness forgetting factor was highly consistent across environments (Spearman’s ρ(73) = 0.41, p &lt; 0.001, see <xref ref-type="fig" rid="fig5">Figure 5C</xref>), suggesting that the number of previous trials that affective state depends on may be a trait-like feature of individuals unrelated to environmental volatility.</p></sec><sec id="s2-4"><title>Depressive symptoms are associated with reduced happiness in volatile environments</title><p>Previous studies have linked learning rates to anxiety (<xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Pulcu and Browning, 2019</xref>) and individuals with high trait anxiety showed less ability to appropriately adjust updating of outcome expectancies between stable and volatile environments. We found that depressive symptoms (PHQ) were uncorrelated across participants with choice accuracy (stable: Spearman’s ρ(73) = −0.06, p = 0.63; volatile: Spearman’s ρ(73) = −0.09, p = 0.46; volatile – stable: Spearman’s ρ(73) = 0.04, p = 0.71) and all parameters estimated in the additive choice model (α, stable: Spearman’s ρ(73) = 0.08, p = 0.51; volatile: Spearman’s ρ(73) = 0.10, p = 0.38; volatile – stable: Spearman’s ρ(73) = 0.19, p = 0.1; ϕ, stable: Spearman’s ρ(73) = 0.20, p = 0.09; volatile: Spearman’s ρ(73) = 0.01, p = 0.94; volatile – stable: Spearman’s ρ(73) = −0.20, p = 0.09; β, stable: Spearman’s ρ(73) = −0.04, p = 0.76; volatile: Spearman’s ρ(73) = −0.07, p = 0.53; volatile – stable: Spearman’s ρ(73) = 0.04, p = 0.75). In the stable environment, where uncertainty and volatility are low, average happiness did not correlate across participants with depressive symptoms (Spearman’s ρ(73) = 0.07, p = 0.58; <xref ref-type="fig" rid="fig6">Figure 6A</xref>, left panel). In the volatile environment, where uncertainty is high and volatility is high, average happiness was correlated with depressive symptoms, with lower happiness associated with higher depressive symptoms (Spearman’s ρ(73) = −0.23, p = 0.043; <xref ref-type="fig" rid="fig6">Figure 6A</xref>, central panel). Finally, the difference between average happiness between volatile and stable environments was also correlated with depressive symptoms even after standardising the variables (<xref ref-type="bibr" rid="bib60">Wilcox and Tian, 2008</xref>) (volatile – stable: Spearman’s ρ(73) = −0.28, p = 0.014; <xref ref-type="fig" rid="fig6">Figure 6A</xref>, right panel). Baseline mood parameters estimated using our happiness model fit to non-z-scored happiness ratings showed the same relationship to depressive symptoms (stable: Spearman’s ρ(73) = −0.07, p = 0.54; volatile: Spearman’s ρ(73) = −0.28, p = 0.017; volatile – stable, standardised: Spearman’s ρ(73) = −0.32, p = 0.0049; see <xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Baseline mood decreases with depressive symptoms in volatile environments.</title><p>(<bold>A</bold>) Average happiness was not correlated with depressive symptoms (PHQ) in the stable task (left panel, blue) but decreased with depressive symptoms in the volatile task (middle panel, orange). The difference in happiness between stable and volatile environments was also significantly related to depression (right panel). (<bold>B</bold>) Baseline mood parameters estimated with non-z-scored happiness ratings showed the same relationship to depressive symptoms as average happiness with lower parameters in volatile than stable environments. *p &lt; 0.05, **p &lt; 0.01.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57977-fig6-v3.tif"/></fig><p>No other happiness model parameters were correlated with depressive symptoms (w<sub>P̂</sub>, stable: Spearman’s ρ(73) = 0.20, p = 0.09; volatile: Spearman’s ρ(73) = 0.15, p = 0.19; volatile – stable: Spearman’s ρ(73) = −0.02, p = 0.84; w<sub>PP̂E,</sub> stable: Spearman’s ρ(73) = 0.09, p = 0.45; volatile: Spearman’s ρ(73) = 0.07, p = 0.54; volatile – stable: Spearman’s ρ(73) = −0.03, p = 0.81; γ, stable: Spearman’s ρ(73) = 0.06, p = 0.63; volatile: Spearman’s ρ(73) = −0.09, p = 0.44; volatile – stable: Spearman’s ρ(73) = −0.12, p = 0.28).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We found that subjects tracked outcome probabilities and made decisions by integrating both learned probability and explicit reward magnitudes. Learning rates adapted to environmental volatility, with a higher learning rate in the more volatile environment consistent with previous studies (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>). That behaviour was consistent with an additive choice model (<xref ref-type="bibr" rid="bib19">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib22">Farashahi et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Rouault et al., 2019</xref>) which is consistent with recent empirical evidence (<xref ref-type="bibr" rid="bib23">Farashahi et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Koechlin, 2020</xref>) that humans and non-human primates adopt a multiplicative strategy under risk when probabilities are explicit, but both species adopt an additive strategy under uncertainty when probabilities must be learned.</p><p>Our tasks required learning the probability of getting a reward and the reward magnitude was explicitly given. In such an environment, mood dynamics were more closely related to learning-relevant variables than learning-irrelevant variables and we found convergent evidence that this was the case across both stable and volatile learning tasks. Mood was sensitive to the combined influence of past chosen subjective probabilities and past PPEs. Parameters for PPE and forgetting factors estimated from happiness ratings were correlated across stable and volatile environments. Finally, we found that although choice accuracy and choice model parameters were not affected by depressive symptoms when changes between safe and volatile environments are signalled, the decrease in happiness observed in the volatile relative to the stable environment was correlated with symptom severity. The same pattern was present for the baseline mood parameter in the happiness model. Experiencing a stable environment with low uncertainty and volatility could attenuate the expression of depressive symptoms on mood. Risky decision tasks used in previous studies (<xref ref-type="bibr" rid="bib48">Rutledge et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>) maximise irreducible uncertainty (i.e., risky options had a 50% probability of each option), which is more comparable to the volatile environment in the current study and may explain the previous finding of a link between baseline mood parameters and depressive symptoms. Computational models that capture ecologically relevant learning and decision processes may provide a critical advantage for understanding the mechanisms that underlie psychiatric symptoms (<xref ref-type="bibr" rid="bib49">Scholl and Klein-Flügge, 2018</xref>). Our findings suggest that subjective feelings measured during tasks in depression-relevant domains may provide additional information not captured by computational models of learning and decision-making. One reason depression might reduce mood more in volatile than stable environments could be an increase in the number of negative prediction errors experienced. Misestimation of the level of uncertainty may also lead to a tendency for negative events to disproportionally affect depressed individuals, and this uncertainty misestimation is believed to contribute to depression and anxiety (<xref ref-type="bibr" rid="bib43">Pulcu and Browning, 2019</xref>). That the learning rate difference between the volatile and the stable environment did not correlate with anxiety symptom severity is consistent with previous findings (<xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>) that anxiety is linked to learning deficits for aversive but not appetitive outcomes. In the aversive domain, anxiety severity might be associated with differences in behavioural adaptation to volatility changes as well as mood.</p><p>It is not yet established what the neural signal associated with PPEs is. On the one hand, reward prediction errors have been associated with neuromodulator dopamine and are thought to be linked to ventral tegmental area (<xref ref-type="bibr" rid="bib2">Bayer and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib16">Cohen et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Hart et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib42">Pessiglione et al., 2006</xref>; <xref ref-type="bibr" rid="bib50">Schultz et al., 1997</xref>). In most studies, RPEs and PPEs are equivalent and therefore the specific link between PPEs and dopamine is less documented. The probability of obtaining reward and probability prediction error have been associated with VTA activity correcting for expected value (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>), suggesting that PPEs may be represented by dopamine. Boosting dopamine levels pharmacologically during risky decision making increases the happiness resulting from smaller rewards to a level similar to that resulting from larger rewards (<xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>). Although happiness in that study was influenced by the history of RPEs, dopamine drug impacts were limited to rewards. Dopamine has also been associated with other signals than prediction errors, for example incentive salience which might relate to ‘wanting’ and might influence choice and action (<xref ref-type="bibr" rid="bib5">Berridge, 2012</xref>; <xref ref-type="bibr" rid="bib53">Smith et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Zhang et al., 2009</xref>).</p><p>Some studies suggest that dopaminergic activity in the midbrain is linked to information-seeking to reduce uncertainty about an upcoming reward, even though such information is not instrumental (<xref ref-type="bibr" rid="bib8">Bromberg-Martin and Hikosaka, 2009</xref>; <xref ref-type="bibr" rid="bib10">Brydevall et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Charpentier et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Gruber and Ranganath, 2019</xref>). The intrinsic reward resulting from reducing uncertainty could influence mood, and mood ratings could then be used to quantify the relative subjective weight of extrinsic and intrinsic reward.</p><p>Previous studies using risky decision tasks where reward and probability were explicitly represented (<xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>) showed that mood dynamics were explained by past expected values and RPEs. The present design allows us to dissociate the impact of learning-relevant and learning-irrelevant information for mood in two different standard learning environments. Our results suggest that when goal attainment requires adaptive behaviour, mood dynamics reflect learning-relevant information. Consistent with the results obtained from risky decision tasks used in previous studies (<xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Rutledge et al., 2017</xref>), potential rewards were a key determinant of behaviour in our task. However, rewards were not a determinant of mood in the current study, in contrast to previous results in risky decision tasks (<xref ref-type="bibr" rid="bib46">Rutledge et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Rutledge et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Rutledge et al., 2017</xref>). Unlike most reinforcement learning experiments, our task design allows dissociating the impacts of PPEs and RPEs on behaviour and mood. Here, our results suggest that happiness does not always depend on reward and preferentially reflects learning about the structure of the environment. However, if learning the structure of the environment requires tracking changing reward magnitudes, we would expect that happiness would track learning-relevant variables (e.g., reward magnitudes and RPEs in such an environment).</p><p>This result might imply a role for mood in learning in line with influential proposals (<xref ref-type="bibr" rid="bib20">Eldar et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Eldar and Niv, 2015</xref>). However, mood did not reflect all learning-relevant information that influenced behaviour. Happiness forgetting factors corresponding to the number of past trials that influence affective state were highly correlated across environments and acted more as a stable trait that differed between individuals and did not adjust to environmental volatility. Overall, our findings show that mood dynamics are sensitive to depressive symptoms and reflect variables relevant to adaptive behaviour irrespective of environmental volatility.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Seventy-five healthy subjects (age range 18–35, 24 males) took part in the experiment. Thirty-seven completed the stable learning task first and the volatile learning task second. Group allocation was randomised. Subjects were paid £10 for their participation. The number of participants recruited for the current cohort was selected to provide &gt;95% power of detecting a similar effect size as that reported in a previous study in which a volatility manipulation was used to influence learning rate (<xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>). All subjects gave informed consent and the Research Ethics Committee of University College London approved the study (Committee approval ID Number: 12673/001).</p></sec><sec id="s4-2"><title>Procedure</title><p>Participants were first instructed about the tasks. They performed 20 practice trials before a test ensuring that they understood that both probability and magnitude mattered to maximise the number of points obtained. They were told that they would be exposed to two environments: an environment where one car is more likely to win for the entire session, and an environment where which car is more likely to win changes occasionally. Because we wanted to maximise the efficiency of the behavioural manipulation (i.e., the difference in learning rate between environments observed in previous studies [<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Massi et al., 2018</xref>]) to study how mood dynamics varied with behavioural sensitivity, we explicitly signalled the environment by using different pairs of cars in the two environments. Moreover, before each condition, an instruction screen explained in which environment participants will be placed. Finally, a fixation symbol displayed in the centre of the screen in each trial was specific for each environment: ‘-’ for the stable environment and ‘~’ for the volatile environment. Participants were given no guidance as to how they should use information about environmental volatility. After completion of the task, participants completed three standard clinical questionnaires: Beck Depression Inventory (BDI-II <xref ref-type="bibr" rid="bib3">Beck et al., 1996</xref>), Patient Health Questionnaire (PHQ-9, <xref ref-type="bibr" rid="bib33">Kroenke et al., 2001</xref>), and the State/Trait Anxiety Inventory (STAI, <xref ref-type="bibr" rid="bib54">Spielberger, 1983</xref>).</p></sec><sec id="s4-3"><title>Experimental task</title><p>The task was implemented using the Cogent toolbox in MATLAB (MathWorks, Inc). Subjects had to choose between two cars, each associated with a probability (20% or 80%) of winning. If the chosen car won, participants earned the corresponding amount of points. In the stable environment (80 trials), the probability to win for the best car was 80%. In the volatile environment (80 trials), reward probabilities switched between 80% for one car and 80% for the other car every 20 trials. The order was counterbalanced between subjects (n = 38 in stable-volatile order, n = 37 in volatile-stable order). The outcomes were locally pseudo-randomised, to ensure that every 10 trials (i.e., trials 1–10, 11–20), the car with the highest outcome probability won on exactly 8 of 10 trials. The possible pairs of rewards were 10–10, 10–40, 10–60, 10–80, 20–40, 40–10, 40–20, 40–40, 60–10, 80–10. Subjects were primed when the side of the screen for each car was swapped (every six to ten trials) with an explicit cue. They were also instructed that the car location was unrelated to the outcome probability and to the change in outcome probabilities in the volatile environment. Every three to four trials, subjects were asked to indicate ‘How happy are you right now?’ on a scale from very unhappy to very happy. They were told to consider these extremes within the context of the experiment. Each trial started with a fixation screen for a duration varying between 0.9 and 1.9 s. Cars were displayed for 1.2 s without any information about reward magnitudes, and no choice was allowed in this phase. Subjects were free to choose the option they preferred without any time constraints as soon as the potential reward for each car was displayed. The chosen option was surrounded by a yellow frame for 1.5 s. Finally, the outcome was displayed for 2 s. Both the car and the reward magnitude frames were green if the chosen car won. They were red and the car was crossed out if the chosen car lost.</p></sec><sec id="s4-4"><title>Data analyses</title><p>Two-sided Wilcoxon signed rank tests were used to compare performance, proportion of win-stay/lose-shift, and model parameters between environments at the group level. Spearman rank correlations across participants were used to test for relationships between parameters and to relate depression scores to behavioural and happiness parameters. All analyses were performed using MATLAB. To test whether the correlation between PHQ score and happiness baseline parameters was higher in the volatile condition than in the stable condition, we correlated the standardised difference between the happiness baseline parameter in the stable and volatile conditions with the standardised PHQ score which quantifies depressive symptoms (<xref ref-type="bibr" rid="bib60">Wilcox and Tian, 2008</xref>). The analysis codes were written in MATLAB and are available at Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/BastienBlain/MSWB_LearningNotReward">https://github.com/BastienBlain/MSWB_LearningNotReward</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e9174f278be2f11c918d94546c15347580285ac9;origin=https://github.com/BastienBlain/MSWB_LearningNotReward;visit=swh:1:snp:8db3baca6ab7c4542691213db4576c2def9c6016;anchor=swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b/">swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b</ext-link>; <xref ref-type="bibr" rid="bib6">Blain, 2020</xref>).</p></sec><sec id="s4-5"><title>Computational models</title><p>All models were fitted to experimental data by minimising the negative log likelihood of the predicted choice probability given different model parameters using the fmincon function in MATLAB (Mathworks Inc). More specifically, parameters were treated as random effects that could differ between subjects (<xref ref-type="bibr" rid="bib32">Kreft and De Leeuw, 1998</xref>): data were fitted for each participant and statistical tests were performed at the group level. We used standard model comparison techniques (<xref ref-type="bibr" rid="bib11">Burnham and Anderson, 2004</xref>; <xref ref-type="bibr" rid="bib51">Schwarz, 1978</xref>) to compare model fits. For each model fit in individual subjects, we computed the Bayesian Information Criterion (BIC), which penalises for model complexity (i.e. number of parameters), and then summed BIC across subjects. The model with the lowest BIC is the preferred model. For all choice models, the learning rate was bounded between 0 and 1, the inverse temperature between 0 and 50 (to avoid ceiling effects), the probability-magnitude relative weight phi between 0 and 1, and the gamma risk aversion parameters between 0 and 10. Note that reward magnitude was normalised between 0 and 1 in the additive model. For the happiness models, we first fitted each happiness model on both environments using the same parameters for both environments. Then each model was fitted for each environment separately, with the starting parameters determined by the joint model fit under standard constraints (the forgetting factor could vary only between 0 and 1 and the baseline mood parameter could only vary between 0 and 100). Because participants vary in how they use the scale, we z-scored happiness ratings for all the analyses reported in the main text, except in the analyses where we asked how the baseline mood parameters are related to depression.</p></sec><sec id="s4-6"><title>Estimated frequency and exceedance probability</title><p>Models were treated as random effects that could differ between subjects and have a fixed (unknown) distribution in the population. Model frequency with which any model prevails in the population, as well as exceedance probability (EP), which measures how likely it is that any given model is more frequent than all other models in the comparison set (<xref ref-type="bibr" rid="bib55">Stephan et al., 2009</xref>), were estimated using the VBA Matlab toolbox (<xref ref-type="bibr" rid="bib17">Daunizeau et al., 2014</xref>). See figure supplementary figure 2 for an illustration of the estimated frequency for three different model spaces. An EP greater than 0.95 is considered significant.</p></sec><sec id="s4-7"><title>Happiness is influenced by multiple past probability prediction errors</title><p>To estimate the influence of the past trials on the current happiness without any assumption regarding the shape of the influence decay, we fitted a general linear model including for each rating the previous 10 probability prediction errors using the Matlab glmfit function. Each value was then tested against 0 at the group level using two-sided Wilcoxon signed rank tests (see supplementary figure 3).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Sankalp Garud, Matilde Vaghi, Benjamin Chew, Paul Sharp, and Rachel Bedder for their help. R.B.R. is supported by a Medical Research Council Career Development Award (MR/N02401X/1), a 2018 NARSAD Young Investigator Grant (27674) from the Brain and Behavior Research Foundation, P and S Fund, and by the National Institute of Mental Health (1R01MH124110). The Max Planck UCL Centre is a joint initiative supported by UCL and the Max Planck Society. The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome Trust (203147/Z/16/Z).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Formal analysis, Supervision, Validation, Investigation, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects gave informed consent and the Research Ethics Committee of University College London approved the study study (Committee approval ID Number: 12673/001).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-57977-transrepform-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and code are available online (<ext-link ext-link-type="uri" xlink:href="https://github.com/BastienBlain/MSWB_LearningNotReward">https://github.com/BastienBlain/MSWB_LearningNotReward</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b/">https://archive.softwareheritage.org/swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b/</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Adaptive critics and the basal ganglia</chapter-title><person-group person-group-type="editor"><name><surname>Houk</surname> <given-names>J. C</given-names></name><name><surname>Davis</surname> <given-names>J. L</given-names></name></person-group><source>Models of Information Processing in the Basal Ganglia, Computational Neuroscience</source><publisher-name>The MIT Press</publisher-name><fpage>215</fpage><lpage>232</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayer</surname> <given-names>HM</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Midbrain dopamine neurons encode a quantitative reward prediction error signal</article-title><source>Neuron</source><volume>47</volume><fpage>129</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.020</pub-id><pub-id pub-id-type="pmid">15996553</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>AT</given-names></name><name><surname>Steer</surname> <given-names>RA</given-names></name><name><surname>Ball</surname> <given-names>R</given-names></name><name><surname>Ranieri</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Comparison of beck depression inventories -IA and -II in psychiatric outpatients</article-title><source>Journal of Personality Assessment</source><volume>67</volume><fpage>588</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1207/s15327752jpa6703_13</pub-id><pub-id pub-id-type="pmid">8991972</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Walton</surname> <given-names>ME</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berridge</surname> <given-names>KC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>From prediction error to incentive salience: mesolimbic computation of reward motivation</article-title><source>European Journal of Neuroscience</source><volume>35</volume><fpage>1124</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.07990.x</pub-id><pub-id pub-id-type="pmid">22487042</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Blain</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>MSWB_LearningNotReward</data-title><source>Software Heritage</source><version designator="swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b">swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e9174f278be2f11c918d94546c15347580285ac9;origin=https://github.com/BastienBlain/MSWB_LearningNotReward;visit=swh:1:snp:8db3baca6ab7c4542691213db4576c2def9c6016;anchor=swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b/">https://archive.softwareheritage.org/swh:1:dir:e9174f278be2f11c918d94546c15347580285ac9;origin=https://github.com/BastienBlain/MSWB_LearningNotReward;visit=swh:1:snp:8db3baca6ab7c4542691213db4576c2def9c6016;anchor=swh:1:rev:b7c4a0cd761dcf249c72caf809dd81af24c4a49b/</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanco</surname> <given-names>NJ</given-names></name><name><surname>Otto</surname> <given-names>AR</given-names></name><name><surname>Maddox</surname> <given-names>WT</given-names></name><name><surname>Beevers</surname> <given-names>CG</given-names></name><name><surname>Love</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The influence of depression symptoms on exploratory decision-making</article-title><source>Cognition</source><volume>129</volume><fpage>563</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2013.08.018</pub-id><pub-id pub-id-type="pmid">24055832</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bromberg-Martin</surname> <given-names>ES</given-names></name><name><surname>Hikosaka</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Midbrain dopamine neurons signal preference for advance information about upcoming rewards</article-title><source>Neuron</source><volume>63</volume><fpage>119</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.06.009</pub-id><pub-id pub-id-type="pmid">19607797</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname> <given-names>M</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Jocham</surname> <given-names>G</given-names></name><name><surname>O'Reilly</surname> <given-names>JX</given-names></name><name><surname>Bishop</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/nn.3961</pub-id><pub-id pub-id-type="pmid">25730669</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brydevall</surname> <given-names>M</given-names></name><name><surname>Bennett</surname> <given-names>D</given-names></name><name><surname>Murawski</surname> <given-names>C</given-names></name><name><surname>Bode</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The neural encoding of information prediction errors during non-instrumental information seeking</article-title><source>Scientific Reports</source><volume>8</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-24566-x</pub-id><pub-id pub-id-type="pmid">29666461</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burnham</surname> <given-names>KP</given-names></name><name><surname>Anderson</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Model Selection and Multi-Model Inference: A Practical Information-Theoretic Approach</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/b97636</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cella</surname> <given-names>M</given-names></name><name><surname>Dymond</surname> <given-names>S</given-names></name><name><surname>Cooper</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Impaired flexible decision-making in major depressive disorder</article-title><source>Journal of Affective Disorders</source><volume>124</volume><fpage>207</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.jad.2009.11.013</pub-id><pub-id pub-id-type="pmid">20004023</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charpentier</surname> <given-names>CJ</given-names></name><name><surname>Bromberg-Martin</surname> <given-names>ES</given-names></name><name><surname>Sharot</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Valuation of knowledge and ignorance in mesolimbic reward circuitry</article-title><source>PNAS</source><volume>115</volume><fpage>E7255</fpage><lpage>E7264</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800547115</pub-id><pub-id pub-id-type="pmid">29954865</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chase</surname> <given-names>HW</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Michael</surname> <given-names>A</given-names></name><name><surname>Bullmore</surname> <given-names>ET</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Robbins</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Approach and avoidance learning in patients with major depression and healthy controls: relation to anhedonia</article-title><source>Psychological Medicine</source><volume>40</volume><fpage>433</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1017/S0033291709990468</pub-id><pub-id pub-id-type="pmid">19607754</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coddington</surname> <given-names>LT</given-names></name><name><surname>Dudman</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The timing of action determines reward prediction signals in identified midbrain dopamine neurons</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1563</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0245-7</pub-id><pub-id pub-id-type="pmid">30323275</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>JY</given-names></name><name><surname>Haesler</surname> <given-names>S</given-names></name><name><surname>Vong</surname> <given-names>L</given-names></name><name><surname>Lowell</surname> <given-names>BB</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title><source>Nature</source><volume>482</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/nature10754</pub-id><pub-id pub-id-type="pmid">22258508</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Adam</surname> <given-names>V</given-names></name><name><surname>Rigoux</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>VBA: a probabilistic treatment of nonlinear models for neurobiological and behavioural data</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003441</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003441</pub-id><pub-id pub-id-type="pmid">24465198</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Berker</surname> <given-names>AO</given-names></name><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Mathys</surname> <given-names>C</given-names></name><name><surname>Marshall</surname> <given-names>L</given-names></name><name><surname>Cross</surname> <given-names>GF</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Bestmann</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computations of uncertainty mediate acute stress responses in humans</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>10996</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms10996</pub-id><pub-id pub-id-type="pmid">27020312</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donahue</surname> <given-names>CH</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamic routing of task-relevant signals for decision making in dorsolateral prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>295</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1038/nn.3918</pub-id><pub-id pub-id-type="pmid">25581364</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname> <given-names>E</given-names></name><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mood as representation of momentum</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.07.010</pub-id><pub-id pub-id-type="pmid">26545853</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname> <given-names>E</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interaction between emotional state and learning underlies mood instability</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6149</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7149</pub-id><pub-id pub-id-type="pmid">25608088</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farashahi</surname> <given-names>S</given-names></name><name><surname>Donahue</surname> <given-names>CH</given-names></name><name><surname>Khorsand</surname> <given-names>P</given-names></name><name><surname>Seo</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Soltani</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Metaplasticity as a neural substrate for adaptive learning and choice under uncertainty</article-title><source>Neuron</source><volume>94</volume><fpage>401</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.044</pub-id><pub-id pub-id-type="pmid">28426971</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farashahi</surname> <given-names>S</given-names></name><name><surname>Donahue</surname> <given-names>CH</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Soltani</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Flexible combination of reward information across primates</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>1215</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0714-3</pub-id><pub-id pub-id-type="pmid">31501543</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fredrickson</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The broaden-and-build theory of positive emotions</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>359</volume><fpage>1367</fpage><lpage>1377</lpage><pub-id pub-id-type="doi">10.1098/rstb.2004.1512</pub-id><pub-id pub-id-type="pmid">15347528</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillan</surname> <given-names>CM</given-names></name><name><surname>Kosinski</surname> <given-names>M</given-names></name><name><surname>Whelan</surname> <given-names>R</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Characterizing a psychiatric symptom dimension related to deficits in goal-directed control</article-title><source>eLife</source><volume>5</volume><elocation-id>e11305</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11305</pub-id><pub-id pub-id-type="pmid">26928075</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruber</surname> <given-names>MJ</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How curiosity enhances Hippocampus-Dependent memory: the prediction, appraisal, curiosity, and exploration (PACE) Framework</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>1014</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.10.003</pub-id><pub-id pub-id-type="pmid">31706791</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname> <given-names>AS</given-names></name><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name><name><surname>Phillips</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phasic dopamine release in the rat nucleus accumbens symmetrically encodes a reward prediction error term</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>698</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2489-13.2014</pub-id><pub-id pub-id-type="pmid">24431428</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herzallah</surname> <given-names>MM</given-names></name><name><surname>Moustafa</surname> <given-names>AA</given-names></name><name><surname>Natsheh</surname> <given-names>JY</given-names></name><name><surname>Abdellatif</surname> <given-names>SM</given-names></name><name><surname>Taha</surname> <given-names>MB</given-names></name><name><surname>Tayem</surname> <given-names>YI</given-names></name><name><surname>Sehwail</surname> <given-names>MA</given-names></name><name><surname>Amleh</surname> <given-names>I</given-names></name><name><surname>Petrides</surname> <given-names>G</given-names></name><name><surname>Myers</surname> <given-names>CE</given-names></name><name><surname>Gluck</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning from negative feedback in patients with major depressive disorder is attenuated by SSRI antidepressants</article-title><source>Frontiers in Integrative Neuroscience</source><volume>7</volume><elocation-id>67</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2013.00067</pub-id><pub-id pub-id-type="pmid">24065894</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Pizzagalli</surname> <given-names>DA</given-names></name><name><surname>Bogdan</surname> <given-names>R</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mapping anhedonia onto reinforcement learning: a behavioural meta-analysis</article-title><source>Biology of Mood &amp; Anxiety Disorders</source><volume>3</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1186/2045-5380-3-12</pub-id><pub-id pub-id-type="pmid">23782813</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname> <given-names>D</given-names></name><name><surname>Tversky</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Prospect theory: an analysis of decision under risk</article-title><source>Econometrica</source><volume>47</volume><fpage>263</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.2307/1914185</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Human Decision-Making beyond the rational decision theory</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>4</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.11.001</pub-id><pub-id pub-id-type="pmid">31767211</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kreft</surname> <given-names>IG</given-names></name><name><surname>De Leeuw</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Introducing Multilevel Modeling</source><publisher-name>Sage</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-73186-5_1</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kroenke</surname> <given-names>K</given-names></name><name><surname>Spitzer</surname> <given-names>RL</given-names></name><name><surname>Williams</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The PHQ-9: validity of a brief depression severity measure</article-title><source>Journal of General Internal Medicine</source><volume>16</volume><fpage>606</fpage><lpage>613</lpage><pub-id pub-id-type="doi">10.1046/j.1525-1497.2001.016009606.x</pub-id><pub-id pub-id-type="pmid">11556941</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kunisato</surname> <given-names>Y</given-names></name><name><surname>Okamoto</surname> <given-names>Y</given-names></name><name><surname>Ueda</surname> <given-names>K</given-names></name><name><surname>Onoda</surname> <given-names>K</given-names></name><name><surname>Okada</surname> <given-names>G</given-names></name><name><surname>Yoshimura</surname> <given-names>S</given-names></name><name><surname>Suzuki</surname> <given-names>S</given-names></name><name><surname>Samejima</surname> <given-names>K</given-names></name><name><surname>Yamawaki</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effects of depression on reward-based decision making and variability of action in probabilistic learning</article-title><source>Journal of Behavior Therapy and Experimental Psychiatry</source><volume>43</volume><fpage>1088</fpage><lpage>1094</lpage><pub-id pub-id-type="doi">10.1016/j.jbtep.2012.05.007</pub-id><pub-id pub-id-type="pmid">22721601</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massi</surname> <given-names>B</given-names></name><name><surname>Donahue</surname> <given-names>CH</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Volatility facilitates value updating in the prefrontal cortex</article-title><source>Neuron</source><volume>99</volume><fpage>598</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.033</pub-id><pub-id pub-id-type="pmid">30033151</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathys</surname> <given-names>C</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A Bayesian foundation for individual learning under uncertainty</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00039</pub-id><pub-id pub-id-type="pmid">21629826</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mellers</surname> <given-names>BA</given-names></name><name><surname>Schwartz</surname> <given-names>A</given-names></name><name><surname>Ho</surname> <given-names>K</given-names></name><name><surname>Ritov</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Decision affect theory: emotional reactions to the outcomes of risky options</article-title><source>Psychological Science</source><volume>8</volume><fpage>423</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1997.tb00455.x</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname> <given-names>PR</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A framework for mesencephalic dopamine systems based on predictive hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id><pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname> <given-names>EM</given-names></name><name><surname>Pechtel</surname> <given-names>P</given-names></name><name><surname>Cohen</surname> <given-names>AL</given-names></name><name><surname>Douglas</surname> <given-names>SR</given-names></name><name><surname>Pizzagalli</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Potentiated processing of negative feedback in depression is attenuated by anhedonia</article-title><source>Depression and Anxiety</source><volume>32</volume><fpage>296</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1002/da.22338</pub-id><pub-id pub-id-type="pmid">25620272</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname> <given-names>AR</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Unexpected but incidental positive outcomes predict Real-World gambling</article-title><source>Psychological Science</source><volume>27</volume><fpage>299</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1177/0956797615618366</pub-id><pub-id pub-id-type="pmid">26796614</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pechtel</surname> <given-names>P</given-names></name><name><surname>Dutra</surname> <given-names>SJ</given-names></name><name><surname>Goetz</surname> <given-names>EL</given-names></name><name><surname>Pizzagalli</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Blunted reward responsiveness in remitted depression</article-title><source>Journal of Psychiatric Research</source><volume>47</volume><fpage>1864</fpage><lpage>1869</lpage><pub-id pub-id-type="doi">10.1016/j.jpsychires.2013.08.011</pub-id><pub-id pub-id-type="pmid">24064208</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname> <given-names>M</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Flandin</surname> <given-names>G</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</article-title><source>Nature</source><volume>442</volume><fpage>1042</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1038/nature05051</pub-id><pub-id pub-id-type="pmid">16929307</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulcu</surname> <given-names>E</given-names></name><name><surname>Browning</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The misestimation of uncertainty in affective disorders</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>865</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.07.007</pub-id><pub-id pub-id-type="pmid">31431340</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname> <given-names>OJ</given-names></name><name><surname>Cools</surname> <given-names>R</given-names></name><name><surname>Carlisi</surname> <given-names>CO</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Drevets</surname> <given-names>WC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Ventral striatum response during reward and punishment reversal learning in unmedicated major depressive disorder</article-title><source>American Journal of Psychiatry</source><volume>169</volume><fpage>152</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2011.11010137</pub-id><pub-id pub-id-type="pmid">22420038</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouault</surname> <given-names>M</given-names></name><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prefrontal mechanisms combining rewards and beliefs in human decision-making</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/s41467-018-08121-w</pub-id><pub-id pub-id-type="pmid">30655534</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Skandali</surname> <given-names>N</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A computational and neural model of momentary subjective well-being</article-title><source>PNAS</source><volume>111</volume><fpage>12252</fpage><lpage>12257</lpage><pub-id pub-id-type="doi">10.1073/pnas.1407535111</pub-id><pub-id pub-id-type="pmid">25092308</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Skandali</surname> <given-names>N</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dopaminergic modulation of decision making and subjective Well-Being</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>9811</fpage><lpage>9822</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0702-15.2015</pub-id><pub-id pub-id-type="pmid">26156984</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Moutoussis</surname> <given-names>M</given-names></name><name><surname>Smittenaar</surname> <given-names>P</given-names></name><name><surname>Zeidman</surname> <given-names>P</given-names></name><name><surname>Taylor</surname> <given-names>T</given-names></name><name><surname>Hrynkiewicz</surname> <given-names>L</given-names></name><name><surname>Lam</surname> <given-names>J</given-names></name><name><surname>Skandali</surname> <given-names>N</given-names></name><name><surname>Siegel</surname> <given-names>JZ</given-names></name><name><surname>Ousdal</surname> <given-names>OT</given-names></name><name><surname>Prabhu</surname> <given-names>G</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Fonagy</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Association of neural and emotional impacts of reward prediction errors with major depression</article-title><source>JAMA Psychiatry</source><volume>74</volume><fpage>790</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1001/jamapsychiatry.2017.1713</pub-id><pub-id pub-id-type="pmid">28678984</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname> <given-names>J</given-names></name><name><surname>Klein-Flügge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Understanding psychiatric disorder by capturing ecologically relevant features of learning and decision-making</article-title><source>Behavioural Brain Research</source><volume>355</volume><fpage>56</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2017.09.050</pub-id><pub-id pub-id-type="pmid">28966147</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating the dimension of a model</article-title><source>The Annals of Statistics</source><volume>6</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepperd</surname> <given-names>JA</given-names></name><name><surname>Mcnulty</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The affective consequences of expected and unexpected outcomes</article-title><source>Psychological Science</source><volume>13</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00416</pub-id><pub-id pub-id-type="pmid">11892785</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>KS</given-names></name><name><surname>Berridge</surname> <given-names>KC</given-names></name><name><surname>Aldridge</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disentangling pleasure from incentive salience and learning signals in brain reward circuitry</article-title><source>PNAS</source><volume>108</volume><fpage>E255</fpage><lpage>E264</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101920108</pub-id><pub-id pub-id-type="pmid">21670308</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spielberger</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="1983">1983</year><source>State Trait Anxiety Inventory for Adults: Sampler Set: Manual, Test, Scoring Key, the Corsini Encyclopedia of Psychology</source><publisher-name>Mind Garden</publisher-name><pub-id pub-id-type="doi">10.1002/9780470479216</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Penny</surname> <given-names>WD</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Moran</surname> <given-names>RJ</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian model selection for group studies</article-title><source>NeuroImage</source><volume>46</volume><fpage>1004</fpage><lpage>1017</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.03.025</pub-id><pub-id pub-id-type="pmid">19306932</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor Tavares</surname> <given-names>JV</given-names></name><name><surname>Clark</surname> <given-names>L</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Williams</surname> <given-names>GB</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Drevets</surname> <given-names>WC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural basis of abnormal response to negative feedback in unmedicated mood disorders</article-title><source>NeuroImage</source><volume>42</volume><fpage>1118</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.05.049</pub-id><pub-id pub-id-type="pmid">18586109</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thoma</surname> <given-names>P</given-names></name><name><surname>Norra</surname> <given-names>C</given-names></name><name><surname>Juckel</surname> <given-names>G</given-names></name><name><surname>Suchan</surname> <given-names>B</given-names></name><name><surname>Bellebaum</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Performance monitoring and empathy during active and observational learning in patients with major depression</article-title><source>Biological Psychology</source><volume>109</volume><fpage>222</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2015.06.002</pub-id><pub-id pub-id-type="pmid">26057196</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villano</surname> <given-names>WJ</given-names></name><name><surname>Otto</surname> <given-names>AR</given-names></name><name><surname>Ezie</surname> <given-names>CEC</given-names></name><name><surname>Gillis</surname> <given-names>R</given-names></name><name><surname>Heller</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal dynamics of real-world emotion are more strongly linked to prediction error than outcome</article-title><source>Journal of Experimental Psychology: General</source><volume>149</volume><fpage>1755</fpage><lpage>1766</lpage><pub-id pub-id-type="doi">10.1037/xge0000740</pub-id><pub-id pub-id-type="pmid">32039625</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vrieze</surname> <given-names>E</given-names></name><name><surname>Pizzagalli</surname> <given-names>DA</given-names></name><name><surname>Demyttenaere</surname> <given-names>K</given-names></name><name><surname>Hompes</surname> <given-names>T</given-names></name><name><surname>Sienaert</surname> <given-names>P</given-names></name><name><surname>de Boer</surname> <given-names>P</given-names></name><name><surname>Schmidt</surname> <given-names>M</given-names></name><name><surname>Claes</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reduced reward learning predicts outcome in major depressive disorder</article-title><source>Biological Psychiatry</source><volume>73</volume><fpage>639</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2012.10.014</pub-id><pub-id pub-id-type="pmid">23228328</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilcox</surname> <given-names>RR</given-names></name><name><surname>Tian</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Comparing dependent correlations</article-title><source>The Journal of General Psychology</source><volume>135</volume><fpage>105</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.3200/GENP.135.1.105-112</pub-id><pub-id pub-id-type="pmid">18318411</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Berridge</surname> <given-names>KC</given-names></name><name><surname>Tindell</surname> <given-names>AJ</given-names></name><name><surname>Smith</surname> <given-names>KS</given-names></name><name><surname>Aldridge</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A neural computational model of incentive salience</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000437</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000437</pub-id><pub-id pub-id-type="pmid">19609350</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.57977.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Lee</surname><given-names>Daeyeol</given-names></name><role>Reviewing Editor</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Lee</surname><given-names>Daeyeol</given-names> </name><role>Reviewer</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>It has been previously shown that happiness or subjective well-being is related to the recency-weighted integration of reward prediction errors (RPE), rather than the magnitude of reward. However, whether reward magnitude or probability plays a more important role in determining happiness had not been tested. In the present study, the authors show that happiness was more reliably related to the probability prediction error (PPE) than RPE, and therefore a factor more important for learning than reward magnitude.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Mood dynamics depend on learning and not reward&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Daeyeol Lee as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>It has been shown previously that the subjective well-being (SWB, happiness) is correlated with the recency-weighted reward-prediction errors (RPE). In this manuscript, the authors showed that during a probabilistic learning task, the SWB is better accounted for by a set of recent probability prediction errors (PPE) than by reward prediction errors (RPE). The key goals of this paper were to (1) test additive and multiplicative models of choice, (2) assess the relationship of learning rate and volatility, and (3) and understand what governs &quot;happiness&quot; during choice and learning. Although RPE is more directly related to the income of the subject and also more closely related to the expected value that is presumably used during decision making, it is PPE that's relevant for learning. Therefore, this finding implies that the subjective mood or SWB might be more related to learning, rather than how much the desirability of the decision outcome deviated from the expected outcome. The experiments and analyses are carefully conducted, and the results are presented clearly.</p><p>Essential revisions:</p><p>1) The finding that probability (errors) but not RPEs mediate (or are related to) self-reports of happiness might be limited to the specific study design used. Imagine you enter a reward rich patch (ice cream) and you eat – your happiness may rise somewhat exponentially as a function of the amount of ice cream you can eat (especially if you think someone may come and take it away; what amount you &quot;got away with&quot; is probably a pretty good happiness indicator and is a dynamic-RPE related variable). Another example is one related to gambling. Imagine you make a gamble and loose. Depending on your state, context, and history, either RPEs or PPEs could be related to your &quot;happiness&quot;. If the authors agree, then it might be helpful to temper the claims and stating the limitations (e.g. how much do these results inform other contexts).</p><p>2) It might be helpful if the authors include a couple of additional models (in addition to models given by Equations 10 and 11) to examine whether SWB is related to variables other than the ones already tested in this study. For example, although this might have been shown in previous studies, the authors have another opportunity to test how strongly the SWB is related to the recency-weighted reward magnitude. In addition, how poorly do the models like this perform if they include the recency-weighted RPE, rather than PPE (e.g., EV deviations and RPE).</p><p>3) The authors justify concentrating on additive models of happiness based on their own previous work. This is not entirely convincing in this study, however. Equations 6-7 of happiness are additive. Have they tested alternative models? Also, related to model conceptualization, for happiness model with P + PPE, isn't this basically just counting wins – assuming forgetting factors are the same? Is this consistent with psychological work on happiness and mood modulation?</p><p>4) Reward-related variables are sometimes referred as &quot;choice-relevant variables&quot;, but its rationale should be explained more clearly. Why aren't the variables relevant for learning not relevant for choice?</p><p>5) When predicting happiness ratings, did authors use a random-effects or fixed-effect model? The relevant details are missing. If a random-effects model was not used, this should be used (instead of a fixed-effects model).</p><p>6) Authors claimed that &quot;depressive symptoms reduce happiness more in volatile than stable environments” (Abstract), but it is not clear whether this was tested statistically in the manuscript. Authors seem to have shown that average happiness significantly correlated with depressive symptoms only in the volatile environment. Authors should perform a direct and statistical comparison to support the conclusion (Niewenhius et al., 2011; Wilcox and Tian, 2008).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.57977.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The finding that probability (errors) but not RPEs mediate (or are related to) self-reports of happiness might be limited to the specific study design used. Imagine you enter a reward rich patch (ice cream) and you eat – your happiness may rise somewhat exponentially as a function of the amount of ice cream you can eat (especially if you think someone may come and take it away; what amount you &quot;got away with&quot; is probably a pretty good happiness indicator and is a dynamic-RPE related variable). Another example is one related to gambling. Imagine you make a gamble and loose. Depending on your state, context, and history, either RPEs or PPEs could be related to your &quot;happiness&quot;. If the authors agree, then it might be helpful to temper the claims and stating the limitations (e.g. how much do these results inform other contexts).</p></disp-quote><p>We agree with the reviewers that our finding that happiness preferentially depends on learning and not necessarily on reward may be related to specific features of the task design. In this case, we think the key relevant feature is that computing RPEs is not adaptive for learning in this environment because PPEs are the quantity required for learning. We agree with the reviewers that the variables that determine happiness should depend on state, context, and history and these are all important areas for future research. We have tempered our claims and stated the limitations in several sections. We have revised the text in the Results as follows:</p><p>“Our results suggest that although reward information influences choice, contrary to what would be predicted from the literature, RPEs and reward magnitudes do not explain happiness when this information is not necessary for participants to learn the structure of the environment. […] Learning and reward are dissociable in our paradigm, and we find in this context that RPEs and reward magnitudes do not explain happiness.”</p><p>We have revised the relevant text in the Discussion as follows:</p><p>“Previous studies using risky decision tasks where reward and probability were explicitly represented (Rutledge et al., 2014, 2015) showed that mood dynamics were explained by past expected values and RPEs. […] However, if learning the structure of the environment requires tracking changing reward magnitudes, we would expect that happiness would track learning-relevant variables (e.g., reward magnitudes and RPEs in such an environment).”</p><disp-quote content-type="editor-comment"><p>2) It might be helpful if the authors include a couple of additional models (in addition to models given by Equations 10 and 11) to examine whether SWB is related to variables other than the ones already tested in this study. For example, although this might have been shown in previous studies, the authors have another opportunity to test how strongly the SWB is related to the recency-weighted reward magnitude. In addition, how poorly do the models like this perform if they include the recency-weighted RPE, rather than PPE (e.g., EV deviations and RPE).</p></disp-quote><p>We previously reported in risky decision tasks (Rutledge et al., 2014, 2015) that recent reward magnitudes are not as good a predictor of SWB as recent RPEs. However, we agree that a thorough test of this possibility for our data set would be valuable, especially because this is the first study to describe SWB in a reinforcement learning context. We included the models described below in the Results. Interestingly, the win-loss model (which also does not incorporate reward magnitude) suggested by the reviewers in major point 3 has a similar overall BIC to the <inline-formula><mml:math id="inf40"><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mtext mathvariant="normal">PPE</mml:mtext><mml:mo accent="true">̂</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> model. Our findings related to the win-loss model strengthens our understanding of the differences between happiness in stable and volatile environments and we discuss these findings at length in response to major point 3 and in the revised manuscript. Briefly, happiness preferentially reflects learning the structure of the world and not reward magnitudes in both stable and volatile environments but not in a way that depends on model-derived expectations to the same degree. To provide a thorough test of models that incorporate reward magnitudes, the Results now include additional models and we have revised the text as follows:</p><p>“We next extended the model space with plausible alternative models. We included two models incorporating the history of reward magnitude. […] Comparison of weights across tasks therefore suggests a reduced impact of expectations on happiness as environmental volatility increases.”</p><p>The updated Table 2 includes information comparing all models tested.</p><disp-quote content-type="editor-comment"><p>3) The authors justify concentrating on additive models of happiness based on their own previous work. This is not entirely convincing in this study, however. Equations 6-7 of happiness are additive. Have they tested alternative models?</p></disp-quote><p>There are two terms in Equations 6 and 7: a constant term and the prediction error term (PPE and RPE, respectively). Our simple additive models already account for almost half of the variance in happiness ratings (mean r<sup>2</sup> = 0.36-0.50). However, because probabilities for consecutive trials are not independent, interaction terms could be important in reinforcement learning tasks for understanding how happiness changes over time.</p><p>We tested whether there was a non-linear effect of recent PPEs on happiness. To that end, we added to the PPE model a “boost” term when two consecutive PPEs have the same sign: two consecutive positive PPEs boost happiness (encoded as 1), whereas two consecutive negative PPEs decrease happiness (encoded as -1). This is in addition to the linear effects in our original model.</p><p><inline-formula><mml:math id="inf41"><mml:mrow><mml:mtext mathvariant="normal">Happiness</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mover><mml:mtext mathvariant="normal">PPE</mml:mtext><mml:mo accent="true">̂</mml:mo></mml:mover></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mover><mml:mrow><mml:mtext mathvariant="normal">PP</mml:mtext><mml:msub><mml:mi>E</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">boost</mml:mtext></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mtext mathvariant="normal">Boos</mml:mtext><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> Positive weights correspond to an enhanced PPE impact when the PPE has the same sign as the PPE on the previous trial. A negative weight corresponds to a reduced PPE impact when the sign of consecutive PPEs is the same. This boost weight was not significantly different from 0 in both the stable (0.19 ± 0.11, z = 0.40, p = 0.68) and volatile (0.21 ± 0.12, z = 0.60, p = 0.55) environments. Model comparison shows that including this additional term is not supported by the data.</p><table-wrap id="resptable1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td><inline-formula><mml:math id="inf42"><mml:mover><mml:mtext mathvariant="normal">PPE</mml:mtext><mml:mo accent="true">̂</mml:mo></mml:mover></mml:math></inline-formula></td><td>3</td><td>0.48</td><td>0.42</td><td>-640</td><td>-436</td><td>242</td><td>710</td></tr><tr><td><inline-formula><mml:math id="inf43"><mml:mover><mml:mtext mathvariant="normal">PPE</mml:mtext><mml:mo accent="true">̂</mml:mo></mml:mover></mml:math></inline-formula> + boost</td><td>4</td><td>0.53</td><td>0.47</td><td>-595</td><td>-408</td><td>287</td><td>737</td></tr></tbody></table></table-wrap><disp-quote content-type="editor-comment"><p>Also, related to model conceptualization, for happiness model with P + PPE, isn't this basically just counting wins – assuming forgetting factors are the same? Is this consistent with psychological work on happiness and mood modulation?</p></disp-quote><p>The effect of expectations on happiness have been reported in multiple studies (Rutledge et al., 2014, 2015). We have revised the Introduction to report some additional psychological studies that find consistent results across several labs:</p><p>“Emotions are widely believed to play a role in adaptive behaviour (Fredrickson, 2004), but no computational framework exists to link them. […] Mood has been proposed to represent environmental momentum, whether an environment is getting better or worse, which could be a useful variable for adaptive behaviour (Eldar et al., 2016; Eldar and Niv, 2015).”</p><p>Evaluating an alternative win-loss model that tests whether participants are, in a sense, counting wins and losses led to some intriguing results. We believe that these results provide a more complete picture of the relationship between happiness and reinforcement learning, particularly in volatile environments, and we appreciate the suggestion. We have added the following text to the Results:</p><p>“We also asked whether the history of wins (excluding any information about reward magnitude) and losses could account for happiness by fitting the following model:<disp-formula id="equ18"><label>(16)</label><mml:math id="m18"><mml:mrow><mml:mtext mathvariant="normal">Happiness</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">win</mml:mtext></mml:msub><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mtext mathvariant="normal">wi</mml:mtext><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">loss</mml:mtext></mml:msub><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mtext mathvariant="normal">los</mml:mtext><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>[…] Learning and reward are dissociable in our paradigm, and we find in this context that RPEs and reward magnitudes do not explain happiness.”</p><disp-quote content-type="editor-comment"><p>4) Reward-related variables are sometimes referred as &quot;choice-relevant variables&quot;, but its rationale should be explained more clearly. Why aren't the variables relevant for learning not relevant for choice?</p></disp-quote><p>This is correct. The variables relevant for learning are also relevant for choice but the converse is not correct. We aimed to find a dissociation between learning-relevant variables, which are indeed necessary to make the best decisions to maximise income, and learning-irrelevant variables that are not relevant to learning but are relevant to make the best decisions to maximise income. In particular, the RPE is a learning-irrelevant variable because computation of this quantity is not part of the learning models that describe behaviour. Similarly, EV is a learning-irrelevant variable because, although it is related to choice, it is not part of the learning process.</p><p>We agree that the term ‘choice relevant’ is inaccurate and misleading, and we have replaced the term with ‘learning irrelevant’.</p><disp-quote content-type="editor-comment"><p>5) When predicting happiness ratings, did authors use a random-effects or fixed-effect model? The relevant details are missing. If a random-effects model was not used, this should be used (instead of a fixed-effects model).</p></disp-quote><p>We agree with the reviewers that a fixed-effects model is not appropriate for these analyses. Details have been added to the Materials and methods to explain our model-fitting procedure more accurately, which corresponds to using random-effects models:</p><p>“All models were fitted to experimental data by minimizing the negative log likelihood of the predicted choice probability given different model parameters using the fmincon function in MATLAB (Mathworks Inc). […] We used standard model comparison techniques (Burnham and Anderson, 2004; Schwarz, 1978) to compare model fits.”</p><disp-quote content-type="editor-comment"><p>6) Authors claimed that &quot;depressive symptoms reduce happiness more in volatile than stable environments” (in Abstract), but it is not clear whether this was tested statistically in the manuscript. Authors seem to have shown that average happiness significantly correlated with depressive symptoms only in the volatile environment. Authors should perform a direct and statistical comparison to support the conclusion (Niewenhius et al., 2011; Wilcox and Tian, 2008).</p></disp-quote><p>Originally, we tested for this difference by correlating the difference in the happiness baseline parameter between the stable and volatile conditions with PHQ score which quantifies depressive symptoms, without standardising. This relationship was shown in Figure 6 (right panel) and was significant (Spearman’s ρ(73) = -0.32, p &lt; 0.01). However, according to the more conservative approach described in one of papers mentioned, Wilcox and Tian, 2008, the variables should be standardised. Standardising the variables also leads to a negative correlation between the difference between volatile and stable happiness parameters and depressive symptoms (Spearman’s ρ(73) = -0.28, p = 0.014). Similar results are found for the baseline mood parameter after standardising the variables (Spearman’s ρ(73) = -0.32, p = 0.0049). We have added this result to the manuscript:</p><p>“In the volatile environment, where uncertainty is high and volatility is high, average happiness was correlated with depressive symptoms, with lower happiness associated with higher depressive symptoms (Spearman’s ρ(73) = -0.23, p = 0.043; Figure 6A, central panel). […] Baseline mood parameters estimated using our happiness model fit to non-z-scored happiness ratings showed the same relationship to depressive symptoms (stable: Spearman’s ρ(73) = -0.07, p = 0.54; volatile: Spearman’s ρ(73) = -0.28, p = 0.017; volatile – stable, standardised: Spearman’s ρ(73) = -0.32, p = 0.0049; see Figure 6B).”</p><p>We have explained this analysis in the revised Materials and methods:</p><p>“Two-sided Wilcoxon signed rank tests were used to compare performance, proportion of win-stay/lose-shift, and model parameters between environments at the group level. […] To test whether the correlation between PHQ score and happiness baseline parameters was higher in the volatile condition than in the stable condition, we correlated the standardised difference between the happiness baseline parameter between the stable and volatile conditions with the standardised PHQ score which quantifies depressive symptoms (Wilcox and Tian, 2008).”</p></body></sub-article></article>