<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">102391</article-id><article-id pub-id-type="doi">10.7554/eLife.102391</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102391.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Hierarchical Bayesian modeling of multiregion brain cell count data</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Dimmock</surname><given-names>Sydney</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0163-2048</contrib-id><email>sd14814.2014@my.bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Exley</surname><given-names>Benjamin MS</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Moore</surname><given-names>Gerald</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Menage</surname><given-names>Lucy</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Delogu</surname><given-names>Alessio</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4414-4714</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Schultz</surname><given-names>Simon R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6794-5813</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Warburton</surname><given-names>E Clea</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2129-2060</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Houghton</surname><given-names>Conor J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5017-9473</contrib-id><email>conor.houghton@bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>O'Donnell</surname><given-names>Cian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2031-9177</contrib-id><email>c.odonnell2@ulster.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>School of Engineering Mathematics and Technology, University of Bristol, Michael Ventris Building</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>School of Physiology, Pharmacology and Neuroscience, University of Bristol, Biomedical Sciences Building, University Walk</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Centre for Neurotechnology and Department of Bioengineering, Imperial College London, South Kensington</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220mzb33</institution-id><institution>Department of Basic and Clinical Neuroscience, Institute of Psychiatry, Psychology and Neuroscience, King’s College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yp9g959</institution-id><institution>School of Computing, Engineering and Intelligent Systems, Ulster University</institution></institution-wrap><addr-line><named-content content-type="city">Derry~Londonderry</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>11</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP102391</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-08-20"><day>20</day><month>08</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-07-21"><day>21</day><month>07</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.20.603979"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-25"><day>25</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102391.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-09-18"><day>18</day><month>09</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102391.2"/></event></pub-history><permissions><copyright-statement>© 2024, Dimmock et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Dimmock et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-102391-v1.pdf"/><abstract><p>We can now collect cell-count data across whole animal brains quantifying recent neuronal activity, gene expression, or anatomical connectivity. This is a powerful approach since it is a multiregion measurement, but because the imaging is done postmortem, each animal only provides one set of counts. Experiments are expensive, and since cells are counted by imaging and aligning a large number of brain sections, they are time-intensive. The resulting datasets tend to be undersampled with fewer animals than brain regions. As a consequence, these data are a challenge for traditional statistical approaches. We present a ‘standard’ partially pooled Bayesian model for multiregion cell-count data and apply it to two example datasets. These examples demonstrate that hierarchical Bayesian methods are well suited to these data. In both cases, the Bayesian model outperformed standard parallel <italic>t</italic>-tests. Overall, inference for cell-count data is substantially improved by the ability of the Bayesian approach to capture nested data and by its rigorous handling of uncertainty in undersampled data.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Bayesian analysis</kwd><kwd>cell-count data</kwd><kwd>hierarchical modeling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0439y7842</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/R513179/1</award-id><principal-award-recipient><name><surname>Dimmock</surname><given-names>Sydney</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0439y7842</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/W024020/1</award-id><principal-award-recipient><name><surname>Schultz</surname><given-names>Simon R</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cwqg982</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/L02134X/1</award-id><principal-award-recipient><name><surname>Warburton</surname><given-names>E Clea</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cwqg982</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/R007020/1</award-id><principal-award-recipient><name><surname>Delogu</surname><given-names>Alessio</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029chgv08</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>206401/Z/17/Z</award-id><principal-award-recipient><name><surname>Warburton</surname><given-names>E Clea</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/012mzw131</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RF-2021-533</award-id><principal-award-recipient><name><surname>Houghton</surname><given-names>Conor J</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03x94j517</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/S026630/1</award-id><principal-award-recipient><name><surname>O'Donnell</surname><given-names>Cian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Bayesian hierarchical models offer powerful statistical tools for neuroscientists to analyze whole-brain cell count data, as demonstrated here using two example datasets from different laboratories.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In studying the brain, we are often confronted with phenomena that involve specific subsets of neurons distributed across many brain regions. Computations, for example, are performed by neuronal networks connecting cells in different parts of the brain. As another example, from development, neurons in different anatomical regions of the brain share the same lineage. Data for each of these types of experiment will be considered here, but the challenge is very general: how to measure and analyze multiregion neuronal data with cellular resolution.</p><p>In a typical cell-count experiment, gene expression is used to tag the specific cells of interest with a targeted indicator (<xref ref-type="bibr" rid="bib34">Kawashima et al., 2014</xref>). The brain is sliced, an entire stack of brain sections from a single animal is imaged, and the images are aligned and registered to a standardized brain atlas such as the Allen mouse atlas (<xref ref-type="bibr" rid="bib37">Lein et al., 2007</xref>; <xref ref-type="bibr" rid="bib44">Oh et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Daigle et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Harris et al., 2019</xref>), the images are segmented into anatomical regions, and the labeled cells in each region are counted. The resulting dataset consists of labeled cell counts across each of ∼10–100 brain regions. This technology is being deployed to address questions in a broad range of neuroscience subfields, e.g.: memory (<xref ref-type="bibr" rid="bib36">Kim and Cho, 2017</xref>; <xref ref-type="bibr" rid="bib28">Haubrich and Nader, 2023</xref>; <xref ref-type="bibr" rid="bib18">Dorst et al., 2024</xref>), neurodegenerative disorders (<xref ref-type="bibr" rid="bib39">Liebmann et al., 2016</xref>), social behavior (<xref ref-type="bibr" rid="bib35">Kim et al., 2015</xref>), and stress (<xref ref-type="bibr" rid="bib6">Bonapersona et al., 2022</xref>).</p><p>Cell counts are often compared across groups of animals which differ by an experimental condition such as drug treatment, genotype, or behavioral manipulation. However, the expense and difficulty of the experiment mean that the number of animals in each group is often small. Ten is a typical number of samples for these experiments, but fewer is not uncommon. This means that these data are undersampled: the dimensionality of the data, which corresponds to the number of brain regions, is much larger than the number of samples, which usually corresponds to the number of animals (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Current statistical methods are not well suited to these nested wide-but-shallow datasets. Furthermore, because of the complicated preparation and imaging procedure, there is often missing data along with variability derived from experimental artifacts.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Introduction.</title><p>(<bold>A</bold>) Each of <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mi>N</mml:mi></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$  N$\end{document}</tex-math></alternatives></inline-formula> animals produces a cell count from a total of <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mi>R</mml:mi></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$  R$\end{document}</tex-math></alternatives></inline-formula> brain regions of interest. Cell-count data is typically undersampled with <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mi>N</mml:mi><mml:mo>≪</mml:mo><mml:mi>R</mml:mi></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$  N\ll R$\end{document}</tex-math></alternatives></inline-formula>. Scientists analyze the brain sections from the experiment for positive signals. Here, an example section is shown where teal points mark cells expressing the immediate early gene c-Fos (green and red lines indicate regions labeled as damaged). The final cell count is equal to the sum of these individual items sagittal brain map taken from the Allen mouse brain atlas: <ext-link ext-link-type="uri" xlink:href="https://mouse.brain-map.org">https://mouse.brain-map.org</ext-link>. (<bold>B</bold>) Partial pooling is a hierarchical structure that jointly models observations from some shared population distribution. It is a continuum that depends on the value of the population variance <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mi>τ</mml:mi></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$  \tau$\end{document}</tex-math></alternatives></inline-formula>. When <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$  \tau=0$\end{document}</tex-math></alternatives></inline-formula>, there is no variation in the population, and each individual observation is modeled as a conditionally independent estimate of some fixed population mean <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mi>θ</mml:mi></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$  \theta$\end{document}</tex-math></alternatives></inline-formula> (complete pooling). As <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mi>τ</mml:mi></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$  \tau$\end{document}</tex-math></alternatives></inline-formula> tends to infinity, observations do not combine inferential strength but inform an independent estimate <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> (no pooling). In between the two extremes, combine. Each observation can contribute to the population estimate while simultaneously supporting a local one to effectively model the variance in the data. The observed data quantities, <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$  y_{i}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$  y_{n}$\end{document}</tex-math></alternatives></inline-formula>, are highlighted with a thick line in the model diagrams. (<bold>C</bold>) An example of partial pooling on simulated count data. As the population standard deviation increases on the <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mi>x</mml:mi></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$  x$\end{document}</tex-math></alternatives></inline-formula>-axis, the individual estimates <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$  \exp(\gamma_{i})$\end{document}</tex-math></alternatives></inline-formula> trace a path from a completely pooled estimate to an unpooled estimate. Circular points give the raw data values. Parameters are exponentiated because the outcomes are Poisson and so parameters are fit on the log scale.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-fig1-v1.tif"/></fig><p>In cell count data, there are two obvious sources of noise. The first of these is easy to describe: if a region has a rate that determines how likely a cell is to be marked for counting, then the actual number of marked cells is sampled from a Poisson distribution. The second source of noise is the animal-to-animal variability of the rate itself, and this depends on diverse features of the individual animal and the experiment that are often unrelated to the phenomenon of interest. The challenge is to control for outliers and ‘poor’ data points whose rate is noisy, while extracting as much information as possible about the underlying process. Dealing with outliers is often an opaque and ad hoc procedure. It is also a binary decision, a point is either excluded, so it does not contribute, or included, noise and all. This is where partial pooling helps. Partial pooling allows for the simultaneous estimation of parameters describing individual data points and parameters describing populations. This helps the data to self-regularize and elegantly balances the contribution of informative and weak observations to parameter values (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Methods.</title><p>A table of partial pooling behavior for different likelihood and prior combinations. Rows are the two prior choices for the population distribution, and columns the two distributions for the data. Within each cell, the expectation of the marginal posterior <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$  p(\exp(\gamma_{i})|\theta,\tau,y)$\end{document}</tex-math></alternatives></inline-formula> is plotted as a function of <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mi>τ</mml:mi></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$  \tau$\end{document}</tex-math></alternatives></inline-formula>. The solid black line is the expectation of the marginal posterior <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$  p(\theta|\tau,y)$\end{document}</tex-math></alternatives></inline-formula> with one standard deviation highlighted in gray. Top left: Combining a normal prior for the population with a Poisson likelihood is unsatisfactory in the presence of a zero observation. The zero observations influence the population mean in an extreme way owing to their high importance under the Poisson likelihood. Bottom left: By changing to a horseshoe prior, the problematic zero observations can escape the regularization machinery. However, regularization of the estimates with positive observations is much less impactful. Top right: A zero-inflated Poisson likelihood accounts for the zero observations with an added process, reducing the burden on the population estimate to compromise between extreme values. Bottom right: No model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-fig2-v1.tif"/></fig><p>In recent years, Bayesian approaches to data analysis have become powerful alternatives to classical frequentist approaches (<xref ref-type="bibr" rid="bib24">Gelman et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">McElreath, 2018</xref>; <xref ref-type="bibr" rid="bib46">van de Schoot et al., 2021</xref>). They have been applied to some types of neuroscience data, including neurolinguistics (<xref ref-type="bibr" rid="bib16">Dimmock et al., 2023</xref>), neural coding (<xref ref-type="bibr" rid="bib7">Brown et al., 1998</xref>; <xref ref-type="bibr" rid="bib48">Zhang et al., 1998</xref>), synaptic parameters (<xref ref-type="bibr" rid="bib42">Moran et al., 2008</xref>; <xref ref-type="bibr" rid="bib12">Costa et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Bird et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Bykowska et al., 2019</xref>), and neuronal-circuit connectivity (<xref ref-type="bibr" rid="bib41">Mishchenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib11">Cinotti and Humphries, 2022</xref>). A Bayesian approach is particularly well suited to cell-count data but has not previously been applied to this problem.</p><p>A Bayesian approach formalizes the process of scientific inference; it distinguishes the data and a probabilistic mathematical model of the data. This model has a likelihood which gives the probability of the observed data for a given set of model parameters. The model often has a hierarchical structure which we compose to reflect the structure of the experiment and the investigators’ hypothesis of how the data depends on experimental condition. This hierarchy determines a set of a priori probabilities for the parameter values. The result of Bayesian inference is a probability distribution for these model parameters given the data, termed the posterior.</p><p>There are three advantages of a Bayesian approach that we want to emphasize: (1) while traditional multilevel models also allow a hierarchy (<xref ref-type="bibr" rid="bib1">Aarts et al., 2014</xref>), Bayesian models are more flexible and the role of the model is clearer, (2) since the result of Bayesian inference is a probability distribution over model parameters, it indicates not just the fitted value of a parameter but the uncertainty of the parameter value. Finally, (3) Bayesian models tend to make more efficient use of data and therefore improve statistical power.</p><p>A Bayesian model also includes a set of probability distributions, referred to as the prior, which represent those beliefs it is reasonable to hold about the statistical model parameters before actually doing the experiment. The prior can be thought of as an advantage; it allows us to include in our analysis our understanding of the data based on previous experiments. The prior also makes explicit in a Bayesian model assumptions that are often implicit in other approaches. However, having to design priors is often considered a challenge, and here we hope to make this more straightforward by suggesting priors that are suitable for this class of data.</p><p>Here, our aim is to introduce a ‘standard’ Bayesian model for cell-count data. We illustrate the application of this model to two datasets, one related to neural activation and the other to developmental lineage. For the second dataset, we also demonstrate a second example extension Bayesian model. In all cases, the Bayesian models produce clearer results than the classical frequentist approach.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Data</title><p>To illustrate our approach, we consider two example applications, one which counts cells active in regions of the recognition memory circuit of rats during a familiarity discrimination task, and the other which examines the distribution of a specific interneuron type in the mouse thalamus.</p><sec id="s2-1-1"><title>Case study 1 - Transient neural activity in the recognition memory circuit</title><p>The recognition memory network (<xref ref-type="fig" rid="fig3">Figure 3</xref>) is a distributed network which has been well studied using a variety of behavioral tasks. It includes the hippocampus (HPC) and perirhinal cortex (PRH), shown to deal with object spatial recognition and familiarity discrimination, respectively (<xref ref-type="bibr" rid="bib2">Barker and Warburton, 2011</xref>; <xref ref-type="bibr" rid="bib19">Ennaceur et al., 1996</xref>; <xref ref-type="bibr" rid="bib43">Norman and Eacott, 2004</xref>); medial prefrontal cortex (MPFC), concerned with executive functions such as decision-making but also with working memory, and the temporal association cortex (TE2) used for acquisition and retrieval of long-term object-recognition memories (<xref ref-type="bibr" rid="bib29">Ho et al., 2011</xref>). The nucleus reuniens (NRe) has reciprocal connectivity to both MPFC and HPC (<xref ref-type="bibr" rid="bib31">Hoover and Vertes, 2012</xref>) and for this reason, it is also believed to be an important component of the circuit (<xref ref-type="bibr" rid="bib3">Barker and Warburton, 2018</xref>; <xref ref-type="bibr" rid="bib2">Barker and Warburton, 2011</xref>). In previous studies, lesions of the NRe have been shown to significantly impair long-term but not short-term object-in-place recognition memory (<xref ref-type="bibr" rid="bib3">Barker and Warburton, 2018</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Recognition memory circuit.</title><p>Schematic of the recognition memory network adapted from <xref ref-type="bibr" rid="bib20">Exley, 2019</xref>. Bold arrows show the assumed two-way connection between the medial prefrontal cortex and the hippocampus facilitated by the nucleus reuniens (NRe). Colors highlight the hippocampus (HPC) (red), MPC (blue), and specific areas of the rhinal cortex (yellow). The NRe was lesioned in the experiment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-fig3-v1.tif"/></fig><p>The data analyzed in this case study were collected to investigate the role of the NRe in the recognition memory circuit through contrasting the neural activation for animals with a lesion in the NRe with neural activation for animals with a sham surgery. The immediate early gene c-Fos is rapidly expressed following strong neural activation and is useful as a marker of transient neural activity. Animals in the experiment performed a familiarity discrimination task (single-item recognition memory), discriminating novel or familiar objects with or without an NRe lesion, and the number of cells that expressed c-Fos was counted in regions across the recognition memory circuit. The two-by-two experimental design allocated animals to each of the four experiment groups <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mtext>sham</mml:mtext><mml:mo>,</mml:mo><mml:mtext>lesion</mml:mtext><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>×</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mtext>novel</mml:mtext><mml:mo>,</mml:mo><mml:mtext>familiar</mml:mtext><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$  \{\text{sham},\text{lesion}\}\times\{\text{novel},\text{familiar}\}$\end{document}</tex-math></alternatives></inline-formula>, and cell counts were recorded from a total of 23 brain regions. The visual cortex V2C and the motor cortex M2C were taken as control regions as they were not expected to show differential c-Fos expression in response to novel or familiar objects (<xref ref-type="bibr" rid="bib20">Exley, 2019</xref>).</p></sec><sec id="s2-1-2"><title>Case study 2 - Ontogeny of inhibitory neurons in mouse thalamus and hypothalamus</title><p>The second dataset comes from a study (<xref ref-type="bibr" rid="bib33">Jager et al., 2021</xref>) that, in part, counted the number of inhibitory interneurons in the thalamocortical regions of mouse. Sox14 is a gene associated with inhibitory neurons in subcortical areas. It is required for the development and migration of local inhibitory interneurons in the dorsal lateral geniculate nucleus (LGd) of the thalamus (<xref ref-type="bibr" rid="bib33">Jager et al., 2021</xref>; <xref ref-type="bibr" rid="bib32">Jager et al., 2016</xref>). Consequently, Sox14 is useful for identifying discrete neuronal populations in the thalamus and hypothalamus (<xref ref-type="bibr" rid="bib26">Golding et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Jager et al., 2016</xref>).</p><p>The experiment compared heterozygous (HET) and knockout (KO) mouse lines. The HET knock-in mouse line <inline-formula><alternatives><mml:math id="inf17"><mml:msup><mml:mtext mathvariant="italic">Sox14</mml:mtext><mml:mtext mathvariant="italic">GFP/+</mml:mtext></mml:msup></mml:math><tex-math id="inft17">\begin{document}$\textsl{Sox14}^{\textsl{GFP/+}}$\end{document}</tex-math></alternatives></inline-formula> marked Sox14-expressing neurons with green fluorescent protein (GFP); the homozygous KO mouse line <inline-formula><alternatives><mml:math id="inf18"><mml:msup><mml:mtext mathvariant="italic">Sox14</mml:mtext><mml:mtext mathvariant="italic">GFP/GFP</mml:mtext></mml:msup></mml:math><tex-math id="inft18">\begin{document}$\textsl{Sox14}^{\textsl{GFP/GFP}}$\end{document}</tex-math></alternatives></inline-formula>, in contrast, was engineered to block the expression of the endogenous Sox14 coding sequence (<xref ref-type="bibr" rid="bib15">Delogu et al., 2012</xref>). Each animal produced two samples, one for each hemisphere. In total, there are ten data points, six belonging to HET (three animals) and four to KO (two animals). Each observation is 50-dimensional, corresponding to 50 individual brain regions in each hemisphere.</p></sec></sec><sec id="s2-2"><title>Hierarchical modeling</title><p>Our goal in both cases is to quantify group differences in the data. We present a ‘standard’ hierarchical model. This model reflects the experimental features common to cell count experiments and reflects the hierarchical structure of cell-count data; the standard model is designed to deal robustly and efficiently with noise. On some occasions, to reflect a specific hypothesis, the structure of a particular experiment or an observed source of noise, this model can be further refined or changed to target the analysis. We will give an example of this for our second dataset.</p><p>At the bottom of the model are the data themselves, the cell counts <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$  y_{i}$\end{document}</tex-math></alternatives></inline-formula>. The index <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle><mml:mi>i</mml:mi></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$  i$\end{document}</tex-math></alternatives></inline-formula> runs over the full set of samples, which in this case comprises 23 brain regions × animals × groups ≈920 datapoints in the first study, and 50 brain regions × HET animals + brain regions × KO animals ≈ 500 datapoints in the second. The basic assumption the model makes is that this count is derived from an underlying propensity, <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$  \lambda_{i} \gt 0$\end{document}</tex-math></alternatives></inline-formula>, which depends on brain region and, potentially, group:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  y_i\sim \dist{Poisson}(\lambda_i)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Hence, the propensity <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$  \lambda_{i}$\end{document}</tex-math></alternatives></inline-formula> is the mean of the Poisson distribution, and a statistical model is used to describe the dependence of this parameter on brain region and animal. Since <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$  \lambda_{i}$\end{document}</tex-math></alternatives></inline-formula> is strictly positive, a log-link function is introduced:<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  \log{\lambda_i} = \theta_{r[i],g[i]}+\gamma_{i} + E_i $$\end{document}</tex-math></alternatives></disp-formula></p><p>where we have used ‘array notation’ (<xref ref-type="bibr" rid="bib23">Gelman and Hill, 2006</xref>), mapping the sample index <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle><mml:mi>i</mml:mi></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$  i$\end{document}</tex-math></alternatives></inline-formula> to properties of the sample, so <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$  r[i]$\end{document}</tex-math></alternatives></inline-formula> returns the region index of observation <inline-formula><alternatives><mml:math id="inf26"><mml:mstyle><mml:mi>i</mml:mi></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$  i$\end{document}</tex-math></alternatives></inline-formula>, and similar for <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$  g[i]$\end{document}</tex-math></alternatives></inline-formula> but for groups and animals. The sample-by-sample variability is given by <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula>; this is modeled as Gaussian noise:<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  \gamma_{i} \sim \dist{Normal}(0, \tau_{r[i],g[i]})$$\end{document}</tex-math></alternatives></disp-formula></p><p>whose size depends on region and group. This equation demonstrates a potentially surprising aspect of partially pooled models: the over-parameterization.</p><p>Ignoring <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$  E_{i}$\end{document}</tex-math></alternatives></inline-formula> for now, the rate has been split between two terms: <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$  \theta_{r[i],g[i]}$\end{document}</tex-math></alternatives></inline-formula> is the fixed effect, which is constant across animals, and <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$  \gamma_{r[i],a[i]}$\end{document}</tex-math></alternatives></inline-formula> is the random effect, which captures the animal to animal variability. While <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$  \theta_{rg}$\end{document}</tex-math></alternatives></inline-formula> models the mean for log cell count for each region, given the condition; <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> models variation around this mean. For this reason, <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> is assumed to follow a normal distribution with zero mean. The regression term may appear over-parameterized, without <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$  \theta_{rg}$\end{document}</tex-math></alternatives></inline-formula> the <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> could ‘do the work’ of matching the data. However, the model is regularized by a prior; observations with a weak likelihood will have their random effect <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> shrunk toward the population location. The amount of regularization depends on the variation in the population, a quantity that is estimated from each likelihood. This is how partial pooling works as an adaptive prior for ‘similar’ parameters (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The data ‘pools’ some evidence while still allowing for individual differences in samples.</p><p>The final term is the exposure <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$  E_{i}$\end{document}</tex-math></alternatives></inline-formula>. Cell counts may be recorded from sections with different areas. The exposure term scales the parameters in the linear model as the recording area increases (<xref ref-type="bibr" rid="bib40">McElreath, 2018</xref>). In our model, the exposure is equal to the logarithm of the recording area; this value is available as part of the experimental data.</p><p>The set of parameters <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$  \tau_{r,g}$\end{document}</tex-math></alternatives></inline-formula> models the population standard deviations of the noise for each region <inline-formula><alternatives><mml:math id="inf40"><mml:mstyle><mml:mi>r</mml:mi></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}$  r$\end{document}</tex-math></alternatives></inline-formula> and animal group <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mi>g</mml:mi></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$  g$\end{document}</tex-math></alternatives></inline-formula>. When working on the log scale, priors for these parameters are typically derived in terms of multiplicative increases. Since the parameters are positive, they are assigned a half-normal distribution<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle  \tau_{r,g} \sim \dist{HalfNormal}(\log(s))$$\end{document}</tex-math></alternatives></disp-formula></p><p>with an appropriately chosen scale <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$  s \gt 1$\end{document}</tex-math></alternatives></inline-formula>. For our analyses, we used <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1.05</mml:mn></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$  s=1.05$\end{document}</tex-math></alternatives></inline-formula> because this gives a HalfNormal distribution with 95% of its density in the interval <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1.1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$  [0,\log(1.1)]$\end{document}</tex-math></alternatives></inline-formula>. This translates into an approximate 10% variation around <inline-formula><alternatives><mml:math id="inf45"><mml:mstyle><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft45">\begin{document}$  \exp(\theta)$\end{document}</tex-math></alternatives></inline-formula> at the upper end, which is a moderately informative prior, reflecting our belief that within-group animal variability is small relative to between-group variability. This regularization also helps model inference when the datasets are undersampled. <xref ref-type="table" rid="table1">Table 1</xref> gives a reference for all the model parameters.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameter table for the hierarchical model.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$E_{i}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">Exposure</td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf47"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$\kappa_{i}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">Horseshoe inflation.</td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf48"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>π</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$\pi$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">Zero inflation</td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf49"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$\gamma_{i}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">Random effect for observation<italic>i</italic></td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf50"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$\theta_{rg}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">Fixed effect for region<italic>r</italic><break/> in group<italic>g</italic></td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf51"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$\tau_{rg}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">Scale of random effects for region<italic>r</italic><break/> in groupg</td></tr></tbody></table></table-wrap></sec><sec id="s2-3"><title>Horseshoe prior</title><p>Cell-count data often has outliers, for example, due to experimental artifacts. Since by default, the likelihood does not account for these outliers, they may cause substantial changes in fitted parameter values. This is demonstrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>, where a careless application of the Poisson distribution on data with several zero counts has a large influence on the posterior distribution. There are two general options for dealing with outliers: either modeling them in the likelihood or in the prior. Although the likelihood option is preferred as it is more direct - see our zero inflation model below - it can be hard to design because it requires knowledge of the outlier generation process. The alternative is via a flexible prior such as the horseshoe (<xref ref-type="bibr" rid="bib10">Carvalho et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Piironen and Vehtari, 2017</xref>). This more generic option may be suitable as a default ‘standard’ approach in the typical case where outliers are poorly understood.</p><p>The horseshoe prior is a hierarchical prior for sparsity. It introduces an auxiliary parameter <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle><mml:msub><mml:mi>κ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$  \kappa_{i}$\end{document}</tex-math></alternatives></inline-formula> that multiplies the population scale <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle><mml:mi>τ</mml:mi></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$  \tau$\end{document}</tex-math></alternatives></inline-formula>. This construction allows surprising observations far from the bulk of the population density to escape regularization.<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  \gamma_i \sim \dist{Normal}(0, \tau_{r[i],g[i]} \times \kappa_i)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle \newcommand{\dist}[1]{\mathrm{#1}} \kappa_i &amp;\sim \dist{HalfNormal}(1). \label{eq:Horseshoe}$$\end{document}</tex-math></alternatives></disp-formula></p><p>An example of this is given in <xref ref-type="fig" rid="fig2">Figure 2</xref> as the bottom left cell of the <inline-formula><alternatives><mml:math id="inf54"><mml:mstyle><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$  2\times 2$\end{document}</tex-math></alternatives></inline-formula> table of models. The horseshoe prior often uses a Cauchy distribution, but in our case, the heavy tail causes problems for the sampling algorithm (see Appendix 1: Horseshoe densities).</p></sec><sec id="s2-4"><title>Zero inflation</title><p>A particular trait of the second dataset is that there are a large number of zero data points (<inline-formula><alternatives><mml:math id="inf55"><mml:mstyle><mml:mo>∼</mml:mo><mml:mn>6</mml:mn></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$  \sim 6$\end{document}</tex-math></alternatives></inline-formula>%). Although a zero observation is always possible for a Poisson distribution, for plausible values of the propensity, zeros should be rare. It is likely that for some regions, the experiment has not worked as expected, and the zeros show that something has ‘gone wrong’ and that the readings are not well described by a Poisson distribution. Here, we extend the model to include this possibility. This is a useful elaboration of the standard model. In the standard model, the horseshoe prior ensures that these anomalous readings only have a small effect on the result, but it is more informative to extend the model to include them. While this particular extension is specific to these data, it also serves as an example of how a standard Bayesian model can serve as a starting point for an iterative investigation of the data.</p><p>The zero-inflated Poisson model is intended to model a situation where there are zeros unrelated to the Poisson distribution. In this case, this might, for example, be the result of an error in the automated registration process that identifies regions and counts their cells. It is a mixture model if<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  y_i \sim \mathrm{ZIPoisson}(\pi,\lambda_i)$$\end{document}</tex-math></alternatives></disp-formula></p><p>There is a probability <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle><mml:mi>π</mml:mi></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$  \pi$\end{document}</tex-math></alternatives></inline-formula> that <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}$  y_{i}=0$\end{document}</tex-math></alternatives></inline-formula> and a <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$  1-\pi$\end{document}</tex-math></alternatives></inline-formula> probability that <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$  y_{i}$\end{document}</tex-math></alternatives></inline-formula> follows a Poisson distribution with rate <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$  \lambda_{i}$\end{document}</tex-math></alternatives></inline-formula>. Importantly, this means there are two ways in which <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$  y_{i}$\end{document}</tex-math></alternatives></inline-formula> can be zero, through the Bernoulli process parameterized by <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle><mml:mi>π</mml:mi></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$  \pi$\end{document}</tex-math></alternatives></inline-formula> or through the Poisson distribution. This has the effect of ‘inflating’ the probability mass at zero with the additional parameter <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle><mml:mi>π</mml:mi></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$  \pi$\end{document}</tex-math></alternatives></inline-formula> giving the proportion of extra zeros in the data that could not be explained by the standard Poisson distribution. This distribution can be visualized in <xref ref-type="fig" rid="fig2">Figure 2</xref>, and further mathematical details are described in Appendix 1: Distributions.</p></sec><sec id="s2-5"><title>Model inference</title><p>Posterior inference was performed with the probabilistic programming language Stan (<xref ref-type="bibr" rid="bib9">Carpenter et al., 2017</xref>), using its custom implementation of the No-U-Turn (NUTS) sampler (<xref ref-type="bibr" rid="bib4">Betancourt, 2016</xref>; <xref ref-type="bibr" rid="bib30">Hoffman and Gelman, 2014</xref>). For each model, the posterior was sampled using four chains for 8000 iterations, with half of these being attributed to the warm-up phase. This gives a total of 16,000 samples from the posterior distribution.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>We describe differences in estimated counts between groups in terms of log<sub>2</sub>-fold changes. Fold changes are useful because they prevent differences that are small in absolute magnitude from being masked by regions with high overall expression. Our results compare Bayesian highest density intervals (HDIs) with the confidence interval (CI) from an uncorrected Welch’s <italic>t</italic>-test. The Bayesian HDI is calculated from the posterior distribution and is the smallest width interval that includes a chosen probability, here 0.95 (to correspond to <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$  \alpha=0.05$\end{document}</tex-math></alternatives></inline-formula>), and summarizes the meaningful uncertainty over a parameter of interest.</p><sec id="s3-1"><title>Case study 1 - Transient neural activity in the recognition memory circuit</title><p>Results for the first dataset are presented in <xref ref-type="fig" rid="fig4">Figure 4</xref>. <xref ref-type="fig" rid="fig4">Figure 4A</xref> plots cell-count differences between the novel and familiar conditions without lesion and <xref ref-type="fig" rid="fig4">Figure 4B</xref> with lesion. These data were collected to investigate the role of different hippocampal and adjacent cortical regions in memory. However, some regions of interest, such as the intermediate dentate gyrus (IDG) and the dorsal subiculum (DSUB), look underpowered: for both regions, there is a markedly nonzero difference in expression between the novel and familiar conditions in the sham animals, but a wide CI overlapping zero makes the evidence unreliable (orange bars, <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Results - Case study 1.</title><p>(<bold>A</bold>) Heatmap of the raw log cell count data. Each row corresponds to a single animal, columns correspond to brain regions. Animals are grouped into lesion-familiar (LF), lesion-novel (LN), sham-familiar (SF), and sham-novel (SN). (<bold>B, C</bold>) <inline-formula><alternatives><mml:math id="inf65"><mml:mstyle><mml:msub><mml:mi>log</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$  \log_{2}$\end{document}</tex-math></alternatives></inline-formula>-fold differences for each surgery type: B shows differences between SF and SN groups; C shows differences between LF and LN groups. The 95% Bayesian highest density interval (HDI) is given in green, and the 95% confidence interval calculated from a Welch’s <inline-formula><alternatives><mml:math id="inf66"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft66">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test in orange. Horizontal lines within the intervals mark the posterior mean of the Bayesian results, and the raw data means in the <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test case. The <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle><mml:mi>x</mml:mi></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$  x$\end{document}</tex-math></alternatives></inline-formula>-axis is ordered in terms of decreasing p-value from the significance test and ticks have been color-paired with the nodes in the recognition memory circuit diagram (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Black ticks are not present in the circuit because they are the control regions in the experiment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-fig4-v1.tif"/></fig><p>In contrast, the Bayesian estimates (green bars, <xref ref-type="fig" rid="fig4">Figure 4</xref>) produce a clear result. For a number of brain regions in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, sham-novel animals have higher expression than sham-familiar ones. These differences disappear in <xref ref-type="fig" rid="fig4">Figure 4B</xref> with lesion-novel and lesion-familiar animals showing roughly equal cell counts. This indicates that the difference is only present when the NRe is intact.</p></sec><sec id="s3-2"><title>Case study 2 - Ontogeny of inhibitory interneurons of the mouse thalamus</title><p>For each of the 50 brain regions, the estimated log<sub>2</sub>-fold difference in GFP-expressing cells between the two genotypes is plotted in <xref ref-type="fig" rid="fig5">Figure 5</xref>. This includes the purple and pink 95% HDI from the horseshoe and zero-inflated Poisson models along with the 95% CI arising from a <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test in orange. For most brain regions, the two Bayesian models gave narrower HDIs than the <inline-formula><alternatives><mml:math id="inf70"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft70">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test CI. Accordingly, the Bayesian models identified a greater number of brain regions that had genotype differences in Sox14-positive cell count in the sense that they found more places where the appropriate uncertainty interval does not overlap zero.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Results - Case study 2.</title><p>(<bold>A</bold>) Heatmap of the raw log cell count data. Each row corresponds to a single animal, columns correspond to brain regions. L and R denote left and right hemispheres, respectively. (<bold>B</bold>) log<sub>2</sub> fold differences in green fluorescent protein (GFP) positive cells between mouse genotypes, heterozygous (HET), and knockout (KO), for each of the 50 recorded brain regions spread across two rows. The 95% Bayesian highest density interval (HDI) is given in purple and pink for the Bayesian horseshoe and zero-inflated model. The 95% confidence interval calculated from a Welch’s <inline-formula><alternatives><mml:math id="inf71"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft71">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test is in orange. Horizontal lines within the intervals mark the posterior mean of the Bayesian results and the data estimate for the <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test. The <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle><mml:mi>x</mml:mi></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$  x$\end{document}</tex-math></alternatives></inline-formula>-axis is ordered in terms of decreasing p-value from the significance test.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-fig5-v1.tif"/></fig><p>Despite the large difference in interval estimation between the Bayesian HDIs and <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test CI for many brain regions, as the data becomes stronger from the perspective of the frequentist p-value toward the right-hand side of the second row in <xref ref-type="fig" rid="fig5">Figure 5</xref>, the model results become much more compatible. The variation within groups is very small for these regions. Further regularization is not necessary, and so the impact of partial pooling has been reduced. The sample estimate of the <inline-formula><alternatives><mml:math id="inf75"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft75">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test has ‘caught up’ to the regularized estimate because the signal is strong.</p><p>The zero-inflated Poisson distribution sometimes differs from the <inline-formula><alternatives><mml:math id="inf76"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft76">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test CIs. One example of this is the result for the dorsal tuberomammillary nucleus (TMd). <xref ref-type="fig" rid="fig6">Figure 6</xref>, bottom row, plots the raw cell-count values for TMd alongside the inferred frequentist mean and two Bayesian model means. For this region, the HET animals have high GFP expression across both hemispheres, yet animal three has a reading of zero for both hemispheres. This injects variability into the standard deviation of the HET group. Consequently, the pooled standard deviation used in the <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test is large and almost certainly guarantees a nonsignificant result. Furthermore, the sample mean of this region looks nothing like zero, but also nothing like the other two animals with positive counts. In addition to the wide interval, the sign of the difference does not agree with the data. The medial preoptic nucleus (MPN) also suffers from poor estimation. Once again, this region contains a single HET animal for which the reading from both hemispheres is zero. The zero-inflated Poisson produces a posterior distribution of the appropriate sign with small uncertainty.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Example data and inferences highlighting model discrepancies.</title><p>On the left under ‘data’: boxplots with medians and interquartile ranges for the raw data for four example brain regions. The shape of each point pairs left and right hemisphere readings in each of the five animals. On the right under ‘inference’: highest density intervals (HDIs) and confidence intervals are plotted. Purple is the Bayesian horseshoe model, pink is the Bayesian ZIP model, and orange is the sample mean. The Bayesian estimates are not strongly influenced by the zero-valued observations (medial preoptic nucleus [MPN], suprachiasmatic nucleus [SCH], dorsal tuberomammillary nucleus [TMd]) or large-valued outliers (medial habenula [MH]) and have means close to the data median. This explains the advantage of the Bayesian results over the confidence interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-fig6-v1.tif"/></fig><p>The two Bayesian models did not always agree. In some cases, such as the medial habenula (MH) and suprachiasmatic nucleus (SCH), the ‘standard’ horseshoe model does not show a genotype difference in cell counts, while the ZIP model indicates that heterozygotes had higher cell counts than KO (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>). The opposite can be seen in the case of the parvicellular ventral posteromedial nucleus of the thalamus (VPMpc), the horseshoe model suggests a genotype difference where the ZIP model did not (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Further examination of the data shows why this happens, for example, for region MH (<xref ref-type="fig" rid="fig6">Figure 6</xref>, top row), the ‘standard’ horseshoe model sensibly ignores the large positive outlier value in the heterozygote data, while the ZIP model does not. As a result, the ZIP model’s estimate for the mean is pulled upward, leading to an inferred difference in heterozygote versus the wild type.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>We have presented a standard workflow for Bayesian analysis of multiregion cell-count data. We propose a likelihood and appropriate priors with a nested hierarchical structure reflecting the structure of the experiment. We applied this to two distinct example datasets and demonstrated that they capture more fruitfully the characteristics of the data when compared to field-standard frequentist analyses.</p><p>For both case studies, the Bayesian uncertainty intervals are more precise than the CIs. These CIs tend to be quite wide on these data because of the small sample size and because of violations to their parametric model assumptions.</p><p>Our standard workflow uses a horseshoe prior, along with the partial pooling. This allows our model to deal effectively with outliers. Furthermore, for the data sizes presented here, a full Bayesian inference using Stan does not require long computation time, or even particularly high-performance hardware. Modern multicore laptop processors are quite sufficient for this task. Fitting a model typically takes less than an hour.</p><p>In our analysis, we have noted examples where different Bayesian models give discrepant conclusions. The obvious question to ask is, which should we trust? The disappointing but inevitable answer is that, as with more traditional methods, Bayesian analysis is only a tool useful for interpreting data and brings with it a set of assumptions and biases regarding the experiment and the data. A Bayesian analysis does not avoid inconsistent or inconclusive results, but it usually makes the assumptions more explicit and transparent. Typically, the solution to these model inconsistencies is to inspect the raw data and ask which model better captures those aspects of the data we are most interested in. Overall, the lesson here is that Bayesian hierarchical modeling has greater flexibility and statistical power, but all statistical analyses, even those claiming to ‘test hypotheses’, just support exploration, and it is ultimately the researcher’s responsibility to make sure that a model’s assumptions are appropriate and its behavior is sensible for the target dataset.</p><p>The horseshoe prior model workflow we have exhibited here is intended as a standard approach. We believe that, without extension, it will provide a robust model for cell-count data. However, we also suggest that the standard workflow can be a useful first step for a more comprehensive, extended model when one is required. We have given an example of this for the second dataset where the anomalous zeros prompted us to change the likelihood to a zero-inflated Poisson. There are other possibilities, e.g., zero inflation is not the only way to handle an anomaly in the number of zeros: the hurdle model is an alternative (<xref ref-type="bibr" rid="bib13">Cragg, 1971</xref>). This is not a mixture model; instead, it restricts the probability of zeros to some value <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mi>π</mml:mi></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$  \pi$\end{document}</tex-math></alternatives></inline-formula> with the probabilities for the positive counts coming from a truncated Poisson distribution. The hurdle model can deflate, as well as inflate, the probability mass at zero. This did not match the situation in the data we considered but might for other datasets. Another extension might involve tighter priors based on previous experiments. This is likely to be very relevant for cell-count data since these experiments are rarely performed in isolation, and so prior information can be leveraged from a history of empirical results.</p><p>One obvious elaboration of our model would replace normal distributions with multivariate normal distributions. This would have two advantages. First, correlations are difficult to estimate for undersampled data. Including correlation matrix priors provides extra information - e.g., based on anatomical connectivity - that can aid the statistical estimation of other parameters. Second, it would more closely match our understanding of the experiment: we know that activity is likely to be correlated across regions, and so it is apposite to include that directly in the model. Unfortunately, the problem of finding a suitable prior for the correlation proved insurmountable: the standard Lewandowski-Kurowicka-Joe distribution (<xref ref-type="bibr" rid="bib38">Lewandowski et al., 2009</xref>) which has been useful in lower-dimensional situations is too regularizing here. This is an area where further work needs to be done.</p><p>It is important to highlight that a mixed effects model is not a uniquely Bayesian construction. Indeed, any model that tries to include more sophistication through hierarchical structures, Bayesian or otherwise, is useful. However, non-Bayesian models can be complicated and opaque; they are also often more restrictive. For example, they often assume normal distributions, and circumventing these restrictions can make the models even less transparent. A Bayesian approach is, at first, unfamiliar; this can make it seem more obscure than better established methods, but, in the long run, Bayesian models are typically clearer and do not involve so many different assumptions and so many fine adjustments.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con5"><p>Resources, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Resources, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Resources, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-102391-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The code necessary to run the models presented in this manuscript can be found at <xref ref-type="bibr" rid="bib17">Dimmock et al., 2025</xref> and on our Github <ext-link ext-link-type="uri" xlink:href="https://BayesianCellCounts.github.io">https://BayesianCellCounts.github.io</ext-link>. The data for case study one on nucleus reuniens lesion are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.12787211">https://doi.org/10.5281/zenodo.12787211</ext-link> (<xref ref-type="bibr" rid="bib21">Exley et al., 2024</xref>). The data from case study two on Sox14 expressing neurons are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.12787287">https://doi.org/10.5281/zenodo.12787287</ext-link> (<xref ref-type="bibr" rid="bib25">Gerald and Sydney, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Andrew Dowsey and Matthew Nolan for useful discussion and helpful suggestions.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aarts</surname><given-names>E</given-names></name><name><surname>Verhage</surname><given-names>M</given-names></name><name><surname>Veenvliet</surname><given-names>JV</given-names></name><name><surname>Dolan</surname><given-names>CV</given-names></name><name><surname>van der Sluis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A solution to dependency: using multilevel analysis to accommodate nested data</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>491</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1038/nn.3648</pub-id><pub-id pub-id-type="pmid">24671065</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barker</surname><given-names>GRI</given-names></name><name><surname>Warburton</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>When is the hippocampus involved in recognition memory?</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10721</fpage><lpage>10731</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6413-10.2011</pub-id><pub-id pub-id-type="pmid">21775615</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barker</surname><given-names>GRI</given-names></name><name><surname>Warburton</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A critical role for the nucleus reuniens in long-term, but not short-term associative recognition memory formation</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>3208</fpage><lpage>3217</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1802-17.2017</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Betancourt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Identifying the Optimal Integration Time in Hamiltonian Monte Carlo</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1601.00225">https://arxiv.org/abs/1601.00225</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bird</surname><given-names>AD</given-names></name><name><surname>Wall</surname><given-names>MJ</given-names></name><name><surname>Richardson</surname><given-names>MJE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bayesian inference of synaptic quantal parameters from correlated vesicle release</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><elocation-id>116</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2016.00116</pub-id><pub-id pub-id-type="pmid">27932970</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonapersona</surname><given-names>V</given-names></name><name><surname>Schuler</surname><given-names>H</given-names></name><name><surname>Damsteegt</surname><given-names>R</given-names></name><name><surname>Adolfs</surname><given-names>Y</given-names></name><name><surname>Pasterkamp</surname><given-names>RJ</given-names></name><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Joëls</surname><given-names>M</given-names></name><name><surname>Sarabdjitsingh</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The mouse brain after foot shock in four dimensions: Temporal dynamics at a single-cell resolution</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2114002119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2114002119</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Tang</surname><given-names>D</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A statistical paradigm for neural spike train decoding applied to position prediction from ensemble firing patterns of rat hippocampal place cells</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>7411</fpage><lpage>7425</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-18-07411.1998</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bykowska</surname><given-names>O</given-names></name><name><surname>Gontier</surname><given-names>C</given-names></name><name><surname>Sax</surname><given-names>AL</given-names></name><name><surname>Jia</surname><given-names>DW</given-names></name><name><surname>Montero</surname><given-names>ML</given-names></name><name><surname>Bird</surname><given-names>AD</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name><name><surname>Pfister</surname><given-names>JP</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Model-based inference of synaptic transmission</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>11</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2019.00021</pub-id><pub-id pub-id-type="pmid">31481887</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Goodrich</surname><given-names>B</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Brubaker</surname><given-names>MA</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Riddell</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stan: a probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id><pub-id pub-id-type="pmid">36568334</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carvalho</surname><given-names>CM</given-names></name><name><surname>Polson</surname><given-names>NG</given-names></name><name><surname>Scott</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The horseshoe estimator for sparse signals</article-title><source>Biometrika</source><volume>97</volume><fpage>465</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1093/biomet/asq017</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cinotti</surname><given-names>F</given-names></name><name><surname>Humphries</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Bayesian mapping of the striatal microcircuit reveals robust asymmetries in the probabilities and distances of connections</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>1417</fpage><lpage>1435</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1487-21.2021</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>van Rossum</surname><given-names>MCW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probabilistic inference of short-term synaptic plasticity in neocortical microcircuits</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00075</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cragg</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Some statistical models for limited dependent variables with application to the demand for durable goods</article-title><source>Econometrica</source><volume>39</volume><elocation-id>829</elocation-id><pub-id pub-id-type="doi">10.2307/1909582</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daigle</surname><given-names>TL</given-names></name><name><surname>Madisen</surname><given-names>L</given-names></name><name><surname>Hage</surname><given-names>TA</given-names></name><name><surname>Valley</surname><given-names>MT</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Larsen</surname><given-names>RS</given-names></name><name><surname>Takeno</surname><given-names>MM</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Mills</surname><given-names>M</given-names></name><name><surname>Bosma-Moody</surname><given-names>A</given-names></name><name><surname>Siverts</surname><given-names>LA</given-names></name><name><surname>Walker</surname><given-names>M</given-names></name><name><surname>Graybuck</surname><given-names>LT</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Fong</surname><given-names>O</given-names></name><name><surname>Nguyen</surname><given-names>TN</given-names></name><name><surname>Garren</surname><given-names>E</given-names></name><name><surname>Lenz</surname><given-names>GH</given-names></name><name><surname>Chavarha</surname><given-names>M</given-names></name><name><surname>Pendergraft</surname><given-names>J</given-names></name><name><surname>Harrington</surname><given-names>J</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Nicovich</surname><given-names>PR</given-names></name><name><surname>McGraw</surname><given-names>MJ</given-names></name><name><surname>Ollerenshaw</surname><given-names>DR</given-names></name><name><surname>Smith</surname><given-names>KA</given-names></name><name><surname>Baker</surname><given-names>CA</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>MZ</given-names></name><name><surname>Boyden</surname><given-names>ES</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name><name><surname>da Costa</surname><given-names>NM</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A suite of transgenic driver and reporter mouse lines with enhanced brain-cell-type targeting and functionality</article-title><source>Cell</source><volume>174</volume><fpage>465</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.035</pub-id><pub-id pub-id-type="pmid">30007418</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delogu</surname><given-names>A</given-names></name><name><surname>Sellers</surname><given-names>K</given-names></name><name><surname>Zagoraiou</surname><given-names>L</given-names></name><name><surname>Bocianowska-Zbrog</surname><given-names>A</given-names></name><name><surname>Mandal</surname><given-names>S</given-names></name><name><surname>Guimera</surname><given-names>J</given-names></name><name><surname>Rubenstein</surname><given-names>JLR</given-names></name><name><surname>Sugden</surname><given-names>D</given-names></name><name><surname>Jessell</surname><given-names>T</given-names></name><name><surname>Lumsden</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Subcortical visual shell nuclei targeted by iprgcs develop from a Sox14+-GABAergic progenitor and require Sox14 to regulate daily activity rhythms</article-title><source>Neuron</source><volume>75</volume><fpage>648</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.06.013</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimmock</surname><given-names>S</given-names></name><name><surname>O’Donnell</surname><given-names>C</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Bayesian analysis of phase data in EEG and MEG</article-title><source>eLife</source><volume>12</volume><elocation-id>e84602</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.84602</pub-id><pub-id pub-id-type="pmid">37698464</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dimmock</surname><given-names>S</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name><name><surname>O’Donnell</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>BayesianCellCounts</data-title><version designator="v1.01">v1.01</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.16340993">https://doi.org/10.5281/zenodo.16340993</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorst</surname><given-names>KE</given-names></name><name><surname>Senne</surname><given-names>RA</given-names></name><name><surname>Diep</surname><given-names>AH</given-names></name><name><surname>de Boer</surname><given-names>AR</given-names></name><name><surname>Suthard</surname><given-names>RL</given-names></name><name><surname>Leblanc</surname><given-names>H</given-names></name><name><surname>Ruesch</surname><given-names>EA</given-names></name><name><surname>Pyo</surname><given-names>AY</given-names></name><name><surname>Skelton</surname><given-names>S</given-names></name><name><surname>Carstensen</surname><given-names>LC</given-names></name><name><surname>Malmberg</surname><given-names>S</given-names></name><name><surname>McKissick</surname><given-names>OP</given-names></name><name><surname>Bladon</surname><given-names>JH</given-names></name><name><surname>Ramirez</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Hippocampal engrams generate variable behavioral responses and brain-wide network states</article-title><source>The Journal of Neuroscience</source><volume>44</volume><elocation-id>e0340232023</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0340-23.2023</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ennaceur</surname><given-names>A</given-names></name><name><surname>Neave</surname><given-names>N</given-names></name><name><surname>Aggleton</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neurotoxic lesions of the perirhinal cortex do not mimic the behavioural effects of fornix transection in the rat</article-title><source>Behavioural Brain Research</source><volume>80</volume><fpage>9</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(96)00006-x</pub-id><pub-id pub-id-type="pmid">8905124</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Exley</surname><given-names>BMS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The role of the nucleus reuniens of the thalamus in the recognition memory network</article-title><publisher-name>Master’s Thesis - University of Bristol</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Exley</surname><given-names>B</given-names></name><name><surname>Dimmock</surname><given-names>S</given-names></name><name><surname>Warburton</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Exley_Warburton_NRe_lesion_cell_count_data</data-title><version designator="01">01</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.12787211">https://doi.org/10.5281/zenodo.12787211</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical Science</source><volume>7</volume><fpage>457</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hill</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Data Analysis Using Regression and Multilevel/Hierarchical Models</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Carlin</surname><given-names>JB</given-names></name><name><surname>Stern</surname><given-names>HS</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Bayesian Data Analysis</source><publisher-name>CRC press</publisher-name><pub-id pub-id-type="doi">10.1201/b16018</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gerald</surname><given-names>M</given-names></name><name><surname>Sydney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Moore_Schultz_Sox14_expressing_neurons</data-title><version designator="01">01</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1477124">https://doi.org/10.5281/zenodo.1477124</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golding</surname><given-names>B</given-names></name><name><surname>Pouchelon</surname><given-names>G</given-names></name><name><surname>Bellone</surname><given-names>C</given-names></name><name><surname>Murthy</surname><given-names>S</given-names></name><name><surname>Di Nardo</surname><given-names>AA</given-names></name><name><surname>Govindan</surname><given-names>S</given-names></name><name><surname>Ogawa</surname><given-names>M</given-names></name><name><surname>Shimogori</surname><given-names>T</given-names></name><name><surname>Lüscher</surname><given-names>C</given-names></name><name><surname>Dayer</surname><given-names>A</given-names></name><name><surname>Jabaudon</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Retinal input directs the recruitment of inhibitory interneurons into thalamic visual circuits</article-title><source>Neuron</source><volume>81</volume><fpage>1057</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.032</pub-id><pub-id pub-id-type="pmid">24607228</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Whitesell</surname><given-names>JD</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Feiner</surname><given-names>A</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Henry</surname><given-names>AM</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Knox</surname><given-names>JE</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Kuang</surname><given-names>X</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Lesnar</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>McConoughey</surname><given-names>S</given-names></name><name><surname>Mortrud</surname><given-names>MT</given-names></name><name><surname>Naeemi</surname><given-names>M</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Ouellette</surname><given-names>B</given-names></name><name><surname>Shen</surname><given-names>E</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical organization of cortical and thalamic connectivity</article-title><source>Nature</source><volume>575</volume><fpage>195</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1716-z</pub-id><pub-id pub-id-type="pmid">31666704</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haubrich</surname><given-names>J</given-names></name><name><surname>Nader</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Network-level changes in the brain underlie fear memory strength</article-title><source>eLife</source><volume>12</volume><elocation-id>88172.3</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.88172.3</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>JW-T</given-names></name><name><surname>Narduzzo</surname><given-names>KE</given-names></name><name><surname>Outram</surname><given-names>A</given-names></name><name><surname>Tinsley</surname><given-names>CJ</given-names></name><name><surname>Henley</surname><given-names>JM</given-names></name><name><surname>Warburton</surname><given-names>EC</given-names></name><name><surname>Brown</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contributions of area Te2 to rat recognition memory</article-title><source>Learning &amp; Memory</source><volume>18</volume><fpage>493</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1101/lm.2167511</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoover</surname><given-names>WB</given-names></name><name><surname>Vertes</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Collateral projections from nucleus reuniens of thalamus to hippocampus and medial prefrontal cortex in the rat: a single and double retrograde fluorescent labeling study</article-title><source>Brain Structure and Function</source><volume>217</volume><fpage>191</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1007/s00429-011-0345-6</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jager</surname><given-names>P</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Zagoraiou</surname><given-names>L</given-names></name><name><surname>Prekop</surname><given-names>HT</given-names></name><name><surname>Partanen</surname><given-names>J</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Wisden</surname><given-names>W</given-names></name><name><surname>Brickley</surname><given-names>SG</given-names></name><name><surname>Delogu</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tectal-derived interneurons contribute to phasic and tonic inhibition in the visual thalamus</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13579</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13579</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jager</surname><given-names>P</given-names></name><name><surname>Moore</surname><given-names>G</given-names></name><name><surname>Calpin</surname><given-names>P</given-names></name><name><surname>Durmishi</surname><given-names>X</given-names></name><name><surname>Salgarella</surname><given-names>I</given-names></name><name><surname>Menage</surname><given-names>L</given-names></name><name><surname>Kita</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>DW</given-names></name><name><surname>Blackshaw</surname><given-names>S</given-names></name><name><surname>Schultz</surname><given-names>SR</given-names></name><name><surname>Brickley</surname><given-names>S</given-names></name><name><surname>Shimogori</surname><given-names>T</given-names></name><name><surname>Delogu</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dual midbrain and forebrain origins of thalamic inhibitory interneurons</article-title><source>eLife</source><volume>10</volume><elocation-id>e59272</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59272</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawashima</surname><given-names>T</given-names></name><name><surname>Okuno</surname><given-names>H</given-names></name><name><surname>Bito</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A new era for functional labeling of neurons: activity-dependent promoters have come of age</article-title><source>Frontiers in Neural Circuits</source><volume>8</volume><elocation-id>00037</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2014.00037</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Venkataraju</surname><given-names>KU</given-names></name><name><surname>Pradhan</surname><given-names>K</given-names></name><name><surname>Mende</surname><given-names>C</given-names></name><name><surname>Taranda</surname><given-names>J</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Rockland</surname><given-names>KS</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Osten</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping social behavior-induced brain activation at cellular resolution in the mouse</article-title><source>Cell Reports</source><volume>10</volume><fpage>292</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2014.12.014</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>WB</given-names></name><name><surname>Cho</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Encoding of discriminative fear memory by input-specific LTP in the Amygdala</article-title><source>Neuron</source><volume>95</volume><fpage>1129</fpage><lpage>1146</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.004</pub-id><pub-id pub-id-type="pmid">28823727</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lein</surname><given-names>ES</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Ao</surname><given-names>N</given-names></name><name><surname>Ayres</surname><given-names>M</given-names></name><name><surname>Bensinger</surname><given-names>A</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Boe</surname><given-names>AF</given-names></name><name><surname>Boguski</surname><given-names>MS</given-names></name><name><surname>Brockway</surname><given-names>KS</given-names></name><name><surname>Byrnes</surname><given-names>EJ</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>T-M</given-names></name><name><surname>Chi Chin</surname><given-names>M</given-names></name><name><surname>Chong</surname><given-names>J</given-names></name><name><surname>Crook</surname><given-names>BE</given-names></name><name><surname>Czaplinska</surname><given-names>A</given-names></name><name><surname>Dang</surname><given-names>CN</given-names></name><name><surname>Datta</surname><given-names>S</given-names></name><name><surname>Dee</surname><given-names>NR</given-names></name><name><surname>Desaki</surname><given-names>AL</given-names></name><name><surname>Desta</surname><given-names>T</given-names></name><name><surname>Diep</surname><given-names>E</given-names></name><name><surname>Dolbeare</surname><given-names>TA</given-names></name><name><surname>Donelan</surname><given-names>MJ</given-names></name><name><surname>Dong</surname><given-names>H-W</given-names></name><name><surname>Dougherty</surname><given-names>JG</given-names></name><name><surname>Duncan</surname><given-names>BJ</given-names></name><name><surname>Ebbert</surname><given-names>AJ</given-names></name><name><surname>Eichele</surname><given-names>G</given-names></name><name><surname>Estin</surname><given-names>LK</given-names></name><name><surname>Faber</surname><given-names>C</given-names></name><name><surname>Facer</surname><given-names>BA</given-names></name><name><surname>Fields</surname><given-names>R</given-names></name><name><surname>Fischer</surname><given-names>SR</given-names></name><name><surname>Fliss</surname><given-names>TP</given-names></name><name><surname>Frensley</surname><given-names>C</given-names></name><name><surname>Gates</surname><given-names>SN</given-names></name><name><surname>Glattfelder</surname><given-names>KJ</given-names></name><name><surname>Halverson</surname><given-names>KR</given-names></name><name><surname>Hart</surname><given-names>MR</given-names></name><name><surname>Hohmann</surname><given-names>JG</given-names></name><name><surname>Howell</surname><given-names>MP</given-names></name><name><surname>Jeung</surname><given-names>DP</given-names></name><name><surname>Johnson</surname><given-names>RA</given-names></name><name><surname>Karr</surname><given-names>PT</given-names></name><name><surname>Kawal</surname><given-names>R</given-names></name><name><surname>Kidney</surname><given-names>JM</given-names></name><name><surname>Knapik</surname><given-names>RH</given-names></name><name><surname>Kuan</surname><given-names>CL</given-names></name><name><surname>Lake</surname><given-names>JH</given-names></name><name><surname>Laramee</surname><given-names>AR</given-names></name><name><surname>Larsen</surname><given-names>KD</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Lemon</surname><given-names>TA</given-names></name><name><surname>Liang</surname><given-names>AJ</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Luong</surname><given-names>LT</given-names></name><name><surname>Michaels</surname><given-names>J</given-names></name><name><surname>Morgan</surname><given-names>JJ</given-names></name><name><surname>Morgan</surname><given-names>RJ</given-names></name><name><surname>Mortrud</surname><given-names>MT</given-names></name><name><surname>Mosqueda</surname><given-names>NF</given-names></name><name><surname>Ng</surname><given-names>LL</given-names></name><name><surname>Ng</surname><given-names>R</given-names></name><name><surname>Orta</surname><given-names>GJ</given-names></name><name><surname>Overly</surname><given-names>CC</given-names></name><name><surname>Pak</surname><given-names>TH</given-names></name><name><surname>Parry</surname><given-names>SE</given-names></name><name><surname>Pathak</surname><given-names>SD</given-names></name><name><surname>Pearson</surname><given-names>OC</given-names></name><name><surname>Puchalski</surname><given-names>RB</given-names></name><name><surname>Riley</surname><given-names>ZL</given-names></name><name><surname>Rockett</surname><given-names>HR</given-names></name><name><surname>Rowland</surname><given-names>SA</given-names></name><name><surname>Royall</surname><given-names>JJ</given-names></name><name><surname>Ruiz</surname><given-names>MJ</given-names></name><name><surname>Sarno</surname><given-names>NR</given-names></name><name><surname>Schaffnit</surname><given-names>K</given-names></name><name><surname>Shapovalova</surname><given-names>NV</given-names></name><name><surname>Sivisay</surname><given-names>T</given-names></name><name><surname>Slaughterbeck</surname><given-names>CR</given-names></name><name><surname>Smith</surname><given-names>SC</given-names></name><name><surname>Smith</surname><given-names>KA</given-names></name><name><surname>Smith</surname><given-names>BI</given-names></name><name><surname>Sodt</surname><given-names>AJ</given-names></name><name><surname>Stewart</surname><given-names>NN</given-names></name><name><surname>Stumpf</surname><given-names>K-R</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Sutram</surname><given-names>M</given-names></name><name><surname>Tam</surname><given-names>A</given-names></name><name><surname>Teemer</surname><given-names>CD</given-names></name><name><surname>Thaller</surname><given-names>C</given-names></name><name><surname>Thompson</surname><given-names>CL</given-names></name><name><surname>Varnam</surname><given-names>LR</given-names></name><name><surname>Visel</surname><given-names>A</given-names></name><name><surname>Whitlock</surname><given-names>RM</given-names></name><name><surname>Wohnoutka</surname><given-names>PE</given-names></name><name><surname>Wolkey</surname><given-names>CK</given-names></name><name><surname>Wong</surname><given-names>VY</given-names></name><name><surname>Wood</surname><given-names>M</given-names></name><name><surname>Yaylaoglu</surname><given-names>MB</given-names></name><name><surname>Young</surname><given-names>RC</given-names></name><name><surname>Youngstrom</surname><given-names>BL</given-names></name><name><surname>Feng Yuan</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Zwingman</surname><given-names>TA</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Genome-wide atlas of gene expression in the adult mouse brain</article-title><source>Nature</source><volume>445</volume><fpage>168</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1038/nature05453</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowski</surname><given-names>D</given-names></name><name><surname>Kurowicka</surname><given-names>D</given-names></name><name><surname>Joe</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating random correlation matrices based on vines and extended onion method</article-title><source>Journal of Multivariate Analysis</source><volume>100</volume><fpage>1989</fpage><lpage>2001</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2009.04.008</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebmann</surname><given-names>T</given-names></name><name><surname>Renier</surname><given-names>N</given-names></name><name><surname>Bettayeb</surname><given-names>K</given-names></name><name><surname>Greengard</surname><given-names>P</given-names></name><name><surname>Tessier-Lavigne</surname><given-names>M</given-names></name><name><surname>Flajolet</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Three-dimensional study of alzheimer’s disease hallmarks using the iDISCO clearing method</article-title><source>Cell Reports</source><volume>16</volume><fpage>1138</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.06.060</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McElreath</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</source><publisher-name>Chapman and Hall/CRC</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishchenko</surname><given-names>Y</given-names></name><name><surname>Vogelstein</surname><given-names>JT</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A Bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data</article-title><source>The Annals of Applied Statistics</source><volume>5</volume><elocation-id>AOAS303</elocation-id><pub-id pub-id-type="doi">10.1214/09-AOAS303</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>RJ</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Rombach</surname><given-names>N</given-names></name><name><surname>O’Connor</surname><given-names>WT</given-names></name><name><surname>Murphy</surname><given-names>KJ</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bayesian estimation of synaptic physiology from the spectral responses of neural masses</article-title><source>NeuroImage</source><volume>42</volume><fpage>272</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.01.025</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>G</given-names></name><name><surname>Eacott</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Impaired object recognition with increasing levels of feature ambiguity in rats with perirhinal cortex lesions</article-title><source>Behavioural Brain Research</source><volume>148</volume><fpage>79</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(03)00176-1</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Winslow</surname><given-names>B</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Henry</surname><given-names>AM</given-names></name><name><surname>Mortrud</surname><given-names>MT</given-names></name><name><surname>Ouellette</surname><given-names>B</given-names></name><name><surname>Nguyen</surname><given-names>TN</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Slaughterbeck</surname><given-names>CR</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Nicholas</surname><given-names>E</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Joines</surname><given-names>KM</given-names></name><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Hohmann</surname><given-names>JG</given-names></name><name><surname>Wohnoutka</surname><given-names>P</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A mesoscale connectome of the mouse brain</article-title><source>Nature</source><volume>508</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1038/nature13186</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piironen</surname><given-names>J</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sparsity information and regularization in the horseshoe and other shrinkage priors</article-title><source>Electronic Journal of Statistics</source><volume>11</volume><elocation-id>EJS1337SI</elocation-id><pub-id pub-id-type="doi">10.1214/17-EJS1337SI</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Schoot</surname><given-names>R</given-names></name><name><surname>Depaoli</surname><given-names>S</given-names></name><name><surname>King</surname><given-names>R</given-names></name><name><surname>Kramer</surname><given-names>B</given-names></name><name><surname>M&quot;artens</surname><given-names>K</given-names></name><name><surname>Tadesse</surname><given-names>MG</given-names></name><name><surname>Vannucci</surname><given-names>M</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Veen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bayesian statistics and modelling</article-title><source>Nature Reviews Methods Primers</source><volume>1</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1038/s43586-020-00003-0</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Simpson</surname><given-names>D</given-names></name><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Bürkner</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rank-Normalization, folding, and localization: an improved Rˆ for assessing convergence of MCMC (with Discussion)</article-title><source>Bayesian Analysis</source><volume>16</volume><fpage>667</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1214/20-BA1221</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Ginzburg</surname><given-names>I</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>1017</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.2.1017</pub-id><pub-id pub-id-type="pmid">9463459</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Full model</title><p>Here, we present the complete mathematical model for each of the three models applied in the main text. In all cases, the exposure term is only necessary for the first case study. The area was not available for the second, so the exposure term was left out.</p><sec sec-type="appendix" id="s8-1"><title>Poisson model</title><p><disp-formula id="equ8"><label>(1)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle y_{i} \sim \dist{\rm Poisson}(\lambda_{i})$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ9"><label>(2)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle \log \lambda_{i} = \theta_{r[i],g[i]} + E_{i} + \gamma_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ10"><label>(3)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle \theta_{rg} \sim \dist{\rm Normal}(5,2)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ11"><label>(4)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \gamma_{i} \sim \dist{\rm Normal}\big(0,\tau_{r[i],g[i]}\big)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ12"><label>(5)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1.05</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle \tau_{rg} \sim \dist{\rm HalfNormal}(\log(1.05))$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec sec-type="appendix" id="s8-2"><title>Horseshoe model</title><p><disp-formula id="equ13"><label>(6)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle y_{i} \sim {\rm Poisson}(\lambda_{i})$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ14"><label>(7)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle  \log \lambda_{i} = \theta_{r[i],g[i]} + E_{i} + \gamma_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ15"><label>(8)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle  \theta_{rg} \sim \dist{\rm Normal}(5,2)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ16"><label>(9)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle  \gamma_{i} \sim \dist{\rm Normal}\big(0 ,\kappa_i \times \tau_{r[i],g[i]}\big)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ17"><label>(10)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1.05</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle  \tau_{rg} \sim \dist{\rm HalfNormal}(\log(1.05))$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ18"><label>(11)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle  \kappa_i \sim \dist{\rm HalfNormal}(1)$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec sec-type="appendix" id="s8-3"><title>Zero-inflated model</title><p><disp-formula id="equ19"><label>(12)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle  y_{i} \sim \dist{\rm ZIPoisson}(\lambda_{i}, \pi)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ20"><label>(13)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle  \pi \sim \dist{\rm Beta}(1,5)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ21"><label>(14)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle  \log \lambda_{i} = \theta_{r[i],g[i]} + E_{i} + \gamma_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ22"><label>(15)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle  \theta_{rg} \sim \dist{\rm Normal}(5,2)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ23"><label>(16)</label><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle \gamma_{i} \sim \dist{\rm Normal}\big(0,\tau_{r[i],g[i]}\big)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ24"><label>(17)</label><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>HalfNormal</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1.05</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle \tau_{rg} \sim \text{HalfNormal}(\log(1.05))$$\end{document}</tex-math></alternatives></disp-formula></p></sec></sec><sec sec-type="appendix" id="s9"><title>Distributions</title><sec sec-type="appendix" id="s9-1"><title>Zero-inflated Poisson distribution</title><p>The zero-inflated Poisson distribution is a mixture distribution with mixing parameter <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle><mml:mi>π</mml:mi></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}$  \pi$\end{document}</tex-math></alternatives></inline-formula>. The distribution is formally defined below. If<disp-formula id="equ25"><label>(18)</label><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle  p_Y(y)\equiv \dist{\rm ZIPoisson}(\pi,\lambda)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ26"><label>(19)</label><alternatives><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:math><tex-math id="t26">\begin{document}$$\displaystyle \newcommand{\dist}[1]{\mathrm{#1}} p_Y(y) = \begin{cases} \pi + (1-\pi)\exp{(-\lambda)} &amp; \text{if}\quad y=0\\ (1-\pi)f(y) &amp; \text{otherwise} \end{cases}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ27"><label>(20)</label><alternatives><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t27">\begin{document}$$\displaystyle  f(y)\equiv \dist{\rm Poisson}(\lambda)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Equivalently, the mixture can be specified with an indicator function.<disp-formula id="equ28"><label>(21)</label><alternatives><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t28">\begin{document}$$\displaystyle  p_Y(y) = \mathbf{1}_0(y)\pi + (1-\pi)f(y) $$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec sec-type="appendix" id="s9-2"><title>HalfNormal distribution</title><p>The HalfNormal distribution coincides with a zero-mean normal distribution truncated at zero. It has a single scale parameter <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle><mml:mi>σ</mml:mi></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$  \sigma$\end{document}</tex-math></alternatives></inline-formula>. If<disp-formula id="equ29"><label>(22)</label><alternatives><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t29">\begin{document}$$\displaystyle  p_Y(y)\equiv \dist{\rm Half\,Normal}(\sigma)$$\end{document}</tex-math></alternatives></disp-formula></p><p>then<disp-formula id="equ30"><label>(23)</label><alternatives><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mrow><mml:mi>σ</mml:mi><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t30">\begin{document}$$\displaystyle p_Y(y) = \frac{\sqrt{2}}{\sigma\pi}\exp{\Big(-\frac{y^2}{2\sigma^2}\Big)}$$\end{document}</tex-math></alternatives></disp-formula></p><p>is the probability density function.</p></sec></sec><sec sec-type="appendix" id="s10"><title>Additional methods</title><sec sec-type="appendix" id="s10-1"><title>Fold differences</title><p>In our results, we present differences between experimental groups in terms of <inline-formula><alternatives><mml:math id="inf81"><mml:mstyle><mml:msub><mml:mi>log</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft81">\begin{document}$  \log_{2}$\end{document}</tex-math></alternatives></inline-formula>-fold differences. We calculate this as follows. The parameter of interest <inline-formula><alternatives><mml:math id="inf82"><mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft82">\begin{document}$  \theta_{r,g=i}$\end{document}</tex-math></alternatives></inline-formula> is modeled on the natural log scale owing to the log-link function necessary for the Poisson regression. At the average animal, the difference<disp-formula id="equ31"><label>(24)</label><alternatives><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t31">\begin{document}$$\displaystyle  \theta_{r,g=i} - \theta_{r, g=j} = \log\frac{\exp(\theta_{r,g=i})}{\exp(\theta_{r, g=j})}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ32"><label>(25)</label><alternatives><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t32">\begin{document}$$\displaystyle  = \log \Delta_{ij}$$\end{document}</tex-math></alternatives></disp-formula></p><p>is the natural log of the ratio of the expected counts. To obtain <inline-formula><alternatives><mml:math id="inf83"><mml:mstyle><mml:msub><mml:mi>log</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft83">\begin{document}$  \log_{2}$\end{document}</tex-math></alternatives></inline-formula>-fold differences, we simply change the base by multiplying <inline-formula><alternatives><mml:math id="inf84"><mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft84">\begin{document}$  \log\Delta_{ij}$\end{document}</tex-math></alternatives></inline-formula> by <inline-formula><alternatives><mml:math id="inf85"><mml:mstyle><mml:msub><mml:mi>log</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft85">\begin{document}$  \log_{2}(e)$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec sec-type="appendix" id="s10-2"><title>Data transformations</title><p>To facilitate a comparison with the Bayesian intervals in terms of <inline-formula><alternatives><mml:math id="inf86"><mml:mstyle><mml:msub><mml:mi>log</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft86">\begin{document}$  \log_{2}$\end{document}</tex-math></alternatives></inline-formula>-fold differences, it was necessary to add one to any zero counts before applying the <inline-formula><alternatives><mml:math id="inf87"><mml:mstyle><mml:mi>t</mml:mi></mml:mstyle></mml:math><tex-math id="inft87">\begin{document}$  t$\end{document}</tex-math></alternatives></inline-formula>-test.</p></sec><sec sec-type="appendix" id="s10-3"><title>Non-centered parameterization</title><p>Hierarchical models can produce geometry that is difficult for the sampler to explore. Fortunately, there exists a simple reparameterization known as non-centering that can remedy this problem. In our model, instead of sampling <inline-formula><alternatives><mml:math id="inf88"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft88">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> directly, we sample the parameter <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$  \tilde{\gamma}_{i}$\end{document}</tex-math></alternatives></inline-formula> instead and use it to reconstruct <inline-formula><alternatives><mml:math id="inf90"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft90">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula>. That is, sample <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}$  \tilde{\gamma}_{i}$\end{document}</tex-math></alternatives></inline-formula> from a standard normal distribution,<disp-formula id="equ33"><label>(26)</label><alternatives><mml:math id="m33"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>Normal</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t33">\begin{document}$$\displaystyle \tilde{\gamma}_{i}\sim\mathrm{Normal}(0,1)$$\end{document}</tex-math></alternatives></disp-formula></p><p>and reconstruct <inline-formula><alternatives><mml:math id="inf92"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft92">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> as a deterministic function of sampled values of <inline-formula><alternatives><mml:math id="inf93"><mml:mstyle><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft93">\begin{document}$  \tilde{\gamma}_{i}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf94"><mml:mstyle><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft94">\begin{document}$  \tau_{r[i],g[i]}$\end{document}</tex-math></alternatives></inline-formula>.<disp-formula id="equ34"><label>(27)</label><alternatives><mml:math id="m34"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t34">\begin{document}$$\displaystyle \gamma_{i}=\theta_{r[i],g[i]}+\tau_{r[i],g[i]}\times\tilde{\gamma}_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>This removes the frustrating joint behavior between  <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf96"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft96">\begin{document}$\tau_{r[i],g[i]}$\end{document}</tex-math></alternatives></inline-formula>, and promotes efficient sampling.</p></sec><sec sec-type="appendix" id="s10-4"><title>Preprocessing - Case study 1</title><p>In these data, some animals produced more than one reading per brain region. Before fitting our model, these were summed together to produce a single count; the exposure term was also properly adjusted to correctly reflect the area of the recording site.</p></sec></sec><sec sec-type="appendix" id="s11"><title>Software, packages, and libraries</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Software packages used.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">R Libraries</th><th align="left" valign="bottom">Version</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom">rstan</td><td align="left" valign="bottom">2.26.3</td><td align="left" valign="bottom">complete Stan library</td></tr><tr><td align="left" valign="bottom">cmdstanr</td><td align="left" valign="bottom">0.5.2</td><td align="left" valign="bottom">lightweight Stan library</td></tr><tr><td align="left" valign="bottom">HDInterval</td><td align="left" valign="bottom">0.2.2</td><td align="left" valign="bottom">calculating HDI in R</td></tr><tr><td align="left" valign="bottom">ggplot2</td><td align="left" valign="bottom">3.4.1</td><td align="left" valign="bottom">plotting</td></tr><tr><td align="left" valign="bottom">bayesplot</td><td align="left" valign="bottom">1.9.0</td><td align="left" valign="bottom">plotting</td></tr><tr><td align="left" valign="bottom">tidyverse</td><td align="left" valign="bottom">1.3.1</td><td align="left" valign="bottom">tibble, tidyr, readr, purr, dplyr, stringr, forcats</td></tr></tbody></table><table-wrap-foot><fn><p>R version 4.2.1 - ‘Funny-looking-kid’.</p></fn><fn><p>Computation was performed locally on a Dell XPS 13 7390 laptop. Intel i7-10510U @ 1.80 GHz, 16 GB of RAM, Ubuntu 20.04.4 LTS.</p></fn><fn><p>Panels composed using Inkscape version 1.2.2.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Acronyms for the brain regions in Case study 1.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Term</th><th align="left" valign="bottom">Definition</th></tr></thead><tbody><tr><td align="left" valign="bottom">ACC</td><td align="left" valign="bottom">Anterior cingulate cortex</td></tr><tr><td align="left" valign="bottom">DCA1/3</td><td align="left" valign="bottom">Dorsal CA1/3</td></tr><tr><td align="left" valign="bottom">DDG</td><td align="left" valign="bottom">Dorsal dentate gyrus</td></tr><tr><td align="left" valign="bottom">DPC</td><td align="left" valign="bottom">Dorsal peduncular cortex</td></tr><tr><td align="left" valign="bottom">DSUB</td><td align="left" valign="bottom">Dorsal subiculum</td></tr><tr><td align="left" valign="bottom">HPC</td><td align="left" valign="bottom">Hippocampus</td></tr><tr><td align="left" valign="bottom">ICA1/3</td><td align="left" valign="bottom">Intermediate CA1/3</td></tr><tr><td align="left" valign="bottom">IDG</td><td align="left" valign="bottom">Intermediate dentate gyrus</td></tr><tr><td align="left" valign="bottom">IFC</td><td align="left" valign="bottom">Infralimbic cortex</td></tr><tr><td align="left" valign="bottom">LENT</td><td align="left" valign="bottom">Lateral entorhinal cortex</td></tr><tr><td align="left" valign="bottom">MOC</td><td align="left" valign="bottom">Medial orbital cortex</td></tr><tr><td align="left" valign="bottom">MPFC</td><td align="left" valign="bottom">Medial prefrontal cortex</td></tr><tr><td align="left" valign="bottom">M2C</td><td align="left" valign="bottom">Motor cortex M2</td></tr><tr><td align="left" valign="bottom">NRe</td><td align="left" valign="bottom">Nucleus reuniens</td></tr><tr><td align="left" valign="bottom">PRL</td><td align="left" valign="bottom">Prelimbic cortex</td></tr><tr><td align="left" valign="bottom">PRH</td><td align="left" valign="bottom">Perirhinal cortex</td></tr><tr><td align="left" valign="bottom">PSTC</td><td align="left" valign="bottom">Postrhinal cortex</td></tr><tr><td align="left" valign="bottom">TE2</td><td align="left" valign="bottom">Temporal association cortex</td></tr><tr><td align="left" valign="bottom">VCA1/3</td><td align="left" valign="bottom">Ventral CA1/3</td></tr><tr><td align="left" valign="bottom">VDG</td><td align="left" valign="bottom">Ventral dentate gyrus</td></tr><tr><td align="left" valign="bottom">VOC</td><td align="left" valign="bottom">Ventral orbital cortex</td></tr><tr><td align="left" valign="bottom">VSUB</td><td align="left" valign="bottom">Ventral subiculum</td></tr><tr><td align="left" valign="bottom">V2C</td><td align="left" valign="bottom">Visual cortex V2</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Acronyms for the brain regions in Case study 2.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Term</th><th align="left" valign="bottom">Definition</th><th align="left" valign="bottom">Term</th><th align="left" valign="bottom">Definition</th></tr></thead><tbody><tr><td align="left" valign="bottom">AHN</td><td align="left" valign="bottom">Anterior hypothalamic nucleus</td><td align="left" valign="bottom">PP</td><td align="left" valign="bottom">Peripeduncular nucleus</td></tr><tr><td align="left" valign="bottom">ARH</td><td align="left" valign="bottom">Arcuate hypothalamic nucleus</td><td align="left" valign="bottom">PR</td><td align="left" valign="bottom">Perireunensis nucleus</td></tr><tr><td align="left" valign="bottom">CL</td><td align="left" valign="bottom">Central lateral nucleus of the thalamus</td><td align="left" valign="bottom">PVa</td><td align="left" valign="bottom">Periventricular hypothalamic nucleus, anterior part</td></tr><tr><td align="left" valign="bottom">DMH</td><td align="left" valign="bottom">Dorsomedial nucleus of the hypothalamus</td><td align="left" valign="bottom">PVH</td><td align="left" valign="bottom">Paraventricular hypothalamic nucleus</td></tr><tr><td align="left" valign="bottom">FF</td><td align="left" valign="bottom">Fields of Forel</td><td align="left" valign="bottom">PVHd</td><td align="left" valign="bottom">Paraventricular hypothalamic nucleus, descending division</td></tr><tr><td align="left" valign="bottom">IGL</td><td align="left" valign="bottom">Intergeniculate leaflet of the lateral geniculate complex</td><td align="left" valign="bottom">PVi</td><td align="left" valign="bottom">Periventricular hypothalamic nucleus, intermediate part</td></tr><tr><td align="left" valign="bottom">LD</td><td align="left" valign="bottom">Lateral dorsal nucleus of thalamus</td><td align="left" valign="bottom">PVp</td><td align="left" valign="bottom">Periventricular hypothalamic nucleus, posterior part</td></tr><tr><td align="left" valign="bottom">LM</td><td align="left" valign="bottom">Lateral mammillary nucleus</td><td align="left" valign="bottom">RCH</td><td align="left" valign="bottom">Retrochiasmatic area</td></tr><tr><td align="left" valign="bottom">LGv</td><td align="left" valign="bottom">Ventral part of the lateral geniculate complex</td><td align="left" valign="bottom">RT</td><td align="left" valign="bottom">Reticular nucleus of the thalamus</td></tr><tr><td align="left" valign="bottom">LGd</td><td align="left" valign="bottom">Dorsal part of the lateral geniculate complex</td><td align="left" valign="bottom">SBPV</td><td align="left" valign="bottom">Subparaventricular zone</td></tr><tr><td align="left" valign="bottom">LH</td><td align="left" valign="bottom">Lateral habenula</td><td align="left" valign="bottom">SCH</td><td align="left" valign="bottom">Suprachiasmatic nucleus</td></tr><tr><td align="left" valign="bottom">LHA</td><td align="left" valign="bottom">Lateral hypothalamic area</td><td align="left" valign="bottom">SGN</td><td align="left" valign="bottom">Suprageniculate nucleus</td></tr><tr><td align="left" valign="bottom">LP</td><td align="left" valign="bottom">Lateral posterior nucleus of the thalamus</td><td align="left" valign="bottom">SPFm</td><td align="left" valign="bottom">Subparafascicular nucleus, magnocellular part</td></tr><tr><td align="left" valign="bottom">MD</td><td align="left" valign="bottom">Mediodorsal nucleus of thalamus</td><td align="left" valign="bottom">SPFp</td><td align="left" valign="bottom">Subparafascicular nucleus, parvicellular part</td></tr><tr><td align="left" valign="bottom">MGd</td><td align="left" valign="bottom">Medial geniculate complex, dorsal part</td><td align="left" valign="bottom">SUM</td><td align="left" valign="bottom">Supramammillary nucleus</td></tr><tr><td align="left" valign="bottom">MGv</td><td align="left" valign="bottom">Medial geniculate complex, ventral part</td><td align="left" valign="bottom">TMd</td><td align="left" valign="bottom">Tuberomammillary nucleus, dorsal part</td></tr><tr><td align="left" valign="bottom">MGm</td><td align="left" valign="bottom">Medial geniculate complex, medial part</td><td align="left" valign="bottom">TMv</td><td align="left" valign="bottom">Tuberomammillary nucleus, ventral part</td></tr><tr><td align="left" valign="bottom">MH</td><td align="left" valign="bottom">Medial habenula</td><td align="left" valign="bottom">TU</td><td align="left" valign="bottom">Tuberal nucleus</td></tr><tr><td align="left" valign="bottom">MMme</td><td align="left" valign="bottom">Medial mammillary nucleus, median part</td><td align="left" valign="bottom">VAL</td><td align="left" valign="bottom">Ventral anterior-lateral complex of the thalamus</td></tr><tr><td align="left" valign="bottom">MPN</td><td align="left" valign="bottom">Medial preoptic nucleus</td><td align="left" valign="bottom">VMH</td><td align="left" valign="bottom">Ventromedial hypothalamic nucleus</td></tr><tr><td align="left" valign="bottom">PH</td><td align="left" valign="bottom">Posterior hypothalamic nucleus</td><td align="left" valign="bottom">VM</td><td align="left" valign="bottom">Ventral medial nucleus of the thalamus</td></tr><tr><td align="left" valign="bottom">PMd</td><td align="left" valign="bottom">Dorsal premammillary nucleus</td><td align="left" valign="bottom">VPL</td><td align="left" valign="bottom">Ventral posterolateral nucleus of the thalamus</td></tr><tr><td align="left" valign="bottom">PMv</td><td align="left" valign="bottom">Ventral premammillary nucleus</td><td align="left" valign="bottom">VPM</td><td align="left" valign="bottom">Ventral posteromedial nucleus of the thalamus</td></tr><tr><td align="left" valign="bottom">PO</td><td align="left" valign="bottom">Posterior complex of the thalamus</td><td align="left" valign="bottom">VPMpc</td><td align="left" valign="bottom">Ventral posteromedial nucleus of the thalamus, parvicellular part</td></tr><tr><td align="left" valign="bottom">POL</td><td align="left" valign="bottom">Posterior limiting nucleus of the thalamus</td><td align="left" valign="bottom">ZI</td><td align="left" valign="bottom">Zona incerta</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s12"><title>Brain-region name acronyms</title><sec sec-type="appendix" id="s12-1"><title>Sampler diagnostics</title><p>The basic Poisson model was sampled excellently. Measures of sampling performance such as <inline-formula><alternatives><mml:math id="inf97"><mml:mstyle><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft97">\begin{document}$  \hat{R}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="bibr" rid="bib22">Gelman and Rubin, 1992</xref>; <xref ref-type="bibr" rid="bib47">Vehtari et al., 2021</xref>) and effective sample size were all satisfactory (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Similarly, for the zero-inflated model, no problems were observed for any of the diagnostics (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Contrasting with this, the horseshoe model exhibited some signs of fitting problems (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). Divergences were not observed, and given the longer chain length, this is reassuring evidence against biased computation. However, for many parameters, the effective sample size is much lower than we would like to see. This is reflected in the trace plots: the sampler is not making large jumps across the parameter space, implying high autocorrelation and low effective sample size. Unfortunately, the horseshoe is notoriously hard to fit, and we resort to brute-force methods, such as increasing the number of iterations and reducing the step size of the sampler to improve the inference. In the following three plots, diagnostics have been summarized with the following three items:</p><list list-type="bullet" id="list1"><list-item><p><bold>A:</bold> The performance of the sampler is illustrated by plotting <inline-formula><alternatives><mml:math id="inf98"><mml:mstyle><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft98">\begin{document}$  \hat{R}$\end{document}</tex-math></alternatives></inline-formula> (R-hat, <inline-formula><alternatives><mml:math id="inf99"><mml:mstyle><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math><tex-math id="inft99">\begin{document}$  \hat{R}\approx 1$\end{document}</tex-math></alternatives></inline-formula> ideal) against the ratio of the effective number of samples (larger is better) for each parameter in the model. Points represent individual parameters in the model and have further been color-coded by their type. For example, all <inline-formula><alternatives><mml:math id="inf100"><mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft100">\begin{document}$  \theta_{r,g}$\end{document}</tex-math></alternatives></inline-formula> are colored in green. Points have also been scaled based on how numerous the parameters are, so the more numerous parameters have smaller dots, the less numerous, larger.</p></list-item><list-item><p><bold>B:</bold> A histogram comparing the marginal energy distribution <inline-formula><alternatives><mml:math id="inf101"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft101">\begin{document}$\pi_{E}$\end{document}</tex-math></alternatives></inline-formula> and the transitional energy distribution <inline-formula><alternatives><mml:math id="inf102"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft102">\begin{document}$\pi_{\Delta E}$\end{document}</tex-math></alternatives></inline-formula> of the Hamiltonian. Ideally, these distributions should match each other closely if the posterior distribution has been properly explored by the sampler.</p></list-item><list-item><p><bold>C:</bold> For each parameter type, the parameter with the ‘poorest’ mixing (largest <inline-formula><alternatives><mml:math id="inf103"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft103">\begin{document}$\hat{R}$\end{document}</tex-math></alternatives></inline-formula>) are presented with a post-warmup trace plot that overlays the ordered sequence of samples from each of the four chains. Corresponding points in A are marked with a black border and zero transparency.</p></list-item></list> <fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Diagnostics - Poisson.</title><p>Standard Poisson model - Case study 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Diagnostics - Horseshoe.</title><p>Horseshoe model - Case study 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Diagnostics - ZIPoisson.</title><p>Zero-inflated Poisson - Case study 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig3-v1.tif"/></fig></sec></sec><sec sec-type="appendix" id="s13"><title>Posterior predictive checking</title><p>Posterior predictive checks use the posterior predictive distribution,<disp-formula id="equ35"><label>(28)</label><alternatives><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t35">\begin{document}$$\displaystyle  p(y^{\text{rep}}|y) = \int p(y^{\text{rep}}|\theta)p(\theta|y)\, d\theta$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf104"><mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft104">\begin{document}$  p(\theta|y)$\end{document}</tex-math></alternatives></inline-formula> is the posterior distribution and <inline-formula><alternatives><mml:math id="inf105"><mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>rep</mml:mtext></mml:mrow></mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft105">\begin{document}$  p(y^{\text{rep}}|\theta)$\end{document}</tex-math></alternatives></inline-formula> is the data distribution for <inline-formula><alternatives><mml:math id="inf106"><mml:mstyle><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>rep</mml:mtext></mml:mrow></mml:msup></mml:mstyle></mml:math><tex-math id="inft106">\begin{document}$  y^{\text{rep}}$\end{document}</tex-math></alternatives></inline-formula> that follows the same form as the likelihood for <inline-formula><alternatives><mml:math id="inf107"><mml:mstyle><mml:mi>y</mml:mi></mml:mstyle></mml:math><tex-math id="inft107">\begin{document}$  y$\end{document}</tex-math></alternatives></inline-formula>. If we can verify that the posterior predictive distribution can generate replicate datasets with similar statistics to the observed data, then we might conclude that our model is consistent with the observed data and useful for answering questions about it. In practice, a Monte Carlo approach is used to approximate statistics of the posterior predictive distribution. For example, if <inline-formula><alternatives><mml:math id="inf108"><mml:mstyle><mml:mi>T</mml:mi></mml:mstyle></mml:math><tex-math id="inft108">\begin{document}$  T$\end{document}</tex-math></alternatives></inline-formula> is the test statistic of interest such as the sample mean, then</p><list list-type="order" id="list2"><list-item><p>For <inline-formula><alternatives><mml:math id="inf109"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft109">\begin{document}$1\dots S,$\end{document}</tex-math></alternatives></inline-formula></p><list list-type="alpha-lower" id="list2subList1"><list-item><p>sample <inline-formula><alternatives><mml:math id="inf110"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∽</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft110">\begin{document}$\theta_s\backsim p(\theta|y)$\end{document}</tex-math></alternatives></inline-formula></p></list-item><list-item><p>sample <inline-formula><alternatives><mml:math id="inf111"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mspace width="negativethinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft111">\begin{document}$y_s^{\mathrm{rep}} \sim p\!\left(y^{\mathrm{rep}} \mid \theta_s\right)$\end{document}</tex-math></alternatives></inline-formula></p></list-item><list-item><p>calculate <inline-formula><alternatives><mml:math id="inf112"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft112">\begin{document}$T(y^{\rm rep}_s)$\end{document}</tex-math></alternatives></inline-formula> where <italic>T</italic> is the statistic of interest</p></list-item></list></list-item><list-item><p>Return <inline-formula><alternatives><mml:math id="inf113"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>≈</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft113">\begin{document}$\{T(y_{1}^{\text{rep}}),\dots,T(y_{S}^{\text{rep}})\}\approx p(T(y^{\text{rep} })|y)$\end{document}</tex-math></alternatives></inline-formula></p></list-item></list> <p>The posterior predictive checks that follow examine two important statistics of count data (<xref ref-type="fig" rid="app1fig4">Appendix 1—figures 4</xref>–<xref ref-type="fig" rid="app1fig6">6</xref>). (1) The standard deviation of the data (dispersion), as panel A. (2) The proportion of zeroes in the data (zero inflation), as panel B. The value of the test statistic applied to the observed data <inline-formula><alternatives><mml:math id="inf114"><mml:mstyle><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft114">\begin{document}$  T(y)$\end{document}</tex-math></alternatives></inline-formula> is plotted as a solid purple line, and the distribution of test statistics <inline-formula><alternatives><mml:math id="inf115"><mml:mstyle><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math><tex-math id="inft115">\begin{document}$  \{T(y_{1}^{\text{rep}}),\dots,T(y_{S}^{\text{rep}})\}$\end{document}</tex-math></alternatives></inline-formula> as a purple histogram.</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>PPC - Poisson.</title><p>Posterior predictive check for the standard Poisson model in Case study 1. (<bold>A</bold>) The proportion of zeroes in the data matches the proportion of zeroes in posterior predictive samples. This proportion is zero. (<bold>B</bold>) The distribution of standard deviations computed over a number of posterior predictive datasets (histogram) aligns with the standard deviation of the data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>PPC - Horseshoe.</title><p>Horseshoe model - Case study 2. Posterior predictive check for the standard horseshoe model in Case study 2. (<bold>A</bold>) The proportion of zeroes in the data is larger than those found in posterior predictive datasets. This makes sense, because the likelihood is still a Poisson distribution. (<bold>B</bold>) The distribution of standard deviations computed over a number of posterior predictive datasets (histogram) aligns with the standard deviation of the data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig5-v1.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>PPC - ZIPoisson.</title><p>Zero-inflated Poisson - Case study 2. (<bold>A</bold>) The proportion of zeroes in the data matches the proportion of zeroes in posterior predictive samples. (<bold>B</bold>) The distribution of standard deviations computed over a number of posterior predictive datasets (histogram) aligns with the standard deviation of the data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig6-v1.tif"/></fig></sec><sec sec-type="appendix" id="s14"><title>Horseshoe densities</title><p>In our model, a horseshoe prior was used to allow some <inline-formula><alternatives><mml:math id="inf116"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft116">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula>, typically those informed by <inline-formula><alternatives><mml:math id="inf117"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math><tex-math id="inft117">\begin{document}$  y_{i}=0$\end{document}</tex-math></alternatives></inline-formula>, to escape regularization by partial pooling. However, we encountered many problems with the default parameterization that assigns a HalfCauchy density to the individual inflation parameters <inline-formula><alternatives><mml:math id="inf118"><mml:mstyle><mml:msub><mml:mi>κ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft118">\begin{document}$  \kappa_{i}$\end{document}</tex-math></alternatives></inline-formula>. In <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7A</xref>, the proportional conditional posterior density<disp-formula id="equ36"><label>(29)</label><alternatives><mml:math id="m36"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t36">\begin{document}$$\displaystyle p(\tilde{\gamma}_{i},\kappa_{i}|\theta,\tau,y_{i})\propto p(y_{i}|\tilde{\gamma}_{i},\theta,\tau,\kappa_{i})p(\tilde{\gamma}_{i})p(\kappa_{i})$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle><mml:mi>θ</mml:mi></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$  \theta$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle><mml:mi>τ</mml:mi></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$  \tau$\end{document}</tex-math></alternatives></inline-formula> have been fixed, is plotted when <inline-formula><alternatives><mml:math id="inf121"><mml:mstyle><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft121">\begin{document}$  y_{i}$\end{document}</tex-math></alternatives></inline-formula> lies close to the population mean (left) or when it equals zero (right). Note that the <inline-formula><alternatives><mml:math id="inf122"><mml:mstyle><mml:mi>x</mml:mi></mml:mstyle></mml:math><tex-math id="inft122">\begin{document}$  x$\end{document}</tex-math></alternatives></inline-formula>-axis is <inline-formula><alternatives><mml:math id="inf123"><mml:mstyle><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft123">\begin{document}$  \tilde{\gamma}_{i}$\end{document}</tex-math></alternatives></inline-formula> and not <inline-formula><alternatives><mml:math id="inf124"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft124">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> because the model samples a non-centered parameterization. That is,<disp-formula id="equ37"><label>(30)</label><alternatives><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t37">\begin{document}$$\displaystyle  \tilde{\gamma}_{i}=\frac{\gamma_{i}-\theta}{\tau\kappa_{i}},\quad\tilde{\gamma}_{i}\sim\mathrm{Normal}(0,1)$$\end{document}</tex-math></alternatives></disp-formula></p><p>captures the deviations of <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula> around the population mean <inline-formula><alternatives><mml:math id="inf126"><mml:mstyle><mml:mi>θ</mml:mi></mml:mstyle></mml:math><tex-math id="inft126">\begin{document}$  \theta$\end{document}</tex-math></alternatives></inline-formula>.</p><p><xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7B</xref> plots samples from the marginal posterior <inline-formula><alternatives><mml:math id="inf127"><mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft127">\begin{document}$  p(\tilde{\gamma},\kappa_{i}|y_{i})$\end{document}</tex-math></alternatives></inline-formula>, when fitting to the data <inline-formula><alternatives><mml:math id="inf128"><mml:mstyle><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>770</mml:mn><mml:mo>,</mml:mo><mml:mn>820</mml:mn><mml:mo>,</mml:mo><mml:mn>713</mml:mn><mml:mo>,</mml:mo><mml:mn>541</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math><tex-math id="inft128">\begin{document}$  \mathbf{y}=\{770,820,713,541,0,0\}$\end{document}</tex-math></alternatives></inline-formula>. Each data point in <xref ref-type="table" rid="table1">Table 1</xref> corresponds to one of the six plots in <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7B</xref>. This small example dataset is, in fact, the cell counts for region TMd recorded from the heterozygote group in Case study 2. The similarities in geometry can be readily seen with <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7A</xref>. A large number of divergences were produced by the sampler as demonstrated by the number of pink points compared to the non-divergent transitions in blue. For fixed <inline-formula><alternatives><mml:math id="inf129"><mml:mstyle><mml:mi>τ</mml:mi></mml:mstyle></mml:math><tex-math id="inft129">\begin{document}$  \tau$\end{document}</tex-math></alternatives></inline-formula> the values <inline-formula><alternatives><mml:math id="inf130"><mml:mstyle><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math><tex-math id="inft130">\begin{document}$  \tilde{\gamma}$\end{document}</tex-math></alternatives></inline-formula> can take increase or decrease with smaller or larger <inline-formula><alternatives><mml:math id="inf131"><mml:mstyle><mml:mi>κ</mml:mi></mml:mstyle></mml:math><tex-math id="inft131">\begin{document}$  \kappa$\end{document}</tex-math></alternatives></inline-formula>, respectively. The Half-Cauchy places an extremely long right tail over <inline-formula><alternatives><mml:math id="inf132"><mml:mstyle><mml:mi>κ</mml:mi></mml:mstyle></mml:math><tex-math id="inft132">\begin{document}$  \kappa$\end{document}</tex-math></alternatives></inline-formula> that frustrates this relationship, resulting in a posterior density that is difficult to sample from.</p></sec><sec sec-type="appendix" id="s15"><title>Modified horseshoe</title><p>For a Poisson model with parameters modeled on the log scale, we consider the Cauchy parameterization to be too extreme. In light of this, we opted for a pragmatic approach, a modification to the original horseshoe by replacing the HalfCauchy distribution with a HalfNormal distribution. The modified horseshoe cuts off the top of the funnels by restricting <inline-formula><alternatives><mml:math id="inf133"><mml:mstyle><mml:mi>κ</mml:mi></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$  \kappa$\end{document}</tex-math></alternatives></inline-formula> to produce pleasant posterior geometry (<xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7A and B</xref>). The modified horseshoe is much easier to sample from, but with the cost of a much more constraining prior over <inline-formula><alternatives><mml:math id="inf134"><mml:mstyle><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft134">\begin{document}$  \gamma_{i}$\end{document}</tex-math></alternatives></inline-formula>.</p><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Horseshoe densities.</title><p>(<bold>A</bold>) Conditional posterior. (<bold>B</bold>) MCMC pair plots. Divergent samples are colored in pink, non-divergent in blue.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Modified horseshoe densities.</title><p>(<bold>A</bold>) The conditional posterior <inline-formula><alternatives><mml:math id="inf135"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>κ</mml:mi><mml:mo>∣</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft135">\begin{document}$p(\tilde{\gamma}, \kappa \mid \theta, \tau, y)$\end{document}</tex-math></alternatives></inline-formula> when <italic>y</italic> = 0 (left) and <italic>y</italic> ≠ 0 (right). (<bold>B</bold>) MCMC pair plots of samples from the marginal posterior density <inline-formula><alternatives><mml:math id="inf136"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>κ</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft136">\begin{document}$p(\tilde{\gamma}, \kappa \mid \mathbf{y})$\end{document}</tex-math></alternatives></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102391-app1-fig8-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102391.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This study proposes an <bold>important</bold> new approach to analyzing cell-count data, which are often undersampled and cannot be accurately assessed using traditional statistical methods. The case studies presented in the article provide <bold>compelling</bold> evidence of the superiority of the proposed methodology over existing approaches, which could promote the use of Bayesian statistics among neuroscientists. The authors have taken steps to make the methodology accessible, although some implementation difficulties are likely to remain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102391.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This work proposes a new approach to analyse cell-count data from multiple brain regions. Collecting such data can be expensive and time-intensive, so, more often than not, the dimensionality of the data is larger than the number of samples. The authors argue that Bayesian methods are much better suited to correctly analyse such data compared to classical (frequentist) statistical methods. They define a hierarchical structure, partial pooling, in which each observation contributes to the population estimate to more accurately explain the variance in the data. They present two case studies in which their method proves more sensitive in identifying regions where there are significant differences between conditions, which otherwise would be hidden.</p><p>Strengths:</p><p>The model is presented clearly, and the advantages of the hierarchical structure are strongly justified. Two alternative ways are presented to account for the presence of zero counts. The first involves the use of a horseshoe prior, which is the more flexible option, while the second involves a modified Poisson likelihood, which is better suited to datasets with a large number of zero counts, perhaps due to experimental artifacts. The results show a clear advantage of the Bayesian method for both case studies.</p><p>The code is freely available, and it does not require a high-performance cluster to execute for smaller datasets. As Bayesian statistical methods become more accessible in various scientific fields, the whole scientific community will benefit from the transition away from p-values. Hierarchical Bayesian models are an especially useful tool that can be applied to many different experimental designs. However, while conceptually intuitive, their implementation can be difficult. The authors provide a good framework with room for improvement.</p><p>Weaknesses:</p><p>As with any Bayesian model, the choice of prior can significantly influence the results. The authors explain how the methodology can be adapted to different data properties, though selecting an appropriate prior or likelihood may not always be straightforward. They propose a 'standard workflow' as an alternative to traditional approaches, which could and should be used alongside established methods while Bayesian techniques continue to evolve and improve.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102391.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dimmock</surname><given-names>Sydney</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Exley</surname><given-names>Benjamin MS</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Moore</surname><given-names>Gerald</given-names></name><role specific-use="author">Author</role><aff><institution>Imperial College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Menage</surname><given-names>Lucy</given-names></name><role specific-use="author">Author</role><aff><institution>Imperial College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Delogu</surname><given-names>Alessio</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220mzb33</institution-id><institution>King's College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Schultz</surname><given-names>Simon R</given-names></name><role specific-use="author">Author</role><aff><institution>Imperial College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Warburton</surname><given-names>Clea</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Houghton</surname><given-names>Conor J</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>O'Donnell</surname><given-names>Cian</given-names></name><role specific-use="author">Author</role><aff><institution>University of Ulster</institution><addr-line><named-content content-type="city">Derry</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>“Alternative possibilities are discussed regarding the prior and likelihood of the model. Given that the second case study inspired the introduction of the zero-inflation likelihood, it is not clear how applicable the general methodology is to various datasets. If every unique dataset requires a tailored prior or likelihood to produce the best results, the methodology will not easily replace more traditional statistical analyses that can be applied in a straightforward manner. Furthermore, the differences between the results produced by the two Bayesian models in case study 2 are not discussed. In specific regions, the models provide conflicting results (e.g., regions MH, VPMpc, RCH, SCH, etc.), which are not addressed by the authors. A third case study would have provided further evidence for the generalizability of the methodology.”</p></disp-quote><p>We hope in this paper to propose a ‘standard workflow’ for these data; this standard workflow uses the horseshoe prior and we propose that this is the approach used to describe cell count data instead of the better established, but to our thinking, inefficient, t-testing approach.</p><p>The horseshoe prior is robust and allows a partially-pooled model to used while weighing-up the contribution of different data points. This is an analogue of excluding outliers and, in any analysis it is normal to investigate further if there are points being excluded as outliers. Often, this reveals a particular challenge with the data, in the case of the data here, there are a lot of zeros, indicating that some samples should be excluded because the preparation failed to tag cells rather than because there were no cells to tag. This idea behind the ZIP example is to show that the Bayesian method can allow for this sort of further investigation and, indeed, as the reviewer notes this sort of extended analysis is often bespoke, tailored to the data.</p><p>We have clearly failed to explain that the ‘standard workflow’ we propose replace the more traditional methods is the first one we describe, with the horseshoe prior; this produces better results on both datasets than the traditional approach. However, we also feel it is useful to show how a more tailored follow-on can be useful; we need to make it clear that this is intended as an illustration of an ‘optional extra’ rather than a part of the more straightforward ‘standard workflow’.</p><p>To make this clearer we have made altered the text in several locations:</p><p>• end of Introduction: added clarifying sentence “Here, our aim is to introduce a ‘standard’ Bayesian model for cell count data. We illustrate the application of this model to two datasets, one related to neural activation and the other to developmental lineage. For the second dataset, we also demonstrate a second example extension Bayesian model.”</p><p>• Section Hierarchical modeling: “Our goal in both cases is to quantify group differences in the data. We present a ‘standard’ hierarchical model. This model reflects the experimental features common to cell count experiments and reflects the hierarchical structure of cell count data; the standard model is designed to deal robustly and efficiently with noise. On some occasions, to reflect a specific hypotheses, the structure of a particular experiment or an observed source of noise, this model can be further refined or changed to target the analysis. We will give an example of this for our second dataset.”</p><p>• Section Horseshoe prior: “The alternative is via a flexible prior such as the horseshoe Carvalho et al., 2010; Piironen and Vehtari, 2017. This more generic option may be suitable as a default ‘standard’ approach in the typical case where outliers are poorly understood.”</p><p>• Discussion: word ‘standard’ added to sentence: “Our standard workflow uses a horseshoe prior, along with the partial pooling, this allows our model to deal effectively with outliers.”</p><p>• Discussion: modified sentence “The horseshoe prior model workflow we have exhibited here is intended as a standard approach.”</p><p>Indeed, because the horseshoe prior deals robustly with outliers, whereas the ZIP is intended to model the outliers, any substantial difference between the two should be examined carefully. The referee is right to point out that we have not explained this in any detail and has helpfully listed a few brain regions were there are differences. This is useful, particularly since the examples listed illustrate in a useful way the opportunities and hazards this sort of data presents. To address this, we have added a new version of Figure 6 to the revised manuscript</p><p>Previously Figure 6 showed two example brain regions: MPN and TMd. We have now added MH and SCH to the figure, and new text commenting on the insights the plots provide, both in the Results and Discussion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>“A clearer link between the experimental data and model-structure terminology would be a benefit to the non-expert reader.”</p></disp-quote><p>This is a very good point and we are acutely aware through our own work how difficult it can be moving between fields with different research goals, different scientific cultures and different technical vocabularies. Just as it can be difficult translating from one language to another without losing nuance and meaning, it can be a real challenge finding technical terms that are useful for the non-expert reader while retaining the precision the application requires! In the long run, we hope that, just as some of the very specialized vocabulary that surrounds frequentist statistics has become familiar to to the working experimental scientists, the precise terminology involved in Bayesian modelling will become familiar and transparent. However, in advance of that day, we have included a glossary of terms at the end of the main text, and have made numerous small tweaks to make sure that link between data and model terminology is clearer and better explained.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations fro the authors):</bold></p><p>(1) “I would strongly recommend that the authors include more case studies in the manuscript, and address the qualitative differences between the different versions of the model.”</p></disp-quote><p>We agree that our method will only become established when it is applied to more datasets, we hope to contribute to further analysis and we know other people are already using the approach on their own data. We do, however, feel that adding more datasets to this paper will make it longer and more complex; the plan, instead, is to use the method on novel datasets to test specific hypotheses, so that the results will include novel scientific findings as well as adding another illustration of the Bayesian approach applied to data that is already well studied.</p><disp-quote content-type="editor-comment"><p>(2) “Figure 6 is not discussed in the main text.”</p></disp-quote><p>We had discussed the results presented in Figure 6 in the second paragraph of the section “Case study two – Ontogeny of inhibitory interneurons of the mouse thalamus”, however the reviewer is right in that we did not directly refer to the Figure – this was an oversight. In any case, in the revised manuscript we present a new version of Figure 6 (in response to above comment), which is now explicitly cited in the text.</p><p>Revised Figure 6: Example data and inferences highlighting model discrepancies. On the left under ‘data’: boxplots with medians and interquartile ranges for the raw data for four example brain regions. The shape of each point pairs left and right hemisphere readings in each of the five animals. On the right under ‘inference’: HDIs and confidence intervals are plotted. Purple is the Bayesian horseshoe model, pink is the Bayesian ZIP model, and orange is the sample mean. The Bayesian estimates are not strongly influenced by the zero-valued observations (MPN, SCH, TMd) or large-valued outliers (MH) and have means close to the data median. This explains the advantage of the Bayesian results over the confidence interval.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations from the authors):</bold></p><p>(1) “This is a generally well-written methodology paper that also provides the underlying code as a resource. As a reviewer outside both cell-count modelling and hierarchical-Bayesian approaches (though with a general interest in the topics) I found the method a little difficult to follow and would have liked to have been left with a better understanding of how the method is applied to the data. For example, in Figure 1 we are introduced to brain region count, animal count, and “items”. Then in the next line: pooling, model, structure, population and etc in subsequent lines. It is not clear what the subscripts (the pools?) are referring to: are they different regions R or animals N? These terms need to be better linked to the data and/or trimmed. Having said that, the later results look like a solid contribution to the field with a significant reduction in uncertainty from the Bayesian approach over the frequentist one. A future version of the manuscript, therefore, would benefit from greater precision of language as well as an economy and greater focus of terms linking the method to the biology. This is particularly the case around the exposition parts in Figure 1, Figure 2, and the “Hierarchical modelling” section.”</p></disp-quote><p>This is another important point. We have now made numerous small changes to tighten up the text in the paper, in response to both this point and the next point.</p><disp-quote content-type="editor-comment"><p>(2) “Language throughout could be sharpened. Subjectivity like “surprising outliers” could be removed and quirky grammar like “often small, ten is a typical” improved. There are also typos “an rate” etc that should be tidied up.”</p></disp-quote><p>As per previous response, we have made numerous tweaks and small improvements and feel that the paper is stronger in this respect.</p><disp-quote content-type="editor-comment"><p>(3) “Figure 1 caption. “It is a spectrum that depends” Is spectrum the right word here? Also, “thicker stroke” what does this refer to? Wasn’t immediately clear. In A, why is the whole animal within the R bracket that signifies brain regions, and then the brain regions are within the N bracket that signifies whole animals? Apart from the teal colouring, what are the other coloured regions in the image referring to? Improving this first figure would greatly help a reader unfamiliar with the context of the approach.”</p></disp-quote><p>We have replaced the word “spectrum” with “continuum”. We have replaced “ Observed quantities have been highlighted with a thicker stroke in the graphical model.” with “The observed data quantities, y<sub>i</sub> to y<sub>n</sub>, are highlighted with a thick line in the model diagrams”. We have added the following text to describe the red and green lines in panel A: “green and red lines indicate regions labeled as damaged”.</p><disp-quote content-type="editor-comment"><p>(4) “On P2 there is no discussion of priors when running through the advantage of the Bayesian approach. Is this a choice or an oversight? Priors do have a role in the later analysis.”</p></disp-quote><p>A short additional paragraph has been added to the introduction outlining the advantage of having a prior, but also noting that the obligation to pick a prior can be intimidating and that suggesting priors is one of the contributions of our paper: “A Bayesian model also includes a set of probability distributions, referred to as the prior, which represent those beliefs it is reasonable to hold about the statistical model parameters before actually doing the experiment. The prior can be thought of as an advantage, it allows us to include in our analysis our understanding of the data based on previous experiments. The prior also makes explicit in a Bayesian model assumptions that are often implicit in other approaches. However, having to design priors is often considered a challenge and here we hope to make this more straightforward by suggesting priors that are suitable for this class of data.”</p><disp-quote content-type="editor-comment"><p>(5) “On P4 more explanation would help greatly. Formulas like 23*10*4 or 50*6+50*4 are presented without explanation. What are the various numbers being multiplied? Regions, animals? Again, a clearer link between biological data and model structure would be advantageous.”</p></disp-quote><p>We have now modified this line to clearly state the numbers’ sources: “The index i runs over the full set of samples, which in this case comprises 23 brain regions ×10 animals ×4 groups ≈920 datapoints in the first study, and 50 brain regions × 6 HET animals + 50 brain regions × 4 KO animals ≈500 datapoints in the second.”</p><disp-quote content-type="editor-comment"><p>(6) “P6 and Results. Is it possible to show examples of the data set sampled from? Perhaps an image or two for the two experiments. Both Figures 4 and 5 as they currently are could be made slightly smaller to provide space for a small explanatory sub-panel. This would help ground the results.”</p></disp-quote><p>This is a good idea. We have now added heatmap visualisations of both entire datasets to revised versions of Figures 4 and 5 (assuming that this is what the reviewer was suggesting).</p></body></sub-article></article>