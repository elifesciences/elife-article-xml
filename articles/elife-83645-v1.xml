<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83645</article-id><article-id pub-id-type="doi">10.7554/eLife.83645</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The shared and unique neural correlates of personal semantic, general semantic, and episodic memory</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-293400"><name><surname>Tanguay</surname><given-names>Annick FN</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6964-3077</contrib-id><email>atang027@uottawa.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-133351"><name><surname>Palombo</surname><given-names>Daniela J</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-294493"><name><surname>Love</surname><given-names>Brittany</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-295007"><name><surname>Glikstein</surname><given-names>Rafael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-294495"><name><surname>Davidson</surname><given-names>Patrick SR</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-193287"><name><surname>Renoult</surname><given-names>Louis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7861-0552</contrib-id><email>L.Renoult@uea.ac.uk</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>School of Psychology, University of Ottawa</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026k5mg93</institution-id><institution>School of Psychology, University of East Anglia</institution></institution-wrap><addr-line><named-content content-type="city">Norwich</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03rmrcq20</institution-id><institution>Department of Psychology, University of British Columbia</institution></institution-wrap><addr-line><named-content content-type="city">Vancouver</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e83645</elocation-id><history><date date-type="received" iso-8601-date="2022-09-22"><day>22</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-10-25"><day>25</day><month>10</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-01-19"><day>19</day><month>01</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/rsb4g"/></event></pub-history><permissions><copyright-statement>© 2023, Tanguay et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Tanguay et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83645-v1.pdf"/><abstract><p>One of the most common distinctions in long-term memory is that between semantic (i.e., general world knowledge) and episodic (i.e., recollection of contextually specific events from one’s past). However, emerging cognitive neuroscience data suggest a surprisingly large overlap between the neural correlates of semantic and episodic memory. Moreover, personal semantic memories (i.e., knowledge about the self and one’s life) have been studied little and do not easily fit into the standard semantic-episodic dichotomy. Here, we used fMRI to record brain activity while 48 participants verified statements concerning general facts, autobiographical facts, repeated events, and unique events. In multivariate analysis, all four types of memory involved activity within a common network bilaterally (e.g., frontal pole, paracingulate gyrus, medial frontal cortex, middle/superior temporal gyrus, precuneus, posterior cingulate, angular gyrus) and some areas of the medial temporal lobe. Yet the four memory types differentially engaged this network, increasing in activity from general to autobiographical facts, from autobiographical facts to repeated events, and from repeated to unique events. Our data are compatible with a component process model, in which declarative memory types rely on different weightings of the same elementary processes, such as perceptual imagery, spatial features, and self-reflection.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>declarative memory</kwd><kwd>episodic memory</kwd><kwd>fMRI</kwd><kwd>personal semantics</kwd><kwd>semantic memory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000286</institution-id><institution>British Academy</institution></institution-wrap></funding-source><award-id>SG142524</award-id><principal-award-recipient><name><surname>Renoult</surname><given-names>Louis</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>RGPIN-2019-04596</award-id><principal-award-recipient><name><surname>Palombo</surname><given-names>Daniela J</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Davidson</surname><given-names>Patrick SR</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Four types of declarative memory rely on different weightings of similar component processes, according to fMRI and subjective ratings.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>One of the most fundamental distinctions in human long-term memory is that between semantic and episodic (<xref ref-type="bibr" rid="bib48">Greenberg and Verfaellie, 2010</xref>; <xref ref-type="bibr" rid="bib131">Squire, 2009</xref>; <xref ref-type="bibr" rid="bib139">Tulving, 1972</xref>; <xref ref-type="bibr" rid="bib140">Tulving, 1983</xref>; <xref ref-type="bibr" rid="bib142">Tulving, 2002</xref>). Semantic memory refers to one’s non-personal, general knowledge of the world (e.g., <italic>I know that yoga is a relaxing form of exercise</italic>), whereas episodic memory concerns the recollection of contextually specific events from one’s personal past (<italic>I remember arriving quite late to my yoga class this weekend, which made my teacher angry</italic>). This distinction is classic (<xref ref-type="bibr" rid="bib59">Herrmann, 1982</xref>; <xref ref-type="bibr" rid="bib110">Renoult and Rugg, 2020</xref>) and remains crucial for cognition and neuroscience today. Importantly, although these two putative memory systems have long been recognized as ‘partially overlapping’ (<xref ref-type="bibr" rid="bib139">Tulving, 1972</xref>) and interacting (<xref ref-type="bibr" rid="bib48">Greenberg and Verfaellie, 2010</xref>), they have largely been investigated via separate research traditions.</p><p>In recent years, however, emerging cognitive neuroscience data have suggested a surprisingly large overlap between the neural correlates of semantic and episodic memory. For example, when one compares the functional neuroimaging findings from the semantic memory literature (a ‘general semantic network’; <xref ref-type="bibr" rid="bib15">Binder, 2016</xref>; <xref ref-type="bibr" rid="bib13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="bib14">Binder and Desai, 2011</xref>) to those from the episodic memory literature (an ‘episodic core recollection network’; <xref ref-type="bibr" rid="bib121">Rugg and Vilberg, 2013</xref>; <xref ref-type="bibr" rid="bib135">Svoboda et al., 2006</xref>), it becomes evident that the two networks share the midline frontal, middle temporal, parahippocampal, ventral parietal, and midline posterior regions (<xref ref-type="bibr" rid="bib109">Renoult et al., 2019</xref>). Although based on the previous literature one might expect that the hippocampus would be much more strongly associated with episodic memory, and the anterior temporal lobe with semantic memory (<xref ref-type="bibr" rid="bib75">Lambon Ralph et al., 2017</xref>), recent research suggests otherwise: The hippocampus can facilitate the acquisition and retrieval of a rich semantic network (<xref ref-type="bibr" rid="bib16">Blumenthal et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Cutler et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Klooster et al., 2020</xref>; <xref ref-type="bibr" rid="bib124">Sheldon and Moscovitch, 2012</xref>), partly through relational processing (<xref ref-type="bibr" rid="bib37">Duff et al., 2019</xref>) or pattern completion (<xref ref-type="bibr" rid="bib129">Solomon and Schapiro, 2020</xref>). Anterior hippocampal atrophy features in neurodegenerative diseases affecting semantic and episodic memory alike (<xref ref-type="bibr" rid="bib22">Chapleau et al., 2016</xref>). Similarly, the lateral anterior temporal lobe may also be involved in episodic recollection, as shown by its recruitment when successfully learning word pairs (<xref ref-type="bibr" rid="bib33">de Chastelaine et al., 2016</xref>; <xref ref-type="bibr" rid="bib109">Renoult et al., 2019</xref>).</p><p>The considerable overlap between the two has sparked a rethinking of the classic semantic-episodic dichotomy (<xref ref-type="bibr" rid="bib109">Renoult et al., 2019</xref>). A major impediment, though, has been that a large proportion of the existing data come from indirect comparisons of semantic and episodic memory between different experiments. As mentioned above, this is partly due to semantic and episodic memory being investigated in somewhat separate fields of study: comprehension of the world through language or object identification for example (<xref ref-type="bibr" rid="bib14">Binder and Desai, 2011</xref>; <xref ref-type="bibr" rid="bib72">Kumar, 2021</xref>; <xref ref-type="bibr" rid="bib75">Lambon Ralph et al., 2017</xref>) vs. memory retrieval to think about the past, and particularly, personal events (<xref ref-type="bibr" rid="bib82">Maguire, 2001</xref>; <xref ref-type="bibr" rid="bib135">Svoboda et al., 2006</xref>). Memory researchers sometimes control for semantic or basic linguistic processes, but rarely fully match the semantic and episodic aspects or compare them. This may stem from the difficulty in minimizing any differences between semantic and episodic memory on task demands, including retrieval times (<xref ref-type="bibr" rid="bib110">Renoult and Rugg, 2020</xref>), and other cognitive operations (e.g., control; <xref ref-type="bibr" rid="bib146">Vatansever et al., 2021</xref>).</p><p>Another critical issue is that several types of declarative memory do not fit easily into the standard semantic-episodic dichotomy. This is the case for ‘personal semantics’, which involves knowledge of one’s personal past (<xref ref-type="bibr" rid="bib25">Conway, 2005</xref>; <xref ref-type="bibr" rid="bib51">Grilli and Verfaellie, 2014</xref>; <xref ref-type="bibr" rid="bib52">Grilli and Verfaellie, 2015</xref>; <xref ref-type="bibr" rid="bib85">Martinelli et al., 2013</xref>; <xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>). Personal semantics lives conceptually between (or, perhaps, across) the boundaries of semantic and episodic memory. It is personal (like episodic memories), but detached from its context of acquisition (like semantic memories). Personal semantics includes personal factual knowledge, such as autobiographical facts (e.g., <italic>I am adept at yoga</italic>), and knowledge of repeated personally experienced events, including contextual details that have been abstracted across several instances (e.g., <italic>my yoga routine when going to the gym</italic>). Many early descriptions assumed that personal semantics was part of semantic memory (<xref ref-type="bibr" rid="bib21">Cermak and O’Connor, 1983</xref>; <xref ref-type="bibr" rid="bib70">Kopelman et al., 1989</xref>), but more recent evaluations suggest that this view was too simple: whereas some forms of personal semantics—such as autobiographical facts—appear to have neural correlates similar to semantic memory, others—such as memories of repeated events—have neural correlates that are more similar to those of episodic memory. For instance, in amnesic patients, memories of unique and repeated episodes are often impaired together, whereas knowledge of general and personal facts are typically better preserved (reviewed in <xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>).</p><p>Personal semantics is an understudied form of memory relative to semantic and episodic memory, even though it has been reported to be the most common form of autobiographical memory elicited in free and cued recall (<xref ref-type="bibr" rid="bib11">Barsalou, 1988</xref>), and in brain stimulation studies (<xref ref-type="bibr" rid="bib28">Curot et al., 2017</xref>). Personal semantics play an important role in the retrieval of specific events (<xref ref-type="bibr" rid="bib23">Conway, 1987</xref>; <xref ref-type="bibr" rid="bib24">Conway and Pleydell-Pearce, 2000</xref>; <xref ref-type="bibr" rid="bib56">Haque and Conway, 2001</xref>), and such facilitation can vary with the type of personal semantics (e.g., repeated events induce a greater facilitation than autobiographical facts; <xref ref-type="bibr" rid="bib127">Sheldon et al., 2020</xref>). The conceptualization of personal semantics is grounded in the close proximity between semantic and episodic memory (as reviewed above; <xref ref-type="bibr" rid="bib37">Duff et al., 2019</xref>; <xref ref-type="bibr" rid="bib109">Renoult et al., 2019</xref>). For instance, some general semantic concepts, such as knowledge about unique entities like famous individuals, can be ‘autobiographically significant’ and are tightly associated with episodic memories (<xref ref-type="bibr" rid="bib74">Lambert, 2020</xref>; <xref ref-type="bibr" rid="bib107">Renoult et al., 2015</xref>; <xref ref-type="bibr" rid="bib150">Westmacott et al., 2004</xref>; <xref ref-type="bibr" rid="bib149">Westmacott and Moscovitch, 2003</xref>).</p><p>Despite the apparent importance of personal semantics and the notable interest generated by a taxonomy of personal semantics (<xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>), few functional neuroimaging studies have directly compared personal semantics to either semantic or episodic memory, and even fewer have compared different types of personal semantics to one another or personal semantics to both semantic and episodic memory. On the one hand, these have indicated some differences across general semantic, personal semantic, and episodic memory: Autobiographical facts have been associated with greater activity in the left medial frontal cortex and retrosplenial cortex than general facts (<xref ref-type="bibr" rid="bib83">Maguire and Frith, 2003</xref>), but have not differed in lateral temporal activity (<xref ref-type="bibr" rid="bib83">Maguire and Frith, 2003</xref>; <xref ref-type="bibr" rid="bib81">Maguire and Mummery, 1999</xref>). Repeated events have elicited less activity than unique events in the frontal pole, parahippocampal gyrus, posterior cingulate, and precuneus (<xref ref-type="bibr" rid="bib42">Ford et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Holland et al., 2011</xref>; <xref ref-type="bibr" rid="bib78">Levine et al., 2004</xref>), but not differed in hippocampal activity (notably, <xref ref-type="bibr" rid="bib3">Addis et al., 2004a</xref>; <xref ref-type="bibr" rid="bib4">Addis et al., 2004b</xref>). On the other hand, many existing data are compatible with the idea that these different types of memories involve different levels of activity within a common network. For instance, several neuroimaging studies have reported a graded increase in activity across semantic memories, autobiographical facts, and episodic memories in medial prefrontal, temporal polar, and retrosplenial cortex (<xref ref-type="bibr" rid="bib82">Maguire, 2001</xref>; <xref ref-type="bibr" rid="bib83">Maguire and Frith, 2003</xref>; <xref ref-type="bibr" rid="bib81">Maguire and Mummery, 1999</xref>). In a recent event-related potential study (<xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>) that compared semantic memory (i.e., general facts), two types of personal semantics (i.e., autobiographical facts and repeated events) and episodic memory (i.e., unique events), a similar graded pattern occurred. Both types of personal semantics produced intermediate mean amplitudes for the N400, an index of semantic processing (<xref ref-type="bibr" rid="bib73">Kutas and Federmeier, 2011</xref>), and for the late positive component, an index of episodic processing (<xref ref-type="bibr" rid="bib120">Rugg and Curran, 2007</xref>), compared to general facts and unique events (<xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>). The two types of personal semantics did not differ from one another in this study (<xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>).</p><p>Given the questions about the relation between memory types across the semantic-episodic spectrum, here we used fMRI to directly compare brain activity during general semantic, episodic, and personal semantic processing within the same participants. They indicated whether closely matched sentences were true in four conditions: (1) facts about people in general (i.e., general facts, reflective of semantic memory), (2) personal events that happened once (i.e., unique events, reflective of episodic memory), (3) facts about themselves (i.e., autobiographical facts, a type of personal semantics), and (4) events that happened repeatedly (i.e., repeated events, a type of personal semantics). The design enabled us to make as close of a comparison as possible between memory conditions. For example, the stimuli differed minimally in wording across conditions, being adjusted only in self-reference (i.e., referring to the self in all conditions but general facts) and temporal specificity (general for both types of facts, somewhat more specific for repeated events, and very specific for unique events). Further, even though semantic and episodic retrieval typically involve different retrieval times, our comparison of these memory types is unbiased by response time differences between conditions (<xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>).</p><p>We leveraged multivariate analysis methods (i.e., partial least squares [PLS]; <xref ref-type="bibr" rid="bib71">Krishnan et al., 2011</xref>; <xref ref-type="bibr" rid="bib88">McIntosh et al., 2004a</xref>; <xref ref-type="bibr" rid="bib89">McIntosh and Lobaugh, 2004b</xref>) to compare brain activity across these four memory conditions within a single analysis in a sample of 48 participants. Our analytical approach tested whether data in the spatial and temporal domain explained adequately two hypothesized relations between memory conditions: (1) a continuum of contextual specificity, and (2) a dissociation between knowledge of facts and recollection of events.</p><p>Although we expected memory types to be distinguishable, we also expected that the neural correlates of autobiographical facts would appear more similar to general facts whereas the neural correlates of repeated events would appear more similar to unique events (<xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>). Further, the relation between memory types could be best described as one of a continuum: They would engage predominantly a common set of regions from the core memory network, but with increased intensity from the least (i.e., general facts) to the most (i.e., unique events) contextually specific memory type. These relations would depend on the differential engagement of component processes, such as sensory-perceptual imagery, spatial and temporal features, and self-reflection, which should be observed at the cognitive and brain level.</p><p>In a behavioral study, we considered three key component processes that could dissociate the four memory types: self-relevance (expecting it to be lower for general facts than all personal forms of memory), visual details (expecting a linear increase), and the integration of details within a scene (expecting it to be low for general/autobiographical facts and high for repeated/unique events). Accordingly, brain regions involved in visuospatial processing and imagery, like the precuneus, and in self-reflection, like the medial prefrontal cortex, should be more tightly associated with memories of unique events, and minimally associated with general factual knowledge (<xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>). Similarly, brain regions known to have a crucial role in spatial processing and in representing scenes, such as the hippocampus, parahippocampal cortex, and retrosplenial cortex, should show greater activity for memories of unique and repeated events than for autobiographical and general facts. The visual details and scene components may depend on hippocampal processes, like relational (<xref ref-type="bibr" rid="bib37">Duff et al., 2019</xref>) or constructive processes (<xref ref-type="bibr" rid="bib57">Hassabis and Maguire, 2007</xref>; <xref ref-type="bibr" rid="bib122">Schacter and Addis, 2007</xref>), to some extent, as the association with a diverse source of personal information becomes stronger from general facts to unique events (<xref ref-type="bibr" rid="bib127">Sheldon et al., 2020</xref>; <xref ref-type="bibr" rid="bib133">Sui and Humphreys, 2015</xref>).</p><p>In this study, we aimed to go beyond dichotomies commonly used in memory research (e.g., semantic vs. episodic, anterior vs. posterior brain regions, different vs. identical) to examine the multidimensional and complex relations across the spectrum in declarative memory, and importantly do so through direct comparisons. Our operationalization captures the prototypical definitions of several memory types, in close alignment with a taxonomy of personal semantics (<xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>), and possible characteristic function in daily life. Additionally, the analyses aim to uncover patterns that could act like heuristics to characterize declarative memory function. Critically, however, our additional focus on component processes in the behavioral study (i.e., amount of visual details, ability to evoke a scene, self-relevance) relies on the theoretical perspective that relations between memory types can be explained through the information accessed and cognitive processes engaged. Thus, it is implicit to our approach that the relations across memory types are not rigid and could be altered depending on task or personal goal (e.g., <xref ref-type="bibr" rid="bib53">Grilli and Verfaellie, 2016</xref>). This study seeks to develop a framework suitable to bridge the divide in research about semantic and episodic aspects in declarative memory, and offers a complementary approach to explore the multiplicity of factors that coalesce to define the mnesic experience.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral study</title><p>A sample of 106 participants rated their subjective experience during memory retrieval, using the same paradigm as the fMRI study, but were not scanned. Repeated-measures ANOVA showed that the main effect of memory was significant for all dependent variables (<italic>p</italic>s &lt; .001). Self-relevance was lowest for general facts compared to all personal forms of memory, and lower for personal semantics (i.e., autobiographical facts and repeated events) than unique events (see <xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Additionally, the amount of visual detail increased from general facts to autobiographical facts to repeated events to unique events (see <xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title><italic>t</italic>-test values for pairwise comparisons of memory types on self-relevance and visual details.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Comparison</th><th align="left" valign="bottom"><italic>t</italic>(105)</th><th align="left" valign="bottom"><italic>p</italic></th><th align="left" valign="bottom"><italic>g</italic></th><th align="left" valign="bottom">CI 95% of <italic>g</italic></th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="5">Self-relevance</td></tr><tr><td align="left" valign="bottom"> General facts vs. autobiographical facts</td><td align="char" char="." valign="bottom">−13.94</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−1.35</td><td align="char" char="." valign="bottom">[–1.61, –1.08]</td></tr><tr><td align="left" valign="bottom"> General facts vs. repeated events</td><td align="char" char="." valign="bottom">−12.23</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−1.18</td><td align="char" char="." valign="bottom">[–1.43, –0.93]</td></tr><tr><td align="left" valign="bottom"> General facts vs. unique events</td><td align="char" char="." valign="bottom">−14.90</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−1.44</td><td align="char" char="." valign="bottom">[–1.71, –1.17]</td></tr><tr><td align="left" valign="bottom"> Autobiographical facts vs. repeated events</td><td align="char" char="." valign="bottom">−0.61</td><td align="char" char="." valign="bottom">.541</td><td align="char" char="." valign="bottom">−0.06</td><td align="char" char="." valign="bottom">[–0.25, 0.13]</td></tr><tr><td align="left" valign="bottom"> Autobiographical facts vs. unique events</td><td align="char" char="." valign="bottom">−4.49</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.44</td><td align="char" char="." valign="bottom">[–0.63, –0.24]</td></tr><tr><td align="left" valign="bottom"> Repeated events vs. unique events</td><td align="char" char="." valign="bottom">−4.32</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.42</td><td align="char" char="." valign="bottom">[–0.62, –0.22]</td></tr><tr><td align="left" valign="bottom" colspan="5">Visual details</td></tr><tr><td align="left" valign="bottom"> General facts vs. autobiographical facts</td><td align="char" char="." valign="bottom">−5.14</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.50</td><td align="char" char="." valign="bottom">[–0.70, –0.30]</td></tr><tr><td align="left" valign="bottom"> General facts vs. repeated events</td><td align="char" char="." valign="bottom">−6.56</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.64</td><td align="char" char="." valign="bottom">[–0.84, –0.43]</td></tr><tr><td align="left" valign="bottom"> General facts vs. unique events</td><td align="char" char="." valign="bottom">−8.98</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.87</td><td align="char" char="." valign="bottom">[–1.09, –0.64]</td></tr><tr><td align="left" valign="bottom"> Autobiographical facts vs. repeated events</td><td align="char" char="." valign="bottom">−2.07</td><td align="char" char="." valign="bottom">.041*</td><td align="char" char="." valign="bottom">−0.20</td><td align="char" char="." valign="bottom">[–0.39, –0.01]</td></tr><tr><td align="left" valign="bottom"> Autobiographical facts vs. unique events</td><td align="char" char="." valign="bottom">−5.46</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.53</td><td align="char" char="." valign="bottom">[–0.73, –0.33]</td></tr><tr><td align="left" valign="bottom"> Repeated events vs. unique events</td><td align="char" char="." valign="bottom">−3.45</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.33</td><td align="char" char="." valign="bottom">[–0.53, –0.14]</td></tr></tbody></table><table-wrap-foot><fn><p>Note: *Significant after correction for multiple comparisons.</p></fn></table-wrap-foot></table-wrap><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Violin plots of (<bold>A</bold>) self-relevance ratings and (<bold>B</bold>) visual details ratings per condition.</title><p>Red points represent scores of individual participants (<italic>N</italic> = 106). A black line shows the median and red lines show the quartiles. *<italic>p</italic> &lt; .05, ***<italic>p</italic> &lt; .001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-fig1-v1.tif"/></fig><p>The conjunction of details within a spatial context, or a scene, is sometimes perceived as integral to the recollection of events (<xref ref-type="bibr" rid="bib58">Hassabis and Maguire, 2009</xref>). From that perspective, visual details and scene imagery can be dissociable constructs. A smaller proportion of general facts evoked a scene compared to the three personal memory types (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig2">Figure 2C</xref>). Similarly, a smaller proportion of autobiographical facts elicited images of a scene compared to repeated events and unique events. Repeated events and unique events did not differ in the proportion of memories perceived as scenes (see Appendix 1 for the statistical tests of vague/nothing and object responses).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title><italic>t</italic>-test values for pairwise comparisons of memory types on the proportion of scenes.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Comparison</th><th align="left" valign="bottom"><italic>t</italic>(105)</th><th align="left" valign="bottom"><italic>p</italic></th><th align="left" valign="bottom"><italic>g</italic></th><th align="left" valign="bottom">CI 95% of <italic>g</italic></th></tr></thead><tbody><tr><td align="left" valign="bottom">General facts vs. autobiographical facts</td><td align="char" char="." valign="bottom">−2.34</td><td align="char" char="." valign="bottom">.021*</td><td align="char" char="." valign="bottom">−0.23</td><td align="char" char="." valign="bottom">[–0.42, –0.03]</td></tr><tr><td align="left" valign="bottom">General facts vs. repeated events</td><td align="char" char="." valign="bottom">−6.51</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.63</td><td align="char" char="." valign="bottom">[–0.84, –0.42]</td></tr><tr><td align="left" valign="bottom">General facts vs. unique events</td><td align="char" char="." valign="bottom">−7.98</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.77</td><td align="char" char="." valign="bottom">[–0.99, –0.56]</td></tr><tr><td align="left" valign="bottom">Autobiographical facts vs. repeated events</td><td align="char" char="." valign="bottom">−4.75</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.46</td><td align="char" char="." valign="bottom">[–0.66, –0.26]</td></tr><tr><td align="left" valign="bottom">Autobiographical facts vs. unique events</td><td align="char" char="." valign="bottom">−6.52</td><td align="char" char="." valign="bottom">&lt; .001*</td><td align="char" char="." valign="bottom">−0.63</td><td align="char" char="." valign="bottom">[–0.84, –0.42]</td></tr><tr><td align="left" valign="bottom">Repeated events vs. unique events</td><td align="char" char="." valign="bottom">−1.28</td><td align="char" char="." valign="bottom">.204</td><td align="char" char="." valign="bottom">−0.12</td><td align="char" char="." valign="bottom">[–0.31, 0.07]</td></tr></tbody></table><table-wrap-foot><fn><p>Note: *Significant after correction for multiple comparisons.</p></fn></table-wrap-foot></table-wrap><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Violin plots of the proportion of yes responses that were perceived as (<bold>A</bold>) nothing/vague, (<bold>B</bold>) an object, (<bold>C</bold>) a scene.</title><p>Red points represent scores of individual participants (<italic>N</italic> = 106). A black line shows the median and red lines show the quartiles. *<italic>p </italic>&lt; .05, **<italic>p </italic>&lt; .01, ***<italic>p </italic>&lt; .001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>fMRI study</title><sec id="s2-2-1"><title>Non-rotated PLS with a priori contrast</title><p>We used a non-rotated PLS to test two theoretically plausible relations between the four memory conditions: a linear contrast (−3, –1, 1, 3) and one comparing general/autobiographical facts and repeated/unique events (−1, –1, 1, 1; abbreviated to facts vs. events subsequently). A linear contrast would be consistent with the continuum perspective of personal semantics (see Box 3 in <xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>), which would predict an increase in activity from general facts to autobiographical facts, from autobiographical facts to repeated events, and from repeated events to unique events. The increase in visual details (described above) followed precisely this pattern. Similarly, personal relevance increased from general facts to personal semantics (autobiographical facts and repeated events) to unique events, suggesting similar dynamics between component processes (e.g., contextual specificity may increase along with personal relevance). The facts vs. events contrast would favor the view of personal semantics as a subtype of general semantics (see Box 2 in <xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>). Of all personal semantics, autobiographical facts correspond best with this view due to its abstraction from events, its more objective quality than other forms of personal knowledge (e.g., trait knowledge), and the feeling of ‘knowing’ the ‘facts’ rather than recollecting events (i.e., ‘noetic’ consciousness; <xref ref-type="bibr" rid="bib142">Tulving, 2002</xref>). Repeated events would instead group with unique events as ‘event memory’ due to the common construction of a scene (<xref ref-type="bibr" rid="bib116">Rubin and Umanath, 2015</xref>). Indeed, participants in the behavioral study perceived scenes as frequently for repeated and unique events. However, it is less clear how one would accommodate the greater number of scenes evoked for autobiographical than general facts in a way that aligns with that perspective. Thus, the conjunction of the visualization of scenes, amount of visual details, and personal relevance agrees most with a continuum of contextual specificity.</p><p>The linear contrast and facts vs. events contrast were significant (<italic>p</italic>s &lt; .001), explaining 48.78% and 51.22% of the cross-block covariance respectively. Brain scores guide the interpretation of PLS results. A participant’s brain score for each task is derived from the multiplication of a voxel’s BOLD signal with how much it contributes to the latent variable (LV; i.e., its salience); the values from all voxels at all lags are then all summed together (see <xref ref-type="fig" rid="fig3">Figures 3a</xref> and <xref ref-type="fig" rid="fig4">4a</xref>; <xref ref-type="bibr" rid="bib71">Krishnan et al., 2011</xref>). Brain scores indicate how well each participant expressed the brain and task relation represented by the LV for each of the memory conditions. The temporal ‘brain scores’ show that the conditions were maximally dissociated at the seventh lag, that is the seventh brain volume after the first acquisition or 8.4 s post cue onset, during the response screen (see <xref ref-type="fig" rid="fig3">Figures 3c</xref> and <xref ref-type="fig" rid="fig4">4c</xref> and <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>). The maximal dissociation suggests this lag characterizes best the LV (see Appendices 2 and 3 for information on additional lags).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>This non-rotated partial least squares (PLS) tested a linear contrast (−3, –1, 1, 3).</title><p>This latent variable (LV) shows regions associated with an increase in activity from general facts to autobiographical facts to repeated events to unique events. (<bold>a</bold>) Average brain score. Error bars are ±1 <italic>SE</italic> of bootstrap estimates. (<bold>c</bold>) Brain scores shown at each lag (i.e., each TR/1.2 s). Error bars are ±1 <italic>SE</italic>. (<bold>b</bold> and <bold>d</bold>) Brain scores with positive saliences shown in warm colors (increased activity from general facts to autobiographical facts to repeated events to unique events) and negative saliences shown in cold colors (decreased activity from general facts to autobiographical facts to repeated events to unique events). Brain scores are projected onto a surface from the Human Connectome Project (S1200; <xref ref-type="bibr" rid="bib145">Van Essen et al., 2012</xref>) using Connectome Workbench (<xref ref-type="bibr" rid="bib84">Marcus et al., 2011</xref>) in (<bold>b</bold>) and the MNI152NLin2009cAsym volume using FSLeyes (<xref ref-type="bibr" rid="bib86">McCarthy, 2021</xref>), in (<bold>d</bold>). Bootstrap ratios are thresholded at ± 3, <italic>p</italic> &lt; .001, cluster size ≥ 80 voxels. See <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> for additional lags.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-fig3-v1.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>This non-rotated partial least squares (PLS) tested a contrast comparing general/autobiographical facts and repeated/unique events (−1, –1, 1, 1).</title><p>(<bold>a</bold>) Average brain score. Error bars are ±1 <italic>SE</italic> of bootstrap estimates. (<bold>c</bold>) Brain scores shown at each lag (i.e., each TR/1.2 s). Error bars are ±1 <italic>SE</italic>. (<bold>b</bold> and <bold>d</bold>) Brain scores with positive saliences shown in warm colors (increased activity for repeated/unique events relative to general/autobiographical) and negative saliences shown in cold colors (reduced activity for repeated/ unique events relative to general/autobiographical). Brain scores are projected onto a surface from the Human Connectome Project (S1200; <xref ref-type="bibr" rid="bib145">Van Essen et al., 2012</xref>) using Connectome Workbench (<xref ref-type="bibr" rid="bib84">Marcus et al., 2011</xref>) in (<bold>b</bold>) and the MNI152Nlin2009cAsym volume using FSLeyes (<xref ref-type="bibr" rid="bib86">McCarthy, 2021</xref>) in (<bold>d</bold>). Bootstrap ratios are thresholded at ± 3, <italic>p</italic> &lt; .001, cluster size ≥ 80 voxels. See <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref> for additional lags.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-fig4-v1.tif"/></fig><p>The linear contrast showed that activity increased from general to autobiographical facts, from autobiographical facts to repeated events, and from repeated to unique events bilaterally in the large regions of the frontal cortex (frontal pole, paracingulate gyrus, frontal medial cortex), precuneus, posterior cingulate cortex, retrosplenial cortex, angular gyrus, and with activity of the right middle frontal gyrus, left parahippocampal gyrus, left hippocampus, and left middle and superior temporal gyrus (see <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig5">5</xref>, <xref ref-type="table" rid="table3">Table 3</xref>, and Appendix 2 for additional lags). Activity progressively decreased from general to autobiographical facts, from autobiographical facts to repeated events, and from repeated to unique events in areas of the right frontal inferior gyrus, superior parietal lobule, supramarginal gyrus, and bilateral lateral occipital cortex. In supplementary analyses, the brain scores of each memory condition differed from one another (see Appendix 2).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Peaks of clusters for the linear contrast at lag 7.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Bootstrap ratio</th><th align="left" valign="bottom">Cluster size (voxels)</th><th align="left" valign="bottom">X (mm)</th><th align="left" valign="bottom">Y (mm)</th><th align="left" valign="bottom">Z (mm)</th><th align="left" valign="bottom">Harvard-Oxford, probability atlas</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="6">Negative saliences</td></tr><tr><td align="char" char="." valign="bottom"> −5.21</td><td align="char" char="." valign="bottom">362</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−80.0</td><td align="char" char="." valign="bottom">25.0</td><td align="left" valign="bottom">72.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> −4.76</td><td align="char" char="." valign="bottom">188</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">−50.0</td><td align="char" char="." valign="bottom">57.5</td><td align="left" valign="bottom">46.0% Right Superior Parietal Lobule; 20.0% Right Angular Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> −4.75</td><td align="char" char="." valign="bottom">103</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">−70.0</td><td align="char" char="." valign="bottom">5.0</td><td align="left" valign="bottom">84.0% Left Lateral Occipital Cortex inferior division</td></tr><tr><td align="char" char="." valign="bottom"> −4.61</td><td align="char" char="." valign="bottom">102</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">15.0</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">46.0% Right Inferior Frontal Gyrus pars opercularis</td></tr><tr><td align="left" valign="bottom" colspan="6">Positive saliences</td></tr><tr><td align="char" char="." valign="bottom"> 5.03</td><td align="char" char="." valign="bottom">113</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">30.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">63.0% Left Paracingulate Gyrus; 13.0% Left Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5.29</td><td align="char" char="." valign="bottom">116</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">20.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">22.0% Right Middle Frontal Gyrus; 19.0% Right Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6.10</td><td align="char" char="." valign="bottom">94</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−12.5</td><td align="left" valign="bottom">37.0% Left Middle Temporal Gyrus Anterior Division; 23.0% Left Superior Temporal Gyrus Anterior Division; 9.0% Left Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6.12</td><td align="char" char="." valign="bottom">105</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">15.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">54.0% Left Middle Frontal Gyrus; 5.0% Left Inferior Frontal Gyrus pars opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 6.24</td><td align="char" char="." valign="bottom">110</td><td align="char" char="." valign="bottom">−27.5</td><td align="char" char="." valign="bottom">−40.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">29.0% Left Parahippocampal Gyrus Posterior Division; 27.0% Left Lingual Gyrus; 8.0% Left Temporal Occipital Fusiform Cortex; 7.0% Left Temporal Fusiform Cortex Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6.25</td><td align="char" char="." valign="bottom">233</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−40.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">48.0% Left Cingulate Gyrus Posterior Division; 31.0% Left Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 6.65</td><td align="char" char="." valign="bottom">137</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6.85</td><td align="char" char="." valign="bottom">401</td><td align="char" char="." valign="bottom">−20.0</td><td align="char" char="." valign="bottom">30.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">51.0% Left Superior Frontal Gyrus; 8.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 7.19</td><td align="char" char="." valign="bottom">647</td><td align="char" char="." valign="bottom">2.5</td><td align="char" char="." valign="bottom">55.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">43.0% Right Frontal Pole; 28.0% Right Frontal Medial Cortex; 8.0% Right Paracingulate Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8.87</td><td align="char" char="." valign="bottom">444</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">44.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 10.36</td><td align="char" char="." valign="bottom">1118</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">54.0% Left Precuneus Cortex; 12.0% Left Supracalcarine Cortex</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Thresholded at bootstrap ratio at ± 3 (<italic>p</italic> &lt; .001), minimal cluster size of 80 voxels, and a minimal distance of 10 voxels. Labels from the Harvard-Oxford (<xref ref-type="bibr" rid="bib36">Desikan et al., 2006</xref>) atlas obtained using AtlasReader (<xref ref-type="bibr" rid="bib95">Notter et al., 2019</xref>).</p></fn></table-wrap-foot></table-wrap><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Percent BOLD signal change at MNI coordinates: (<bold>a</bold>) –27.5 –40 –10 (posterior division of the left parahippocampal gyrus), (<bold>b</bold>) –15.0 –62.5 22.5 (left precuneus cortex), (<bold>c</bold>) –57.5 –5 –12.5 (anterior division of the left middle temporal gyrus), (<bold>d</bold>) 2.5 55.0 –7.5 (right frontal pole), (<bold>e</bold>) –47.5 –70 5 (inferior division of the left lateral occipital cortex).</title><p>(<bold>a</bold> and <bold>c</bold>) were common peaks for the two contrasts (linear and facts vs. events). (<bold>b</bold>, <bold>d</bold>, and <bold>e</bold>) were peaks of the linear contrast, but facts vs. events had a peak at nearby location (i.e., –47.5 –70 2.5 to e). GF = general facts, AF = autobiographical facts; RE = repeated events; UE = unique events.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-fig5-v1.tif"/></fig><p>The facts vs. events contrast was similar to the linear contrast (see <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>, <xref ref-type="table" rid="table4">Table 4</xref>, and Appendix 3 for additional lags) with the first set of regions (i.e., frontal pole, precuneus, etc.) being associated with greater activity for events than facts, and the second set of regions (i.e., right frontal inferior gyrus, etc.) being associated with reduced activity for events than facts. In supplementary analyses, the statistical comparison of brain scores showed that general and autobiographical facts did not differ in how well they expressed the ‘facts’ portion of the LV, and repeated and unique events did not differ in how well they expressed the ‘events’ portion of the LV (see Appendix 3).</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Peaks of clusters for the facts vs. events contrast at lag 7.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Bootstrap ratio</th><th align="left" valign="bottom">Cluster size (voxels)</th><th align="left" valign="bottom">X (mm)</th><th align="left" valign="bottom">Y (mm)</th><th align="left" valign="bottom">Z (mm)</th><th align="left" valign="bottom">Harvard-Oxford, probability atlas</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="6">Negative saliences</td></tr><tr><td align="char" char="." valign="bottom"> −5.94</td><td align="left" valign="bottom">251</td><td align="left" valign="bottom">42.5</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">85.0% Right Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> −5.59</td><td align="left" valign="bottom">170</td><td align="left" valign="bottom">57.5</td><td align="left" valign="bottom">12.5</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">55.0% Right Inferior Frontal Gyrus pars opercularis; 23.0% Right Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> −5.30</td><td align="left" valign="bottom">465</td><td align="left" valign="bottom">57.5</td><td align="left" valign="bottom">−62.5</td><td align="left" valign="bottom">−10</td><td align="left" valign="bottom">52.0% Right Lateral Occipital Cortex Inferior Division; 15.0% Right Inferior Temporal Gyrus temporooccipital part; 9.0% Right Middle Temporal Gyrus temporooccipital part</td></tr><tr><td align="char" char="." valign="bottom"> −5.17</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">67.5</td><td align="left" valign="bottom">−42.5</td><td align="left" valign="bottom">22.5</td><td align="left" valign="bottom">19.0% Right Supramarginal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> −5.05</td><td align="left" valign="bottom">153</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">−70</td><td align="left" valign="bottom">55</td><td align="left" valign="bottom">58.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> −5.05</td><td align="left" valign="bottom">300</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">−35</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">NA</td></tr><tr><td align="char" char="." valign="bottom"> −4.70</td><td align="left" valign="bottom">202</td><td align="left" valign="bottom">−47.5</td><td align="left" valign="bottom">−70</td><td align="left" valign="bottom">2.5</td><td align="left" valign="bottom">87.0% Left Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="left" valign="bottom" colspan="6">Positive saliences</td></tr><tr><td align="char" char="." valign="bottom"> 4.32</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">−5</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">37.5</td><td align="left" valign="bottom">63.0% Left Paracingulate Gyrus; 13.0% Left Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 4.87</td><td align="left" valign="bottom">101</td><td align="left" valign="bottom">−10</td><td align="left" valign="bottom">−95</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">65.0% Left Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 5.85</td><td align="left" valign="bottom">155</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">22.5</td><td align="left" valign="bottom">42.5</td><td align="left" valign="bottom">29.0% Right Superior Frontal Gyrus; 23.0% Right Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5.86</td><td align="left" valign="bottom">87</td><td align="left" valign="bottom">17.5</td><td align="left" valign="bottom">−95</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">50.0% Right Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 6.21</td><td align="left" valign="bottom">153</td><td align="left" valign="bottom">−52.5</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">37.5</td><td align="left" valign="bottom">54.0% Left Middle Frontal Gyrus; 5.0% Left Inferior Frontal Gyrus pars opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 6.40</td><td align="left" valign="bottom">115</td><td align="left" valign="bottom">−57.5</td><td align="left" valign="bottom">−5</td><td align="left" valign="bottom">−12.5</td><td align="left" valign="bottom">37.0% Left Middle Temporal Gyrus Anterior Division; 23.0% Left Superior Temporal Gyrus Anterior Division; 9.0% Left Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6.65</td><td align="left" valign="bottom">115</td><td align="left" valign="bottom">−27.5</td><td align="left" valign="bottom">−40</td><td align="left" valign="bottom">−10</td><td align="left" valign="bottom">29.0% Left Parahippocampal Gyrus Posterior Division; 27.0% Left Lingual Gyrus; 8.0% Left Temporal Occipital Fusiform Cortex; 7.0% Left Temporal Fusiform Cortex Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6.88</td><td align="left" valign="bottom">487</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">55</td><td align="left" valign="bottom">−5</td><td align="left" valign="bottom">17.0% Right Frontal Pole; 12.0% Left Frontal Pole; 9.0% Right Paracingulate Gyrus; 9.0% Right Frontal Medial Cortex; 6.0% Left Paracingulate Gyrus; 6.0% Left Frontal Medial Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 7.08</td><td align="left" valign="bottom">421</td><td align="left" valign="bottom">−20</td><td align="left" valign="bottom">17.5</td><td align="left" valign="bottom">47.5</td><td align="left" valign="bottom">37.0% Left Superior Frontal Gyrus; 6.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 7.09</td><td align="left" valign="bottom">163</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">−75</td><td align="left" valign="bottom">37.5</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9.43</td><td align="left" valign="bottom">470</td><td align="left" valign="bottom">−32.5</td><td align="left" valign="bottom">−82.5</td><td align="left" valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9.55</td><td align="left" valign="bottom">1394</td><td align="left" valign="bottom">−7.5</td><td align="left" valign="bottom">−57.5</td><td align="left" valign="bottom">12.5</td><td align="left" valign="bottom">51.0% Left Precuneus Cortex; 7.0% Left Intracalcarine Cortex; 5.0% Left Supracalcarine Cortex</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Thresholded at bootstrap ratio at ± 3 (<italic>p</italic> &lt; .001), minimal cluster size of 80 voxels, and a minimal distance of 10 voxels. Labels from the Harvard-Oxford (<xref ref-type="bibr" rid="bib36">Desikan et al., 2006</xref>) atlas obtained using AtlasReader (<xref ref-type="bibr" rid="bib95">Notter et al., 2019</xref>).</p></fn></table-wrap-foot></table-wrap></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Recent progress in cognitive neuroscience has reignited interest in the classic semantic-episodic distinction (<xref ref-type="bibr" rid="bib109">Renoult et al., 2019</xref>) and in memory types that may fall between or cut across it (<xref ref-type="bibr" rid="bib51">Grilli and Verfaellie, 2014</xref>; <xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>). In this study, we directly compared four types of memory: one prototypical of semantic memory, one prototypical of episodic memory, and two intermediate (i.e., two types of personal semantics). We closely matched the four memory conditions: We varied the stimuli only in self-relevance (i.e., not self-relevant for general facts and self-relevant for other conditions) and temporal specificity (atemporal/general for both types of facts, more specific for repeated events, and very specific for unique events), in keeping with the distinctions across these four memory types. The general facts condition was thus operationalized as concerning knowledge of people in general (versus the self for the other conditions) and what they generally do (versus what they do at specific times). In that respect, the general facts condition takes the typical operationalization of general semantic memory as reflecting general knowledge of the world. In future investigations, one could consider comparing knowledge of the self to knowledge of specific individuals, which would allow one to evaluate a more specific aspect of semantic memory, but may be associated with other challenges (e.g., either systematically comparing the self to a specific individual, limiting generalizability, or thinking about a different individual in different trials, which would add a task-switching element). All conditions peaked at around the same time in key fMRI analyses.</p><p>Personal semantic, general semantic, and episodic memory had both shared and unique neural correlates. The shared neural correlates were revealed when contrasting the four memory conditions with a control condition (see Appendix 4). Activity in several regions of the ‘core memory network’ (<xref ref-type="bibr" rid="bib6">Addis et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Burianova and Grady, 2007</xref>; <xref ref-type="bibr" rid="bib121">Rugg and Vilberg, 2013</xref>; <xref ref-type="bibr" rid="bib135">Svoboda et al., 2006</xref>) dissociated the four memory conditions from the control condition, including in the inferior/middle frontal gyrus, caudate, lingual gyrus, parahippocampal gyrus, and hippocampus bilaterally, and the left middle/superior temporal gyrus.</p><p>In contrast, the unique neural correlates were evident when examining the four memory conditions on their own. The non-rotated PLS converges with the data-driven PLS (see Appendix 4) to suggest the facts vs. events contrast dominates to explain the spatiotemporal relations across memory conditions, although the difference in covariance explained between the two LVs of the non-rotated PLS was slight. In fact, both a priori contrasts captured aspects of the spatiotemporal relations adequately. The percentage of signal change (see <xref ref-type="fig" rid="fig5">Figure 5</xref>) illustrates the complementarity of the two perspectives to encapsulate the relation between memory conditions. That is, activity increased (or decreased) continuously across memory types, but the extent of the increase (or decrease) confers greater similarity between general and autobiographical facts, and between repeated and unique events. Thus, several regions showed a relatively small increase in activity from general facts to autobiographical facts, a relatively large increase from autobiographical facts to repeated events, and a relatively small increase from repeated events to unique events; these include the precuneus, posterior cingulate, angular gyrus and middle frontal gyrus bilaterally, and left parahippocampal gyrus, left hippocampus, and left middle/superior temporal gyrus (see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> and <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>). [We obtained comparable non-rotated PLS results while including only voxels within the default mode and medial temporal networks from <xref ref-type="bibr" rid="bib10">Barnett et al., 2021</xref>: 49.56% crossblock covariance (<italic>p</italic> &lt; .001) for the linear contrast, and 50.44% crossblock covariance (<italic>p</italic> &lt; .001) for the facts vs. events contrast. Within the selected networks, the same regions contributed to dissociate the memory conditions, and temporal brain scores peaked at lag 7. This supplementary analysis reinforces the importance of regions within the core memory network to determine the relation between memory conditions (see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> and <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>), even though we found additional contributors at the whole brain level.] Activity instead decreased in a commensurate manner predominantly in the right hemisphere, particularly the frontal pole, inferior frontal gyrus, and supramarginal cortex. These findings are compatible with a continuum perspective of declarative memory, because quantitative rather than qualitative variations in brain activity suffice to characterize the relation between these memory types (<xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>).</p><p>The different types of cues used in our experiment were used to trigger different ‘retrieval modes’ in our participants, ‘a necessary condition for retrieval’ (<xref ref-type="bibr" rid="bib140">Tulving, 1983</xref>) that is maintained as a tonic state during a retrieval task (<xref ref-type="bibr" rid="bib119">Rugg and Wilding, 2000</xref>). The behavioral data revealed the sentence cues induced typical phenomenological experience associated with these memory types. Self-relevance was rated lowest for general facts compared to all personal forms of memory, and lower for personal semantics (i.e., autobiographical facts and repeated events) than unique events. Additionally, the amount of visual details increased from general facts to autobiographical facts to repeated events to unique events. This is consistent with previous studies. For example, as compared to unique events, repeated events are generally remembered less vividly: they are associated with reduced temporal specificity, personal significance, emotionality, and amount of details (e.g., <xref ref-type="bibr" rid="bib4">Addis et al., 2004b</xref>; <xref ref-type="bibr" rid="bib61">Holland et al., 2011</xref>; <xref ref-type="bibr" rid="bib78">Levine et al., 2004</xref>). Lastly, the four memory conditions differed in the proportion of scenes that came to mind during retrieval. A smaller proportion of general facts and autobiographical facts were categorized as scenes compared to repeated events and unique events, which did not differ in scene responses. This is consistent with the idea that both memories of unique and repeated events share a spatial organization that gives them their ‘basic context’, as well as a first person perspective, detailed visual imagery (<xref ref-type="bibr" rid="bib57">Hassabis and Maguire, 2007</xref>; <xref ref-type="bibr" rid="bib94">Nadel, 1991</xref>; <xref ref-type="bibr" rid="bib114">Robin, 2018</xref>; <xref ref-type="bibr" rid="bib113">Robin et al., 2016</xref>; <xref ref-type="bibr" rid="bib118">Rubin, 2022</xref>; <xref ref-type="bibr" rid="bib116">Rubin and Umanath, 2015</xref>). Thus, scenes are thought to be a dominant and integral feature of events (<xref ref-type="bibr" rid="bib57">Hassabis and Maguire, 2007</xref>; <xref ref-type="bibr" rid="bib94">Nadel, 1991</xref>; <xref ref-type="bibr" rid="bib114">Robin, 2018</xref>; <xref ref-type="bibr" rid="bib113">Robin et al., 2016</xref>; <xref ref-type="bibr" rid="bib118">Rubin, 2022</xref>; <xref ref-type="bibr" rid="bib116">Rubin and Umanath, 2015</xref>) in a way that has not been argued for ‘facts’. Although context can shape semantic processing (<xref ref-type="bibr" rid="bib152">Yee and Thompson-Schill, 2016</xref>), the representation of general and experience-far autobiographical facts (<xref ref-type="bibr" rid="bib53">Grilli and Verfaellie, 2016</xref>) may rely less on entities within a context or the context itself and more or as much on the conceptual representation of individual entities, such as objects and words, than repeated and unique events. Facts, in particular personal facts, may evoke episodes and scenes along with them, as in autobiographically significant concepts (<xref ref-type="bibr" rid="bib107">Renoult et al., 2015</xref>; <xref ref-type="bibr" rid="bib150">Westmacott et al., 2004</xref>; <xref ref-type="bibr" rid="bib149">Westmacott and Moscovitch, 2003</xref>), for example, a person could automatically recall going to a gift store when trying to decide what is their favorite gift. Nevertheless, ‘scenes’ would not be an integral component to general/autobiographical facts in the same way that they would be for repeated/unique events (<xref ref-type="bibr" rid="bib118">Rubin, 2022</xref>; <xref ref-type="bibr" rid="bib117">Rubin et al., 2019</xref>; <xref ref-type="bibr" rid="bib116">Rubin and Umanath, 2015</xref>). In this study, the importance of visuospatial processes (in subjective ratings and corresponding brain regions) was not commensurate with characteristics of the stimuli, which differed little across conditions and primarily increased in temporal specificity (e.g., general facts: ‘Most people wear jeans.’; autobiographical facts: ‘Sometimes I wear jeans.’; repeated events: ‘When at work, I have worn jeans.’; unique events: ‘Yesterday, I wore jeans.’).</p><p>The importance of situational or contextual elements featured strongly in the neuroimaging data as in the behavioral data. The core network, also known as the default mode network, has been subdivided into an anterior temporal network linked to ‘entities’ (<xref ref-type="bibr" rid="bib104">Ranganath and Ritchey, 2012</xref>; <xref ref-type="bibr" rid="bib105">Reagh and Ranganath, 2018</xref>) or ‘conceptual remembering’ (<xref ref-type="bibr" rid="bib126">Sheldon et al., 2019</xref>) and a posterior medial network linked to ‘situational models’ (<xref ref-type="bibr" rid="bib104">Ranganath and Ritchey, 2012</xref>; <xref ref-type="bibr" rid="bib105">Reagh and Ranganath, 2018</xref>) or ‘perceptual remembering’ (<xref ref-type="bibr" rid="bib126">Sheldon et al., 2019</xref>). Although these networks process different kinds of information, neither is strictly dedicated to semantic or episodic memory (<xref ref-type="bibr" rid="bib105">Reagh and Ranganath, 2018</xref>; <xref ref-type="bibr" rid="bib126">Sheldon et al., 2019</xref>). For instance, knowledge can facilitate the search and construction of events (<xref ref-type="bibr" rid="bib64">Irish and Piguet, 2013</xref>) and semantic memory can integrate contextual information (e.g., <xref ref-type="bibr" rid="bib47">Greenberg et al., 2009</xref>; <xref ref-type="bibr" rid="bib124">Sheldon and Moscovitch, 2012</xref>). Accordingly, in our study the dissociation between general/autobiographical facts and repeated/unique events did not have a clear posterior medial to anterior temporal demarcation (i.e., posterior medial activity for events and anterior temporal activity for facts). Instead, regions primarily within the posterior medial network (i.e., angular gyrus, posterior cingulate gyrus, precuneus, and parahippocampal gyrus; <xref ref-type="bibr" rid="bib111">Ritchey et al., 2015</xref>) dissociated memory conditions, whereas those within the anterior temporal network (i.e., frontal orbital cortex, inferior anterior temporal gyrus, temporal pole, bilateral amygdala, and perirhinal cortex; <xref ref-type="bibr" rid="bib111">Ritchey et al., 2015</xref>) did not. Indeed, events contained greater contextual information, as suggested by the increased proportion of scenes they evoked, as compared to factual memories. Further, cues were more temporally specific for repeated/unique events than general/autobiographical facts. Consistent with this, many regions of the posterior medial network were associated with greater activity for repeated/unique events than general/autobiographical facts (similar to <xref ref-type="bibr" rid="bib42">Ford et al., 2011</xref>; <xref ref-type="bibr" rid="bib78">Levine et al., 2004</xref>; <xref ref-type="bibr" rid="bib81">Maguire and Mummery, 1999</xref>), and showed a linear increase from the most general type of memory (i.e., general facts) to the most specific memory (i.e., unique events). Therefore, activity in regions associated with visuospatial processing (e.g., precuneus) and scenes (e.g., medial temporal regions) coheres with behavioral data to support the prominence of contextual specificity in determining the relation across memory types. Activity in medial frontal regions is in harmony with ratings of visual details and scene perception, likewise increasing along a continuum of contextual specificity (see <xref ref-type="fig" rid="fig5">Figure 5d</xref>). However, this anterior portion of the medial frontal cortex may correspond best to self-processing rather than ‘situational’ processing or mental time travel (<xref ref-type="bibr" rid="bib79">Lieberman et al., 2019</xref>). If activity in the medial frontal cortex in our study reflected exclusively self-processing, one would expect a greater proximity between autobiographical facts and repeated events on the basis of subjective ratings of self-relevance. The additional concordance of medial frontal cortex with a continuum of contextual specificity could be a corollary of the strong links between aspects of self-relevance and episodic simulation (<xref ref-type="bibr" rid="bib54">Grysman et al., 2013</xref>; <xref ref-type="bibr" rid="bib67">King et al., 2022</xref>; <xref ref-type="bibr" rid="bib142">Tulving, 2002</xref>; <xref ref-type="bibr" rid="bib147">Verfaellie et al., 2019</xref>), in addition to this region’s role in modulating recollection (<xref ref-type="bibr" rid="bib87">McCormick et al., 2020</xref>), for example through engaging schema-related information (<xref ref-type="bibr" rid="bib44">Gilboa and Marlatte, 2017</xref>).</p><p>Taken together, our data correspond with a continuum perspective of declarative memory, with the different memory types varying in magnitude of activation within a common network of brain regions. What underlies this overlap in the neural substrates of semantic and episodic memory? A parsimonious explanation is that semantic and episodic memory rely on similar elementary component processes (<xref ref-type="bibr" rid="bib20">Cabeza and Moscovitch, 2013</xref>; <xref ref-type="bibr" rid="bib77">Larsen, 1992</xref>; <xref ref-type="bibr" rid="bib92">Moscovitch, 1992</xref>; <xref ref-type="bibr" rid="bib103">Rajah and McIntosh, 2005</xref>; <xref ref-type="bibr" rid="bib106">Renoult et al., 2012</xref>; <xref ref-type="bibr" rid="bib118">Rubin, 2022</xref>). All types of memories would depend on a similar network of brain regions but with different weighting of certain nodes in the network. The identification of the relative contribution of different component processes is a critical next step. <italic>Some</italic> of the characteristics that would influence differences in hippocampal activity and other regions of the core memory network include: the number of details (<xref ref-type="bibr" rid="bib137">Thakral et al., 2020</xref>), their association (<xref ref-type="bibr" rid="bib37">Duff et al., 2019</xref>; <xref ref-type="bibr" rid="bib129">Solomon and Schapiro, 2020</xref>), their integration within a scene (<xref ref-type="bibr" rid="bib94">Nadel, 1991</xref>; <xref ref-type="bibr" rid="bib114">Robin, 2018</xref>; <xref ref-type="bibr" rid="bib113">Robin et al., 2016</xref>; <xref ref-type="bibr" rid="bib116">Rubin and Umanath, 2015</xref>) or within a situational model (<xref ref-type="bibr" rid="bib105">Reagh and Ranganath, 2018</xref>; <xref ref-type="bibr" rid="bib134">Summerfield et al., 2010</xref>), their coarseness and precision (<xref ref-type="bibr" rid="bib27">Craik, 2020</xref>; <xref ref-type="bibr" rid="bib38">Ekstrom and Yonelinas, 2020</xref>), their type and modality (e.g., perceptual, spatial, temporal, social; <xref ref-type="bibr" rid="bib14">Binder and Desai, 2011</xref>; <xref ref-type="bibr" rid="bib53">Grilli and Verfaellie, 2016</xref>; <xref ref-type="bibr" rid="bib126">Sheldon et al., 2019</xref>), their stability (<xref ref-type="bibr" rid="bib8">Auger and Maguire, 2018</xref>), as well as their projection into a temporally distant time (<xref ref-type="bibr" rid="bib7">Andrews-Hanna et al., 2010</xref>), the open-endedness of the representation (<xref ref-type="bibr" rid="bib124">Sheldon and Moscovitch, 2012</xref>), the demands on pattern separation to construct unique representations or identify distinguishing features (<xref ref-type="bibr" rid="bib115">Rolls and Kesner, 2006</xref>; <xref ref-type="bibr" rid="bib123">Schapiro et al., 2017</xref>), and the likelihood of eliciting a specific event (<xref ref-type="bibr" rid="bib107">Renoult et al., 2015</xref>; <xref ref-type="bibr" rid="bib150">Westmacott et al., 2004</xref>; <xref ref-type="bibr" rid="bib149">Westmacott and Moscovitch, 2003</xref>). For instance, episodic memory would typically rely to a greater degree than semantic memory on rich sensory-perceptual imagery, complex situational models or scenes, spatial and temporal features, and self-reflection. Accordingly, instead of activating different networks of brain regions, semantic and episodic processes may activate a similar network but with different degrees of magnitude, or recruit these brain regions in a complementary manner (<xref ref-type="bibr" rid="bib128">Sherman et al., 2023</xref>). How each component is involved would also depend on the task at hand (e.g., <xref ref-type="bibr" rid="bib55">Gurguryan and Sheldon, 2019</xref>); each component could be more or less engaged regardless of the memory type (e.g., semantic details can be thought of in rich detail, in relation to the self, or in relation to a spatial context). Therefore, there would be a ‘neural-psychological representation correspondence’ (<xref ref-type="bibr" rid="bib93">Moscovitch and Gilboa, 2022</xref>) that includes elements of consciousness (e.g., feeling of being transported in time; <xref ref-type="bibr" rid="bib142">Tulving, 2002</xref>) and that transcends categories of memory. Our neuroimaging data, and complementary behavioral data obtained outside the scanner, are compatible with this component process view and inconsistent with strictly separated memory systems. The operationalization of different types of personal semantics and their inclusion in a model of declarative memory does not promote fragmentation. Rather, a taxonomy of personal semantics offers an opportunity to explore what brings all forms of memory together and what can sometimes pull them apart.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>fMRI study participants</title><p>Fifty-three recruited participants (35 women, 18 men) were aged 24.89 years on average (<italic>SD</italic> = 4.51; range: 19–34) and attained a mean of 16.43 years of education (<italic>SD</italic> = 2.42). This sample size was the largest that was possible to achieve. The sample size is larger than similar studies (e.g., <italic>N</italic> ~12–28; <xref ref-type="bibr" rid="bib5">Addis et al., 2011</xref>; <xref ref-type="bibr" rid="bib18">Burianova and Grady, 2007</xref>; <xref ref-type="bibr" rid="bib42">Ford et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Holland et al., 2011</xref>). No formal power analysis was conducted. Participants responded to ads on university campuses and on social media. Candidates were retained if they were right-handed, native English speakers, free of any contraindication for MRI (e.g., ferromagnetic metal, back pain, claustrophobia), aged between 18 and 35 years, and had not experienced head injury, or neurological and psychiatric disorders. We excluded three participants because of an inadvertent phase encoding switch, one for an incidental finding, and one for ineligibility. Participants received a compensation of 30 CAD. The study received REB approval at the University of Ottawa (H08-16-32) and the Royal Ottawa Mental Health Centre (ROH; 2016023).</p></sec><sec id="s4-2"><title>Behavioral study participants</title><p>The total sample included 181 participants (143 women, 34 men, 2 non-binary; 2 missing values); they were 18.79 years old on average (<italic>SD</italic> = 1.58; range: 17–34; 2 missing values) and had attained a mean of 12.67 years of education (<italic>SD</italic> = 1.02; 3 missing values). The possible small differences between proximal memory types (e.g., <xref ref-type="bibr" rid="bib4">Addis et al., 2004b</xref>; <xref ref-type="bibr" rid="bib81">Maguire and Mummery, 1999</xref>; <xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>) justified the large sample size. We recruited participants from a pool of students who received a credit toward their introductory or research methods course. The ad specified the eligibility criteria, which were the same as the fMRI study apart from those for MRI safety. We excluded participants who disclosed information that conflicted with the criteria (<italic>n</italic> for primary reason listed): 18–35 years of age (<italic>n</italic> = 1), right-handed (<italic>n</italic> = 2), normal or corrected-to-normal vision (<italic>n</italic> = 1), no neurological or psychiatric disorder (past/present) or no loss of consciousness lasting more than 10 min (<italic>n</italic> = 26), and native English speakers (<italic>n</italic> = 35). We also excluded 1 participant for being inattentive during testing (i.e., looking at a cellphone during the task), and 9 for implausible data (i.e., always responding ‘yes’) or incomplete data (e.g., entire task or questionnaire). After exclusions we had a total of 106 participants (87 women, 19 men) with a mean age of 18.74 (<italic>SD</italic> = 1.05, range 18–23) and 12.67 years of education on average (<italic>SD</italic> = 0.97). The study received REB approval at the University of Ottawa (H08-16-32).</p></sec><sec id="s4-3"><title>fMRI task and procedure</title><p>All tasks described below were administered via E-Prime 2.0 (<xref ref-type="bibr" rid="bib101">Psychology Software Tools, 2012</xref>). We optimized the sentence verification task used in an electrophysiological study (<xref ref-type="bibr" rid="bib43">Foster et al., 2012</xref>; <xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>) for fMRI. The four experimental conditions (general facts, autobiographical facts, repeated events, unique events) consisted of the same 70 main clauses (material available in Appendix 5). The conditions differed in two aspects: (1) The tense changed from past tense for unique events, to present perfect for repeated events, to present for facts (general and autobiographical). The type of awareness associated with semantic memory is indeed thought to be centered in the present, whereas episodic recollection is oriented toward the past (<xref ref-type="bibr" rid="bib141">Tulving, 2001</xref>; <xref ref-type="bibr" rid="bib142">Tulving, 2002</xref>). (2) We added distinct cue words that preceded each condition and gave different degrees of temporal specificity. We used 6–7 cues per condition. In the unique events condition, we used specific time cues (last night, last week-end, this morning, this week, today, yesterday) to promote access to specific instances of events (e.g., ‘Yesterday, I took a picture’). In the repeated events condition, we used script-like cues (when alone, when at a clinic, when at work, when on the bus, when on vacation, when shopping, when with friends), and constrained their temporal scope by asking participants to verify sentences by thinking about events that happened repeatedly within the last year (e.g., ‘When shopping, I have taken a picture’). In the autobiographical facts condition, we used general time cues (everyday, often, rarely, sometimes, usually, very often) for participants to report what is usual for them (‘Very often, I take pictures’). For the general facts condition, the first person personal pronoun (I) and the 6 cues were replaced by six distinct third person perspectives (everyone, few people, many people, most people, no one, some people), and participants had to report what they thought was generally true for people in their country (‘Few people take pictures’). The task involved pressing one of two buttons to produce a ‘yes’ or ‘no’ response to indicate whether the statements were true.</p><p>The cue (e.g., last weekend) was presented for ~2 s, followed with the main clause (e.g., I went to the pharmacy) for ~4 s. Participants responded after each statement during a response screen that lasted ~3.5 s and which displayed the response options. The separate response screen minimized the contribution of response-related motor activity during the experimental tasks (see Appendix 5 for the analysis of response time). Participants completed an odd/even task during a jittered interval that lasted from 0 to 12 s (mean of 4.6 s; adapted from <xref ref-type="bibr" rid="bib80">Madore et al., 2016</xref>). Participants indicated whether a digit was even using the same response options and buttons as the main task (i.e., yes, no). Each digit was shown in letters for 2 s; thus, participants performed the task for up to six digits (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). An odd/even task is frequently used in autobiographical memory and future thinking research (<xref ref-type="bibr" rid="bib98">Parlar et al., 2018</xref>; <xref ref-type="bibr" rid="bib136">Svoboda and Levine, 2009</xref>; <xref ref-type="bibr" rid="bib137">Thakral et al., 2020</xref>) to reveal the core memory network (<xref ref-type="bibr" rid="bib132">Stark and Squire, 2001</xref>). We obtained duration for the intervals from Optseq2, with some adaptations (<xref ref-type="bibr" rid="bib30">Dale, 1999</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Task structure and durations.</title><p>Each of the seven runs included the four memory conditions (general facts, autobiographical facts, repeated events, unique events). Each memory block started with instructions, followed with the 10 trials for that memory condition. Each trial unfolded in this order: fixation cross (0.5 s), cue (2 s), sentence (4 s), response screen (3.5 s). We presented the odd/even task (range 0–12 s, <italic>M</italic> = 4.6 s, 1 number per 2 s) during the interstimulus interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-fig6-v1.tif"/></fig><p>We blocked the trials of a given memory condition together within each run. We selected a mixed design to minimize switch costs and to repeat the instructions briefly prior to the series of trials. We asked participants to think about what is usually true for people in general in this country (for general facts) or usually true for oneself (for autobiographical facts), and whether events happened repeatedly within the last year (for repeated events) or at a specific time within the last week (for unique events). Each of the seven runs included the four memory conditions, each comprising of 10 trials. A run lasted 10.6 min. We attributed the stimuli to a run to maximize the likelihood of obtaining a ‘yes’ response for each memory condition within each run. The likelihood was determined based on the frequency of yes responses per item in <xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>, and pilot data. The aim of the piloting was to obtain a comparable number of yes and no responses (in other words approximately 35 in each condition; see Appendix 5 for the description and analysis of the proportion of yes responses). The attribution of stimuli to a run was fixed. All participants received the same randomized order of the memory conditions, but a different randomized order of stimuli within each memory block. The stimuli were displayed in Arial in size 30, in white color over a black background. The task was presented visually on a mirror mounted over the head coil, and responses were made by pressing one of two buttons (for yes or no) on an MR safe response box.</p><p>All data were acquired within a single session, except for one participant who completed the study in two sessions because of a scanner issue. Participants practiced with a short version of the task outside and inside the scanner. In a post-scan interview, participants described briefly what they were thinking about during a few trials. Scanner time was 90 min. For the other steps, starting with consent and ending with debriefing, the study required an additional 60 min approximately.</p></sec><sec id="s4-4"><title>Behavioral study task</title><p>We tested participants in groups of 1–4. They completed individually the sentence verification task (described above), but we replaced the control task with subjective ratings, and participants completed only two runs (i.e., 20 stimuli per memory condition). The stimuli were randomized such that each participants received 20 randomly selected stimuli per condition and all stimuli should be represented equally across all participants. After their yes/no response, participants performed a scene rating, details rating, and a self-relevance rating (no time limit). The order of the ratings was counterbalanced across participants. Participants indicated whether what they pictured in their mind was: (1) nothing/vague, (2) objects only, or (3) a whole scene (based on <xref ref-type="bibr" rid="bib97">Palombo et al., 2018</xref>). Participants rated the amount of visual details that came to mind from 1 (none) to 5 (a lot; based on <xref ref-type="bibr" rid="bib32">D’Argembeau and Van der Linden, 2006</xref>). We provided examples of visual details and emphasized the distinction between scenes and details. (‘Note that you could remember a lot of bits and pieces about each individual object or people without putting them into an integrated whole’.) We specified that participants should consider both the amount of details of individual objects or people and the overall number of objects and people. Participants also rated how closely their thoughts were related to themselves from 1 (very remote) to 5 (very close). We explained that ‘very close’ meant that the thoughts were directly self-related, ‘very remote’ signified those thoughts concerned strangers or people in general. Intermediate ratings applied to acquaintances. After the practice, the researcher monitored the participants’ attentiveness from a control room. After task completion, participants filled the demographic questionnaire and the Center for Epidemiologic Studies Depression Scale (<xref ref-type="bibr" rid="bib102">Radloff, 1977</xref>). The study lasted 60 min.</p></sec><sec id="s4-5"><title>fMRI image acquisition</title><p>We acquired the images on a Siemens Biograph mMR (Siemens Healthineers, Erlangen, Germany; <xref ref-type="bibr" rid="bib35">Delso et al., 2011</xref>) with a 32-channel head coil (Ceresensa, London, Canada) at the Brain Imaging Centre (BIC) of the ROH. We collected the functional data with a multiband accelerated EPI sequence from CMRR that is sensitive to BOLD (<xref ref-type="bibr" rid="bib91">Moeller et al., 2010</xref>; <xref ref-type="bibr" rid="bib151">Xu et al., 2013</xref>; aligned to anterior and posterior commissure, TR = 1200 ms, TE = 33, flip angle = 65 degrees, FOV = 200 mm, voxel size = 2.5 mm<sup>3</sup>, slice thickness = 2.5 mm, 530 volumes per run, multiband factor = 6, covering the whole brain). The phase encoding alternated between anterior to posterior (for the first, third, fifth, seventh run) and posterior to anterior (for the second, fourth, sixth run).</p><p>Due to a technical error, the TE was 40 ms instead of the intended 33 ms for the second, fourth, and sixth run of the first 28 participants. For this reason, we opted for the field map-free method for distortion correction that is implemented in FMRIPREP (<xref ref-type="bibr" rid="bib39">Esteban et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Esteban et al., 2019</xref>) instead of correcting with the reverse phase encoding measurements. Moreover, the encoding phase direction was kept constant for subsequent participants (anterior to posterior) as alternating between phase encoding was originally intended for distortion correction (with reverse phase encoding), which would not be possible for the whole sample. Phase encoding or TE differences cannot explain differences between conditions. The task was divided into seven runs which each included the four memory conditions, and the run order was randomized across all participants.</p><p>The parameters for the anatomical scan, a T1-weighted pulse sequence (MPRAGE; <xref ref-type="bibr" rid="bib34">Deichmann et al., 2000</xref>), of the first 28 participants had the following parameters: sagittal orientation, TR = 2530 ms, TE = 3.36 ms, TI = 1100 ms, flip angle = 7 degrees, slice thickness = 1 mm, FOV = 256 mm, voxel size = 1 mm<sup>3</sup>, with an acceleration factor of 2 using GRAPPA. For the latter group of 20 participants, we acquired a T1-weighted pulse sequence (MEMPRAGE; <xref ref-type="bibr" rid="bib144">van der Kouwe et al., 2008</xref>) to enhance the quality of the T1w: sagittal orientation, TR = 2500 ms, TE 1 = 1.69 ms, TE 2 = 3.55 ms, TE 3 = 5.41, TE 4 = 7.27, flip angle = 10 degrees, slice thickness = 1 mm, FOV = 256 mm, voxel size = 1 mm<sup>3</sup>, with an acceleration factor of 2 using GRAPPA.</p></sec><sec id="s4-6"><title>fMRI analyses</title><sec id="s4-6-1"><title>FMRIPREP pipeline</title><p>The MRI data were preprocessed using FMRIPREP (<xref ref-type="bibr" rid="bib39">Esteban et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Esteban et al., 2019</xref>) . The processing steps are described below verbatim as intended by FMRIPREP authors (<ext-link ext-link-type="uri" xlink:href="https://fmriprep.readthedocs.io/">https://fmriprep.readthedocs.io/</ext-link>; made available through CCO license).</p><p>&quot;Results included in this manuscript come from preprocessing performed using fMRIPprep 1.2.3 (<xref ref-type="bibr" rid="bib39">Esteban et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Esteban et al., 2019</xref>), which is based on Nipype 1.1.6-dev (<xref ref-type="bibr" rid="bib45">Gorgolewski et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Gorgolewski et al., 2018</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002502">SCR_002502</ext-link>). Anatomical Data Preprocessing. The T1-weighted (T1w) image was corrected for intensity non-uniformity using N4BiasFieldCorrection (<xref ref-type="bibr" rid="bib143">Tustison et al., 2010</xref>, ANTs 2.2.0), and used as T1w reference throughout the workflow. The T1w reference was then skull-stripped using antsBrainExtraction.sh (ANTs 2.2.0), using OASIS as target template. Brain surfaces were reconstructed using recon-all (FreeSurfer 6.0.1, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001847">SCR_001847</ext-link>, <xref ref-type="bibr" rid="bib31">Dale et al., 1999</xref>), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray matter (GM) of Mindboggle (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002438">SCR_002438</ext-link>, <xref ref-type="bibr" rid="bib68">Klein et al., 2017</xref>). Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="bib41">Fonov et al., 2009</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_008796">SCR_008796</ext-link>) was performed through nonlinear registration with antsRegistration (ANTs 2.2.0, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_004757">SCR_004757</ext-link>, <xref ref-type="bibr" rid="bib9">Avants et al., 2008</xref>), using brain-extracted versions of both T1w volume and template. Brain tissue segmentation of cerebrospinal fluid (CSF), white matter (WM), and GM was performed on the brain-extracted T1w using fast (FSL 5.0.9, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002823">SCR_002823</ext-link>, <xref ref-type="bibr" rid="bib153">Zhang et al., 2001</xref>). For each of the seven BOLD runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. A deformation field to correct for susceptibility distortions was estimated based on fMRIPrep’s fieldmap-less approach. The deformation field is that resulting from co-registering the BOLD reference to the same-subject T1w reference with its intensity inverted (<xref ref-type="bibr" rid="bib63">Huntenburg, 2014</xref>; <xref ref-type="bibr" rid="bib148">Wang et al., 2017</xref>). Registration is performed with antsRegistration (ANTs 2.2.0), and the process regularized by constraining deformation to be nonzero only along the phase-encoding direction, and modulated with an average fieldmap template (<xref ref-type="bibr" rid="bib138">Treiber et al., 2016</xref>). Based on the estimated susceptibility distortion, an unwarped BOLD reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (<xref ref-type="bibr" rid="bib50">Greve and Fischl, 2009</xref>). Co-registration was configured with nine degrees of freedom to account for distortions remaining in the BOLD reference. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9, <xref ref-type="bibr" rid="bib65">Jenkinson et al., 2002</xref>). The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled to MNI152NLin2009cAsym standard space, generating a preprocessed BOLD run in MNI152NLin2009cAsym space. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Several confounding time-series were calculated based on the <italic>preprocessed BOLD</italic>: framewise displacement (FD), DVARS, and three region-wise global signals. FD and DVARS are calculated for each functional run, both using their implementations in <italic>Nipype</italic> (following the definitions by <xref ref-type="bibr" rid="bib100">Power et al., 2014</xref>). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (<italic>CompCor</italic>, <xref ref-type="bibr" rid="bib12">Behzadi et al., 2007</xref>). Principal components are estimated after high-pass filtering the <italic>preprocessed BOLD</italic> time-series (using a discrete cosine filter with 128 s cut-off) for the two <italic>CompCor</italic> variants: temporal (tCompCor) and anatomical (aCompCor). Six tCompCor components are then calculated from the top 5% variable voxels within a mask covering the subcortical regions. This subcortical mask is obtained by heavily eroding the brain mask, which ensures it does not include cortical GM regions. For aCompCor, six components are calculated within the intersection of the aforementioned mask and the union of CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run (using the inverse BOLD-to-T1w transformation). The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The BOLD time-series were resampled to surfaces on the following spaces: <italic>fsnative</italic>, <italic>fsaverage</italic>. All resamplings can be performed with <italic>a single interpolation step</italic> by composing all the pertinent transformations (i.e., head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and template spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (<xref ref-type="bibr" rid="bib76">Lanczos, 1964</xref>). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer). Many internal operations of fMRIPrep use Nilearn 0.4.2 (<xref ref-type="bibr" rid="bib2">Abraham et al., 2014</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001362">SCR_001362</ext-link>), mostly within the functional processing workflow. For more details of the pipeline, see <ext-link ext-link-type="uri" xlink:href="https://fmriprep.readthedocs.io/en/latest/workflows.html">the section corresponding to workflows in fMRIPrep’s documentation</ext-link>.&quot;</p></sec><sec id="s4-6-2"><title>FSL</title><p>The fMRIPrep output (in MNI152NLin2009cAsym space) was spatially smoothed using a Gaussian kernel of FWHM 5 mm and grand-mean intensity normalized using FEAT (FMRI Expert Analysis Tool) Version 6.00 from FSL (FMRIB’s Software Library, <ext-link ext-link-type="uri" xlink:href="https://www.fmrib.ox.ac.uk">https://www.fmrib.ox.ac.uk</ext-link>; <xref ref-type="bibr" rid="bib66">Jenkinson et al., 2012</xref>). We did not apply slice timing correction with fMRIPrep or FSL because of the rapid multi-band acquisition.</p></sec><sec id="s4-6-3"><title>PLS</title><p>We used a multivariate approach, PLS correlation (<xref ref-type="bibr" rid="bib71">Krishnan et al., 2011</xref>; <xref ref-type="bibr" rid="bib88">McIntosh et al., 2004a</xref>; <xref ref-type="bibr" rid="bib89">McIntosh and Lobaugh, 2004b</xref>), implemented in the PLSgui/PLScmd toolbox for Matlab (<ext-link ext-link-type="uri" xlink:href="https://www.rotman-baycrest.on.ca/index.php?section=84;">https://www.rotman-baycrest.on.ca/index.php?section=84;</ext-link> <xref ref-type="bibr" rid="bib90">McIntosh et al., 2011</xref>). PLS is a technique developed for chemometrics (<xref ref-type="bibr" rid="bib71">Krishnan et al., 2011</xref>) and widely adopted for neuroimaging use. Strengths of this analysis are that it is well suited for designs with multiple sets of measures and collinearity, and it mitigates power issues encountered in univariate analyses (<xref ref-type="bibr" rid="bib96">O’Toole et al., 2007</xref>). It performs a singular value decomposition of the relation between two data matrices, X (brain) and Y (task design) to identify how tasks and voxels covary maximally together. PLS produces LVs that represent the similarities and differences in covariance patterns between the two matrices (akin to PCA eigenvectors). Saliences indicate the strength of the contribution of tasks and voxels to an LV (as indexed through bootstrap values, described below).</p><p>We specified that each trial began at cue onset and comprised 12 TRs (or 14.4 s) to encompass the typical time-window of a BOLD response (<xref ref-type="bibr" rid="bib88">McIntosh et al., 2004a</xref>). We retained only yes responses like <xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>, because they presumably reflect access to information consistent with the memory condition. Memory accuracy is often difficult to assess for autobiographical memory (<xref ref-type="bibr" rid="bib19">Cabeza and St Jacques, 2007</xref>), and so was not considered for any of the conditions. We retained trials that had 3–6 numbers for the control task, thus lasting 6–12 s. We assessed the significance of LVs (<italic>p</italic> &lt; .05) via 1000 permutations (randomizing the labels of conditions without replacement). We tested the stability of each voxel’s contribution to the LV via 500 iterations of bootstrap estimation (resampling participants with replacement; <xref ref-type="bibr" rid="bib71">Krishnan et al., 2011</xref>; <xref ref-type="bibr" rid="bib88">McIntosh et al., 2004a</xref>; <xref ref-type="bibr" rid="bib89">McIntosh and Lobaugh, 2004b</xref>). The threshold of ± 3 bootstrap ratio (equivalent to <italic>p </italic>&lt; .001; as in <xref ref-type="bibr" rid="bib60">Hirshhorn et al., 2012</xref>; <xref ref-type="bibr" rid="bib154">Ziaei et al., 2016</xref>) was used to determine whether a voxel made a reliable contribution to the LV. A reliable voxel contributes to the overall task and brain pattern. The PLS tests the association between the task and all voxels at all TRs in a single analysis, and so does not require correction for multiple comparison (<xref ref-type="bibr" rid="bib89">McIntosh and Lobaugh, 2004b</xref>). An additional threshold of a minimum of 80 voxels with a minimum distance of 10 voxels was used to facilitate the summary of findings in the results section and the tables (for a similar cluster size, see <xref ref-type="bibr" rid="bib17">Bowen et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Hirshhorn et al., 2012</xref>; <xref ref-type="bibr" rid="bib112">Robin et al., 2015</xref>; <xref ref-type="bibr" rid="bib154">Ziaei et al., 2016</xref>). The bootstrap percentile estimates are unreliable. Thus, we reported the standard error of the bootstrap estimation instead of the confidence intervals along with <italic>t</italic>-tests (see Appendices 2 and 3); the interpretation is similar.</p></sec><sec id="s4-6-4"><title>Statistical analyses of brain scores and behavioral data</title><p>We compared the four memory conditions (general facts, autobiographical facts, repeated events, unique events) on brain scores, the proportion of ‘yes’ responses and response time during the fMRI task, and on mean self-relevance, mean visual details and proportion of trials associated with one of the three scene categories during the behavioral task. We followed up on significant main effects with paired samples <italic>t</italic>-tests, reporting Hedges’ <italic>g</italic> as the measure of effect size, and correcting each set of post hoc tests (i.e., six tests) with the Holm-Bonferroni procedure (<xref ref-type="bibr" rid="bib62">Holm, 1979</xref>). We attributed the value equivalent to <italic>z</italic> ± 2.58 to bring univariate outliers closer to the mean. Scene ratings were excessively skewed (see <xref ref-type="fig" rid="fig2">Figure 2</xref>), but parametric tests gave similar results than nonparametric and are reported for simplicity. We applied a Greenhouse-Geisser correction (<xref ref-type="bibr" rid="bib49">Greenhouse and Geisser, 1959</xref>) when the sphericity assumption was violated. All these analyses focused on trials associated with ‘yes’ responses (as in fMRI analyses) and were executed in SPSS v. 28 (<xref ref-type="bibr" rid="bib26">Corp, 2021</xref>).</p></sec><sec id="s4-6-5"><title>Open science statement</title><p>The instructions and stimuli are available in Appendix 5. End-stage data and some scripts are available on <ext-link ext-link-type="uri" xlink:href="https://osf.io/">https://osf.io/</ext-link> (<ext-link ext-link-type="uri" xlink:href="https://osf.io/py5k6/">https://osf.io/py5k6/</ext-link>). Additional information may be requested to the corresponding authors. fMRI standards guided the reporting in this paper (<xref ref-type="bibr" rid="bib99">Poldrack et al., 2008</xref>).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Validation, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Funding acquisition, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study received REB approval at the University of Ottawa (H08-16-32), and the Royal Ottawa Mental Health Centre (ROH; 2016023).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83645-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>End-stage data (i.e., PLS BSR nifti files) and some scripts (e.g., for event files) are available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/py5k6/">https://osf.io/py5k6/</ext-link>). The authors did not receive approval from the ethics committee to deposit data in a public repository. Additional information may be requested to the corresponding authors. Deidentified data may be shared (i.e., converted dicom files to nifti without identifiers, defaced T1w, other data with minimal demographic/health information to avoid identification). Researchers will be asked to sign a confidentiality agreement and to describe briefly the purpose to ensure it fits with the general purpose of the study described in the consent form. No formal application will be required. Data may not be analyzed for commercial purposes.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Tanguay</surname><given-names>A</given-names></name><name><surname>Palombo</surname><given-names>D</given-names></name><name><surname>Love</surname><given-names>B</given-names></name><name><surname>Glikstein</surname><given-names>R</given-names></name><name><surname>Davidson</surname><given-names>P</given-names></name><name><surname>Renoult</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Personal Semantic, General Semantic, and Episodic Memory: Shared and Unique Neural Correlates</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/py5k6/">py5k6</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We gratefully acknowledge the participants who generously took part in the study; Sara Trincao-Batra and Talia Chin for help with recruitment, testing or preparing the data for analysis; Melina Langevin and Andrew Wilson for collecting the data for the behavioral analysis; Kirandeep K Dogra for the validation of sample descriptives and analyses in SI; Emily Wang with help preparing tables, Emma Haight for feedback on the manuscript; Drs. Donna Rose Addis, Samuel Fynes-Clinton, Andra Smith for advice, and particularly Dr. Randy McIntosh for advice on many aspects of the PLS analyses. Preparation of this manuscript was supported by grants from Natural Sciences and Engineering Research Council of Canada (NSERC; to PSRD), from the British Academy (grant number SG142524 to LR), funds from the Faculty of Social Sciences of the University of Ottawa (to PSRD), an Ontario Graduate Scholarship (to AFNT). DJP is supported by an NSERC Discovery Grant (RGPIN-2019-04596).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Williams</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Partial least squares methods: Partial least squares correlation and partial least square regression</chapter-title><person-group person-group-type="editor"><name><surname>Reisfeld</surname><given-names>B</given-names></name><name><surname>Mayeno</surname><given-names>AN</given-names></name></person-group><source>Computational Toxicology</source><publisher-name>Humana Press</publisher-name><fpage>549</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1007/978-1-62703-059-5_23</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Crawley</surname><given-names>AP</given-names></name><name><surname>McAndrews</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2004">2004a</year><article-title>Characterizing spatial and temporal features of autobiographical memory retrieval networks: A partial least squares approach</article-title><source>NeuroImage</source><volume>23</volume><fpage>1460</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.08.007</pub-id><pub-id pub-id-type="pmid">15589110</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Crawley</surname><given-names>AP</given-names></name><name><surname>McAndrews</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2004">2004b</year><article-title>Recollective qualities modulate hippocampal activation during autobiographical memory retrieval</article-title><source>Hippocampus</source><volume>14</volume><fpage>752</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1002/hipo.10215</pub-id><pub-id pub-id-type="pmid">15318333</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Cheng</surname><given-names>T</given-names></name><name><surname>Roberts</surname><given-names>R</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal contributions to the episodic simulation of specific and general future events</article-title><source>Hippocampus</source><volume>21</volume><fpage>1045</fpage><lpage>1052</lpage><pub-id pub-id-type="doi">10.1002/hipo.20870</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Moloney</surname><given-names>EEJ</given-names></name><name><surname>Tippett</surname><given-names>LJ</given-names></name><name><surname>P Roberts</surname><given-names>R</given-names></name><name><surname>Hach</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Characterizing cerebellar activity during autobiographical memory retrieval: ALE and functional connectivity investigations</article-title><source>Neuropsychologia</source><volume>90</volume><fpage>80</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.05.025</pub-id><pub-id pub-id-type="pmid">27235570</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews-Hanna</surname><given-names>JR</given-names></name><name><surname>Reidler</surname><given-names>JS</given-names></name><name><surname>Sepulcre</surname><given-names>J</given-names></name><name><surname>Poulin</surname><given-names>R</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional-anatomic fractionation of the brain’s default network</article-title><source>Neuron</source><volume>65</volume><fpage>550</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.02.005</pub-id><pub-id pub-id-type="pmid">20188659</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auger</surname><given-names>SD</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Retrosplenial cortex indexes stability beyond the spatial domain</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1472</fpage><lpage>1481</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2602-17.2017</pub-id><pub-id pub-id-type="pmid">29311139</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>AJ</given-names></name><name><surname>Reilly</surname><given-names>W</given-names></name><name><surname>Dimsdale-Zucker</surname><given-names>HR</given-names></name><name><surname>Mizrak</surname><given-names>E</given-names></name><name><surname>Reagh</surname><given-names>Z</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Intrinsic connectivity reveals functionally distinct cortico-hippocampal networks in the human brain</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001275</pub-id><pub-id pub-id-type="pmid">34077415</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><year iso-8601-date="1988">1988</year><chapter-title>The content and organization of autobiographical memory</chapter-title><person-group person-group-type="editor"><name><surname>Neisser</surname><given-names>U</given-names></name><name><surname>Winograd</surname><given-names>E</given-names></name></person-group><source>Remembering Reconsidered</source><publisher-name>Cambridge University Press</publisher-name><fpage>193</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1017/CBO9780511664014</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name><name><surname>Graves</surname><given-names>WW</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Where is the semantic system? A critical review and meta-analysis of 120 functional neuroimaging studies</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2767</fpage><lpage>2796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp055</pub-id><pub-id pub-id-type="pmid">19329570</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neurobiology of semantic memory</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.10.001</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>In defense of abstract conceptual representations</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1096</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0909-1</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blumenthal</surname><given-names>A</given-names></name><name><surname>Duke</surname><given-names>D</given-names></name><name><surname>Bowles</surname><given-names>B</given-names></name><name><surname>Gilboa</surname><given-names>A</given-names></name><name><surname>Rosenbaum</surname><given-names>RS</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name><name><surname>McRae</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Abnormal semantic knowledge in a case of developmental amnesia</article-title><source>Neuropsychologia</source><volume>102</volume><fpage>237</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.06.018</pub-id><pub-id pub-id-type="pmid">28625659</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowen</surname><given-names>HJ</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name><name><surname>Spaniol</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Age differences in the neural response to negative feedback</article-title><source>Aging, Neuropsychology, and Cognition</source><volume>26</volume><fpage>463</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1080/13825585.2018.1475003</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burianova</surname><given-names>H</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Common and unique neural activations in autobiographical, episodic, and semantic retrieval</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1520</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.9.1520</pub-id><pub-id pub-id-type="pmid">17714013</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabeza</surname><given-names>R</given-names></name><name><surname>St Jacques</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional neuroimaging of autobiographical memory</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>219</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.02.005</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabeza</surname><given-names>R</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Memory systems, processing modes, and components: Functional neuroimaging evidence</article-title><source>Perspectives on Psychological Science</source><volume>8</volume><fpage>49</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1177/1745691612469033</pub-id><pub-id pub-id-type="pmid">24163702</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cermak</surname><given-names>LS</given-names></name><name><surname>O’Connor</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The anterograde and retrograde retrieval ability of a patient with amnesia due to encephalitis</article-title><source>Neuropsychologia</source><volume>21</volume><fpage>213</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(83)90039-8</pub-id><pub-id pub-id-type="pmid">6877576</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapleau</surname><given-names>M</given-names></name><name><surname>Aldebert</surname><given-names>J</given-names></name><name><surname>Montembeault</surname><given-names>M</given-names></name><name><surname>Brambati</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Atrophy in Alzheimer’s Disease and Semantic Dementia: an ALE meta-analysis of Voxel-Based Morphometry Studies</article-title><source>Journal of Alzheimer’s Disease</source><volume>54</volume><fpage>941</fpage><lpage>955</lpage><pub-id pub-id-type="doi">10.3233/JAD-160382</pub-id><pub-id pub-id-type="pmid">27567843</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Verifying autobiographical facts</article-title><source>Cognition</source><volume>26</volume><fpage>39</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(87)90013-8</pub-id><pub-id pub-id-type="pmid">3608395</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname><given-names>MA</given-names></name><name><surname>Pleydell-Pearce</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The construction of autobiographical memories in the self-memory system</article-title><source>Psychological Review</source><volume>107</volume><fpage>261</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.107.2.261</pub-id><pub-id pub-id-type="pmid">10789197</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Memory and the self☆</article-title><source>Journal of Memory and Language</source><volume>53</volume><fpage>594</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2005.08.005</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Corp</surname><given-names>IBM</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>IBM SPSS statistics for Macintosh</data-title><version designator="28.0">28.0</version><publisher-name>IBM Corp</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craik</surname><given-names>FIM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Remembering: An activity of mind and brain</article-title><source>Annual Review of Psychology</source><volume>71</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010419-051027</pub-id><pub-id pub-id-type="pmid">31283427</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curot</surname><given-names>J</given-names></name><name><surname>Busigny</surname><given-names>T</given-names></name><name><surname>Valton</surname><given-names>L</given-names></name><name><surname>Denuelle</surname><given-names>M</given-names></name><name><surname>Vignal</surname><given-names>JP</given-names></name><name><surname>Maillard</surname><given-names>L</given-names></name><name><surname>Chauvel</surname><given-names>P</given-names></name><name><surname>Pariente</surname><given-names>J</given-names></name><name><surname>Trebuchon</surname><given-names>A</given-names></name><name><surname>Bartolomei</surname><given-names>F</given-names></name><name><surname>Barbeau</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Memory scrutinized through electrical brain stimulation: A review of 80 years of experiential phenomena</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>78</volume><fpage>161</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.04.018</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutler</surname><given-names>RA</given-names></name><name><surname>Duff</surname><given-names>MC</given-names></name><name><surname>Polyn</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Searching for semantic knowledge: A vector space semantic analysis of the feature generation task</article-title><source>Frontiers in Human Neuroscience</source><volume>13</volume><elocation-id>341</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2019.00341</pub-id><pub-id pub-id-type="pmid">31680903</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Optimal experimental design for event-related fMRI</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:2/3&lt;109::AID-HBM7&gt;3.0.CO;2-W</pub-id><pub-id pub-id-type="pmid">10524601</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical Surface-Based Analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Argembeau</surname><given-names>A</given-names></name><name><surname>Van der Linden</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Individual differences in the phenomenology of mental time travel: The effect of vivid visual imagery and emotion regulation strategies</article-title><source>Consciousness and Cognition</source><volume>15</volume><fpage>342</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2005.09.001</pub-id><pub-id pub-id-type="pmid">16230028</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Chastelaine</surname><given-names>M</given-names></name><name><surname>Mattson</surname><given-names>JT</given-names></name><name><surname>Wang</surname><given-names>TH</given-names></name><name><surname>Donley</surname><given-names>BE</given-names></name><name><surname>Rugg</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural correlates of recollection and retrieval monitoring: Relationships with age and recollection performance</article-title><source>NeuroImage</source><volume>138</volume><fpage>164</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.071</pub-id><pub-id pub-id-type="pmid">27155127</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Good</surname><given-names>CD</given-names></name><name><surname>Josephs</surname><given-names>O</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Optimization of 3-D MP-RAGE sequences for structural brain imaging</article-title><source>NeuroImage</source><volume>12</volume><fpage>112</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0601</pub-id><pub-id pub-id-type="pmid">10875908</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delso</surname><given-names>G</given-names></name><name><surname>Fürst</surname><given-names>S</given-names></name><name><surname>Jakoby</surname><given-names>B</given-names></name><name><surname>Ladebeck</surname><given-names>R</given-names></name><name><surname>Ganter</surname><given-names>C</given-names></name><name><surname>Nekolla</surname><given-names>SG</given-names></name><name><surname>Schwaiger</surname><given-names>M</given-names></name><name><surname>Ziegler</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Performance measurements of the Siemens mMR integrated whole-body PET/MR scanner</article-title><source>Journal of Nuclear Medicine</source><volume>52</volume><fpage>1914</fpage><lpage>1922</lpage><pub-id pub-id-type="doi">10.2967/jnumed.111.092726</pub-id><pub-id pub-id-type="pmid">22080447</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><name><surname>Killiany</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duff</surname><given-names>MC</given-names></name><name><surname>Covington</surname><given-names>NV</given-names></name><name><surname>Hilverman</surname><given-names>C</given-names></name><name><surname>Cohen</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Semantic memory and the hippocampus: Revisiting, reaffirming, and extending the reach of their critical relationship</article-title><source>Frontiers in Human Neuroscience</source><volume>13</volume><elocation-id>471</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2019.00471</pub-id><pub-id pub-id-type="pmid">32038203</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>AD</given-names></name><name><surname>Yonelinas</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Precision, binding, and the hippocampus: Precisely what are we talking about?</article-title><source>Neuropsychologia</source><volume>138</volume><elocation-id>107341</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107341</pub-id><pub-id pub-id-type="pmid">31945386</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Fmriprep</data-title><version designator="1.2.3">1.2.3</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1489485">https://doi.org/10.5281/zenodo.1489485</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: A robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ford</surname><given-names>JH</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Giovanello</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differential neural activity during search of specific and general autobiographical memories elicited by musical cues</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>2514</fpage><lpage>2526</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.04.032</pub-id><pub-id pub-id-type="pmid">21600227</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>BL</given-names></name><name><surname>Dastjerdi</surname><given-names>M</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural populations in human posteromedial cortex display opposing responses during memory and numerical processing</article-title><source>PNAS</source><volume>109</volume><fpage>15514</fpage><lpage>15519</lpage><pub-id pub-id-type="doi">10.1073/pnas.1206580109</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilboa</surname><given-names>A</given-names></name><name><surname>Marlatte</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neurobiology of schemas and schema-mediated memory</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>618</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.04.013</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: A flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Ziegler</surname><given-names>E</given-names></name><name><surname>Ellis</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Nipype</data-title><version designator="1.8.3">1.8.3</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6834519">https://doi.org/10.5281/zenodo.6834519</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>DL</given-names></name><name><surname>Keane</surname><given-names>MM</given-names></name><name><surname>Ryan</surname><given-names>L</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Impaired category fluency in medial temporal lobe amnesia: the role of episodic memory</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>10900</fpage><lpage>10908</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1202-09.2009</pub-id><pub-id pub-id-type="pmid">19726648</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>DL</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Interdependence of episodic and semantic memory: evidence from neuropsychology</article-title><source>Journal of the International Neuropsychological Society</source><volume>16</volume><fpage>748</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1017/S1355617710000676</pub-id><pub-id pub-id-type="pmid">20561378</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenhouse</surname><given-names>SW</given-names></name><name><surname>Geisser</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>On methods in the analysis of profile data</article-title><source>Psychometrika</source><volume>24</volume><fpage>95</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1007/BF02289823</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grilli</surname><given-names>MD</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Personal semantic memory: insights from neuropsychological research on amnesia</article-title><source>Neuropsychologia</source><volume>61</volume><fpage>56</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.06.012</pub-id><pub-id pub-id-type="pmid">24949553</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grilli</surname><given-names>MD</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Supporting the self-concept with memory: insight from amnesia</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>10</volume><fpage>1684</fpage><lpage>1692</lpage><pub-id pub-id-type="doi">10.1093/scan/nsv056</pub-id><pub-id pub-id-type="pmid">25964501</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grilli</surname><given-names>MD</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-near but not experience-far autobiographical facts depend on the medial temporal lobe for retrieval: Evidence from amnesia</article-title><source>Neuropsychologia</source><volume>81</volume><fpage>180</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.12.023</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grysman</surname><given-names>A</given-names></name><name><surname>Prabhakar</surname><given-names>J</given-names></name><name><surname>Anglin</surname><given-names>SM</given-names></name><name><surname>Hudson</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The time travelling self: comparing self and other in narratives of past and future events</article-title><source>Consciousness and Cognition</source><volume>22</volume><fpage>742</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2013.04.010</pub-id><pub-id pub-id-type="pmid">23703026</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurguryan</surname><given-names>L</given-names></name><name><surname>Sheldon</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Retrieval orientation alters neural activity during autobiographical memory recollection</article-title><source>NeuroImage</source><volume>199</volume><fpage>534</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.077</pub-id><pub-id pub-id-type="pmid">31152842</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haque</surname><given-names>S</given-names></name><name><surname>Conway</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Sampling the process of autobiographical memory construction</article-title><source>European Journal of Cognitive Psychology</source><volume>13</volume><fpage>529</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1080/09541440125757</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Deconstructing episodic memory with construction</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>299</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.05.001</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The construction system of the brain</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>364</volume><fpage>1263</fpage><lpage>1271</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0296</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The semantic-episodic distinction and the history of long-term memory typologies</article-title><source>Bulletin of the Psychonomic Society</source><volume>20</volume><fpage>207</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.3758/BF03334817</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirshhorn</surname><given-names>M</given-names></name><name><surname>Grady</surname><given-names>C</given-names></name><name><surname>Rosenbaum</surname><given-names>RS</given-names></name><name><surname>Winocur</surname><given-names>G</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain regions involved in the retrieval of spatial and episodic details associated with a familiar environment: an fMRI study</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>3094</fpage><lpage>3106</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.08.008</pub-id><pub-id pub-id-type="pmid">22910274</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holland</surname><given-names>AC</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Kensinger</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neural correlates of specific versus general autobiographical memory construction and elaboration</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>3164</fpage><lpage>3177</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.07.015</pub-id><pub-id pub-id-type="pmid">21803063</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holm</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>A simple sequentially rejective multiple test procedure</article-title><source>Scandinavian Journal of Statistics</source><volume>6</volume><fpage>65</fpage><lpage>70</lpage></element-citation></ref><ref id="bib63"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Huntenburg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Evaluating nonlinear coregistration of BOLD EPI and T1w images</article-title><publisher-name>Freie Universität, Berlin</publisher-name><ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11858/00-001M-0000-002B-1CB5-A">http://hdl.handle.net/11858/00-001M-0000-002B-1CB5-A</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irish</surname><given-names>M</given-names></name><name><surname>Piguet</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The pivotal role of semantic memory in remembering the past and imagining the future</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>7</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2013.00027</pub-id><pub-id pub-id-type="pmid">23565081</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(02)91132-8</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FSL</article-title><source>NeuroImage</source><volume>62</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id><pub-id pub-id-type="pmid">21979382</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>CI</given-names></name><name><surname>Romero</surname><given-names>ASL</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>St Jacques</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The influence of shifting perspective on episodic and semantic details during autobiographical memory recall</article-title><source>Memory</source><volume>30</volume><fpage>942</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1080/09658211.2022.2061003</pub-id><pub-id pub-id-type="pmid">35392765</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Bao</surname><given-names>FS</given-names></name><name><surname>Giard</surname><given-names>J</given-names></name><name><surname>Häme</surname><given-names>Y</given-names></name><name><surname>Stavsky</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>N</given-names></name><name><surname>Rossa</surname><given-names>B</given-names></name><name><surname>Reuter</surname><given-names>M</given-names></name><name><surname>Chaibub Neto</surname><given-names>E</given-names></name><name><surname>Keshavan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mindboggling morphometry of human brains</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005350</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id><pub-id pub-id-type="pmid">28231282</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klooster</surname><given-names>NB</given-names></name><name><surname>Tranel</surname><given-names>D</given-names></name><name><surname>Duff</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The hippocampus and semantic memory over time</article-title><source>Brain and Language</source><volume>201</volume><elocation-id>104711</elocation-id><pub-id pub-id-type="doi">10.1016/j.bandl.2019.104711</pub-id><pub-id pub-id-type="pmid">31739112</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopelman</surname><given-names>MD</given-names></name><name><surname>Wilson</surname><given-names>BA</given-names></name><name><surname>Baddeley</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The autobiographical memory interview: A new assessment of autobiographical and personal semantic memory in amnesic patients</article-title><source>Journal of Clinical and Experimental Neuropsychology</source><volume>11</volume><fpage>724</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1080/01688638908400928</pub-id><pub-id pub-id-type="pmid">2808661</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnan</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>LJ</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Partial Least Squares (PLS) methods for neuroimaging: A tutorial and review</article-title><source>NeuroImage</source><volume>56</volume><fpage>455</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.034</pub-id><pub-id pub-id-type="pmid">20656037</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Semantic memory: A review of methods, models, and current challenges</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>28</volume><fpage>40</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.3758/s13423-020-01792-x</pub-id><pub-id pub-id-type="pmid">32885404</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Lambert</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The benefits of the autobiographical significance of general knowledge in young and older adults</article-title><publisher-name>University of East Anglia</publisher-name><ext-link ext-link-type="uri" xlink:href="https://ueaeprints.uea.ac.uk/id/eprint/82642/">https://ueaeprints.uea.ac.uk/id/eprint/82642/</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural and computational bases of semantic cognition</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanczos</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Evaluation of noisy data</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><volume>1</volume><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1137/0701007</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="1992">1992</year><chapter-title>Personal context in autobiographical and narrative memories</chapter-title><person-group person-group-type="editor"><name><surname>Conway</surname><given-names>MA</given-names></name><name><surname>Rubin</surname><given-names>DC</given-names></name><name><surname>Spinnler</surname><given-names>H</given-names></name><name><surname>Wagenaar</surname><given-names>WA</given-names></name></person-group><source>Theoretical Perspectives on Autobiographical Memory</source><publisher-name>Springer Netherlands</publisher-name><fpage>53</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1007/978-94-015-7967-4_4</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levine</surname><given-names>B</given-names></name><name><surname>Turner</surname><given-names>GR</given-names></name><name><surname>Tisserand</surname><given-names>D</given-names></name><name><surname>Hevenor</surname><given-names>SJ</given-names></name><name><surname>Graham</surname><given-names>SJ</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The functional neuroanatomy of episodic and semantic autobiographical remembering: A prospective functional MRI study</article-title><source>Journal of Cognitive Neuroscience</source><volume>16</volume><fpage>1633</fpage><lpage>1646</lpage><pub-id pub-id-type="doi">10.1162/0898929042568587</pub-id><pub-id pub-id-type="pmid">15601525</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieberman</surname><given-names>MD</given-names></name><name><surname>Straccia</surname><given-names>MA</given-names></name><name><surname>Meyer</surname><given-names>ML</given-names></name><name><surname>Du</surname><given-names>M</given-names></name><name><surname>Tan</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Social, self, (situational), and affective processes in medial prefrontal cortex (MPFC): Causal, multivariate, and reverse inference evidence</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>99</volume><fpage>311</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.12.021</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madore</surname><given-names>KP</given-names></name><name><surname>Szpunar</surname><given-names>KK</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Episodic specificity induction impacts activity in a core brain network during construction of imagined future experiences</article-title><source>PNAS</source><volume>113</volume><fpage>10696</fpage><lpage>10701</lpage><pub-id pub-id-type="doi">10.1073/pnas.1612278113</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>EA</given-names></name><name><surname>Mummery</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Differential modulation of a common memory retrieval network revealed by positron emission tomography</article-title><source>Hippocampus</source><volume>9</volume><fpage>54</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1999)9:1&lt;54::AID-HIPO6&gt;3.0.CO;2-O</pub-id><pub-id pub-id-type="pmid">10088900</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neuroimaging studies of autobiographical event memory</article-title><source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source><volume>356</volume><elocation-id>1413</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2001.0944</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>EA</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Aging affects the engagement of the hippocampus during autobiographical memory retrieval</article-title><source>Brain</source><volume>126</volume><fpage>1511</fpage><lpage>1523</lpage><pub-id pub-id-type="doi">10.1093/brain/awg157</pub-id><pub-id pub-id-type="pmid">12805116</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>DS</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Olsen</surname><given-names>T</given-names></name><name><surname>Hodge</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Prior</surname><given-names>F</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Laumann</surname><given-names>T</given-names></name><name><surname>Curtiss</surname><given-names>SW</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Informatics and data mining tools and strategies for the human connectome project</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00004</pub-id><pub-id pub-id-type="pmid">21743807</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinelli</surname><given-names>P</given-names></name><name><surname>Sperduti</surname><given-names>M</given-names></name><name><surname>Piolino</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural substrates of the self-memory system: New insights from a meta-analysis</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>1515</fpage><lpage>1529</lpage><pub-id pub-id-type="doi">10.1002/hbm.22008</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Fsleyes</data-title><version designator="1.3.0">1.3.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5576035">https://doi.org/10.5281/zenodo.5576035</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>C</given-names></name><name><surname>Barry</surname><given-names>DN</given-names></name><name><surname>Jafarian</surname><given-names>A</given-names></name><name><surname>Barnes</surname><given-names>GR</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>vmPFC Drives hippocampal processing during autobiographical memory recall regardless of remoteness</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>5972</fpage><lpage>5987</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhaa172</pub-id><pub-id pub-id-type="pmid">32572443</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Chau</surname><given-names>WK</given-names></name><name><surname>Protzner</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2004">2004a</year><article-title>Spatiotemporal analysis of event-related fMRI data using partial least squares</article-title><source>NeuroImage</source><volume>23</volume><fpage>764</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.05.018</pub-id><pub-id pub-id-type="pmid">15488426</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Lobaugh</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2004">2004b</year><article-title>Partial least squares analysis of neuroimaging data: applications and advances</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S250</fpage><lpage>S263</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.020</pub-id><pub-id pub-id-type="pmid">15501095</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Lobaugh</surname><given-names>NJ</given-names></name><name><surname>Chau</surname><given-names>W</given-names></name><name><surname>Shen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><data-title>Plsgui/Plscmd</data-title><source>Baycrest</source><ext-link ext-link-type="uri" xlink:href="https://www.rotman-baycrest.on.ca/index.php?section=84">https://www.rotman-baycrest.on.ca/index.php?section=84</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Strupp</surname><given-names>J</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiband multislice GE‐EPI at 7 tesla, with 16‐fold acceleration using partial parallel imaging with application to high spatial and temporal whole‐brain fMRI</article-title><source>Magnetic Resonance in Medicine</source><volume>63</volume><fpage>1144</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Memory and working-with-memory: A component process model based on modules and central systems</article-title><source>Journal of Cognitive Neuroscience</source><volume>4</volume><fpage>257</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1162/jocn.1992.4.3.257</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Gilboa</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Has the concept of systems consolidation outlived its usefulness? Identification and evaluation of premises underlying systems consolidation</article-title><source>Faculty Reviews</source><volume>11</volume><elocation-id>33</elocation-id><pub-id pub-id-type="doi">10.12703/r/11-33</pub-id><pub-id pub-id-type="pmid">36532709</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The hippocampus and space revisited</article-title><source>Hippocampus</source><volume>1</volume><fpage>221</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1002/hipo.450010302</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Notter</surname><given-names>M</given-names></name><name><surname>Gale</surname><given-names>D</given-names></name><name><surname>Herholz</surname><given-names>P</given-names></name><name><surname>Markello</surname><given-names>R</given-names></name><name><surname>Notter-Bielser</surname><given-names>ML</given-names></name><name><surname>Whitaker</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>AtlasReader: A Python package to generate coordinate tables, region labels, and informative figures from statistical MRI images</article-title><source>Journal of Open Source Software</source><volume>4</volume><elocation-id>1257</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01257</pub-id><pub-id pub-id-type="pmid">32775955</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Toole</surname><given-names>AJ</given-names></name><name><surname>Jiang</surname><given-names>F</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Pénard</surname><given-names>N</given-names></name><name><surname>Dunlop</surname><given-names>JP</given-names></name><name><surname>Parent</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis of functional neuroimaging data</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1735</fpage><lpage>1752</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.11.1735</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palombo</surname><given-names>DJ</given-names></name><name><surname>Hayes</surname><given-names>SM</given-names></name><name><surname>Peterson</surname><given-names>KM</given-names></name><name><surname>Keane</surname><given-names>MM</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Medial temporal lobe contributions to episodic future thinking: Scene construction or future projection?</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>447</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw381</pub-id><pub-id pub-id-type="pmid">27913433</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parlar</surname><given-names>M</given-names></name><name><surname>Densmore</surname><given-names>M</given-names></name><name><surname>Hall</surname><given-names>GBC</given-names></name><name><surname>Lanius</surname><given-names>R</given-names></name><name><surname>McKinnon</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural and behavioural correlates of autobiographical memory retrieval in patients with major depressive disorder and a history of trauma exposure</article-title><source>Neuropsychologia</source><volume>110</volume><fpage>148</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.07.004</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Fletcher</surname><given-names>PC</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Worsley</surname><given-names>KJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Guidelines for reporting an fMRI study</article-title><source>NeuroImage</source><volume>40</volume><fpage>409</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.11.048</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Mitra</surname><given-names>A</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>NeuroImage</source><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Psychology Software Tools</collab></person-group><year iso-8601-date="2012">2012</year><data-title>E-prime</data-title><version designator="2.0">2.0</version><source>Pstnet</source><ext-link ext-link-type="uri" xlink:href="https://pstnet.com/">https://pstnet.com/</ext-link></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radloff</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>The CES-D scale: A self-report depression scale for research in the general population</article-title><source>Applied Psychological Measurement</source><volume>1</volume><elocation-id>100306</elocation-id><pub-id pub-id-type="doi">10.1177/014662167700100306</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajah</surname><given-names>MN</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Overlap in the functional neural systems involved in semantic and episodic memory retrieval</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>470</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1162/0898929053279478</pub-id><pub-id pub-id-type="pmid">15814006</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Ritchey</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Two cortical systems for memory-guided behaviour</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/nrn3338</pub-id><pub-id pub-id-type="pmid">22992647</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reagh</surname><given-names>ZM</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What does the functional organization of cortico-hippocampal networks tell us about the functional organization of memory?</article-title><source>Neuroscience Letters</source><volume>680</volume><fpage>69</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2018.04.050</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renoult</surname><given-names>L</given-names></name><name><surname>Davidson</surname><given-names>PSR</given-names></name><name><surname>Palombo</surname><given-names>DJ</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Personal semantics: at the crossroads of semantic and episodic memory</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>550</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.09.003</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renoult</surname><given-names>L</given-names></name><name><surname>Davidson</surname><given-names>PSR</given-names></name><name><surname>Schmitz</surname><given-names>E</given-names></name><name><surname>Park</surname><given-names>L</given-names></name><name><surname>Campbell</surname><given-names>K</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Autobiographically significant concepts: more episodic than semantic in nature? An electrophysiological investigation of overlapping types of memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>57</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00689</pub-id><pub-id pub-id-type="pmid">25061931</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renoult</surname><given-names>L</given-names></name><name><surname>Tanguay</surname><given-names>AN</given-names></name><name><surname>Beaudry</surname><given-names>M</given-names></name><name><surname>Tavakoli</surname><given-names>P</given-names></name><name><surname>Rabipour</surname><given-names>S</given-names></name><name><surname>Campbell</surname><given-names>K</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name><name><surname>Davidson</surname><given-names>PSR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Personal semantics: Is it distinct from episodic and semantic memory? An electrophysiological study of memory for autobiographical facts and repeated events in honor of Shlomo Bentin</article-title><source>Neuropsychologia</source><volume>83</volume><fpage>242</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.08.013</pub-id><pub-id pub-id-type="pmid">26277459</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renoult</surname><given-names>L</given-names></name><name><surname>Irish</surname><given-names>M</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Rugg</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From knowing to remembering: The semantic–episodic distinction</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>1041</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.09.008</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renoult</surname><given-names>L</given-names></name><name><surname>Rugg</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An historical perspective on Endel Tulving’s episodic-semantic distinction</article-title><source>Neuropsychologia</source><volume>139</volume><elocation-id>107366</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107366</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Libby</surname><given-names>LA</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Cortico-hippocampal systems involved in memory and cognition</chapter-title><source>In Progress in Brain Research</source><publisher-name>Elsevier</publisher-name><fpage>45</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/bs.pbr.2015.04.001</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robin</surname><given-names>J</given-names></name><name><surname>Hirshhorn</surname><given-names>M</given-names></name><name><surname>Rosenbaum</surname><given-names>RS</given-names></name><name><surname>Winocur</surname><given-names>G</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional connectivity of hippocampal and prefrontal networks during episodic and spatial memory based on real-world environments</article-title><source>Hippocampus</source><volume>25</volume><fpage>81</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1002/hipo.22352</pub-id><pub-id pub-id-type="pmid">25154600</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robin</surname><given-names>J</given-names></name><name><surname>Wynn</surname><given-names>J</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The spatial scaffold: The effects of spatial context on memory for events</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>42</volume><fpage>308</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1037/xlm0000167</pub-id><pub-id pub-id-type="pmid">26375782</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial scaffold effects in event memory and imagination</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>9</volume><elocation-id>e1462</elocation-id><pub-id pub-id-type="doi">10.1002/wcs.1462</pub-id><pub-id pub-id-type="pmid">29485243</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Kesner</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A computational theory of hippocampal function, and empirical tests of the theory</article-title><source>Progress in Neurobiology</source><volume>79</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2006.04.005</pub-id><pub-id pub-id-type="pmid">16781044</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DC</given-names></name><name><surname>Umanath</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Event memory: A theory of memory for laboratory, autobiographical, and fictional events</article-title><source>Psychological Review</source><volume>122</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1037/a0037907</pub-id><pub-id pub-id-type="pmid">25330330</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DC</given-names></name><name><surname>Deffler</surname><given-names>SA</given-names></name><name><surname>Umanath</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Scenes enable a sense of reliving: Implications for autobiographical memory</article-title><source>Cognition</source><volume>183</volume><fpage>44</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.10.024</pub-id><pub-id pub-id-type="pmid">30412854</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A conceptual space for episodic and semantic memory</article-title><source>Memory &amp; Cognition</source><volume>50</volume><fpage>464</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.3758/s13421-021-01148-3</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rugg</surname><given-names>MD</given-names></name><name><surname>Wilding</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Retrieval processing and episodic memory</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>108</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01445-5</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rugg</surname><given-names>MD</given-names></name><name><surname>Curran</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Event-related potentials and recognition memory</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>251</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.04.004</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rugg</surname><given-names>MD</given-names></name><name><surname>Vilberg</surname><given-names>KL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain networks underlying episodic memory retrieval</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.11.005</pub-id><pub-id pub-id-type="pmid">23206590</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cognitive neuroscience of constructive memory: remembering the past and imagining the future</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>362</volume><fpage>773</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2087</pub-id><pub-id pub-id-type="pmid">17395575</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Complementary learning systems within the hippocampus: A neural network modelling approach to reconciling episodic memory with statistical learning</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160049</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0049</pub-id><pub-id pub-id-type="pmid">27872368</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheldon</surname><given-names>S</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The nature and time-course of medial temporal lobe contributions to semantic retrieval: an fMRI study on verbal fluency</article-title><source>Hippocampus</source><volume>22</volume><fpage>1451</fpage><lpage>1466</lpage><pub-id pub-id-type="doi">10.1002/hipo.20985</pub-id><pub-id pub-id-type="pmid">22102548</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheldon</surname><given-names>S</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Same as it ever was: vividness modulates the similarities and differences between the neural networks that support retrieving remote and recent autobiographical memories</article-title><source>NeuroImage</source><volume>83</volume><fpage>880</fpage><lpage>891</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.082</pub-id><pub-id pub-id-type="pmid">23845428</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheldon</surname><given-names>S</given-names></name><name><surname>Fenerci</surname><given-names>C</given-names></name><name><surname>Gurguryan</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A neurocognitive perspective on the forms and functions of autobiographical memory retrieval</article-title><source>Frontiers in Systems Neuroscience</source><volume>13</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2019.00004</pub-id><pub-id pub-id-type="pmid">30760984</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheldon</surname><given-names>S</given-names></name><name><surname>Peters</surname><given-names>S</given-names></name><name><surname>Renoult</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Altering access to autobiographical episodes with prior semantic knowledge</article-title><source>Consciousness and Cognition</source><volume>86</volume><elocation-id>103039</elocation-id><pub-id pub-id-type="doi">10.1016/j.concog.2020.103039</pub-id><pub-id pub-id-type="pmid">33220651</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname><given-names>BE</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Goldfarb</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Multiple memory subsystems: Reconsidering memory in the mind and brain</article-title><source>Perspectives on Psychological Science</source><elocation-id>17456916231179146</elocation-id><pub-id pub-id-type="doi">10.1177/17456916231179146</pub-id><pub-id pub-id-type="pmid">37390333</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname><given-names>SH</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Semantic search as pattern completion across a concept</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.12.003</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spreng</surname><given-names>RN</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Patterns of brain activity supporting autobiographical memory, prospection, and theory of mind, and their relationship to the default mode network</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>1112</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21282</pub-id><pub-id pub-id-type="pmid">19580387</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Memory and brain systems: 1969-2009</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>12711</fpage><lpage>12716</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3575-09.2009</pub-id><pub-id pub-id-type="pmid">19828780</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>CEL</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>When zero is not zero: the problem of ambiguous baseline conditions in fMRI</article-title><source>PNAS</source><volume>98</volume><fpage>12760</fpage><lpage>12766</lpage><pub-id pub-id-type="doi">10.1073/pnas.221462998</pub-id><pub-id pub-id-type="pmid">11592989</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sui</surname><given-names>J</given-names></name><name><surname>Humphreys</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The integrative self: How self-reference integrates perception and memory</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>719</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.08.015</pub-id><pub-id pub-id-type="pmid">26447060</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>JJ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Differential engagement of brain regions within a “core” network during scene construction</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>1501</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2010.01.022</pub-id><pub-id pub-id-type="pmid">20132831</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Svoboda</surname><given-names>E</given-names></name><name><surname>McKinnon</surname><given-names>MC</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The functional neuroanatomy of autobiographical memory: A meta-analysis</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>2189</fpage><lpage>2208</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.05.023</pub-id><pub-id pub-id-type="pmid">16806314</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Svoboda</surname><given-names>E</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The effects of rehearsal on the functional neuroanatomy of episodic autobiographical and semantic remembering: A functional magnetic resonance imaging study</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>3073</fpage><lpage>3082</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3452-08.2009</pub-id><pub-id pub-id-type="pmid">19279244</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thakral</surname><given-names>PP</given-names></name><name><surname>Madore</surname><given-names>KP</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The core episodic simulation network dissociates as a function of subjective experience and objective content</article-title><source>Neuropsychologia</source><volume>136</volume><elocation-id>107263</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.107263</pub-id><pub-id pub-id-type="pmid">31743681</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treiber</surname><given-names>JM</given-names></name><name><surname>White</surname><given-names>NS</given-names></name><name><surname>Steed</surname><given-names>TC</given-names></name><name><surname>Bartsch</surname><given-names>H</given-names></name><name><surname>Holland</surname><given-names>D</given-names></name><name><surname>Farid</surname><given-names>N</given-names></name><name><surname>McDonald</surname><given-names>CR</given-names></name><name><surname>Carter</surname><given-names>BS</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Chen</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Characterization and correction of geometric distortions in 814 diffusion weighted images</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0152472</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0152472</pub-id><pub-id pub-id-type="pmid">27027775</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>Episodic and semantic memory</chapter-title><person-group person-group-type="editor"><name><surname>Tulving</surname><given-names>E</given-names></name><name><surname>Donaldson</surname><given-names>W</given-names></name></person-group><source>Organization of memory</source><fpage>382</fpage><lpage>402</lpage><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib140"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1983">1983</year><source>Elements of episodic memory</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Episodic memory and common sense: How far apart? Philosophical Transactions of the Royal Society of London</article-title><source>Series B, Biological Sciences</source><volume>356</volume><fpage>1505</fpage><lpage>1515</lpage><pub-id pub-id-type="doi">10.1098/rstb.2001.0937</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Episodic memory: from mind to brain</article-title><source>Annual Review of Psychology</source><volume>53</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.53.100901.135114</pub-id><pub-id pub-id-type="pmid">11752477</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 Bias Correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Kouwe</surname><given-names>AJW</given-names></name><name><surname>Benner</surname><given-names>T</given-names></name><name><surname>Salat</surname><given-names>DH</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Brain morphometry with multiecho MPRAGE</article-title><source>NeuroImage</source><volume>40</volume><fpage>559</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.12.025</pub-id></element-citation></ref><ref id="bib145"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Barch</surname><given-names>D</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Bucholz</surname><given-names>R</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Curtiss</surname><given-names>SW</given-names></name><name><surname>Della Penna</surname><given-names>S</given-names></name><name><surname>Feinberg</surname><given-names>D</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Heath</surname><given-names>AC</given-names></name><name><surname>Larson-Prior</surname><given-names>L</given-names></name><name><surname>Marcus</surname><given-names>D</given-names></name><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Prior</surname><given-names>F</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The human connectome project: A data acquisition perspective</article-title><source>NeuroImage</source><volume>62</volume><fpage>2222</fpage><lpage>2231</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id></element-citation></ref><ref id="bib146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatansever</surname><given-names>D</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Varying demands for cognitive control reveals shared neural processes supporting semantic and episodic memory retrieval</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2134</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22443-2</pub-id></element-citation></ref><ref id="bib147"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verfaellie</surname><given-names>M</given-names></name><name><surname>Wank</surname><given-names>AA</given-names></name><name><surname>Reid</surname><given-names>AG</given-names></name><name><surname>Race</surname><given-names>E</given-names></name><name><surname>Keane</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Self-related processing and future thinking: Distinct contributions of ventromedial prefrontal cortex and the medial temporal lobes</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>115</volume><fpage>159</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.01.028</pub-id><pub-id pub-id-type="pmid">30826623</pub-id></element-citation></ref><ref id="bib148"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Peterson</surname><given-names>DJ</given-names></name><name><surname>Gatenby</surname><given-names>JC</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Grabowski</surname><given-names>TJ</given-names></name><name><surname>Madhyastha</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evaluation of Field Map and Nonlinear Registration Methods for Correction of Susceptibility Artifacts in Diffusion MRI</article-title><source>Frontiers in Neuroinformatics</source><volume>11</volume><elocation-id>17</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00017</pub-id><pub-id pub-id-type="pmid">28270762</pub-id></element-citation></ref><ref id="bib149"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westmacott</surname><given-names>R</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The contribution of autobiographical significance to semantic memory</article-title><source>Memory &amp; Cognition</source><volume>31</volume><fpage>761</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.3758/BF03196114</pub-id></element-citation></ref><ref id="bib150"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westmacott</surname><given-names>R</given-names></name><name><surname>Black</surname><given-names>SE</given-names></name><name><surname>Freedman</surname><given-names>M</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The contribution of autobiographical significance to semantic memory: evidence from Alzheimer’s disease, semantic dementia, and amnesia</article-title><source>Neuropsychologia</source><volume>42</volume><fpage>25</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(03)00147-7</pub-id></element-citation></ref><ref id="bib151"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Strupp</surname><given-names>J</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Feinberg</surname><given-names>DA</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evaluation of slice accelerations using multiband echo planar imaging at 3T</article-title><source>NeuroImage</source><volume>83</volume><fpage>991</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.055</pub-id></element-citation></ref><ref id="bib152"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yee</surname><given-names>E</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Putting concepts into context</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1015</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0948-7</pub-id></element-citation></ref><ref id="bib153"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref><ref id="bib154"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziaei</surname><given-names>M</given-names></name><name><surname>Burianová</surname><given-names>H</given-names></name><name><surname>von Hippel</surname><given-names>W</given-names></name><name><surname>Ebner</surname><given-names>NC</given-names></name><name><surname>Phillips</surname><given-names>LH</given-names></name><name><surname>Henry</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The impact of aging on the neural networks involved in gaze and emotional processing</article-title><source>Neurobiology of Aging</source><volume>48</volume><fpage>182</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2016.08.026</pub-id><pub-id pub-id-type="pmid">27716474</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Behavioural study</title><sec sec-type="appendix" id="s8-1"><title>Scene rating: nothing/vague</title><p>The four memory conditions differed in their proportion of memory that elicited nothing or a vague image, <italic>F</italic>(2.79, 293.34) = 6.82, <italic>p </italic>&lt; .001, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.06 (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). A greater proportion of general facts (<italic>M</italic> = 0.16, <italic>SE</italic> = 0.02) evoked nothing or something vague compared to repeated events (<italic>M</italic> = 0.10, <italic>SE</italic> = 0.01), <italic>t</italic>(105) = 3.86, <italic>p</italic> &lt; .001, <italic>g</italic> = 0.37, CI 95% [0.18, 0.57], and unique events (<italic>M</italic> = 0.10, <italic>SE</italic> = 0.01), <italic>t</italic>(105) = 3.14, <italic>p</italic> = .002, <italic>g</italic> = 0.30, CI 95% [0.11, 0.50]. Similarly, a greater proportion of autobiographical facts (<italic>M</italic> = 0.14, <italic>SE</italic> = 0.02) were associated with nothing or a vague image compared to repeated events, <italic>t</italic>(105) = 2.94, <italic>p </italic>= .004, <italic>g</italic> = 0.29, CI 95% [0.09, 0.48]. No other comparisons were significant after correction: general facts vs. autobiographical facts, <italic>t</italic>(105) = 1.47, <italic>p</italic> = .144, <italic>g</italic> = 0.14, CI 95% [–0.05, 0.33]; autobiographical facts vs. unique events, <italic>t</italic>(105) = 2.20, <italic>p</italic> = .030, <italic>g</italic> = 0.21, CI 95% [0.02, 0.40], and repeated events vs. unique events, <italic>t</italic>(105) = –0.24, <italic>p </italic>= .814, <italic>g</italic> = –0.02, CI 95% [–0.21, 0.17].</p></sec><sec sec-type="appendix" id="s8-2"><title>Scene rating: object</title><p>The four memory conditions differed in the proportion of memory perceived as an object during retrieval, <italic>F</italic>(3, 315) = 17.11, <italic>p</italic> &lt; .001, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.14 (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). A greater proportion of general facts (<italic>M</italic> = 0.44, <italic>SE</italic> = 0.02) and autobiographical facts (<italic>M</italic> = 0.42, <italic>SE</italic> = 0.02) were perceived as objects compared to repeated events (<italic>M</italic> = 0.34, <italic>SE</italic> = 0.02) and unique events (<italic>M</italic> = 0.30, <italic>SE</italic> = 0.02): general facts vs. repeated events, <italic>t</italic>(105) = 4.17, <italic>p</italic> &lt; .001, <italic>g</italic> = 0.40, CI 95% [0.21, 0.60]; general facts vs. unique events, <italic>t</italic>(105) = 5.94, <italic>p</italic> &lt; .001, <italic>g</italic> = 0.57, CI 95% [0.37, 0.78]; autobiographical facts vs. repeated events, <italic>t</italic>(105) = 3.36, <italic>p </italic>= .001, <italic>g</italic> = 0.33, CI 95% [0.13, 0.52]; autobiographical facts vs. unique events, <italic>t</italic>(105) = 5.53, <italic>p </italic>&lt; .001, <italic>g</italic> = 0.54, CI 95% [0.33, 0.74]. General facts did not differ from autobiographical facts, <italic>t</italic>(105) = 1.22, <italic>p </italic>= .226, <italic>g</italic> = 0.12, CI 95% [–0.07, 0.31], and repeated events did not differ from unique events, <italic>t</italic>(105) = 1.82, <italic>p </italic>= .071, <italic>g</italic> = 0.18, CI 95% [–0.02, 0.37]. Therefore, facts were more frequently associated with the visualization of isolated objects than events.</p></sec></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Non-rotated PLS with a priori contrasts: linear contrast</title><sec sec-type="appendix" id="s9-1"><title>Statistical comparison of brain scores</title><p>All memory conditions differed from one another: general facts vs. autobiographical facts, <italic>t</italic>(47) = –6.97, <italic>p</italic> &lt; .001, <italic>g</italic> = −1.00, CI 95% [–1.34, –0.65]; general facts vs. repeated events, <italic>t</italic>(47) = –16.99, <italic>p</italic> &lt; .001, <italic>g</italic> = –2.43, CI 95% [–2.99, –1.86]; general events vs. unique events, <italic>t</italic>(47) = –18.19, <italic>p</italic> &lt; .001, <italic>g</italic> = –2.61, CI 95% [–3.20, –2.01]; autobiographical facts vs. repeated events, <italic>t</italic>(47) = –12.35, <italic>p</italic> &lt; .001, <italic>g</italic> = –1.77, CI 95% [–2.22, –1.31]; autobiographical facts vs. unique events, <italic>t</italic>(47) = –16.02, <italic>p </italic>&lt; .001, <italic>g</italic> = –2.29, CI 95% [–2.83, –1.75]; repeated events vs. unique events, <italic>t</italic>(47) = –6.53, <italic>p</italic> &lt; .001, <italic>g</italic> = –0.94, CI 95% [–1.27, –0.59]. Thus, unique events were more robustly associated with activity as described in the main text than repeated events, and repeated events compared to autobiographical facts, and autobiographical facts compared to general facts.</p><table-wrap id="app2table1" position="float"><label>Appendix 2—table 1.</label><caption><title>Peaks of clusters for the linear contrast at lags 5–6 and 8–9.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Lag</th><th align="left" valign="bottom">Bootstrap ratio</th><th align="left" valign="bottom">Cluster size (voxels)</th><th align="left" valign="bottom">X (mm)</th><th align="left" valign="bottom">Y (mm)</th><th align="left" valign="bottom">Z (mm)</th><th align="left" valign="bottom">Harvard-Oxford, probability atlas</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="7">Negative saliences</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−5.92</td><td align="char" char="." valign="bottom">99</td><td align="char" char="." valign="bottom">60.0</td><td align="char" char="." valign="bottom">10.0</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">39.0% Right Precentral Gyrus; 28.0% Right Inferior Frontal Gyrus pars opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−5.47</td><td align="char" char="." valign="bottom">204</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">46.0% Right Lateral Occipital Cortex Superior Division; 18.0% Right Lateral Occipital Cortex inferior division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−5.35</td><td align="char" char="." valign="bottom">279</td><td align="char" char="." valign="bottom">55.0</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">30.0</td><td align="left" valign="bottom">29.0% Right Supramarginal Gyrus Posterior Division; 9.0% Right Parietal Operculum Cortex; 8.0% Right Planum Temporale; 6.0% Right Supramarginal Gyrus Anterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−4.41</td><td align="char" char="." valign="bottom">104</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">25.0</td><td align="left" valign="bottom">46.0% Left Parietal Operculum Cortex; 18.0% Left Planum Temporale; 8.0% Left Supramarginal Gyrus Anterior Division; 6.0% Left Supramarginal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.65</td><td align="char" char="." valign="bottom">189</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">59.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−5.51</td><td align="char" char="." valign="bottom">387</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">10.0</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex inferior division; 8.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−5.38</td><td align="char" char="." valign="bottom">124</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">60.0</td><td align="left" valign="bottom">55.0% Right Superior Parietal Lobule</td></tr><tr><td align="left" valign="bottom" colspan="7">Positive saliences</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">5.22</td><td align="char" char="." valign="bottom">129</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">77.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">5.58</td><td align="char" char="." valign="bottom">208</td><td align="char" char="." valign="bottom">−20.0</td><td align="char" char="." valign="bottom">30.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">51.0% Left Superior Frontal Gyrus; 8.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">5.88</td><td align="char" char="." valign="bottom">151</td><td align="char" char="." valign="bottom">17.5</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">7.5</td><td align="left" valign="bottom">56.0% Right Precuneus Cortex; 9.0% Right Cingulate Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">6.08</td><td align="char" char="." valign="bottom">237</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">−60.0</td><td align="char" char="." valign="bottom">20.0</td><td align="left" valign="bottom">49.0% Left Precuneus Cortex; 11.0% Left Supracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.01</td><td align="char" char="." valign="bottom">167</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">−15.0</td><td align="left" valign="bottom">49.0% Left Parahippocampal Gyrus Posterior Division; 29.0% Left Temporal Fusiform Cortex Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.50</td><td align="char" char="." valign="bottom">312</td><td align="char" char="." valign="bottom">−20.0</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">53.0% Left Superior Frontal Gyrus; 10.0% Left Frontal Pole; 6.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">7.11</td><td align="char" char="." valign="bottom">379</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">−2.5</td><td align="left" valign="bottom">70.0% Left Paracingulate Gyrus; 15.0% Left Cingulate Gyrus Anterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">7.18</td><td align="char" char="." valign="bottom">114</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">8.14</td><td align="char" char="." valign="bottom">234</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">9.21</td><td align="char" char="." valign="bottom">856</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">54.0% Left Precuneus Cortex; 12.0% Left Supracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">5.05</td><td align="char" char="." valign="bottom">152</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">30.0</td><td align="char" char="." valign="bottom">47.5</td><td align="left" valign="bottom">34.0% Right Superior Frontal Gyrus; 22.0% Right Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">5.83</td><td align="char" char="." valign="bottom">115</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.40</td><td align="char" char="." valign="bottom">261</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">60.0% Left Cingulate Gyrus Posterior Division; 25.0% Left Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.62</td><td align="char" char="." valign="bottom">91</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−12.5</td><td align="left" valign="bottom">37.0% Left Middle Temporal Gyrus Anterior Division; 23.0% Left Superior Temporal Gyrus Anterior Division; 9.0% Left Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">8.25</td><td align="char" char="." valign="bottom">449</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">47.0% Left Superior Frontal Gyrus; 10.0% Left Middle Frontal Gyrus; 10.0% Left Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">9.97</td><td align="char" char="." valign="bottom">367</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">10.03</td><td align="char" char="." valign="bottom">961</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">43.0% Left Frontal Medial Cortex; 38.0% Left Paracingulate Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">10.43</td><td align="char" char="." valign="bottom">1096</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">42.0% Left Precuneus Cortex; 16.0% Left Intracalcarine Cortex; 12.0% Left Supracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">5.66</td><td align="char" char="." valign="bottom">134</td><td align="char" char="." valign="bottom">−10.0</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">11.0% Left Superior Frontal Gyrus; 9.0% Left Paracingulate Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">6.16</td><td align="char" char="." valign="bottom">196</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">81.0% Left Cingulate Gyrus Posterior Division; 16.0% Left Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.72</td><td align="char" char="." valign="bottom">910</td><td align="char" char="." valign="bottom">−7.5</td><td align="char" char="." valign="bottom">55.0</td><td align="char" char="." valign="bottom">10.0</td><td align="left" valign="bottom">43.0% Left Paracingulate Gyrus; 25.0% Left Frontal Pole; 5.0% Left Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.98</td><td align="char" char="." valign="bottom">476</td><td align="char" char="." valign="bottom">−20.0</td><td align="char" char="." valign="bottom">17.5</td><td align="char" char="." valign="bottom">47.5</td><td align="left" valign="bottom">37.0% Left Superior Frontal Gyrus; 6.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">8.94</td><td align="char" char="." valign="bottom">862</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">31.0% Left Precuneus Cortex; 23.0% Left Supracalcarine Cortex; 21.0% Left Intracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">9.80</td><td align="char" char="." valign="bottom">243</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr></tbody></table><table-wrap-foot><fn><p>Thresholded at bootstrap ratio at ± 3 (<italic>p</italic> &lt; .001), minimal cluster size of 80 voxels, and a minimal distance of 10 voxels. Labels from the Harvard-Oxford (<xref ref-type="bibr" rid="bib36">Desikan et al., 2006</xref>) atlas obtained using AtlasReader (<xref ref-type="bibr" rid="bib95">Notter et al., 2019</xref>).</p></fn></table-wrap-foot></table-wrap><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Brain scores in orange and blue tones for the latent variable (LV) 1 of the non-rotated partial least squares (PLS) with the linear contrast between memory conditions overlayed on the default mode and medial temporal networks from <xref ref-type="bibr" rid="bib10">Barnett et al., 2021</xref> shown in white with black contour and projected onto a surface template from the Human Connectome Project (<xref ref-type="bibr" rid="bib145">Van Essen et al., 2012</xref>) using Connectome Workbench (<xref ref-type="bibr" rid="bib84">Marcus et al., 2011</xref>).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-app2-fig1-v1.tif"/></fig></sec></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Non-rotated PLS with a priori contrasts: facts vs. events</title><sec sec-type="appendix" id="s10-1"><title>Statistical comparison of brain scores</title><p>The brain scores of general and autobiographical facts did not differ from one another, <italic>t</italic>(47) = –0.72, <italic>p</italic> = .477, <italic>g</italic> = –0.10, CI 95% [–0.38, 0.18], and those of repeated and unique events did not differ from one another, <italic>t</italic>(47) = –0.65, <italic>p</italic> = .517, <italic>g</italic> = –0.09, CI 95% [–0.37, 0.19]. Hence, general and autobiographical facts did not differ in how well they expressed the ‘facts’ portion of the LV, and repeated and unique events did not differ in how well they expressed the ‘events’ portion of the LV. All other comparisons were significant: general facts and repeated events, <italic>t</italic>(47) = –15.53, <italic>p</italic> &lt; .001, <italic>g</italic> = –2.22, CI 95% [–2.75, –1.69]; general facts and unique events, <italic>t</italic>(47) = –14.76, <italic>p</italic> &lt; .001, <italic>g</italic> = –2.11, CI 95% [–2.62, –1.60]; autobiographical facts and repeated events, <italic>t</italic>(47) = –15.25<italic>, p </italic>&lt; .001, <italic>g</italic> = –2.18, CI 95% [–2.70, –1.66]; autobiographical facts and unique events, <italic>t</italic>(47) = –14.97, <italic>p </italic>&lt; .001, <italic>g</italic> = –2.14, CI 95% [–2.66, –1.63].</p><table-wrap id="app3table1" position="float"><label>Appendix 3—table 1.</label><caption><title>Peaks of clusters for the facts vs. events contrast at lags 5–6 and 8–9.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Lag</th><th align="left" valign="bottom">Bootstrap ratio</th><th align="left" valign="bottom">Cluster size (voxels)</th><th align="left" valign="bottom">X (mm)</th><th align="left" valign="bottom">Y (mm)</th><th align="left" valign="bottom">Z (mm)</th><th align="left" valign="bottom">Harvard-Oxford, probability atlas</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="7">Negative saliences</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−5.82</td><td align="char" char="." valign="bottom">140</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">43.0% Right Inferior Frontal Gyrus pars opercularis; 39.0% Right Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−4.92</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">67.5</td><td align="char" char="." valign="bottom">−42.5</td><td align="char" char="." valign="bottom">27.5</td><td align="left" valign="bottom">14.0% Right Supramarginal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−7.13</td><td align="char" char="." valign="bottom">182</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">39.0% Right Precentral Gyrus; 28.0% Right Inferior Frontal Gyrus pars opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−6.18</td><td align="char" char="." valign="bottom">267</td><td align="char" char="." valign="bottom">55</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">29.0% Right Supramarginal Gyrus Posterior Division; 9.0% Right Parietal Operculum Cortex; 8.0% Right Planum Temporale; 6.0% Right Supramarginal Gyrus Anterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−5.69</td><td align="char" char="." valign="bottom">303</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">55</td><td align="left" valign="bottom">NA</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−5.63</td><td align="char" char="." valign="bottom">363</td><td align="char" char="." valign="bottom">52.5</td><td align="char" char="." valign="bottom">−75</td><td align="char" char="." valign="bottom">−2.5</td><td align="left" valign="bottom">80.0% Right Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−5.13</td><td align="char" char="." valign="bottom">294</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">25</td><td align="left" valign="bottom">46.0% Left Supramarginal Gyrus Posterior Division; 13.0% Left Angular Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−4.96</td><td align="char" char="." valign="bottom">213</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">87.0% Right Frontal Pole; 5.0% Right Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−4.64</td><td align="char" char="." valign="bottom">145</td><td align="char" char="." valign="bottom">−60</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">79.0% Left Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−4.59</td><td align="char" char="." valign="bottom">204</td><td align="char" char="." valign="bottom">−50</td><td align="char" char="." valign="bottom">−75</td><td align="char" char="." valign="bottom">−2.5</td><td align="left" valign="bottom">78.0% Left Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−4.52</td><td align="char" char="." valign="bottom">85</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">27.5</td><td align="left" valign="bottom">74.0% Left Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−5.56</td><td align="char" char="." valign="bottom">137</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">54.0% Right Frontal Pole; 21.0% Right Inferior Frontal Gyrus pars triangularis</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−5.00</td><td align="char" char="." valign="bottom">111</td><td align="char" char="." valign="bottom">12.5</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">70</td><td align="left" valign="bottom">29.0% Right Lateral Occipital Cortex Superior Division; 6.0% Right Superior Parietal Lobule</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.91</td><td align="char" char="." valign="bottom">288</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">59.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.88</td><td align="char" char="." valign="bottom">136</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">−75</td><td align="char" char="." valign="bottom">7.5</td><td align="left" valign="bottom">62.0% Left Lateral Occipital Cortex Inferior Division; 11.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.76</td><td align="char" char="." valign="bottom">85</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">12.5</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">43.0% Right Inferior Frontal Gyrus pars opercularis; 27.0% Right Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.64</td><td align="char" char="." valign="bottom">140</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">27.0% Right Angular Gyrus; 27.0% Right Supramarginal Gyrus Posterior Division; 11.0% Right Superior Parietal Lobule</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.42</td><td align="char" char="." valign="bottom">120</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">−35</td><td align="char" char="." valign="bottom">32.5</td><td align="left" valign="bottom">22.0% Right Parietal Operculum Cortex; 18.0% Right Supramarginal Gyrus Posterior Division; 10.0% Right Supramarginal Gyrus Anterior Division; 8.0% Right Planum Temporale</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−5.53</td><td align="char" char="." valign="bottom">466</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">−80</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">46.0% Right Lateral Occipital Cortex Inferior Division; 34.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−4.81</td><td align="char" char="." valign="bottom">106</td><td align="char" char="." valign="bottom">−40</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">40.0% Left Lateral Occipital Cortex Inferior Division; 35.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−4.80</td><td align="char" char="." valign="bottom">187</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">44.0% Right Superior Parietal Lobule; 14.0% Right Angular Gyrus; 8.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="left" valign="bottom" colspan="7">Positive saliences</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">5.54</td><td align="char" char="." valign="bottom">131</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">5.60</td><td align="char" char="." valign="bottom">193</td><td align="char" char="." valign="bottom">−27.5</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">37.0% Left Middle Frontal Gyrus; 15.0% Left Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">5.79</td><td align="char" char="." valign="bottom">378</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">−60</td><td align="char" char="." valign="bottom">20</td><td align="left" valign="bottom">49.0% Left Precuneus Cortex; 11.0% Left Supracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">7.25</td><td align="char" char="." valign="bottom">101</td><td align="char" char="." valign="bottom">17.5</td><td align="char" char="." valign="bottom">−92.5</td><td align="char" char="." valign="bottom">0</td><td align="left" valign="bottom">48.0% Right Occipital Pole; 6.0% Right Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">7.29</td><td align="char" char="." valign="bottom">150</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">−95</td><td align="char" char="." valign="bottom">0</td><td align="left" valign="bottom">55.0% Left Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">5.58</td><td align="char" char="." valign="bottom">152</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">−17.5</td><td align="left" valign="bottom">44.0% Left Hippocampus; 12.0% Left Parahippocampal Gyrus Posterior Division; 6.0% Left Parahippocampal Gyrus Anterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">5.76</td><td align="char" char="." valign="bottom">228</td><td align="char" char="." valign="bottom">−5</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">−10</td><td align="left" valign="bottom">61.0% Left Frontal Pole; 24.0% Left Frontal Medial Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.10</td><td align="char" char="." valign="bottom">96</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">35.0% Right Middle Frontal Gyrus; 34.0% Right Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.38</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">−75</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.44</td><td align="char" char="." valign="bottom">120</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">−95</td><td align="char" char="." valign="bottom">−2.5</td><td align="left" valign="bottom">62.0% Left Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.92</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">17.5</td><td align="char" char="." valign="bottom">−95</td><td align="char" char="." valign="bottom">0</td><td align="left" valign="bottom">50.0% Right Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">7.52</td><td align="char" char="." valign="bottom">333</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">40.0% Left Superior Frontal Gyrus; 14.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">8.26</td><td align="char" char="." valign="bottom">261</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">8.68</td><td align="char" char="." valign="bottom">823</td><td align="char" char="." valign="bottom">−15</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">54.0% Left Precuneus Cortex; 12.0% Left Supracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">4.60</td><td align="char" char="." valign="bottom">84</td><td align="char" char="." valign="bottom">−5</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">40</td><td align="left" valign="bottom">47.0% Left Paracingulate Gyrus; 35.0% Left Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">4.64</td><td align="char" char="." valign="bottom">110</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">54.0% Left Middle Frontal Gyrus; 5.0% Left Inferior Frontal Gyrus pars opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">5.60</td><td align="char" char="." valign="bottom">183</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">43.0% Right Superior Frontal Gyrus; 18.0% Right Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">5.99</td><td align="char" char="." valign="bottom">143</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">−75</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">68.0% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.34</td><td align="char" char="." valign="bottom">94</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">−5</td><td align="char" char="." valign="bottom">−12.5</td><td align="left" valign="bottom">37.0% Left Middle Temporal Gyrus Anterior Division; 23.0% Left Superior Temporal Gyrus Anterior Division; 9.0% Left Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.55</td><td align="char" char="." valign="bottom">255</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">−35</td><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">60.0% Left Cingulate Gyrus Posterior Division; 25.0% Left Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">7.14</td><td align="char" char="." valign="bottom">438</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">47.0% Left Superior Frontal Gyrus; 20.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">10.43</td><td align="char" char="." valign="bottom">1115</td><td align="char" char="." valign="bottom">−5</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">15</td><td align="left" valign="bottom">42.0% Left Precuneus Cortex; 16.0% Left Intracalcarine Cortex; 12.0% Left Supracalcarine Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">10.58</td><td align="char" char="." valign="bottom">424</td><td align="char" char="." valign="bottom">−35</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">57.0% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">11.07</td><td align="char" char="." valign="bottom">668</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">43.0% Left Frontal Medial Cortex; 38.0% Left Paracingulate Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">5.15</td><td align="char" char="." valign="bottom">153</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">29.0% Right Middle Frontal Gyrus; 24.0% Right Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">5.36</td><td align="char" char="." valign="bottom">85</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">−5</td><td align="char" char="." valign="bottom">−12.5</td><td align="left" valign="bottom">37.0% Left Middle Temporal Gyrus Anterior Division; 23.0% Left Superior Temporal Gyrus Anterior Division; 9.0% Left Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">6.20</td><td align="char" char="." valign="bottom">188</td><td align="char" char="." valign="bottom">−7.5</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">25.0% Left Superior Frontal Gyrus; 6.0% Left Paracingulate Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.35</td><td align="char" char="." valign="bottom">1051</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">−5</td><td align="left" valign="bottom">25.0% Right Frontal Pole; 20.0% Left Frontal Pole; 5.0% Right Frontal Medial Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.50</td><td align="char" char="." valign="bottom">554</td><td align="char" char="." valign="bottom">−25</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">30.0% Left Superior Frontal Gyrus; 29.0% Left Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.85</td><td align="char" char="." valign="bottom">1075</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">25</td><td align="left" valign="bottom">74.0% Left Precuneus Cortex; 16.0% Right Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">9.38</td><td align="char" char="." valign="bottom">280</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−82.5</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">65.0% Left Lateral Occipital Cortex Superior Division</td></tr></tbody></table><table-wrap-foot><fn><p>Thresholded at bootstrap ratio at ± 3 (<italic>p</italic> &lt; .001), minimal cluster size of 80 voxels, and a minimal distance of 10 voxels. Labels from the Harvard-Oxford (<xref ref-type="bibr" rid="bib36">Desikan et al., 2006</xref>) atlas obtained using AtlasReader (<xref ref-type="bibr" rid="bib95">Notter et al., 2019</xref>).</p></fn></table-wrap-foot></table-wrap><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Brain scores for the latent variable (LV) 1 of the non-rotated partial least squares (PLS) with the facts vs. events contrast between memory conditions overlayed on the default mode and medial temporal networks from <xref ref-type="bibr" rid="bib10">Barnett et al., 2021</xref> shown in white with black contour and projected onto a surface template from the Human Connectome Project (<xref ref-type="bibr" rid="bib145">Van Essen et al., 2012</xref>) using Connectome Workbench (<xref ref-type="bibr" rid="bib84">Marcus et al., 2011</xref>).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-app3-fig1-v1.tif"/></fig></sec></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s11"><title>Mean-centered PLS with memory and control condition</title><sec sec-type="appendix" id="s11-1"><title>PLS results</title><p>PLS produces LVs; in this case, LVs were variables that explained a maximum of the covariance between the four memory conditions (general facts, autobiographical facts, repeated events, unique event) and the spatiotemporal aspects of brain activity (<xref ref-type="bibr" rid="bib1">Abdi and Williams, 2013</xref>; <xref ref-type="bibr" rid="bib71">Krishnan et al., 2011</xref>; <xref ref-type="bibr" rid="bib88">McIntosh et al., 2004a</xref>; <xref ref-type="bibr" rid="bib89">McIntosh and Lobaugh, 2004b</xref>). The first significant LV from the PLS analysis distinguished the four memory conditions from a control condition (i.e., odd/even judgments), explaining 84.94% of the cross-block covariance (<italic>p </italic>&lt; .001, see <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>). The inclusion of a control condition—involving little memory processes like odd/even judgments—along with memory conditions in a PLS tends to pull out the memory network because the dissociation between the control and memory conditions explains a maximum of the covariance in the data (<xref ref-type="bibr" rid="bib3">Addis et al., 2004a</xref>; <xref ref-type="bibr" rid="bib18">Burianova and Grady, 2007</xref>; <xref ref-type="bibr" rid="bib125">Sheldon and Levine, 2013</xref>; <xref ref-type="bibr" rid="bib130">Spreng and Grady, 2010</xref>).</p><p>At lag 7, increased activity in regions from the ‘core memory network’ dissociated the memory conditions from the control condition, including large regions of the prefrontal cortex and parietal cortex bilaterally, the medial temporal lobe, and the left lateral temporal cortex. The voxels that contributed the most to this brain LV, named ‘saliences’, can be found in <xref ref-type="table" rid="app4table1">Appendix 4—table 1</xref> and <xref ref-type="table" rid="app4table2">Appendix 4—table 2</xref> (<xref ref-type="bibr" rid="bib1">Abdi and Williams, 2013</xref>).</p><p>A second significant LV (<italic>p</italic> = .028) accounted for 6.41% of the covariance. As this LV is similar to the non-rotated PLS described in the main text (i.e., general/autobiographical facts vs. repeated/unique events), we do not discuss it further. No other LVs were significant, <italic>p </italic>&gt; .05.</p><table-wrap id="app4table1" position="float"><label>Appendix 4—table 1.</label><caption><title>Peaks of clusters for latent variable (LV) 1 of the mean-centered partial least squares (PLS) (memory and control) at lag 7.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Bootstrap ratio</th><th align="left" valign="bottom">Cluster size (voxels)</th><th align="left" valign="bottom">X (mm)</th><th align="left" valign="bottom">Y (mm)</th><th align="left" valign="bottom">Z (mm)</th><th align="left" valign="bottom">Harvard-Oxford, probability atlas</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="6">Negative saliences</td></tr><tr><td align="left" valign="bottom"> −19.56</td><td align="char" char="." valign="bottom">9172</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">49% Left Precuneus Cortex; 8% Left Cingulate Gyrus Posterior Division</td></tr><tr><td align="left" valign="bottom"> −15.77</td><td align="char" char="." valign="bottom">5015</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">67% Left Frontal Orbital Cortex; 7% Left Inferior Frontal Gyrus Pars Triangularis</td></tr><tr><td align="left" valign="bottom"> −13.30</td><td align="char" char="." valign="bottom">260</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−47.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="left" valign="bottom"> −13.21</td><td align="char" char="." valign="bottom">2622</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">30.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">45% Left Paracingulate Gyrus; 30% Left Superior Frontal Gyrus</td></tr><tr><td align="left" valign="bottom"> −12.42</td><td align="char" char="." valign="bottom">364</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">−72.5</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">57% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="left" valign="bottom"> −12.25</td><td align="char" char="." valign="bottom">289</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">60.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">36% Right Frontal Pole; 31% Left Frontal Pole</td></tr><tr><td align="left" valign="bottom"> −11.48</td><td align="char" char="." valign="bottom">846</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">−37.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="left" valign="bottom"> −8.03</td><td align="char" char="." valign="bottom">805</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">−17.5</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">50% Right Precentral Gyrus; 26% Right Postcentral Gyrus</td></tr><tr><td align="left" valign="bottom"> −7.96</td><td align="char" char="." valign="bottom">221</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">36% Right Frontal Pole; 7% Right Frontal Orbital Cortex</td></tr><tr><td align="left" valign="bottom"> −5.23</td><td align="char" char="." valign="bottom">130</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">−20.0</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">45% Right Parietal Operculum Cortex; 31% Right Central Opercular Cortex; 5% Right Postcentral Gyrus</td></tr><tr><td align="left" valign="bottom"> −5.10</td><td align="char" char="." valign="bottom">192</td><td align="char" char="." valign="bottom">55.0</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">41% Right Inferior Frontal Gyrus Pars Triangularis; 21% Right Inferior Frontal Gyrus Pars Opercularis; 5% Right Middle Frontal Gyrus</td></tr><tr><td align="left" valign="bottom" colspan="6">Positive saliences</td></tr><tr><td align="left" valign="bottom"> 5.77</td><td align="char" char="." valign="bottom">143</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">−22.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="left" valign="bottom"> 6.37</td><td align="char" char="." valign="bottom">231</td><td align="char" char="." valign="bottom">20.0</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">18% Right Superior Frontal Gyrus</td></tr><tr><td align="left" valign="bottom"> 6.84</td><td align="char" char="." valign="bottom">587</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">42% Right Precuneus Cortex; 17% Right Cuneal Cortex; 10% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="left" valign="bottom"> 7.86</td><td align="char" char="." valign="bottom">329</td><td align="char" char="." valign="bottom">−27.5</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">57.5</td><td align="left" valign="bottom">39% Left Precentral Gyrus; 9% Left Superior Frontal Gyrus</td></tr><tr><td align="left" valign="bottom"> 7.87</td><td align="char" char="." valign="bottom">224</td><td align="char" char="." valign="bottom">−20.0</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">−47.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="left" valign="bottom"> 8.51</td><td align="char" char="." valign="bottom">284</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">32% Right Planum Polare; 25% Right Insular Cortex</td></tr><tr><td align="left" valign="bottom"> 8.63</td><td align="char" char="." valign="bottom">417</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">47.5</td><td align="left" valign="bottom">30% Right Cingulate Gyrus Posterior Division; 27% Right Precentral Gyrus; 16% Right Precuneus Cortex; 9% Right Postcentral Gyrus</td></tr><tr><td align="left" valign="bottom"> 8.74</td><td align="char" char="." valign="bottom">1477</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="left" valign="bottom"> 9.21</td><td align="char" char="." valign="bottom">536</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">20.0</td><td align="left" valign="bottom">35% Right Precentral Gyrus; 31% Right Inferior Frontal Gyrus Pars Opercularis</td></tr><tr><td align="left" valign="bottom"> 10.10</td><td align="char" char="." valign="bottom">2341</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">5.0</td><td align="left" valign="bottom">8% Left Lateral Ventricle</td></tr><tr><td align="left" valign="bottom"> 10.43</td><td align="char" char="." valign="bottom">2352</td><td align="char" char="." valign="bottom">−60.0</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">20% Left Supramarginal Gyrus Anterior Division</td></tr><tr><td align="left" valign="bottom"> 12.24</td><td align="char" char="." valign="bottom">1112</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">43% Right Supramarginal Gyrus Posterior Division; 18% Right Supramarginal Gyrus Anterior Division</td></tr><tr><td align="left" valign="bottom"> 12.40</td><td align="char" char="." valign="bottom">3966</td><td align="char" char="." valign="bottom">20.0</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">−25.0</td><td align="left" valign="bottom">No Label</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Thresholded at bootstrap ratio at ± 3 (<italic>p</italic> &lt; .001), minimal cluster size of 80 voxels, and a minimal distance of 10 voxels. Labels from the Harvard-Oxford (<xref ref-type="bibr" rid="bib36">Desikan et al., 2006</xref>) atlas obtained using AtlasReader (<xref ref-type="bibr" rid="bib95">Notter et al., 2019</xref>).</p></fn></table-wrap-foot></table-wrap><table-wrap id="app4table2" position="float"><label>Appendix 4—table 2.</label><caption><title>Peaks of clusters for latent variable (LV) 1 of the mean-centered partial least squares (PLS) (memory and control) at lags 5–6 and 8–9.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Lag</th><th align="left" valign="bottom">Bootstrap ratio</th><th align="left" valign="bottom">Cluster size (voxels)</th><th align="left" valign="bottom">X (mm)</th><th align="left" valign="bottom">Y (mm)</th><th align="left" valign="bottom">Z (mm)</th><th align="left" valign="bottom">Harvard-Oxford, probability atlas</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="7">Negative saliences</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−17.08</td><td align="char" char="." valign="bottom">2903</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">74% Left Precuneus Cortex; 19% Left Cingulate Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−15.47</td><td align="char" char="." valign="bottom">5464</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">74% Left Frontal Orbital Cortex; 6% Left Frontal Operculum Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−14.49</td><td align="char" char="." valign="bottom">436</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">−95.0</td><td align="char" char="." valign="bottom">−5.0</td><td align="left" valign="bottom">55% Left Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−13.18</td><td align="char" char="." valign="bottom">1486</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">30.0</td><td align="left" valign="bottom">54% Left Lateral Occipital Cortex Superior Division; 8% Left Angular Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−12.36</td><td align="char" char="." valign="bottom">555</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="char" char="." valign="bottom">−12.5</td><td align="left" valign="bottom">28% Left Superior Temporal Gyrus Anterior Division; 24% Left Middle Temporal Gyrus Anterior Division; 14% Left Superior Temporal Gyrus Posterior Division; 5% Left Middle Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−12.16</td><td align="char" char="." valign="bottom">1856</td><td align="char" char="." valign="bottom">20.0</td><td align="char" char="." valign="bottom">−95.0</td><td align="char" char="." valign="bottom">0.0</td><td align="left" valign="bottom">44% Right Occipital Pole</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−10.94</td><td align="char" char="." valign="bottom">380</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">−72.5</td><td align="char" char="." valign="bottom">35.0</td><td align="left" valign="bottom">47% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−10.07</td><td align="char" char="." valign="bottom">160</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−12.5</td><td align="left" valign="bottom">57% Right Frontal Pole; 11% Right Frontal Orbital Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−8.82</td><td align="char" char="." valign="bottom">416</td><td align="char" char="." valign="bottom">35.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">−30.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−8.51</td><td align="char" char="." valign="bottom">105</td><td align="char" char="." valign="bottom">60.0</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−17.5</td><td align="left" valign="bottom">48% Right Middle Temporal Gyrus Anterior Division; 14% Right Middle Temporal Gyrus Posterior Division; 5% Right Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−8.11</td><td align="char" char="." valign="bottom">134</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">−50.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−7.17</td><td align="char" char="." valign="bottom">658</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">44% Right Postcentral Gyrus; 23% Right Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">−4.62</td><td align="char" char="." valign="bottom">124</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">50% Right Inferior Frontal Gyrus Pars Triangularis; 14% Right Inferior Frontal Gyrus Pars Opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−19.95</td><td align="char" char="." valign="bottom">7309</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">−57.5</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">74% Left Precuneus Cortex; 19% Left Cingulate Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−16.49</td><td align="char" char="." valign="bottom">7663</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">67% Left Frontal Orbital Cortex; 7% Left Inferior Frontal Gyrus Pars Triangularis</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−13.44</td><td align="char" char="." valign="bottom">675</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">−72.5</td><td align="char" char="." valign="bottom">35.0</td><td align="left" valign="bottom">78% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−12.30</td><td align="char" char="." valign="bottom">388</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">−70.0</td><td align="char" char="." valign="bottom">35.0</td><td align="left" valign="bottom">78% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−11.98</td><td align="char" char="." valign="bottom">208</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">−45.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−9.92</td><td align="char" char="." valign="bottom">649</td><td align="char" char="." valign="bottom">35.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">−30.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−8.61</td><td align="char" char="." valign="bottom">228</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">44% Right Frontal Pole; 11% Right Frontal Orbital Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−8.51</td><td align="char" char="." valign="bottom">757</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">44% Right Postcentral Gyrus; 5% Right Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−8.42</td><td align="char" char="." valign="bottom">135</td><td align="char" char="." valign="bottom">60.0</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−17.5</td><td align="left" valign="bottom">48% Right Middle Temporal Gyrus Anterior Division; 14% Right Middle Temporal Gyrus Posterior Division; 5% Right Superior Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">−4.73</td><td align="char" char="." valign="bottom">95</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">20.0</td><td align="left" valign="bottom">40% Right Inferior Frontal Gyrus Pars Triangularis; 16% Right Inferior Frontal Gyrus Pars Opercularis; 7% Right Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−17.60</td><td align="char" char="." valign="bottom">11,337</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">51% Left Precuneus Cortex; 20% Left Cingulate Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−14.98</td><td align="char" char="." valign="bottom">8569</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">27.5</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">67% Left Frontal Orbital Cortex; 7% Left Inferior Frontal Gyrus Pars Triangularis</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−13.29</td><td align="char" char="." valign="bottom">299</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">−45.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−11.48</td><td align="char" char="." valign="bottom">315</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">−70.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">59% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−8.13</td><td align="char" char="." valign="bottom">135</td><td align="char" char="." valign="bottom">−10.0</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">97% Brain-Stem</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−7.75</td><td align="char" char="." valign="bottom">238</td><td align="char" char="." valign="bottom">35.0</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">−2.5</td><td align="left" valign="bottom">40% Right Insular Cortex; 36% Right Frontal Orbital Cortex; 5% Right Frontal Operculum Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−7.45</td><td align="char" char="." valign="bottom">93</td><td align="char" char="." valign="bottom">−10.0</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">58% Left Lateral Ventricle; 41% Left Caudate</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−7.06</td><td align="char" char="." valign="bottom">285</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">−25.0</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">76% Right Parietal Operculum Cortex; 6% Right Planum Temporale; 5% Right Heschl’s Gyrus (includes H1 and H2)</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−7.02</td><td align="char" char="." valign="bottom">788</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">61% Right Precentral Gyrus; 13% Right Postcentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−5.23</td><td align="char" char="." valign="bottom">157</td><td align="char" char="." valign="bottom">−2.5</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">62.5</td><td align="left" valign="bottom">50% Left Juxtapositional Lobule Cortex; 34% Left Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">−4.85</td><td align="char" char="." valign="bottom">96</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">44% Left Parietal Operculum Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−17.53</td><td align="char" char="." valign="bottom">11,252</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">−80.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">43% Left Occipital Fusiform Gyrus; 13% Left Lingual Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−14.91</td><td align="char" char="." valign="bottom">9678</td><td align="char" char="." valign="bottom">−47.5</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−5.0</td><td align="left" valign="bottom">70% Left Frontal Pole; 8% Left Inferior Frontal Gyrus Pars Triangularis; 5% Left Frontal Orbital Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−14.14</td><td align="char" char="." valign="bottom">332</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">−52.5</td><td align="char" char="." valign="bottom">−45.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−9.50</td><td align="char" char="." valign="bottom">728</td><td align="char" char="." valign="bottom">−60.0</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">−5.0</td><td align="left" valign="bottom">33% Left Middle Temporal Gyrus Temporooccipital Part; 18% Left Middle Temporal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−9.11</td><td align="char" char="." valign="bottom">251</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">−70.0</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">59% Right Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−8.04</td><td align="char" char="." valign="bottom">581</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">−17.5</td><td align="char" char="." valign="bottom">12.5</td><td align="left" valign="bottom">69% Right Insular Cortex; 9% Right Heschl’s Gyrus (includes H1 and H2)</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−7.82</td><td align="char" char="." valign="bottom">210</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">−10.0</td><td align="left" valign="bottom">47% Right Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−6.74</td><td align="char" char="." valign="bottom">120</td><td align="char" char="." valign="bottom">17.5</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">17.5</td><td align="left" valign="bottom">72% Right Caudate</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−6.15</td><td align="char" char="." valign="bottom">195</td><td align="char" char="." valign="bottom">37.5</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">42.5</td><td align="left" valign="bottom">57% Right Precentral Gyrus; 16% Right Postcentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−6.06</td><td align="char" char="." valign="bottom">262</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">53% Left Parietal Operculum Cortex; 6% Left Central Opercular Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−5.81</td><td align="char" char="." valign="bottom">105</td><td align="char" char="." valign="bottom">55.0</td><td align="char" char="." valign="bottom">20.0</td><td align="char" char="." valign="bottom">7.5</td><td align="left" valign="bottom">47% Right Inferior Frontal Gyrus Pars Opercularis; 25% Right Inferior Frontal Gyrus Pars Triangularis</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">−5.73</td><td align="char" char="." valign="bottom">302</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">−25.0</td><td align="char" char="." valign="bottom">70.0</td><td align="left" valign="bottom">45% Right Precentral Gyrus; 15% Right Postcentral Gyrus</td></tr><tr><td align="left" valign="bottom" colspan="7">Positive saliences</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">7.42</td><td align="char" char="." valign="bottom">671</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">78% Right Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">7.64</td><td align="char" char="." valign="bottom">380</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">−50.0</td><td align="char" char="." valign="bottom">−30.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">8.93</td><td align="char" char="." valign="bottom">189</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">−52.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">10.12</td><td align="char" char="." valign="bottom">1257</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">0.0</td><td align="left" valign="bottom">71% Right Insular Cortex; 7% Right Central Opercular Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">13.79</td><td align="char" char="." valign="bottom">5490</td><td align="char" char="." valign="bottom">25.0</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−20.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 5</td><td align="char" char="." valign="bottom">16.45</td><td align="char" char="." valign="bottom">11,000</td><td align="char" char="." valign="bottom">−42.5</td><td align="char" char="." valign="bottom">−17.5</td><td align="char" char="." valign="bottom">60.0</td><td align="left" valign="bottom">46% Left Precentral Gyrus; 15% Left Postcentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">6.87</td><td align="char" char="." valign="bottom">84</td><td align="char" char="." valign="bottom">−15.0</td><td align="char" char="." valign="bottom">−27.5</td><td align="char" char="." valign="bottom">37.5</td><td align="left" valign="bottom">23% Left Precentral Gyrus; 18% Left Cingulate Gyrus Posterior Division; 5% Left Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">7.61</td><td align="char" char="." valign="bottom">318</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">−50.0</td><td align="char" char="." valign="bottom">−30.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">7.95</td><td align="char" char="." valign="bottom">672</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">47.5</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">88% Right Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">9.13</td><td align="char" char="." valign="bottom">132</td><td align="char" char="." valign="bottom">10.0</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">39% Right Cingulate Gyrus Posterior Division; 32% Right Precuneus Cortex; 5% Right Postcentral Gyrus; 5% Right Precentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">10.23</td><td align="char" char="." valign="bottom">1203</td><td align="char" char="." valign="bottom">40.0</td><td align="char" char="." valign="bottom">−10.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">55% Right Insular Cortex; 20% Right Planum Polare</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">13.30</td><td align="char" char="." valign="bottom">945</td><td align="char" char="." valign="bottom">15.0</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">−45.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">13.58</td><td align="char" char="." valign="bottom">10,698</td><td align="char" char="." valign="bottom">−50.0</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">50.0</td><td align="left" valign="bottom">36% Left Supramarginal Gyrus Anterior Division; 23% Left Postcentral Gyrus; 14% Left Superior Parietal Lobule</td></tr><tr><td align="char" char="." valign="bottom"> 6</td><td align="char" char="." valign="bottom">15.35</td><td align="char" char="." valign="bottom">4346</td><td align="char" char="." valign="bottom">17.5</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−17.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">5.27</td><td align="char" char="." valign="bottom">123</td><td align="char" char="." valign="bottom">22.5</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">60.0</td><td align="left" valign="bottom">43% Right Superior Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">5.35</td><td align="char" char="." valign="bottom">132</td><td align="char" char="." valign="bottom">−25.0</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">32.5</td><td align="left" valign="bottom">24% Left Lateral Occipital Cortex Superior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.13</td><td align="char" char="." valign="bottom">322</td><td align="char" char="." valign="bottom">−45.0</td><td align="char" char="." valign="bottom">−70.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">75% Left Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.45</td><td align="char" char="." valign="bottom">271</td><td align="char" char="." valign="bottom">15.0</td><td align="char" char="." valign="bottom">−62.5</td><td align="char" char="." valign="bottom">−45.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">6.71</td><td align="char" char="." valign="bottom">140</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">−52.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">7.23</td><td align="char" char="." valign="bottom">90</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">−92.5</td><td align="char" char="." valign="bottom">−5.0</td><td align="left" valign="bottom">53% Left Occipital Pole; 21% Left Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">7.35</td><td align="char" char="." valign="bottom">331</td><td align="char" char="." valign="bottom">12.5</td><td align="char" char="." valign="bottom">−65.0</td><td align="char" char="." valign="bottom">40.0</td><td align="left" valign="bottom">43% Right Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">7.99</td><td align="char" char="." valign="bottom">230</td><td align="char" char="." valign="bottom">−17.5</td><td align="char" char="." valign="bottom">−5.0</td><td align="char" char="." valign="bottom">30.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">8.04</td><td align="char" char="." valign="bottom">159</td><td align="char" char="." valign="bottom">5.0</td><td align="char" char="." valign="bottom">−27.5</td><td align="char" char="." valign="bottom">45.0</td><td align="left" valign="bottom">62% Right Cingulate Gyrus Posterior Division; 15% Right Precentral Gyrus; 7% Right Precuneus Cortex</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">8.10</td><td align="char" char="." valign="bottom">331</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">10.0</td><td align="left" valign="bottom">87% Right Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">8.14</td><td align="char" char="." valign="bottom">455</td><td align="char" char="." valign="bottom">50.0</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">22.5</td><td align="left" valign="bottom">41% Right Precentral Gyrus; 33% Right Inferior Frontal Gyrus Pars Opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">8.52</td><td align="char" char="." valign="bottom">384</td><td align="char" char="." valign="bottom">−25.0</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">10% Left Lateral Ventricle</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">9.08</td><td align="char" char="." valign="bottom">840</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−35.0</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">48% Left Supramarginal Gyrus Anterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">9.84</td><td align="char" char="." valign="bottom">1103</td><td align="char" char="." valign="bottom">−17.5</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">5.0</td><td align="left" valign="bottom">6% Left Lateral Ventricle</td></tr><tr><td align="char" char="." valign="bottom"> 8</td><td align="char" char="." valign="bottom">11.53</td><td align="char" char="." valign="bottom">3742</td><td align="char" char="." valign="bottom">57.5</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">55.0</td><td align="left" valign="bottom">22% Right Supramarginal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">5.45</td><td align="char" char="." valign="bottom">152</td><td align="char" char="." valign="bottom">22.5</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">60.0</td><td align="left" valign="bottom">40% Right Superior Frontal Gyrus; 6% Right Middle Frontal Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">6.37</td><td align="char" char="." valign="bottom">131</td><td align="char" char="." valign="bottom">−22.5</td><td align="char" char="." valign="bottom">−75.0</td><td align="char" char="." valign="bottom">−47.5</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.02</td><td align="char" char="." valign="bottom">444</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">55.0</td><td align="left" valign="bottom">33% Left Supramarginal Gyrus Anterior Division; 8% Left Supramarginal Gyrus Posterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.02</td><td align="char" char="." valign="bottom">123</td><td align="char" char="." valign="bottom">7.5</td><td align="char" char="." valign="bottom">−32.5</td><td align="char" char="." valign="bottom">47.5</td><td align="left" valign="bottom">30% Right Cingulate Gyrus Posterior Division; 27% Right Precentral Gyrus; 16% Right Precuneus Cortex; 9% Right Postcentral Gyrus</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.70</td><td align="char" char="." valign="bottom">150</td><td align="char" char="." valign="bottom">−17.5</td><td align="char" char="." valign="bottom">−7.5</td><td align="char" char="." valign="bottom">30.0</td><td align="left" valign="bottom">No Label</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">7.89</td><td align="char" char="." valign="bottom">284</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">45.0</td><td align="char" char="." valign="bottom">10.0</td><td align="left" valign="bottom">87% Right Frontal Pole</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">8.30</td><td align="char" char="." valign="bottom">532</td><td align="char" char="." valign="bottom">−25.0</td><td align="char" char="." valign="bottom">−55.0</td><td align="char" char="." valign="bottom">15.0</td><td align="left" valign="bottom">10% Left Lateral Ventricle</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">9.07</td><td align="char" char="." valign="bottom">714</td><td align="char" char="." valign="bottom">−12.5</td><td align="char" char="." valign="bottom">30.0</td><td align="char" char="." valign="bottom">2.5</td><td align="left" valign="bottom">12% Left Lateral Ventricle</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">9.20</td><td align="char" char="." valign="bottom">433</td><td align="char" char="." valign="bottom">42.5</td><td align="char" char="." valign="bottom">10.0</td><td align="char" char="." valign="bottom">30.0</td><td align="left" valign="bottom">25% Right Precentral Gyrus; 21% Right Middle Frontal Gyrus; 17% Right Inferior Frontal Gyrus Pars Opercularis</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">9.23</td><td align="char" char="." valign="bottom">553</td><td align="char" char="." valign="bottom">−30.0</td><td align="char" char="." valign="bottom">−92.5</td><td align="char" char="." valign="bottom">−5.0</td><td align="left" valign="bottom">46% Left Occipital Pole; 14% Left Lateral Occipital Cortex Inferior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">10.06</td><td align="char" char="." valign="bottom">2805</td><td align="char" char="." valign="bottom">55.0</td><td align="char" char="." valign="bottom">−37.5</td><td align="char" char="." valign="bottom">52.5</td><td align="left" valign="bottom">64% Right Supramarginal Gyrus Posterior Division; 11% Right Supramarginal Gyrus Anterior Division</td></tr><tr><td align="char" char="." valign="bottom"> 9</td><td align="char" char="." valign="bottom">10.78</td><td align="char" char="." valign="bottom">840</td><td align="char" char="." valign="bottom">32.5</td><td align="char" char="." valign="bottom">−95.0</td><td align="char" char="." valign="bottom">−7.5</td><td align="left" valign="bottom">62% Right Occipital Pole; 10% Right Lateral Occipital Cortex Inferior Division</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Thresholded at bootstrap ratio at ± 3 (<italic>p</italic> &lt; .001), minimal cluster size of 80 voxels, and a minimal distance of 10 voxels. Labels from the Harvard-Oxford (<xref ref-type="bibr" rid="bib36">Desikan et al., 2006</xref>) atlas obtained using AtlasReader (<xref ref-type="bibr" rid="bib95">Notter et al., 2019</xref>).</p></fn></table-wrap-foot></table-wrap><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>This mean-centered partial least squares (PLS) included the control task in addition to the memory conditions.</title><p>The first significant latent variable (LV) identified activation that dissociated all memory conditions from the control task. (a) Average brain score. Error bars are ±1 <italic>SE</italic> of bootstrap estimates. (<bold>c</bold>) Brain scores shown at each lag (i.e., each TR/1.2 s). Error bars are ±1 <italic>SE</italic>. (<bold>b</bold> and <bold>d</bold>) Brain scores at lag 7 with positive saliences shown in warm colors (increased activity for the control task relative to memory conditions) and negative saliences shown in cold colors (increased activity for the memory conditions relative to the control task). Brain scores are projected onto a surface from the Human Connectome Project (S1200; <xref ref-type="bibr" rid="bib145">Van Essen et al., 2012</xref>) using Connectome Workbench (<xref ref-type="bibr" rid="bib84">Marcus et al., 2011</xref>) in (<bold>b</bold>) and the MNI152NLin2009cAsym volume using FSLeyes (<xref ref-type="bibr" rid="bib86">McCarthy, 2021</xref>) in (<bold>d</bold>). Bootstrap ratios are thresholded at ± 3, <italic>p</italic> &lt; .001, cluster size ≥ 80 voxels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-app4-fig1-v1.tif"/></fig><fig id="app4fig2" position="float"><label>Appendix 4—figure 2.</label><caption><title>Brain scores for the latent variable (LV) 1 of the mean-centered partial least squares (PLS) that comprised the control task and the memory conditions overlayed on the default mode and medial temporal networks from <xref ref-type="bibr" rid="bib10">Barnett et al., 2021</xref> shown in white with black contour and projected onto a surface template from the Human Connectome Project (<xref ref-type="bibr" rid="bib145">Van Essen et al., 2012</xref>) using Connectome Workbench (<xref ref-type="bibr" rid="bib84">Marcus et al., 2011</xref>).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-app4-fig2-v1.tif"/></fig></sec></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s12"><title>Stimuli list</title><table-wrap id="app5table1" position="float"><label>Appendix 5—table 1.</label><caption><title>Practice task.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">Unique events (UE)<italic>‘Please respond (yes or no) if the following events happened to you.’ <sup>1</sup></italic></th><th align="left" valign="bottom" colspan="2">Repeated events (RE)‘Please respond (yes or no) if the following events have happened to you repeatedly in the last year.’</th><th align="left" valign="bottom" colspan="2">Autobiographical facts (AF)<bold>‘</bold>Please respond (yes or no) according to what is usually true for you.’</th><th align="left" valign="bottom" colspan="2">General facts (GF)‘Please respond (yes or no) according to what is usually true for people in this country.’</th></tr><tr><th align="left" valign="bottom">StimuliID</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th></tr></thead><tbody><tr><td align="left" valign="bottom">Ex. 1</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I bought a lottery ticket.</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have bought a lottery ticket.</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I buy lottery tickets.</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Buy lottery tickets.</td></tr><tr><td align="left" valign="bottom">Ex. 2</td><td align="left" valign="bottom">Last weekend</td><td align="left" valign="bottom">I attended a wedding.</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I attended a wedding.</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I attend weddings.</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Attends weddings.</td></tr></tbody></table><table-wrap-foot><fn><p>For the first one, please answer yes or no according to what happened to you once. For example, it could say: This morning, I paid bills. If that happened once, you would say yes.</p></fn><fn><p>For the second one, please answer yes or no accord to what happened to you repeatedly in the last year. For example, it could say: When at work, I paid bills. The key word is ‘repeatedly’. If that happened repeatedly, you would say yes.</p></fn><fn><p>For the third one, please answer yes or no according to what is usually true to you. For example, it could say: Often, I pay bills. If that were true for you, you would say yes.</p></fn><fn><p>Last, please answer yes or no according to what you think is true of most people in this country. For example, a statement could be: Everyone pays bills. If you agree with this, you would say ‘yes’.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app5table2" position="float"><label>Appendix 5—table 2.</label><caption><title>Stimuli list.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="3">Unique events (UE)<italic>‘Please respond (yes or no) if the following events happened to you.’ <sup>1</sup></italic></th><th align="left" valign="bottom" colspan="3">Repeated events (RE)‘Please respond (yes or no) if the following events have happened to you repeatedly in the last year.’</th><th align="left" valign="bottom" colspan="3">Autobiographical facts (AF)‘Please respond (yes or no) according to what is usually true for you.’</th><th align="left" valign="bottom" colspan="3">General facts (GF)‘Please respond (yes or no) according to what is usually true for people in this country.’</th></tr><tr><th align="left" valign="bottom">StimuliID</th><th align="left" valign="bottom">Run</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th><th align="left" valign="bottom">Run</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th><th align="left" valign="bottom">Run</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th><th align="left" valign="bottom">Run</th><th align="left" valign="bottom">Cue</th><th align="left" valign="bottom">Sentence</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I wore white socks.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have worn white socks.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I wear white socks.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Wears white socks.</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I took a shower.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have taken a shower.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I take showers.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Take showers.</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I used a computer.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have used a computer.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I use a computer.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Use a computer.</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I ate breakfast.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have eaten breakfast.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I eat breakfast.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Skip breakfast.</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I made my bed.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have made my bed.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I make my bed.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Make their bed.</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I drove on a highway.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When going to a clinic</td><td align="left" valign="bottom">I have driven on a highway.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I drive on a highway.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Drive on the highway.</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I ate a chocolate.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have eaten a chocolate.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">do I eat chocolate.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Eat chocolate.</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I read a book.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When on the bus</td><td align="left" valign="bottom">I have read books.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I read books.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Read books.</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I ate at a restaurant.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have eaten at a restaurant.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I eat at restaurants.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Eats at restaurants.</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I watched tv.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have watched TV.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I watch TV.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Watches TV.</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I went shopping.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have gone shopping.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I go shopping.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Go shopping.</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I drank coffee.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have drunk coffee.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I drink coffee.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Drink coffee.</td></tr><tr><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I talked on the phone.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When on the bus</td><td align="left" valign="bottom">I have talked on the phone.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I talk on the phone.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Talks on the phone.</td></tr><tr><td align="char" char="." valign="bottom">14</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I ate pizza.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have eaten pizza.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I eat pizza.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Eat pizza.</td></tr><tr><td align="char" char="." valign="bottom">15</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I went to the movies.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have gone to the movies.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I go to the movies.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Go to the movies.</td></tr><tr><td align="char" char="." valign="bottom">16</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I read a newspaper.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When on the bus</td><td align="left" valign="bottom">I have read a newspaper.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I read a newspaper.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Reads a newspaper.</td></tr><tr><td align="char" char="." valign="bottom">17</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I spent money.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have spent money.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I spend money.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Spends money.</td></tr><tr><td align="char" char="." valign="bottom">18</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I rented a movie.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have rented a movie.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I rent movies.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Rent movies.</td></tr><tr><td align="char" char="." valign="bottom">19</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I read a magazine.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have read a magazine.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I read magazines.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Read magazines.</td></tr><tr><td align="char" char="." valign="bottom">20</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I listened to music.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have listened to music.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I listen to music.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Listen to music.</td></tr><tr><td align="char" char="." valign="bottom">21</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I washed dishes.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have washed dishes.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I wash the dishes.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Washes the dishes.</td></tr><tr><td align="char" char="." valign="bottom">22</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I talked to a family member.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at a clinic</td><td align="left" valign="bottom">I have talked to a family member.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I talk to a family member.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Talk to family members.</td></tr><tr><td align="char" char="." valign="bottom">23</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I wore jeans.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have worn jeans.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I wear jeans.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Wear jeans.</td></tr><tr><td align="char" char="." valign="bottom">24</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I slept well.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have slept well.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I sleep well.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Sleeps well.</td></tr><tr><td align="char" char="." valign="bottom">25</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I woke up early.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have woken up early.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I wake up early.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Wakes up early.</td></tr><tr><td align="char" char="." valign="bottom">26</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I had chicken.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have eaten chicken.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I eat chicken.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Eats chicken.</td></tr><tr><td align="char" char="." valign="bottom">27</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I listened to the radio.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at a clinic</td><td align="left" valign="bottom">I have listened to the radio.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I listen to the radio.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Listen to the radio.</td></tr><tr><td align="char" char="." valign="bottom">28</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I went to bed early.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have gone to bed early.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I go to bed early.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Go to bed early.</td></tr><tr><td align="char" char="." valign="bottom">29</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I took a nap.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have taken naps.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I take naps.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Takes naps.</td></tr><tr><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I cooked dinner.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have cooked dinner.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I cook dinner.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Cook dinner.</td></tr><tr><td align="char" char="." valign="bottom">31</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I went dancing.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have gone dancing.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I go dancing.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Go dancing.</td></tr><tr><td align="char" char="." valign="bottom">32</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I watched sports.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have watched sports.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I watch sports.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Watches sports.</td></tr><tr><td align="char" char="." valign="bottom">33</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I checked my email.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When on the bus</td><td align="left" valign="bottom">I have checked my email.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I check my email.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Checks their email.</td></tr><tr><td align="char" char="." valign="bottom">34</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I played with a dog.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have played with a dog.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I play with dogs.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Play with dogs.</td></tr><tr><td align="char" char="." valign="bottom">35</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I bought a CD.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have bought CDs.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I buy CDs.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Buys CDs.</td></tr><tr><td align="char" char="." valign="bottom">36</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I ate fries.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When on the bus</td><td align="left" valign="bottom">I have eaten fries.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I eat fries.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Eat fries.</td></tr><tr><td align="char" char="." valign="bottom">37</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I went to the mall.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have gone to the mall.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I go to the mall.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Goes to the mall.</td></tr><tr><td align="char" char="." valign="bottom">38</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I drank juice.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have drunk juice.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I drink juice.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Drink juice.</td></tr><tr><td align="char" char="." valign="bottom">39</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I went on a walk.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have gone on a walk.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I go on walks.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Go on walks.</td></tr><tr><td align="char" char="." valign="bottom">40</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I ate candy.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have eaten candy.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I eat candy.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Eat candy.</td></tr><tr><td align="char" char="." valign="bottom">41</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I went to the bank.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have gone to the bank.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I go to the bank.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Go to the bank.</td></tr><tr><td align="char" char="." valign="bottom">42</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I played a video game.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have played a video game.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I play video games.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Play video games.</td></tr><tr><td align="char" char="." valign="bottom">43</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I worked out.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have worked out.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I work out.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Work out.</td></tr><tr><td align="char" char="." valign="bottom">44</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I did my laundry.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have done my laundry.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I do my laundry.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Does their laundry.</td></tr><tr><td align="char" char="." valign="bottom">45</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I ate pancakes.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have eaten pancakes.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I eat pancakes.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Eat pancakes.</td></tr><tr><td align="char" char="." valign="bottom">46</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I logged on to Facebook.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have logged on to Facebook.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I log on to Facebook.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Logs on to Facebook.</td></tr><tr><td align="char" char="." valign="bottom">47</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I sent a text message.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When on the bus</td><td align="left" valign="bottom">I have sent a text message.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I send text messages.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Send text messages.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I ate a sandwich.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have eaten a sandwich.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I eat sandwiches.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Some people</td><td align="left" valign="bottom">Eat sandwiches.</td></tr><tr><td align="char" char="." valign="bottom">49</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I hugged a friend.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have hugged a friend.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I hug a friend.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Hug friends.</td></tr><tr><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I watered a plant.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have watered a plant.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I water a plant.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Water plants.</td></tr><tr><td align="char" char="." valign="bottom">51</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I sang a tune.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have sung a tune.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I sing a tune.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Sings tunes.</td></tr><tr><td align="char" char="." valign="bottom">52</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I kissed somebody.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have kissed somebody.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I kiss somebody.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Kiss others.</td></tr><tr><td align="char" char="." valign="bottom">53</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I bought a gift.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have bought a gift.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I buy gifts.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Buys gifts.</td></tr><tr><td align="char" char="." valign="bottom">54</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I went to the gym.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have been to the gym.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I go to the gym.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Go to the gym.</td></tr><tr><td align="char" char="." valign="bottom">55</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I missed a meeting.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have missed a meeting.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Often</td><td align="left" valign="bottom">I miss a meeting.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Misses meetings.</td></tr><tr><td align="char" char="." valign="bottom">56</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I had a drink.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I have had a drink.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I have a drink.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Gave a drink.</td></tr><tr><td align="char" char="." valign="bottom">57</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I took a picture.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have taken a picture.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I take pictures.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Take pictures.</td></tr><tr><td align="char" char="." valign="bottom">58</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I went to a pharmacy.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have been to the pharmacy.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I go to the pharmacy.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Goes to the pharmacy.</td></tr><tr><td align="char" char="." valign="bottom">59</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">This morning</td><td align="left" valign="bottom">I prayed.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have prayed.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I pray.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Pray.</td></tr><tr><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I heard jokes.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have heard jokes.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I hear jokes.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Hears jokes.</td></tr><tr><td align="char" char="." valign="bottom">61</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I gave to a charity.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I have given to charity.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I give to charity.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Give to charity.</td></tr><tr><td align="char" char="." valign="bottom">62</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I visited a museum.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When with friends</td><td align="left" valign="bottom">I have visited a museum.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I visit a museum.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Visit museums.</td></tr><tr><td align="char" char="." valign="bottom">63</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I took a course.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have taken a course.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I take a course.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Take a course.</td></tr><tr><td align="char" char="." valign="bottom">64</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Last week-end</td><td align="left" valign="bottom">I had a cold.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">When on vacation</td><td align="left" valign="bottom">I have had a cold.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Rarely</td><td align="left" valign="bottom">I have a cold.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Many people</td><td align="left" valign="bottom">Have a cold.</td></tr><tr><td align="char" char="." valign="bottom">65</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">This week</td><td align="left" valign="bottom">I went swimming.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When at work</td><td align="left" valign="bottom">I have gone swimming.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Sometimes</td><td align="left" valign="bottom">I go swimming.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Goes swimming.</td></tr><tr><td align="char" char="." valign="bottom">66</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I checked the news online.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at a clinic</td><td align="left" valign="bottom">I have checked the news online.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I check the news online.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Few people</td><td align="left" valign="bottom">Check the news online.</td></tr><tr><td align="char" char="." valign="bottom">67</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">Last night</td><td align="left" valign="bottom">I drove my car.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">When going to a clinic</td><td align="left" valign="bottom">I have driven my car.</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I drive my car.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Drive their car.</td></tr><tr><td align="char" char="." valign="bottom">68</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I brushed my teeth.</td><td align="left" valign="bottom">D</td><td align="left" valign="bottom">When at a clinic</td><td align="left" valign="bottom">I brushed my teeth.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Very often</td><td align="left" valign="bottom">I brush my teeth.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Everyone</td><td align="left" valign="bottom">Brushes their teeth.</td></tr><tr><td align="char" char="." valign="bottom">69</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Yesterday</td><td align="left" valign="bottom">I ate fruit.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When shopping</td><td align="left" valign="bottom">I ate fruit.</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Every day</td><td align="left" valign="bottom">I eat fruits.</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">Most people</td><td align="left" valign="bottom">Eat fruit.</td></tr><tr><td align="char" char="." valign="bottom">70</td><td align="left" valign="bottom">C</td><td align="left" valign="bottom">Today</td><td align="left" valign="bottom">I washed my face.</td><td align="left" valign="bottom">B</td><td align="left" valign="bottom">When alone</td><td align="left" valign="bottom">I washed my face.</td><td align="left" valign="bottom">E</td><td align="left" valign="bottom">Usually</td><td align="left" valign="bottom">I wash my face.</td><td align="left" valign="bottom">G</td><td align="left" valign="bottom">No one</td><td align="left" valign="bottom">Washes their face.</td></tr></tbody></table><table-wrap-foot><fn><p>Note: <sup>1</sup>Brief instructions displayed at the beginning of each block within each run. <sup>2</sup>The memory order within each run was based on the Latin Square technique. Run A and G: RE, UE, AF, GF; Run B and D: UE, AF, GF, RE; Run C: AF, GF, RE, UE; Run E and F: GF, RE, UE, AF. Run order was randomized for each participant. Stimuli were randomized within each block per participant.</p></fn></table-wrap-foot></table-wrap></sec><sec sec-type="appendix" id="s13"><title>Response time</title><p>We entered response time in a repeated-measures ANOVA with memory (general facts, autobiographical facts, repeated events, unique events) and response (yes, no) as factors. Participants were faster to respond yes (<italic>M</italic> = 730.97, <italic>SE</italic> = 24.58) than to respond no (<italic>M</italic> = 757.38, <italic>SE</italic> = 27.25), <italic>F</italic>(1, 47) = 6.21, <italic>p </italic>= .016, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.12. We followed up on a main effect of memory, <italic>F</italic>(3, 141) = 3.38, <italic>p </italic>= .020, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.07, with paired samples <italic>t</italic>-tests corrected with a Holm-Bonferroni procedure. We computed the mean of yes and no responses for each condition prior to that step. Unique events (<italic>M</italic> = 724.35, <italic>SE</italic> = 25.85) had a faster response time than repeated events (<italic>M</italic> = 750.55, <italic>SE</italic> = 26.21), <italic>t</italic>(47) = 2.93, <italic>p</italic> = .005, <italic>g</italic> = 0.42, CI 95% [0.13, 0.71], and general facts (<italic>M</italic> = 753.92, <italic>SE</italic> = 25.43), <italic>t</italic>(47) = 2.75, <italic>p </italic>= .009, <italic>g</italic> = 0.39, CI 95% [0.10, 0.68], but not autobiographical facts (<italic>M</italic> = 747.89, <italic>SE</italic> = 27.21), <italic>t</italic>(47) = 2.15, <italic>p </italic>= .037, <italic>g</italic> = 0.31, CI 95% [0.02, 0.59]. None of the other comparisons were significant: general facts vs. autobiographical facts: <italic>t</italic>(47) = 0.62, <italic>p</italic> = .541, <italic>g</italic> = 0.09, CI 95% [–0.19, 0.37]; general facts vs. repeated events: <italic>t</italic>(47) = 0.31, <italic>p </italic>= .758, <italic>g</italic> = 0.04, CI 95% [–0.24, 0.33]; and autobiographical facts vs. repeated events: <italic>t</italic>(47) = –0.25, <italic>p </italic>= .804, <italic>g</italic> = –0.04, CI 95% [–0.32, 0.25]. The interaction between memory and response was not significant, <italic>F</italic>(3, 141) = 0.97, <italic>p</italic> = .411, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.02. Nevertheless, as we included only yes responses in the fMRI analyses, we repeated the ANOVA with yes responses and keeping only the memory factor. The main effect of memory was not significant, <italic>F</italic>(2.56, 120.09) = 1.23<italic>, p</italic> = .302, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.03, suggesting that all memory conditions produced similar response times when considering yes responses only. Even though seemingly similar to <xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>, for yes responses, participants waited until a response screen to produce their response and their speed does not reflect a true reaction time.</p></sec><sec sec-type="appendix" id="s14"><title>Proportion of yes responses</title><p>We entered the proportion of yes responses in a repeated-measures ANOVA with memory as a factor (general facts, autobiographical facts, repeated events, unique events). The main effect of memory was significant, <italic>F</italic>(2.01, 94.45) = 41.50, <italic>p</italic> &lt; .001, <italic>η</italic><sub><italic>p</italic></sub><sup>2</sup> = 0.47 (see <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref> ). We followed up with paired sample <italic>t</italic>-tests that were corrected using a Holm-Bonferroni procedure. All <italic>t</italic>-tests were significant. As expected and as in <xref ref-type="bibr" rid="bib108">Renoult et al., 2016</xref>, unique events (<italic>M</italic> = 0.42, <italic>SE</italic> = 0.01) had a smaller proportion of yes responses than general facts (<italic>M</italic> = 0.55, <italic>SE</italic> = 0.01), <italic>t</italic>(47) = 10.03, <italic>p</italic> &lt; .001, <italic>g</italic> = 1.44, CI 95% [1.03, 1.84], autobiographical facts (<italic>M</italic> = 0.59, <italic>SE</italic> = 0.01), <italic>t</italic>(47) = 14.09, <italic>p</italic> &lt; .001, <italic>g</italic> = 2.02, CI 95% [1.52, 2.51], and repeated events (<italic>M</italic> = 0.50, <italic>SE</italic> = 0.02) <italic>t</italic>(47) = 5.29, <italic>p </italic>&lt; .001, <italic>g</italic> = 0.76, CI 95% [0.44, 1.07]. Further, repeated events had a smaller proportion of yes responses than general facts, <italic>t</italic>(47) = 2.22, <italic>p</italic> = .031, <italic>g</italic> = 0.32, CI 95% [0.03, 0.61], and autobiographical facts, <italic>t</italic>(47) = 4.24, <italic>p</italic> &lt; .001, <italic>g</italic> = 0.61, CI 95% [0.30, 0.91]. The proportion of yes responses was larger for autobiographical facts than general facts, <italic>t</italic>(47) = –3.24, <italic>p </italic>= .002, <italic>g</italic> = –0.46, CI 95% [–0.76, –0.17]. In raw numbers, this translates into an <italic>M</italic> of 37.06 trials (<italic>SE</italic> = 0.77, range from 22 to 51) for general facts, <italic>M</italic> of 39.81 trials (<italic>SE</italic> = 0.76, range from 24 to 56) for autobiographical facts, <italic>M</italic> of 33.77 trials (<italic>SE</italic> = 1.34, range from 14 to 57) for repeated events, and <italic>M</italic> of 28.02 trials (<italic>SE</italic> = 0.70, range from 18 to 38) for unique events.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Proportion of yes responses relative to the total number of responses a participant made.</title><p>Red points represent scores of individual participants (<italic>N</italic> = 48). A black line shows the median and red lines show the quartiles. *<italic>p</italic> &lt; .05, **<italic>p</italic> &lt; .01, ***<italic>p</italic> &lt; .001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83645-app5-fig1-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83645.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>There has been much theoretical and empirical work focused on the distinctions between episodic and semantic memory. The current work is important because it provides convincing and compelling evidence that this two-part model is not sufficient to explain the range of memory retrieval modes, particularly when considering personal semantic memory. This will be of great interest to memory researchers and will become a benchmark for future studies of autobiographical memory.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83645.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Roberts</surname><given-names>Reece</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03b94tp07</institution-id><institution>University of Auckland</institution></institution-wrap><country>New Zealand</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Personal Semantic, General Semantic, and Episodic Memory: Shared and Unique Neural Correlates&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Chris Baker as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Reece Roberts (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Both reviewers felt that the paper tackles an incredibly important distinction between episodic and semantic memory by considering some forms of memory that lie in between like personal semantics (self-knowledge). Instead of a dichotomy between episodic and semantic memory, the authors propose that the distinctions lie along a continuum. The reviewers both supported the use of PLS for addressing the questions proposed. The reviews did have several helpful considerations for the authors to address in a revision and I list them below (as well as include the comments at the end of the letter).</p><p>First, both reviewers noted that the sheer amount of results reported is overwhelming and they agreed (as do I) that the authors should limit reporting to the hypothesis-driven results (non-rotated) and leave the data-driven results for Supplemental data. We would not want the main targeted hypothesis-driven results to be lost in a sea of reporting, as they are close to doing now.</p><p>Second, it was suggested that the authors consider a greater link between the neuroimaging dataset and the behavioral dataset. This can be largely discussed in the discussion since it would be hard to propose a formal cross-experiment analysis. But comparing and contrasting the two results a bit more would be helpful and allow readers to generate new hypotheses to test in future studies.</p><p>Third, Reviewer 1 pointed out correctly that the contrast weights used to define a linear contrast do not do so in its current form. This analysis should be redone if the authors want to claim it is a linear (equidistant between conditions) effect. If not, please restate the purpose and interpretation of the analysis.</p><p>Fourth, there were several important detailed analysis issues brought up also by Reviewer #2 that should be addressed. One in particular related to the modeling of each trial with the inclusion of the cue itself, rather than the question. This reviewer rightly raises an important question about how the inclusion of the cue itself may be resulting in more overlap between conditions. Please address this and other questions about blocking trials.</p><p>Fifth, both reviewers felt the paper could be strengthened by not simply discarding questions with 'no' responses but rather using them to contrast with the 'yes' trials. Could the authors consider these additional analyses and what they might add to the paper?</p><p>Sixth, in terms of added discussion it was noted that reference to how dementias characterized by semantic or episodic memory loss have overlapping atrophy, particularly in the anterior hippocampus, would be an important empirical datapoint to include.</p><p>Chapleau, M., Aldebert, J., Montembeault, M., and Brambati, S. M. (2016). Atrophy in Alzheimer's Disease and Semantic Dementia: An ALE Meta-Analysis of Voxel-Based Morphometry Studies. Journal of Alzheimer's disease: JAD, 54(3), 941-955. https://doi.org/10.3233/JAD-160382</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Using both neuroimaging and behavioural data, the current study investigates the relationship between personal semantics (i.e., self-knowledge) and the classical distinction between episodic and semantic memory. In the fMRI study, the authors employed a design in which participants were required to make yes/no decisions to a range of questions that either involved memory for unique events, memory for repeated events, autobiographical knowledge, or general knowledge. In a separate behavioural experiment, a range of subjective/phenomenological ratings (e.g., self-relevance, visual detail, scene rating) was made to the same conditions.</p><p>The manuscript is well-written and tackles a topic of central importance in the cognitive neuroscience of memory. While episodic and semantic memory are routinely thought of as two separate, interacting systems, the authors persuasively argue against such a clear-cut dichotomy in the introduction. Instead, they propose a continuum/spectrum ranging from the recollection of specific and repeated events (episodic) to personal semantics and general facts (semantic). The use of PLS is appropriate as it allows for the detection of whole-brain patterns associated with underlying latent variables and is frequently used in autobiographical memory and future imagination literature for this reason. The sample size is appropriate (final N = 48), and substantially larger than that of similar studies. A side note: the authors cite Grady et al. (2021) to claim that the ideal N is 80 in PLS analyses, but the Grady study specifically investigated how sample sizes affected the stability of brain-behaviour correlations so is not relevant to the current study.</p><p>Despite the many positive aspects of the current study, there are several issues that, if addressed, would benefit the clarity and impact of the manuscript. First, the authors use both a combination of data-driven (i.e., mean-centered) and hypothesis-driven (i.e., non-rotated) PLS analyses, and the volume of results reported could be substantially reduced if, instead, a series of hypothesis-driven analyses were carried out that mapped onto the authors' hypotheses. While data-driven PLS approaches are useful for exploring data, a drawback is that patterns of covariance are often detected that are not related to any specific hypotheses. In addition, the number of significant latent variables produces a large volume of results that are spread across the main manuscript and appendices. As the authors do have specific hypotheses, a series of targeted, non-rotated hypotheses would be more appropriate in delineating the relationship between the experimental conditions.</p><p>Second, I think greater emphasis could have been placed on outlining the relationship between neuroimaging and behavioural experiments. While there is probably no way to link the two statistically, as the two datasets involve separate sets of participants, I do think the Discussion section would benefit greatly from the authors theorising about how the differences in conditions that appear in the behavioural experiment map onto brain regions apparent in the fMRI results. For example, there is a linear increase in visual details in the behavioural experiment and a set of regions that show the same effect in the neuroimaging experiment – which of those brain regions do the authors propose is associated with the increase in visual detail? Which are associated with the increase in self-relevance? It should be added that the interpretation of functional roles of brain regions would be greatly enhanced if percent signal change values were extracted and plotted for key regions in the core network.</p><p>Regarding the non-rotated PLS analysis testing for a contrast across the four memory conditions, the authors coded for the following contrast: [-2,-1, 1, 2]. This is described as a linear contrast, but this is not the case, as it does not code for an equal difference between conditions. It codes for a smaller difference between two &quot;episodic&quot; conditions and the two &quot;semantic&quot; conditions and a larger difference between the repeated events and autobiographical facts conditions. This can be observed in Figures5A and 5C. In addition, the BSR images (Figure 5B and Figure 5D) look very similar to those observed in the first mean-centered &quot;Memory only&quot; LV, which distinguishes between episodic and semantic conditions (Figure 3). A true linear contrast would be: [-3 -1 1 3]. If the goal was to run a linear contrast, this analysis should be redone. Alternatively, the authors should provide a rationale about why the contrast was coded in the way that it was-what are the reasons for predicting larger differences between repeated events and autobiographical facts conditions than, for example, the unique and repeated events?</p><p>Lastly, the central idea of the current manuscript is that within the extended core memory network (negatively weighted regions in Figure 1), a set of elementary operations are weighted differently depending on the type of memory being recalled. For this framework to hold, it's important to know if the differences observed between the memory conditions (e.g., Figure 3, Figure 5) are observed in a set of regions within the core memory network or if there are regions outside the core network that show differences between the memory conditions.</p><p>As stated in the public review, I think this manuscript has the potential to be of interest to many researchers in the cognitive neuroscience of memory. In addition to the issues raised in the public review, I think the design of the neuroimaging experiment allows for supplementary analyses that could produce results that corroborate the main findings. If I understand the design correctly, only the &quot;Yes&quot; responses were included in the fMRI analyses and the &quot;No&quot; responses were discarded. I wonder if there is an opportunity to directly contrast the Yes and No responses for each condition, as presumably, these should isolate the neural correlates associated with each memory condition and there should be notable differences in those correlates for each memory condition (e.g. greater hippocampal responses in the unique event Yes &gt; No contrast relative to the general facts Yes &gt; No contrast).</p><p>I think the addition of the large volume of results in the appendix section is commendable in some respects, I am skeptical if a reader will actually consume the large volume of data in the tables. Perhaps linking to the overlay nifti files hosted on NeuroVault may be more appropriate?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This study uses advanced fMRI methods to address important and theoretically-motivated questions about memory retrieval as it occurs in the real world. Central to this study is the notion that, when studying how people recall information for their lives, we can understand a potential intermediary form of memory that exists between episodic and semantic memory: personal semantics. The main study was an fMRI study in which participants responded to questions that involved accessing information from episodic memories, two forms of personal semantics – repeated events and autobiographical knowledge – as well as semantic memory. A parallel behavioural experiment on a different sample of participants was conducted to examine how these forms of information differ with respect to subjective ratings.</p><p>A major strength of this paper is that it questions the classic division between episodic and semantic memory, proposing alternate views for how to consider retrieval from declarative memory. The use of PLS to analyze the data was a smart choice, albeit there are some considerations for these analyses. It is unfortunate that the neuroimaging and behavioural experiments were not collected on the sample. It might be useful to consider how the behavioural results can inform the neuroimaging experiment rather than considering the behavioural experiment solo.</p><p>I have some suggestions to help strengthen the manuscript, which are listed below in order of presentation rather than importance.</p><p>The introduction reviews all the key pieces of literature. When noting the overlap between episodic and semantic memory wrt the hippocampus, you may wish to include literature that notes how dementias characterised by semantic or episodic memory loss do have overlapping atrophy, particularly in the anterior hippocampus</p><p>Chapleau, M., Aldebert, J., Montembeault, M., and Brambati, S. M. (2016). Atrophy in Alzheimer's Disease and Semantic Dementia: An ALE Meta-Analysis of Voxel-Based Morphometry Studies. Journal of Alzheimer's disease: JAD, 54(3), 941-955. https://doi.org/10.3233/JAD-160382</p><p>In the introduction. I think the rationale for the study could be strengthened by explicitly stating how the resulting work with contribute to new memory theories and extend from the reviewed literature. Simply said – note exactly what knowledge gap is being filled.</p><p>In terms of the methods, the choice to block the conditions within each run raises the question of whether the results are due to differences in the content being accessed (semantic or episodic) or retrieval state (retrieval mode, in the words of Tulving) effects. Could the authors speak to this? (I come back to this with some suggestions for the discussion)</p><p>For the analysis, the authors note that the PLS analysis began at cue onset, and they estimated activity for 12 TR (14.4. seconds). What was the choice to analyze at the cue onset rather than the question? It seems to me that including the BOLD response associated with the cue means that your signal included both activities related to that cue, which is different across the conditions (some are temporal cues, some are activity cues, and some reflect people), as well as activity in accessing the intended information (a fact or event).</p><p>I also found that the stimuli for the general facts condition seem to reflect opinions (Few people take pictures) rather than facts (The Queen of England is dead). Can the authors speak to this?</p><p>The PLS analysis also seems to be focused on yes responses, however, from Appendix 6 it seems like this means that between.6 and.4 proportion of the trials are not being analyzed. Would it not be useful for the authors in the trials by yes or no responses to see how ts drove neural activity?</p><p>In terms of the control task, the authors note why they included a control task e (page 7, line 11), but I would like to know why the authors selected the odd/even task. That is, what processes are being controlled for with this task? It seems to me that it would be general internally directed attention vs externally directed attention rather than specifying a memory network, per se. Does the choice of this task alter the pattern pulled out by PLS?</p><p>In terms of results, from Figure 1 it also seems that the negative brain pattern was being represented more significantly by RE and UE than GF or AF (as the CI for the brain scores does not overlap) – is this correct? If so, this would suggest to me that there is more dissociation between facts vs events than commonalities.</p><p>In how the results are presented, I had a hard time integrating the behavioural and neuroimaging data, perhaps the paper would benefit from presenting the behavioural study first to show the distinctions in the memory cues and then the neuroimaging data to examine neural mechanisms.</p><p>The authors introduce the idea of the results reflecting the engagement of the DMN and different subsystems, but their results present patterns of activity that are not constrained to these networks or reflect connectivity, a better metric of network engagement. Perhaps the authors could constrain the PLS analysis to the DMN network so that they map their findings to the reviewed work about DMN contributions to autobiographical memory.</p><p>I also think it would be great to focus the discussion on specific theories that fall within a component process framework to suggest why the resulting patterns emerged, such as theories that result reflect accessing context dependent vs independent information (Grilli et al.,) or different levels of consciousness (Tulving). In fact, I was curious if the authors thought that the distinctions are because different content is being accessed or the same content at different levels of representation. Both have relevance to theories on accessing information from memory that are noted. For example, the PMAT model discussing distinctions in content versus the TTT (Gilboa and Moscovitch) would note distinctions in representation, at least that is my understanding.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83645.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>Using both neuroimaging and behavioural data, the current study investigates the relationship between personal semantics (i.e., self-knowledge) and the classical distinction between episodic and semantic memory. In the fMRI study, the authors employed a design in which participants were required to make yes/no decisions to a range of questions that either involved memory for unique events, memory for repeated events, autobiographical knowledge, or general knowledge. In a separate behavioural experiment, a range of subjective/phenomenological ratings (e.g., self-relevance, visual detail, scene rating) was made to the same conditions.</p><p>The manuscript is well-written and tackles a topic of central importance in the cognitive neuroscience of memory. While episodic and semantic memory are routinely thought of as two separate, interacting systems, the authors persuasively argue against such a clear-cut dichotomy in the introduction. Instead, they propose a continuum/spectrum ranging from the recollection of specific and repeated events (episodic) to personal semantics and general facts (semantic). The use of PLS is appropriate as it allows for the detection of whole-brain patterns associated with underlying latent variables and is frequently used in autobiographical memory and future imagination literature for this reason.</p><p>1. The sample size is appropriate (final N = 48), and substantially larger than that of similar studies. A side note: the authors cite Grady et al. (2021) to claim that the ideal N is 80 in PLS analyses, but the Grady study specifically investigated how sample sizes affected the stability of brain-behaviour correlations so is not relevant to the current study.</p></disp-quote><p>Thank you for pointing out the important nuance of Grady et al. We have edited this statement in the methods. “This sample size was the largest that was possible to achieve. The sample size is larger than similar studies (e.g. <italic>N</italic> ~12-28; Addis et al., 2011; Burianova and Grady, 2007; Ford et al., 2011; Holland et al., 2011). No formal power analysis was conducted.”.</p><disp-quote content-type="editor-comment"><p>2. Despite the many positive aspects of the current study, there are several issues that, if addressed, would benefit the clarity and impact of the manuscript. First, the authors use both a combination of data-driven (i.e., mean-centered) and hypothesis-driven (i.e., non-rotated) PLS analyses, and the volume of results reported could be substantially reduced if, instead, a series of hypothesis-driven analyses were carried out that mapped onto the authors' hypotheses. While data-driven PLS approaches are useful for exploring data, a drawback is that patterns of covariance are often detected that are not related to any specific hypotheses. In addition, the number of significant latent variables produces a large volume of results that are spread across the main manuscript and appendices. As the authors do have specific hypotheses, a series of targeted, non-rotated hypotheses would be more appropriate in delineating the relationship between the experimental conditions.</p></disp-quote><p>We now report only hypotheses-driven analyses in the paper to reduce the volume of results. Given the conceptual similarity of the non-rotated (NR) PLS contrasts and mean-centered (MC) PLS with memory conditions only, we omitted the latter. Nevertheless, the ability to elucidate the relation between memory types with little constraints, in a data-driven manner, was one of the strongest appeals of PLS. Hence, we moved the MC PLS with memory conditions and the control task to the Supplementary Information. The benefit of preserving this MC PLS is twofold. First, a potential pitfall is that readers may interpret PLS results like univariate analyses. The first mean-centered PLS clarifies the narrative through highlighting the commonalities across memory conditions and this follows in the footsteps of many PLS papers in the literature (e.g., Addis et al., 2004; Sheldon and Levine, 2005). Incidentally, the LV reinforces confidence in the validity of our findings because the typical declarative memory network emerges from the memory conditions vs. control LV (i.e., the first LV). As described in the original submission of the manuscript, the second LV from that MC PLS was relatively similar to the general/autobiographical facts vs. repeated/unique events LV (simplified to facts vs. events) of the second MC PLS, now omitted. The finding using a data-driven analysis bolsters confidence in the NR PLS findings and informs the interpretation of data.</p><disp-quote content-type="editor-comment"><p>3. Second, I think greater emphasis could have been placed on outlining the relationship between neuroimaging and behavioural experiments. While there is probably no way to link the two statistically, as the two datasets involve separate sets of participants, I do think the Discussion section would benefit greatly from the authors theorising about how the differences in conditions that appear in the behavioural experiment map onto brain regions apparent in the fMRI results. For example, there is a linear increase in visual details in the behavioural experiment and a set of regions that show the same effect in the neuroimaging experiment – which of those brain regions do the authors propose is associated with the increase in visual detail? Which are associated with the increase in self-relevance?</p></disp-quote><p>We followed the suggestion from Comment R2.9 to invert the order of fMRI and behavioural results, and further anchored the a-priori contrasts in behavioural findings. We also discussed the behavioural results before some of the neuroimaging findings to better contrast their relation in the discussion, refining the links as appropriate.</p><p>In results: “We used a non-rotated PLS to test two theoretically plausible relations between the four memory conditions: a linear contrast (-3, -1, 1, 3) and one comparing general/autobiographical facts and repeated/unique events (-1, -1, 1, 1; abbreviated to facts vs. events subsequently). A linear contrast would be consistent with the continuum perspective of personal semantics (see Box 3; Renoult et al., 2012), which would predict an increase in activity from general facts to autobiographical facts, from autobiographical facts to repeated events, and from repeated events to unique events. The increase in visual details (described above) followed precisely this pattern. Similarly, personal relevance increased from general facts to personal semantics (autobiographical facts and repeated events) to unique events, suggesting similar dynamics between component processes (e.g., contextual specificity may increase along with personal relevance). The facts vs. events contrast would favour the view of personal semantics as a subtype of general semantics (Box 2; Renoult et al., 2012). Of all personal semantics, autobiographical facts corresponds best with this view due to its abstraction from events, its more objective quality than other forms of personal knowledge (e.g., trait knowledge), and the feeling of “knowing” the “facts” rather than recollecting events (i.e., “noetic” consciousness; Tulving, 2002). Repeated events would instead group with unique events as “event memory” due to the common construction of a scene (Rubin and Umanath, 2015). Indeed, participants in the behavioural study perceived scenes as frequently for repeated and unique events. However, it is less clear how one would accommodate the greater number of scenes evoked for autobiographical than general facts in a way that aligns with that perspective. Thus, the conjunction of the visualization of scenes, amount of visual details, and personal relevance agrees most with a continuum of contextual specificity.”</p><p>In discussion: “The importance of situational or contextual elements featured strongly in the neuroimaging data as in the behavioural data. The core network, also known as the default mode network, has been subdivided into an anterior temporal network linked to “entities” (Ranganath and Ritchey, 2012; Reagh and Ranganath, 2018) or “conceptual remembering” (Sheldon et al., 2019) and a posterior medial network linked to “situational models” (Ranganath and Ritchey, 2012; Reagh and Ranganath, 2018) or “perceptual remembering” (Sheldon et al., 2019). Although these networks process different kinds of information, neither is strictly dedicated to semantic or episodic memory (Reagh and Ranganath, 2018; Sheldon et al., 2019). For instance, knowledge can facilitate the search and construction of events (Irish and Piguet, 2013) and semantic memory can integrate contextual information (e.g., Greenberg et al., 2009; Sheldon and Moscovitch, 2012). Accordingly, in our study the dissociation between general/autobiographical facts and repeated/unique events did not have a clear posterior medial to anterior temporal demarcation (i.e., posterior medial activity for events and anterior temporal activity for facts). Instead, regions primarily within the posterior medial network (i.e., angular gyrus, posterior cingulate gyrus, precuneus, and parahippocampal gyrus; Ritchey et al., 2015) dissociated memory conditions, whereas those within the anterior temporal network (i.e., frontal orbital cortex, inferior anterior temporal gyrus, temporal pole, bilateral amygdala, and perirhinal cortex; Ritchey et al., 2015) did not. Indeed, events contained greater contextual information, as suggested by the increased proportion of scenes they evoked, as compared to factual memories. Further, cues were more temporally specific for repeated/unique events than general/autobiographical facts. Consistent with this, many regions of the posterior medial network were associated with greater activity for repeated/unique events than general/autobiographical facts (similar to Ford et al., 2011; Levine et al., 2004; Maguire and Mummery, 1999), and showed a linear increase from the most general type of memory (i.e., general facts) to the most specific memory (i.e., unique events). Therefore, activity in regions associated with visuospatial processing (e.g., precuneus) and scenes (e.g., medial temporal regions) coheres with behavioural data to support the prominence of contextual specificity in determining the relation across memory types. Activity in medial frontal regions is in harmony with ratings of visual details and scene perception, likewise increasing along a continuum of contextual specificity (see Figure 5d). However, this anterior portion of the medial frontal cortex may correspond best to self-processing rather than “situational” processing or mental time travel (Lieberman et al., 2019). If activity in the medial frontal cortex in our study reflected exclusively self-processing, one would expect a greater proximity between autobiographical facts and repeated events on the basis of subjective ratings of self-relevance. The additional concordance of medial frontal cortex with a continuum of contextual specificity could be a corollary of the strong links between aspects of self-relevance and episodic simulation (Tulving et al., 2002; King et al., 2022; Grysman et al., 2013; Verfaellie et al., 2019), in addition to this region’s role in modulating recollection (McCormick et al., 2020), for example through engaging schema-related information (Gilboa and Marlatte, 2017).”</p><disp-quote content-type="editor-comment"><p>4. It should be added that the interpretation of functional roles of brain regions would be greatly enhanced if percent signal change values were extracted and plotted for key regions in the core network.</p></disp-quote><p>We added percent signal change for peaks within key regions of the core network in Figure 5. Two of the peaks (i.e., a and c) were common between the two LVs, and so also serve to illustrate the compatibility of the two contrasts.</p><disp-quote content-type="editor-comment"><p>5. Regarding the non-rotated PLS analysis testing for a contrast across the four memory conditions, the authors coded for the following contrast: [-2,-1, 1, 2]. This is described as a linear contrast, but this is not the case, as it does not code for an equal difference between conditions. It codes for a smaller difference between two &quot;episodic&quot; conditions and the two &quot;semantic&quot; conditions and a larger difference between the repeated events and autobiographical facts conditions. This can be observed in Figures5A and 5C. In addition, the BSR images (Figure 5B and Figure 5D) look very similar to those observed in the first mean-centered &quot;Memory only&quot; LV, which distinguishes between episodic and semantic conditions (Figure 3). A true linear contrast would be: [-3 -1 1 3]. If the goal was to run a linear contrast, this analysis should be redone. Alternatively, the authors should provide a rationale about why the contrast was coded in the way that it was-what are the reasons for predicting larger differences between repeated events and autobiographical facts conditions than, for example, the unique and repeated events?</p></disp-quote><p>Thank you for noting this error. We repeated this PLS analysis with the correct contrasts: [-3 -1 1 3]. The corrected results are now reported in the paper with a pattern of results that are similar (albeit not identical) to the erroneous contrast. Critically, the new PLS results do not change the interpretations made in our initial draft.</p><disp-quote content-type="editor-comment"><p>6. Lastly, the central idea of the current manuscript is that within the extended core memory network (negatively weighted regions in Figure 1), a set of elementary operations are weighted differently depending on the type of memory being recalled. For this framework to hold, it's important to know if the differences observed between the memory conditions (e.g., Figure 3, Figure 5) are observed in a set of regions within the core memory network or if there are regions outside the core network that show differences between the memory conditions.</p></disp-quote><p>In response to this comment, we repeated the non-rotated PLS using a mask of the default mode network (DMN; anterior temporal, medial prefrontal, posterior medial) and medial temporal network from Barnett et al. (2021) to test whether these regions would suffice to dissociate the memory conditions. Note that Barnett et al. conceptualize an MTL network as dissociable from the DMN, however the authors reference this subnetwork as a critical DMN bridge between DMN and the hippocampus and other networks. In light of this interpretation and the more general disagreement to date about the exact neuroanatomical boundaries of the DMN, we felt it was important to include Barnett's MTL network in our mask. Using Barnett et al.’s (2021) mask rather than a mask of the extended core network from our data (i.e., negatively weighted regions now shown in Appendix 4<bold>—</bold>figure 1) helped to avoid circularity.</p><p>The results of this ancillary non-rotated PLS were comparable to those from the main text. To further highlight the importance of the core memory network in determining the relation between memory conditions, we now display brain scores overlayed on Barnett et al.’s (2021) mask in Appendices 2 and 3 Figure<bold>—</bold>1, Appendix 4<bold>—</bold>Figure 1. The regions associated with a linear increase and showing increased activity for repeated/unique events than general/autobiographical facts predominantly fall within Barnett et al.’s (2021) networks. Altogether our data are consistent primarily with a quantitative rather than qualitative difference between memory conditions, that is, with mostly the same regions being engaged but to a different extent. Nevertheless, our data do not exclude the contribution of other regions. Regions associated with less decreased activity for general/autobiographical facts than repeated/unique events (i.e., negative saliences in Table 4) fall largely outside of the extended core network shown in Appendix 4<bold>—</bold>figure 1 and Barnett et al.’s network: the right frontal pole, the right inferior frontal gyrus, the right lateral occipital cortex, the right supramarginal gyrus, and the left occipital cortex. We nuanced the conclusions accordingly.</p><p>We added the following footnote in the discussion: “We obtained comparable non-rotated PLS results while including only voxels within the default mode and medial temporal networks from Barnett et al. (2021): 49.56% crossblock covariance (<italic>p</italic> &lt; .001) for the linear contrast, and 50.44% crossblock covariance (<italic>p</italic> &lt; .001). Within the selected networks, the same regions contributed to dissociate the memory conditions, and temporal brain scores peaked at lag 7. This supplementary analysis reinforces the importance of regions within the core memory network to determine the relation between memory conditions (see Appendices 2 and 3, figure 1), even though we found additional contributors at the whole brain level.”</p><disp-quote content-type="editor-comment"><p>7. As stated in the public review, I think this manuscript has the potential to be of interest to many researchers in the cognitive neuroscience of memory. In addition to the issues raised in the public review, I think the design of the neuroimaging experiment allows for supplementary analyses that could produce results that corroborate the main findings. If I understand the design correctly, only the &quot;Yes&quot; responses were included in the fMRI analyses and the &quot;No&quot; responses were discarded. I wonder if there is an opportunity to directly contrast the Yes and No responses for each condition, as presumably, these should isolate the neural correlates associated with each memory condition and there should be notable differences in those correlates for each memory condition (e.g. greater hippocampal responses in the unique event Yes &gt; No contrast relative to the general facts Yes &gt; No contrast).</p></disp-quote><p>We weighed the need to simplify and reduce to avoid “a sea of reporting” with the benefits of including ‘no’ responses in the analyses. The inclusion of ‘no’ diverges substantially from planned analyses and from the approach previously taken in Renoult et al. (2016). Our approach follows many memory researchers who retain only responses for which a memory trace is presumably present (e.g., Addis et al., 2011; Burianova and Grady, 2007).</p><p>The interpretation of ‘no’ responses remains ambiguous. For example, Maguire et al. (2001) explored the thought processes associated with ‘false’ (akin to ‘no’) responses in a pilot study about autobiographical memory, public events and general knowledge. The ‘false’ statements elicited recollection much like ‘true’ statements (Maguire et al., 2001). The literature on recall-to-reject further cautions that recollection is not specific to the recollection of a true event. In Maguire et al. (2001), the ‘false’ statements were modifications of ‘true’ statements, and so were associated with the recollection of elements associated with the ‘true’ statements. In our case, the statements could have additionally produced a mixture of absence of recall, and poor subjective and/or objective recollection of events. For example, for the statement “Yesterday evening, I took a shower”, a person could: 1) not recall anything, 2) know they didn’t or think about a relevant personal semantics (e.g., I always take showers in the morning, it’s always sunny when I take a shower), 3) think they may have taken a shower but be unsure about the timing or other aspects of the event; 4) recall going to the gym in the morning and taking a shower right after (i.e., recall a related event), or 5) recollect an outing at the movie theatre that fully occupied the evening (i.e., recall an unrelated event). Even more problematic for our purpose (i.e., contrasting the four memory conditions), the meaning of ‘no’ response could vary based on the condition. A ‘no’ response for Repeated events could mean the event never happened or it happened once but not repeatedly, for example. Subsequent research could take on the endeavour of contrasting “yes” and “no” responses, equipped with clear hypotheses about the differences between yes and no responses, and how that could alter the relation across memory conditions. In such a study, one might wish to equate the number of ‘yes’ and ‘no’ responses within each condition, measure subjective memory (e.g., confidence, vividness) after each response, and use alternative designs to account for the contribution of a retrieval mode.</p><p>In sum, we opt to not analyze ‘no’ responses to closely follow planned analyses, extending prior findings with ERPs now with fMRI (Renoult et al., 2016), and we retain only responses that generate greater interpretative clarity to illuminate the relation between memory conditions.</p><p>We added in the methods: “We retained only yes responses like Renoult et al. (2016) because they presumably reflect access to information consistent with the memory condition.”</p><disp-quote content-type="editor-comment"><p>9. I think the addition of the large volume of results in the appendix section is commendable in some respects, I am skeptical if a reader will actually consume the large volume of data in the tables. Perhaps linking to the overlay nifti files hosted on NeuroVault may be more appropriate?</p></disp-quote><p>We now make available the thresholded (80 voxels cluster) and unthresholded nifti files with bootstrap ratios for all lags on osf.io. Accordingly, we simplified all the tables to include only the Harvard-Oxford atlas (because of its importance in the in-text summary) for lag 7 and for additional lags in the appendices (i.e., Supplementary information).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This study uses advanced fMRI methods to address important and theoretically-motivated questions about memory retrieval as it occurs in the real world. Central to this study is the notion that, when studying how people recall information for their lives, we can understand a potential intermediary form of memory that exists between episodic and semantic memory: personal semantics. The main study was an fMRI study in which participants responded to questions that involved accessing information from episodic memories, two forms of personal semantics – repeated events and autobiographical knowledge – as well as semantic memory. A parallel behavioural experiment on a different sample of participants was conducted to examine how these forms of information differ with respect to subjective ratings.</p><p>A major strength of this paper is that it questions the classic division between episodic and semantic memory, proposing alternate views for how to consider retrieval from declarative memory. The use of PLS to analyze the data was a smart choice, albeit there are some considerations for these analyses. It is unfortunate that the neuroimaging and behavioural experiments were not collected on the sample. It might be useful to consider how the behavioural results can inform the neuroimaging experiment rather than considering the behavioural experiment solo.</p><p>I have some suggestions to help strengthen the manuscript, which are listed below in order of presentation rather than importance.</p><p>1. The introduction reviews all the key pieces of literature. When noting the overlap between episodic and semantic memory wrt the hippocampus, you may wish to include literature that notes how dementias characterised by semantic or episodic memory loss do have overlapping atrophy, particularly in the anterior hippocampus</p><p>Chapleau, M., Aldebert, J., Montembeault, M., and Brambati, S. M. (2016). Atrophy in Alzheimer's Disease and Semantic Dementia: An ALE Meta-Analysis of Voxel-Based Morphometry Studies. Journal of Alzheimer's disease: JAD, 54(3), 941-955. https://doi.org/10.3233/JAD-160382</p></disp-quote><p>Thank you for suggesting the addition of this highly relevant paper. We added a sentence and the reference in the introduction. “Anterior hippocampal atrophy features in neurodegenerative diseases affecting semantic and episodic memory alike (Chapleau et al., 2016).”</p><disp-quote content-type="editor-comment"><p>2. In the introduction. I think the rationale for the study could be strengthened by explicitly stating how the resulting work with contribute to new memory theories and extend from the reviewed literature. Simply said – note exactly what knowledge gap is being filled.</p></disp-quote><p>In the introduction, we justified the novelty of our approach through these points:</p><p>– Inferences about the differences between memory types rely predominantly on data from distinct studies or indirect comparisons (e.g., studies of semantic memory and of episodic memory).</p><p>– Further, even in direct comparisons, qualitative gaps exist between the typical conditions used to measure episodic versus semantic retrieval (e.g., gaps in task difficulty, reaction time [RT], and executive demands). Our study used stimuli that were closely matched across conditions and produced similar RTs across conditions in a prior ERP study (Renoult et al., 2016).</p><p>– Personal semantics is an understudied form of memory, despite evidence that it appears to be the more common for autobiographical memory elicited in free and cued recall (Barsalou, 1988) and in brain stimulation studies (Curot et al., 2017).</p><p>– Our review and taxonomy of personal semantics (Renoult et al., 2012) has generated quite a bit of interest (432 citations since) but very few empirical studies have investigated the neural correlates of the various types of personal semantics. Rarely, if ever, have two types of personal semantics been directly compared to both semantic and episodic memory. Their inclusion within a single study is key to understanding the neural substrates of declarative memory.</p><p>To address your comment, we added this paragraph to the introduction to clearly state the gap of knowledge: “In this study, we aimed to go beyond dichotomies commonly used in memory research (e.g., semantic vs. episodic, anterior vs. posterior brain regions, different vs. identical) to examine the multidimensional and complex relations across the spectrum in declarative memory, and importantly do so through direct comparisons. Our operationalization captures the prototypical definitions of several memory types, in close alignment with a taxonomy of personal semantics (Renoult et al., 2012), and possible characteristic function in daily life. Additionally, the analyses aim to uncover patterns that could act like heuristics to characterize declarative memory function. Critically, however, our additional focus on component processes in the behavioural study (i.e., amount of visual details, ability to evoke a scene, self-relevance) relies on the theoretical perspective that relations between memory types can be explained through the information accessed and cognitive processes engaged. Thus, it is implicit to our approach that the relations across memory types are not rigid and could be altered depending on task or personal goal (e.g., Grilli and Verfaellie, 2016). This study seeks to develop a framework suitable to bridge the divide in research about semantic and episodic aspects in declarative memory, and offers a complementary approach to explore the multiplicity of factors that coalescence to define the mnesic experience.”</p><disp-quote content-type="editor-comment"><p>3. In terms of the methods, the choice to block the conditions within each run raises the question of whether the results are due to differences in the content being accessed (semantic or episodic) or retrieval state (retrieval mode, in the words of Tulving) effects. Could the authors speak to this? (I come back to this with some suggestions for the discussion).</p></disp-quote><p>Thank you for this comment. The different types of cues used in our experiment indeed were used to trigger different “retrieval modes” in our participants, “a necessary condition for retrieval” (Tulving, 1983, page 169) that is maintained as a tonic state during a retrieval task (Rugg and Wilding, 2000). The behavioral data confirmed that this manipulation was successful in inducing typical phenomenological experience associated with these memory types. The self-relevance of the content being accessed was rated lowest for general facts compared to all personal forms of memory, and lower for personal semantics (i.e., autobiographical facts and repeated events) than unique events. Additionally, the amount of visual details retrieved increased from general facts to autobiographical facts to repeated events to unique events. Lastly, the four memory conditions differed in the proportion of scenes that came to mind during retrieval. A smaller proportion of general facts and autobiographical facts were categorized as scenes compared to repeated events and unique events, which did not differ in scene responses.</p><p>We added this statement in the discussion to clarify this distinction: “The different types of cues used in our experiment were used to trigger different “retrieval modes” in our participants, “a necessary condition for retrieval” (Tulving, 1983, p. 169) that is maintained as a tonic state during a retrieval task (Rugg and Wilding, 2000). The behavioral data revealed the sentence cues induced typical phenomenological experience associated with these memory types. Self-relevance was rated lowest for general facts compared to all personal forms of memory, and lower for personal semantics (i.e., autobiographical facts and repeated events) than unique events. Additionally, the amount of visual details increased from general facts to autobiographical facts to repeated events to unique events. This is consistent with previous studies. For example, as compared to unique events, repeated events are generally remembered less vividly: they are associated with reduced temporal specificity, personal significance, emotionality, and amount of details (e.g., Addis, Moscovitch, et al., 2004; Holland et al., 2011; Levine et al., 2004). Lastly, the four memory conditions differed in the proportion of scenes that came to mind during retrieval. A smaller proportion of general facts and autobiographical facts were categorized as scenes compared to repeated events and unique events, which did not differ in scene responses.</p><disp-quote content-type="editor-comment"><p>4. For the analysis, the authors note that the PLS analysis began at cue onset, and they estimated activity for 12 TR (14.4. seconds). What was the choice to analyze at the cue onset rather than the question? It seems to me that including the BOLD response associated with the cue means that your signal included both activities related to that cue, which is different across the conditions (some are temporal cues, some are activity cues, and some reflect people), as well as activity in accessing the intended information (a fact or event).</p></disp-quote><p>PLS does not make assumptions about the shape and timing of the hemodynamic response function (McIntosh et al. 2004). Therefore, for PLS analyses, it is recommended to include a large time-window to capture the entirety of the BOLD response (McIntosh et al. 2004). In our case, starting at cue onset signifies that the 12 TRs cover predominantly the cue, sentence and response periods. The percent signal change (see Figure 5) suggests the 12 TRs adequately encompass the rise, peak, and recovery of the BOLD response. Importantly, spatiotemporal PLS alleviates concern in interpretability as the temporal brain scores show the periods that contribute maximally to dissociate the memory conditions as set up in the contrasts, that is, in our case from TR 5 to TR 9 with the peak at TR 7. The peak occurs late in the epoch suggesting the cue period does not drive the findings. We think that the addition of a percent signal change figure (based on comment R1.4) significantly facilitates the interpretation of our findings.</p><disp-quote content-type="editor-comment"><p>5. I also found that the stimuli for the general facts condition seem to reflect opinions (Few people take pictures) rather than facts (The Queen of England is dead). Can the authors speak to this?</p></disp-quote><p>To begin, the General “facts” condition is reflective of semantic memory and its name mimics Autobiographical “facts”, much like Unique events is reflective of episodic memory and its name mirrors Repeated events. Several considerations motivated the operationalization of the General facts condition. First, the stimuli differed minimally in wording across conditions, being adjusted only in self-reference (i.e., referring to the self in all conditions but general facts) and temporal specificity (general for both types of facts, somewhat more specific for repeated events, and very specific for unique events). The general facts condition was thus operationalized as concerning knowledge of people in general (versus the self for the other conditions) and what they generally do (versus what they do at specific times). In that respect, the General facts condition takes the typical operationalization of general semantic memory as reflecting general knowledge of the world. For example, in the case of a “Few people take pictures”, a person could think of devices used to take pictures, occasions when people take pictures, and who usually takes pictures (e.g., parents, tourists).</p><p>According to the dictionary definition, opinions are “not based on evidence or ideas that everyone agrees on”, whereas facts are “undeniably true” (Antidote 9, Druide Informatique). To further match General facts with the other memory conditions, we did not manipulate the level of confidence or indisputability attached to statements. All conditions had some statements for which a “yes” response was more frequent (though “yes” was purposefully never unanimous), and some for which a “yes” response was less frequent. As you indicate, subjective feelings apply equally in semantic and episodic memory (cf. Mazancieux et al., 2020), and subjective feelings of confidence (for example) as well as accuracy may vary across statements within a condition, and this for all conditions. We did not consider accuracy because even if this were feasible for General facts statements, it would not be for the autobiographical memory conditions. Regardless of a statement’s truth in the absolute sense, we would expect a “yes” response to a statement to reflect a retrieval mode and access to information consistent with the memory condition.</p><p>Lastly, in future investigations one could consider comparing knowledge of the self to specific individuals (e.g., as you suggest “The Queen of England is dead”), which would certainly be interesting too to evaluate a more specific aspect of semantic memory, but may be associated with other challenges (e.g., either systematically comparing the self to a specific individual, limiting generalizability, or thinking about a different individual in different trials, which would add a task-switching element).</p><p>We added these sentences to the discussion: “The general facts condition was thus operationalized as concerning knowledge of people in general (versus the self for the other conditions) and what they generally do (versus what they do at specific times). In that respect, the general facts condition takes the typical operationalization of general semantic memory as reflecting general knowledge of the world. In future investigations, one could consider comparing knowledge of the self to knowledge of specific individuals, which would allow to evaluate a more specific aspect of semantic memory, but may be associated with other challenges (e.g., either systematically comparing the self to a specific individual, limiting generalizability, or thinking about a different individual in different trials, which would add a task-switching element).”</p><p>We edited the methods to include these statements: “We retained only yes responses like Renoult et al. (2016) because they presumably reflect access to information consistent with the memory condition. Memory accuracy is often difficult to assess for autobiographical memory (Cabeza and St Jacques, 2007), and so was not considered for any of the conditions.”</p><disp-quote content-type="editor-comment"><p>6. The PLS analysis also seems to be focused on yes responses, however, from Appendix 6 it seems like this means that between.6 and.4 proportion of the trials are not being analyzed. Would it not be useful for the authors in the trials by yes or no responses to see how ts drove neural activity?</p></disp-quote><p>Reviewer 1 raised this point (Comment R1.7). “We weighed the need to simplify and reduce to avoid “a sea of reporting” with the benefits of including ‘no’ responses in the analyses. The inclusion of ‘no’ diverges substantially from planned analyses and from the approach previously taken in Renoult et al. (2016). Our approach follows many memory researchers who retain only responses for which a memory trace is presumably present (e.g., Addis et al., 2011; Burianova and Grady, 2007).</p><p>The interpretation of ‘no’ responses remains ambiguous. For example, Maguire et al. (2001) explored the thought processes associated with ‘false’ (akin to ‘no’) responses in a pilot study about autobiographical memory, public events and general knowledge. The ‘false’ statements elicited recollection much like ‘true’ statements (Maguire et al., 2001). The literature on recall-to-reject further cautions that recollection is not specific to the recollection of a true event. In Maguire et al. (2001), the ‘false’ statements were modifications of ‘true’ statements, and so were associated with the recollection of elements associated with the ‘true’ statements. In our case, the statements could have additionally produced a mixture of absence of recall, and poor subjective and/or objective recollection of events. For example, for the statement “Yesterday evening, I took a shower”, a person could: 1) not recall anything, 2) know they didn’t or think about a relevant personal semantics (e.g., I always take showers in the morning, it’s always sunny when I take a shower), 3) think they may have taken a shower but be unsure about the timing or other aspects of the event; 4) recall going to the gym in the morning and taking a shower right after (i.e., recall a related event), or 5) recollect an outing at the movie theatre that fully occupied the evening (i.e., recall an unrelated event). Even more problematic for our purpose (i.e., contrasting the four memory conditions), the meaning of ‘no’ response could vary based on the condition. A ‘no’ response for Repeated events could mean the event never happened or it happened once but not repeatedly, for example. Subsequent research could take on the endeavour of contrasting “yes” and “no” responses, equipped with clear hypotheses about the differences between yes and no responses, and how that could alter the relation across memory conditions. In such a study, one might wish to equate the number of ‘yes’ and ‘no’ responses within each condition, measure subjective memory (e.g., confidence, vividness) after each response, and use alternative designs to account for the contribution of a retrieval mode.</p><p>In sum, we opt to not analyze ‘no’ responses to closely follow planned analyses, extending prior findings with ERPs now with fMRI (Renoult et al., 2016), and we retain only responses that generate greater interpretative clarity to illuminate the relation between memory conditions.</p><p>We added in the methods: “We retained only yes responses like Renoult et al. (2016) because they presumably reflect access to information consistent with the memory condition.”</p><disp-quote content-type="editor-comment"><p>7. In terms of the control task, the authors note why they included a control task e (page 7, line 11), but I would like to know why the authors selected the odd/even task. That is, what processes are being controlled for with this task? It seems to me that it would be general internally directed attention vs externally directed attention rather than specifying a memory network, per se. Does the choice of this task alter the pattern pulled out by PLS?</p></disp-quote><p>We concur with you and the authors that inspired the original design of the study (Foster et al., 2012) that mathematical operations or judgements on numbers involve “external attention” (Foster et al., 2012, p. 15514), whereas memory judgements are internally focused. Several studies on autobiographical memory and future thinking use odd/even judgements (e.g., Parlar et al., 2018; Svoboda and Levine, 2009; Thakral et al., 2020) as memory-related cognition functions permeate internally-focused cognition and a contrast with an “odd/even” task helps to reveal the memory-related brain activity (Stark and Squire, 2001). We modelled our approach of an odd/even task during the interstimulus interval after Madore et al. (2016). Thus, the control condition facilitated the comparison between memory conditions at minimal cost to scan time due to its insertion within the interstimulus interval. We added a justification for the odd and even task in the methods: “An odd/even task is frequently used in autobiographical memory and future thinking research (Parlar et al., 2018; Svoboda and Levine, 2009; Thakral et al., 2020) to reveal the core memory network (Stark and Squire, 2001).”</p><disp-quote content-type="editor-comment"><p>8. In terms of results, from Figure 1 it also seems that the negative brain pattern was being represented more significantly by RE and UE than GF or AF (as the CI for the brain scores does not overlap) – is this correct? If so, this would suggest to me that there is more dissociation between facts vs events than commonalities.</p></disp-quote><p>This mean-centered PLS showed that the four memory conditions engaged a common set of regions when compared to the odd/even task. You are correct that some memory conditions appear to express that network more than others. This is conceptually similar to the findings from the second LV from that PLS analysis (described briefly in text), the non-rotated PLS now reported in the main text, and was previously shown in the mean-centered PLS with the memory conditions only. The direct juxtaposition of the linear and facts vs. events contrasts in the non-rotated PLS along with the percent signal change in Figure 5 help to clarify the extent of similarities and differences across memory conditions.</p><p>We clarified the interpretation in the discussion:</p><p>“ In contrast, the unique neural correlates were evident when examining the four memory conditions on their own. The non-rotated PLS converges with the data-driven PLS (see Appendix 4) to suggest the facts vs. events contrast dominates to explain the spatiotemporal relations across memory conditions, although the difference in covariance explained between the two LVs of the non-rotated PLS was slight. In fact, both a-priori contrasts captured aspects of the spatiotemporal relations adequately. The percentage of signal change (see Figure 5) illustrates the complementarity of the two perspectives to encapsulate the relation between memory conditions. That is, activity increased (or decreased) continuously across memory types, but the extent of the increase (or decrease) confers greater similarity between general and autobiographical facts, and between repeated and unique events. Thus, several regions showed a relatively small increase in activity from general facts to autobiographical facts, a relatively large increase from autobiographical facts to repeated events, and a relatively small increase from repeated events to unique events; these include the precuneus, posterior cingulate, angular gyrus and middle frontal gyrus bilaterally, and left parahippocampal gyrus, left hippocampus, and left middle/superior temporal gyrus (see Appendices 2 and 3, figure 1). Activity instead decreased in a commensurate manner predominantly in the right hemisphere, particularly the frontal pole, inferior frontal gyrus, and supramarginal cortex. These findings are compatible with a continuum perspective of declarative memory, because quantitative rather than qualitative variations in brain activity suffice to characterize the relation between these memory types (Renoult et al., 2012).”</p><disp-quote content-type="editor-comment"><p>9. In how the results are presented, I had a hard time integrating the behavioural and neuroimaging data, perhaps the paper would benefit from presenting the behavioural study first to show the distinctions in the memory cues and then the neuroimaging data to examine neural mechanisms.</p></disp-quote><p>We present the behavioural results first as suggested. We also revised the results and discussion to better integrate behaviour and neuroimaging results throughout the manuscript. Notably:</p><p>In the Results section: “We used a non-rotated PLS to test two theoretically plausible relations between the four memory conditions: a linear contrast (-3, -1, 1, 3) and one comparing general/autobiographical facts and repeated/unique events (-1, -1, 1, 1; abbreviated to facts vs. events subsequently). A linear contrast would be consistent with the continuum perspective of personal semantics (see Box 3; Renoult et al., 2012), which would predict an increase in activity from general facts to autobiographical facts, from autobiographical facts to repeated events, and from repeated events to unique events. The increase in visual details (described above) followed precisely this pattern. Similarly, personal relevance increased from general facts to personal semantics (autobiographical facts and repeated events) to unique events, suggesting similar dynamics between component processes (e.g., contextual specificity may increase along with personal relevance). The facts vs. events contrast would favour the view of personal semantics as a subtype of general semantics (Box 2; Renoult et al., 2012). Of all personal semantics, autobiographical facts corresponds best with this view due to its abstraction from events, its more objective quality than other forms of personal knowledge (e.g., trait knowledge), and the feeling of “knowing” the “facts” rather than recollecting events (i.e., “noetic” consciousness; Tulving, 2002). Repeated events would instead group with unique events as “event memory” due to the common construction of a scene (Rubin and Umanath, 2015). Indeed, participants in the behavioural study perceived scenes as frequently for repeated and unique events. However, it is less clear how one would accommodate the greater number of scenes evoked for autobiographical than general facts in a way that aligns with that perspective. Thus, the conjunction of the visualization of scenes, amount of visual details, and personal relevance agrees most with a continuum of contextual specificity.”</p><p>In the discussion: “The importance of situational or contextual elements featured strongly in the neuroimaging data as in the behavioural data. The core network, also known as the default mode network, has been subdivided into an anterior temporal network linked to “entities” (Ranganath and Ritchey, 2012; Reagh and Ranganath, 2018) or “conceptual remembering” (Sheldon et al., 2019) and a posterior medial network linked to “situational models” (Ranganath and Ritchey, 2012; Reagh and Ranganath, 2018) or “perceptual remembering” (Sheldon et al., 2019). Although these networks process different kinds of information, neither is strictly dedicated to semantic or episodic memory (Reagh and Ranganath, 2018; Sheldon et al., 2019). For instance, knowledge can facilitate the search and construction of events (Irish and Piguet, 2013) and semantic memory can integrate contextual information (e.g., Greenberg et al., 2009; Sheldon and Moscovitch, 2012). Accordingly, in our study the dissociation between general/autobiographical facts and repeated/unique events did not have a clear posterior medial to anterior temporal demarcation (i.e., posterior medial activity for events and anterior temporal activity for facts). Instead, regions primarily within the posterior medial network (i.e., angular gyrus, posterior cingulate gyrus, precuneus, and parahippocampal gyrus; Ritchey et al., 2015) dissociated memory conditions, whereas those within the anterior temporal network (i.e., frontal orbital cortex, inferior anterior temporal gyrus, temporal pole, bilateral amygdala, and perirhinal cortex; Ritchey et al., 2015) did not. Indeed, events contained greater contextual information, as suggested by the increased proportion of scenes they evoked, as compared to factual memories. Further, cues were more temporally specific for repeated/unique events than general/autobiographical facts. Consistent with this, many regions of the posterior medial network were associated with greater activity for repeated/unique events than general/autobiographical facts (similar to Ford et al., 2011; Levine et al., 2004; Maguire and Mummery, 1999), and showed a linear increase from the most general type of memory (i.e., general facts) to the most specific memory (i.e., unique events). Therefore, activity in regions associated with visuospatial processing (e.g., precuneus) and scenes (e.g., medial temporal regions) coheres with behavioural data to support the prominence of contextual specificity in determining the relation across memory types. Activity in medial frontal regions is in harmony with ratings of visual details and scene perception, likewise increasing along a continuum of contextual specificity (see Figure 5d). However, this anterior portion of the medial frontal cortex may correspond best to self-processing rather than “situational” processing or mental time travel (Lieberman et al., 2019). If activity in the medial frontal cortex in our study reflected exclusively self-processing, one would expect a greater proximity between autobiographical facts and repeated events on the basis of subjective ratings of self-relevance. The additional concordance of medial frontal cortex with a continuum of contextual specificity could be a corollary of the strong links between aspects of self-relevance and episodic simulation (Tulving et al., 2002; King et al., 2022; Grysman et al., 2013; Verfaellie et al., 2019), in addition to this region’s role in modulating recollection (McCormick et al., 2020), for example through engaging schema-related information (Gilboa and Marlatte, 2017).”</p><disp-quote content-type="editor-comment"><p>10. The authors introduce the idea of the results reflecting the engagement of the DMN and different subsystems, but their results present patterns of activity that are not constrained to these networks or reflect connectivity, a better metric of network engagement. Perhaps the authors could constrain the PLS analysis to the DMN network so that they map their findings to the reviewed work about DMN contributions to autobiographical memory.</p></disp-quote><p>Your observation aligns with Comment R1.6. This was our response:</p><p>“In response to this comment, we repeated the non-rotated PLS using a mask of the default mode network (DMN; anterior temporal, medial prefrontal, posterior medial) and medial temporal network from Barnett et al. (2021) to test whether these regions would suffice to dissociate the memory conditions. Note that Barnett et al., conceptualize an MTL network as dissociable from the DMN, however the authors reference this subnetwork as a critical DMN bridge between DMN and the hippocampus and other networks. In light of this interpretation and the more general disagreement to date about the exact neuroanatomical boundaries of the DMN, we felt it was important to include Barnett's MTL network in our mask. Using Barnett et al.’s (2021) mask rather than a mask of the extended core network from our data (i.e., negatively weighted regions now shown in Appendix 4<bold>—</bold>figure 1) helped to avoid circularity.</p><p>The results of this ancillary non-rotated PLS were comparable to those from the main text. To further highlight the importance of the core memory network in determining the relation between memory conditions, we now display brain scores overlayed on Barnett et al.’s (2021) mask in Appendices 2 and 3 Figure<bold>—</bold>1, Appendix 4<bold>—</bold>Figure 1. The regions associated with a linear increase and showing increased activity for repeated/unique events than general/autobiographical facts predominantly fall within Barnett et al.’s (2021) networks. Altogether our data are consistent primarily with a quantitative rather than qualitative difference between memory conditions, that is, with mostly the same regions being engaged but to a different extent. Nevertheless, our data do not exclude the contribution of other regions. Regions associated with less decreased activity for general/autobiographical facts than repeated/unique events (i.e., negative saliences in Table 4) fall largely outside of the extended core network shown in Appendix 4<bold>—</bold>figure 1 and Barnett et al.’s network: the right frontal pole, the right inferior frontal gyrus, the right lateral occipital cortex, the right supramarginal gyrus, and the left occipital cortex. We nuanced the conclusions accordingly.</p><p>We added the following footnote in the discussion: “We obtained comparable non-rotated PLS results while including only voxels within the default mode and medial temporal networks from Barnett et al. (2021): 49.56% crossblock covariance (<italic>p</italic> &lt;.001) for the linear contrast, and 50.44% crossblock covariance (<italic>p</italic> &lt;.001). Within the selected networks, the same regions contributed to dissociate the memory conditions, and temporal brain scores peaked at lag 7. This supplementary analysis reinforces the importance of regions within the core memory network to determine the relation between memory conditions (see Appendices 2 and 3, figure 1), even though we found additional contributors at the whole brain level.”</p><disp-quote content-type="editor-comment"><p>11. I also think it would be great to focus the discussion on specific theories that fall within a component process framework to suggest why the resulting patterns emerged, such as theories that result reflect accessing context dependent vs independent information (Grilli et al.,) or different levels of consciousness (Tulving). In fact, I was curious if the authors thought that the distinctions are because different content is being accessed or the same content at different levels of representation. Both have relevance to theories on accessing information from memory that are noted. For example, the PMAT model discussing distinctions in content versus the TTT (Gilboa and Moscovitch) would note distinctions in representation, at least that is my understanding.</p></disp-quote><p>The emphasis on the component process view in the conclusion is compatible with numerous perspectives and theories as listed here: “<italic>Some</italic> of the characteristics that would influence differences in hippocampal activity and other regions of the core memory network include: the number of details (Thakral et al., 2020), their association (Duff et al., 2020; Solomon and Schapiro, 2020), their integration within a scene (Nadel, 1991; Robin, 2018; Robin et al., 2016; Rubin and Umanath, 2015) or within a situational model (Reagh and Ranganath, 2018; Summerfield et al., 2010), their coarseness and precision (Craik, 2020; Ekstrom and Yonelinas, 2020), their type and modality (e.g., perceptual, spatial, temporal, social; Binder and Desai, 2011; Grilli and Verfaellie, 2016; Sheldon et al., 2019), their stability (Auger and Maguire, 2018), as well as their projection into a temporally-distant time (Andrews-Hanna et al., 2010), the open-endedness of the representation (Sheldon and Moscovitch, 2012), the demands on pattern separation to construct unique representations or identify distinguishing features (Rolls and Kesner, 2006; Schapiro et al., 2017), and the likelihood of eliciting a specific event (Renoult et al., 2015; Westmacott et al., 2004; Westmacott and Moscovitch, 2003).” Future research will be necessary to better delineate which account(s) best explains relations between memory conditions.</p><p>Nevertheless, we clarify our perspective by spelling out the assumption of a “neural-psychological representation correspondence” (Moscovitch and Gilboa, 2022) that integrates elements of consciousness as additional “components” (e.g., the feeling of the self travelling in time; Tulving, 2002).</p><p>The question of the distinction between &quot;different content” and &quot;different levels of representation of the same content&quot; is a difficult one and prone to different answers depending on how one would operationalise the notion of “content”. In our experiment, even though the sentence clauses referred to the same content across conditions (e.g., watering plants), the representational content of the elicited memories would not have been identical due to the use of the different cues, encouraging participants to process the stimuli with different levels of temporal specificity and personal relevance (e.g., Last week-end, I watered a plant versus Most people water plants).</p><p>In the discussion: “What underlies this overlap in the neural substrates of semantic and episodic memory? A parsimonious explanation is that semantic and episodic memory rely on similar elementary component processes (Cabeza and Moscovitch, 2013; Larsen, 1992; Moscovitch, 1992; Renoult et al., 2012; Rubin, 2021). All types of memories would depend on a similar network of brain regions but with different weighting of certain nodes in the network. The identification of the relative contribution of different component processes is a critical next step. <italic>Some</italic> of the characteristics that would influence differences in hippocampal activity and other regions of the core memory network include: the number of details (Thakral et al., 2020), their association (Duff et al., 2020; Solomon and Schapiro, 2020), their integration within a scene (Nadel, 1991; Robin, 2018; Robin et al., 2016; Rubin and Umanath, 2015) or within a situational model (Reagh and Ranganath, 2018; Summerfield et al., 2010), their coarseness and precision (Craik, 2020; Ekstrom and Yonelinas, 2020), their type and modality (e.g., perceptual, spatial, temporal, social; Binder and Desai, 2011; Grilli and Verfaellie, 2016; Sheldon et al., 2019), their stability (Auger and Maguire, 2018), as well as their projection into a temporally-distant time (Andrews-Hanna et al., 2010), the open-endedness of the representation (Sheldon and Moscovitch, 2012), the demands on pattern separation to construct unique representations or identify distinguishing features (Rolls and Kesner, 2006; Schapiro et al., 2017), and the likelihood of eliciting a specific event (Renoult et al., 2015; Westmacott et al., 2004; Westmacott and Moscovitch, 2003). For instance, episodic memory would typically rely to a greater degree than semantic memory on rich sensory-perceptual imagery, complex situational models or scenes, spatial and temporal features, and self-reflection. Accordingly, instead of activating different networks of brain regions, semantic and episodic processes may activate a similar network but with different degrees of magnitude, or recruit these brain regions in a complementary manner (Sherman et al., 2023). How each component is involved would also depend on the task at hand (e.g., Gurguryan and Sheldon, 2019); each component could be more or less engaged regardless of the memory type (e.g., semantic details can be thought of in rich details, in relation to the self, or in relation to a spatial context). Therefore, there would be a “neural-psychological representation correspondence” (Moscovitch and Gilboa, 2022) that includes elements of consciousness (e.g., feeling of being transported in time; Tulving, 2002) and that transcends categories of memory.”</p><p>Other sections of the manuscript also discuss our findings in relation to theoretical perspectives from the literature. In the introduction: “Critically, however, our additional focus on component processes in the behavioural study (i.e., amount of visual details, ability to evoke a scene, self-relevance) relies on the theoretical perspective that relations between memory types can be explained through the information accessed and cognitive processes engaged. Thus, it is implicit to our approach that the relations across memory types are not rigid and could be altered depending on task or personal goal (e.g., Grilli and Verfaellie, 2016). This study seeks to develop a framework suitable to bridge the divide in research about semantic and episodic aspects in declarative memory, and offers a complementary approach to explore the multiplicity of factors that coalescence to define the mnesic experience.”</p><p>In the discussion: “The importance of situational or contextual elements featured strongly in the neuroimaging data as in the behavioural data. The core network, also known as the default mode network, has been subdivided into an anterior temporal network linked to “entities” (Ranganath and Ritchey, 2012; Reagh and Ranganath, 2018) or “conceptual remembering” (Sheldon et al., 2019) and a posterior medial network linked to “situational models” (Ranganath and Ritchey, 2012; Reagh and Ranganath, 2018) or “perceptual remembering” (Sheldon et al., 2019). Although these networks process different kinds of information, neither is strictly dedicated to semantic or episodic memory (Reagh and Ranganath, 2018; Sheldon et al., 2019). For instance, knowledge can facilitate the search and construction of events (Irish and Piguet, 2013) and semantic memory can integrate contextual information (e.g., Greenberg et al., 2009; Sheldon and Moscovitch, 2012). Accordingly, in our study the dissociation between general/autobiographical facts and repeated/unique events did not have a clear posterior medial to anterior temporal demarcation (i.e., posterior medial activity for events and anterior temporal activity for facts). Instead, regions primarily within the posterior medial network (i.e., angular gyrus, posterior cingulate gyrus, precuneus, and parahippocampal gyrus; Ritchey et al., 2015) dissociated memory conditions, whereas those within the anterior temporal network (i.e., frontal orbital cortex, inferior anterior temporal gyrus, temporal pole, bilateral amygdala, and perirhinal cortex; Ritchey et al., 2015) did not. Indeed, events contained greater contextual information, as suggested by the increased proportion of scenes they evoked, as compared to factual memories. Further, cues were more temporally specific for repeated/unique events than general/autobiographical facts. Consistent with this, many regions of the posterior medial network were associated with greater activity for repeated/unique events than general/autobiographical facts (similar to Ford et al., 2011; Levine et al., 2004; Maguire and Mummery, 1999), and showed a linear increase from the most general type of memory (i.e., general facts) to the most specific memory (i.e., unique events). Therefore, activity in regions associated with visuospatial processing (e.g., precuneus) and scenes (e.g., medial temporal regions) coheres with behavioural data to support the prominence of contextual specificity in determining the relation across memory types. Activity in medial frontal regions is in harmony with ratings of visual details and scene perception, likewise increasing along a continuum of contextual specificity (see Figure 5d). However, this anterior portion of the medial frontal cortex may correspond best to self-processing rather than “situational” processing or mental time travel (Lieberman et al., 2019). If activity in the medial frontal cortex in our study reflected exclusively self-processing, one would expect a greater proximity between autobiographical facts and repeated events on the basis of subjective ratings of self-relevance. The additional concordance of medial frontal cortex with a continuum of contextual specificity could be a corollary of the strong links between aspects of self-relevance and episodic simulation (Tulving et al., 2002; King et al., 2022; Grysman et al., 2013; Verfaellie et al., 2019), in addition to this region’s role in modulating recollection (McCormick et al., 2020), for example through engaging schema-related information (Gilboa and Marlatte, 2017).”</p></body></sub-article></article>