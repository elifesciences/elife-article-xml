<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">31640</article-id><article-id pub-id-type="doi">10.7554/eLife.31640</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neuronal populations in the occipital cortex of the blind synchronize to the temporal dynamics of speech</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-96886"><name><surname>Van Ackeren</surname><given-names>Markus Johannes</given-names></name><email>mvanackeren@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96887"><name><surname>Barbero</surname><given-names>Francesca M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4445-4402</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96888"><name><surname>Mattioni</surname><given-names>Stefania</given-names></name><email>stefania.mattioni@unitn.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96889"><name><surname>Bottini</surname><given-names>Roberto</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7941-7762</contrib-id><email>Roberto.Bottini@unitn.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-96236"><name><surname>Collignon</surname><given-names>Olivier</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1882-3550</contrib-id><email>olivier.collignon@uclouvain.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Center for Mind/Brain Studies</institution><institution>University of Trento</institution><addr-line><named-content content-type="city">Trento</named-content></addr-line><country>Italy</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Institute of research in Psychology</institution><institution>University of Louvain</institution><addr-line><named-content content-type="city">Louvain</named-content></addr-line><country>Belgium</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Institute of Neuroscience</institution><institution>University of Louvain</institution><addr-line><named-content content-type="city">Louvain</named-content></addr-line><country>Belgium</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-14601"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>01</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e31640</elocation-id><history><date date-type="received" iso-8601-date="2017-08-30"><day>30</day><month>08</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-01-16"><day>16</day><month>01</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Van Ackeren et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Van Ackeren et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-31640-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.31640.001</object-id><p>The occipital cortex of early blind individuals (EB) activates during speech processing, challenging the notion of a hard-wired neurobiology of language. But, at what stage of speech processing do occipital regions participate in EB? Here we demonstrate that parieto-occipital regions in EB enhance their synchronization to acoustic fluctuations in human speech in the theta-range (corresponding to syllabic rate), irrespective of speech intelligibility. Crucially, enhanced synchronization to the intelligibility of speech was selectively observed in primary visual cortex in EB, suggesting that this region is at the interface between speech perception and comprehension. Moreover, EB showed overall enhanced functional connectivity between temporal and occipital cortices that are sensitive to speech intelligibility and altered directionality when compared to the sighted group. These findings suggest that the occipital cortex of the blind adopts an architecture that allows the tracking of speech material, and therefore does not fully abstract from the reorganized sensory inputs it receives.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.31640.002</object-id><title>eLife digest</title><p>Scientists once thought that certain parts of the brain were hard-wired to process information from specific senses or to perform specific tasks. For example, some had concluded that language processing is built into certain parts of the brain, because the way the brain responds to language is remarkably similar in different people even from very early on in life. Yet other studies with individuals who were born blind emphasize that experience also shapes the way the brain works. In people who are born blind, parts of the brain that typically interpret visual information in sighted people are often put to other uses.</p><p>Now, van Ackeren et al. show that people who became blind early in life are able to repurpose parts of the brain that are more typically used for vision to understand spoken language instead. A technique called magnetoencephalography was used to map how different parts of the brain respond when both people with sight and those who are blind listen to recordings of someone talking. In some of the experiments, the speech was distorted, making it unintelligible. In both groups, areas of the brain known to process sound information showed patterns of activity that match the rhythms present in the speech. The group with blindness also showed similar activity in parts of the brain usually used to process visual information, and even more so when they were exposed to intelligible speech.</p><p>The experiments show that brain efficiently reshapes to adapt to a world with no visual input. It may do this by making use of connections that already exist between the auditory and visual brain centers. For instance, very young children use these connections to link what they hear to the lip movements of adults. Future studies are needed to determine if individuals whose ability to see is restored would be able to process the visual information or if the adaptation of the visual processing parts of the brain to help understand speech would interfere with their sight.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>blindness</kwd><kwd>neuroplasticity</kwd><kwd>crossmodal</kwd><kwd>language</kwd><kwd>speech-tracking</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>337573</award-id><principal-award-recipient><name><surname>Van Ackeren</surname><given-names>Markus Johannes</given-names></name><name><surname>Mattioni</surname><given-names>Stefania</given-names></name><name><surname>Bottini</surname><given-names>Roberto</given-names></name><name><surname>Collignon</surname><given-names>Olivier</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Blind people re-purpose the brain's visual areas for tracking speech rhythm.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The human cortex comprises a number of specialized units, functionally tuned to specific types of information. How this functional architecture emerges, persists, and develops throughout a person’s life are among the most challenging and exciting questions in neuroscience research. Although there is little debate that both genetic and environmental influences affect brain development, it is currently not known how these two factors shape the functional architecture of the cortex. A key topic in this debate is the organization of the human language system. Language is commonly thought to engage a well-known network of regions around the lateral sulcus. The consistency of this functional mapping across individuals and its presence early in development are remarkable, and often used to argue that the neurobiological organization of the human language system is the result of innate constraints (<xref ref-type="bibr" rid="bib35">Dehaene-Lambertz et al., 2006</xref>; <xref ref-type="bibr" rid="bib15">Berwick et al., 2013</xref>). Does the existence of a highly consistent set of regions for language acquisition and processing imply that this network is ‘hardwired’ and immutable to experience? Strong nativist theories for linguistic innateness leave little room for plasticity due to experience (<xref ref-type="bibr" rid="bib8">Bates, 1999</xref>), suggesting that we should conceive ‘the growth of language as analogous to the development of a bodily organ’ (<xref ref-type="bibr" rid="bib27">Chomsky, 1976</xref>, p.11). However, studies in infants born with extensive damage to cortical regions that are typically involved in language processing may develop normal language abilities, thereby demonstrating that the language network is subject to reorganization (<xref ref-type="bibr" rid="bib9">Bates, 2005</xref>). Perhaps the most intriguing demonstrations to show that the neurobiology of language is susceptible to change due to experience come from studies showing functional selectivity to language in primary and secondary ‘visual’ areas in congenitally blind individuals (<xref ref-type="bibr" rid="bib103">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="bib23">Burton, 2003</xref>; <xref ref-type="bibr" rid="bib3">Amedi et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="bib6">Arnaud et al., 2013</xref>). Such reorganization of the language network is particularly fascinating because it arises in the absence of injury to the core language network (<xref ref-type="bibr" rid="bib9">Bates, 2005</xref>; <xref ref-type="bibr" rid="bib7">Atilgan et al., 2017</xref>).</p><p>However, the level at which the occipital cortex is involved in speech representation in the early blind (EB), remains poorly understood. Speech comprehension requires that the brain extracts meaning from the acoustic features of sounds (<xref ref-type="bibr" rid="bib33">de Heer et al., 2017</xref>). Although several neuroimaging studies have yielded valuable insights about the processing of speech in EB adults (<xref ref-type="bibr" rid="bib6">Arnaud et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="bib24">Büchel, 2003</xref>; <xref ref-type="bibr" rid="bib62">Lane et al., 2015</xref>; <xref ref-type="bibr" rid="bib103">Röder et al., 2002</xref>) and infants (<xref ref-type="bibr" rid="bib11">Bedny et al., 2015</xref>), these methods do not adequately capture the fast and continuous nature of speech processing. Because speech unfolds over time, understanding spoken language relies on the ability to track the incoming acoustic signal in near real-time (<xref ref-type="bibr" rid="bib87">Peelle and Davis, 2012</xref>). Indeed, speech is a fluctuating acoustic signal that rhythmically excites neuronal populations in the brain (<xref ref-type="bibr" rid="bib92">Poeppel et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Peelle et al., 2013</xref>). Several studies have demonstrated that neuronal populations in auditory areas entrain to the acoustic fluctuations that are present in human speech around the syllabic rate (<xref ref-type="bibr" rid="bib70">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib53">Kayser et al., 2009</xref>; <xref ref-type="bibr" rid="bib114">Szymanski et al., 2011</xref>; <xref ref-type="bibr" rid="bib122">Zoefel and VanRullen, 2015</xref>). It has therefore been suggested that entrainment reflects a key mechanism underlying hearing by facilitating the parsing of individual syllables through adjusting the sensory gain relative to fluctuations in the acoustic energy (<xref ref-type="bibr" rid="bib44">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib87">Peelle and Davis, 2012</xref>; <xref ref-type="bibr" rid="bib37">Ding and Simon, 2014</xref>). Crucially, because some regions that track the specific acoustic rhythm of speech are sensitive to speech intelligibility, neural synchronization is not only driven by changes in the acoustic cue of the auditory stimuli, but also reflects cortical encoding and processing of the auditory signal (<xref ref-type="bibr" rid="bib88">Peelle et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Ding and Simon, 2014</xref>). Speech tracking is therefore an invaluable tool to probe regions that interface speech perception and comprehension (<xref ref-type="bibr" rid="bib92">Poeppel et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Peelle et al., 2013</xref>).</p><p>Does the occipital cortex of EB people synchronize to speech rhythm? Is this putative synchronization of neural activity to speech influenced by comprehension? Addressing these questions would provide novel insights into the functional organization of speech processing rooted in the occipital cortex of EB people. In the current study, we investigated whether neuronal populations in blind occipital cortex synchronize to rhythmic dynamics of speech, by relating the amplitude fluctuations in speech to electromagnetic dynamics recorded from the participant’s brain. To this end, we quantified the local brain activity and directed connectivity in a group of early blind (EB; n = 17) and sighted individuals (SI; n = 16) using magnetoencephalography (MEG) while participants listened to short narrations from audiobooks. If the occipital cortex of the blind entrains to speech rhythms, this will support the idea that this region processes low-level acoustic features relevant for understanding language. We further tested whether the putative synchronization of occipital responses in EB people benefits from linguistic information or only relates to acoustic information. To separate linguistic and acoustic processes, we relied on a noise-vocoding manipulation. This method spectrally distorted the speech signal in order to impair intelligibility gradually, but systematically preserves the slow amplitude fluctuations responsible for speech rhythm (<xref ref-type="bibr" rid="bib110">Shannon et al., 1995</xref>; <xref ref-type="bibr" rid="bib87">Peelle and Davis, 2012</xref>). Furthermore, to go beyond differences in the local encoding of speech rhythm in the occipital cortex, we also investigated whether the connectivity between occipital and temporal regions sensitive to speech comprehension is altered in early blindness.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Story comprehension</title><p>Participants listened to either natural speech segments (nat-condition) or to altered vocoded versions of these segments (see Materials and methods for details). In the 8-channel vocoded condition, the voice of the speaker is highly distorted but the intelligibility is unperturbed. By contrast, in the 1-channel vocoded condition, the speech is entirely unintelligible. After listening to each speech segment, participants were provided with a short statement about the segment, and asked to indicate whether the statement was true or false. Behavioral performance on these comprehension statements was analyzed using linear mixed-effects models with maximum likelihood estimation. This method is a linear regression that takes into account dependencies in the data, as present in repeated measures designs. Blindness and intelligibility were included as fixed effects, while subject was modeled as a random effect. Intelligibility was nested in subjects. Intelligibility had a significant effect on story comprehension (<italic>χ</italic>(2)=110.7, p&lt;0.001). The effect of blindness and the interaction between intelligibility and blindness were non-significant (<italic>χ</italic>(1)=1.14, p=0.286 and <italic>χ</italic>(2)=0.24, p=0.889). Orthogonal contrasts demonstrated that speech comprehension was stronger in the nat and 8-channel condition versus the 1-channel condition (b = 0.78, <italic>t</italic>(62)=14.43, p&lt;0.001, r = 0.88). There was no difference between the nat and the 8-channel condition (b = 0.02, <italic>t</italic>(62)=1.39, p=0.17). Thus, speech intelligibility was reduced in the 1-channel, but not the 8-channel vocoded condition (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The lack of effect for the factor blindness suggests that there is no evidence for a potential difference in comprehension, attention or motivation between groups. Descriptive statistics of the group, and condition means are depicted in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31640.003</object-id><label>Table 1.</label><caption><title>Proportion of correct responses for the story comprehension questions.</title><p>Depicted are the number of subjects (N) in the early blind (EB) and sighted individuals (SI) groups, the mean (M), and the standard error of the mean (SE) for each of the three conditions (Natural, 8-channel, 1-channel).</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th valign="top"/><th colspan="2">Natural</th><th colspan="2">8-channel</th><th colspan="2">1-channel</th></tr><tr><th>Group</th><th valign="top">N</th><th>M</th><th>SE</th><th>M</th><th>SE</th><th>M</th><th>SE</th></tr></thead><tbody><tr><td> EB</td><td valign="top">17</td><td>0.899</td><td>0.019</td><td>0.853</td><td>0.031</td><td>0.559</td><td>0.027</td></tr><tr><td> SI</td><td valign="top">15</td><td>0.914</td><td>0.020</td><td>0.890</td><td>0.025</td><td>0.576</td><td>0.034</td></tr></tbody></table></table-wrap><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31640.004</object-id><label>Figure 1.</label><caption><title>Cerebro-acoustic coherence in EB and SI.</title><p>(<bold>A</bold>) Segments from each condition (nat, 8-channel, and 1-channel) in the time domain (see also <xref ref-type="video" rid="video1">Video 1</xref>). Overlaid is the amplitude envelope of each condition. (<bold>B</bold>) Spectrogram of the speech samples from (<bold>A</bold>). The effect of spectral vocoding on the fine spectral detail in all three conditions. (<bold>C</bold>) The superimposed amplitude envelopes of the samples for each condition are highly similar, despite distortions in the fine spectral detail. (<bold>D</bold>) Behavioral performance on the comprehension statements reveals that comprehension is unperturbed in the nat and 8-channel conditions, whereas the 1-channel condition elicits chance performance. (<bold>E</bold>) Coherence spectrogram extracted from bilateral temporal sensors reveals a peak in cerebro-acoustic coherence at 7 Hz, across groups and conditions. The shaded area depicts the frequency range of interest in the current study. The topography shows the spatial extent of coherence values in the range 4–9 Hz. Enhanced coherence is observed selectively in bilateral temporal sensors. (<bold>F</bold>) Source reconstruction of the raw coherence confirms that cerebro-acoustic coherence is strongest in the vicinity of auditory cortex, bilaterally, extending into superior and middle temporal gyrus. (<bold>G</bold>) Statistically thresholded maps for the contrast between natural and 1-channel vocoded speech show an effect of intelligibility (p&lt;0.05, FWE-corrected) in right STG. (<bold>H</bold>) Enhanced envelope tracking is observed for EB versus SI in a bilateral parieto-occipital network along the medial wall centered on Precuneus (p&lt;0.05, FWE-corrected). (<bold>I</bold>) The statistical map shows the interaction effect between blindness and intelligibility: Early blind (EB) individuals show enhanced synchronization during intelligible (nat) versus non-intelligible speech (1-channel) as compared to SI in right calcarine sulcus (CS) (p&lt;0.005, uncorrected). (<bold>J</bold>) Boxplots for three regions identified in the whole-brain analysis (top panel, STG; middle panel, parieto-occipital cortex; bottom panel, calcarine sulcus).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31640-fig1-v3"/></fig></sec><sec id="s2-2"><title>Tracking intelligible speech in blind and sighted temporal cortex</title><p>The overall coherence spectrum, which highlights the relationship between the amplitude envelope of speech and the signal recorded from the brain, was maximal over temporal sensors between 6 Hz and 7 Hz (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). This first analysis was performed on the combined dataset, and hence is not prone to bias or circularity for subsequent analyses, targeting group differences. The peak in the current study is slightly higher than those reported in previous studies (<xref ref-type="bibr" rid="bib45">Gross et al., 2013</xref>). A likely explanation for this shift is the difference in syllabic rate between English (~6.2 Hz), used in previous studies, and Italian (~7 Hz) (<xref ref-type="bibr" rid="bib90">Pellegrino et al., 2011</xref>). The syllabic rate is the main carrier of amplitude fluctuations in speech, and thus is most prone to reset oscillatory activity.</p><p>To capture the temporal scale of cerebro-acoustic coherence effects (6–7 Hz) optimally, and to achieve a robust source estimate, source reconstruction was performed on two separate frequency windows (6 ± 2, and 7 ± 2 Hz) for each subject and condition. The two source images were averaged for subsequent analysis, yielding a single image representing the frequency range 4–9 Hz. By combining the two frequency bands, we acquire a source estimate that emphasizes the center of our frequency band of interest (6–7 Hz) and tapers off towards the edges. This source estimate optimally represents the coherence spectrum observed in sensor space. The choice of the center frequency was also restricted by the length of the time window used for the analysis. That is, with a 1 s time window and a resulting 1 Hz frequency resolution, a non-integer center frequency at, for example, 6.5 Hz was not feasible. The source-reconstructed coherence at the frequency-range of interest confirmed that cerebro-acoustic coherence was strongest across groups and conditions in bilateral temporal lobes, including primary auditory cortex. (<xref ref-type="fig" rid="fig1">Figure 1F</xref>).</p><p>To test whether envelope tracking in the current study is modulated by intelligibility, we compared the coherence maps for the intelligible (nat) versus non-intelligible (1-channel) condition, in all groups combined (i.e., EB and SI), with dependent-samples permutation t-tests in SPM. The resulting statistical map (<xref ref-type="fig" rid="fig1">Figure 1G</xref>, p&lt;0.05, FWE-corrected) revealed a cluster in right superior and middle temporal cortex (STG, MTG), where synchronization to the envelope of speech was stronger when participants had access to the content of the story. In other words, the effect of intelligibility on the sensory response (coherence with the speech envelope) suggests an interface between acoustic speech processing and comprehension in STG that is present in both groups.</p></sec><sec id="s2-3"><title>Speech tracking in blind parieto-occipital cortex</title><p>Whether EB individuals recruit additional neural substrate for tracking the envelope of speech was tested using independent-samples permutation t-tests in SPM, contrasting coherence maps between EB and SI for all three conditions combined. The statistical maps (<xref ref-type="fig" rid="fig1">Figure 1H</xref>, p&lt;0.05, FWE-corrected) revealed enhanced coherence in EB versus SI in parieto-occipital cortex along the medial wall, centered on bilateral Precuneus, branching more extensively into the right hemisphere (see <xref ref-type="table" rid="table2">Table 2</xref>). This main effect of group highlights that neuronal populations in blind parieto-occipital cortex show enhanced synchronization to the acoustic speech signal. That is, blind participants show a stronger sensitivity to the sensory properties of the external speech stimulus at this level.</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.31640.005</object-id><label>Table 2.</label><caption><title>MNI (Montreal Neurological Institute) coordinates and p-values for the three contrasts tested with threshold-free cluster enhancement (TFCE).</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th colspan="2" valign="bottom">MNI-coordinates (mm)</th></tr><tr><th valign="bottom"/><th valign="bottom">p-value (FWE-cor)</th><th valign="bottom"><bold>x</bold></th><th valign="bottom"><bold>y</bold></th><th valign="bottom"><bold>z</bold></th></tr></thead><tbody><tr><td colspan="5" valign="bottom">Intelligibility</td></tr><tr><td valign="bottom"> Superior temporal gyrus</td><td valign="bottom">0.027</td><td valign="bottom">64</td><td valign="bottom">−16</td><td valign="bottom">0</td></tr><tr><td colspan="5" valign="bottom">Blindness</td></tr><tr><td valign="bottom"> Posterior cingulate</td><td valign="bottom">0.030</td><td valign="bottom">8</td><td valign="bottom">−40</td><td valign="bottom">40</td></tr><tr><td valign="bottom"> Postcentral sulcus</td><td valign="bottom">0.033</td><td valign="bottom">16</td><td valign="bottom">−32</td><td valign="bottom">64</td></tr><tr><td valign="bottom"> Precuneus</td><td valign="bottom">0.033</td><td valign="bottom">-8</td><td valign="bottom">−64</td><td valign="bottom">48</td></tr><tr><td colspan="5" valign="bottom">Blindness x intelligibility</td></tr><tr><td valign="bottom"> Calcarine sulcus</td><td valign="bottom">0.007</td><td valign="bottom">16</td><td valign="bottom">−88</td><td valign="bottom">0</td></tr></tbody></table></table-wrap><p>Finally, to investigate whether and where envelope tracking is sensitive to top-down predictions during intelligible speech comprehension, we subtracted the unintelligible (1-channel) from the intelligible (nat) condition, and computed independent-samples t-tests between groups. As highlighted in the introduction, we were particularly interested in the role played by the Calcarine sulcus (V1) in processing semantic attributes of speech (<xref ref-type="bibr" rid="bib22">Burton et al., 2002</xref>; <xref ref-type="bibr" rid="bib103">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="bib5">Amedi et al., 2003</xref>), and therefore we restricted the statistical analysis to the area around the primary visual cortex. The search volume was constructed from four 10 mm spheres around coordinates in bilateral Calcarine sulcus ([−7,–81, −3]; [−6,–85, 4]; [12, -87, 0; 10, –84, 6]). The coordinates were extracted from a previous study on speech comprehension in EB (<xref ref-type="bibr" rid="bib22">Burton et al., 2002</xref>). The resulting mask also include regions highlighted in similar studies by other groups (<xref ref-type="bibr" rid="bib103">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>).</p><p>A significant effect of the interaction between blindness and intelligibility [(nat<sub>EB</sub> – 1-channel<sub>EB</sub>) – (nat<sub>SI</sub> – 1-channel<sub>SI</sub>)] was observed in right calcarine sulcus. To explore the spatial specificity of the effect, we show the whole-brain statistical map for the interaction between intelligibility and blindness at a more liberal threshold (p&lt;0.005, uncorrected) in <xref ref-type="fig" rid="fig1">Figure 1I</xref>. This shows that intelligible speech selectively engages the area around the right calcarine sulcus in the blind versus the sighted. Specifically, the region corresponding to right ‘visual’ cortex showed enhanced sensitivity to intelligible speech in EB versus SI (p&lt;0.05, FWE-corrected). However, the analysis contrasting the two intelligible conditions (nat and 8-channel) did not yield a significant effect in the EB, suggesting that the low-level degrading of the stimulus alone does not drive the effect, but rather the intelligibility of the speech segment. Follow-up post hoc comparisons between the two groups revealed that coherence was stronger during the intelligible condition for EB versus SI (t[30] = 3.09, p=0.004), but not during the unintelligible condition (t[30] = –1.08, p=0.29).</p><p>A group difference between EB and SI across conditions was not observed in the calcarine region (t[30] = 1.1, p=0.28). The lack of an overall group effect in calcarine sulcus suggests that there is not a simple enhanced sensory response to speech in the blind. This was different to the response we observed in parietal cortex, in which synchronization was stronger in the blind than in the sighted. Rather, the response in calcarine sulcus only differed between the two groups when speech was intelligible. While overall coherence with the speech envelope was equally high in EB and SI, the blind population showed significantly higher coherence in the intelligible speech condition than did the sighted, who showed higher cerebro-acoustic coherence in the unintelligible condition (1-Chan). The latter is reminiscent of the fact that the more adverse the listening condition (low signal-to-noise ratio or audiovisual incongruence), the more the visual cortex is entrained to the visual speech signal of actual acoustic speech when presented together with varying levels of acoustic noise (<xref ref-type="bibr" rid="bib84">Park et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Giordano et al., 2017</xref>). Moreover, <xref ref-type="bibr" rid="bib43">Giordano et al. (2017)</xref> showed an increase of directed connectivity between superior frontal regions and visual cortex under the most challenging (acoustic noise and uninformative visual cues) conditions, again suggesting a link between the reorganization observed in the occipital cortex of blind individuals and typical multisensory pathways involving the occipital cortex in audio-visual speech processing (<xref ref-type="bibr" rid="bib54">Kayser et al., 2008</xref>). Visual deprivation since birth, however, triggers a functional reorganization of the calcarine region that can then dynamically interact with the intelligibility of the speech signal. For illustration purposes, cerebro-acoustic coherence from functional peak locations for intelligibility (across groups) in STG, blindness in the parieto-occipital cortex (<xref ref-type="fig" rid="fig1">Figure 1G</xref>; POC), and the interaction are represented as boxplots in <xref ref-type="fig" rid="fig1">Figure 1J</xref>.</p></sec><sec id="s2-4"><title>Changes in occipito-temporal connectivity in the EB</title><p>To further investigate whether cerebro-acoustic peaks in CS and STG which are sensitive to intelligible speech in EB are indicative of a more general re-organization of the network, we conducted functional connectivity analysis. Statistical analysis of the connectivity estimates was performed on the mean phase-locking value in the theta (4–8 Hz) range.</p><p>Using linear mixed-effects models, blindness (EB, SI), intelligibility (nat, 1-channel), and the interaction between blindness and intelligibility were added to the model in a hierarchical fashion. Blindness and intelligibility were modeled as fixed effects, while subject was a random effect. Intelligibility was nested in subject. A main effect was observed only for blindness (<italic>χ</italic>[1] = 4.32, p=0.038) and was caused by greater connectivity for EB versus SI (<xref ref-type="fig" rid="fig2">Figure 2A–B</xref>). The main effects of intelligibility and of the interaction between intelligibility and blindness were non-significant (p=0.241 and p=0.716, respectively).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.31640.006</object-id><label>Figure 2.</label><caption><title>Occipital-temporal connectivity in EB and SI.</title><p>(<bold>A</bold>) Spectra illustrate the phase locking between CS and STG in EB (dark curve) and in SI (light curve). Shaded areas illustrate the SEM. A difference between the curves is observed in the theta range. (<bold>B</bold>) Boxplots depict the mean phase locking between CS and STG for EB (black) and SI (white) in the theta range. Connectivity is enhanced in EB versus SI. (<bold>C</bold>) Directional connectivity analysis using the phase-slope index (PSI). Positive values suggest enhanced directionality from STG to CS, and negative values represent enhanced connectivity in the opposite direction. The boxplots highlight the strong feed-forward drive from CS to STG shown by SI, whereas blind individuals show a more balanced pattern, with a non-significant trend in the opposite direction.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31640-fig2-v3"/></fig><p>Subsequently, linear mixed-effects models were applied to test for the effects of blindness and intelligibility on the directional information flow (PSI) between CS and STG. The fixed and random effects structure was the same as that described in the previous analysis. Here, only the main effect of blindness was significant (<italic>χ</italic>[1] = 4.54, p=0.033), indicating that the directional information flow differs between groups. The effect of intelligibility and the interaction between intelligibility and blindness were both non-significant (p<italic>=</italic>0.51 and p=0.377, respectively). Follow-up post-hoc one-sample t-tests on the phase slope estimates for each group individually revealed a significant direction bias from CS to STG for SI (<italic>t</italic>[14] = –2.22, p=0.044). No directional bias was found for EB (<italic>t</italic>[16] = 0.74, p=0.47). As depicted in <xref ref-type="fig" rid="fig2">Figure 2C–D</xref>, these results suggest that CS projects predominantly to STG in SI, whereas in EB, this interaction is more balanced and trending in the opposite direction.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>To test whether neuronal populations in the early blind’s occipital cortex synchronize to the temporal dynamics present in human speech, we took advantage of the statistical and conceptual power offered by correlating magnetoencephalographic recordings with the envelope of naturalistic continuous speech. Using source-localized MEG activity, we confirm and extend previous research (<xref ref-type="bibr" rid="bib109">Scott et al., 2000</xref>; <xref ref-type="bibr" rid="bib32">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="bib76">Narain et al., 2003</xref>; <xref ref-type="bibr" rid="bib99">Rodd et al., 2005</xref>, <xref ref-type="bibr" rid="bib100">2010</xref>; <xref ref-type="bibr" rid="bib82">Okada et al., 2010</xref>; <xref ref-type="bibr" rid="bib89">Peelle et al., 2010</xref>) by showing that temporal brain regions entrain to the natural rhythm of speech signal in both blind and sighted groups (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Strikingly, we show that following early visual deprivation, occipito-parietal regions enhance their synchronization to the acoustic rhythm of speech (<xref ref-type="fig" rid="fig1">Figure 1G</xref>) independently of language content. That is, the region around the Precuneus/Cuneus area seems to be involved in low-level processing of acoustic information alone. Previous studies have demonstrated that this area enhances its response to auditory information after early, but not late, visual deprivation (<xref ref-type="bibr" rid="bib28">Collignon et al., 2013</xref>) and maintains this elevated response to sound even after sight-restoration early in life (<xref ref-type="bibr" rid="bib29">Collignon et al., 2015</xref>). These studies did not find that this crossmodal response was related to a specific cognitive function, but rather pointed to a more general response to sounds (<xref ref-type="bibr" rid="bib28">Collignon et al., 2013</xref>, <xref ref-type="bibr" rid="bib29">2015</xref>). Interestingly, the current data suggest that this area is capable of parsing complex temporal patterns in sounds but is insensitive to the intelligibility of speech, again suggesting a more general role in sound processing.</p><p>In order to isolate the brain regions that are modulated by speech comprehension, we identified regions showing a greater response to amplitude modulations that convey speech information as compared to amplitude modulations that do not. In addition to the right STG observed in both groups (<xref ref-type="fig" rid="fig1">Figure 1H</xref>), the blind showed enhanced cerebro-acoustic coherence during intelligible speech in the vicinity of calcarine sulcus (V1; <xref ref-type="fig" rid="fig1">Figure 1I</xref>). This pattern of local encoding was accompanied by enhanced occipito-temporal connectivity during speech comprehension in EB as compared to SI. SI show the expected feed-forward projections from occipital to temporal regions (<xref ref-type="bibr" rid="bib61">Lamme et al., 1998</xref>), whereas EB show a more balanced connectivity profile, trending towards the reverse temporal to occipital direction (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). These findings support the idea of a reverse hierarchical model (<xref ref-type="bibr" rid="bib24">Büchel, 2003</xref>) of the occipital cortex in EB, where the regions typically coding for ‘low-level’ visual features in the sighted (e.g. visual contrast or orientation) participate in higher-level function (e.g. speech intelligibility). Indeed, previous studies have found increased activity in the primary ‘visual’ cortex of EB people during Braille reading (<xref ref-type="bibr" rid="bib104">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="bib22">Burton et al., 2002</xref>, <xref ref-type="bibr" rid="bib21">2012</xref>), verbal memory and verb generation tasks (<xref ref-type="bibr" rid="bib5">Amedi et al., 2003</xref>), and during auditory language-related processing (<xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>). In line with our results, activity in primary occipital regions in EB people is stronger in a semantic versus a phonologic task (<xref ref-type="bibr" rid="bib20">Burton et al., 2003</xref>), and vary as a function of syntactic and semantic complexity (<xref ref-type="bibr" rid="bib103">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Lane et al., 2015</xref>). Moreover, repetitive transcranial magnetic stimulation (rTMS) over the occipital pole induces more semantic errors than phonologic errors in a verb-generation task in EB people (<xref ref-type="bibr" rid="bib5">Amedi et al., 2003</xref>). As we show that occipital regions entrain to the envelope of speech and are enhanced by its intelligibility, our results clearly suggest that the involvement of the occipital pole for language is not fully abstracted from sensory inputs as previously suggested (<xref ref-type="bibr" rid="bib12">Bedny, 2017</xref>). The role of the occipital cortex in tracking the flow of speech in EB people may constitute an adaptive strategy to boost perceptual sensitivity at informational peaks in language.</p><sec id="s3-1"><title>Mechanistic origins of envelope tracking in occipital cortex</title><p>Although language is not the only cognitive process that selectively activates the occipital cortex of EB people, it is arguably one of the most puzzling. The reason is that reorganization of other processes, such as auditory motion perception and tactile object recognition, appears to follow the topography of the functionally equivalent visual processes in the sighted brain (<xref ref-type="bibr" rid="bib97">Ricciardi et al., 2007a</xref>; <xref ref-type="bibr" rid="bib4">Amedi et al., 2010</xref>; <xref ref-type="bibr" rid="bib38">Dormal et al., 2016</xref>). For example, the hMT+/V5 complex, which is typically involved in visual motion in the sighted, selectively processes auditory (<xref ref-type="bibr" rid="bib94">Poirier et al., 2006</xref>; <xref ref-type="bibr" rid="bib38">Dormal et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Jiang et al., 2016</xref>) or tactile (<xref ref-type="bibr" rid="bib98">Ricciardi et al., 2007b</xref>) motion in blind people. However, in the case of language, such recruitment is striking in light of the cognitive and evolutionary differences between vision and language (<xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>). This led to the proposal that, at birth, human cortical areas are cognitively pluripotent: capable of assuming a broad range of unrelated cognitive functions (<xref ref-type="bibr" rid="bib12">Bedny, 2017</xref>). However, this argument resides on the presupposition that language has no computational relation with vision. But does this proposition tally with what we know about the relationship between the visual system and the classical language network? Rhythmic information in speech has a well-known language-related surrogate in the visual domain: lip movements (<xref ref-type="bibr" rid="bib64">Lewkowicz and Hansen-Tift, 2012</xref>; <xref ref-type="bibr" rid="bib84">Park et al., 2016</xref>). Indeed, both acoustic and visual speech signals exhibit rhythmic temporal patterns at prosodic and syllabic rates (<xref ref-type="bibr" rid="bib26">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib108">Schwartz and Savariaux, 2014</xref>; <xref ref-type="bibr" rid="bib43">Giordano et al., 2017</xref>). The perception of lip kinematics that are naturally linked to amplitude fluctuations in speech serves as an important vehicle for the everyday use of language (<xref ref-type="bibr" rid="bib57">Kuhl and Meltzoff, 1982</xref>; <xref ref-type="bibr" rid="bib119">Weikum et al., 2007</xref>; <xref ref-type="bibr" rid="bib64">Lewkowicz and Hansen-Tift, 2012</xref>) and helps language understanding, particularly in noisy conditions (<xref ref-type="bibr" rid="bib102">Ross et al., 2007</xref>). Indeed, reading lips in the absence of any sound activates both primary and association auditory regions overlapping with regions that are active during the actual perception of spoken words (<xref ref-type="bibr" rid="bib25">Calvert et al., 1997</xref>). The synchronicity between auditory and visual speech entrains rhythmic activity in the observer’s primary auditory and visual regions, and facilitates perception by aligning neural excitability with acoustic or visual speech features (<xref ref-type="bibr" rid="bib106">Schroeder et al., 2008</xref>; <xref ref-type="bibr" rid="bib107">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="bib44">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib73">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="bib87">Peelle and Davis, 2012</xref>; <xref ref-type="bibr" rid="bib116">van Wassenhove, 2013</xref>; <xref ref-type="bibr" rid="bib121">Zion Golumbic et al., 2013b</xref>; <xref ref-type="bibr" rid="bib84">Park et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Giordano et al., 2017</xref>). These results strongly suggest that both the auditory and the visual components of speech are processed together at the earliest level possible in neural circuitry, based on the shared slow temporal modulations (around 2–7 Hz range) present across modalities (<xref ref-type="bibr" rid="bib26">Chandrasekaran et al., 2009</xref>). Corroborating this idea, it has been demonstrated that neuronal populations in visual cortex follow the temporal dynamics of lip movements in sighted individuals, similar to the way in which temporal regions follow the acoustic and visual fluctuations of speech (<xref ref-type="bibr" rid="bib69">Luo et al., 2010</xref>; <xref ref-type="bibr" rid="bib120">Zion Golumbic et al., 2013a</xref>; <xref ref-type="bibr" rid="bib84">Park et al., 2016</xref>). Similar to temporal cortex, occipital cortex in the sighted also shows enhanced lip tracking when attention is directed to speech content. This result highlights the fact that a basic oscillatory architecture for tracking the dynamic aspects of (visual-) speech in occipital cortex exists even in sighted individuals.</p><p>Importantly, audiovisual integration of the temporal dynamics of speech has been suggested to play a key role when learning speech early in life: young infants detect, match, and integrate the auditory and visual temporal coherence of speech (<xref ref-type="bibr" rid="bib57">Kuhl and Meltzoff, 1982</xref>; <xref ref-type="bibr" rid="bib101">Rosenblum et al., 1997</xref>; <xref ref-type="bibr" rid="bib66">Lewkowicz, 2000</xref>, <xref ref-type="bibr" rid="bib67">2010</xref>; <xref ref-type="bibr" rid="bib19">Brookes et al., 2001</xref>; <xref ref-type="bibr" rid="bib85">Patterson and Werker, 2003</xref>; <xref ref-type="bibr" rid="bib63">Lewkowicz and Ghazanfar, 2006</xref>; <xref ref-type="bibr" rid="bib58">Kushnerenko et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Bristow et al., 2009</xref>; <xref ref-type="bibr" rid="bib95">Pons et al., 2009</xref>; <xref ref-type="bibr" rid="bib117">Vouloumanos et al., 2009</xref>; <xref ref-type="bibr" rid="bib65">Lewkowicz et al., 2010</xref>; <xref ref-type="bibr" rid="bib77">Nath et al., 2011</xref>). For instance, young infants between 4 and 6 months of age can detect their native language from lip movements only (<xref ref-type="bibr" rid="bib119">Weikum et al., 2007</xref>). Around the same period, children detect synchrony between lip movements and speech sounds, and distribute more attention towards the mouth than towards the eyes (<xref ref-type="bibr" rid="bib64">Lewkowicz and Hansen-Tift, 2012</xref>). Linking what they hear to the lip movements may provide young infants with a stepping-stone towards language production (<xref ref-type="bibr" rid="bib64">Lewkowicz and Hansen-Tift, 2012</xref>). Moreover, infants aged 10 weeks already exhibit a McGurk effect, again highlighting the early multisensory nature of speech perception (<xref ref-type="bibr" rid="bib101">Rosenblum et al., 1997</xref>). Taken together, these results suggest that an audio-visual link between observing lip movements and hearing speech sounds is present at very early developmental stages, potentially from birth, which helps infants acquire their first language.</p><p>In line with these prior studies, the current results may support the biased connectivity hypothesis of cross-modal reorganization (<xref ref-type="bibr" rid="bib96">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Hannagan et al., 2015</xref>; <xref ref-type="bibr" rid="bib113">Striem-Amit et al., 2015</xref>). Indeed, it has been argued that reorganization in blind occipital cortex may be constrained by functional pathways to other sensory and cognitive systems that are also present in sighted individuals (<xref ref-type="bibr" rid="bib39">Elman et al., 1996</xref>; <xref ref-type="bibr" rid="bib47">Hannagan et al., 2015</xref>). This hypothesis may explain the overlap in functional specialization between blind and sighted individuals (<xref ref-type="bibr" rid="bib30">Collignon et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Dormal et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">He et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Jiang et al., 2016</xref>; <xref ref-type="bibr" rid="bib86">Peelen et al., 2013</xref>; <xref ref-type="bibr" rid="bib91">Pietrini et al., 2004</xref>; <xref ref-type="bibr" rid="bib118">Weeks et al., 2000</xref>; <xref ref-type="bibr" rid="bib94">Poirier et al., 2006</xref>). In our experiment, sensitivity to acoustic dynamics of intelligible speech in blind occipital cortex could arise from pre-existing occipito-temporal pathways connecting the auditory and visual system that are particularly important for the early developmental stages of language acquisition. In fact, the reorganization of this potentially predisposed pathway to process language content would explain how language-selective response may appear in blind children as young as 3 years old (<xref ref-type="bibr" rid="bib11">Bedny et al., 2015</xref>).</p><p>Previous studies have suggested that language processing in the occipital cortex arises through top-down projections from frontal regions typically associated with the classical language network (<xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="bib34">Deen et al., 2015</xref>), and that the representational content is symbolic and abstract rather than sensory (<xref ref-type="bibr" rid="bib12">Bedny, 2017</xref>). Our results contrast with this view by showing that neuronal populations in (peri-)calcarine cortex align to the temporal dynamics of intelligible speech, and are functionally connected to areas sensitive to auditory information in temporal cortex. In sighted individuals, regions of the temporal lobe including STS are sensitive to acoustic features of speech, whereas higher-level regions such as anterior temporal cortex and left inferior frontal gyrus are relatively insensitive to these features and therefore do not entrain to the syllabic rate of speech (<xref ref-type="bibr" rid="bib32">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="bib50">Hickok and Poeppel, 2007</xref>; see confirmation in <xref ref-type="fig" rid="fig1">Figure 1F–G</xref>). This suggests that occipital areas respond, at least partially, to speech at a much lower (sensory) level than previously thought in EB brains, which may be caused by the reorganization of existing multisensory pathways connecting the ‘auditory’ and ‘visual’ centers in the brain.</p><p>Functional dependencies between sensory systems exist between the earliest stages of sensory processing in both human (<xref ref-type="bibr" rid="bib42">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib54">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib107">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="bib75">Murray et al., 2016</xref>) and nonhuman primates (<xref ref-type="bibr" rid="bib40">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib60">Lakatos et al., 2007</xref>; <xref ref-type="bibr" rid="bib107">Schroeder and Lakatos, 2009</xref>). Several neuroimaging studies have demonstrated enhanced functional connectivity in sighted individuals between auditory and visual cortices under multisensory conditions (see <xref ref-type="bibr" rid="bib75">Murray et al., 2016</xref> for a recent review), including multisensory speech (<xref ref-type="bibr" rid="bib43">Giordano et al., 2017</xref>). Moreover, neuroimaging studies have shown that hearing people consistently activate left temporal regions during silent speech-reading (<xref ref-type="bibr" rid="bib25">Calvert et al., 1997</xref>; <xref ref-type="bibr" rid="bib71">MacSweeney et al., 2000</xref>, <xref ref-type="bibr" rid="bib72">2001</xref>). We therefore postulate that brain networks that are typically dedicated to the integration of audio-visual speech signal, might be reorganized in the absence of visual inputs and might lead to an extension of speech tracking in the occipital cortex (<xref ref-type="bibr" rid="bib31">Collignon et al., 2009</xref>). Although an experience-dependent mechanism related to EB affects the strength and directionality of the connectivity between the occipital and temporal regions, the presence of intrinsic connectivity between these regions — which can also be observed in sighted individuals (e.g. for multisensory integration) — may constrain the expression of the plasticity observed in our task. Building on this connectivity bias, the occipital pole may extend its sensitivity to the intelligibility of speech, a computation this region is obviously not originally dedicated to.</p><p>A number of recent studies have suggested that visual deprivation reinforces the functional connections between the occipital cortex and auditory regions typically classified as the language network (<xref ref-type="bibr" rid="bib48">Hasson et al., 2016</xref>; <xref ref-type="bibr" rid="bib105">Schepers et al., 2012</xref>). Previous studies using dynamic causal modelling support the idea that auditory information reaches the reorganized occipital cortex of the blind through direct temporo-occipital connection, rather than using subcortical (<xref ref-type="bibr" rid="bib56">Klinge et al., 2010</xref>) or top-down pathways (<xref ref-type="bibr" rid="bib28">Collignon et al., 2013</xref>). In support of these studies, we observed that the overall magnitude of functional connectivity between occipital and temporal cortex is higher in blind people than in sighted people during natural speech comprehension. Moreover, directional connectivity analysis revealed that the interaction between the two cortices is also qualitatively different: sighted individuals show a strong feed-forward drive towards temporal cortex, whereas blind individuals show a more balanced information flow, and a trend in the reverse direction. These results highlight one possible pathway by which the speech signal is enhanced in the occipital cortex of EB individuals. However, this does not mean that the changes in connectivity between blind and sighted individuals are limited to this specific network (e.g. see <xref ref-type="bibr" rid="bib55">Kayser et al., 2015</xref>; <xref ref-type="bibr" rid="bib83">Park et al., 2015</xref>).</p></sec><sec id="s3-2"><title>Speech comprehension in the right hemisphere</title><p>We observed that neuronal populations in right superior temporal cortex synchronize to the temporal dynamics of intelligible, but not non-intelligible, speech in both EB and SI groups. Why does speech intelligibility modulate temporal regions of the right, but not the left, hemisphere? According to an influential model of speech comprehension – the asymmetric sampling in time model (AST; <xref ref-type="bibr" rid="bib44">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib50">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib93">Poeppel, 2003</xref>) – there is a division of labour between the left- and right auditory cortices (<xref ref-type="bibr" rid="bib93">Poeppel, 2003</xref>; <xref ref-type="bibr" rid="bib16">Boemio et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Hickok and Poeppel, 2007</xref>), with the left auditory cortex being more sensitive to high-frequency information (+20 Hz), whereas the right temporal cortex is more sensitive to low-frequency information (~6 Hz) such as syllable sampling and prosody (<xref ref-type="bibr" rid="bib13">Belin et al., 1998</xref>; <xref ref-type="bibr" rid="bib93">Poeppel, 2003</xref>; <xref ref-type="bibr" rid="bib16">Boemio et al., 2005</xref>; <xref ref-type="bibr" rid="bib81">Obleser et al., 2008</xref>; <xref ref-type="bibr" rid="bib44">Giraud and Poeppel, 2012</xref>). Several studies have shown that the right hemisphere is specifically involved in the representation of connected speech (<xref ref-type="bibr" rid="bib17">Bourguignon et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Fonteneau et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Horowitz-Kraus et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Alexandrou et al., 2017</xref>), whereas other studies have directly demonstrated the prevalence of speech-to-brain entrainment while listening to sentences or stories in delta and theta bands in the right hemisphere more than in the left hemisphere (<xref ref-type="bibr" rid="bib70">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib1">Abrams et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Giordano et al., 2017</xref>). The present study therefore replicates these results by showing enhanced phase coupling between the right hemisphere and the speech envelope at the syllabic rate (low-frequency phase of speech envelope), consistent with the AST model. An interesting observation in the current study is that right hemispheric sensitivity to intelligible speech in temporal areas coincides with the enhanced right hemispheric sensitivity to intelligible speech in the occipital cortex of blind individuals.</p><p>Having more cortical tissue devoted to sentence processing and understanding could potentially support enhanced sentence comprehension. Previous studies have indeed demonstrated that, as compared to sighted individuals, blind people have enhanced speech discrimination in noisy environments (<xref ref-type="bibr" rid="bib78">Niemeyer and Starlinger, 1981</xref>), as well as the capability to understand speech displayed at a much faster rate (sighted listeners at rates of 9–14 syllables/s andblind listeners at rates of 17–22 syllables/s; <xref ref-type="bibr" rid="bib74">Moos and Trouvain, 2007</xref>). Crucially, listening to intelligible ultra-fast speech (as compared to reverse speech) has been shown to cause enhanced activity in the right primary visual cortex in early and late blind individuals when compared to sighted controls, and activity in this region correlates with individual ultra-fast speech perception skills [<xref ref-type="bibr" rid="bib36">Dietrich et al., 2013</xref>]). These results raise the interesting possibility that the engagement of right V1 in the analysis of the speech envelope, as demonstrated in our study, may support the enhanced encoding of early supra-segmental aspects of the speech signal, supporting the ability to understand an ultra-fast speech signal. However, our behavioral task (speech comprehension) did not allow us to directly assess this link between the reorganized occipital cortex and speech comprehension, as by design performance was almost at ceiling in the nat and 8-chan condition but at chance in the 1-chan condition (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><p>However, it is important to note that linking brain activity and behavior is not a trivial issue. Many studies have not found a direct link between crossmodal reorganization and non-visual processing. More generally, as a behavioral outcome is the end product of a complex interactive process between several brain regions, linking the role of one region in isolation (e.g. the reorganized region in the occipital cortex of blind people) to behavior (e.g. performance) in a multifaced task is not straightforward. An absence of a direct relation between behavior (e.g. speech processing) and crossmodal plasticity could be explained by the fact that this complex process is supported by additional networks other than the reorganized ones. Interestingly, recent studies have shown that early visual deprivation triggers a game of ‘balance’ between the brain systems typically dedicated to a specific process and the reorganized occipital cortex (<xref ref-type="bibr" rid="bib38">Dormal et al., 2016</xref>). It has indeed been proposed that early visual deprivation triggers a redeployment mechanism that would reallocate part of the processing typically implemented in the preserved networks (i.e. the temporal or frontal cortices for speech processing) to the occipital cortex deprived of its most salient input (vision). Two recent studies using multivoxel pattern analysis (MVPA) showed that the ability to decode the different auditory motion stimuli was enhanced in hMT+ (a region typically involved in visual motion in sighted individuals) of early blind individuals, whereas an enhanced decoding accuracy was observed in the planum temporale in the sighted group (<xref ref-type="bibr" rid="bib52">Jiang et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Dormal et al., 2016</xref>). Moreover, <xref ref-type="bibr" rid="bib11">Bedny et al. (2015)</xref> reported an enhanced activation of occipital cortex and a simultaneous deactivation of prefrontal regions during a linguistic task in blind children. The authors suggested that the increased involvement of the occipital cortex might decrease the pressure on the prefrontal areas to specialize in language processing. Interestingly, a transcranial magnetic stimulation (TMS) study reported a reduced disruptive effect of TMS applied over the left inferior prefrontal cortex during linguistic tasks for both blind and sighted individuals, whereas TMS applied over the occipital cortex caused more disruption in early blind as compared to sighted people (<xref ref-type="bibr" rid="bib3">Amedi et al., 2004</xref>). Such studies support the idea that brain regions that are typically recruited for specific tasks in sighted individuals may become less essential in EB people if they concomitantly recruit occipital regions for the same task. An important open question for future research therefore concerns the relative behavioral contribution of occipital and perisylvian cortex to speech understanding.</p></sec><sec id="s3-3"><title>Conclusions</title><p>We demonstrate that the primary ‘visual’ cortex synchronizes to the temporal dynamics of intelligible speech at the rate of syllable transitions in language (~6–7 Hz). Our results demonstrate that this neural population is involved in processing the sensory signal of speech, and therefore contrasts with the proposition that occipital involvement in speech processing is abstracted from its sensory input and purely reflects higher-level operations similar to those observed in prefrontal regions (<xref ref-type="bibr" rid="bib12">Bedny, 2017</xref>). Blindness, due to the absence of organizing visual input, leaves the door open for sensory and functional colonization of occipital regions. This colonization might however not be stochastic, but could be constrained by modes of information processing that are natively anchored in specific brain regions and networks. Even if the exact processing mode is still to be unveiled by future research, we hypothesise that the mapping of language onto occipital cortex builds upon pre-existing oscillatory architecture typically linking auditory and visual speech rhythm (<xref ref-type="bibr" rid="bib26">Chandrasekaran et al., 2009</xref>). Our study therefore supports the view that the development of functional specialisation in the human cortex is the product of a dynamic interplay between genetic and environmental factors during development, rather than being predetermined at birth (<xref ref-type="bibr" rid="bib39">Elman et al., 1996</xref>).</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-31640-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.31640.007</object-id><label>Video 1.</label><caption><title>Movie of the natural and two vocoding conditions used for one exemplar of auditory segment used in our experiment.</title></caption></media></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Seventeen early blind (11 female; mean ± SD, 32.9 ± 10.19 years; range, 20–67 years) and sixteen sighted individuals (10 female; mean ± SD, 32.2 ± 9.92 years; range, 20–53 years) participated in the current study. There was no age difference between the blind and the sighted group (<italic>t</italic>[30] = 0.19, p=0.85). All blind participants were either totally blind or severely visually impaired from birth; however, two participants reported residual visual perception before the age of 3 and one before the age of 4, as well as one participant who lost their sight completely at age 10. Causes of vision loss were damage or detached retina (10), damage to the optic nerve (3), infection of the eyes (1), microphtalmia (2), and hypoxia (1). Although some participants reported residual light perception, none were able to use vision functionally. All participants were proficient braille readers and native speakers of Italian. None of them suffered from a known neurological or peripheral auditory disease. The data from one sighted individual were not included because of the discovery of a brain structural abnormality that was unknown to the experimenters at the time of the experiment. The project was approved by the local ethical committee at the University of Trento. In agreement with the Declaration of Helsinki, all participants provided written informed consent before participating in the study.</p></sec><sec id="s4-2"><title>Experimental design</title><p>Auditory stimuli were delivered into the magnetically shielded MEG room via stereo loudspeakers using a Panphonics Sound Shower two amplifier at a comfortable sound level, which was the same for all participants. Stimulus presentation was controlled via the Matlab (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link>) Psychophysics Toolbox 3 (http://psychtoolbox.org; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002881">SCR_002881</ext-link>) running on a Dell Alienware Aurora PC under Windows 7 (64 bit). Both sighted and blind participants were blindfolded during the experiment, and the room was dimly lit to allow for visual monitoring of the participant via a video feed from the MEG room. Instructions were provided throughout the experiment using previous recordings from one of the experimenters (FB).</p><p>The stimuli consisted of 14 short segments (~1 min) from popular audiobooks (e.g., Pippi Longstocking and Candide) in Italian (nat). Furthermore, channel-vocoding in the Praat software was used to produce two additional control conditions. First, the original sound file was band-pass filtered into 1 (1-channel) or 8 (8-channel) logarithmically spaced frequency bands. The envelope for each of these bands was computed, filled with Gaussian white noise, and the different signals were recombined into a single sound file. The resulting signal has an amplitude envelope close to the original, while the fine spectral detail was gradually distorted. Perceptually, the voice of the speaker is highly distorted in the 8-channel condition, but intelligibility is unperturbed. By contrast, in the 1-channel condition, speech is entirely unintelligible. In total, 42 sound files were presented in a pseudo-randomized fashion, distributed among seven blocks. Each block contained two sound files from each condition, and the same story was never used twice in the same block. Examples of the stimuli are provided in supplementary media content ( see <xref ref-type="video" rid="video1">Video 1</xref>).</p><p>To verify story comprehension, each speech segment was followed by a single-sentence statement about the story. Participants were instructed to listen to the story carefully, and to judge whether the statement at the end was true or false, using a nonmagnetic button box. Responses were provided with the index and middle finger of the right hand.</p></sec><sec id="s4-3"><title>MEG data acquisition and preprocessing</title><p>MEG was recorded continuously from a 306 triple sensor (204 planar gradiometers; 102 magnetometers) whole-head system (Elekta Neuromag, Helsinki, Finland) using a sampling rate of 1 kHz and an online band-bass filter between 0.1 and 300 Hz. The headshape of each individual participant was measured using a Polhemus FASTRAK 3D digitizer. Head position of the subject was recorded continuously using five localization coils (forehead, mastoids).</p><p>Data pre-processing was performed using the open-source Matlab toolbox Fieldtrip (www.fieldtriptoolbox.org; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_004849">SCR_004849</ext-link>). First the continuous data were filtered (high-pass Butterworth filter at 1 Hz, low-pass Butterworth filter at 170 Hz, and DFT filter at 50, 100, and 150 Hz to remove line-noise artefacts in the signal), and downsampled to 256 Hz. Next, the data were epoched into segments of 1 s for subsequent analysis.</p><p>Rejection of trials containing artefacts and bad channels was performed using a semi-automatic procedure. First, outliers were rejected using a pre-screening based on the variance and range in each trial/channel. Then, algorithmically guided visual inspection of the raw data was performed to remove any remaining sources of noise.</p></sec><sec id="s4-4"><title>Extraction of the speech amplitude envelope</title><p>Amplitude envelopes of the stories were computed using the Chimera toolbox (<xref ref-type="bibr" rid="bib26">Chandrasekaran et al., 2009</xref>) and custom code, following the procedure described by Gross and colleagues (<xref ref-type="bibr" rid="bib45">Gross et al., 2013</xref>). First, the sound files were band-pass filtered between 100 and 1000 Hz into nine frequency-bands, using a fourth order Butterworth filter. The filter was applied in forward and backward direction to avoid any phase shifts with respect to the original signal, and frequency-bands were spaced with equal width along the human basilar membrane. Subsequently, the analytic amplitude for each filtered segment was computed as the absolute of the Hilbert transform. Finally, the amplitude envelopes of all nine bands were summed and scaled to a maximum value of 1. The resulting envelope was combined with the MEG data, and processed identically henceforth.</p></sec><sec id="s4-5"><title>Analysis of cerebro-acoustic coherence</title><p>To determine where, and at what temporal scale, neuronal populations follow the temporal dynamics of speech, we computed spectral coherence between the speech envelope and the MEG signal. Coherence is a statistic that quantifies the phase relationship between two signals, and can be used to relate oscillatory activity in the brain with a peripheral measure such as a speech signal (<xref ref-type="bibr" rid="bib88">Peelle et al., 2013</xref>). The first analysis was conducted in sensor space, across conditions and participants, to determine the temporal scale at which coherence between the speech envelope and the MEG signal is strongest. To this end, a Hanning taper was applied to the 1 s data segments, and Fourier transformation was used to compute the cross-spectral density between 1 and 30 Hz, with a step size of 1 Hz. To render the coherence values more normally distributed, a Fisher z-transform was applied by computing the inverse hyperbolic tangent (atanh).</p><p>Source-space analysis was centered on the coherence frequency-band of interest (FOI) identified in the sensor space analysis. Source reconstruction was performed using a frequency domain beamformer called Dynamic Imaging of Coherent Sources (DICS) (<xref ref-type="bibr" rid="bib46">Gross et al., 2001</xref>; <xref ref-type="bibr" rid="bib68">Liljeström et al., 2005</xref>). DICS was used to localize spectral coherence, observed at the sensors, on a three-dimensional grid (8 × 8 × 8 mm). The forward model was based on a realistic single-shell headmodel (<xref ref-type="bibr" rid="bib80">Nolte, 2003</xref>) for each individual. As structural scans could not be acquired for all participants, we approximated individual anatomy by warping a structural MNI template brain (MNI, Montreal, Quebec, Canada; <ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/brainweb">www.bic.mni.mcgill.ca/brainweb</ext-link>) into individual headspace using the information from each participant’s headshape.</p></sec><sec id="s4-6"><title>Source connectivity analysis</title><p>Functional connectivity between occipital and temporal cortex was computed by extracting virtual sensor time-series at the locations of interest in CS and STG using time-domain beamforming. These virtual sensor time series were used to compute non-directional and directional connectivity metrics. The rationale behind focusing our analyses on STG and CS is to limit our hypothesis space by the results as they unfolded in our analytic steps. Indeed, we found that the right STG was the only region modulated by the intelligibility of speech in both groups, confirming previous results using similar methods (<xref ref-type="bibr" rid="bib70">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib1">Abrams et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Giordano et al., 2017</xref>). In addition to STG, we found enhanced cerebro-acoustic coherence for intelligible speech in CS of EB individuals. We therefore decided to focus our connectivity analyses on these two ROIs since they were functionally defined from our first analytical step (modulation of cerebro-acoustic coherence by speech intelligibility). The advantage of following this procedure is that connectivity analyses are based on nodes that we can functionally interpret and for which we have clear predictions. We successfully used the same hierarchical analytic structure in previous studies (<xref ref-type="bibr" rid="bib28">Collignon et al., 2013</xref>, <xref ref-type="bibr" rid="bib29">2015</xref>; <xref ref-type="bibr" rid="bib14">Benetti et al., 2017</xref>), following guidelines on how to investigate directional connectivity in brain networks (<xref ref-type="bibr" rid="bib112">Stephan et al., 2010</xref>).</p><p>Virtual sensor time-series at the two locations of interest were extracted using a time-domain vector-beamforming technique called linear constrained minimum variance (LCMV) beamforming (<xref ref-type="bibr" rid="bib115">Van Veen et al., 1997</xref>). First, average covariance matrices were computed for each participant to estimate common spatial filter coefficients. These filter coefficients were multiplied with the single-trial cleaned MEG data. To reduce the resulting three-dimensional time-series to one singular value, decomposition (SVD) was applied, resulting in a single time-series for each trial and participant.</p><p>Functional connectivity between the two regions was computed at peak locations using the phase-locking value (PLV) (<xref ref-type="bibr" rid="bib59">Lachaux et al., 1999</xref>). First, single-trial virtual sensor time courses were converted to the frequency domain (0–50 Hz) using Fourier transformation. The data were padded to 2 s and a Hanning taper was applied to reduce spectral leakage. Coherence was used as a proxy for functional connectivity. To disentangle phase consistency between regions from joint fluctuations in power, the spectra were normalized with respect to the amplitude, resulting in an estimate of the phase-locking (<xref ref-type="bibr" rid="bib59">Lachaux et al., 1999</xref>) between regions. Connectivity estimates were normalized using a Fischer z-transform (atanh), as in the analysis of cerebro-acoustic coherence.</p><p>PLV is a symmetric proxy for connectivity and does not allow for inferences regarding the direction of information flow between two regions of interest. To test whether differences in functional connectivity between EB and SI are accompanied by changes in the directionality of the flow of information between the regions of interest, we computed the phase slope index (PSI) (<xref ref-type="bibr" rid="bib79">Nolte et al., 2008</xref>). The PSI deduces net directionality from the time delay between two time series (x1 and x2). Time series x1 is said to precede, and hence drive, time series x2 in a given frequency band if the phase difference between x1 and x2 increases with higher frequencies. Consequently, a negative phase slope reflects a net information flux in the reverse direction, that is, from x2 to x1. Here, we computed the PSI using a bandwidth of ±5 Hz around the frequencies of interest. Following the recommendation by Nolte and colleagues (<xref ref-type="bibr" rid="bib79">Nolte et al., 2008</xref>), PSI estimates were normalized with the standard error, which was computed using the jackknife method.</p></sec><sec id="s4-7"><title>Statistical analysis</title><p>Statistical testing of the behavioral comprehension scores, as well as the connectivity estimates, was performed using linear mixed-effects models in R. Differences between conditions and groups in source space were evaluated using Statistical Parametric Mapping (SPM12; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_007037">SCR_007037</ext-link>), and the Threshold-Free Cluster Enhancement (TFCE) toolboxes in Matlab. TFCE (<xref ref-type="bibr" rid="bib111">Smith and Nichols, 2009</xref>) computes new values for each voxel in a statistical map as a function of the original voxel value and the values of the surrounding voxels. By enhancing the statistical values in voxels with a high T-value that are also part of a local cluster, TFCE optimally combines the benefits of voxel-wise and cluster-based methods. TFCE was applied to the whole-brain contrasts. Final correction for multiple comparisons was applied using a maximum statistic based on 1000 permutations of group membership (independent testing) or conditions (dependent testing). Here, we applied a variance smoothing of 15 mm FWHM to reduce the effects of high spatial frequency noise in the statistical maps. The smoothing kernel used in the current study is higher than that in comparable fMRI studies because of the inherent smoothness of the source-reconstructed MEG data.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The project was funded by the ERC grant MADVIS – Mapping the Deprived Visual System: Cracking function for Prediction (Project: 337573, ERC-20130StG) awarded to Olivier Collignon. We would also like extend our gratitude to Valeria Occelli, and to the Masters students who assisted with the data acquisition (Marco Barilari, Giorgia Bertonati, and Elisa Crestale) for their kind assistance with the blind participants. In addition, we would like to thank Gianpiero Monittola for ongoing support with the hardware and Jodie Davies-Thompson for insightful comments on the manuscript. Finally, we would like to thank the organizations for the blind in Trento, Mantova, Genova, Savona, Cuneo, Torino, Trieste and Milano for their help in recruiting participants.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Validation, Methodology, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Validation, Investigation, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation, Validation, Investigation, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The project was approved by the local ethical committee at the University of Trento (protocol 2014-007). In agreement with the Declaration of Helsinki, all participants provided written informed consent to participate in the study.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.31640.008</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-31640-transrepform-v3.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrams</surname> <given-names>DA</given-names></name><name><surname>Nicol</surname> <given-names>T</given-names></name><name><surname>Zecker</surname> <given-names>S</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Right-hemisphere auditory cortex is dominant for coding syllable patterns in speech</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>3958</fpage><lpage>3965</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0187-08.2008</pub-id><pub-id pub-id-type="pmid">18400895</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandrou</surname> <given-names>AM</given-names></name><name><surname>Saarinen</surname> <given-names>T</given-names></name><name><surname>Mäkelä</surname> <given-names>S</given-names></name><name><surname>Kujala</surname> <given-names>J</given-names></name><name><surname>Salmelin</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The right hemisphere is highlighted in connected natural speech production and perception</article-title><source>NeuroImage</source><volume>152</volume><fpage>628</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.006</pub-id><pub-id pub-id-type="pmid">28268122</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Floel</surname> <given-names>A</given-names></name><name><surname>Knecht</surname> <given-names>S</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name><name><surname>Cohen</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Transcranial magnetic stimulation of the occipital pole interferes with verbal processing in blind subjects</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1266</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1038/nn1328</pub-id><pub-id pub-id-type="pmid">15467719</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Raz</surname> <given-names>N</given-names></name><name><surname>Azulay</surname> <given-names>H</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical activity during tactile exploration of objects in blind and sighted humans</article-title><source>Restorative Neurology and Neuroscience</source><volume>28</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.3233/RNN-2010-0503</pub-id><pub-id pub-id-type="pmid">20404404</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Raz</surname> <given-names>N</given-names></name><name><surname>Pianka</surname> <given-names>P</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Early 'visual' cortex activation correlates with superior verbal memory performance in the blind</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>758</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1038/nn1072</pub-id><pub-id pub-id-type="pmid">12808458</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnaud</surname> <given-names>L</given-names></name><name><surname>Sato</surname> <given-names>M</given-names></name><name><surname>Ménard</surname> <given-names>L</given-names></name><name><surname>Gracco</surname> <given-names>VL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Repetition suppression for speech processing in the associative occipital and parietal cortex of congenitally blind adults</article-title><source>PLoS One</source><volume>8</volume><elocation-id>e64553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0064553</pub-id><pub-id pub-id-type="pmid">23717628</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atilgan</surname> <given-names>H</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Structural neuroplasticity of the superior temporal plane in early and late blindness</article-title><source>Brain and Language</source><volume>170</volume><fpage>71</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2017.03.008</pub-id><pub-id pub-id-type="pmid">28426947</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Language and the infant brain</article-title><source>Journal of Communication Disorders</source><volume>32</volume><fpage>195</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/S0021-9924(99)00015-5</pub-id><pub-id pub-id-type="pmid">10466093</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Plasticity, Localization, and Language Development</chapter-title><person-group person-group-type="editor"><name><surname>Milbrath</surname> <given-names>C. onstance</given-names></name></person-group><source>Biol Knowl Revisit From Neurogenes to Psychogenes</source><volume>9351</volume><fpage>205</fpage><lpage>253</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname> <given-names>M</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Dodell-Feder</surname> <given-names>D</given-names></name><name><surname>Fedorenko</surname> <given-names>E</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Language processing in the occipital cortex of congenitally blind adults</article-title><source>PNAS</source><volume>108</volume><fpage>4429</fpage><lpage>4434</lpage><pub-id pub-id-type="doi">10.1073/pnas.1014818108</pub-id><pub-id pub-id-type="pmid">21368161</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname> <given-names>M</given-names></name><name><surname>Richardson</surname> <given-names>H</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>&quot;Visual&quot; cortex responds to spoken language in blind children</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11674</fpage><lpage>11681</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0634-15.2015</pub-id><pub-id pub-id-type="pmid">26290244</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence from blindness for a cognitively pluripotent cortex</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>637</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.06.003</pub-id><pub-id pub-id-type="pmid">28821345</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P</given-names></name><name><surname>Zilbovicius</surname> <given-names>M</given-names></name><name><surname>Crozier</surname> <given-names>S</given-names></name><name><surname>Thivard</surname> <given-names>L</given-names></name><name><surname>Fontaine</surname> <given-names>A</given-names></name><name><surname>Masure</surname> <given-names>MC</given-names></name><name><surname>Samson</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Lateralization of speech and auditory temporal processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>10</volume><fpage>536</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1162/089892998562834</pub-id><pub-id pub-id-type="pmid">9712682</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benetti</surname> <given-names>S</given-names></name><name><surname>van Ackeren</surname> <given-names>MJ</given-names></name><name><surname>Rabini</surname> <given-names>G</given-names></name><name><surname>Zonca</surname> <given-names>J</given-names></name><name><surname>Foa</surname> <given-names>V</given-names></name><name><surname>Baruffaldi</surname> <given-names>F</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Pavani</surname> <given-names>F</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Functional selectivity for face processing in the temporal voice area of early deaf individuals</article-title><source>PNAS</source><volume>114</volume><fpage>E6437</fpage><lpage>E6446</lpage><pub-id pub-id-type="doi">10.1073/pnas.1618287114</pub-id><pub-id pub-id-type="pmid">28652333</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berwick</surname> <given-names>RC</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name><name><surname>Chomsky</surname> <given-names>N</given-names></name><name><surname>Bolhuis</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evolution, brain, and the nature of language</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>89</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.12.002</pub-id><pub-id pub-id-type="pmid">23313359</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boemio</surname> <given-names>A</given-names></name><name><surname>Fromm</surname> <given-names>S</given-names></name><name><surname>Braun</surname> <given-names>A</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>389</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1038/nn1409</pub-id><pub-id pub-id-type="pmid">15723061</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname> <given-names>M</given-names></name><name><surname>De Tiège</surname> <given-names>X</given-names></name><name><surname>de Beeck</surname> <given-names>MO</given-names></name><name><surname>Ligot</surname> <given-names>N</given-names></name><name><surname>Paquier</surname> <given-names>P</given-names></name><name><surname>Van Bogaert</surname> <given-names>P</given-names></name><name><surname>Goldman</surname> <given-names>S</given-names></name><name><surname>Hari</surname> <given-names>R</given-names></name><name><surname>Jousmäki</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The pace of prosodic phrasing couples the listener's cortex to the reader's voice</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>314</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1002/hbm.21442</pub-id><pub-id pub-id-type="pmid">22392861</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bristow</surname> <given-names>D</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Mattout</surname> <given-names>J</given-names></name><name><surname>Soares</surname> <given-names>C</given-names></name><name><surname>Gliga</surname> <given-names>T</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Mangin</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hearing faces: how the infant brain matches the face it sees with the speech it hears</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>905</fpage><lpage>921</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21076</pub-id><pub-id pub-id-type="pmid">18702595</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brookes</surname> <given-names>H</given-names></name><name><surname>Slater</surname> <given-names>A</given-names></name><name><surname>Quinn</surname> <given-names>PC</given-names></name><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name><name><surname>Hayes</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Three-month-old infants learn arbitrary auditory-visual pairings between voices and faces</article-title><source>Infant and Child Development</source><volume>10</volume><fpage>75</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1002/icd.249</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname> <given-names>H</given-names></name><name><surname>Diamond</surname> <given-names>JB</given-names></name><name><surname>McDermott</surname> <given-names>KB</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dissociating cortical regions activated by semantic and phonological tasks: a FMRI study in blind and sighted people</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>1965</fpage><lpage>1982</lpage><pub-id pub-id-type="doi">10.1152/jn.00279.2003</pub-id><pub-id pub-id-type="pmid">12789013</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname> <given-names>H</given-names></name><name><surname>Sinclair</surname> <given-names>RJ</given-names></name><name><surname>Agato</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Recognition memory for Braille or spoken words: an fMRI study in early blind</article-title><source>Brain Research</source><volume>1438</volume><fpage>22</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2011.12.032</pub-id><pub-id pub-id-type="pmid">22251836</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname> <given-names>H</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Diamond</surname> <given-names>JB</given-names></name><name><surname>Raichle</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Adaptive changes in early and late blind: a FMRI study of verb generation to heard nouns</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>3359</fpage><lpage>3371</lpage><pub-id pub-id-type="doi">10.1152/jn.00129.2002</pub-id><pub-id pub-id-type="pmid">12466452</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visual cortex activity in early and late blind people</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>23</volume><fpage>4005</fpage><lpage>4011</lpage><pub-id pub-id-type="pmid">12764085</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical hierarchy turned on its head</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>657</fpage><lpage>658</lpage><pub-id pub-id-type="doi">10.1038/nn0703-657</pub-id><pub-id pub-id-type="pmid">12830152</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname> <given-names>GA</given-names></name><name><surname>Bullmore</surname> <given-names>ET</given-names></name><name><surname>Brammer</surname> <given-names>MJ</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>Williams</surname> <given-names>SC</given-names></name><name><surname>McGuire</surname> <given-names>PK</given-names></name><name><surname>Woodruff</surname> <given-names>PW</given-names></name><name><surname>Iversen</surname> <given-names>SD</given-names></name><name><surname>David</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Activation of auditory cortex during silent lipreading</article-title><source>Science</source><volume>276</volume><fpage>593</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1126/science.276.5312.593</pub-id><pub-id pub-id-type="pmid">9110978</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>C</given-names></name><name><surname>Trubanova</surname> <given-names>A</given-names></name><name><surname>Stillittano</surname> <given-names>S</given-names></name><name><surname>Caplier</surname> <given-names>A</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The natural statistics of audiovisual speech</article-title><source>PLoS Computational Biology</source><volume>5</volume><elocation-id>e1000436</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id><pub-id pub-id-type="pmid">19609344</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chomsky</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1976">1976</year><source>Reflections on Language</source></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>Albouy</surname> <given-names>G</given-names></name><name><surname>Vandewalle</surname> <given-names>G</given-names></name><name><surname>Voss</surname> <given-names>P</given-names></name><name><surname>Phillips</surname> <given-names>C</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Impact of blindness onset on the functional organization and the connectivity of the occipital cortex</article-title><source>Brain</source><volume>136</volume><fpage>2769</fpage><lpage>2783</lpage><pub-id pub-id-type="doi">10.1093/brain/awt176</pub-id><pub-id pub-id-type="pmid">23831614</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>de Heering</surname> <given-names>A</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name><name><surname>Lewis</surname> <given-names>TL</given-names></name><name><surname>Maurer</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Long-lasting crossmodal cortical reorganization triggered by brief postnatal visual deprivation</article-title><source>Current Biology</source><volume>25</volume><fpage>2379</fpage><lpage>2383</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.07.036</pub-id><pub-id pub-id-type="pmid">26299512</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Vandewalle</surname> <given-names>G</given-names></name><name><surname>Voss</surname> <given-names>P</given-names></name><name><surname>Albouy</surname> <given-names>G</given-names></name><name><surname>Charbonneau</surname> <given-names>G</given-names></name><name><surname>Lassonde</surname> <given-names>M</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization for auditory-spatial processing in the occipital cortex of congenitally blind humans</article-title><source>PNAS</source><volume>108</volume><fpage>4435</fpage><lpage>4440</lpage><pub-id pub-id-type="doi">10.1073/pnas.1013928108</pub-id><pub-id pub-id-type="pmid">21368198</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Voss</surname> <given-names>P</given-names></name><name><surname>Lassonde</surname> <given-names>M</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Cross-modal plasticity for the spatial processing of sounds in visually deprived subjects</article-title><source>Experimental Brain Research</source><volume>192</volume><fpage>343</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1553-z</pub-id><pub-id pub-id-type="pmid">18762928</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Hierarchical processing in spoken language comprehension</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>3423</fpage><lpage>3431</lpage><pub-id pub-id-type="pmid">12716950</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heer</surname> <given-names>WA</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hierarchical cortical organization of human speech processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6539</fpage><lpage>6557</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3267-16.2017</pub-id><pub-id pub-id-type="pmid">28588065</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deen</surname> <given-names>B</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name><name><surname>Bedny</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Occipital cortex of blind individuals is functionally coupled with executive control areas of frontal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>1633</fpage><lpage>1647</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00807</pub-id><pub-id pub-id-type="pmid">25803598</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Hertz-Pannier</surname> <given-names>L</given-names></name><name><surname>Dubois</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Nature and nurture in language acquisition: anatomical and functional brain-imaging studies in infants</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.05.011</pub-id><pub-id pub-id-type="pmid">16815562</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietrich</surname> <given-names>S</given-names></name><name><surname>Hertrich</surname> <given-names>I</given-names></name><name><surname>Ackermann</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultra-fast speech comprehension in blind subjects engages primary visual cortex, fusiform gyrus, and pulvinar - a functional magnetic resonance imaging (fMRI) study</article-title><source>BMC Neuroscience</source><volume>14</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-14-74</pub-id><pub-id pub-id-type="pmid">23879896</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical entrainment to continuous speech: functional roles and interpretations</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>311</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00311</pub-id><pub-id pub-id-type="pmid">24904354</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Yakobov</surname> <given-names>E</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Auditory motion in the sighted and blind: Early visual deprivation triggers a large-scale imbalance between auditory and &quot;visual&quot; brain regions</article-title><source>NeuroImage</source><volume>134</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.027</pub-id><pub-id pub-id-type="pmid">27107468</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Elman</surname> <given-names>JL</given-names></name><name><surname>Bates</surname> <given-names>EA</given-names></name><name><surname>Johnson</surname> <given-names>MH</given-names></name><name><surname>Karmiloff-Smith</surname> <given-names>A</given-names></name><name><surname>Parisi</surname> <given-names>D</given-names></name><name><surname>Plunkett</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Rethinking Innateness: Connectionist Perspective on Development</source></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falchier</surname> <given-names>A</given-names></name><name><surname>Clavagnier</surname> <given-names>S</given-names></name><name><surname>Barone</surname> <given-names>P</given-names></name><name><surname>Kennedy</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>22</volume><fpage>5749</fpage><lpage>5759</lpage><pub-id pub-id-type="pmid">12097528</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonteneau</surname> <given-names>E</given-names></name><name><surname>Bozic</surname> <given-names>M</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Brain network connectivity during language comprehension: interacting linguistic and perceptual subsystems</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3962</fpage><lpage>3976</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu283</pub-id><pub-id pub-id-type="pmid">25452574</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname> <given-names>BL</given-names></name><name><surname>Ince</surname> <given-names>RAA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title><source>eLife</source><volume>6</volume><elocation-id>e24763</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.24763</pub-id><pub-id pub-id-type="pmid">28590903</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Hoogenboom</surname> <given-names>N</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Schyns</surname> <given-names>P</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Belin</surname> <given-names>P</given-names></name><name><surname>Garrod</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLoS Biology</source><volume>11</volume><fpage>e1001752</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id><pub-id pub-id-type="pmid">24391472</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kujala</surname> <given-names>J</given-names></name><name><surname>Hamalainen</surname> <given-names>M</given-names></name><name><surname>Timmermann</surname> <given-names>L</given-names></name><name><surname>Schnitzler</surname> <given-names>A</given-names></name><name><surname>Salmelin</surname> <given-names>R</given-names></name><name><surname>Hämäläinen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic imaging of coherent sources: Studying neural interactions in the human brain</article-title><source>PNAS</source><volume>98</volume><fpage>694</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1073/pnas.98.2.694</pub-id><pub-id pub-id-type="pmid">11209067</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannagan</surname> <given-names>T</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Origins of the specialization for letters and numbers in ventral occipitotemporal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>374</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.006</pub-id><pub-id pub-id-type="pmid">26072689</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Andric</surname> <given-names>M</given-names></name><name><surname>Atilgan</surname> <given-names>H</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Congenital blindness is associated with large-scale reorganization of anatomical networks</article-title><source>NeuroImage</source><volume>128</volume><fpage>362</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.048</pub-id><pub-id pub-id-type="pmid">26767944</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Han</surname> <given-names>Z</given-names></name><name><surname>Lin</surname> <given-names>N</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Selectivity for large nonmanipulable objects in scene-selective visual cortex does not require visual experience</article-title><source>NeuroImage</source><volume>79</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.051</pub-id><pub-id pub-id-type="pmid">23624496</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horowitz-Kraus</surname> <given-names>T</given-names></name><name><surname>Grainger</surname> <given-names>M</given-names></name><name><surname>DiFrancesco</surname> <given-names>M</given-names></name><name><surname>Vannest</surname> <given-names>J</given-names></name><name><surname>Holland</surname> <given-names>SK</given-names></name><collab>CMIND Authorship Consortium</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Right is not always wrong: DTI and fMRI evidence for the reliance of reading comprehension on language-comprehension networks in the right hemisphere</article-title><source>Brain Imaging and Behavior</source><volume>9</volume><fpage>19</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1007/s11682-014-9341-9</pub-id><pub-id pub-id-type="pmid">25515348</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>F</given-names></name><name><surname>Stecker</surname> <given-names>GC</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name><name><surname>Fine</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Early blindness results in developmental plasticity for auditory motion processing within auditory and occipital cortex</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>324</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00324</pub-id><pub-id pub-id-type="pmid">27458357</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Montemurro</surname> <given-names>MA</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spike-phase coding boosts and stabilizes information carried by spatial and temporal spike patterns</article-title><source>Neuron</source><volume>61</volume><fpage>597</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.008</pub-id><pub-id pub-id-type="pmid">19249279</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual modulation of neurons in auditory cortex</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>1560</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm187</pub-id><pub-id pub-id-type="pmid">18180245</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>SJ</given-names></name><name><surname>Ince</surname> <given-names>RA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Irregular speech rate dissociates auditory cortical entrainment, evoked responses, and frontal alpha</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14691</fpage><lpage>14701</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2243-15.2015</pub-id><pub-id pub-id-type="pmid">26538641</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinge</surname> <given-names>C</given-names></name><name><surname>Eippert</surname> <given-names>F</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Corticocortical connections mediate primary visual cortex responses to auditory stimulation in the blind</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12798</fpage><lpage>12805</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2384-10.2010</pub-id><pub-id pub-id-type="pmid">20861384</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname> <given-names>PK</given-names></name><name><surname>Meltzoff</surname> <given-names>AN</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The bimodal perception of speech in infancy</article-title><source>Science</source><volume>218</volume><fpage>1138</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1126/science.7146899</pub-id><pub-id pub-id-type="pmid">7146899</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kushnerenko</surname> <given-names>E</given-names></name><name><surname>Teinonen</surname> <given-names>T</given-names></name><name><surname>Volein</surname> <given-names>A</given-names></name><name><surname>Csibra</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Electrophysiological evidence of illusory audiovisual speech percept in human infants</article-title><source>PNAS</source><volume>105</volume><fpage>11442</fpage><lpage>11445</lpage><pub-id pub-id-type="doi">10.1073/pnas.0804275105</pub-id><pub-id pub-id-type="pmid">18682564</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lachaux</surname> <given-names>JP</given-names></name><name><surname>Rodriguez</surname> <given-names>E</given-names></name><name><surname>Martinerie</surname> <given-names>J</given-names></name><name><surname>Varela</surname> <given-names>FJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Measuring phase synchrony in brain signals</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>194</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:4&lt;194::AID-HBM4&gt;3.0.CO;2-C</pub-id><pub-id pub-id-type="pmid">10619414</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Chen</surname> <given-names>CM</given-names></name><name><surname>O'Connell</surname> <given-names>MN</given-names></name><name><surname>Mills</surname> <given-names>A</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex</article-title><source>Neuron</source><volume>53</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.12.011</pub-id><pub-id pub-id-type="pmid">17224408</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Supèr</surname> <given-names>H</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name><name><surname>Feedforward</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80042-1</pub-id><pub-id pub-id-type="pmid">9751656</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lane</surname> <given-names>C</given-names></name><name><surname>Kanjlia</surname> <given-names>S</given-names></name><name><surname>Omaki</surname> <given-names>A</given-names></name><name><surname>Bedny</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>&quot;Visual&quot; cortex of congenitally blind adults responds to syntactic movement</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>12859</fpage><lpage>12868</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1256-15.2015</pub-id><pub-id pub-id-type="pmid">26377472</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The decline of cross-species intersensory perception in human infants</article-title><source>PNAS</source><volume>103</volume><fpage>6771</fpage><lpage>6774</lpage><pub-id pub-id-type="doi">10.1073/pnas.0602027103</pub-id><pub-id pub-id-type="pmid">16618919</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name><name><surname>Hansen-Tift</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Infants deploy selective attention to the mouth of a talking face when learning speech</article-title><source>PNAS</source><volume>109</volume><fpage>1431</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1073/pnas.1114783109</pub-id><pub-id pub-id-type="pmid">22307596</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name><name><surname>Leo</surname> <given-names>I</given-names></name><name><surname>Simion</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Intersensory perception at birth: newborns match nonhuman primate faces and voices</article-title><source>Infancy</source><volume>15</volume><fpage>46</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1111/j.1532-7078.2009.00005.x</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Infants' perception of the audible, visible, and bimodal attributes of multimodal syllables</article-title><source>Child Development</source><volume>71</volume><fpage>1241</fpage><lpage>1257</lpage><pub-id pub-id-type="doi">10.1111/1467-8624.00226</pub-id><pub-id pub-id-type="pmid">11108094</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Infant perception of audio-visual speech synchrony</article-title><source>Developmental Psychology</source><volume>46</volume><fpage>66</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1037/a0015579</pub-id><pub-id pub-id-type="pmid">20053007</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liljeström</surname> <given-names>M</given-names></name><name><surname>Kujala</surname> <given-names>J</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Salmelin</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuromagnetic localization of rhythmic activity in the human brain: a comparison of three methods</article-title><source>NeuroImage</source><volume>25</volume><fpage>734</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.11.034</pub-id><pub-id pub-id-type="pmid">15808975</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>Z</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation</article-title><source>PLoS Biology</source><volume>8</volume><elocation-id>e1000445</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000445</pub-id><pub-id pub-id-type="pmid">20711473</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id><pub-id pub-id-type="pmid">17582338</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname> <given-names>M</given-names></name><name><surname>Amaro</surname> <given-names>E</given-names></name><name><surname>Calvert</surname> <given-names>GA</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>David</surname> <given-names>AS</given-names></name><name><surname>McGuire</surname> <given-names>P</given-names></name><name><surname>Williams</surname> <given-names>SC</given-names></name><name><surname>Woll</surname> <given-names>B</given-names></name><name><surname>Brammer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Silent speechreading in the absence of scanner noise: an event-related fMRI study</article-title><source>Neuroreport</source><volume>11</volume><fpage>1729</fpage><lpage>1733</lpage><pub-id pub-id-type="doi">10.1097/00001756-200006050-00026</pub-id><pub-id pub-id-type="pmid">10852233</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname> <given-names>M</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>Calvert</surname> <given-names>GA</given-names></name><name><surname>McGuire</surname> <given-names>PK</given-names></name><name><surname>David</surname> <given-names>AS</given-names></name><name><surname>Suckling</surname> <given-names>J</given-names></name><name><surname>Andrew</surname> <given-names>C</given-names></name><name><surname>Woll</surname> <given-names>B</given-names></name><name><surname>Brammer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dispersed activation in the left temporal cortex for speech-reading in congenitally deaf people</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>268</volume><fpage>451</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1098/rspb.2000.0393</pub-id><pub-id pub-id-type="pmid">11296856</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title><source>Nature</source><volume>485</volume><fpage>233</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature11020</pub-id><pub-id pub-id-type="pmid">22522927</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moos</surname> <given-names>A</given-names></name><name><surname>Trouvain</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Comprehension of ultra-fast speech–blind vs.’normally hearing’ persons</article-title><conf-name><italic>Proc 16th Int Congr Phonetic Sci</italic></conf-name><fpage>677</fpage><lpage>680</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>MM</given-names></name><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Wallace</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multisensory processes: a balancing act across the lifespan</article-title><source>Trends in Neurosciences</source><volume>39</volume><fpage>567</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2016.05.003</pub-id><pub-id pub-id-type="pmid">27282408</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narain</surname> <given-names>C</given-names></name><name><surname>Scott</surname> <given-names>SK</given-names></name><name><surname>Wise</surname> <given-names>RJ</given-names></name><name><surname>Rosen</surname> <given-names>S</given-names></name><name><surname>Leff</surname> <given-names>A</given-names></name><name><surname>Iversen</surname> <given-names>SD</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Defining a left-lateralized response specific to intelligible speech using fMRI</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1362</fpage><lpage>1368</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg083</pub-id><pub-id pub-id-type="pmid">14615301</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>AR</given-names></name><name><surname>Fava</surname> <given-names>EE</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of interindividual differences in children's audiovisual speech perception</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13963</fpage><lpage>13971</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2605-11.2011</pub-id><pub-id pub-id-type="pmid">21957257</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niemeyer</surname> <given-names>W</given-names></name><name><surname>Starlinger</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Do the blind hear better? investigations on auditory processing in congenital or early acquired blindness II. central functions</article-title><source>International Journal of Audiology</source><volume>20</volume><fpage>510</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.3109/00206098109072719</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname> <given-names>G</given-names></name><name><surname>Ziehe</surname> <given-names>A</given-names></name><name><surname>Nikulin</surname> <given-names>VV</given-names></name><name><surname>Schlögl</surname> <given-names>A</given-names></name><name><surname>Krämer</surname> <given-names>N</given-names></name><name><surname>Brismar</surname> <given-names>T</given-names></name><name><surname>Müller</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Robustly estimating the flow direction of information in complex physical systems</article-title><source>Physical Review Letters</source><volume>100</volume><elocation-id>234101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.100.234101</pub-id><pub-id pub-id-type="pmid">18643502</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Physics in Medicine and Biology</source><volume>48</volume><fpage>3637</fpage><lpage>3652</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id><pub-id pub-id-type="pmid">14680264</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname> <given-names>J</given-names></name><name><surname>Eisner</surname> <given-names>F</given-names></name><name><surname>Kotz</surname> <given-names>SA</given-names></name><name><surname>a</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bilateral speech comprehension reflects differential sensitivity to spectral and temporal features</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>8116</fpage><lpage>8123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1290-08.2008</pub-id><pub-id pub-id-type="pmid">18685036</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname> <given-names>K</given-names></name><name><surname>Rong</surname> <given-names>F</given-names></name><name><surname>Venezia</surname> <given-names>J</given-names></name><name><surname>Matchin</surname> <given-names>W</given-names></name><name><surname>Hsieh</surname> <given-names>IH</given-names></name><name><surname>Saberi</surname> <given-names>K</given-names></name><name><surname>Serences</surname> <given-names>JT</given-names></name><name><surname>Hickok</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hierarchical organization of human auditory cortex: evidence from acoustic invariance in the response to intelligible speech</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2486</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp318</pub-id><pub-id pub-id-type="pmid">20100898</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>H</given-names></name><name><surname>Ince</surname> <given-names>RA</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>H</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Lip movements entrain the observers' low-frequency brain oscillations to facilitate speech intelligibility</article-title><source>eLife</source><volume>5</volume><elocation-id>e14521</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14521</pub-id><pub-id pub-id-type="pmid">27146891</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname> <given-names>ML</given-names></name><name><surname>Werker</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Two-month-old infants match phonetic information in lips and voice</article-title><source>Developmental Science</source><volume>6</volume><fpage>191</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1111/1467-7687.00271</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Lu</surname> <given-names>X</given-names></name><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tool selectivity in left occipitotemporal cortex develops without vision</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1225</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00411</pub-id><pub-id pub-id-type="pmid">23647514</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural oscillations carry speech rhythm through to comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1378</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs118</pub-id><pub-id pub-id-type="pmid">22610394</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hierarchical processing for speech in human auditory cortex and beyond</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><fpage>2486</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2010.00051</pub-id><pub-id pub-id-type="pmid">20661456</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pellegrino</surname> <given-names>F</given-names></name><name><surname>Coupé</surname> <given-names>C</given-names></name><name><surname>Marsico</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Across-language perspective on speech information rate</article-title><source>Language</source><volume>87</volume><fpage>539</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1353/lan.2011.0057</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietrini</surname> <given-names>P</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ricciardi</surname> <given-names>E</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Wu</surname> <given-names>WH</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Guazzelli</surname> <given-names>M</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Beyond sensory images: Object-based representation in the human ventral pathway</article-title><source>PNAS</source><volume>101</volume><fpage>5658</fpage><lpage>5663</lpage><pub-id pub-id-type="doi">10.1073/pnas.0400707101</pub-id><pub-id pub-id-type="pmid">15064396</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Idsardi</surname> <given-names>WJ</given-names></name><name><surname>van Wassenhove</surname> <given-names>V</given-names></name><name><surname>Wassenhove</surname> <given-names>van</given-names></name> </person-group><year iso-8601-date="2008">2008</year><article-title>Speech perception at the interface of neurobiology and linguistics</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>363</volume><fpage>1071</fpage><lpage>1086</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2160</pub-id><pub-id pub-id-type="pmid">17890189</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as ‘asymmetric sampling in time’</article-title><source>Speech Communication</source><volume>41</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00107-3</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirier</surname> <given-names>C</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Scheiber</surname> <given-names>C</given-names></name><name><surname>Renier</surname> <given-names>L</given-names></name><name><surname>Vanlierde</surname> <given-names>A</given-names></name><name><surname>Tranduy</surname> <given-names>D</given-names></name><name><surname>Veraart</surname> <given-names>C</given-names></name><name><surname>De Volder</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Auditory motion perception activates visual motion areas in early blind subjects</article-title><source>NeuroImage</source><volume>31</volume><fpage>279</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.11.036</pub-id><pub-id pub-id-type="pmid">16443376</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pons</surname> <given-names>F</given-names></name><name><surname>Lewkowicz</surname> <given-names>DJ</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name><name><surname>Sebastián-Gallés</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Narrowing of intersensory speech perception in infancy</article-title><source>PNAS</source><volume>106</volume><fpage>10598</fpage><lpage>10602</lpage><pub-id pub-id-type="doi">10.1073/pnas.0904134106</pub-id><pub-id pub-id-type="pmid">19541648</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname> <given-names>L</given-names></name><name><surname>Szwed</surname> <given-names>M</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A ventral visual stream reading center independent of visual experience</article-title><source>Current Biology</source><volume>21</volume><fpage>363</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.01.040</pub-id><pub-id pub-id-type="pmid">21333539</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricciardi</surname> <given-names>E</given-names></name><name><surname>Vanello</surname> <given-names>N</given-names></name><name><surname>Sani</surname> <given-names>L</given-names></name><name><surname>Gentili</surname> <given-names>C</given-names></name><name><surname>Scilingo</surname> <given-names>EP</given-names></name><name><surname>Landini</surname> <given-names>L</given-names></name><name><surname>Guazzelli</surname> <given-names>M</given-names></name><name><surname>Bicchi</surname> <given-names>A</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007a</year><article-title>The effect of visual experience on the development of functional architecture in hMT+</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2933</fpage><lpage>2939</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm018</pub-id><pub-id pub-id-type="pmid">17372275</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricciardi</surname> <given-names>E</given-names></name><name><surname>Vanello</surname> <given-names>N</given-names></name><name><surname>Sani</surname> <given-names>L</given-names></name><name><surname>Gentili</surname> <given-names>C</given-names></name><name><surname>Scilingo</surname> <given-names>EP</given-names></name><name><surname>Landini</surname> <given-names>L</given-names></name><name><surname>Guazzelli</surname> <given-names>M</given-names></name><name><surname>Bicchi</surname> <given-names>A</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007b</year><article-title>The effect of visual experience on the development of functional architecture in hMT+</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2933</fpage><lpage>2939</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm018</pub-id><pub-id pub-id-type="pmid">17372275</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodd</surname> <given-names>JM</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The neural mechanisms of speech comprehension: fMRI studies of semantic ambiguity</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1261</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi009</pub-id><pub-id pub-id-type="pmid">15635062</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodd</surname> <given-names>JM</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The role of domain-general frontal systems in language comprehension: evidence from dual-task interference and semantic ambiguity</article-title><source>Brain and Language</source><volume>115</volume><fpage>182</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2010.07.005</pub-id><pub-id pub-id-type="pmid">20709385</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblum</surname> <given-names>LD</given-names></name><name><surname>Schmuckler</surname> <given-names>MA</given-names></name><name><surname>Johnson</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The McGurk effect in infants</article-title><source>Perception &amp; Psychophysics</source><volume>59</volume><fpage>347</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.3758/BF03211902</pub-id><pub-id pub-id-type="pmid">9136265</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname> <given-names>LA</given-names></name><name><surname>Saint-Amour</surname> <given-names>D</given-names></name><name><surname>Leavitt</surname> <given-names>VM</given-names></name><name><surname>Javitt</surname> <given-names>DC</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Do you see what I am saying? Exploring visual enhancement of speech comprehension in noisy environments</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>1147</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl024</pub-id><pub-id pub-id-type="pmid">16785256</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Röder</surname> <given-names>B</given-names></name><name><surname>Stock</surname> <given-names>O</given-names></name><name><surname>Bien</surname> <given-names>S</given-names></name><name><surname>Neville</surname> <given-names>H</given-names></name><name><surname>Rösler</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Speech processing activates visual cortex in congenitally blind humans</article-title><source>European Journal of Neuroscience</source><volume>16</volume><fpage>930</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.02147.x</pub-id><pub-id pub-id-type="pmid">12372029</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname> <given-names>N</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Grafman</surname> <given-names>J</given-names></name><name><surname>Ibañez</surname> <given-names>V</given-names></name><name><surname>Deiber</surname> <given-names>MP</given-names></name><name><surname>Dold</surname> <given-names>G</given-names></name><name><surname>Hallett</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Activation of the primary visual cortex by Braille reading in blind subjects</article-title><source>Nature</source><volume>380</volume><fpage>526</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1038/380526a0</pub-id><pub-id pub-id-type="pmid">8606771</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schepers</surname> <given-names>IM</given-names></name><name><surname>Hipp</surname> <given-names>JF</given-names></name><name><surname>Schneider</surname> <given-names>TR</given-names></name><name><surname>Röder</surname> <given-names>B</given-names></name><name><surname>Engel</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functionally specific oscillatory activity correlates between visual and auditory cortex in the blind</article-title><source>Brain</source><volume>135</volume><fpage>922</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1093/brain/aws014</pub-id><pub-id pub-id-type="pmid">22366801</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Kajikawa</surname> <given-names>Y</given-names></name><name><surname>Partan</surname> <given-names>S</given-names></name><name><surname>Puce</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuronal oscillations and visual amplification of speech</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.01.002</pub-id><pub-id pub-id-type="pmid">18280772</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title><source>Trends in Neurosciences</source><volume>32</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.09.012</pub-id><pub-id pub-id-type="pmid">19012975</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>JL</given-names></name><name><surname>Savariaux</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>No, there is no 150 ms lead of visual speech on auditory speech, but a range of audiovisual asynchronies varying from small audio lead to large audio lag</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003743</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003743</pub-id><pub-id pub-id-type="pmid">25079216</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>SK</given-names></name><name><surname>Blank</surname> <given-names>CC</given-names></name><name><surname>Rosen</surname> <given-names>S</given-names></name><name><surname>Wise</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title><source>Brain</source><volume>123</volume><fpage>2400</fpage><lpage>2406</lpage><pub-id pub-id-type="doi">10.1093/brain/123.12.2400</pub-id><pub-id pub-id-type="pmid">11099443</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>RV</given-names></name><name><surname>Zeng</surname> <given-names>FG</given-names></name><name><surname>Kamath</surname> <given-names>V</given-names></name><name><surname>Wygonski</surname> <given-names>J</given-names></name><name><surname>Ekelid</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Penny</surname> <given-names>WD</given-names></name><name><surname>Moran</surname> <given-names>RJ</given-names></name><name><surname>den Ouden</surname> <given-names>HEM</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ten simple rules for dynamic causal modeling</article-title><source>NeuroImage</source><volume>49</volume><fpage>3099</fpage><lpage>3109</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.015</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Ovadia-Caro</surname> <given-names>S</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Margulies</surname> <given-names>DS</given-names></name><name><surname>Villringer</surname> <given-names>A</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional connectivity of visual cortex in the blind follows retinotopic organization principles</article-title><source>Brain</source><volume>138</volume><fpage>1679</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1093/brain/awv083</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szymanski</surname> <given-names>FD</given-names></name><name><surname>Rabinowitz</surname> <given-names>NC</given-names></name><name><surname>Magri</surname> <given-names>C</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The laminar and temporal structure of stimulus information in the phase of field potentials of auditory cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>15787</fpage><lpage>15801</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1416-11.2011</pub-id><pub-id pub-id-type="pmid">22049422</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname> <given-names>BD</given-names></name><name><surname>van Drongelen</surname> <given-names>W</given-names></name><name><surname>Yuchtman</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wassenhove</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech through ears and eyes: interfacing the senses with the supramodal brain</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>388</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00388</pub-id><pub-id pub-id-type="pmid">23874309</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vouloumanos</surname> <given-names>A</given-names></name><name><surname>Druhen</surname> <given-names>MJ</given-names></name><name><surname>Hauser</surname> <given-names>MD</given-names></name><name><surname>Huizink</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Five-month-old infants' identification of the sources of vocalizations</article-title><source>PNAS</source><volume>106</volume><fpage>18867</fpage><lpage>18872</lpage><pub-id pub-id-type="doi">10.1073/pnas.0906049106</pub-id><pub-id pub-id-type="pmid">19846770</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weeks</surname> <given-names>R</given-names></name><name><surname>Horwitz</surname> <given-names>B</given-names></name><name><surname>Aziz-Sultan</surname> <given-names>A</given-names></name><name><surname>Tian</surname> <given-names>B</given-names></name><name><surname>Wessinger</surname> <given-names>CM</given-names></name><name><surname>Cohen</surname> <given-names>LG</given-names></name><name><surname>Hallett</surname> <given-names>M</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A positron emission tomographic study of auditory localization in the congenitally blind</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>20</volume><fpage>2664</fpage><lpage>2672</lpage><pub-id pub-id-type="pmid">10729347</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weikum</surname> <given-names>WM</given-names></name><name><surname>Vouloumanos</surname> <given-names>A</given-names></name><name><surname>Navarra</surname> <given-names>J</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name><name><surname>Sebastián-Gallés</surname> <given-names>N</given-names></name><name><surname>Werker</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual language discrimination in infancy</article-title><source>Science</source><volume>316</volume><elocation-id>1159</elocation-id><pub-id pub-id-type="doi">10.1126/science.1137686</pub-id><pub-id pub-id-type="pmid">17525331</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname> <given-names>E</given-names></name><name><surname>Cogan</surname> <given-names>GB</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Visual input enhances selective speech envelope tracking in auditory cortex at a &quot;cocktail party&quot;</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1417</fpage><lpage>1426</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3675-12.2013</pub-id><pub-id pub-id-type="pmid">23345218</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname> <given-names>EM</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Bickel</surname> <given-names>S</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Schevon</surname> <given-names>CA</given-names></name><name><surname>McKhann</surname> <given-names>GM</given-names></name><name><surname>Goodman</surname> <given-names>RR</given-names></name><name><surname>Emerson</surname> <given-names>R</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a &quot;cocktail party&quot;</article-title><source>Neuron</source><volume>77</volume><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id><pub-id pub-id-type="pmid">23473326</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname> <given-names>B</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective perceptual phase entrainment to speech rhythm in the absence of spectral energy fluctuations</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>1954</fpage><lpage>1964</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-14.2015</pub-id><pub-id pub-id-type="pmid">25653354</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31640.010</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Neuronal populations in the occipital cortex of the blind synchronize to the temporal dynamics of speech&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Andrew King as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Elana Zion Golumbic (Reviewer #2); Yanchao Bi (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This paper examines the role of the deprived visual cortex in the processing of language in blind individuals. Using magnetoencephalography, the authors report that, compared to individuals who could see, early-blind subjects show more synchronized brain responses within the calcarine sulcus to the temporal dynamics of speech. Intriguingly, they observed enhanced synchronization to the intelligibility of speech in the region of the primary visual cortex, and changes in the flow of information between areas of the occipital and temporal cortex that are sensitive to speech intelligibility. The reviewers agreed that this is an important topic and that this is a timely study that builds nicely on previous work on the neural tracking of speech and in the domain of language-system plasticity in the blind. However, various issues were identified with the analysis and presentation of the data, which have implications for the conclusions made by the authors.</p><p>Essential revisions:</p><p>1) The authors offer up a compelling narrative, but do not address the fundamental question of why blind participants need to recruit visual cortices for speech comprehension. What is the added functional value of having a more expanded brain network? The manuscript provides no explanation for this central aspect of the results. This needs to be addressed in the Discussion.</p><p>2) The reviewers found your description of the data analysis confusing and inadequate. Please be more specific about exactly which group or interaction effects are used as the marker for sensory processing. If this is based on the interaction between group and speech intelligibility in the calcarine sulcus, as assumed by one of the reviewers, other contrasts, e.g., the main effect of group (do blind and sighted subjects differ in terms of the coherence effects?) should also be reported. Given that the three conditions were matched on speech envelope properties, wouldn't interaction with intelligibility actually reflect top down modulation, which is presumably from lexical/semantic/syntactic properties rather than sensory processes?</p><p>3) There are several conflicting sentences in the Results, which make it unclear whether synchronization is present in both sighted and early-blind individuals, with the two groups differing in the degree of tracking, or is present only in the early blind. From <xref ref-type="fig" rid="fig1">Figure 1J</xref> it seems that the sighted did in fact show both a general coherence effect (at least for the 8 and 1 channel conditions) and an intelligibility effect (negative correlation). Demonstrating clearly whether the temporal locking effects are present in the sighted subjects and how exactly the early-blind group behaves in comparison is important because this directly links to the conclusions drawn in the paper. It appears from this figure that the functional role of the calcarine sulcus in synchronizing to speech in these populations may need reconsideration.</p><p>4) The reviewers pointed out the analysis of source reconstructions on two separate frequency windows that were largely overlapping needs clearer explanation and justification.</p><p>5) Another issue concerns the presentation of the same story clip in 3 different versions to each participant. Again, this requires clearer explanation and justification.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31640.011</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>This paper examines the role of the deprived visual cortex in the processing of language in blind individuals. Using magnetoencephalography, the authors report that, compared to individuals who could see, early-blind subjects show more synchronized brain responses within the calcarine sulcus to the temporal dynamics of speech. Intriguingly, they observed enhanced synchronization to the intelligibility of speech in the region of the primary visual cortex, and changes in the flow of information between areas of the occipital and temporal cortex that are sensitive to speech intelligibility. The reviewers agreed that this is an important topic and that this is a timely study that builds nicely on previous work on the neural tracking of speech and in the domain of language-system plasticity in the blind. However, various issues were identified with the analysis and presentation of the data, which have implications for the conclusions made by the authors.</p><p>Essential revisions:</p><p>1) The authors offer up a compelling narrative, but do not address the fundamental question of why blind participants need to recruit visual cortices for speech comprehension. What is the added functional value of having a more expanded brain network? The manuscript provides no explanation for this central aspect of the results. This needs to be addressed in the Discussion.</p></disp-quote><p>Why crossmodal plasticity arises in the brain of blind individuals is a crucial question that the field is currently trying to address. It is a very complex question, however, since the underlying mechanisms governing the expression of crossmodal plasticity is the result of a complex interplay between intrinsic organizational principles and experience-dependent influences on these intrinsic neurophysiological constraints.</p><p>One possibility that has been proposed in the literature is that the reorganization observed in the occipital cortex of blind people would support enhanced perceptual abilities for the reorganized function (De Villers-Sidani et al., 2016). The rational is simple: since an extension in the cortical territories underlying a computation is observed in the blind, this extension may support enhanced compensatory behavior. For instance, it has been demonstrated that early blind individuals with the higher auditory localization abilities (Gougoux et al., 2005) or superior verbal memory (Amedi et al., 2003) also show higher occipital activity levels. Moreover, occipital cortical thickness predicts performance on pitch and musical tasks in blind individuals (Voss and Zatorre, 2012). Similarly, in early deaf humans the higher the recruitment of temporal regions for face processing the better they are at processing faces (Benetti et al., 2017) and temporarily deactivating the functioning of crossmodally reorganized regions of the temporal cortex of deaf cats also alleviates their superior performance in visuo-spatial processing (Lomber et al., 2010). All-together these studies suggest that the crossmodal recruitment of sensory deprived regions may induce enhanced abilities in the remaining senses.</p><p>In relation to the topic of the current study, having more cortical tissue devoted to sentence processing and understanding could potentially support enhanced sentence comprehension. Previous studies have indeed demonstrated that blind people have enhanced speech discrimination in noisy environments (Niemeyer and Starlinger, 1981) and the capability to understand speech displayed at a much faster rate than sighted persons (sighted listeners at rates between 9-14 syllables/s and to blind listeners between 17-22 syllables /s; (Moos and Trouvain, 2007) Crucially, listening to intelligible ultra-fast speech (compared to reverse speech) engage enhanced activity in the right primary visual cortex in early and late blind individuals when compared to sighted controls; and activity in this region correlated with individual ultra-fast speech perception skills (Dietrich et al., 2013). These results raise the interesting possibility that the engagement of right V1 in the analysis of the speech envelope as demonstrated in our study may support the enhanced encoding of early supra-segmental aspects of the speech signal, supporting the ability to understand ultra-fast speech signal. Our behavioral task (speech comprehension) did not however create enough between-subject variability to assess directly this link between the reorganized occipital cortex and speech comprehension (performance was almost at ceiling in the Nat and 8-chan condition but at chance in the 1-chan condition, as targeted- see <xref ref-type="table" rid="table1">table 1</xref>).</p><p>However, it is important to note that linking brain activity and behavior is not a trivial issue. Many studies did not find a direct link between crossmodal reorganization and non-visual processing. More generally, since a behavioral outcome is the end product of a complex interactive process between several brain regions, linking the role of one region in isolation (e.g. the reorganized region in the occipital cortex of blind people) to behavior (e.g. performance) in a multifaced task is not straightforward. The absence of a direct relation between behavior, e.g. speech processing, and crossmodal plasticity could be explained, for instance, by the fact that this complex process will be supported by additional networks than the reorganized ones. Interestingly, recent studies have shown that early visual deprivation triggers a game of “balance” between the brain systems typically dedicated to a specific process and the reorganized occipital cortex (Dormal et al., 2016). It has indeed been proposed that early visual deprivation triggers a redeployment mechanism that would reallocate part of the processing typically implemented in the preserved networks (i.e. the temporal or frontal cortices for speech processing) to the occipital cortex deprived of its most salient input (vision). Two recent studies using multivoxel pattern analysis (MVPA) showed that the ability to decode the different auditory motion stimuli was enhanced in hMT+ (a region typically involved in visual motion in sighted) of early blind while an enhanced decoding accuracy was observed in the sighted group in the planum temporale (Dormal et al., 2016; Jiang et al., 2016). Moreover, Bedny and collaborators (2015) reported enhanced activation of occipital cortex and a simultaneous deactivation of prefrontal regions during a linguistic task in blind children. The authors suggested that the increased involvement of the occipital cortex might decrease the pressure on the prefrontal areas to specialize for language. Interestingly, a Transcranial Magnetic Stimulation (TMS) study reported a reduced disruptive effect of TMS applied over the left inferior prefrontal cortex during linguistic tasks, while TMS application over the occipital had more effects in early blind compared to sighted people (Amedi et al., 2004). This evidence supports the idea that brain regions normally recruited for specific tasks in sighted might become less essential in EB in the case they concomitantly recruit occipital regions for the same task. An important open question for future research therefore concerns the relative behavioral contribution of occipital and perisylvian cortex to speech understanding.</p><p>We now have summarized these arguments in the revised Discussion section of our manuscript.</p><disp-quote content-type="editor-comment"><p>2) The reviewers found your description of the data analysis confusing and inadequate. Please be more specific about exactly which group or interaction effects are used as the marker for sensory processing. If this is based on the interaction between group and speech intelligibility in the calcarine sulcus, as assumed by one of the reviewers, other contrasts, e.g., the main effect of group (do blind and sighted subjects differ in terms of the coherence effects?) should also be reported. Given that the three conditions were matched on speech envelope properties, wouldn't interaction with intelligibility actually reflect top down modulation, which is presumably from lexical/semantic/syntactic properties rather than sensory processes?</p></disp-quote><p>We thank the reviewers for pointing out that our previous description of the data analysis was not clear enough. We now have thoroughly modified our data analysis section in order to make it more streamlined. We clarify here, in brief, the specific points raised by the reviewers.</p><p>The marker for sensory processing of speech is the cerebro-acoustic coherence that is independent of the intelligibility of speech. Therefore, the regions that are solely sensitive to the acoustic information of our signal (e.g. variation of acoustic energy across time, at the syllabic rate) are tracking the signal envelope for the unaltered speech segments but also for the two vocoded conditions, including the 1-chanel vocoding condition during which speech is not intelligible. <xref ref-type="fig" rid="fig1">Figure 1F</xref> therefore shows the regions that synchronize most strongly to the acoustic properties of speech independently of the group (sighted and blind) and independently of the condition of presentation (Natural speech, 8 channels vocoding and 1 channel vocoding). <xref ref-type="fig" rid="fig1">Figure 1H</xref> however shows regions where the cerebro-acoustic coherence is higher in the blind than in the sighted, but still independently of condition presentation. This is the effect of group. These regions therefore reorganize in the blind to track acoustic stimulation but do not necessarily show an effect of intelligibility of the speech signal. It is certainly the case that phase-locked responses to sensory stimuli are not unique to human speech comprehension, as evidenced by oscillatory responses to simple auditory stimuli in non-human primates (Lakatos et al., 2007; Schroeder et al., 2008). Might there be additional information in the speech signal that affects the phase-locked cortical response to spoken language in the occipital cortex of the blind? This is exactly what we target with the group-level effect of blindness. The interaction effect we observed in calcarine sulcus (CS) highlights that synchronization to intelligible speech is different in the blind versus sighted. (see <xref ref-type="fig" rid="fig1">Figure 1I</xref>). As such, the enhanced neural phase-locking observed in the calcarine sulcus of early blind individuals is not solely driven by changes in the acoustic cue of the auditory stimuli, but also reflects cortical encoding and processing of the speech signal.</p><p>As we now discuss at length in the manuscript, the calcarine sulcus in the blind might therefore locate at the interface between high-level speech comprehension and acoustic processing. This sheds important new lights on language processing in the occipital cortex of the blind by demonstrating that the involvement of this neural population relates to the sensory signal of speech and therefore contrasts with the dominant proposition that occipital involvement in speech processing is abstracted from its sensory input and purely reflect higher-level operations similar to those observed in prefrontal regions (Bedny, 2017)</p><disp-quote content-type="editor-comment"><p>3) There are several conflicting sentences in the Results, which make it unclear whether synchronization is present in both sighted and early-blind individuals, with the two groups differing in the degree of tracking, or is present only in the early blind. From <xref ref-type="fig" rid="fig1">Figure 1J</xref> it seems that the sighted did in fact show both a general coherence effect (at least for the 8 and 1 channel conditions) and an intelligibility effect (negative correlation). Demonstrating clearly whether the temporal locking effects are present in the sighted subjects and how exactly the early-blind group behaves in comparison is important because this directly links to the conclusions drawn in the paper. It appears from this figure that the functional role of the calcarine sulcus in synchronizing to speech in these populations may need reconsideration.</p></disp-quote><p>We thank the reviewers for this valuable question. With the evidence presented in the current study we are describing differences in the degree of cerebro-acoustic tracking. A binary classification as to whether tracking does, or does not occur in one of the groups was not intended. We outline our reasons for this argument below.</p><p>First, from a practical point of view, coherence is a bivariate metric that ranges between 0 and 1. Thus, with no negative values, a simple one-sample t-test in this case is not possible. Indeed, the majority of studies use group or condition comparisons to identify regions where cerebro-acoustic tracking is modulated. Here we use the contrast between intelligible and vocoded speech to highlight areas sensitive to speech intelligibility, and the effect of blindness on target areas where sensitivity to the speech rhythm. Hence all our conclusions are limited to describing modulatory effects.</p><p>Second, while it is theoretically possible to compare the results with a surrogate distribution to mimic a one-sample t-test, we believe that this analysis would not add to our argument. The current study was designed to address questions regarding group and condition differences. Additional one-sample t-test would not affect these conclusions. For example, if we do find a significant difference from zero, the conclusions would still hold, while a lack of an effect could not be interpreted. That said, our main proposal is based on the idea that the blind recycle existing functional pathways, and therefore we would not be surprised to find weak forms of cerebro-acoustic coherence in the occipital cortex of the sighted as well. For example, following the reviewer’s suggestion, we now have also examined the group effect as a post-hoc comparison in CS. Here we do not find that coherence is overall (across the 3 conditions) higher in blind versus sighted individuals. However, we do find a qualitative difference where the blind show enhanced coherence during intelligible speech compared to sighted individuals, while in the unintelligible condition, we find no significant difference between groups. The fact that the occipital cortex of sighted individuals shows stronger entrainment for altered speech condition may relate to the previous demonstration that the more adverse the listening condition (low signal-to-noise-ratio or audio-visual incongruence), the more the visual cortex is entrained to the speech signal of actual acoustic speech presented together with varying levels of acoustic noise (Park et al., 2016; Giordano et al., 2017). Moreover, Giordano and colleagues (2017), showed an increase of directed connectivity between superior frontal regions and visual cortex under the most challenging (acoustic noise and uninformative visual cues) conditions. Kayser et al. (Kayser et al., 2015) also proposed top-down processes modulating acoustic entrainment. In case of absence of structuring visual inputs since birth, the occipital cortex that does not play a role in integrating visual and auditory speech signals, like in the sighted, would now start to show enhanced coherence during intelligible speech, like what is typically observed in temporal regions. This hypothesis again suggests a link between the reorganization observed in blind individuals and the typical multisensory structure involving the occipital cortex in audio-visual speech processing (Kayser et al., 2007).</p><p>We now have added the following paragraph to the Results section to highlight that idea.</p><p>“A group difference between blind and sighted individuals across conditions was not observed in the calcarine region (t(30)=1.1, p=.28). The lack of an overall group effect in calcarine sulcus suggests that there is not a simple enhanced sensory response to speech in the blind. In this respect, calcarine sulcus responds differently than the region wide identified in parietal cortex where synchronization was overall stronger in the blind. Rather, the two groups differ only when speech is intelligible. But, while overall coherence with the speech envelope is equally high in early blind and sighted individuals, the blind population show significantly higher coherence in the intelligible speech condition compared to the sighted, who show higher cerebro-acoustic coherence in the unintelligible condition (1-Chan). The latter is reminiscent to the fact that the more adverse the listening condition (low signal-to-noise-ratio or audio-visual incongruence), the more the visual cortex is entrained to visual speech signal of actual acoustic speech presented together with varying levels of acoustic noise (Park et al., 2016; Giordano et al., 2017). Moreover, Giordano and colleagues (2017), showed an increase of directed connectivity between superior frontal regions and visual cortex under the most challenging (acoustic noise and uninformative visual cues) conditions. This hypothesis again suggests a link between the reorganization observed in the occipital cortex of blind individuals and typical multisensory pathways involving the occipital cortex in audio-visual speech processing (Kayser et al., 2007). Visual deprivation since birth however triggers a functional reorganization of the calcarine region that may now dynamically interact with the intelligibility of speech signal.”</p><disp-quote content-type="editor-comment"><p>4) The reviewers pointed out the analysis of source reconstructions on two separate frequency windows that were largely overlapping needs clearer explanation and justification.</p></disp-quote><p>We apologize for this lack of clarity. We have decided to combine the reconstructions with filters for two different frequency bands to optimally capture the shape of the coherence spectrum we saw in sensors space. We have decided to opt for this approach to cover the broad range of frequencies where syllables can occur. In addition, our choice was restricted by our decision to segment the trials into 1s segments, and hence compute a frequency spectrum with integer frequencies. A center-frequency at 6.5Hz was thus not feasible. We have now elaborated on this approach in the Materials and methods.</p><p>By combining the two frequency-bands we acquire a source estimate that emphasizes the center of our frequency band of interest (6-7Hz) and tapers off towards the edges, which optimally represents the coherence spectrum observed in sensor space. The choice of the center frequency was also restricted by the length of the time window used for the analysis. That is, with a 1s time window and a resulting 1Hz frequency resolution, a non-integer center frequency at e.g. 6.5Hz was not feasible.</p><disp-quote content-type="editor-comment"><p>5) Another issue concerns the presentation of the same story clip in 3 different versions to each participant. Again, this requires clearer explanation and justification.</p></disp-quote><p>Because the key analyses involved contrasting the different conditions, we wanted to make sure that the speech envelopes for the three different conditions were exactly the same and, so it was crucial to keep the sentences the same. However, to avoid contamination as much as possible, we ensured that during the randomization procedure of the trials two versions of the same story were never presented close to each other. Due to the length and complexity of the stories presented in the current study, it would be very difficult to reconstruct a story in the 1chan condition from a previously heard comprehensible version. We argue that this is a different situation than hearing short sentences in close succession during a vocoded and intelligible condition. In addition, if this effect existed, it would be balanced out due to the randomization procedure.</p></body></sub-article></article>