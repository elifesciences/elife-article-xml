<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">36928</article-id><article-id pub-id-type="doi">10.7554/eLife.36928</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Spatial sampling in human visual cortex is modulated by both spatial and feature-based attention</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-109338"><name><surname>van Es</surname><given-names>Daniel Marten</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7067-6394</contrib-id><email>daan.van.es@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-109931"><name><surname>Theeuwes</surname><given-names>Jan</given-names></name><email>j.theeuwes@vu.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-74959"><name><surname>Knapen</surname><given-names>Tomas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5863-8689</contrib-id><email>tknapen@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Behavioural and Movement Sciences</institution><institution>Vrije Universiteit Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>The Netherlands</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Spinoza Centre for Neuroimaging</institution><institution>Royal Academy of Sciences</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>The Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>12</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e36928</elocation-id><history><date date-type="received" iso-8601-date="2018-03-26"><day>26</day><month>03</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-11-13"><day>13</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, van Es et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>van Es et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-36928-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.36928.001</object-id><p>Spatial attention changes the sampling of visual space. Behavioral studies suggest that feature-based attention modulates this resampling to optimize the attended feature's sampling. We investigate this hypothesis by estimating spatial sampling in visual cortex while independently varying both feature-based and spatial attention. Our results show that spatial and feature-based attention interacted: resampling of visual space depended on both the attended location and feature (color vs. temporal frequency). This interaction occurred similarly throughout visual cortex, regardless of an area's overall feature preference. However, the interaction did depend on spatial sampling properties of voxels that prefer the attended feature. These findings are parsimoniously explained by variations in the precision of an attentional gain field. Our results demonstrate that the deployment of spatial attention is tailored to the spatial sampling properties of units that are sensitive to the attended feature.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.36928.002</object-id><title>eLife digest</title><p>Much like digital cameras record images using a grid of tiny pixels, our own visual experience results from the activity of many neurons, each with its own receptive field. A neuron’s receptive field is the area of visual space – the scene in front of our eyes – to which that neuron responds. But whereas digital pixels have fixed locations, the receptive fields of neurons do not. If we switch our attention to a different area of the scene in front of us, visual neurons move their receptive fields to cover that area instead. We do not need to move our eyes for this to happen, just the focus of our attention.</p><p>Moving receptive fields in this way enables the visual system to generate more detailed vision at the new attended location. Unlike a digital camera, the brain is thus much more than a passive recording device. But does the movement of receptive fields also depend on what we are attending to at a given location? Paying attention to tiny details, for example, might require many receptive fields to move by large amounts to produce vision with high enough resolution.</p><p>Van Es et al. have now answered this question by using a brain scanner to measure receptive fields in healthy volunteers. The volunteers focused on different visual features, such as color or motion, and to various visual locations. When the volunteers attended to color, their attention was more tightly focused than when they attended to motion. This might be because processing color requires fine-detail vision, whereas we can detect movement with our attention spread over a larger area. As a result, receptive fields moved more when the volunteers attended to color than when they attended to motion.</p><p>Movement of visual receptive fields thus depends on what we attend to, as well as where we focus our attention. This adds to our understanding of how the brain filters the information bombarding our senses. This might lead to better diagnosis and treatment of disorders that include attentional problems, such as autism and ADHD. The results could also help develop artificial intelligence systems that, like the visual system, can process information flexibly to achieve different goals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>spatial attention</kwd><kwd>feature-based attention</kwd><kwd>population receptive fields</kwd><kwd>fMRI</kwd><kwd>vision</kwd><kwd>attention</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>ORA #464-11-030</award-id><principal-award-recipient><name><surname>Theeuwes</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The sampling of visual space is not only warped by attention towards locations but also depends on attended features.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The resolution of the visual system is highest at the fovea and decreases gradually with increasing eccentricity. But the visual system’s resolution is not fixed. Attention can be directed to a location in space and/or a visual feature, which temporarily improves behavioral performance (<xref ref-type="bibr" rid="bib80">Posner et al., 1980</xref>; <xref ref-type="bibr" rid="bib87">Rossi and Paradiso, 1995</xref>; <xref ref-type="bibr" rid="bib34">Found and Müller, 1996</xref>; <xref ref-type="bibr" rid="bib16">Carrasco and Yeshurun, 1998</xref>; <xref ref-type="bibr" rid="bib122">Yeshurun and Carrasco, 1999</xref>; <xref ref-type="bibr" rid="bib59">Kumada, 2001</xref>; <xref ref-type="bibr" rid="bib89">Sàenz et al., 2003</xref>; <xref ref-type="bibr" rid="bib117">Wolfe et al., 2003</xref>; <xref ref-type="bibr" rid="bib104">Theeuwes and Van der Burg, 2007</xref>) at the cost of reduced sensitivity for non-attended locations and features (<xref ref-type="bibr" rid="bib50">Kastner and Pinsk, 2004</xref>; <xref ref-type="bibr" rid="bib78">Pestilli and Carrasco, 2005</xref>; <xref ref-type="bibr" rid="bib113">Wegener et al., 2008</xref>).</p><p>Attending a location in space increases activity in units representing the attended location, as shown by both electrophysiological (<xref ref-type="bibr" rid="bib65">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib83">Reynolds et al., 2000</xref>) and fMRI studies (<xref ref-type="bibr" rid="bib105">Tootell et al., 1998</xref>; <xref ref-type="bibr" rid="bib99">Silver et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Datta and DeYoe, 2009</xref>). In addition, spatial receptive fields were shown to shift toward an attended location in macaque MT+ (<xref ref-type="bibr" rid="bib119">Womelsdorf et al., 2006</xref>) and V4 (<xref ref-type="bibr" rid="bib22">Connor et al., 1997</xref>). Using fMRI to measure population receptive fields (pRFs; <xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib31">Dumoulin and Knapen, 2018</xref>), it was found that pRF shifts induced by spatial attention occur throughout human visual cortex (<xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib96">Sheremata and Silver, 2015</xref>; <xref ref-type="bibr" rid="bib111">Vo et al., 2017</xref>), a process thought to improve visual resolution at the attended location (<xref ref-type="bibr" rid="bib5">Anton-Erxleben and Carrasco, 2013</xref>; <xref ref-type="bibr" rid="bib52">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib111">Vo et al., 2017</xref>). Such <italic>spatial resampling</italic> is understood to be the result of an interaction between bottom-up sensory signals and a top-down attentional gain field (<xref ref-type="bibr" rid="bib120">Womelsdorf et al., 2008</xref>; <xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib69">Miconi and VanRullen, 2016</xref>).</p><p>Feature-based attention, for example directed toward color or motion, selectively increases activity in those units that represent the attended feature, as evidenced by electrophysiological (<xref ref-type="bibr" rid="bib107">Treue and Maunsell, 1996</xref>; <xref ref-type="bibr" rid="bib106">Treue et al., 1999</xref>; <xref ref-type="bibr" rid="bib68">McAdams and Maunsell, 2000</xref>; <xref ref-type="bibr" rid="bib67">Maunsell and Treue, 2006</xref>; <xref ref-type="bibr" rid="bib72">Müller et al., 2006</xref>; <xref ref-type="bibr" rid="bib126">Zhang and Luck, 2009</xref>; <xref ref-type="bibr" rid="bib127">Zhou and Desimone, 2011</xref>), fMRI (<xref ref-type="bibr" rid="bib88">Saenz et al., 2002</xref>; <xref ref-type="bibr" rid="bib95">Serences and Boynton, 2007</xref>; <xref ref-type="bibr" rid="bib47">Jehee et al., 2011</xref>), and behavioral reports (<xref ref-type="bibr" rid="bib89">Sàenz et al., 2003</xref>; <xref ref-type="bibr" rid="bib114">White and Carrasco, 2011</xref>). These studies consistently show that feature-based attention modulates processing irrespective of the attended stimulus's spatial location. In addition, feature-based attention also appears to shift featural tuning curves toward the attended value, as reported by both electrophysiological (<xref ref-type="bibr" rid="bib71">Motter, 1994</xref>; <xref ref-type="bibr" rid="bib28">David et al., 2008</xref>) and fMRI studies (<xref ref-type="bibr" rid="bib25">Çukur et al., 2013</xref>).</p><p>The similarity in the effects of feature-based and spatial attention on affected neural units suggests a common neural mechanism for both sources of attention (<xref ref-type="bibr" rid="bib39">Hayden and Gallant, 2005</xref>; <xref ref-type="bibr" rid="bib20">Cohen and Maunsell, 2011</xref>). Yet spatial attention necessitates retinotopically precise feedback (<xref ref-type="bibr" rid="bib69">Miconi and VanRullen, 2016</xref>), whereas feature-based attention operates throughout the visual field (<xref ref-type="bibr" rid="bib67">Maunsell and Treue, 2006</xref>). Studies investigating whether one source of attention potentiates the other generally find that interactions are either nonexistent or very weak at the earliest stages of processing (<xref ref-type="bibr" rid="bib28">David et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Hayden and Gallant, 2009</xref>; <xref ref-type="bibr" rid="bib75">Patzwahl and Treue, 2009</xref>; <xref ref-type="bibr" rid="bib126">Zhang and Luck, 2009</xref>), but emerge at later stages of visual processing (<xref ref-type="bibr" rid="bib43">Hillyard and Münte, 1984</xref>; <xref ref-type="bibr" rid="bib36">Handy et al., 2001</xref>; <xref ref-type="bibr" rid="bib9">Bengson et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Ibos and Freedman, 2016</xref>), and ultimately influence behavior (<xref ref-type="bibr" rid="bib54">Kingstone, 1992</xref>; <xref ref-type="bibr" rid="bib58">Kravitz and Behrmann, 2011</xref>; <xref ref-type="bibr" rid="bib61">Leonard et al., 2015</xref>; <xref ref-type="bibr" rid="bib115">White et al., 2015</xref>; <xref ref-type="bibr" rid="bib73">Nordfang et al., 2018</xref>). In addition, the effects of feature-based compared to spatial attention arise earlier in time. This occurs both when only feature-based attention is endogenously cued and subsequently guides spatial attention towards the attended feature's location (<xref ref-type="bibr" rid="bib44">Hopf et al., 2004</xref>), and when both types of attention are endogenously cued (<xref ref-type="bibr" rid="bib39">Hayden and Gallant, 2005</xref>; <xref ref-type="bibr" rid="bib4">Andersen et al., 2011</xref>). This supports the idea that feature-based attention can direct spatial attention toward or away from specific locations containing attended or unattended features (<xref ref-type="bibr" rid="bib21">Cohen and Shoup, 1997</xref>; <xref ref-type="bibr" rid="bib18">Cepeda et al., 1998</xref>; <xref ref-type="bibr" rid="bib15">Burnett et al., 2016</xref>).</p><p>The studies mentioned above investigated modulatory effects of feature-based attention on spatial attention by measuring changes in response amplitude (e.g. ERP/firing rate). However, no study to date has investigated the effect of feature-based attention on spatial sampling. Yet, exactly this relationship is predicted by behavioral studies. Especially when attention is endogenously cued, feature-based attention has been argued to influence the spatial resampling induced by spatial attention to optimize sampling of visual features for behavior (<xref ref-type="bibr" rid="bib121">Yeshurun and Carrasco, 1998</xref>; <xref ref-type="bibr" rid="bib123">Yeshurun and Carrasco, 2000</xref>; <xref ref-type="bibr" rid="bib124">Yeshurun et al., 2008</xref>; <xref ref-type="bibr" rid="bib8">Barbot and Carrasco, 2017</xref>). Specifically, these authors suggested that when attending features that are processed by neurons with smaller receptive fields, a greater degree of spatial resampling is required to resolve the required featural resolution at the attended location. In the current study, we put this hypothesis to the test by measuring the brain's representation of space under conditions of differential attention.</p><p>Specifically, we measured pRFs under conditions of differential spatial attention (i.e. toward fixation or the mapping stimulus) and feature-based attention (i.e. toward the mapping stimulus's temporal frequency or color content). One important reason for studying the effects of attention to color and temporal frequency is that they are known to be processed at different spatial scales. Specifically, color is generally processed at a finer spatial scale compared to temporal frequency. First, this is a result of color compared to temporal frequency information being processed predominantly by the parvocellular rather than the magnocellular pathways, which in turn pool differentially across visual space (<xref ref-type="bibr" rid="bib91">Schiller and Malpeli, 1978</xref>; <xref ref-type="bibr" rid="bib42">Hicks et al., 1983</xref>; <xref ref-type="bibr" rid="bib30">Denison et al., 2014</xref>). Second, temporal frequency and color are processed across different cortical areas (i.e. MT +compared to hV4; <xref ref-type="bibr" rid="bib63">Liu and Wandell (2005)</xref>; <xref ref-type="bibr" rid="bib12">Brouwer and Heeger, 2009</xref>, <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2013</xref>; <xref ref-type="bibr" rid="bib116">Winawer et al., 2010</xref>) that have differential spatial precision (<xref ref-type="bibr" rid="bib3">Amano et al., 2009</xref>; <xref ref-type="bibr" rid="bib116">Winawer et al., 2010</xref>). Third, preference for color compared to temporal frequency is generally greater in the fovea compared to the periphery (<xref ref-type="bibr" rid="bib26">Curcio et al., 1990</xref>; <xref ref-type="bibr" rid="bib6">Azzopardi et al., 1999</xref>; <xref ref-type="bibr" rid="bib11">Brewer et al., 2005</xref>), where receptive fields are generally smaller (<xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>). The hypothesized relation between spatial scale of attended features and spatial resampling as proposed by the behavioral studies mentioned above thus implies that attending color compared to temporal frequency at a particular location should lead to stronger spatial resampling. In addition, as color and temporal frequency are differentially processed across cortical areas, studying these features allows us to investigate whether modulations of spatial resampling by feature-based attention are specific for areas that prefer the attended feature. We specifically chose temporal frequency and not coherent motion, as coherent motion signals have been shown to influence pRF measurements (<xref ref-type="bibr" rid="bib38">Harvey and Dumoulin, 2016</xref>). It was previously shown that attention can be directed to both feature domains (<xref ref-type="bibr" rid="bib118">Wolfe and Horowitz, 2004</xref>; <xref ref-type="bibr" rid="bib17">Cass et al., 2011</xref>).</p><p>We characterized how spatial attention influences the sampling of visual space, and subsequently investigated how feature-based attention modulates this spatial resampling. In addition, an explicit gain-field interaction model allowed us to formally capture the pRF position changes resulting from our attentional manipulations (<xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>). Finally, we also performed a full-field stimulation experiment which allowed us to relate these attentional modulations to each voxel's bottom-up preference for color and temporal frequency.</p><p>In brief, our results show that pRF changes are indeed stronger when attending the stimulus's color compared to temporal frequency content. These modulations occurred similarly throughout the visual system, regardless of an area's bottom-up feature preference. We show that these feature-based attentional modulations can be explained by changes in the precision of the attentional gain field. Together, this confirms the idea that the degree of spatial resampling is dependent on the spatial scale at which attended features are processed.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>For the purpose of our study, we adapted a standard traversing bar-stimulus based retinotopic mapping paradigm (<xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>). Specifically, our bar stimulus was made up out of many Gabor elements, each with one of two color compositions (blue/yellow or green/magenta) and with particular temporal frequency (high or low). Throughout the duration of a bar pass, participants reported changes in fixation mark luminance, in the predominant Gabor color composition, or in the predominant Gabor temporal frequency (see <xref ref-type="fig" rid="fig1">Figure 1A</xref> and Materials and methods section for additional details). Importantly, visual stimulation was identical across conditions and only the top-down factor of attention was manipulated. In addition, behavioral difficulty was kept constant between conditions and across three levels of eccentricity using a Quest staircase procedure. Thus, spatial attention was either directed towards the fixation mark (<italic>Attend Fixation</italic>) or towards the bar stimulus (<italic>Attend Stimulus</italic>). In addition, feature-based attention was either directed towards color (<italic>Attend Color</italic>), or towards temporal frequency (<italic>Attend TF</italic>). This setup allowed us to fit separate population receptive field (pRF) models to data from the different attentional conditions for each voxel (see <xref ref-type="fig" rid="fig1">Figure 1B</xref> and Materials and methods section for additional details).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.003</object-id><label>Figure 1.</label><caption><title>Experimental design and pRF fitting procedure.</title><p>(<bold>A</bold>) Experimental design. Throughout a bar pass participants reported either changes in color (Attend Color) or temporal frequency (Attend TF) of Gabor elements within the moving bar stimulus, or changes in fixation mark luminance (Attend Fixation), while maintaining accurate fixation. Participants were informed auditorily about the upcoming task 2 s before each bar pass. (<bold>B</bold>) Overview of pRF fitting procedure. pRF parameters were estimated from each voxel's BOLD signal time course in a three-step procedure. First, a design matrix was constructed based on 31 × 31 pixels' visual stimulation time course of the entire experiment, which was convolved with a participant-specific HRF (derived from separate data, see Feature preference and HRF Mapper). L2-regularized regression was used to find the position of the spatial response profile's peak of each voxel. Second, to find precise estimates of pRF center location and size, we used a more fine-grained 101 × 101 design matrix and gradient descent to fit a single parameterized 2D Gaussian pRF model to data from all attention conditions combined, initialized at the L2-regression derived peak location. Third, 2D Gaussian pRF models were fitted to data from the different attention conditions separately, initialized with the parameters resulting from step 2.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig1-v1.tif"/></fig><p>Our principal aim is to understand whether feature-based attention influences spatial resampling induced by spatial attention. Therefore, we first characterize in detail the changes in spatial sampling (i.e. pRF properties) that resulted from differential allocation of spatial attention. Second, we capture these pRF modulations by spatial attention in an attentional gain field modeling framework. Third, we investigate how feature-based attention modulated the pattern of pRF changes and how it affected attentional gain field model parameters. Finally, we relate these feature-based attentional modulations to (1) bottom-up feature preference and (2) to differences in the spatial scale at which color and temporal frequency are processed.</p><sec id="s2-1"><title>Region of interest definition</title><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows voxels' <italic>Attend Fixation</italic> location preferences, by depicting color-coded polar angle coordinates on an inflated cortical surface for one example participant's right hemisphere. We examined the relation between pRF eccentricity and size within each of the retinotopic regions, and performed further analyses on those regions that showed clear progressions of polar angle on the surface as well as positive size-eccentricity relations, as shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. In addition, we created a <italic>combined ROI</italic> that pooled voxels across selected ROIs to evaluate pRF changes across the visual system.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.004</object-id><label>Figure 2.</label><caption><title>ROI definition.</title><p>(<bold>A</bold>) Attend Fixation pRF polar angle maps for an example participant with retinotopic areas up to the intra-parietal sulcus defined by hand. (<bold>B</bold>) Attend Fixation pRF size as a function of eccentricity for all areas that showed robust relationships. (<bold>C</bold>) Distribution of explained variance across the different regions of interest. The violins are normalized to have equal maximum width. (<bold>D</bold>) Number of voxels per ROI included in all subsequent analyses. All error bars and shaded error regions denote 95% CI of data and linear fits, respectively, across voxels. Also see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s2">2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Eccentricity-size relations for all statistical methods.</title><p>This figure shows that eccentricity-size relations are positive in all ROIs for both the 'super subject' method (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> - Table 1) and the 'over subjects' method (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> - Table 11). Regarding the individual subjects, we find that although slopes between eccentricity and size were positive in all ROIs in all subjects, this was not always significant with α of 0.05. Yet, this occured for maximally one out of five subjects in some ROIs: in MT+ in s3 (p = 0.058), in V3AB (p = 0.073) and IPS0 (p = 0.078) in s5 and VO in s1 (p = 0.146). In the 'super subject' and 'per subject' methods, error bars denote 95% CI over voxels; in the 'over subjects' method, error bars denote 95% CI over subjects. See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Explained variance and voxel count for all statistical methods.</title><p>The violin plots depict the distribution of explained variance across voxels for the different ROIs and for all subjects.The horizontal line at 0.10 indicates the exclusion criterion (voxels with explained variance lower than 0.10 are also excluded from these violin plots). The bar plots show the number of voxels that survived different rejection criteria for all ROIs and for all subjects. The first bar of each ROI depicts all voxels within the retinotopically defined mask where explained variance is larger than &gt;0.01. This serves as a baseline of 'all' voxels within that area. The second bar depicts all voxels where explained variance is larger than &gt;0.10 (the criteria used in all subsequent analyses). The third bar depicts the number of voxels where explained variance is &gt;0.10 and where pRF eccentricity was smaller than 3.3 dva. The last bar depicts the number of voxels where explained variance is &gt;0.10, where pRF eccentricity is &lt;3.3 dva and where pRF size is &lt;7.2 dva. The figure shows that most voxels are rejected either because their explained variance was too low or because they fell outside of the stimulus region. The additional pRF size rejection criterion virtually did not reject any additional voxels. The last bar of the 'super subject' is shown in the main figure. See Materials and methods section for definitions of 'super subject' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig2-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>pRF changes induced by spatial attention</title><p>To quantify pRF changes resulting from differential allocation of spatial attention, we created an <italic>Attend Stimulus</italic> condition by averaging pRF parameters between the <italic>Attend Color</italic> and <italic>Attend TF</italic> conditions. To inspect how spatial attention affected pRF positions, we plotted a vector from the <italic>Attend Fixation</italic> to the <italic>Attend Stimulus</italic> pRF position. For visualization purposes, we created visual field quadrant representations by multiplying both the <italic>Attend Fixation</italic> and <italic>Attend Stimulus</italic> pRF x- and y-coordinates with the sign of pRF x- and y-coordinates in the <italic>Attend Fixation</italic> condition (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). This means, for example, that pRFs in the upper-right quadrant were unaffected (i.e. x- and y-coordinate multiplied by 1), while pRFs in the lower-right quadrant were mirrored along the y-axis (i.e. x-coordinate multiplied by 1, y-coordinate multiplied by −1). Note that mirroring based on the <italic>Attend Fixation</italic> condition preserves any pRF shifts across a meridian, allowing pRFs to shift outside the target visual field quadrant. Visual inspection of these pRF position shifts shows both increasing shift magnitude up the visual hierarchy and shifts occurring mainly along the radial dimension (i.e. toward or away from the fovea; <xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.007</object-id><label>Figure 3.</label><caption><title>Effect of spatial attention on pRF position.</title><p>(<bold>A</bold>) Plotting strategy. For pRF shift visualizations, all pRF positions are mirrored into one quadrant of the visual field. Then, vectors representing the shift of pRF centers between conditions were drawn from the <italic>Attend Fixation</italic> to the <italic>Attend Stimulus</italic> pRF position. (<bold>B</bold>) Shift vectors as described in (<bold>A</bold>). pRF shift magnitude increased up the visual hierarchy, and shifts appear to occur mainly in the radial direction (i.e. changes in pRF eccentricity). Dotted lines demarcate eccentricity bins used in subsequent analyses. (<bold>C</bold>) Changes in pRF position in the horizontal, vertical and radial directions as a proportion of the length of the shift vectors, as depicted in (<bold>B</bold>). The magnitude of pRF shifts is consistently best described by changes in pRF eccentricity. (<bold>D</bold>) pRF x, y and eccentricity position shifts plotted as a function of polar angle, for different shift direction hypotheses. The data closely match the radial shift direction hypothesis, showing strongest pRF x shifts close to the horizontal meridian, strongest pRF y shifts close to the vertical meridian and strong pRF eccentricity changes across all polar angles. In (<bold>C</bold>), single, double and triple asterisks indicate significant differences with FDR corrected p &lt; 0.05, &lt; 0.01 and &lt; 0.001, respectively. Also see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.008</object-id><label>Figure 3—figure supplement 1.</label><caption><title>pRF shift plots for all statistical methods.</title><p>Shift vectors run from the <italic>Attend Fixation</italic> to the <italic>Attend Stimulus</italic> pRF location. This figure indicates that the shift patterns seen in the 'super subject' are almost identical to the 'over subjects' method, and highly agree with the individual subject figures. Note the radial shift direction that is readily apparent in all ROIs in all subjects. Also note how the combined ROI clearly shows parafoveal pRFs shifting toward the periphery and peripheral pRFs shifting toward the fovea across subjects. Finally, the absence of pRFs near the vertical meridian in data from all subjects in VO, IPS0 and MT+ highlights the overrepresentation of the horizontal meridian that is found in all ROIs (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>- Tables 3, 14 and 15). See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.009</object-id><label>Figure 3—figure supplement 2.</label><caption><title>pRF shift directions for all statistical methods.</title><p>The 'over subjects' panel replicates the 'super subject' results, namely that shifts are best explained by eccentricity changes, followed by changes in pRF x and finally in pRF y in all ROIs (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1 </xref>- Table 2). The dominance of eccentricity over x changes was significant in all individual subjects ('per subject' method) in the combined ROI, and in at least two (but often five) subjects in all other ROIs (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>- Table 13). The individual subject evidence also replicates the finding that pRFs shifted more in the x compared to y directions in hV4/VO/LO/V3AB/IPS0/MT+ and in the combined ROI (i.e. significant in at least two out of five subjects; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> - Table 12). The dominance of x over y shifts was explained from a non-uniformity in the distribution of pRF polar angle (i.e. overrepresentation of horizontal meridian, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 3). This was also found in most individual subjects for most ROIs (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Tables 14 and 15). Single, double and triple asterisks indicate significant differences at p &lt; 0.05, 0.01 and 0.001, respectively. In the 'super subject' and 'per subject' methods, error bars denote 95% CI of data over voxels; in the 'over subjects' method, error bars denote 95% CI over subjects. See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.010</object-id><label>Figure 3—figure supplement 3.</label><caption><title>pRF x, y and eccentricity position shifts as a function of polar angle for all statistical methods.</title><p>The data from all statistical methods closely matches the radial shift direction hypothesis, showing strongest pRF x shifts close to the horizontal meridian, strongest pRF y shifts close to the vertical meridian and no polar angle dependence of pRF eccentricity shifts. Evidence for this is the more positive slope of y change over polar angle compared to slope of x change over polar angle in the 'super subject' method (see main text for statistics), for the 'over subjects' method (slope difference = 0.219, t<sub>(4)</sub> = 3.616, p = 0.022, Cohen's d = 1.808). Error bars reflect 95% CIs across voxels in the 'super subject' and 'per subject' methods, and across subjects in the 'over subjects' method. See Materials and methods section for definitions of 'super subject' and 'over subjects' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig3-figsupp3-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>pRF shift direction</title><p>This latter observation seems at apparent odds with a recent study reporting that pRFs shift mainly in the horizontal direction (<xref ref-type="bibr" rid="bib96">Sheremata and Silver, 2015</xref>). To quantify the observed direction of pRF shifts we computed the ratio of shifts in the radial, horizontal and vertical directions (see <xref ref-type="fig" rid="fig3">Figure 3C</xref>). In line with the data of <xref ref-type="bibr" rid="bib96">Sheremata and Silver (2015)</xref>, we find that changes of pRF horizontal location consistently better describe the overall shifts than do changes of pRF vertical location in all ROIs except V1/2/3 (p's &lt; 0.05, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 2). We also find that pRF shifts are described even better by shifts in the radial dimension (i.e. changes in eccentricity) compared to shifts in the horizontal direction in all ROIs (p's &lt; 0.01, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 2). <xref ref-type="fig" rid="fig3">Figure 3D</xref> is intended to ease interpretation of these results. It depicts how different hypotheses regarding the underlying directionality of pRF shifts (i.e. horizontal, vertical or radial - i.e. foveopetal/foveofugal), translate into changes in measured pRF x, y and eccentricity as a function of quarter visual field polar angle (i.e. from vertical to horizontal meridian). For example, if pRFs shift primarily in the radial direction (right hypothesis column, <xref ref-type="fig" rid="fig3">Figure 3D</xref>), this would result in the strongest pRF x-direction changes close to the horizontal meridian and the strongest pRF y-direction changes close to the vertical meridian. pRF eccentricity changes however, would show no dependence on polar angle. <xref ref-type="fig" rid="fig3">Figure 3D</xref>, right column, shows that the data (<italic>combined ROI</italic>) correspond most to the radial shift hypothesis. To quantify this visual intuition, we compared the slopes of the change in pRF <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi></mml:math></inline-formula> over polar angle by binning polar angle into three bins and comparing the first and last bins (i.e. horizontal and vertical meridian, respectively). This showed that, compared to the slope of pRF <inline-formula><mml:math id="inf3"><mml:mi>y</mml:mi></mml:math></inline-formula> change over polar angle, the slope of pRF <inline-formula><mml:math id="inf4"><mml:mi>x</mml:mi></mml:math></inline-formula> change was more negative (p &lt; 0.001, Cohen's d = 0.677, N = 11946). This pattern of results can only be explained by pRFs shifting in the radial direction. Visual field coverage is known to be non-uniform such that the horizontal meridian is overrepresented at both subcortical (<xref ref-type="bibr" rid="bib92">Schneider et al., 2004</xref>) and cortical (<xref ref-type="bibr" rid="bib102">Swisher et al., 2007</xref>; <xref ref-type="bibr" rid="bib98">Silva et al., 2018</xref>) levels and was also clearly present in our data (Rayleigh tests for non-uniformity in ROIs separately, p's &lt; 0.001, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 3). This means that shifts that occur exclusively in the radial dimension appear as a dominance of horizontal compared to vertical shifts when averaging over the visual field.</p></sec><sec id="s2-4"><title>pRF changes across eccentricity</title><p>To further inspect the attention-induced radial shifts described above, we plotted the difference between <italic>Attend Stimulus</italic> and <italic>Attend Fixation</italic> pRF eccentricity for each of four <italic>Attend Fixation</italic> pRF eccentricity bins (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This returned varying patterns of pRF eccentricity changes across the different ROIs. The <italic>combined ROI</italic> shows that overall, parafoveal pRFs shifted away from the fovea, while peripheral pRFs shifted toward the fovea. These outward shifting parafoveal pRFs are found in all other ROIs except V1 and V2, whereas the inward shifting peripheral pRFs are also present in V1, V2 and V3 (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Tables 4 and 5).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.011</object-id><label>Figure 4.</label><caption><title>Effect of spatial attention on pRF eccentricity and size.</title><p>Difference between <italic>Attend Stimulus</italic> and <italic>Attend Fixation</italic> pRF eccentricity (<bold>A</bold>) and size (<bold>B</bold>) as a function of <italic>Attend Fixation</italic> eccentricity. Overall, parafoveal pRFs tend to shift away from the fovea and increase in size, while peripheral pRFs tend to shift toward the fovea and decrease in size. (<bold>C</bold>) Changes in pRF eccentricity and size were strongly correlated in all ROIs. In (<bold>A</bold>) and (<bold>B</bold>), markers are increased in size when bootstrapped distributions differ from 0 with FDR corrected p &lt; 0.05. In (<bold>C</bold>) the markers' error bar denotes 95% CI of data over voxels and shaded error regions denote 95% CI of linear fit parameters over bins. Also see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s2">2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Effect of spatial attention on pRF eccentricity and size for all statistical methods.</title><p>Difference between <italic>Attend Stimulus</italic> and <italic>Attend Fixation</italic> pRF eccentricity and size as a function of <italic>Attend Fixation</italic> eccentricity (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Tables 4-7 and 16-23). The data show a transition from inwards to outwards shifts across the visual hierarchy in all subjects. Additionally, the progressions of change over eccentricity show some variability within ROIs between subjects. Finally, data from the combined ROI in most subjects reveal that parafoveal pRFs tend to shift away from the fovea and increase in size, while peripheral pRFs tend to shift toward the fovea and decrease in size. Error bars reflect 95% CIs across voxels in the 'super subject' and 'per subject' methods, and across subjects in the 'over subjects' method. Increased marker size indicates significance with p &lt; 0.05 (FDR corrected for 'super subject'). See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.013</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Correlation between pRF eccentricity and size changes for all statistical methods.</title><p>In the 'over subjects' method, we find that pRF eccentricity and size changes are significant in all ROIs except IPSO (although p = 0.070). In the 'per subject method', we find such a significant correlation in at least two (but often five) subjects (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 24). Error bars reflect 95% CIs across voxels in the 'super subject' and 'per subject' methods, and across subjects in the 'over subjects' method. See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig4-figsupp2-v1.tif"/></fig></fig-group><p>In addition to pRF position changes, we also inspected changes in pRF size induced by differences in spatial attention as a function of <italic>Attend Fixation</italic> pRF eccentricity (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Overall, parafoveal pRFs increased in size, while peripheral pRFs decreased in size. These expanding parafoveal pRFs were present in all ROIs except V2/3, whereas shrinking peripheral pRFs were found in all ROIs except V1, MT+ and IPS0 (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Tables 6 and 7). Overall, this pattern of results is strikingly similar to the changes in pRF eccentricity described above. In fact, the changes in pRF size and eccentricity were strongly correlated in all ROIs (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, Pearson R over 20 5-percentile bins between 0.74 and 0.99, p's &lt; 0.001, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 8). Together, these results show that attention to the stimulus caused parafoveal pRFs to shift away from the fovea and increase in size, whereas peripheral pRFs shifted toward the fovea and decreased in size.</p></sec><sec id="s2-5"><title>Formal account for observed pattern of pRF shifts</title><p>To provide a mechanistic explanation for the complex pattern of pRF shifts described above, we modeled our results using a multiplicative Gaussian gain field model (<xref ref-type="bibr" rid="bib120">Womelsdorf et al., 2008</xref>; <xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>). We adapted this framework to work in conditions where attention shifted over space as a function of time (see Materials and methods). In brief, this modeling procedure used the <italic>Attend Fixation</italic> pRF, one attentional gain field at fixation and another convolved with the stimulus to predict the <italic>Attend Stimulus</italic> pRF position. We determined optimal attentional gain field sizes by minimizing the difference between observed and predicted <italic>Attend Stimulus</italic> pRF positions in the quadrant visual field format of <xref ref-type="fig" rid="fig3">Figure 3B</xref>. <xref ref-type="fig" rid="fig5">Figure 5A</xref> illustrates that model predictions closely followed the data, thereby accurately reproducing radially shifting pRFs. Examining the predicted change in pRF eccentricity as a function of eccentricity (i.e. the dominant pRF shift direction; <xref ref-type="fig" rid="fig5">Figure 5B</xref>) showed that the model was able to capture widely varying eccentricity change profiles across ROIs using very similar attentional gain field sizes (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). This shows that a common attentional influence can result in very different pRF shift patterns, which then depend on differential spatial sampling properties across ROIs (i.e. distribution of pRF sizes and positions). In sum, these results show that the attentional gain field model provides a parsimonious and powerful account for the variety of pRF shift patterns across ROIs.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.014</object-id><label>Figure 5.</label><caption><title>Capturing spatial attention in attentional gain field model.</title><p>(<bold>A</bold>) Observed (black) and predicted (color) pRF shifts. (<bold>B</bold>) Observed and predicted changes in pRF eccentricity (the main pRF shift direction) as a function of eccentricity. Markers depict data and lines the corresponding attentional gain field model fit. (<bold>C</bold>) Fitted fixation and stimulus attentional gain field sizes. Dots depict individual subjects, and bars the average across subjects. (<bold>D-F</bold>). Left panels depict changes in eccentricity induced by attending fixation (dark gray lines) and by attending the stimulus (light gray lines). Although both sources of attention cause a pull toward the fovea in all ROIs, relative shift magnitude differs across eccentricity. The right panels show how the difference between both spatial attention conditions results in the patterns as observed in (<bold>B</bold>). In (<bold>B</bold>), error bars denote 95% CIs over subjects. Plotting conventions as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Also see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.015</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Attentional gain field modeling results for each subject - shift vectors.</title><p>Arrows depict observed (black) and predicted (color) pRF shifts. This shows that the model closely captured the data in all individual subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.016</object-id><label>Figure 5—figure supplement 2.</label><caption><title>Attentional gain field modeling results for each subject - changes in pRF eccentricity.</title><p>Observed (markers) and predicted (solid lines) pRF eccentricity difference between <italic>Attend Fixation</italic> and <italic>Attend Stimulus</italic> conditions for each subject. This shows that although individual subjects show some variation in the changes in pRF eccentricity across eccentricity, this is well captured by the attentional gain field model. This suggests that individual differences in average pRF changes are not the result of a differential attentional mechanism operating between subjects, but rather that this results from known individual differences in baseline distribution of pRF parameters (e.g. see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Shaded areas indicate 95% CI over voxels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig5-figsupp2-v1.tif"/></fig></fig-group><p>We further investigated how the model was able to reproduce the eccentricity-dependent eccentricity changes we reported above. For this, we inspected pRF shifts induced by attending either fixation or the stimulus relative to the stimulus drive (i.e. the pRF outside the influence of attention derived from the model). For illustrative purposes, we display results for V2, V3 and IPS0 as these areas showed marked differences in their eccentricity change profile (<xref ref-type="fig" rid="fig5">Figure 5D–F</xref>). The left panels of each figure reveal the effects of attending fixation and the stimulus separately. This shows that both sources of spatial attention pull the measured pRFs toward the fovea, albeit with differing relative magnitudes across eccentricity. The right panel of each figure shows that the resulting difference between attending fixation and the stimulus constitutes the eccentricity-dependent patterns observed in the data (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>Together, these analyses show that existing multiplicative gain field models of attention can be extended to predict pRF shifts in situations where spatial attention shifts over time. Additionally, it confirms, extends and quantifies earlier reports showing that the precision of the attentional gain field is similar across the visual hierarchy (<xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>).</p></sec><sec id="s2-6"><title>Feature-based attentional modulation</title><p>Having established (1) the pattern of changes in spatial sampling (i.e. changes in pRF size and eccentricity) resulting from differential allocation of spatial attention, and (2) a mechanistic explanation of these changes, we next examined how this pattern was modulated by differences in feature-based attention. <xref ref-type="fig" rid="fig6">Figure 6A</xref> shows how pRF eccentricity and size are differentially affected by attending color or temporal frequency within the stimulus for the <italic>combined ROI</italic>. This illustrates that while both tasks caused similar pRF changes, these effects were generally more pronounced when attending color.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.017</object-id><label>Figure 6.</label><caption><title>Feature-based attentional modulation of pRF changes.</title><p>(<bold>A</bold>) Differences in pRF eccentricity and size relative to the <italic>Attend Fixation</italic> condition, for both the <italic>Attend Color</italic> and <italic>Attend TF</italic> conditions. The changes in both eccentricity and size are more pronounced when attending changes in color versus TF changes in the bar. (<bold>B</bold>) The Attentional Modulation Index (AMI) combines eccentricity and size changes to form one robust index of spatial attention and is greater when attending color compared to TF. (<bold>C</bold>) The feature AMI quantifies this difference. Positive values of this feature AMI across eccentricity confirm stronger pRF modulations when attending color compared to TF. (<bold>D</bold>) Average feature AMI for each ROI, extending greater observed pRF modulations when attending color compared to TF to all individual ROIs. (<bold>E</bold>) Average feature AMI as a function of average feature preference across ROIs. Feature preference increases with higher color compared to TF preference. Although hV4 and VO are relatively sensitive to color and MT+ is relatively sensitive to TF, feature AMI is comparable in these areas. Error bars denote 95% CI over voxels. Also see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref>–<xref ref-type="fig" rid="fig6s4">4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.018</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Feature-based attentional modulation of pRF parameters across eccentricity for each ROI.</title><p>The bottom-right panel shows the feature AMI for all ROIs together for the 'super subject'. This shows that feature AMI is generally positive along eccentricity across ROIs. In addition, differences in this pattern between ROIs are comparable to differences in the magnitude of pRF changes that resulted from differential spatial attention (<xref ref-type="fig" rid="fig4">Figure 4</xref>). <xref ref-type="fig" rid="fig5">Figure 5</xref> showed that these differential patterns along eccentricity are well explained using a similar attentional influence across visual regions. This means that differential feature AMI patterns along eccentricity are likely the result of differential pRF center and size distributions between ROIs. Inspecting differences in feature AMI between visual regions should therefore be inspected as an average across the visual field (as presented in <xref ref-type="fig" rid="fig6">Figure 6D and E</xref>). Error bars denote 95% CI over voxels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.019</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Feature AMI compared to feature preference across ROIs with different ways of computing the FAMI.</title><p>In (<bold>A</bold>), FAMI only includes the change in pRF size, whereas in (<bold>B</bold>), FAMI only includes the change in pRF eccentricity. This reveals comparable results to use of the combined FAMI pRF size and eccentricity measure. This corroborates that changes in pRF eccentricity and size are closely coupled, warranting the combination of both measures into a single attentional index as presented in the main figure. Error bars denote 95% CI over voxels. Filled markers indicate a significant deviation from 0 with FDR corrected p &lt; 0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig6-figsupp2-v1.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.020</object-id><label>Figure 6—figure supplement 3.</label><caption><title>Feature-based attentional modulation of pRF size and eccentricity changes in the <italic>combined</italic> ROI for all statistical methods.</title><p>Differences in pRF eccentricity and size relative to the <italic>Attend Fixation</italic> condition, for both the <italic>Attend Color</italic> and <italic>Attend TF</italic> conditions separately.The changes in both eccentricity and size are more pronounced when attending changes in color versus TF in the bar. This pattern is most clear in the 'super subject' and 'over subjects' methods (albeit with larger variance), and is present in most individual subjects. Error bars reflect 95% CIs across voxels in the 'super subject' and 'per subject' methods, and across subjects in the 'over subjects' method. See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig6-figsupp3-v1.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.021</object-id><label>Figure 6—figure supplement 4.</label><caption><title>Feature AMI compared to feature preference for each ROI, for all statistical methods.</title><p>The y-axis displays feature AMI, which increases when pRF modulations (size and eccentricity changes combined, see Materials and methods) are greater when attending color compared to TF. The x-axis displays feature preference, which increases with higher color compared to TF preference. pRF modulations were greater when attending color in all ROIs in the 'super subject' method, and were unrelated to feature preference (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 9). In the 'over subjects' method, we confirm that feature AMI was not different between ROIs (RM ANOVA, factor of ROI: F<sub>(8,32)</sub> = 0.631, p = 0.746, η<sup>2</sup>p= 0.066), and that it was on average 0.059 over ROIs across subjects, F<sub>(1,4)</sub> = 18.868, p = 0.012, η<sup>2</sup>p = 0.394). Additionally, we found that for the ‘over subjects’ method, feature AMI was significantly positive in the combined ROI t<sub>(4)</sub> = 4.316, p = 0.012, Cohen’s d = 2.158 (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 25 for all ROIs). Finally, we also found no correlation with feature preference in the 'over subjects' method (mean Spearman correlation = 0.20, t<sub>(4)</sub> = 0.850, p = 0.434, Cohen's d = 0.446; mean Pearson correlation = 0.22, t<sub>(4)</sub> = 0.770, p = 0.476, Cohen’s d = 0.429). Error bars reflect 95% CIs across voxels in the 'super subject' and 'per subject' methods, and across subjects in the 'over subjects' method. See Material and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig6-figsupp4-v1.tif"/></fig></fig-group><p>To quantify the modulation of feature-based attention per voxel, we first set up a single robust index of the degree to which spatial attention resampled visual space, combining changes in pRF eccentricity and size (as these were highly correlated, see <xref ref-type="fig" rid="fig4">Figure 4C</xref>). This Attentional Modulation Index (AMI, see Materials and methods) is depicted in <xref ref-type="fig" rid="fig6">Figure 6B</xref> for the <italic>combined ROI</italic> when attending color and TF. We then quantified the difference in this AMI between attending color and temporal frequency as a feature-based Attentional Modulation Index (feature AMI, see Materials and methods). Positive values of feature AMI indicate that attending color induced greater pRF changes, while negative values indicate that attending TF led to stronger pRF changes. <xref ref-type="fig" rid="fig6">Figure 6C</xref> shows that this feature AMI was positive across eccentricity in the <italic>combined ROI</italic>. Inspecting the average feature AMI across voxels within each ROI (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) reveals that attending changes in color compared to TF in the bar stimulus produced stronger spatial resampling in all ROIs (p's &lt; 0.01, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 9). Specifically, the feature AMI was around 0.05 on average across ROIs. As the feature AMI is a contrast measure where difference is divided by the sum, this corresponds to roughly 10% stronger pRF changes when attending color compared to temporal frequency. In some ROIs (V3AB/IPS0, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), the FAMI reached values of 0.20, which corresponds roughly to 50% stronger pRF changes when attending color compared to temporal frequency. Computing the AMI with either pRF eccentricity or size changes separately (i.e. not as a combined measure) yields similar results (see <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>).</p></sec><sec id="s2-7"><title>Feature AMI and feature preference</title><p>The feature-based modulations we describe above are possibly related to differences in bottom-up preference for the attended features. Feature-based attention is known to increase activity of neurons selective for the attended feature, regardless of spatial location (<xref ref-type="bibr" rid="bib107">Treue and Maunsell, 1996</xref>; <xref ref-type="bibr" rid="bib106">Treue et al., 1999</xref>; <xref ref-type="bibr" rid="bib68">McAdams and Maunsell, 2000</xref>; <xref ref-type="bibr" rid="bib67">Maunsell and Treue, 2006</xref>; <xref ref-type="bibr" rid="bib72">Müller et al., 2006</xref>; <xref ref-type="bibr" rid="bib126">Zhang and Luck, 2009</xref>; <xref ref-type="bibr" rid="bib127">Zhou and Desimone, 2011</xref>). This suggests that if activity in a given voxel is modulated more strongly by changes in a certain feature, this could lead to a greater apparent pRF shift when attending that feature. To test this hypothesis, we estimated the difference in response amplitude to the presence of color and temporal frequency within a full-field stimulus (in a separate experiment, see Materials and methods). We then summarized each voxel's relative preference for color and temporal frequency by means of a feature preference index. Higher values of this feature preference index indicate greater preference for color compared to TF. <xref ref-type="fig" rid="fig6">Figure 6E</xref> displays the feature AMI as a function of feature preference, for each ROI. Note that feature preference was negative on average in most ROIs, suggesting that our TF manipulation (7 vs 0 Hz grayscale Gabors) caused stronger response modulations compared to our color manipulation (colored vs grayscale Gabors). Although this induced an offset across the brain, variations in this measure across ROIs replicate known specializations of the visual system with high precision (<xref ref-type="bibr" rid="bib63">Liu and Wandell, 2005</xref>; <xref ref-type="bibr" rid="bib12">Brouwer and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2013</xref>): while areas MT+ and V1 show the strongest preference for TF compared to color, areas V4 and VO show the strongest preference for color compared to TF. Importantly, regardless of these large variations in feature preferences between MT+/V1 and VO/hV4, average feature AMI was nearly equal in these ROIs. In addition, there was no correlation between feature preference and feature AMI across all ROIs (R = 0.20, p = 0.608, N = 9, rho = 0.37, p = 0.332, N = 9). These results show that the observed feature-based attentional modulations occur globally across the brain, and do not depend on bottom-up feature preference.</p></sec><sec id="s2-8"><title>Feature preference and spatial sampling</title><p>How do we explain that attending color in the stimulus induced greater changes in spatial sampling? Behavioral studies have suggested that the influence of spatial attention should be adjusted by feature-based attention to improve sampling of attended visual features (<xref ref-type="bibr" rid="bib124">Yeshurun et al., 2008</xref>; <xref ref-type="bibr" rid="bib8">Barbot and Carrasco, 2017</xref>). Specifically, these authors suggested that attending features processed by relatively smaller receptive fields requires a greater degree of spatial resampling. This implies that if color-preferring voxels are relatively small, this could explain the greater degree of resampling observed when attending color. This is indeed predicted by the fact that color compared to temporal frequency information is predominantly processed by the parvocellular compared to the magnocellular pathway, where spatial sampling is generally more fine-grained (<xref ref-type="bibr" rid="bib91">Schiller and Malpeli, 1978</xref>; <xref ref-type="bibr" rid="bib42">Hicks et al., 1983</xref>; <xref ref-type="bibr" rid="bib30">Denison et al., 2014</xref>). In addition, pRF size varies on average between visual regions, such that pRF size is generally larger in area MT+ (preferring temporal frequency) compared to hV4 (preferring color). Finally, both pRF size (<xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>) and color compared to TF preference (<xref ref-type="bibr" rid="bib26">Curcio et al., 1990</xref>; <xref ref-type="bibr" rid="bib6">Azzopardi et al., 1999</xref>; <xref ref-type="bibr" rid="bib11">Brewer et al., 2005</xref>) are known to be strongly eccentricity-dependent such that foveal voxels have relatively small pRFs and are relatively color-sensitive. We also clearly observe both effects in our data (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> and <xref ref-type="fig" rid="fig7">Figure 7</xref>, correlation between feature-preference and eccentricity is negative except in LO and VO, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> - Table 10). In sum, the greater amount of spatial resampling when attending color can be parsimoniously explained by color being sampled by relatively smaller pRFs.</p><fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.022</object-id><label>Figure 7.</label><caption><title>Feature preference and eccentricity.</title><p>Preference to color compared to TF is greatest near the fovea. Error bars denote 95% CI over voxels. Also see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.023</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Color compared to TF preference versus eccentricity correlations for all statistical methods.</title><p>The 'super subject' method shows negative Spearman correlations in all ROIs except VO and LO (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 10). The 'over subjects' method shows that this correlation is negative on average and does not vary across ROI (RM ANOVA with main factor of ROI F<sub>(8,32)</sub> = 0.354, p = 0.937, η<sup>2</sup>p = 0.030, on average -0.22 over ROIs, F<sub>(1,4)</sub> = 15.630, p = 0.017, η<sup>2</sup>p = 0.641). When looking at individual subjects, we find that correlations are positive in at least two but often in the majority of subjects across ROIs (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Table 26). Error bars reflect 95% CIs across voxels in the 'super subject' and 'per subject' methods, and across subjects in the 'over subjects' method. See Materials and methods section for definitions of 'super subject', 'over subjects' and 'per subject' methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig7-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-9"><title>Feature-based attention influences attentional gain field precision</title><p>Smaller pRFs also require a more precise attentional gain field to shift a given distance (a property of the multiplication of Gaussians). Combining this with our observation that pRFs experience greater shifts when attending color, we predict that attentional gain fields should be more precise in this condition. To test this, we repeated the attentional gain field modeling procedure described above, replacing the <italic>Attend Stimulus</italic> data with the <italic>Attend Color</italic> and <italic>Attend TF</italic> data in two separate fit procedures. Both our data (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and previous findings (<xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>) showed that a single attentional gain field affects the different visual regions similarly. In addition, our results presented above showed that pRF modulation by feature-based attention was not related to feature preference across ROIs. We therefore analyzed feature-based attentional modulations of the attentional gain field both on data from all ROIs fitted together (the 'combined ROI'), and as the median across individually fitted ROIs. This analysis returned smaller fitted stimulus attentional gain field sizes in the <italic>Attend Color</italic> compared to the <italic>Attend TF</italic> fit procedure (<xref ref-type="fig" rid="fig8">Figure 8</xref>) both in the <italic>combined ROI</italic> (0.094 dva smaller over subjects when attending color, t(4) = 9.021, p = 0.001, Cohen's d = 4.511) and across ROIs (median over ROIs on average 0.061 dva smaller over subjects when attending color, t(4) = 4.243, p = 0.013, Cohen's d = 2.121). As the <italic>Attend Fixation</italic> data were used as input in both modeling procedures, we verified that the estimated fixation attentional gain field was not different between procedures (p's of. 693 and. 224 and Cohen's d of −0.213 and −0.719 for across ROIs and <italic>combined ROI</italic>, respectively). These analyses show that the stronger influence of spatial attention when attending color is realized by a more precise attentional gain field located at the stimulus. In sum, our results suggest that (1) the attentional system adjusts its influence in accordance with the spatial sampling characteristics of units that prefer the attended feature and (2) that it does this equally across visual regions regardless of their bottom-up feature preference.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.024</object-id><label>Figure 8.</label><caption><title>Feature-based attentional modulation of attentional gain field sizes.</title><p>Results are shown for the combined ROI and for the median across ROIs. Positive values here indicate smaller attentional gain fields when attending color compared to temporal frequency in the bar. This means that positive values demarcate stronger spatial resampling when attending color (in correspondence with <xref ref-type="fig" rid="fig6">Figure 6D and E</xref>). Error bars represent 95% CI over subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig8-v1.tif"/></fig></sec><sec id="s2-10"><title>Across-subject consistency</title><p>To evaluate the stability of our results, we repeated all analyses for individual subjects. The figures and details for these results can be found in the supplements of the specific figures, and in additional statistical tables. This showed that although spatial attention resulted in somewhat different patterns of pRF shifts between subjects, these individual differences were well captured by the attentional gain field model. This suggests that individual differences in pRF changes are likely the result of the known individual differences in pRF parameter distributions (e.g. eccentricity-size relations [<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, and see <xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>]). In addition, these analyses showed that spatial resampling was consistently modulated by feature-based attention across subjects (i.e. feature AMI was on average 0.059 greater than 0 (F<sub>(1,4)</sub> = 18.868, p = 0.012, <inline-formula><mml:math id="inf5"><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi></mml:math></inline-formula> = 0.394), and was not different between ROIs (F<sub>(8,32)</sub> = 0.631, p = 0.746, <inline-formula><mml:math id="inf6"><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi></mml:math></inline-formula> = 0.066)).</p></sec><sec id="s2-11"><title>Task and fixation performance</title><p>Finally, we verified that the pRF results were not affected by differences in fixation accuracy or behavioral performance (see <xref ref-type="fig" rid="fig9">Figure 9</xref>). To provide evidence in favor of these null hypotheses, we performed JZL Bayes factor analyses (using JASP; <xref ref-type="bibr" rid="bib64">Love et al., 2015</xref>) as frequentist statistics are not capable of providing evidence for the null (<xref ref-type="bibr" rid="bib2">Altman and Bland, 1995</xref>). We rotated recorded eye position to the direction of bar movement and computed the median and standard deviation of position along this dimension across bar passes per bar position (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). We next set up a model including the factor of attention condition (3 levels), bar position (24 levels) and their interaction. We found that when predicting gaze position, the evidence was in favor of the null hypothesis with a Bayes Factor (BF) of 18620. When predicting gaze variability, however, we found evidence against the null hypothesis with a BF of 5.980. Evidence for including each of the factors (condition, bar position and their interaction) into the model returned BFs of 0.713, 547.193 and 0.017, respectively. The BF of 0.713 for the factor of condition means that we cannot determine whether gaze variability was different between conditions. However, even if this were the case, this could only lead to an offset in pRF size and not to changes in pRF position (<xref ref-type="bibr" rid="bib62">Levin et al., 2010</xref>; <xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Hummer et al., 2016</xref>). In addition, any anisotropy in gaze position variability could potentially lead to offsets in pRF center positions. Nevertheless, these biases would be identical for all pRFs throughout the visual field. As we find that peripheral pRFs shift inwards and decrease in size and central pRFs shift outwards and increase in size, global offsets in pRF size and position cannot parsimoniously explain our results. More importantly, the analyses also showed that although bar position influenced gaze variability (BF of 547.193), it did not do so differently between attention conditions (BF of 0.017).</p><fig-group><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.025</object-id><label>Figure 9.</label><caption><title>Task and fixation performance.</title><p>(<bold>A</bold>) Behavioral accuracy per attention condition and per bar stimulus eccentricity bin. Horizontal line denotes Quest target of 83%; chance level was 50%. (<bold>B</bold>) Median (top panel) and standard deviation (bottom panel) gaze position in the direction of bar movement per bar position. Error bars denote 95% CI across five participants. Also see <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.36928.026</object-id><label>Figure 9—figure supplement 1.</label><caption><title>Ratio of Gabor elements of either feature value used by the Quest procedure to equate difficulty.</title><p>In the <italic>Attend Color</italic> condition, this reflected the ratio of elements that were either blue/yellow or green/magenta. In the <italic>Attend TF</italic> condition this reflected the ratio of elements that were either the high or low TF. A RM ANOVA returned a main effect of eccentricity (F<sub>(2,8)</sub> = 6.515, p = 0.021, η<sup>2</sup>p = 0.620), a main effect of condition (F<sub>(1,4)</sub> = 31.025, p = 0.005, η<sup>2</sup>p = 0.886) and no interaction between eccentricity and condition (F<sub>(2,8)</sub> = 1.323, p = 0.319, η<sup>2</sup>p = 0.249). First, this shows (as expected) that to equate difficulty, the ratio of elements with the target feature value increased with eccentricity. In addition, it shows that the number of elements with either high or low temporal frequency was higher than the ratio of elements that was either blue/yellow or green/magenta. This suggests that the two temporal frequencies used in our experiment were perceptually more similar than the two color combinations. Importantly, however, the Quest procedure successfully eliminated any effects of this potential difference on task difficulty between conditions (see results on accuracy, <xref ref-type="fig" rid="fig9">Figure 9A</xref>). The lack of interaction between attention condition and eccentricity is at apparent odds with the observed decreasing relative preference for color compared to temporal frequency with increasing eccentricity (<xref ref-type="fig" rid="fig7">Figure 7</xref>). This could suggest that the greater degree of spatial resampling we observed when attending color discounted the lower relative sensitivity for color in the periphery.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig9-figsupp1-v1.tif"/></fig></fig-group><p>Although we used a Quest procedure to equate difficulty across attention conditions and across different levels of eccentricity, it is possible that this procedure stabilized at a faulty difficulty level. To verify whether the Quest procedure successfully equated performance, we used a similar Bayesian approach, testing whether a model including attention condition (three levels) and stimulus eccentricity (three levels) influenced behavioral performance (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). This returned evidence for the null hypothesis with a BF of 6.25. Together, these results show that differences in pRF parameters between conditions cannot be explained by either fixation accuracy or behavioral difficulty.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We investigated how spatial and feature-based attention jointly modulate the sampling of visual space. We found that directing covert spatial attention toward a moving bar stimulus altered the eccentricity and size of pRFs in concert. These changes in spatial sampling were parsimoniously explained by an attentional gain field model. Attending color changes within this stimulus induced stronger pRF changes compared to attending temporal frequency changes. These feature-based attentional modulations occurred globally throughout the brain, irrespective of a visual region's average feature preference. We suggest that the greater degree of spatial resampling when attending color is related to smaller pRF sizes in relatively color preferring voxels. In addition, we showed that the greater degree of spatial resampling when attending color is caused by a more precise attentional gain field on the stimulus.</p><p>Previous behavioral reports suggested that the spatial scale at which an attended feature is processed influenced the degree of spatial resampling (<xref ref-type="bibr" rid="bib121">Yeshurun and Carrasco, 1998</xref>; <xref ref-type="bibr" rid="bib123">Yeshurun and Carrasco, 2000</xref>; <xref ref-type="bibr" rid="bib124">Yeshurun et al., 2008</xref>; <xref ref-type="bibr" rid="bib8">Barbot and Carrasco, 2017</xref>). Specifically, features processed at a finer spatial scale require a greater degree of spatial resampling. This means that the greater degree of spatial resampling when attending color compared to temporal frequency could be explained by color being sampled at a finer spatial scale. This is a canonical difference between parvocellular small, color-sensitive receptive fields and magnocellular large, temporal frequency-sensitive receptive fields (<xref ref-type="bibr" rid="bib91">Schiller and Malpeli, 1978</xref>; <xref ref-type="bibr" rid="bib42">Hicks et al., 1983</xref>; <xref ref-type="bibr" rid="bib30">Denison et al., 2014</xref>). In addition, both our data and previous findings show that color is preferentially processed by visual areas that on average have smaller receptive field sizes (i.e. hV4 compared to MT+ in <xref ref-type="bibr" rid="bib63">Liu and Wandell (2005)</xref>; <xref ref-type="bibr" rid="bib3">Amano et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Brouwer and Heeger, 2009</xref>, <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2013</xref>; <xref ref-type="bibr" rid="bib116">Winawer et al., 2010</xref>). Finally, both the current and previous studies show that pRF size (<xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>) and color compared to temporal frequency preference (<xref ref-type="bibr" rid="bib26">Curcio et al., 1990</xref>; <xref ref-type="bibr" rid="bib6">Azzopardi et al., 1999</xref>; <xref ref-type="bibr" rid="bib11">Brewer et al., 2005</xref>) vary across eccentricity such that foveal voxels have smaller pRFs and are more color-sensitive. In sum, we suggest that the greater degree of spatial resampling when attending color compared to temporal frequency can be explained by the difference in spatial scale at which these features are processed. We therefore predict that our results should generalize to any other comparison of attended visual features as long as these features differ in their spatial scale. This includes attending different feature values such as high compared to low spatial frequency, or attending different feature dimensions such as faces (broader spatial scale) versus letters (finer spatial scale).</p><p>Electrophysiological studies on the interaction between feature-based and spatial attention generally measure overall response amplitudes rather than changes in spatial sampling. This implies that interactions between feature-based and spatial attention are weak to non-existent in relatively early stages of processing (<xref ref-type="bibr" rid="bib28">David et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Hayden and Gallant, 2009</xref>; <xref ref-type="bibr" rid="bib75">Patzwahl and Treue, 2009</xref>; <xref ref-type="bibr" rid="bib126">Zhang and Luck, 2009</xref>), but develop at later stages of visual processing (<xref ref-type="bibr" rid="bib43">Hillyard and Münte, 1984</xref>; <xref ref-type="bibr" rid="bib36">Handy et al., 2001</xref>; <xref ref-type="bibr" rid="bib4">Andersen et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Bengson et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Ibos and Freedman, 2016</xref>), but see <xref ref-type="bibr" rid="bib33">Egner et al., 2008</xref>). We add to this (1) that feature-based attention modulates the effects of spatial attention on spatial resampling, and (2) that these interactions occur globally throughout the brain, manifesting themselves in even the earliest cortical visual regions. Interactions between spatial and feature-based attention in the early stages of processing could be concealed when focusing on changes in response amplitude rather than changes in spatial sampling. However, it is important to note that measuring spatial sampling at the level of voxels does not allow us to determine whether observed changes in spatial sampling are the result of changes in spatial sampling of individual neurons, or rather the result of differential weighting of subpopulations of neurons within a voxel. Nevertheless, it has been shown that spatial attention does influence spatial sampling of individual neurons (<xref ref-type="bibr" rid="bib22">Connor et al., 1997</xref>; <xref ref-type="bibr" rid="bib119">Womelsdorf et al., 2006</xref>). Yet, future studies are required to extend our conclusions regarding the interactions between feature-based and spatial attention to the single neuron level.</p><p>The greater degree of spatial resampling when attending color compared to temporal frequency occurred throughout the brain, irrespective of visual regions' preference for the attended features. In other words, while MT+ and hV4 differed greatly in their relative feature preference, both areas showed comparable pRF changes resulting from differences in feature-based attention. This stands in apparent contrast to previous studies reporting that feature-based attention selectively enhances responses in cortical areas specialized in processing the attended feature (<xref ref-type="bibr" rid="bib23">Corbetta et al., 1990</xref>; <xref ref-type="bibr" rid="bib19">Chawla et al., 1999</xref>; <xref ref-type="bibr" rid="bib74">O'Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib93">Schoenfeld et al., 2007</xref>; <xref ref-type="bibr" rid="bib7">Baldauf and Desimone, 2014</xref>). However, attending a stimulus consisting of multiple features was shown to spread attentional response modulations of one of the object's feature dimensions to other constituent feature dimensions (<xref ref-type="bibr" rid="bib51">Katzner et al., 2009</xref>; <xref ref-type="bibr" rid="bib25">Çukur et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">Kay and Yeatman, 2017</xref>), albeit somewhat later in time (±60 ms; <xref ref-type="bibr" rid="bib94">Schoenfeld et al., 2014</xref>). This could mean that the global pattern of pRF shifts we observed is caused by such an object-based attentional transfer mechanism. In addition, changing the sampling of visual space globally throughout the brain enhances stability in the representation of space. Different modifications of visual space per visual region would require an additional mechanism linking different spatial representations. Instead, the global nature of spatial resampling we observe supports a temporally dynamic but spatially consistent adaptation of visual space.</p><p>An important remaining question pertains to the source of the interactions between feature-based and spatial attention. Signals of spatial selection are thought to originate from a network of frontal and parietal areas, identified using fMRI (<xref ref-type="bibr" rid="bib97">Shulman et al., 2002</xref>; <xref ref-type="bibr" rid="bib99">Silver et al., 2005</xref>; <xref ref-type="bibr" rid="bib49">Jerde et al., 2012</xref>; <xref ref-type="bibr" rid="bib101">Sprague and Serences, 2013</xref>; <xref ref-type="bibr" rid="bib103">Szczepanski et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">Kay and Yeatman, 2017</xref>; <xref ref-type="bibr" rid="bib66">Mackey et al., 2017</xref>) and electrophysiology (<xref ref-type="bibr" rid="bib70">Moore and Armstrong, 2003</xref>; <xref ref-type="bibr" rid="bib35">Gregoriou et al., 2009</xref>). As we focused on careful measurement of spatial sampling in feature-sensitive visual cortex with a relatively small stimulus region, we did not include the frontoparietal regions containing large receptive fields in our analyses. A recent study suggested a central role for the ventral prearcuate gyrus for conjoined spatial and feature-based attentional modulations (<xref ref-type="bibr" rid="bib10">Bichot et al., 2015</xref>). Correspondingly, signals of feature selection in humans have been localized to a likely human homologue of this area, the inferior frontal junction (IFJ; <xref ref-type="bibr" rid="bib125">Zanto et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Baldauf and Desimone, 2014</xref>). This region is therefore a possible candidate for controlling the interactions between feature-based and spatial attention.</p><p>The average changes in pRF size and eccentricity for each visual region in our data are largely consistent with previous studies in which attention was devoted to a peripheral stimulus versus fixation (<xref ref-type="bibr" rid="bib52">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib96">Sheremata and Silver, 2015</xref>). Moreover, our analyses go beyond these average pRF changes by investigating the spatial structure of the complex pattern of pRF changes that resulted from such differential spatial attention. The resulting characterization details how the sampling of visual space by single voxel pRFs is affected by spatial attention, which is of specific relevance for future studies that determine spatial selectivity for voxel selections. First, we show that attending the stimulus compared to fixation caused pRFs to shift radially. Although a previous study reported a dominant horizontal shift direction (<xref ref-type="bibr" rid="bib96">Sheremata and Silver, 2015</xref>), we suggest that the overrepresentation of the horizontal meridian (<xref ref-type="bibr" rid="bib92">Schneider et al., 2004</xref>; <xref ref-type="bibr" rid="bib102">Swisher et al., 2007</xref>) made radially shifting pRFs appear as predominantly horizontal changes. Second, we report closely coupled pRF eccentricity and size changes that were dependent on pRF eccentricity. Specifically, we found that parafoveal pRFs shifted toward the periphery and increased in size, whereas peripheral pRFs shifted toward the fovea and decreased in size. This finding supports the resolution hypothesis of attention (<xref ref-type="bibr" rid="bib5">Anton-Erxleben and Carrasco, 2013</xref>), which posits that spatial attention acts to reduce resolution differences between the fovea and periphery. We note that the functional implication of pRF size changes was recently questioned, as stimulus encoding fidelity was shown to be unaffected by pRF size changes (<xref ref-type="bibr" rid="bib111">Vo et al., 2017</xref>). However, the exact functional significance of changes in pRF size bears no consequence for the conclusions currently presented. As we observed a strong correlation between pRF eccentricity and size changes, we combined both measures into a single robust index. Our relevant quantifications are therefore agnostic to the potentially separable functional implications of changes in pRF size and eccentricity.</p><p>The pattern of pRF shifts we observe is described well by an attentional gain field model (<xref ref-type="bibr" rid="bib84">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>). First, this highlights that a simple and well-understood mechanism underpins the apparent complexity of the observed pattern of pRF changes. Second, it extends the utility of such attentional gain field models to situations in which attention is dynamically deployed over space and time during the mapping of the pRF (<xref ref-type="bibr" rid="bib52">Kay et al., 2015</xref>). In agreement with earlier reports (<xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib82">Puckett and DeYoe, 2015</xref>), we found that the best-fitting model implemented comparable attentional gain field sizes across visual regions. This strongly points to spatial attention being implemented as a global influence across visual cortex. We conclude that differences in pRF shift patterns between different visual regions depended primarily on differences in visual sampling (i.e. differences in pRF center and size distributions) rather than on differing attentional influences. Despite the broad correspondence between model fits and data, the model did not capture the observed decreases in pRF eccentricity of the most foveal pRFs in V1. Two recent studies showed that in early visual areas, spatial attention shifted pRFs away from the attended location, but toward the attended location in higher visual areas (<xref ref-type="bibr" rid="bib29">de Haas et al., 2014</xref>; <xref ref-type="bibr" rid="bib111">Vo et al., 2017</xref>). Other studies showed that in precisely these visual regions, both the pRF and the attentional gain field are composed of a suppressive surround in addition to their positive peak (<xref ref-type="bibr" rid="bib128">Zuiderbaan et al., 2012</xref>; <xref ref-type="bibr" rid="bib82">Puckett and DeYoe, 2015</xref>). We leave the question of whether these suppressive surrounds could explain such repulsive shifts in lower visual cortex for future research. As a more general aside, gain fields have been shown to influence visual processing at the motor stage (<xref ref-type="bibr" rid="bib110">Van Opstal et al., 1995</xref>; <xref ref-type="bibr" rid="bib100">Snyder et al., 1998</xref>; <xref ref-type="bibr" rid="bib108">Trotter and Celebrini, 1999</xref>). Thus, our results further establish the close link between attentional and motor processes (<xref ref-type="bibr" rid="bib85">Rizzolatti et al., 1987</xref>; <xref ref-type="bibr" rid="bib24">Corbetta et al., 1998</xref>).</p><p>In sum, we show that visuospatial sampling is not only affected by attended locations but also depends on the spatial sampling properties of units that prefer attended visual features. The global nature of these modulations highlights the flexibility of the brain’s encoding of sensory information to meet task demands (<xref ref-type="bibr" rid="bib86">Rosenholtz, 2016</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Five participants (two female, two authors, aged between 25 and 37) participated in the study. All gave informed consent, and procedures were approved by the ethical review board of the University of Amsterdam (2016-BC-7145), where scanning took place.</p></sec><sec id="s4-2"><title>Apparatus</title><sec id="s4-2-1"><title>MRI acquisition</title><p>All MRI data were acquired on a Philips Achieva 3T scanner (Philips Medical Systems), equipped with a 32-channel head coil. T1-weighted images were acquired for each participant with isotropic resolution of 1 mm, repetition time (TR) of 8.2 ms, TE of 3.73 ms, flip angle of 8°. Functional T2*-weighted data consisted of 30 2D slices of echo planar images (EPI) with isotropic resolution of 2.5 mm, with a 0.25 mm slice gap, TR of 1600 ms, TE of 27.62 ms, and a flip angle of 70°. Each participant completed between 6 and 8 Attention-pRF Mapping runs (20 min each) and 2–3 Feature preference and HRF Mapping runs (10 min each), spread over 2 (N = 1) or 3 (N = 4) sessions within a 2-week period (see Experimental design).</p></sec><sec id="s4-2-2"><title>Gaze recording</title><p>During all functional runs, gaze position was recorded using an Eyelink 1000 (SR Research, Osgoode, Ontario, Canada), sampled at 1000 Hz. A 9-point calibration-validation procedure was run at the start of each session.</p></sec><sec id="s4-2-3"><title>Stimulus presentation</title><p>Visual stimuli were created in PsychoPy (<xref ref-type="bibr" rid="bib77">Peirce, 2008</xref>) and run on a 15-inch 2013 MacBook Pro Retina. Participants viewed a 32-inch MR-ready LCD 'BOLDscreen' monitor (resolution: 1920 × 1080, refresh rate: 100 Hz; Cambridge Research Systems), at 156 cm distance from the participants' eyes at the end of the bore, via a helmet-mounted front-silvered mirror. Auditory stimuli were presented through headphones using the MRConfon system.</p></sec></sec><sec id="s4-3"><title>Experimental design</title><sec id="s4-3-1"><title>Attention-PRF mapping stimulus</title><p>A bar stimulus of 0.9 degrees of visual angle (dva) width traversed a circular aperture of 7.2 dva in one of eight directions (cardinal and diagonal orientations in both directions, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>), completing a full pass in 38.4 s by stepping 0.34 dva every 1.6 s, and pausing 4.8 s between each bar pass. One run contained 24 bar passes in total (three for every direction), plus four blank periods of 38.4 s when no bar stimulus was shown. Throughout the experiment, a gray disk of 9.6 arcmin (60 cd/m<sup>2</sup>), with a 4.2 arcmin black rim (0 cd/m<sup>2</sup>) was present on the screen as a fixation mark.</p><p>Although our stimulus spanned roughly 5% of the visual field (assuming a visual field of 144 dva), this corresponds to roughly 25% of V1 surface area because of cortical magnification (<xref ref-type="bibr" rid="bib1">Adams and Horton, 2003</xref>). In addition, as cortical magnification increases up the visual hierarchy, this estimate provides a lower bound for the proportion of cortical surface stimulated by our stimulus (<xref ref-type="bibr" rid="bib37">Harvey and Dumoulin, 2011</xref>). We opted for this relatively small stimulus size as increasing the stimulus size also increases bar step size (assuming identical stimulation time). This in turn would have decreased the spatial precision at which (changes in) pRF parameters could be determined. Throughout the results section, we use the terms 'parafoveal' and 'peripheral' to indicate positions relative to the stimulated portion of visual field.</p><p>The bar stimulus was composed of 1500 Gabor elements (4.34 cycle/dva spatial frequency, 9 arcmin sd, average luminance of 60 cd/m<sup>2</sup>) projected over a dark-gray background (15 cd/m<sup>2</sup>). Three times per bar location (i.e. every 533 ms), Gabor element parameters were updated to a new random location (uniformly distributed over the spatial extent of the bar at full width), a random orientation (uniformly drawn between 0 and 360°), a random color combination (either blue/yellow, or green/magenta) and a random new temporal frequency (TF; either high or low). We opted for color pairs that were opposite in terms of color opponency (<xref ref-type="bibr" rid="bib41">Hering, 1874</xref>). This ensured that the Gabor peaks and troughs were made up of colors that provided strong color contrast. In addition, this meant that we presented stimuli to all color channels, in effect serving as a potent and full-range color stimulus. The high and low temporal frequencies were chosen per participant to facilitate their ability to distinguish TF changes (6 and 4 Hz in three participants, 7 and 3 Hz in two participants). The overall color and/or TF composition of the bar was transiently altered on some of these parameter updates, by changing the ratio of Gabor elements assigned either color combination or either TF (as targets for the behavioral tasks, see below). The temporal predictability of these events was minimized by randomly drawing occurrences according to an exponential distribution (mean 4 s, minimum 2 s). Additionally, the fixation mark central disk luminance either increased or decreased, with probability and duration of these occurrences identical to those of changes in the bar stimulus composition. These three types of transients (fixation mark luminance, bar color and TF composition) were independent, meaning they were randomly interleaved and could be combined on the screen. Importantly, this design ensured that physical stimulation was equated across all three attention conditions, which we describe below.</p></sec><sec id="s4-3-2"><title>Attention-PRF mapping task</title><p>For an overview of the stimulus and behavioral task see <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Before each bar pass an automated voice (Apple OSX Dictation voice 'Kathy') informed participants to perform a two-alternative forced-choice task (2AFC) on one of the three stimulus parameter deviations. Task-relevant visual stimulus changes were accompanied by an auditory pure tone (440 Hz). This auditory cue alerted the participant to respond, while task-irrelevant stimulus changes occurred independently and without warning tone. This ensured that all task-related information was conveyed to the participant by auditory means, without concurrent changes in visual stimulation. The different stimulus changes (i.e. color, TF and fixation luminance) occurred independently and thus sometimes simultaneously, meaning the auditory tone was not reliably predictive of the stimulus dimension to attend. Therefore, participants needed to stably maintain condition-specific top-down attention throughout the duration of a bar pass. In the <italic>Attend Color</italic> condition, participants judged the relative predominance of blue/yellow or green/magenta Gabor elements in the bar stimulus, whereas in the <italic>Attend TF</italic> condition, participants judged the relative predominance of high compared to low TF Gabor elements in the bar stimulus. In the <italic>Attend Fixation</italic> condition, participants judged whether the central disk of the fixation mark increased or decreased in luminance. The magnitude of the stimulus changes was titrated by means of a Quest staircase procedure (<xref ref-type="bibr" rid="bib112">Watson and Pelli, 1983</xref>), set to approximate 83% correct performance. To equate task difficulty across not only conditions but also bar stimulus eccentricity, we used separate Quest staircases at three different bar stimulus eccentricities in each of the attention conditions. Additionally, there was a separate staircase for the <italic>Attend Fixation</italic> task when no bar stimulus was on screen. This made for a total of 10 separate staircases during the experiment. Participants extensively practiced the task outside the scanner and staircases were reset before scanning. Each experimental run contained one bar pass per task condition, per direction, in random order (total of 24 bar passes per run).</p></sec><sec id="s4-3-3"><title>Feature preference and HRF mapper</title><p>We performed a separate randomized fast event-related fMRI experiment to (1) determine each voxel's relative preference for color and TF, and (2) to find the parameters that best described each participant's HRF, to be used in the pRF estimation procedure (see below). Full-field stimuli consisted of 8000 Gabor elements, uniformly distributed throughout the full circular aperture traversed by the pRF mapping stimulus ensuring identical density compared to the pRF mapping stimulus. Also, every 533 ms, all Gabor elements were assigned a new random orientation and location. These stimuli were presented for 3.2 s, with an inter-trial interval of 3.2 s. In a full factorial 2 × 2 design, we varied the color and TF content of the stimulus in an on-off fashion. That is, the TF of the Gabor elements was either 0 or 7 Hz, and the elements were either grayscale or colored (balanced blue/yellow and green/magenta). Trial order was determined based on an M-sequence (<xref ref-type="bibr" rid="bib14">Buracas and Boynton, 2002</xref>), with no-stimulus (null) trials interspersed as a fifth category of trials. During this experiment, participants performed the same 2-AFC fixation-point luminance task as in the Attention-pRF Mapping Task (<italic>Attend Fixation</italic>), using a separate staircase. A single HRF was determined per participant using the R1-GLM approach (<xref ref-type="bibr" rid="bib76">Pedregosa et al., 2015</xref>) on data from all conditions. The median HRF from the 1000 most responsive voxels (highest beta-weights in the colored high TF condition) was used as the participant-specific HRF. Visualizing the anatomical distribution of these voxels predominantly returned locations within early visual cortex. This, combined with the fact that attention was directed away from the stimulus, confirms that HRF estimation was mainly based on bottom-up color- and temporal frequency-driven visual responses.</p></sec></sec><sec id="s4-4"><title>Data analysis</title><sec id="s4-4-1"><title>MRI preprocessing</title><p>T1-weighted images were first segmented automatically using Freesurfer, after which the pial and grey/white matter surfaces were hand-edited. Regions of interest (ROIs) were defined on surface projected retinotopic maps using Freesurfer without the use of spatial smoothing. For every participant, one session's EPI image was selected as the target EPI, which was registered to his/her Freesurfer segmented T1-weighted image using the bbregister utility, after which the registration was hand-adjusted. Then, all EPI images were first motion corrected to their middle volume using FSL (<xref ref-type="bibr" rid="bib48">Jenkinson et al., 2012</xref>) MCFLIRT to correct for within-run motion. Then, all EPI images were registered both linearly (using FLIRT) and non-linearly (using FNIRT) to the mean-motion corrected target EPI to correct for between-run and session motion and inhomogeneities in B0 field. Low-frequency drifts were removed using a third order Savitzky-Golay filter (<xref ref-type="bibr" rid="bib90">Savitzky and Golay, 1964</xref>) with a window length of 120 s. Arbitrary BOLD units were converted to percent-signal change on a per-run basis.</p></sec><sec id="s4-4-2"><title>pRF fitting procedure</title><p>pRF fitting and (statistical) parameter processing was performed using custom-written python pipeline (<xref ref-type="bibr" rid="bib109">van Es and Knapen, 2018</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/eLifeProduction/pRF_attention_analysis">https://github.com/eLifeProduction/pRF_attention_analysis</ext-link>). Links to the data files required to reproduce the figures and analyses can be found in the readme of this repository. The fitting routines relied heavily on the scipy and numpy packages. We approximated the pRF by a two-dimensional isotropic Gaussian function. For an overview of our pRF fitting procedure see <xref ref-type="fig" rid="fig1">Figure 1B</xref>. A predicted time course for a given Gaussian function can be created by first computing the overlap of this function with a model of the stimulus for each timepoint, and then convolving this overlap with the participant-specific HRF (<xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>). It is possible to find these Gaussian parameter estimates using a minimization algorithm, but such an approach is at risk of missing the global optimum when parameters are not initialized at appropriate values. Recently, a model-free reverse-correlation-like method was developed, generating a pRF spatial profile without requiring any pre-set parameters (for details see <xref ref-type="bibr" rid="bib60">Lee et al., 2013</xref>). Briefly, we employed this method using L2 regularized (Ridge) regression on a participant-specific-HRF convolved design matrix coding the stimulus position in a 31x31 grid for each timepoint, predicting data from all attention conditions together. Using a high regularization parameter (<inline-formula><mml:math id="inf7"><mml:mi>λ</mml:mi></mml:math></inline-formula> = 10<sup>6</sup>), we used this procedure not to maximize explained signal variance, but to robustly determine the pRF center, which was defined as the position of the maximum weight. Having determined these approximate initial values for the pRF center, we next initialized a minimization procedure (<xref ref-type="bibr" rid="bib81">Powell (1964)</xref> algorithm) at these location values, fitting position (<inline-formula><mml:math id="inf8"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:math></inline-formula>), size, baseline and amplitude parameters of an isotropic 2D Gaussian to data from all conditions together using a design matrix with size 101x101 for enhanced precision. Then, all resulting Gaussian parameters were used to initialize a second minimization procedure which fitted a Gaussian for each attention condition separately at the same time (all parameters independent except for one shared baseline parameter). This approach allowed us to recover fine-grained differences in pRF parameters under conditions of differential attention.</p></sec><sec id="s4-4-3"><title>pRF selection</title><p>We discarded pRFs that were either at the edge of the stimulus region (above 3.3 dva in the <italic>Attend Fixation</italic> condition), or had size (standard deviation) larger than our stimulus diameter (7.2 dva) in any of the tasks (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for details on the number of voxels that were rejected based on these criteria). Additionally, each voxel's contribution to all analyses was weighted according to the quality of fit of the pRF model, which was defined as 1 minus the ratio of residual to observed variance:<disp-formula id="equ1"><mml:math id="m1"><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow> <mml:mi/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow> <mml:mi/><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula>where <inline-formula><mml:math id="inf9"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf10"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:mi>p</mml:mi></mml:math></inline-formula> refer to voxel index, measured BOLD time-course and predicted time-course, respectively. We disregarded voxels with an <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-4-4"><title>pRF parameter analyses</title><p>Our sample size and statistical procedure were based on two recent and comparable studies (<xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Kay et al., 2015</xref>) that adopted a 'dense sampling of individual brain approach' (<xref ref-type="bibr" rid="bib79">Poldrack, 2017</xref>). This approach favors careful measurement of individual brains at the expense of large sample sizes in terms of the number of subjects. After defining visual ROIs per participant using standard retinotopic mapping procedures (<xref ref-type="bibr" rid="bib32">Dumoulin and Wandell, 2008</xref>), this entailed pooling voxels for each ROI across participants. These results are presented in the main text and in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Tables 1-10. To verify that resulting effects are not driven by single participants, we also performed all analyses for each participant separately, the results of which can be found in the figure supplements and in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> -Tables 11-26. Within the figure supplements, we refer to the pooling of voxels across subjects as ‘super subject’, to the analyses of individual subjects as ‘per subject’ and to the average across subjects as ‘over subjects’.</p><p>When analyses were computed across voxels (cf. <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig4">4</xref>, <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig7">7</xref> and in the ‘super subject’ and ‘per subject’ methods in the figure supplements), p-values and confidence intervals were computed using 10<sup>5</sup> fold bootstrap procedures. To test whether bootstrapped distributions differed from a certain threshold, p-values were defined as the ratio of bootstrap samples below versus above that threshold multiplied by 2 (all reported p-values are two-tailed). In the ‘super subject’ method, resulting p-values were corrected for multiple comparisons across ROIs using False Discovery Rate (FDR). When analyses were computed across participants, (cf. <xref ref-type="fig" rid="fig5">Figures 5</xref>, <xref ref-type="fig" rid="fig8">8</xref> and <xref ref-type="fig" rid="fig9">9</xref> and the ‘over subjects’ method in the figure supplements), statistics were performed using ANOVAs and/or t-tests. Our rationale for using bootstrapping across voxels and more classical tests across subjects is that while bootstrapping closely represents the actual distribution of the data, it is not stable for N &lt; 20. We therefore made the additional assumption of normality when computing statistics across subjects.</p><p>The computation of the attentional modulation index (see below) included a division operation. This meant that when the denominator was very close to 0, this modulation index resulted in very extreme values. To prevent these extreme values from obfuscating general tendencies, we used a conservative threshold of five two-sided median absolute deviations. For reasons of consistency, we used this same outlier rejection procedure for all other analyses (resulting in slightly different N per analysis, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> - Tables). Importantly, in all of these remaining analyses, outlier rejection did not meaningfully alter any result. Outlier rejection was performed per visual field bin (cf. <xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig3">3B/D</xref>, <xref ref-type="fig" rid="fig4">4A/B</xref> and <xref ref-type="fig" rid="fig6">6A/B/C</xref>, scatters in <xref ref-type="fig" rid="fig7">Figure 7</xref>), per percentile bin (cf. <xref ref-type="fig" rid="fig4">Figure 4C</xref>), or per ROI (cf. 3C, 6D/E, correlations in <xref ref-type="fig" rid="fig7">Figure 7</xref>). When analyses were performed across participants, outliers were again rejected at the voxel level per participant, but all participants were always included (cf. <xref ref-type="fig" rid="fig5">Figure 5A/B/C/D/E/F and and 8</xref>). When comparing correlations to 0, correlations were Fisher transformed using the inverse hyperbolic tangent function.</p></sec><sec id="s4-4-5"><title>Feature attention modulation index</title><p>We computed a per-voxel index to quantify how strongly feature-based attention modulated the effects of spatial attention on spatial sampling (feature-based attention modulation index, or feature AMI). This measure combined pRF eccentricity and size parameters, as our results showed that spatial attention affected these parameters in concert (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). Per voxel and per attention condition to the bar stimulus (<italic>Attend Color</italic> and <italic>Attend TF</italic>), we set up a two-dimensional vector containing difference in pRF eccentricity and size relative to the <italic>Attend Fixation</italic> condition. To ensure that pRF size and eccentricity contributed equally to the feature AMI, differences in pRF parameters in the <italic>Attend Color</italic> and <italic>Attend TF</italic> conditions relative to the <italic>Attend Fixation</italic> condition were normalized by the variance in the difference between the <italic>Attend Stimulus</italic> and <italic>Attend Fixation</italic> conditions. We then computed a feature AMI by dividing the difference between the norms of these vectors by their sum. This way, positive values of feature AMI indicate greater spatial attention effects on pRF parameters in the <italic>Attend Color</italic> condition than in the <italic>Attend TF</italic> condition and vice versa. Note that this measure quantifies the effects of feature-based attention regardless of the affected pRF parameter (i.e. eccentricity and size) and the sign of these changes (i.e. shifts toward or away from the fovea and increases or decreases in size).</p></sec><sec id="s4-4-6"><title>Attentional gain field modeling</title><p>To provide a parsimonious mechanistic account of how attention to the moving bar stimulus changed pRF position, we adapted an existing attentional gain model of attention (<xref ref-type="bibr" rid="bib120">Womelsdorf et al., 2008</xref>; <xref ref-type="bibr" rid="bib84">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib55">Klein et al., 2014</xref>). This model conceptualizes the measured Gaussian pRF as the multiplication between a Gaussian stimulus-driven pRF (<italic>SD</italic>, i.e. the pRF outside the influence of attention), and a Gaussian attentional gain field (<italic>AF</italic>). Following the properties of Gaussian multiplication, the narrower the AF the stronger the influence on the SD, and the narrower the SD the smaller the resulting shift. An overview of model mechanics is shown in <xref ref-type="fig" rid="fig10">Figure 10</xref>. We estimated the SD by dividing the measured <italic>Attend Fixation</italic> pRF by an AF at fixation (<inline-formula><mml:math id="inf13"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <xref ref-type="fig" rid="fig10">Figure 10A</xref>). As attention to the stimulus shifted the locus of attention as the bar moved across the screen, we modeled the effect of attention for each bar stimulus position separately (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Each unique bar stimulus (24 bar positions for each of 8 directions) was convolved with a Gaussian kernel (<inline-formula><mml:math id="inf14"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and multiplied with the estimated SD. This yielded one predicted pRF per bar position. These predicted pRFs were then scaled to a maximum of 1 and averaged over the different bar positions. This averaged profile essentially represented the pRF 'smeared' across visual space as spatial attention moved along with the bar stimulus throughout the recording. The peak of this averaged profile was then designated as the predicted pRF center location in the <italic>Attend Stimulus</italic> condition (<xref ref-type="fig" rid="fig10">Figure 10C</xref>). Thus, the modeling procedure consisted of two separate AFs, one at fixation (<inline-formula><mml:math id="inf15"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and one convolved with the bar stimulus (<inline-formula><mml:math id="inf16"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), which were estimated at the same time. The input to the model was the <italic>Attend Fixation</italic> pRF, and the output was the predicted position for the <italic>Attend Stimulus</italic> pRF. Formally, this is given by: <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow> <mml:mi/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac> <mml:mi/><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∙</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∙</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula>where <inline-formula><mml:math id="inf18"><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> represents the stimulus design matrix at timepoint <inline-formula><mml:math id="inf19"><mml:mi>t</mml:mi></mml:math></inline-formula> convolved with the AF toward the stimulus, <inline-formula><mml:math id="inf20"><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> represents the estimation of the SD, and the denominator ensures scaling to a maximum of 1. To estimate how well a set of AF parameters fit the data across the entire visual field, we minimized the L2 distance between the predicted and measured <italic>Attend Stimulus</italic> pRF positions of the 64 vectors derived from the quadrant visual field format of <xref ref-type="fig" rid="fig3">Figure 3B</xref>. AF sizes were determined at an ROI level, thus assuming that attention influenced all pRFs within an ROI similarly, while possibly varying between ROIs. The model was evaluated for a 50 x 50 evenly spaced grid of AF sizes, where the AF at fixation varied between 1.5 and 2.5 dva, and the AF convolved with the stimulus varied between 0.6 and 1.6 dva (i.e. 0.02 dva precision). The convolution between the stimulus and the <inline-formula><mml:math id="inf21"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> resulted in effective AF size to be 0.9 (bar stimulus width) larger than the <inline-formula><mml:math id="inf22"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> itself. These parameter ranges therefore result in equal effective AF sizes. Reported sizes are the standard deviation of the 2D Gaussians, with 0.9 (the bar width) added to <inline-formula><mml:math id="inf23"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> sizes. Modeling was performed for each participant separately.</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.36928.027</object-id><label>Figure 10.</label><caption><title>Schematic overview of modeling procedure.</title><p>(<bold>A</bold>) The Stimulus Drive (SD) was estimated by dividing the measured Attend Fixation pRF by an AF at fixation (<inline-formula><mml:math id="inf24"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). (<bold>B</bold>) Attention toward the bar stimulus at a given timepoint <inline-formula><mml:math id="inf25"><mml:mi>t</mml:mi></mml:math></inline-formula> was modeled as the multiplication of the estimated SD with the bar stimulus at that timepoint (<inline-formula><mml:math id="inf26"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> ) convolved with another AF (<inline-formula><mml:math id="inf27"><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). These predicted Attend Stimulus pRFs were averaged over all timepoints. The maximum position of this profile was taken as the predicted Attend Stimulus pRF position. (<bold>C</bold>) The predicted pRF shift ran from the measured Attend Fixation pRF toward the predicted Attend Stimulus position.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-36928-fig10-v1.tif"/></fig></sec><sec id="s4-4-7"><title>Gaze data processing</title><p>Gaze data was cleaned by linearly interpolating blinks detected by the EyeLink software. Transient occasions in which the tracker lost the pupil because of partial occlusion by the eyelid leading to high-frequency, high-amplitude signal components were detected and corrected as follows. Pupil size was first high-pass filtered at 10 Hz (the pupil impulse response function is a low-pass filter with a cutoff below 10 Hz (<xref ref-type="bibr" rid="bib56">Knapen et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Korn and Bach, 2016</xref>), after which those timepoints in which the acceleration of pupil size was greater than 10<sup>5</sup> mm/s, and their neighbors within 5 s, were replaced with NaN values. Drift correction was performed within each bar-pass by subtracting the median gaze position. All gaze positions were rotated to the direction of bar movement, after which we analyzed the median and variance (standard deviation) of the component in the direction of bar movement (i.e. the component relevant for the pRF measurement).</p></sec></sec><sec id="s4-5"><title>Statistical tables file</title><p>All statistical Tables referenced in this manuscript can be found in a separate file attached to this submission, termed ‘<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>’. These Tables specify the N, effect sizes and p-values for all ROIs and for the different statistical methods (‘super subject’, ‘over subjects’ and ‘per subject’; see Materials and methods for definitions).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This study was supported in part by an Open Research Area grant (ORA; #464-11-030) issued by the Netherlands Organization for Scientific Research (NWO) to JT.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Funding acquisition, Writing—original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Participant signed informed consent before participation in this study. The study was approved by the ethical review board of the University of Amsterdam (approval number 2016-BC-7145).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.36928.028</object-id><label>Supplementary file 1.</label><caption><title>Statistical tables.</title></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-36928-supp1-v1.pdf"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.36928.029</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-36928-transrepform-v1.docx"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All attention-pRF and feature-GLM results have been deposited on Figshare at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.c.4012717.v1">https://doi.org/10.6084/m9.figshare.c.4012717.v1</ext-link>, under a Public Domain Dedication License. pRF fitting and (statistical) parameter processing code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/daanvanes/PRF_attention_analysis">https://github.com/daanvanes/PRF_attention_analysis</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/eLifeProduction/pRF_attention_analysis">https://github.com/eLifeProduction/pRF_attention_analysis</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Daniel</surname><given-names>Marten van Es</given-names></name><name><surname>Jan</surname><given-names>Theeuwes</given-names></name><name><surname>Tomas</surname><given-names>Knapen</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>pRF spatial and feature-based attention data</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.c.4012717.v1</pub-id></element-citation></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname> <given-names>DL</given-names></name><name><surname>Horton</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A precise retinotopic map of primate striate cortex generated from the representation of angioscotomas</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>3771</fpage><lpage>3789</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-09-03771.2003</pub-id><pub-id pub-id-type="pmid">12736348</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altman</surname> <given-names>DG</given-names></name><name><surname>Bland</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Statistics notes: Absence of evidence is not evidence of absence</article-title><source>Bmj</source><volume>311</volume><elocation-id>485</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.311.7003.485</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amano</surname> <given-names>K</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual field maps, population receptive field sizes, and visual field coverage in the human MT+ complex</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>2704</fpage><lpage>2718</lpage><pub-id pub-id-type="doi">10.1152/jn.00102.2009</pub-id><pub-id pub-id-type="pmid">19587323</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname> <given-names>SK</given-names></name><name><surname>Fuchs</surname> <given-names>S</given-names></name><name><surname>Müller</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Effects of feature-selective and spatial attention at different stages of visual processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>238</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21328</pub-id><pub-id pub-id-type="pmid">19702461</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anton-Erxleben</surname> <given-names>K</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1038/nrn3443</pub-id><pub-id pub-id-type="pmid">23422910</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azzopardi</surname> <given-names>P</given-names></name><name><surname>Jones</surname> <given-names>KE</given-names></name><name><surname>Cowey</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Uneven mapping of magnocellular and parvocellular projections from the lateral geniculate nucleus to the striate cortex in the macaque monkey</article-title><source>Vision Research</source><volume>39</volume><fpage>2179</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(98)00319-8</pub-id><pub-id pub-id-type="pmid">10343800</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname> <given-names>D</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbot</surname> <given-names>A</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention Modifies Spatial Resolution According to Task Demands</article-title><source>Psychological Science</source><volume>28</volume><fpage>285</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1177/0956797616679634</pub-id><pub-id pub-id-type="pmid">28118103</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengson</surname> <given-names>JJ</given-names></name><name><surname>Lopez-Calderon</surname> <given-names>J</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The spotlight of attention illuminates failed feature-based expectancies</article-title><source>Psychophysiology</source><volume>49</volume><fpage>1101</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2012.01385.x</pub-id><pub-id pub-id-type="pmid">22775503</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bichot</surname> <given-names>NP</given-names></name><name><surname>Heard</surname> <given-names>MT</given-names></name><name><surname>DeGennaro</surname> <given-names>EM</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Source for Feature-Based Attention in the Prefrontal Cortex</article-title><source>Neuron</source><volume>88</volume><fpage>832</fpage><lpage>844</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.001</pub-id><pub-id pub-id-type="pmid">26526392</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brewer</surname> <given-names>AA</given-names></name><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Wade</surname> <given-names>AR</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual field maps and stimulus selectivity in human ventral occipital cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1102</fpage><lpage>1109</lpage><pub-id pub-id-type="doi">10.1038/nn1507</pub-id><pub-id pub-id-type="pmid">16025108</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname> <given-names>GJ</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding and reconstructing color from responses in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13992</fpage><lpage>14003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id><pub-id pub-id-type="pmid">19890009</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname> <given-names>GJ</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Categorical clustering of the neural representation of color</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15454</fpage><lpage>15465</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2472-13.2013</pub-id><pub-id pub-id-type="pmid">24068814</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buracas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Efficient design of event-related fMRI experiments using M-sequences</article-title><source>NeuroImage</source><volume>16</volume><fpage>801</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1116</pub-id><pub-id pub-id-type="pmid">12169264</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnett</surname> <given-names>KE</given-names></name><name><surname>Close</surname> <given-names>AC</given-names></name><name><surname>d'Avossa</surname> <given-names>G</given-names></name><name><surname>Sapir</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spatial attention can be biased towards an expected dimension</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>69</volume><fpage>2218</fpage><lpage>2232</lpage><pub-id pub-id-type="doi">10.1080/17470218.2015.1111916</pub-id><pub-id pub-id-type="pmid">27033515</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrasco</surname> <given-names>M</given-names></name><name><surname>Yeshurun</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The contribution of covert attention to the set-size and eccentricity effects in visual search</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>24</volume><fpage>673</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.24.2.673</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cass</surname> <given-names>J</given-names></name><name><surname>Van der Burg</surname> <given-names>E</given-names></name><name><surname>Alais</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Finding flicker: critical differences in temporal frequency capture attention</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00320</pub-id><pub-id pub-id-type="pmid">22110460</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cepeda</surname> <given-names>NJ</given-names></name><name><surname>Cave</surname> <given-names>KR</given-names></name><name><surname>Bichot</surname> <given-names>NP</given-names></name><name><surname>Kim</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Spatial selection via feature-driven inhibition of distractor locations</article-title><source>Perception &amp; Psychophysics</source><volume>60</volume><fpage>727</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.3758/BF03206059</pub-id><pub-id pub-id-type="pmid">9682600</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chawla</surname> <given-names>D</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The physiological basis of attentional modulation in extrastriate visual areas</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>671</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1038/10230</pub-id><pub-id pub-id-type="pmid">10404202</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Using neuronal populations to study the mechanisms underlying spatial and feature attention</article-title><source>Neuron</source><volume>70</volume><fpage>1192</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.029</pub-id><pub-id pub-id-type="pmid">21689604</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>A</given-names></name><name><surname>Shoup</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Perceptual dimensional constraints in response selection processes</article-title><source>Cognitive Psychology</source><volume>32</volume><fpage>128</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1006/cogp.1997.0648</pub-id><pub-id pub-id-type="pmid">9095680</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname> <given-names>CE</given-names></name><name><surname>Preddie</surname> <given-names>DC</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Spatial attention effects in macaque area V4</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>3201</fpage><lpage>3214</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-09-03201.1997</pub-id><pub-id pub-id-type="pmid">9096154</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Miezin</surname> <given-names>FM</given-names></name><name><surname>Dobmeyer</surname> <given-names>S</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Attentional modulation of neural processing of shape, color, and velocity in humans</article-title><source>Science</source><volume>248</volume><fpage>1556</fpage><lpage>1559</lpage><pub-id pub-id-type="doi">10.1126/science.2360050</pub-id><pub-id pub-id-type="pmid">2360050</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Akbudak</surname> <given-names>E</given-names></name><name><surname>Conturo</surname> <given-names>TE</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Ollinger</surname> <given-names>JM</given-names></name><name><surname>Drury</surname> <given-names>HA</given-names></name><name><surname>Linenweber</surname> <given-names>MR</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name><name><surname>Raichle</surname> <given-names>ME</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A common network of functional areas for attention and eye movements</article-title><source>Neuron</source><volume>21</volume><fpage>761</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80593-0</pub-id><pub-id pub-id-type="pmid">9808463</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Çukur</surname> <given-names>T</given-names></name><name><surname>Nishimoto</surname> <given-names>S</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention during natural vision warps semantic representation across the human brain</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>763</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/nn.3381</pub-id><pub-id pub-id-type="pmid">23603707</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname> <given-names>CA</given-names></name><name><surname>Sloan</surname> <given-names>KR</given-names></name><name><surname>Kalina</surname> <given-names>RE</given-names></name><name><surname>Hendrickson</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Human photoreceptor topography</article-title><source>The Journal of Comparative Neurology</source><volume>292</volume><fpage>497</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1002/cne.902920402</pub-id><pub-id pub-id-type="pmid">2324310</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname> <given-names>R</given-names></name><name><surname>DeYoe</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>I know where you are secretly attending! The topography of human visual attention revealed with fMRI</article-title><source>Vision Research</source><volume>49</volume><fpage>1037</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.01.014</pub-id><pub-id pub-id-type="pmid">19533912</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Mazer</surname> <given-names>JA</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title><source>Neuron</source><volume>59</volume><fpage>509</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.07.001</pub-id><pub-id pub-id-type="pmid">18701075</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haas</surname> <given-names>B</given-names></name><name><surname>Schwarzkopf</surname> <given-names>DS</given-names></name><name><surname>Anderson</surname> <given-names>EJ</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Perceptual load affects spatial tuning of neuronal populations in human early visual cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>R66</fpage><lpage>R67</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.11.061</pub-id><pub-id pub-id-type="pmid">24456976</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denison</surname> <given-names>RN</given-names></name><name><surname>Vu</surname> <given-names>AT</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Feinberg</surname> <given-names>DA</given-names></name><name><surname>Silver</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional mapping of the magnocellular and parvocellular subdivisions of human LGN</article-title><source>NeuroImage</source><volume>102</volume><fpage>358</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.019</pub-id><pub-id pub-id-type="pmid">25038435</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Knapen</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How visual cortical organization is altered by ophthalmologic and neurologic disorders</article-title><source>Annual Review of Vision Science</source><volume>4</volume><fpage>357</fpage><lpage>379</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091517-033948</pub-id><pub-id pub-id-type="pmid">29889657</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname> <given-names>T</given-names></name><name><surname>Monti</surname> <given-names>JM</given-names></name><name><surname>Trittschuh</surname> <given-names>EH</given-names></name><name><surname>Wieneke</surname> <given-names>CA</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name><name><surname>Mesulam</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural integration of top-down spatial and feature-based information in visual search</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>6141</fpage><lpage>6151</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1262-08.2008</pub-id><pub-id pub-id-type="pmid">18550756</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Found</surname> <given-names>A</given-names></name><name><surname>Müller</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Searching for unknown feature targets on more than one dimension: investigating a &quot;dimension-weighting&quot; account</article-title><source>Perception &amp; Psychophysics</source><volume>58</volume><fpage>88</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.3758/BF03205479</pub-id><pub-id pub-id-type="pmid">8668524</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gregoriou</surname> <given-names>GG</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Zhou</surname> <given-names>H</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>High-frequency, long-range coupling between prefrontal and visual cortex during attention</article-title><source>Science</source><volume>324</volume><fpage>1207</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1126/science.1171402</pub-id><pub-id pub-id-type="pmid">19478185</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Handy</surname> <given-names>TC</given-names></name><name><surname>Green</surname> <given-names>V</given-names></name><name><surname>Klein</surname> <given-names>RM</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Combined expectancies: event-related potentials reveal the early benefits of spatial attention that are obscured by reaction time measures</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>27</volume><fpage>303</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.27.2.303</pub-id><pub-id pub-id-type="pmid">11318049</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>BM</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The relationship between cortical magnification factor and population receptive field size in human visual cortex: constancies in cortical architecture</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13604</fpage><lpage>13612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2572-11.2011</pub-id><pub-id pub-id-type="pmid">21940451</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>BM</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual motion transforms visual space representations similarly throughout the human visual hierarchy</article-title><source>NeuroImage</source><volume>127</volume><fpage>173</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.070</pub-id><pub-id pub-id-type="pmid">26666897</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Time course of attention reveals different mechanisms for spatial and feature-based attention in area V4</article-title><source>Neuron</source><volume>47</volume><fpage>637</fpage><lpage>643</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.07.020</pub-id><pub-id pub-id-type="pmid">16129394</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Combined effects of spatial and feature-based attention on responses of V4 neurons</article-title><source>Vision Research</source><volume>49</volume><fpage>1182</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.06.011</pub-id><pub-id pub-id-type="pmid">18619996</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hering</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1874">1874</year><article-title>Grundrisse einer Theorie des Farbensinnes. Sitzungsberichte der osterreichischen Akademie der Wissenschaften, mathematisch-naturwissenschaftliche</article-title><source>Klasse Abteiling III</source><volume>70</volume><fpage>169</fpage><lpage>204</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hicks</surname> <given-names>TP</given-names></name><name><surname>Lee</surname> <given-names>BB</given-names></name><name><surname>Vidyasagar</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The responses of cells in macaque lateral geniculate nucleus to sinusoidal gratings</article-title><source>The Journal of Physiology</source><volume>337</volume><fpage>183</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1983.sp014619</pub-id><pub-id pub-id-type="pmid">6875927</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Münte</surname> <given-names>TF</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Selective attention to color and location: an analysis with event-related brain potentials</article-title><source>Perception &amp; Psychophysics</source><volume>36</volume><fpage>185</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.3758/BF03202679</pub-id><pub-id pub-id-type="pmid">6514528</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname> <given-names>JM</given-names></name><name><surname>Boelmans</surname> <given-names>K</given-names></name><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Attention to features precedes attention to locations in visual search: evidence from electromagnetic brain responses in humans</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>1822</fpage><lpage>1832</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3564-03.2004</pub-id><pub-id pub-id-type="pmid">14985422</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummer</surname> <given-names>A</given-names></name><name><surname>Ritter</surname> <given-names>M</given-names></name><name><surname>Tik</surname> <given-names>M</given-names></name><name><surname>Ledolter</surname> <given-names>AA</given-names></name><name><surname>Woletz</surname> <given-names>M</given-names></name><name><surname>Holder</surname> <given-names>GE</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Schmidt-Erfurth</surname> <given-names>U</given-names></name><name><surname>Windischberger</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Eyetracker-based gaze correction for robust mapping of population receptive fields</article-title><source>NeuroImage</source><volume>142</volume><fpage>211</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.003</pub-id><pub-id pub-id-type="pmid">27389789</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibos</surname> <given-names>G</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Interaction between spatial and feature attention in posterior parietal cortex</article-title><source>Neuron</source><volume>91</volume><fpage>931</fpage><lpage>943</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.025</pub-id><pub-id pub-id-type="pmid">27499082</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>Brady</surname> <given-names>DK</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attention improves encoding of task-relevant features in the human visual cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>8210</fpage><lpage>8219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6153-09.2011</pub-id><pub-id pub-id-type="pmid">21632942</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FSL</article-title><source>NeuroImage</source><volume>62</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id><pub-id pub-id-type="pmid">21979382</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerde</surname> <given-names>TA</given-names></name><name><surname>Merriam</surname> <given-names>EP</given-names></name><name><surname>Riggall</surname> <given-names>AC</given-names></name><name><surname>Hedges</surname> <given-names>JH</given-names></name><name><surname>Curtis</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prioritized maps of space in human frontoparietal cortex</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>17382</fpage><lpage>17390</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3810-12.2012</pub-id><pub-id pub-id-type="pmid">23197729</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname> <given-names>S</given-names></name><name><surname>Pinsk</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual attention as a multilevel selection process</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>4</volume><fpage>483</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.3758/CABN.4.4.483</pub-id><pub-id pub-id-type="pmid">15849892</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katzner</surname> <given-names>S</given-names></name><name><surname>Busse</surname> <given-names>L</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention to the color of a moving stimulus modulates motion-signal processing in macaque area mt: evidence for a unified attentional system</article-title><source>Frontiers in Systems Neuroscience</source><volume>3</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.012.2009</pub-id><pub-id pub-id-type="pmid">19893762</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Weiner</surname> <given-names>KS</given-names></name><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention reduces spatial uncertainty in human ventral temporal cortex</article-title><source>Current Biology</source><volume>25</volume><fpage>595</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.12.050</pub-id><pub-id pub-id-type="pmid">25702580</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Yeatman</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bottom-up and top-down computations in word- and face-selective cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22341</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22341</pub-id><pub-id pub-id-type="pmid">28226243</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingstone</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Combining Expectancies</article-title><source>The Quarterly Journal of Experimental Psychology Section A</source><volume>44</volume><fpage>69</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1080/14640749208401284</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname> <given-names>BP</given-names></name><name><surname>Harvey</surname> <given-names>BM</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attraction of position preference by spatial attention throughout human visual cortex</article-title><source>Neuron</source><volume>84</volume><fpage>227</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.047</pub-id><pub-id pub-id-type="pmid">25242220</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapen</surname> <given-names>T</given-names></name><name><surname>de Gee</surname> <given-names>JW</given-names></name><name><surname>Brascamp</surname> <given-names>J</given-names></name><name><surname>Nuiten</surname> <given-names>S</given-names></name><name><surname>Hoppenbrouwers</surname> <given-names>S</given-names></name><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cognitive and ocular factors jointly determine pupil responses under equiluminance</article-title><source>Plos One</source><volume>11</volume><elocation-id>e0155574</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0155574</pub-id><pub-id pub-id-type="pmid">27191166</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korn</surname> <given-names>CW</given-names></name><name><surname>Bach</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A solid frame for the window on cognition: Modeling event-related pupil responses</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.1167/16.3.28</pub-id><pub-id pub-id-type="pmid">26894512</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Behrmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Space-, object-, and feature-based attention interact to organize visual scenes</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>73</volume><fpage>2434</fpage><lpage>2447</lpage><pub-id pub-id-type="doi">10.3758/s13414-011-0201-z</pub-id><pub-id pub-id-type="pmid">22006523</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumada</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Feature-based control of attention: evidence for two forms of dimension weighting</article-title><source>Perception &amp; Psychophysics</source><volume>63</volume><fpage>698</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.3758/BF03194430</pub-id><pub-id pub-id-type="pmid">11436738</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Papanikolaou</surname> <given-names>A</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Smirnakis</surname> <given-names>SM</given-names></name><name><surname>Keliris</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A new method for estimating population receptive field topography in visual cortex</article-title><source>NeuroImage</source><volume>81</volume><fpage>144</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.026</pub-id><pub-id pub-id-type="pmid">23684878</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname> <given-names>CJ</given-names></name><name><surname>Balestreri</surname> <given-names>A</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interactions between space-based and feature-based attention</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>41</volume><fpage>11</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1037/xhp0000011</pub-id><pub-id pub-id-type="pmid">25285472</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>N</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Dougherty</surname> <given-names>RF</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical maps and white matter tracts following long period of visual deprivation and retinal image restoration</article-title><source>Neuron</source><volume>65</volume><fpage>21</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.12.006</pub-id><pub-id pub-id-type="pmid">20152110</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Specializations for chromatic and temporal signals in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>3459</fpage><lpage>3468</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4206-04.2005</pub-id><pub-id pub-id-type="pmid">15800201</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Love</surname> <given-names>J</given-names></name><name><surname>Selker</surname> <given-names>R</given-names></name><name><surname>Marsman</surname> <given-names>M</given-names></name><name><surname>Jamil</surname> <given-names>T</given-names></name><name><surname>Dropmann</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>JASP Project</data-title><source>Computer Software</source><version designator="0.7">Jasp (version 0.7)</version><publisher-name>Amsterdam, The Netherlands</publisher-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Chelazzi</surname> <given-names>L</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>24</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.1.24</pub-id><pub-id pub-id-type="pmid">9120566</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackey</surname> <given-names>WE</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Curtis</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual field map clusters in human frontoparietal cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22974</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22974</pub-id><pub-id pub-id-type="pmid">28628004</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname> <given-names>JH</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Feature-based attention in visual cortex</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>317</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.04.001</pub-id><pub-id pub-id-type="pmid">16697058</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname> <given-names>CJ</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attention to both space and feature modulates neuronal responses in macaque area V4</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>1751</fpage><lpage>1755</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.83.3.1751</pub-id><pub-id pub-id-type="pmid">10712494</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miconi</surname> <given-names>T</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A feedback model of attention explains the diverse effects of attention on neural firing rates and receptive field structure</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004770</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004770</pub-id><pub-id pub-id-type="pmid">26890584</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Armstrong</surname> <given-names>KM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selective gating of visual signals by microstimulation of frontal cortex</article-title><source>Nature</source><volume>421</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1038/nature01341</pub-id><pub-id pub-id-type="pmid">12540901</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neural correlates of attentive selection for color or luminance in extrastriate area V4</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>2178</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-04-02178.1994</pub-id><pub-id pub-id-type="pmid">8158264</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>MM</given-names></name><name><surname>Andersen</surname> <given-names>S</given-names></name><name><surname>Trujillo</surname> <given-names>NJ</given-names></name><name><surname>Valdés-Sosa</surname> <given-names>P</given-names></name><name><surname>Malinowski</surname> <given-names>P</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Feature-selective attention enhances color signals in early visual areas of the human brain</article-title><source>PNAS</source><volume>103</volume><fpage>14250</fpage><lpage>14254</lpage><pub-id pub-id-type="doi">10.1073/pnas.0606668103</pub-id><pub-id pub-id-type="pmid">16956975</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nordfang</surname> <given-names>M</given-names></name><name><surname>Staugaard</surname> <given-names>C</given-names></name><name><surname>Bundesen</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attentional weights in vision as products of spatial and nonspatial components</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>1043</fpage><lpage>1051</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1337-1</pub-id><pub-id pub-id-type="pmid">28634784</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Craven</surname> <given-names>KM</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>fMRI evidence for objects as the units of attentional selection</article-title><source>Nature</source><volume>401</volume><fpage>584</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1038/44134</pub-id><pub-id pub-id-type="pmid">10524624</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patzwahl</surname> <given-names>DR</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Combining spatial and feature-based attention within the receptive field of MT neurons</article-title><source>Vision Research</source><volume>49</volume><fpage>1188</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.04.003</pub-id><pub-id pub-id-type="pmid">19362573</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Eickenberg</surname> <given-names>M</given-names></name><name><surname>Ciuciu</surname> <given-names>P</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Data-driven HRF estimation for encoding and decoding models</article-title><source>NeuroImage</source><volume>104</volume><fpage>209</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.09.060</pub-id><pub-id pub-id-type="pmid">25304775</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Generating stimuli for neuroscience using psychoPy</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.010.2008</pub-id><pub-id pub-id-type="pmid">19198666</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pestilli</surname> <given-names>F</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attention enhances contrast sensitivity at cued and impairs it at uncued locations</article-title><source>Vision Research</source><volume>45</volume><fpage>1867</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.01.019</pub-id><pub-id pub-id-type="pmid">15797776</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Precision neuroscience: Dense sampling of individual brains</article-title><source>Neuron</source><volume>95</volume><fpage>727</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.002</pub-id><pub-id pub-id-type="pmid">28817793</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname> <given-names>MI</given-names></name><name><surname>Snyder</surname> <given-names>CR</given-names></name><name><surname>Davidson</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Attention and the detection of signals</article-title><source>Journal of Experimental Psychology: General</source><volume>109</volume><fpage>160</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.109.2.160</pub-id><pub-id pub-id-type="pmid">7381367</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Powell</surname> <given-names>MJD</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>An efficient method for finding the minimum of a function of several variables without calculating derivatives</article-title><source>The Computer Journal</source><volume>7</volume><fpage>155</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1093/comjnl/7.2.155</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puckett</surname> <given-names>AM</given-names></name><name><surname>DeYoe</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The attentional field revealed by single-voxel modeling of fMRI time courses</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>5030</fpage><lpage>5042</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3754-14.2015</pub-id><pub-id pub-id-type="pmid">25810532</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname> <given-names>JH</given-names></name><name><surname>Pasternak</surname> <given-names>T</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attention increases sensitivity of V4 neurons</article-title><source>Neuron</source><volume>26</volume><fpage>703</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81206-4</pub-id><pub-id pub-id-type="pmid">10896165</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname> <given-names>JH</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Riggio</surname> <given-names>L</given-names></name><name><surname>Dascola</surname> <given-names>I</given-names></name><name><surname>Umiltá</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Reorienting attention across the horizontal and vertical meridians: evidence in favor of a premotor theory of attention</article-title><source>Neuropsychologia</source><volume>25</volume><fpage>31</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(87)90041-8</pub-id><pub-id pub-id-type="pmid">3574648</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Capabilities and limitations of peripheral vision</article-title><source>Annual Review of Vision Science</source><volume>2</volume><fpage>437</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035733</pub-id><pub-id pub-id-type="pmid">28532349</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname> <given-names>AF</given-names></name><name><surname>Paradiso</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Feature-specific effects of selective visual attention</article-title><source>Vision Research</source><volume>35</volume><fpage>621</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)00156-G</pub-id><pub-id pub-id-type="pmid">7900301</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saenz</surname> <given-names>M</given-names></name><name><surname>Buracas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Global effects of feature-based attention in human visual cortex</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>631</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1038/nn876</pub-id><pub-id pub-id-type="pmid">12068304</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sàenz</surname> <given-names>M</given-names></name><name><surname>Buraĉas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Global feature-based attention for motion and color</article-title><source>Vision Research</source><volume>43</volume><fpage>629</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(02)00595-3</pub-id><pub-id pub-id-type="pmid">12604099</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savitzky</surname> <given-names>A</given-names></name><name><surname>Golay</surname> <given-names>MJE</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Smoothing and differentiation of data by simplified least squares procedures</article-title><source>Analytical Chemistry</source><volume>36</volume><fpage>1627</fpage><lpage>1639</lpage><pub-id pub-id-type="doi">10.1021/ac60214a047</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname> <given-names>PH</given-names></name><name><surname>Malpeli</surname> <given-names>JG</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Functional specificity of lateral geniculate nucleus laminae of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>41</volume><fpage>788</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1152/jn.1978.41.3.788</pub-id><pub-id pub-id-type="pmid">96227</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname> <given-names>KA</given-names></name><name><surname>Richter</surname> <given-names>MC</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Retinotopic organization and functional subdivisions of the human lateral geniculate nucleus: a high-resolution functional magnetic resonance imaging study</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>8975</fpage><lpage>8985</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2413-04.2004</pub-id><pub-id pub-id-type="pmid">15483116</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Hopf</surname> <given-names>JM</given-names></name><name><surname>Martinez</surname> <given-names>A</given-names></name><name><surname>Mai</surname> <given-names>HM</given-names></name><name><surname>Sattler</surname> <given-names>C</given-names></name><name><surname>Gasde</surname> <given-names>A</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatio-temporal analysis of feature-based attention</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2468</fpage><lpage>2477</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl154</pub-id><pub-id pub-id-type="pmid">17204821</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Hopf</surname> <given-names>JM</given-names></name><name><surname>Merkel</surname> <given-names>C</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Object-based attention involves the sequential activation of feature-specific cortical modules</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>619</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1038/nn.3656</pub-id><pub-id pub-id-type="pmid">24561999</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname> <given-names>JT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Feature-based attentional modulations in the absence of direct visual stimulation</article-title><source>Neuron</source><volume>55</volume><fpage>301</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.015</pub-id><pub-id pub-id-type="pmid">17640530</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheremata</surname> <given-names>SL</given-names></name><name><surname>Silver</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hemisphere-dependent attentional modulation of human parietal visual field representations</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>508</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2378-14.2015</pub-id><pub-id pub-id-type="pmid">25589746</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>d'Avossa</surname> <given-names>G</given-names></name><name><surname>Tansy</surname> <given-names>AP</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Two attentional processes in the parietal lobe</article-title><source>Cerebral Cortex</source><volume>12</volume><fpage>1124</fpage><lpage>1131</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.11.1124</pub-id><pub-id pub-id-type="pmid">12379601</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname> <given-names>MF</given-names></name><name><surname>Brascamp</surname> <given-names>JW</given-names></name><name><surname>Ferreira</surname> <given-names>S</given-names></name><name><surname>Castelo-Branco</surname> <given-names>M</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Harvey</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Radial asymmetries in population receptive field size and cortical magnification factor in early visual cortex</article-title><source>NeuroImage</source><volume>167</volume><fpage>41</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.021</pub-id><pub-id pub-id-type="pmid">29155078</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname> <given-names>MA</given-names></name><name><surname>Ress</surname> <given-names>D</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Topographic maps of visual spatial attention in human parietal cortex</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>1358</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1152/jn.01316.2004</pub-id><pub-id pub-id-type="pmid">15817643</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Grieve</surname> <given-names>KL</given-names></name><name><surname>Brotchie</surname> <given-names>P</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Separate body- and world-referenced representations of visual space in parietal cortex</article-title><source>Nature</source><volume>394</volume><fpage>887</fpage><lpage>891</lpage><pub-id pub-id-type="doi">10.1038/29777</pub-id><pub-id pub-id-type="pmid">9732870</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname> <given-names>TC</given-names></name><name><surname>Serences</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1879</fpage><lpage>1887</lpage><pub-id pub-id-type="doi">10.1038/nn.3574</pub-id><pub-id pub-id-type="pmid">24212672</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swisher</surname> <given-names>JD</given-names></name><name><surname>Halko</surname> <given-names>MA</given-names></name><name><surname>Merabet</surname> <given-names>LB</given-names></name><name><surname>McMains</surname> <given-names>SA</given-names></name><name><surname>Somers</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual topography of human intraparietal sulcus</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>5326</fpage><lpage>5337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0991-07.2007</pub-id><pub-id pub-id-type="pmid">17507555</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szczepanski</surname> <given-names>SM</given-names></name><name><surname>Pinsk</surname> <given-names>MA</given-names></name><name><surname>Douglas</surname> <given-names>MM</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name><name><surname>Saalmann</surname> <given-names>YB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Functional and structural architecture of the human dorsal frontoparietal attention network</article-title><source>PNAS</source><volume>110</volume><fpage>15806</fpage><lpage>15811</lpage><pub-id pub-id-type="doi">10.1073/pnas.1313903110</pub-id><pub-id pub-id-type="pmid">24019489</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theeuwes</surname> <given-names>J</given-names></name><name><surname>Van der Burg</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The role of spatial and nonspatial information in visual selection</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>33</volume><fpage>1335</fpage><lpage>1351</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.33.6.1335</pub-id><pub-id pub-id-type="pmid">18085947</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Hadjikhani</surname> <given-names>N</given-names></name><name><surname>Hall</surname> <given-names>EK</given-names></name><name><surname>Marrett</surname> <given-names>S</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Vaughan</surname> <given-names>JT</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The retinotopy of visual spatial attention</article-title><source>Neuron</source><volume>21</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80659-5</pub-id><pub-id pub-id-type="pmid">9883733</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Martínez Trujillo</surname> <given-names>JC</given-names></name><name><surname>Trujillo</surname> <given-names>JCM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Attentional modulation of visual motion processing in cortical areas MT and MST</article-title><source>Nature</source><volume>382</volume><fpage>539</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1038/382539a0</pub-id><pub-id pub-id-type="pmid">8700227</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trotter</surname> <given-names>Y</given-names></name><name><surname>Celebrini</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Gaze direction controls response gain in primary visual-cortex neurons</article-title><source>Nature</source><volume>398</volume><fpage>239</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/18444</pub-id><pub-id pub-id-type="pmid">10094046</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>van Es</surname> <given-names>DM</given-names></name><name><surname>Knapen</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title><italic>pRF_attention_analysis</italic></data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/daanvanes/PRF_attention_analysis">https://github.com/daanvanes/PRF_attention_analysis</ext-link></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Opstal</surname> <given-names>AJ</given-names></name><name><surname>Hepp</surname> <given-names>K</given-names></name><name><surname>Suzuki</surname> <given-names>Y</given-names></name><name><surname>Henn</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Influence of eye position on activity in monkey superior colliculus</article-title><source>Journal of Neurophysiology</source><volume>74</volume><fpage>1593</fpage><lpage>1610</lpage><pub-id pub-id-type="doi">10.1152/jn.1995.74.4.1593</pub-id><pub-id pub-id-type="pmid">8989396</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vo</surname> <given-names>VA</given-names></name><name><surname>Sprague</surname> <given-names>TC</given-names></name><name><surname>Serences</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial tuning shifts increase the discriminability and fidelity of population codes in visual cortex</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>37</volume><fpage>3386</fpage><lpage>3401</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-16.2017</pub-id><pub-id pub-id-type="pmid">28242794</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>AB</given-names></name><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>QUEST: a Bayesian adaptive psychometric method</article-title><source>Perception &amp; Psychophysics</source><volume>33</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.3758/BF03202828</pub-id><pub-id pub-id-type="pmid">6844102</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wegener</surname> <given-names>D</given-names></name><name><surname>Ehn</surname> <given-names>F</given-names></name><name><surname>Aurich</surname> <given-names>MK</given-names></name><name><surname>Galashan</surname> <given-names>FO</given-names></name><name><surname>Kreiter</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Feature-based attention and the suppression of non-relevant object features</article-title><source>Vision Research</source><volume>48</volume><fpage>2696</fpage><lpage>2707</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.08.021</pub-id><pub-id pub-id-type="pmid">18824190</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname> <given-names>AL</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Feature-based attention involuntarily and simultaneously improves visual performance across locations</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1167/11.6.15</pub-id><pub-id pub-id-type="pmid">21602553</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname> <given-names>AL</given-names></name><name><surname>Rolfs</surname> <given-names>M</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Stimulus competition mediates the joint effects of spatial and feature-based attention</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/15.14.7</pub-id><pub-id pub-id-type="pmid">26473316</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Horiguchi</surname> <given-names>H</given-names></name><name><surname>Sayres</surname> <given-names>RA</given-names></name><name><surname>Amano</surname> <given-names>K</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mapping hV4 and ventral occipital cortex: the venous eclipse</article-title><source>Journal of Vision</source><volume>10</volume><pub-id pub-id-type="doi">10.1167/10.5.1</pub-id><pub-id pub-id-type="pmid">20616143</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname> <given-names>JM</given-names></name><name><surname>Butcher</surname> <given-names>SJ</given-names></name><name><surname>Lee</surname> <given-names>C</given-names></name><name><surname>Hyle</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Changing your mind: on the contributions of top-down and bottom-up guidance in visual search for feature singletons</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>29</volume><fpage>483</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.29.2.483</pub-id><pub-id pub-id-type="pmid">12760630</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname> <given-names>JM</given-names></name><name><surname>Horowitz</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>495</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1038/nrn1411</pub-id><pub-id pub-id-type="pmid">15152199</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname> <given-names>T</given-names></name><name><surname>Anton-Erxleben</surname> <given-names>K</given-names></name><name><surname>Pieper</surname> <given-names>F</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamic shifts of visual receptive fields in cortical area MT by spatial attention</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1156</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/nn1748</pub-id><pub-id pub-id-type="pmid">16906153</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname> <given-names>T</given-names></name><name><surname>Anton-Erxleben</surname> <given-names>K</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>8934</fpage><lpage>8944</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4030-07.2008</pub-id><pub-id pub-id-type="pmid">18768687</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname> <given-names>Y</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Attention improves or impairs visual performance by enhancing spatial resolution</article-title><source>Nature</source><volume>396</volume><fpage>72</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1038/23936</pub-id><pub-id pub-id-type="pmid">9817201</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname> <given-names>Y</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Spatial attention improves performance in spatial resolution tasks</article-title><source>Vision Research</source><volume>39</volume><fpage>293</fpage><lpage>306</lpage><pub-id pub-id-type="pmid">10326137</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname> <given-names>Y</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The locus of attentional effects in texture segmentation</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>622</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1038/75804</pub-id><pub-id pub-id-type="pmid">10816320</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname> <given-names>Y</given-names></name><name><surname>Montagna</surname> <given-names>B</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>On the flexibility of sustained attention and its effects on a texture segmentation task</article-title><source>Vision Research</source><volume>48</volume><fpage>80</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.10.015</pub-id><pub-id pub-id-type="pmid">18076966</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanto</surname> <given-names>TP</given-names></name><name><surname>Rubens</surname> <given-names>MT</given-names></name><name><surname>Bollinger</surname> <given-names>J</given-names></name><name><surname>Gazzaley</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Top-down modulation of visual feature processing: the role of the inferior frontal junction</article-title><source>NeuroImage</source><volume>53</volume><fpage>736</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.012</pub-id><pub-id pub-id-type="pmid">20600999</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Feature-based attention modulates feedforward visual processing</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>24</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1038/nn.2223</pub-id><pub-id pub-id-type="pmid">19029890</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>H</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Feature-based attention in the frontal eye field and area V4 during visual search</article-title><source>Neuron</source><volume>70</volume><fpage>1205</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.032</pub-id><pub-id pub-id-type="pmid">21689605</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuiderbaan</surname> <given-names>W</given-names></name><name><surname>Harvey</surname> <given-names>BM</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Modeling center-surround configurations in population receptive fields using fMRI</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/12.3.10</pub-id><pub-id pub-id-type="pmid">22408041</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36928.033</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Spatial sampling in human visual cortex is modulated by both spatial and feature-based attention&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Sabine Kastner as the Senior Editor. The following individual involved in review of your submission has agreed to reveal his identity: Michael Silver (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study measured effects of feature-based attention on pRF eccentricity and size changes related to spatial attention. The authors investigated the hypothesis that feature-based attention optimizes the attended feature's spatial sampling by using a pRF mapping stimulus that varied along two feature dimensions: color and temporal frequency, and they found that attention to color showed stronger modulation of pRF eccentricity and size change compared to attention to temporal frequency. The authors interpret these results with an attentional gain field model by noting that receptive fields preferring color are typically smaller and more foveal compared to those preferring temporal frequency, consistent with these receptive fields having greater changes in eccentricity. They further demonstrate that attentional gain fields were smaller for the <italic>Attend Color</italic> condition, indicating greater precision of the attentional gain field for color receptive fields. The editors were impressed with this elegant and well-written paper that contains thorough and principled analysis procedures. However, they were concerned regarding some of the statistical analyses (see details below). Also, the authors should clarify how the results advance the field's knowledge about effects of spatial and feature-based attention (and their interaction). The application of the attentional gain field model to these findings was perceived as a strength. Overall, the editors felt that major revisions will be necessary to strengthen the study, so that it can be further pursued at <italic>eLife</italic>.</p><p>Essential revisions:</p><p>1) Feature-based attention has long been thought to have modulatory effects on spatial attention (Saenz, Buračas and Boynton, 2002; Maunsell and Treue, 2006). The current study supports this idea by reporting specific analyses where color and temporal frequency are used as attended features and spatial resampling is assessed by measuring populations receptive fields. Despite the effort, the findings of the study are mostly confirmatory in nature. The manuscript would benefit from a reorganized presentation of the results to highlight the novel contributions of the study as opposed to confirmation of prior findings.</p><p>2) There isn't adequate information about how the target features were selected. Why should we be interested in attending to temporal frequency versus color in the context of this study? (It is also unclear/unexplained why blue-yellow and cyan-magenta were selected as representative colors.) Would anything change if different colors, a broader range of temporal frequencies, or entirely different feature sets (e.g., orientation) were used?</p><p>3) It is reported that pRF changes for <italic>Attend Color</italic> case are larger compared to <italic>Attend Temporal Frequency</italic> case. It would, however, be better if the difference could be presented in a more conspicuous way. For instance, the way it is presented in Figure 5A gives the impression that the difference is almost negligible. If there is merely a slight difference, more detailed discussion is warranted regarding the reasons underlying pRF changes when attending to color vs. temporal frequency.</p><p>4) Related to the previous comment: When the regions primarily engaged by the two distinct features compared lie at different points in the hierarchy of visual cortex, the strength of retinotopic representations (and thus spatial) will inevitably vary. Wouldn't that introduce a bias in assessment of interactions between spatial and feature-based attention?</p><p>5) There is too much variability across subjects in the attentional gain field sizes of temporal frequency vs. color (Figure 7). Given that there appears to be poor consistency across subjects for this measure, this figure does not convey information in a convincing way.</p><p>6) The authors combine pRF eccentricity and size to obtain a single index, because they claim there's a high correlation between the two. We don't think this is warranted. For instance, Table 22 shows that there's a significant correlation between eccentricity and size changes in V1 for only 2 out of 5 subjects, and the average correlation is not very high for this ROI either (R=0.461). This also makes it questionable to present the results using voxels from a &quot;combined ROI&quot;, given the large degree of variability across individual ROIs.</p><p>7) In most of the supplementary figures (e.g., the first one), results for individual subjects are not shown at the same scale across subjects, which makes them confusing. Also, there's barely any change in the pRF size with increasing eccentricity for some ROIs of some subjects (e.g., V1 of s5). Some extended treatment and discussion of the consistency of the presented results across subjects would greatly benefit the manuscript.</p><p>8) We found it difficult that when statistical analyses are performed across voxels bootstrap tests were applied, and yet across subject tests were based on ANOVA or t-tests. If the same type of metric is evaluated in both cases, shouldn't you make the same assumptions about the distribution of the metric?</p><p>9) More detailed description of the outlier rejection method is needed, as this changes the set of voxels over which the results are reported. This procedure can potentially introduce biases within and across analyses, so it must be motivated very clearly. It is uncommon to reject voxels like this. If the goal is to report robust estimates of central tendency and variance of parameter estimates, then why not calculate them based on median and IQR?</p><p>10) It is stated that explained variance during pRF estimation is used to weigh the contributions of individual voxels to estimated parameters. This is understandable, but we would have expected to see actual reports of the explained variance across ROIs as well. This could help readers to judge and interpret the differences observed across ROIs.</p><p>11) The stimulus aperture size comprises a very small proportion of the visual field and is also much smaller than those used in other fMRI studies of attentional modulation of pRFs. The authors should provide more discussion about this issue, particularly with respect to the generalizability of their findings. Also, it seems problematic to make the distinction between parafoveal and peripheral pRFs, given that the locations of these pRFs only differ by a couple degrees of visual angle.</p><p>12) Figure 8 shows that behavioral accuracy (proportion correct trials) was approximately 80% for the different tasks and eccentricity bins. However, a Quest procedure was used to adaptively change stimulus parameters to generate target performance levels of 83%. Thus, the finding that percent correct did not vary across tasks and eccentricity thresholds is a trivial consequence of the adaptive psychophysical procedure, and statistical analysis providing evidence for the null hypothesis is relatively meaningless. While it would be difficult to directly compare task difficulty across the three tasks, the authors could compute psychophysical thresholds for each task and test whether these are influenced by eccentricity.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.36928.034</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Feature-based attention has long been thought to have modulatory effects on spatial attention (Saenz, Buračas and Boynton, 2002; Maunsell and Treue, 2006). The current study supports this idea by reporting specific analyses where color and temporal frequency are used as attended features and spatial resampling is assessed by measuring populations receptive fields. Despite the effort, the findings of the study are mostly confirmatory in nature. The manuscript would benefit from a reorganized presentation of the results to highlight the novel contributions of the study as opposed to confirmation of prior findings.</p></disp-quote><p>We would like to thank the reviewers for pointing out the necessity for better separating our confirmatory from our novel contributions. This comment has prompted us to rewrite the Introduction, Results and Discussion sections of the manuscript. The interaction between spatial and feature-based attention has indeed received generous scientific attention. Nevertheless, it remains elusive whether feature-based attention can influence spatial resampling that results from spatial attention. Although this relationship was predicted by behavioral work (Yeshurun and Carrasco, 1998, 2000; Yeshurun et al., 2008; Barbot and Carrasco, 2017), it has remained untested in electrophysiological recordings or functional neuroimaging. We are the first to directly assess this interaction in the present manuscript. In order to appreciate this novel contribution, we argue that it is of great importance to first present readers with a detailed description of the spatial resampling that resulted from spatial attention (which is mainly of confirmatory nature). Then, we can address how this is modulated by feature based attention. For this reason, we did not consider it opportune to change the order of presentation within the Results section. Instead, we implemented various changes to the text in order to better manage reader's expectations. For this, we first more concretely stress the novelty of our study in the Introduction:</p><p>“The studies mentioned above investigated modulatory effects of feature-based attention on spatial attention by measuring changes in response amplitude (e.g. ERP/firing rate). […] In the current study, we put this hypothesis to the test by measuring the brain's representation of space under conditions of differential attention.”</p><p>In addition, we added a paragraph in the Results section to reiterate this principal and novel aim, and to manage the reader's expectations about confirmatory vs. novel results:</p><p>“Our principal aim is to understand whether feature-based attention influences spatial resampling induced by spatial attention. […] Finally, we relate these feature-based attentional modulations to (1) bottom-up feature preference and (2) to differences in the spatial scale at which color and temporal frequency are processed.”</p><p>Finally, we also changed the order of the Discussion such that we now first discuss our novel findings, before we move on to relating our more confirmatory findings to the literature.</p><disp-quote content-type="editor-comment"><p>2) There isn't adequate information about how the target features were selected. Why should we be interested in attending to temporal frequency versus color in the context of this study? (It is also unclear/unexplained why blue-yellow and cyan-magenta were selected as representative colors.) Would anything change if different colors, a broader range of temporal frequencies, or entirely different feature sets (e.g., orientation) were used?</p></disp-quote><p>We will first clarify our reasoning for opting for color and temporal frequency. Next, we will argue why our results should not be specific for the specific features used in our experiment.</p><p>There are two important reasons for why we studied attending color versus temporal frequency. First, behavioral work suggested that the spatial scale at which features are processed should determine the degree of spatial resampling (Yeshurun and Carrasco, 1998, 2000; Yeshurun et al., 2008; Barbot and Carrasco, 2017). We therefore opted to use features that are known to be processed at different spatial scales. We previously only stressed a single factor that contributes such differential spatial sampling between these features: the eccentricity dependence of both pRF size and relative preference for color compared to temporal frequency. However, the cortical areas that preferentially process color and temporal frequency (i.e. MT+ compared to hV4; Liu and Wandell, 2005; Brouwer and Heeger, 2009, 2013; Winawer et al., 2010) differ in their average receptive field sizes (Amano et al., 2009; Winawer et al., 2010). Therefore, temporal frequency compared to color is processed on average by neurons with larger receptive fields. Finally, a third factor contributing to differential spatial sampling is that color compared to temporal frequency depends more on the parvocellular versus the magnocellular pathway, where spatial pooling is more fine-grained (Schiller and Malpeli, 1978; Hicks et al., 1983; Denison et al., 2014). In addition to this differential spatial sampling argument, the second main reason for opting for color and temporal frequency is that they are known to be processed in a distributed manner throughout the cortex (color: hV4 / temporal frequency: MT+). This provides an anatomically segregated handle for investigating the relationship between bottom-up feature preference and feature-based attentional modulations. As previously noted in our Materials and methods section, we specifically chose temporal frequency and not coherent motion, as coherent motion signals have been shown to influence pRF measurements (Harvey and Dumoulin, 2016). Also, it was previously shown that attention can be directed to these features (Wolfe and Horowitz, 2004; Cass et al., 2011). We now added a paragraph to our Introduction detailing this multi-faceted argument for why attending color and temporal frequency are of particular interest to our study:</p><p>“One important reason for studying the effects of attention to color and temporal frequency is that they are known to be processed at different spatial scales. […] Moreover, it was previously shown that attention can be directed to both feature domains (Wolfe and Horowitz, 2004; Cass et al., 2011).”</p><p>We also incorporated the two additional factors that contribute to differential spatial resampling between color and temporal frequency (i.e. parvocellular vs. magnocellular and offset in pRF size between hV4/MT+) in our Results section:</p><p>“What could then explain the fact that attending color in the stimulus induced greater changes in spatial sampling? […] In sum, the greater amount of spatial resampling when attending color can be parsimoniously explained by color being sampled by relatively smaller pRFs.”</p><p>And we discuss these two factors in our Discussion:</p><p>“Previous behavioral reports suggest that the spatial scale at which an attended feature is processed should influence the degree of spatial resampling (Yeshurun and Carrasco, 1998, 2000; Yeshurun et al., 2008; Barbot and Carrasco, 2017). […] Finally, both the current and previous studies show that pRF size (Dumoulin and Wandell, 2008) and color compared to temporal frequency preference (Curcio et al., 1990; Azzopardi et al., 1999; Brewer et al., 2005) vary across eccentricity such that foveal voxels have smaller pRFs and are more color sensitive.”</p><p>Previously, as mentioned above, we mainly emphasized the eccentricity gradient of both feature-preference and pRF size as underlying the finer spatial scale of color compared to temporal frequency processing. The initial version of our manuscript therefore contained a note about alternative eccentricity-dependent effects of attention. However, as we now stress two other factors that contribute to differences in spatial sampling between the two features, we currently argue that including this note leads to more distraction than that it is clarifying. We therefore removed these sentences from the Discussion:</p><p>“It must however be noted that as pRF size is closely related to pRF eccentricity, it is conceivable that some additional eccentricity dependent attentional modulation influenced our results. […] Therefore, although differential influences of attention across eccentricities have been observed before in the brain (e.g. Roberts et al., 2007; Bressler et al., 2013), these are likely brought about by differences in pRF size.”</p><p>Regarding the choice of specific colors, we would first like to note that after additional consideration we deem the name 'green' as more appropriate for the color previously indicated with 'cyan' (RGB color: (0,255,128)). We have therefore renamed the color pairs: blue/yellow and green/magenta. In our experiment, we opted for color pairs that were opposite in terms of color opponency (blue/yellow and green/magenta, Hering (1874)). This ensured that the Gabor peaks and troughs were made up of colors that provided strong color contrast. In addition, this meant that we presented stimuli to all color channels, in effect serving as a potent and full range color stimulus. We added these lines to the Materials and methods section:</p><p>“We opted for color pairs that were opposite in terms of color opponency (Hering (1874)). […] In addition, this meant that we presented stimuli to all color channels, in effect serving as a potent and full range color stimulus.”</p><p>Similarly, we used multiple temporal frequencies (3-7 Hz), in order to serve as a relatively general and broad temporal frequency stimulus. We argued above that it is the difference in spatial scale at which features are processed that influences the degree of spatial resampling. This implies that the particular choice of colors or temporal frequencies can only influence the degree of resampling if one has a particular reason for believing that different feature values are processed at different spatial scales. Attending these particular feature values in different attentional conditions should then lead to differential spatial resampling. We would like to point out however that in our experiment, attention was always devoted to all colors simultaneously within the <italic>Attend Color</italic> condition and to all temporal frequencies in the <italic>Attend TF</italic> condition. We therefore argue that our results reflect a general situation of attending color compared to temporal frequency. Finally, our results should indeed generalize to attending other feature dimensions (e.g. attending faces versus letters), as long as these features differ in the spatial scale of the visual system’s sensitivities to them. We added the abovementioned logic to the Discussion:</p><p>“In sum, we suggest that the greater degree of spatial resampling when attending color compared to temporal frequency can be explained by the difference in spatial scale at which these features are processed. […] This includes attending different feature values such as high compared to low spatial frequency, or attending different feature dimensions such as faces (broader spatial scale) versus letters (finer spatial scale).”</p><disp-quote content-type="editor-comment"><p>3) It is reported that pRF changes for Attend Color case are larger compared to Attend Temporal Frequency case. It would, however, be better if the difference could be presented in a more conspicuous way. For instance, the way it is presented in Figure 5A gives the impression that the difference is almost negligible. If there is merely a slight difference, more detailed discussion is warranted regarding the reasons underlying pRF changes when attending to color vs. temporal frequency.</p></disp-quote><p>Indeed, the absolute difference as presented in Figure 6A is relatively small. It should be noted however that this measure averages over the many changes in pRF parameters across attention to all possible bar positions. The magnitude of this measure should therefore not be interpreted as absolute shifts in degrees towards single attentional locations. A more interpretable measure is that of the feature based attentional modulation index (FAMI). This measure takes into account the average shift over all bar locations and computes feature based attentional modulations relative to this baseline. This FAMI measure is a contrast-like measure (difference divided by sum), and yields values around 0.05. This means that pRF changes were on average 10% larger when attending color compared to when attending temporal frequency. In fact, in some ROIs, the FAMI reaches values of 0.20, which roughly corresponds to 50% stronger pRF changes when attending color compared to temporal frequency. We would argue that an increase in pRF changes of 10-50% is not negligible and in fact is likely to have a substantial effect on visual information processing and perception. We added this logic to the Results section:</p><p>“Specifically, the feature AMI was around.05 on average across ROIs. As the feature AMI is a contrast measure where difference is divided by the sum, this roughly corresponds to 10% stronger pRF changes when attending color compared to temporal frequency. In some ROIs (V3AB/IPS0, see Figure 6—figure supplement 2), the FAMI reached values of.20, which roughly corresponds to 50% stronger pRF changes when attending color compared to temporal frequency.”</p><disp-quote content-type="editor-comment"><p>4) Related to the previous comment: When the regions primarily engaged by the two distinct features compared lie at different points in the hierarchy of visual cortex, the strength of retinotopic representations (and thus spatial) will inevitably vary. Wouldn't that introduce a bias in assessment of interactions between spatial and feature-based attention?</p></disp-quote><p>We agree that we did not sufficiently address the differences in visual hierarchy at which the two attended features are processed. In the current version of the manuscript, we now explicitly include this argument as one of the factors contributing to differences in the spatial scale at which the two attended features are processed (see our reply to Essential revision point 2). Indeed, it should be expected that offsets in pRF size should lead to differential pRF changes. Specifically, Gaussian interaction models of attention (Klein et al., 2014) suggest that the larger the stimulus drive (i.e. the pRF outside influence of attention), the greater the impact of attention. In correspondence, we observed that absolute pRF shifts were larger in areas with larger average pRFs (see Figures 3 and 4). Importantly however, our assessment of feature based attentional modulation is a contrast measure (AMI), relative to the absolute pRF modulation resulting from differential spatial attention. This analysis therefore controls for any offset in pRF size. Moreover, any difference in pRF size is also taken into account by the attentional gain field modeling (i.e. it is input to the model). Together, this means that differences in pRF size between visual areas that preferentially process the attended feature should not lead to biases in the assessment of interactions between spatial and feature based attention in our analyses. Nevertheless, (as we argue above) these offsets of pRF size between these regions does contribute to explaining why differential spatial resampling is required when attending color compared to temporal frequency.</p><disp-quote content-type="editor-comment"><p>5) There is too much variability across subjects in the attentional gain field sizes of temporal frequency vs. color (Figure 7). Given that there appears to be poor consistency across subjects for this measure, this figure does not convey information in a convincing way.</p></disp-quote><p>This comment has prompted us to change the presentation of our data in this figure for improved clarity (current Figure 8). Specifically, both previous findings (Klein et al., 2014) and our data (Figure 5) show that a single attentional gain field operates throughout visual cortex. In addition, we did not find that feature-based attentional modulation was related to feature preference across visual areas (Figure 6E). We therefore summarized feature-based attentional modulations of the attentional gain field across visual areas as the median across individually fitted ROIs (previously dark shaded area) and as the combined ROI (i.e. fitted on data from all ROIs together). We clarified this in our manuscript:</p><p><italic>“</italic>Both our data (Figure 5) and previous findings (Klein et al., 2014) showed that a single attentional gain field affects the different visual regions similarly. […] We therefore analyzed feature-based attentional modulations of the attentional gain field both on data from all ROIs fitted together (the 'combined ROI'), and as the median across individually fitted ROIs.”</p><disp-quote content-type="editor-comment"><p>6) The authors combine pRF eccentricity and size to obtain a single index, because they claim there's a high correlation between the two. We don't think this is warranted. For instance, Table 22 shows that there's a significant correlation between eccentricity and size changes in V1 for only 2 out of 5 subjects, and the average correlation is not very high for this ROI either (R=0.461). This also makes it questionable to present the results using voxels from a &quot;combined ROI&quot;, given the large degree of variability across individual ROIs.</p></disp-quote><p>In order to verify whether our feature-based attentional modulation (feature AMI) results were not specific to our combined measure of eccentricity and size changes, we repeated this analysis using pRF size and eccentricity separately and report them in the manuscript (see the novel Figure 6—figure supplement 3). This analysis returned highly similar results compared to our initial combined pRF size and eccentricity feature-based AMI. In our opinion, it is warranted to combine the pRF size and eccentricity differences into a single measure of attentional modulation. We added a reference to this figure in the Results section:</p><p>“Computing the AMI with either pRF eccentricity or size changes separately (i.e. not as a combined measure) returned similar results (see Figure 6—figure supplement 3).”</p><p>We agree that it is striking that the correlation between eccentricity and size is somewhat lower in V1 compared to the other ROIs. We suspect that this might be due to the fact that pRFs are very small in V1. This in turn means that absolute attention-induced changes in pRF eccentricity and size are also very small and therefore harder to measure with high precision. In addition, we showed above that feature-based attentional pRF modulations are present for both eccentricity and size changes separately. So, although these correlations are smaller in V1, we conclude it is likely that this is not the result of fundamental differences in the effects of attention. We continue to feel that it is meaningful to combine all voxels into a single 'combined' ROI, as it serves to show the predominant effects present across all voxels in the visual brain. We want to stress that the combined ROI is not meant to present results averaged across ROI. Indeed, the different ROIs contribute meaningfully differently to this combined ROI (also see Essential revision point 10). The individual ROI results then serve to specify this overall effect per ROI. In our view, the two analyses serve complementary purposes. For these reasons, we believe that including the combined ROI helps the reader to better appreciate the predominant patterns in the data.</p><disp-quote content-type="editor-comment"><p>7) In most of the supplementary figures (e.g., the first one), results for individual subjects are not shown at the same scale across subjects, which makes them confusing. Also, there's barely any change in the pRF size with increasing eccentricity for some ROIs of some subjects (e.g., V1 of s5). Some extended treatment and discussion of the consistency of the presented results across subjects would greatly benefit the manuscript.</p></disp-quote><p>In order to provide additional assessment of the stability of the results across subjects, we matched the scale of all the individual subject figures. In addition, we performed additional analyses (see novel Figure 2—figure supplement 1, and Tables 1 and 11) on the slope between eccentricity and size in all ROIs and in all individual subjects. Although this relationship was numerically positive in all ROIs in all subjects, it did not cross the significance threshold of α=.05 in some ROIs (VO/V3AB/IPS0/MT+; p’s of. 146,. 073,. 078 and. 058 respectively) in no more than 1 out of 5 subjects. We therefore removed the incorrect 'in all participants' statement from the main text and apologize for its initial inclusion. In order to provide more extended discussion of the results across subjects, we added a novel paragraph to the Results section:</p><p>“In order to evaluate the stability of our results we repeated all analyses for individual subjects. […] In addition, these analyses showed that spatial resampling was consistently modulated by feature-based attention across subjects (i.e. feature AMI was on average 0.059 greater than 0 (F<sub>(1,4)</sub> = 18.868, p =.012, η2p=.394), and was not different between ROIs (F<sub>(8,32)</sub> = 0.631, p =.746, η2p=.066)).”</p><disp-quote content-type="editor-comment"><p>8) We found it difficult that when statistical analyses are performed across voxels bootstrap tests were applied, and yet across subject tests were based on ANOVA or t-tests. If the same type of metric is evaluated in both cases, shouldn't you make the same assumptions about the distribution of the metric?</p></disp-quote><p>Our rationale for using bootstrapping across voxels is that this allowed for the computation of confidence intervals for metrics that can only be derived across a set of voxels (e.g. correlation values). This also allowed us to visualize the distribution of the metric in a data-driven fashion. Letting go of the assumption of normality in bootstrapping thereby increased the specificity of the derived results. We would preferably also have used bootstrapping when calculating statistics over subjects, but bootstrapping is not stable for N&lt;20. We made the additional assumption of normality for across subjects in order to increase stability and robustness of the derived p-values. We thus utilized both bootstrapping and more classical statistics to optimize the balance between sensitivity and robustness. We added these sentences to the Materials and methods section in order to clarify this issue:</p><p>“Our rationale for using bootstrapping across voxels and more classical tests across subjects is that while bootstrapping closely represents the actual distribution of the data, it is not stable for N &lt; 20. We therefore made the additional assumption of normality when computing statistics across subjects.”</p><p>In order to complete the consistency of using bootstrapping across participants and classical statistics across voxels, we now changed the visualization of confidence intervals in Figure 9A from using bootstrapping to classical statistics. Note that this is merely a matter of visualization, as the Bayesian analyses reported are not affected. Updated CIs are slightly wider, as classical statistics is more conservative than bootstrapping for small Ns.</p><disp-quote content-type="editor-comment"><p>9) More detailed description of the outlier rejection method is needed, as this changes the set of voxels over which the results are reported. This procedure can potentially introduce biases within and across analyses, so it must be motivated very clearly. It is uncommon to reject voxels like this. If the goal is to report robust estimates of central tendency and variance of parameter estimates, then why not calculate them based on median and IQR?</p></disp-quote><p>In computing the AMI, division by values close to zero sometimes resulted in extreme values. In order to remove these values from further analysis, we opted to use a very conservative outlier rejection method, using 5 median absolute deviations as a threshold. We specifically opted to apply the same outlier rejection method to all other analyses for consistency. In order to verify whether outlier rejection had any significant effect on the other analyses, we also repeated all analyses without outlier rejection and found no meaningful changes. Using the median and IQR, as the reviewers suggest, would have made for a suitable alternative. This should have yielded very similar results, as our method similarly rejected outliers based on deviations from the median. Indeed, our approach changed the set of voxels over which the results are reported. Yet, the IQR method would similarly base its results on different subpopulations of voxels per analysis (namely those voxels not too far from the median), without explicitly stating so in changes in N. We therefore argue that our approach is both conservative, consistent (across analyses) and transparent (stating the N). We've added a more detailed description of this outlier rejection method in the Materials and methods section:</p><p>“The computation of the attentional modulation index (see below) included a division operation. […] Importantly, in all of these remaining analyses, outlier rejection did not meaningfully alter any result.”</p><p>In order to further increase the consistency of our outlier rejection method, we simplified the determination of outliers in the feature-AMI analysis. Previously, outliers were determined based on multiple measures, including the color AMI, the TF AMI, the combined AMI and on feature-preference. Currently, we only determine outliers based on the feature-AMI in this analysis. This increased the number of included voxels and slightly changed the exact statistical values in the feature-AMI analyses (Tables 9 and 25 and legend of Figure 6—figure supplement 4). In maximally 1/5 subjects, this resulted in changes of statistical significance (in V3, LO, and MT+). Specifically, in V3 this resulted in one additional subject showing significantly positive AMI; in MT+ it resulted in 1 fewer subject showing significantly positive AMI; in LO, it resulted in 1 fewer subject showing significantly negative AMI. As a result, none of the subjects show a significantly negative AMI in any ROI. In addition, it did not change any of the conclusions about statistical significance for the ‘super subject’ method nor when computing t- and F-tests over subject values (‘over subjects’ method).</p><disp-quote content-type="editor-comment"><p>10) It is stated that explained variance during pRF estimation is used to weigh the contributions of individual voxels to estimated parameters. This is understandable, but we would have expected to see actual reports of the explained variance across ROIs as well. This could help readers to judge and interpret the differences observed across ROIs.</p></disp-quote><p>We agree. In order to present additional information regarding the distribution of explained variance across the different ROIs, we added two panels to Figure 2 (and added Figure 2—figure supplement 2). The first panel (C) depicts visualizations of the distributions of explained variance across voxels for each ROI. The second panel (D) displays visualizations of the number of voxels per ROI. Combining the information from these two figures provides the reader with a comprehensive overview of the total amount of signal that was present in each ROI. In addition, we added a supplementary figure that details this information per subject, and also shows how many voxels remain in each ROI after the application of the different rejection criteria.</p><disp-quote content-type="editor-comment"><p>11) The stimulus aperture size comprises a very small proportion of the visual field and is also much smaller than those used in other fMRI studies of attentional modulation of pRFs. The authors should provide more discussion about this issue, particularly with respect to the generalizability of their findings. Also, it seems problematic to make the distinction between parafoveal and peripheral pRFs, given that the locations of these pRFs only differ by a couple degrees of visual angle.</p></disp-quote><p>We understand that our stimulus could be considered small compared to the number of degrees of visual angle that spans our visual world (about 5% assuming full visual field of 144 degrees). Even though our stimulus only spanned roughly 5% of the visual field, this corresponds to approximately 25% of V1 surface due to cortical magnification (Adams and Horton, 2003). As this bias for over representing the central part of the visual field increases up the visual hierarchy (Harvey and Dumoulin, 2011), this percentage represents a minimum bound of cortical surface area stimulated by our stimulus. Comparing our stimulus size to other studies of attention and pRFs, we find a maximum stimulus eccentricity of 4.5 degrees in Vo et al., 2017, (corresponding to roughly 30% of V1 surface as approximated based on the results by Adams and Horton, 2003), of 5 degrees eccentricity in Klein et al., 2014, (~33% V1 surface), of 7.5 degrees in Kay et al., 2015, (~45% of V1 surface), and of 14 degrees in Sheremata and Silver, 2015, (~60% of V1 surface). Although our stimulus size is thus relatively small, it is generally comparable to two of these studies in terms of proportion of stimulated V1 (25% in our study compared to 30% and 33% in Vo et al., 2017, and Klein et al., 2014, respectively). Yet, one might argue that 25% is still a low number, and that the stimulus preferably should have been larger. However, increasing stimulus size also increases bar stimulus step size (keeping stimulation time equal). This means that the precision at which pRF properties like position and size can be determined decreases with increasing stimulus size. As we sought to measure changes in spatial sampling with the highest possible fidelity, we opted to keep the stimulus size relatively small in order to maximize our sensitivity for measuring changes in pRF parameters. Finally, we understand that the term 'parafoveal' and 'peripheral' are not correct in absolute terms. We meant to use those terms as relative, rather than absolute measures of eccentricity. We now specifically added a section to clarify this in our Materials and methods section:</p><p>“Although our stimulus spanned roughly 5% of the visual field (assuming a visual field of 144 dva), this corresponds to roughly 25% of V1 surface area due to cortical magnification (Adams and Horton, 2003). […] Throughout the Results section, we use the terms 'parafoveal' and 'peripheral' to indicate positions relative to the stimulated portion of visual field.”</p><disp-quote content-type="editor-comment"><p>12) Figure 8 shows that behavioral accuracy (proportion correct trials) was approximately 80% for the different tasks and eccentricity bins. However, a Quest procedure was used to adaptively change stimulus parameters to generate target performance levels of 83%. Thus, the finding that percent correct did not vary across tasks and eccentricity thresholds is a trivial consequence of the adaptive psychophysical procedure, and statistical analysis providing evidence for the null hypothesis is relatively meaningless. While it would be difficult to directly compare task difficulty across the three tasks, the authors could compute psychophysical thresholds for each task and test whether these are influenced by eccentricity.</p></disp-quote><p>We agree with the reviewers that not finding any difference in task performance is to be expected using a Quest staircase procedure. Nevertheless, using this procedure does not necessarily guarantee success. It is for instance possible that the procedure settles at a wrong difficulty level, making this verification necessary. We would argue that Quest and similar procedures are too often used as a panacea without this necessary verification. The analysis presented in the manuscript therefore serve to verify the validity of the Quest procedure in achieving the desired level of accuracy. We changed the introductory text to this part of the Results section to more clearly state this aim:</p><p>“Although we used a Quest procedure to equate difficulty across attention conditions and across different levels of eccentricity, it is possible that this procedure stabilized at a faulty difficulty level. In order to verify whether the Quest procedure successfully equated performance we used a similar Bayesian approach, testing whether a model including attention condition (3 levels) and stimulus eccentricity (3 levels) influenced behavioral performance (Figure 9A).”</p><p>We would like to note that stimuli were the same across attention conditions, and that our procedure successfully equated task difficulty. These were the chief aims of our behavioral paradigm. Yet, this reviewer comment has prompted us to also analyze the ratio of Gabor elements of either feature value used by the Quest procedure in order to equate difficulty. We placed these analyses in a novel figure supplement (see Figure 9—figure supplement 1). The legend of this supplement reads as the following:</p><p><bold>“</bold>Figure 9—figure supplement 1.Ratio of Gabor elements of either feature value used by the Quest procedure in order to equate difficulty. […] This could suggest that the greater degree of spatial resampling we observed when attending color discounted the lower relative sensitivity for color in the periphery.”</p></body></sub-article></article>