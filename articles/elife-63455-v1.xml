<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63455</article-id><article-id pub-id-type="doi">10.7554/eLife.63455</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Bi-channel Image Registration and Deep-learning Segmentation (BIRDS) for efficient, versatile 3D mapping of mouse brain</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-210455"><name><surname>Wang</surname><given-names>Xuechun</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-209955"><name><surname>Zeng</surname><given-names>Weilin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-209899"><name><surname>Yang</surname><given-names>Xiaodan</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-209900"><name><surname>Fang</surname><given-names>Chunyu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-209901"><name><surname>Han</surname><given-names>Yunyun</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-111080"><name><surname>Fei</surname><given-names>Peng</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3764-817X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">School of Optical and Electronic Information</institution>, <institution>Huazhong University of Science and Technology</institution>, <addr-line><named-content content-type="city">Wuhan</named-content></addr-line>, <country>China</country></aff><aff id="aff2"><institution content-type="dept">School of Basic Medicine</institution>, <institution>Tongji Medical College, Huazhong University of Science and Technology</institution>, <addr-line><named-content content-type="city">Wuhan</named-content></addr-line>, <country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-72659"><name><surname>Smith</surname><given-names>Jeffrey C</given-names></name><role>Reviewing editor</role><aff><institution>National Institute of Neurological Disorders and Stroke</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>yhan@hust.edu.cn</email> (YH);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>feipeng@hust.edu.cn</email> (PF);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>18</day><month>01</month><year>2021</year></pub-date><volume>10</volume><elocation-id>e63455</elocation-id><history><date date-type="received"><day>25</day><month>09</month><year>2020</year></date><date date-type="accepted"><day>27</day><month>12</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Wang et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Wang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63455-v1.pdf"/><abstract><p>We have developed an open-source software called BIRDS (bi-channel image registration and deep-learning segmentation) for the mapping and analysis of 3D microscopy data and applied this to the mouse brain. The BIRDS pipeline includes image pre-processing, bi-channel registration, automatic annotation, creation of a 3D digital frame, high-resolution visualization, and expandable quantitative analysis. This new bi-channel registration algorithm is adaptive to various types of whole-brain data from different microscopy platforms and shows dramatically improved registration accuracy. Additionally, as this platform combines registration with neural networks, its improved function relative to other platforms lies in the fact that the registration procedure can readily provide training data for network construction, while the trained neural network can efficiently segment incomplete/defective brain data that is otherwise difficult to register. Our software is thus optimized to enable either minute-timescale registration-based segmentation of cross-modality, whole-brain datasets or real-time inference-based image segmentation of various brain regions of interest. Jobs can be easily submitted and implemented via a Fiji plugin that can be adapted to most computing environments.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>National Key R&amp;D program of China</institution></institution-wrap></funding-source><award-id>2017YFA0700501</award-id><principal-award-recipient><name><surname>Fei</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>21874052</award-id><principal-award-recipient><name><surname>Fei</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31871089</award-id><principal-award-recipient><name><surname>Han</surname><given-names>Yunyun</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Innovation Fund of WNLO</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Fei</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution>Junior Thousand Talents Program of China</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Fei</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution>Junior Thousand Talents Program of China</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Han</surname><given-names>Yunyun</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution>The FRFCU</institution></institution-wrap></funding-source><award-id>HUST:2172019kfyXKJC077</award-id><principal-award-recipient><name><surname>Han</surname><given-names>Yunyun</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The Allen CCF is open access and available with related tools at https://atlas.brain-map.org/The datasets (Brain1~5)  have been deposited in Dryad at https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALgThe code and plugin can be found at the following link：https://github.com/bleach1by1/BIRDS_pluginhttps://github.com/bleach1by1/birds_reghttps://github.com/bleach1by1/birds_dl.githttps://github.com/bleach1by1/BIRDS_demoAll data generated or analysed during this study are included in the manuscript. Source data files have been provided for Figures 1, 2, 3, 4, 5 and Figure 2-figure supplement 3,4; Figure 5-figure supplement 2,3</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Yunyun Han</collab><collab>Justus M Kebschull</collab><collab>Robert A A Campbell</collab><collab>Devon Cowan</collab><collab>Fabia Imhof</collab><collab>Anthony M Zador &amp; Thomas D Mrsic-Flogel</collab></person-group><year iso-8601-date="2018">2018</year><source>Brain 1 and 2</source><ext-link ext-link-type="uri" xlink:href="https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALg">https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALg</ext-link><comment>Dryad Digital Repository</comment></element-citation><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Jun Nie  Sa Liu  Tingting Yu  Yusha Li  Junyu Ping  Peng Wan  Fang Zhao  Yujie Huang  Wei Mei  Shaoqun Zeng  Dan Zhu  Peng Fei</collab></person-group><year iso-8601-date="2019">2019</year><source>Brain 5</source><ext-link ext-link-type="uri" xlink:href="https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALg">https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALg</ext-link><comment>Dryad Digital Repository</comment></element-citation><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Chunyu Fang</collab><collab>Tingting Yu</collab><collab>Tingting Chu</collab><collab>Wenyang Feng</collab><collab>Fang Zhao</collab><collab>Xuechun Wang</collab><collab>Yujie Huang</collab><collab>Yusha Li</collab><collab>Peng Wan</collab><collab>Wei Mei</collab><collab>and Dan Zhu</collab><collab>Peng Fei</collab></person-group><year iso-8601-date="2020">2020</year><source>Brain 3 and 4</source><ext-link ext-link-type="uri" xlink:href="https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALg">https://datadryad.org/stash/share/4fesXcJif0L2DnSj7YmjREe37yPm1bEnUiK49ELtALg</ext-link><comment>Dryad Digital Repository</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-63455-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>