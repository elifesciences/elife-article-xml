<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103788</article-id><article-id pub-id-type="doi">10.7554/eLife.103788</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103788.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Differential destinations, dynamics, and functions of high- and low-order features in the feedback signal during object processing</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hou</surname><given-names>Wenhao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-8171-2184</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>He</surname><given-names>Sheng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5547-923X</contrib-id><email>hes@ibp.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Jiedong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4432-2752</contrib-id><email>zhangjiedong@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tyv8576</institution-id><institution>State Key Laboratory of Cognitive Science and Mental Health, Institute of Biophysics, Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution>Institute of AI, Hefei Comprehensive National Science Center</institution><addr-line><named-content content-type="city">Hefei</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kok</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>01</month><year>2026</year></pub-date><volume>13</volume><elocation-id>RP103788</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-11-01"><day>01</day><month>11</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-11-01"><day>01</day><month>11</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.11.01.621525"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-23"><day>23</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103788.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-10-06"><day>06</day><month>10</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103788.2"/></event></pub-history><permissions><copyright-statement>© 2024, Hou et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Hou et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103788-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-103788-figures-v1.pdf"/><abstract><p>Brain is a hierarchical information processing system, in which the feedback signals from high-level to low-level regions are critical. The feedback signals may convey complex high-order features (e.g. category, identity) and simple low-order features (e.g. orientation, spatial frequency) to sensory cortex to interact with the feedforward information, but how these types of feedback information are represented and how they differ in facilitating visual processing remains unclear. The current study used the peripheral object discrimination task, 7T fMRI, and MEG to isolate feedback from feedforward signals in human early visual cortex. The results showed that feedback signals conveyed both low-order features natively encoded in early visual cortex and high-order features generated in high-level regions, but with different spatial and temporal properties. The high-order feedback information targeted both superficial and deep layers, whereas the low-order feedback information reached only deep layers in V1. In addition, MEG results revealed that the feedback information from occipitotemporal to early visual cortex emerged around 200 ms after stimulus onset, and only the representational strength of high-order feedback information was significantly correlated with behavioral performance. These results indicate that the complex and simple components of feedback information play different roles in predictive processing mechanisms to facilitate sensory processing.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object processing</kwd><kwd>feedback signal</kwd><kwd>7T fMRI</kwd><kwd>MEG</kwd><kwd>laminar profiles</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Brain Science and Brain-like Intelligence Technology - National Science and Technology Major Project</institution></institution-wrap></funding-source><award-id>2021ZD0204200</award-id><principal-award-recipient><name><surname>He</surname><given-names>Sheng</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Brain Science and Brain-like Intelligence Technology - National Science and Technology Major Project</institution></institution-wrap></funding-source><award-id>2021ZD0203800</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Jiedong</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>Key Research Program of Frontier Sciences, Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>KJZD-SW-L08</award-id><principal-award-recipient><name><surname>He</surname><given-names>Sheng</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Young Scientists in Basic Research</institution></institution-wrap></funding-source><award-id>YSBR-071</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Jiedong</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Feedback signals to the early visual cortex convey both high-order and low-order visual information, but with different laminar profiles, and the high-order information is important for object recognition.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>As sensory information is processed in the primate brain, the neural system actively interprets the sensory input to generate a consistent and meaningful perception. This process relies on the interplay between feedforward and feedback processes in the neural processing hierarchy. Anatomically, feedback connections are widely distributed in the visual cortex (<xref ref-type="bibr" rid="bib14">Felleman and Van Essen, 1991</xref>). Functionally, behavioral and neural evidence has shown that neural feedback to early visual cortex is critical for many visual functions, such as object recognition and visual awareness (<xref ref-type="bibr" rid="bib28">Kar and DiCarlo, 2021</xref>; <xref ref-type="bibr" rid="bib36">Kreiman and Serre, 2020</xref>; <xref ref-type="bibr" rid="bib37">Lamme et al., 1998</xref>). However, limited knowledge of the nature of the feedback information hinders our understanding of how neural feedback interacts with visual input.</p><p>Significant transformations of neural representations occur across the visual processing hierarchy. Compared to early visual cortex, where neurons encode low-order features like orientation and spatial frequency (<xref ref-type="bibr" rid="bib25">Hubel and Wiesel, 1977</xref>), neurons in higher-level cortex have larger receptive fields and more complex tuning of high-order features (<xref ref-type="bibr" rid="bib2">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Desimone et al., 1984</xref>), such as category and identity. What kind of information is conveyed by feedback signals? Based on predictive coding theory (<xref ref-type="bibr" rid="bib17">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib45">Rao and Ballard, 1999</xref>), the feedback signal is compared to the input signal to generate prediction errors for further processing. However, the high-order representation is more abstract and invariant, and a feedback signal with such information may not be feasible for a direct ‘comparison’. For an effective comparison, it is reasonable to assume that the feedback signal should be in the form of low-order predicted information. However, previous evidence suggests that object category information could be decoded in the feedback signals (<xref ref-type="bibr" rid="bib13">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Williams et al., 2008</xref>). It remains unclear whether this category decoding originated from high-order object category information or was driven by low-order (i.e. object shape and orientation) information (<xref ref-type="bibr" rid="bib43">Morgan et al., 2019</xref>) that could be directly compared with early visual cortex representation. Therefore, it is crucial to dissociate these two types of features and investigate what kind of information is conveyed in feedback signals during object processing.</p><p>A notable feature of the primate brain is the presence of layered structures, with each layer serving a distinct function (<xref ref-type="bibr" rid="bib14">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib22">Harris and Mrsic-Flogel, 2013</xref>; <xref ref-type="bibr" rid="bib46">Rockland and Pandya, 1979</xref>; <xref ref-type="bibr" rid="bib59">Wong-Riley, 1978</xref>). In the early visual cortex, feedforward signals are directed toward the middle layers, while both deep and superficial layers receive feedback signals. The output signals to downstream regions are generated in the superficial layer. The feedback information may originate from object processing areas in the temporal cortex, but in the early visual cortex, the laminar profiles of the high- and low-order information in feedback signals remain unknown. It is important to clarify where the different types of input and feedback information arrive in early visual cortex.</p><p>Plenty of evidence indicates that feedback signals can enhance visual processing efficiency and further optimize behavioral performance (<xref ref-type="bibr" rid="bib19">Gilbert and Li, 2013</xref>; <xref ref-type="bibr" rid="bib60">Wyatte et al., 2014</xref>). However, the behavioral implications of high- and low-order feedback information remain unclear. It is important to examine whether and when each type of feedback information correlates with behavioral performance during visual processing.</p><p>Examining neural information in the feedback signal can be challenging due to the highly tangled distribution of the feedforward and feedback signals during visual processing in the cortex. To isolate the feedback signals, we took advantage of a peripheral object recognition paradigm. Previous studies have shown that when objects are presented in the visual periphery for identification, their information is fed back to foveal V1 (<xref ref-type="bibr" rid="bib58">Williams et al., 2008</xref>). The spatial separation between the input signal and feedback signal in the retinotopic visual cortex enabled the extraction of various types of feature information in the feedback signal without interference from feedforward visual input. Additionally, the high spatial resolution of 7T MRI and high temporal resolution of MEG recording allowed for the evaluation of laminar profiles and temporal dynamics of feedback signals, respectively, as well as information flow among different cortical regions. The results indicate that, in addition to the low-order features, feedback signals also transmit high-order category information to V1. Importantly, the high-order information cannot be observed in feedforward processing in V1. Interestingly, high- and low-order feedback information exhibit different laminar profiles. The quality of high-order information in feedback signal also shows a significant correlation with behavioral performance. The high-order information originates from object processing regions in the occipitotemporal cortex approximately 200 ms after visual input. Apparently, high-order feedback information from downstream regions and its interaction with locally encoded low-order information may be critical for efficient visual processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Orientation and category information in the feedback signal to V1 during peripheral object processing</title><p>In Experiment 1, peripheral object task was used during the 7T fMRI scan. The stimuli were novel objects from two categories (smoothie and cubie) and two orientations (vertical and horizontal) (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). In each trial, two objects from the same category with the same orientation were briefly presented (100 ms) in the peripheral visual field (7° from fixation) (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Participants were asked to judge whether the two objects were identical while maintaining fixation. To optimize fMRI signal in the experimental runs, a block design was used, with each block consisting of eight trials testing objects from the same category and orientation (see <italic>Materials and methods</italic> for more details).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Feedback information to foveal V1 in the peripheral object task (n=18).</title><p>(<bold>A</bold>) Four types of objects (two orientations×two object categories) were used in the experiment. (<bold>B</bold>) Two objects from the same type were presented in the periphery, and participants had to discriminate whether they were identical. (<bold>C</bold>) Strong fMRI responses were found in peripheral V1 corresponding to the stimulus locations and high-level regions, but not in foveal V1. (<bold>D</bold>) Schematic representation of feedforward and feedback connections in different cortical layers of V1. (<bold>E</bold>) High-order category information from feedback signals could be decoded in foveal, but not peripheral, V1. (<bold>F</bold>) The category information could be decoded in superficial and deep layers of foveal V1, but absent in all layers of peripheral V1. (<bold>G–H</bold>) Low-order orientation information from feedback signals could be decoded only in the deep layer of foveal V1 and across layers in peripheral V1. Error bars reflect ±1 SEM. * indicates paired t-test with significance of p&lt;0.05. ** indicates paired t-test with significance of p&lt;0.01. † indicates marginal significance. The p-values shown in this figure are uncorrected for multiple comparisons. The dashed line represents the chance level of decoding performance. See also <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Activation maps of the foveal stimulus and the sizes of the foveal V1 region of interest (ROI) in two example participants.</title><p>(<bold>A</bold>) Voxels showing stronger fMRI responses to the foveal checkerboard than to the peripheral checkerboard. (<bold>B</bold>) Voxels showing stronger fMRI responses to foveal objects than to the baseline. Blue solid lines indicate the spatial range of the foveal V1 ROI for each participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Example registration of functional images to anatomical space in four participants.</title><p>The green and blue lines represent the white matter and pial surfaces, respectively, both derived from the anatomical image. Transparent colored overlays mark the regions of interest (ROIs).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Fine-scale laminar profiles of orientation and category information in foveal V1 in peripheral object task.</title><p>Significant category information was found in the deep and superficial layers (ts&gt;1.94, ps&lt;0.035), and significant orientation information was found in the deep layers (ts&gt;2.26, ps&lt;0.019), but no significant difference was observed between two types of information (ts&lt;0.61, ps&gt;0.55). Error bars reflect ±1 SEM. * indicates paired t-test with significance of p&lt;0.05. The dashed line represents the chance level of decoding performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig1-figsupp3-v1.tif"/></fig></fig-group><p>In addition to the 12 experimental runs, three localizer runs were included in the scan to identify regions of interest (ROIs) in each hemisphere. In one of the localizer runs, flashing checkerboards were presented either at the periphery where objects were presented during the task or at the fovea. ROIs of peripheral V1 and foveal V1 were identified using contrast maps between these two conditions (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). In the other two localizer runs, everyday objects and scrambled objects were presented in different blocks while participants performed a one-back task. These conditions were used to identify two high-level object processing regions: the lateral object complex (LOC) (<xref ref-type="bibr" rid="bib21">Grill-Spector et al., 2001</xref>) in the ventral pathway and the posterior intraparietal sulcus (pIPS) (<xref ref-type="bibr" rid="bib29">Kastner et al., 2017</xref>) in the dorsal pathway.</p><p>The neural responses based on BOLD signal in the four ROIs were estimated. Univariate analysis shows significant fMRI responses in three ROIs (peripheral V1, LOC, pIPS) during the task (ts&gt;3.30, ps&lt;0.01), while no significant fMRI response was observed in foveal V1 (t(17)=–0.09, p=0.93) (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) due to the absence of visual input in the fovea.</p><p>Multivariate analysis (linear support vector machine [SVM]) was used to evaluate the information encoded in each ROI by classifying the neural response patterns elicited by objects from different conditions. The neural patterns were classified based on low-order orientation information (horizontal or vertical) or high-order category information (cubie or smoothie). The classification accuracy was used to indicate the existence of information in each ROI.</p><p>Although the average fMRI responses in foveal V1 did not differ from the baseline, the neural response patterns there supported significantly above-chance decoding accuracies for high-order categories (t(17)=3.14, p=0.003) (see <xref ref-type="fig" rid="fig1">Figure 1E</xref>). The object category information was also examined in other cortical regions. The decoding performances were significantly above chance in the LOC and pIPS (ts&gt;1.94, ps&lt;0.04), but not in the peripheral V1 (t(17)=–1.19, p=0.87) (see <xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p>Further, the 7T fMRI’s submillimeter resolution enabled us to examine the laminar profiles of cortical regions. Previous neurophysiology studies have shown that in V1, feedforward and feedback signals differentially target different layers. Here, based on anatomical images of each participant, the gray matter was segmented into deep, middle, and superficial layers (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The neural responses in these layers from each ROI were extracted and analyzed separately.</p><p>In foveal V1, the high-order category information could be decoded in the superficial and deep layers (ts&gt;2.37, uncorrected ps&lt;0.02, false discovery rate [FDR]-corrected qs&lt;0.05), but not in the middle layer (t(17)=–0.62, uncorrected p=0.73). In peripheral V1, none of the layers showed significant decoding performance (ts&lt;0.10, uncorrected ps&gt;0.46) (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). While the chance-level decoding performance in peripheral V1 rejects the possibility that the high-order information is laterally communicated from peripheral V1, the laminar profile of information in foveal V1 more clearly demonstrates that high-order category information was transmitted to the deep and superficial layers of foveal V1 via feedback connections. In the two high-level regions, category information was successfully decoded in the middle and superficial layers of pIPS (ts&gt;2.95, uncorrected ps&lt;0.005, FDR-corrected qs&lt;0.03) and the middle layer of LOC (t(16)=2.69, uncorrected p=0.008, FDR-corrected q=0.03), with a trend in the superficial layer of the LOC (t(16)=1.92, uncorrected p=0.04, FDR-corrected q=0.09), suggesting that the category information was present in the middle layer in these regions and further processed and outputted to other cortical regions through the superficial layer.</p><p>For the low-order orientation information, the decoding performance was significant in peripheral V1 and LOC (ts&gt;2.32, ps&lt;0.02), but not in pIPS (t(17)=0.00, p=0.50) (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). In foveal V1, the orientation decoding performances were marginally significant (t(17)=1.50, p=0.08). Further layer analysis showed that in foveal V1, significant orientation decoding was observed in the deep layer (t(17)=2.43, uncorrected p=0.01, FDR-corrected q=0.04), but not in the middle and superficial layers (ts&lt;0.98, uncorrected ps&gt;0.17) (<xref ref-type="fig" rid="fig1">Figure 1H</xref>). The fine-scale laminar profiles of different types of feedback information in foveal V1 supported this observation (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). In peripheral V1 where stimuli were presented, not surprisingly, significant orientation decoding performances were observed in the middle and superficial layers (ts&gt;3.06, uncorrected ps&lt;0.004, FDR-corrected qs&lt;0.03). In the two high-level visual regions, orientation information was only marginally significant in the deep layer of LOC (t(16)=2.26, uncorrected p=0.02, FDR-corrected q=0.05). The results indicate the presence of orientation information in the feedback signal in foveal V1; however, the strength of the low-order orientation information appears to be weaker and less extensive across cortical layers compared to the high-order category information.</p></sec><sec id="s2-2"><title>High-order category information in V1 not driven by low-level features</title><p>Is the successful decoding of category information observed in foveal V1 truly driven by high-order information in the feedback signals, or could it be driven by some low-level confounding features embedded in the stimuli that are natively encoded in V1? To address this concern, Experiment 2 was conducted to examine the visual representation in foveal V1 with stimuli directly presented at the fovea, thus providing strong representation of low-level features in the feedforward processing. In addition, during the scan, participants were asked to perform a one-back task regarding the fixation color. This manipulation makes the object stimuli task-irrelevant, minimizing the feedback signal during object processing (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Object representation in foveal V1 during a fixation task (n=18).</title><p>(<bold>A</bold>) The object stimulus was presented at the fovea as a task-irrelevant stimulus and participants performed a one-back task to the fixation color. (<bold>B–C</bold>) Category information could not be decoded in foveal V1. (<bold>D–E</bold>) Orientation information could be decoded in all layers of foveal V1. Error bars reflect ±1 SEM. * indicates paired t-test with significance of p&lt;0.05. ** indicates paired t-test with significance of p&lt;0.01. The p-values shown in this figure are uncorrected. The dashed line represents the chance level of decoding performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig2-v1.tif"/></fig><p>Decoding analysis revealed significant orientation information in foveal V1 (t(17)=6.94, p&lt;0.001), but no category information could be detected there (t(17)=0.38, p=0.35). Not surprisingly, there is significant category information in the LOC (t(16)=2.48, p=0.01) (<xref ref-type="fig" rid="fig2">Figure 2B and D</xref>). Further layer analysis revealed that category information was marginally significant in the middle layer (t(16)=1.92, uncorrected p=0.04, FDR-corrected q=0.17) and superficial layer of LOC (t(16)=2.40, uncorrected p=0.01, FDR-corrected q=0.09), while orientation information was significant in all three layers of foveal V1 (ts&gt;5.35, uncorrected ps&lt;0.001, FDR-corrected qs&lt;0.001) (<xref ref-type="fig" rid="fig2">Figure 2C and E</xref>). No significant category information was observed in either layer of the foveal V1 (ts&lt;0.80, uncorrected ps&gt;0.21), indicating that without task-driven feedback signals, feedforward processing alone was insufficient to generate category information in foveal V1. These observations support the interpretation that the category information in foveal V1 observed in the peripheral object discrimination task was coming from the feedback signals.</p></sec><sec id="s2-3"><title>Temporal dynamics of feedback category and orientation information</title><p>The two 7T fMRI experiments revealed distinct neural representations conveyed in the feedback signal and their laminar profiles in V1. In Experiment 3, the temporal dynamics of high- and low-order feature representations in the feedback signal were examined using MEG. The MEG experiment included two tasks, the peripheral object task (as in Experiment 1) and the foveal object task (as in Experiment 2) (see <italic>Materials and methods</italic> for more details). The order of the two task blocks was counterbalanced across participants.</p><p>In the peripheral object experiment (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), to examine the neural representations of high- and low-order information, we trained and tested SVMs for them at each time point after the stimulus onset (see <italic>Materials and methods</italic>). Source localization analysis was applied to map the MEG signals from the sensors to the cortex in each participant. Dynamic <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/topics/neuroscience/statistical-parametric-mapping">statistical parametric mapping</ext-link> (dSPM) (<xref ref-type="bibr" rid="bib10">Dale et al., 2000</xref>) was used to extract neural response patterns in three cortical regions: the early visual cortex, the occipitotemporal cortex, and the posterior parietal cortex. The limited spatial resolution of the source localization method made it difficult to separate the neural responses between foveal and peripheral regions in the early visual cortex. To estimate the strength of the neural representations in the foveal region in the early visual cortex, we applied cross-location decoding analysis to the data from the peripheral object task. During the task, two objects were presented in each trial, either at the upper-right and lower-left visual fields or at the upper-left and lower-right visual fields, thus the visual information was sent to different retinotopic areas in the early visual cortex during feedforward processing. The neural representations elicited by feedforward processing from the two pairs of locations are spatially separated and could not be generalized across locations. However, during feedback processing, object information was fed back to the same foveal cortex, allowing for generalization of feedback neural representation across different peripheral locations. Here, when the decoders were trained and tested using data from objects presented at the same locations, the results showed that both category and orientation information could be decoded from all three cortical regions (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Next, the decoders were trained and tested using data from different locations, and the decoding performance generally decreased in all cortical regions (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). To compare the temporal dynamics between same-location and cross-location decoding, we normalized the peak performances of the same-location decoding and cross-location decoding (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The results indicate that, for both category and orientation information in the early visual cortex, decoding performances were substantially delayed for cross-location data compared to the same-location data. This is consistent with the idea that feedback signals in the early visual cortex, which were slower than feedforward signals, supported the generalized cross-location decoding. In contrast, the dynamics of decoding performances showed location invariance in occipitotemporal cortex. In the posterior parietal cortex, delayed performance was observed for cross-location decoding, possibly due to the extensive retinotopic representations in parietal cortex. Further latency analysis, which estimated the time from stimulus onset to 75% of peak decoding performance, showed consistent results that for both high- and low-order information, the latency was shortest in early visual cortex for same-location decoding but was shortest in occipitotemporal cortex for cross-location decoding (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Temporal dynamics of object representation in different cortical regions (n=15).</title><p>(<bold>A</bold>) In the MEG experiment, two objects of the same type were presented in the periphery. The locations of the objects changed across trials. (<bold>B–C</bold>) Using the source localization algorithm, the neural responses of three cortical regions were extracted. In all regions, both category and orientation information could be decoded either within the same location set (<bold>B</bold>) or across different location sets (<bold>C</bold>). (<bold>D</bold>) Same-location and cross-location decoding performances were normalized and plotted together to facilitate their comparison. The dynamics of decoding performance were significantly delayed for cross-location decoding in early visual cortex (solid red and blue lines). The dashed black line represents the chance level of decoding performance. The gray bar on the time axis indicates the presentation time of the object stimuli. Colored shaded areas reflect ±1 SEM. The colored bars below the time courses indicate the significance (cluster-forming threshold p&lt;0.01, corrected significance threshold p&lt;0.01) of the decoding accuracy in a cluster permutation test. See also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Latency for category (<bold>A</bold>) and orientation (<bold>B</bold>) information in three cortical regions during peripheral object recognition task.</title><p>Time courses of decoding accuracy were smoothed with a Gaussian kernel with a half-width of 150 ms, and latency was estimated as the time from stimulus onset to 75% of peak decoding performance. When calculating the group mean of latency, data exceeding 2 SDs were excluded. The latency of cross-location decoding for category information was significantly longer in early visual cortex than in occipitotemporal cortex (t(11)=2.86, p=0.03), and the effect was marginally significant for orientation information (t(12)=2.52, p=0.06), indicating the decoding performances were driven by feedback signals. In same-location decoding, the latency of category information was similar between early visual cortex and occipitotemporal cortex, and the latency of orientation information is much shorter in early visual cortex (t(13)=4.01, p=0.001). * indicates paired t-test with significance of p&lt;0.05. ** indicates paired t-test with significance of p&lt;0.01. † indicates marginal significance (Holm-Bonferroni corrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig3-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-4"><title>The transmission and behavioral relevance of high- and low-order feedback information</title><p>To evaluate the transmission of high- and low-order information between different cortical regions, Granger causality analysis was performed on the temporal dynamics of neural representation. First, the strength of neural representation at each time point in each ROI was estimated by calculating the distance from the neural response pattern to the decoding hyperplane in the high-dimensional space for each time point in each trial (<xref ref-type="bibr" rid="bib27">Jia et al., 2020</xref>). Then, the Granger causality analysis tested whether the variation in neural representation strength at the current time in the target ROI could be explained by past neural representations in a source ROI, beyond the explanation provided by past representations in the target ROI itself (<xref ref-type="bibr" rid="bib3">Barnett and Seth, 2014</xref>; <xref ref-type="bibr" rid="bib49">Seth, 2010</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Information transmission between cortical regions revealed by Granger causality analysis (n=15).</title><p>(<bold>A</bold>) The strength of representation was estimated by the distance from the neural response pattern to the decoding hyperplane. The dynamics of representation strength were used to estimate information transmission between cortical regions. (<bold>B–C</bold>) The Granger causality of feedforward (blue) and feedback (orange) category (<bold>B</bold>) and orientation (<bold>C</bold>) information between three cortical regions during peripheral object discrimination task. (<bold>D–E</bold>) The Granger causality of feedforward and feedback information of category (<bold>D</bold>) and orientation (<bold>E</bold>) during a fixation detection task with an object presented at the fovea. The directions and initial timings of information flow between cortical regions were also indicated by arrows and the onset times next to the arrows. The gray bar on the time axis indicates the presentation time of the object stimuli. The colored shaded areas reflect ±1 SEM. The colored bars below the time courses indicate the significance (cluster-forming threshold p&lt;0.01, corrected significance threshold p&lt;0.01) of Granger causality in a cluster permutation test.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig4-v1.tif"/></fig><p>In the peripheral object task, significant Granger causality in the feedforward direction was found for high-order category information from early visual cortex to occipitotemporal cortex (190 ms) and to posterior parietal cortex (100 ms). Notably, significant Granger causality was observed in the feedback direction from occipitotemporal cortex to early visual cortex emerging around 220 ms and from posterior parietal cortex to early visual cortex emerging at 520 ms (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). For low-order orientation information, significant Granger causality in the feedforward direction was found from early visual cortex to occipitotemporal cortex and posterior parietal cortex, both emerging around 140 ms. For the feedback direction, significant Granger causality was observed from occipitotemporal cortex to early visual cortex emerging around 250 ms (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). These results further support that feedback signals from occipitotemporal cortex to early visual cortex contain both high- and low-order information. The broad temporal overlap between feedforward and feedback processes, especially for the high-order category information, is consistent with a recurrent processing for visual object recognition.</p><p>In the foveal object task, feedforward category information was observed from the early visual cortex to both occipitotemporal cortex (200 ms) and posterior parietal cortex (310 ms). Feedback category information to the early visual cortex was observed from the occipitotemporal cortex (160 ms) and posterior parietal cortex (360 ms) (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). For orientation information in the feedforward direction, significant Granger causality was found from early visual cortex to occipitotemporal cortex (120 ms) and to posterior parietal cortex (110 ms). For the feedback orientation information, significant Granger causality was observed from occipitotemporal cortex to early visual cortex emerging at 290 ms (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). These results suggest that, for foveally presented objects, the feedforward-feedback recurrent processes between occipitotemporal cortex and early visual cortex occur in a narrower time window.</p><p>After tracking the dynamic transmission of feedback signals, we also examined the behavioral relevance of different types of information in the feedback signal at each time point to uncover the contribution of feedback signals to behavioral performance. The reaction time was recorded for each trial during the MEG session, which allowed us to calculate its correlation with the strength of time-resolved high-order and low-order information in the feedback signal during the peripheral object discrimination task. Similar to Granger causality analysis, the distance from the neural response pattern to the decoding hyperplane was used to estimate the strength of neural representation (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). To concentrate on the feedback signal in early visual cortex, we trained the hyperplane with data from trials in which objects were presented at different locations from the current trial (i.e. cross-location decoding). For each time point, the correlation between the reaction time and the neural representation strength across different trials was calculated for each participant (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The positive correlation indicated that better representation quality linked with faster reaction time. The group-averaged results revealed significant behavioral correlations in the early visual cortex for high-order category information, emerging around 210 ms after stimulus onset, which is consistent with the results from Granger causality analysis. Additionally, significant behavioral correlations were observed in the occipito-temporal cortex (190 ms) (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). However, no significant behavioral correlation was observed in the early visual cortex for low-order orientation information. Apparently, neither high- nor low-order information in the parietal cortex was correlated with behavioral response.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Behavioral relevance of feedback information in the visual cortex (n=15).</title><p>(<bold>A</bold>) The strength of information representation at each time point of each trial was estimated by cross-location decoding, its correlation with reaction time across trials was calculated to estimate its behavioral relevance. (<bold>B</bold>) Significant behavioral relevance was observed for category information in early visual cortex between 200 and 400 ms after stimulus onset, consistent with the time window of feedback signals. Colored shaded areas reflect ±1 SEM. The colored bars below the time courses indicate the significance (cluster-forming threshold p&lt;0.05, corrected significance threshold p&lt;0.05) of the correlation in a cluster permutation test. (<bold>C</bold>) Schematic summary depiction of information flow among key cortical regions (top) and different types of feedback information from temporal cortex to different cortical layers of V1 (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103788-fig5-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Understanding what types of information are conveyed by feedback signals is critical to the investigation of the computational mechanism of feedback processing. Our results demonstrate that the feedback signals to V1 convey both low-order orientation and high-order category information. The orientation information, which is considered natively encoded in V1, could be detected in the feedback signals. The observation supports the notion that the feedback process elicits comparable neural representations to those of the feedforward process (<xref ref-type="bibr" rid="bib31">Keller et al., 2020</xref>; <xref ref-type="bibr" rid="bib33">Kirchberger et al., 2023</xref>), enhancing processing efficiency by predicting the input signals (<xref ref-type="bibr" rid="bib17">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib45">Rao and Ballard, 1999</xref>). Notably, reliable representations of category information were also detected in the feedback signals to V1, while this information could not be observed in V1 in a feedforward-dominated process. These results firmly establish that the category information in V1 originated from a high-level region in the visual hierarchy, rather than generated locally in V1. This suggests that during the recurrent processes for object identification, high-level regions communicate feedback signals to V1, and these feedback signals not only contain local V1-native features that enable predictive error encoding, but also communicate more complex and invariant neural information to constrain the predictive processing.</p><p>Moreover, the current study found that the laminar profile of high-order information differed from that of low-order information in the feedback signals (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Orientation representation was observed in the deep layer, while high-order category representation was found in both deep and superficial layers. The deep and superficial layers of V1 have different roles in visual processing. Both deep and superficial layers receive feedback signals from downstream high-level regions, but the superficial layer also generates output signals for further processing downstream. Specifically, within the superficial layer, output signals are generated in layers 2/3 and sent to the downstream regions targeting layer 4. Meanwhile, layer 1 receives feedback signals that can modulate the output signals in layers 2/3 (<xref ref-type="bibr" rid="bib47">Schuman et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Shipp, 2007</xref>). Our study suggests that the orientation information in feedback signals may reflect the mechanisms predicting the input signal during object processing. The predicted orientation representation in the deep layer of V1 was consistent with previous findings that the neural representations of expected orientation were only observed in the deep layer of V1 (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). This laminar profile was also observed in the V1 neural responses to subjective contours in illusory Kanizsa figures (<xref ref-type="bibr" rid="bib34">Kok et al., 2016</xref>), supporting the predictive coding theory. The high-order category information, which was shown to be fed back to both the superficial and deep layers, may support additional mechanisms during the feedback processing. The category information observed in the superficial layer may arise from layer 1, and its function could be to modulate the output signals in layers 2/3. The feedback information to the superficial layer may be linked to task-driven top-down modulation, as similar laminar profiles have been observed previously in top-down attention and working memory tasks (<xref ref-type="bibr" rid="bib39">Lawrence et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Lawrence et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">van Kerkoerle et al., 2017</xref>). Therefore, we hypothesize that the high-order information in the feedback signal, especially in the superficial layer, guides and constrains feedforward processing to enhance processing efficiency in the object identification task. In other words, if the experimental design required participants to discriminate orientation rather than object identity, we would expect stronger orientation information in foveal V1 and significant decoding performance of orientation feedback information in the superficial layer of foveal V1. Recent studies <xref ref-type="bibr" rid="bib5">Bergmann et al., 2024</xref>; <xref ref-type="bibr" rid="bib26">Iamshchinina et al., 2021</xref> have also highlighted the relationship between feedback information and neural representations in V1 superficial layer. The functional relevance of the feedback signals, especially the high-order information, was also demonstrated in the MEG results, which consistently showed a significant trial-to-trial correlation between the high-order feedback information and the behavioral performances.</p><p>The next question is how high-order information was encoded in the early visual cortex. Recent two-photon imaging has revealed the existence of neurons in the superficial layer of V1 that are selective to complex patterns and features (<xref ref-type="bibr" rid="bib52">Tang et al., 2018a</xref>; <xref ref-type="bibr" rid="bib53">Tang et al., 2018b</xref>; <xref ref-type="bibr" rid="bib56">Victor et al., 2006</xref>; <xref ref-type="bibr" rid="bib57">Vinje and Gallant, 2000</xref>). Although the distribution of such complex-feature selective neurons is very sparse, they likely play an important role in the representation and utilization of high-order feedback information. Meanwhile, in accordance with the sparse distribution of the complex feature selective neurons in V1, the feedback signals did not significantly increase the general fMRI response amplitude in foveal V1. Currently, the response dynamics and connectivity patterns of these V1 neurons remain unclear; future studies should compare high-order and low-order information dynamics across different cortical layers to further advance our understanding of how high-order feedback information interacts with feedforward visual processing to increase efficiency.</p><p>The Granger causality analysis from MEG results reveals that the backward information transition of both high-order category information and low-order orientation information from the occipitotemporal cortex to the early visual cortex emerged approximately 200 ms after the peripheral object onset. Consistently, the behavioral correlation results show that high-order information in the early visual cortex began to correlate with behavioral performance around 200 ms. These results suggest that functional-relevant high-order information reached foveal V1 approximately 200 ms after stimulus onset in the peripheral object identification task. The temporal dynamics of feedback information in this study are consistent with previous observations (<xref ref-type="bibr" rid="bib7">Chambers et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Ge et al., 2020</xref>). However, it is important to note that the temporal dynamics of the feedback process may vary depending on the visual inputs and task demands. On one hand, low-quality visual input, such as low-contrast or occluded objects, may delay the initial feedforward and model generating process (<xref ref-type="bibr" rid="bib35">Kovács et al., 1995</xref>; <xref ref-type="bibr" rid="bib44">Nielsen et al., 2006</xref>; <xref ref-type="bibr" rid="bib55">VanRullen and Thorpe, 2001</xref>), which in turn delays the feedback signals. On the other hand, previous research has shown that the foveal feedback mechanism exhibits some degree of flexibility. For instance, increasing the high-level operation of the peripheral task can delay the temporal dynamics of the foveal feedback (<xref ref-type="bibr" rid="bib13">Fan et al., 2016</xref>). Additionally, there is also evidence that the feedback process operates differently for peripheral vision compared with central vision, with the central-peripheral dichotomy theory proposing a weaker feedback for peripheral vision (<xref ref-type="bibr" rid="bib62">Zhaoping, 2024</xref>). Therefore, there are multiple factors that may lead to temporal variations of the feedback signal (<xref ref-type="bibr" rid="bib60">Wyatte et al., 2014</xref>).</p><p>In the current study, fMRI signals from early visual cortex and two high-level brain regions (LOC and pIPS) were recorded. Neural dynamics of these regions were extracted from MEG signals. Decoding analyses based on fMRI and MEG signals consistently showed that object category information could be decoded from both regions. These findings raise an important question: which region is the source of category-specific feedback to the early visual cortex, and how is this information processed in high-level brain regions? Further Granger causality analysis indicates that the feedback information in foveal V1 was mainly driven by signals from the LOC. Layer-specific analysis showed that category information could be decoded in the middle and superficial layers of the LOC. A reasonable interpretation of this result is that feedforward information from the early visual cortex was received by the LOC’s middle layer, then the category information was generated and fed back to foveal V1 through the LOC’s superficial layer. A recent study found that, in object-selective regions in temporal cortex, the deep layer showed the strongest fMRI responses during an imagery task (<xref ref-type="bibr" rid="bib6">Carricarte et al., 2024</xref>). Together, the results suggest that the deep and superficial layers correspond to different feedback mechanisms. It is worth noting that other cortical regions may also generate feedback signals to the early visual cortex. The current study did not have simultaneously recorded fMRI signals from the prefrontal cortex, but it has been shown that feedback signals can be traced back to the prefrontal cortex during complex cognitive tasks, such as working memory (<xref ref-type="bibr" rid="bib11">Degutis et al., 2024</xref>; <xref ref-type="bibr" rid="bib15">Finn et al., 2019</xref>). Further fMRI studies with submillimeter resolution and whole-brain coverage are needed to test other potential feedback pathways during object processing.</p><p>The peripheral object identification task required processing of both category and orientation information, and the results showed that both kinds of information were fed back to early visual cortex. Evidence has shown that the foveal feedback occurrence depends on the requirement for distinguishing fine object details in the periphery (<xref ref-type="bibr" rid="bib13">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Yu and Shim, 2016</xref>). Object category feedback information was not observed in foveal V1 when participants were distinguishing between colors instead of distinguishing between object identities (<xref ref-type="bibr" rid="bib58">Williams et al., 2008</xref>). It is apparent that the feedback mechanism could flexibly select the information, especially the high-order information, sent back to the early visual cortex depending on the task requirements. However, it remains unclear whether low-order information conveyed in the feedback signals is more independent of the task demand. Supposing the function of the low-order feedback information is to support the realization of predictive coding in early visual cortex, then the low-order information could be more intrinsic and task-invariant in feedback signals.</p><p>The hierarchical structure and extensive feedback connections are key features of the human neural system. Feedback signals are believed to be important for efficient neural processing, but the algorithm of these signals in neural computation remains largely unknown. Our findings dissociated various types of information in the feedback signals, uncovered their laminar profiles, and traced their temporal dynamics across the cortical hierarchy. These findings reveal the multiple components in the feedback signals and contribute to the comprehensive understanding of the interactive feedforward and feedback computational mechanisms, a key feature in human intelligence.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>In the fMRI experiments, 22 participants were recruited, all of whom had normal or corrected-to-normal vision. In the data analysis, 4 participants were excluded due to excessive head movement. Therefore, data from 18 effective participants (10 females; aged 19–29) was included in the results. The MEG experiment recruited 15 participants (10 females; aged 19–29). Before the experiments, all participants gave written consent, and the institutional review board of the Institute of Biophysics, Chinese Academy of Sciences, approved the protocols (#2017-IRB-004).</p></sec><sec id="s4-2"><title>fMRI experiments</title><sec id="s4-2-1"><title>Stimuli and procedures</title><p>OpenGL was used to generate object stimuli. The stimuli consisted of two categories (cubie vs. smoothie) and two orientations (vertical vs. horizontal). For each category, 18 individual objects were created.</p><p>During the peripheral object task, objects from the same condition were presented in the upper-left and lower-right visual field, 7° away from the fixation. The object size was either 3°×1.5° or 1.5°×3° depending on the stimulus conditions. Objects were presented for 100 ms in each trial, followed by 1900 ms of blank screen. Participants were asked to judge whether the two objects were identical. To maximize the fMRI signals, block design was used, with each block containing 8 trials from the same condition. During each block, the fixation was presented for 200 ms repeatedly, with 300 ms interval between two fixations. The color of the fixation changed randomly, but this should be ignored in the peripheral task.</p><p>In the foveal task of the fMRI experiment, an object of the same size as in the peripheral task was presented at foveal for 100 ms, followed by a 1900 ms interval. All other parameters were the same as in the peripheral task. The participants completed a one-back task where they had to press a button when the fixation color was repeated between two adjacent presentations.</p><p>The fMRI experiment contained 12 task runs. Each run included 8 blocks, with 1 block from each condition in each task, interleaved with 12 s blank periods. The peripheral and foveal task blocks were presented alternately, and the orders were counterbalanced across runs. The orders of the object condition were pseudorandom across runs. The accuracy of behavioral responses in the peripheral object task was 67.4% ± 4.4%.</p><p>In addition to the task runs, three localizer runs were included in the scan. In one of the runs, flashing checkerboards with a size of 3° were presented in different blocks at either the foveal or peripheral visual field, which was used to localize the ROIs of foveal and peripheral V1. The locations of the checkerboards matched the object locations in the task runs. The other two runs were used to localize LOC and pIPS. Everyday object images and phase-scrambled images were presented in different blocks.</p></sec><sec id="s4-2-2"><title>MRI scanning and preprocessing</title><p>MRI data were collected on a Siemens Magnetom 7 Tesla MRI system (passively shielded, 45 mT/s slew rate) (Siemens, Erlangen, Germany), with a 32-channel receive 1-channel transmit head coil (NOVA Medical, Inc, Wilmington, MA, USA), at the Beijing MRI Center for Brain Research (BMCBR). High-resolution T1-weighted anatomical images (0.7 mm isotropic voxel size) were acquired with an MPRAGE sequence (256 sagittal slices, acquisition matrix = 320 × 320, field of view [FOV]=223 × 223 mm<sup>2</sup>, GRAPPA factor = 3, TR = 4000 ms, TE = 3.05 ms, TI = 0 ms, flip angle = 0°, pixel bandwidth = 240 Hz per pixel). GE-EPI sequences were used to collect functional data in the main experiment (TR = 2000 ms, TE = 23 ms, 0.8 mm isotropic voxels, FOV = 128 × 128 mm<sup>2</sup>, GRAPPA factor = 3, partial Fourier 6/8, 31 slices of 0.8 mm thickness, flip angle is about 80, pixel bandwidth = 1157 Hz per pixel). During the scan, GE-EPI images with reversed-phase encoding direction from experiment functional scan were collected to correct the spatial distortion of EPI images.</p><p>MRI image data were analyzed with FreeSurfer (CorTechs Inc, Charlestown, MA, USA) (<xref ref-type="bibr" rid="bib16">Fischl, 2012</xref>), AFNI (<ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov">http://afni.nimh.nih.gov</ext-link>) (<xref ref-type="bibr" rid="bib9">Cox, 1996</xref>), and MRIpy package (<ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link>; <xref ref-type="bibr" rid="bib23">herrlich10, 2025</xref>). For anatomical data, to reconstruct the cortical surfaces, anatomical data were further processed by FreeSurfer, including gray and white matter segmentation and identification of V1 region. SUMA and MRIpy package were used to generate equi-volume surfaces. For each voxel, its volume percentages of WM, CSF, and different cortical layers (deep, middle, superficial) were calculated. For function data, preprocessing included slice-timing correction, motion correction, distortion correction using reversed-phase encoding EPI images, and intensity normalization. An additional spatial smoothing with a 2 mm Gaussian kernel was applied to the localizer data. Beta values of stimulus-evoked responses were estimated with GLM for each voxel.</p></sec><sec id="s4-2-3"><title>Regions of interest</title><p>Foveal V1 and peripheral V1 were defined on the surface of each participant, with contrast between the foveal and peripheral checkerboards (p&lt;0.01, uncorrected). The anatomical labeling of V1 was used to constrain the spatial locations of the ROIs. The LOC was defined as regions responding more strongly to everyday objects than to scrambled objects in ventral occipital-temporal cortex. The pIPS was located in the intraparietal sulcus, which responded more strongly to real objects than to the rest conditions. All ROIs were defined on the gray matter surface and then converted to volume. For the layer analysis, the voxels in the ROI could be further classified into deep, middle, and superficial layers based on their dominant volume percentages generated from the anatomical data analysis. Due to the limited FOV size of the EPI sequence, the LOC was not covered in one participant. For each ROI, the number of voxels depended on the size of the activated region, as estimated from the localizer data. The numbers are as follows: foveal V1, 2185±389; peripheral V1, 1294±215; LOC, 3451±863; and pIPS, 5154±1517.</p></sec><sec id="s4-2-4"><title>Correcting the vasculature-related signals</title><p>Two analyses were performed to remove vasculature-related signals. The distribution of beta values in each ROI was fitted by two Gaussian distributions. Voxels that fell into the higher-response Gaussian distribution were excluded from further analyses (<xref ref-type="bibr" rid="bib30">Kay et al., 2019</xref>). Second, the mean EPI signal was calculated for each voxel, and the spatial trend of EPI signal was removed in each ROI. Similar to the beta-value analysis, two Gaussian distributions were used to fit the distribution of EPI signal in each ROI. Voxels falling into the lower-response Gaussian distribution were also excluded from further analyses (<xref ref-type="bibr" rid="bib30">Kay et al., 2019</xref>). The proportions of voxels removed were as follows: V1 foveal, 22.5% ± 6.6%; V1 peripheral, 6.8% ± 3.9%; LOC, 16.1% ± 8.1%; and pIPS, 5.1% ± 3.2%.</p></sec><sec id="s4-2-5"><title>Decoding analysis</title><p>For each ROI and each participant, a linear classifier (<ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</ext-link>) (<xref ref-type="bibr" rid="bib8">Chang and Lin, 2011</xref>) was trained to classify the neural response patterns from different conditions. The beta values from each block (i.e. averaged neural responses in each block) were used as data samples. To estimate the decoding performances, supervised learning and a leave-one-run-out cross-validation approach were used. The neural response patterns of the same condition were averaged in each run. Before each training procedure for each ROI, the top 500 voxels were selected based on their t-value between two conditions. For the layer analysis in each ROI, the top 300 voxels were selected for each layer. If a layer had fewer than 300 voxels, then all the voxels were included in the training procedure.</p></sec></sec><sec id="s4-3"><title>MEG experiment</title><sec id="s4-3-1"><title>Stimuli and procedures</title><p>The MEG experiment used similar stimuli and tasks as the fMRI experiment. For the peripheral object task, two objects were presented in the peripheral visual fields (7° from fixation) for 100 ms in each trial. The object locations were either upper left and lower right or upper right and lower left, which was random across trials. Participants had to determine whether the two objects were identical within 2100 ms of stimulus onset. Each participant completed 104 trials for every condition at each location set. For the foveal task, each trial lasted between 900 and 1300 ms randomly, and the object was presented at the fixation for 100 ms. Participants were instructed to press a button when the fixation color turned white. Trials in which a key was pressed were excluded from later analyses. 104 valid trials were collected for each object condition for each participant.</p><p>Each participant completed four task runs, each consisting of one peripheral task block and one foveal task block. The order of the two tasks was counterbalanced within each participant. The accuracy of the peripheral object task was 67.3% ± 5.1%.</p></sec><sec id="s4-3-2"><title>MEG data acquisition and preprocessing</title><p>The MEG experiment was conducted using a CTF system at the BMCBR. The data analysis was completed using the MNE Python toolbox (<xref ref-type="bibr" rid="bib20">Gramfort et al., 2013</xref>) and custom codes (<xref ref-type="bibr" rid="bib24">Hou et al., 2025</xref>). The raw data were sampled at 300 Hz and then downsampled to 100 Hz. A bandpass filter between 1 and 30 Hz was applied.</p><p>Independent component analysis was used to eliminate biological artifacts, including heartbeat and ocular artifacts. For each trial, the MEG data were baseline-corrected using a time window from 243 to 43 ms before stimulus onset. Trials with instantaneous distortion were excluded from further analysis, along with their adjacent trials. One participant did not complete all trials in the foveal task, resulting in approximately 92 trials per condition.</p></sec><sec id="s4-3-3"><title>Source reconstruction</title><p>The cortical surface was reconstructed for each individual participant based on the anatomical T1 data acquired in MRI scan. A boundary element model was set up based on the inner skull boundary extracted via watershed algorithm (<xref ref-type="bibr" rid="bib48">Ségonne et al., 2004</xref>). To extract the source space and calculate the forward solution, the coordinate frames were aligned based on fiducials and digitizer points on the head surface. For each hemisphere, 4098 source points were generated to build the source space. A regularized noise covariance matrix was estimated using MEG data from 243 to 43 ms before stimulus onset. Source estimates were obtained using dSPM (<xref ref-type="bibr" rid="bib10">Dale et al., 2000</xref>), a linear minimum-norm inverse method. A loose value of 0.2 was used when calculating the inverse operator. The resulting source activations were projected onto the surface normal.</p><p>Three ROIs, early visual cortex, occipitotemporal cortex, and posterior-parietal cortex, were identified using the anatomical labels in the surface reconstructed by FreeSurfer (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>). The data from the source points within each ROI were extracted for further analyses.</p></sec><sec id="s4-3-4"><title>MEG decoding</title><p>The decoding analysis employed a neural decoding toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.readout.info">http://www.readout.info</ext-link>) (<xref ref-type="bibr" rid="bib42">Meyers, 2013</xref>) and custom MATLAB codes (<xref ref-type="bibr" rid="bib24">Hou et al., 2025</xref>). The trials from each condition were randomly divided into eight splits and were averaged within each split. Then, a cross-validator was created with a leave-one-split-out procedure, similar to the decoding analysis in the fMRI data. The 100 most informative channels or sources were selected based on the F-value across all conditions and were further used to train and test decoders.</p><p>The above process was repeated 20 times at each time point to estimate the temporal dynamics of high- and low-order information. In the peripheral task, in addition to training and testing decoders with data from identical stimulus locations, the decoders were also trained and tested across different stimulus locations to examine the location-invariant visual representation. The temporal dynamics of decoding performances were smoothed with a Gaussian kernel with a half-width of 100 ms.</p></sec><sec id="s4-3-5"><title>Granger causality analysis</title><p>The Granger causality analysis was conducted with the Multivariate Granger Causality (MVGC) toolbox (<xref ref-type="bibr" rid="bib3">Barnett and Seth, 2014</xref>; <xref ref-type="bibr" rid="bib49">Seth, 2010</xref>) using the time courses of visual information represented in each ROI, which were estimated by the distances from the neural response pattern to the decoding hyperplane in the high-dimensional spaces of neural responses (<xref ref-type="bibr" rid="bib27">Jia et al., 2020</xref>). The similar training and testing procedures were applied to the MEG time courses as in the decoding analysis, but the distance of neural response pattern to the trained hyperplane was calculated at each time point. A further distance in the correct direction indicated a higher quality of neural representation at each time point. The distances were normalized by subtracting the mean and dividing by the standard deviation of trials. Next, an autoregressive model was used to examine whether the quality of the neural representation in the current time in a target region could be explained by the past representation quality in a source region, beyond the explanation supplied by the past of the target region itself. The Granger causal influence was defined as ln(U<sub>reduced</sub>/U<sub>full</sub>), where U is the unexplained variance by the model. To create baselines for statistical comparisons, the MEG data before the stimulus onset (–200 to 0 ms) were extracted and the same autoregressive model was applied to estimate the increase of variance explanation in the noise background (<xref ref-type="bibr" rid="bib32">Kietzmann et al., 2019</xref>). For each pair of ROIs, we tested both directions of Granger causality with the all other ROIs serving as regression factors. The analysis was conducted for each time point using a 150 ms sliding time windows preceding this time point, and a model order (i.e. the number of time-lags) of 5 (10, 20, 30, 40, 50 ms) was selected. To enhance visibility, the time courses of the Granger causality influence were smoothed by a Gaussian kernel with a half-width of 50 ms.</p></sec><sec id="s4-3-6"><title>Behavioral correlation of neural representation</title><p>For each time point in each trial, the distance from the neural response pattern to the decoding hyperplane was calculated to estimate the quality of neural representation. The decoding hyperplane was trained with the neural responses of objects presented at the location set different from the current trial (cross-location decoding). Then, for each condition at each time point, the Pearson correlation coefficient was calculated between the decoding distances and the behavioral reaction times across trials. Finally, the correlation coefficients were averaged across all conditions to generate the time course of behavioral correlation of neural representation for each participant. This procedure was repeated for each brain region. The positive correlation coefficient meant that the better quality of neural representation related to a faster reaction. Only the data from correct trials were used to estimate the behavioral correlation. The trials with reaction time outside two SDs were excluded from the analysis. For correlation calculation in each time point, the neural response patterns that were incorrectly classified with high distance (outside two SDs) were excluded. The time courses of behavioral correlation were smoothed by a Gaussian kernel with a half-width of 100 ms.</p></sec><sec id="s4-3-7"><title>Significance testing</title><p>In the fMRI response analyses, two-tailed t-test was used to assess whether the BOLD signal change was different from baseline. In the decoding analysis, a one-tailed t-test was used to examine whether the decoding accuracy exceeded the chance level. To control for multiple comparisons in the laminar analysis, the FDR procedure (<xref ref-type="bibr" rid="bib4">Benjamini and Hochberg, 1995</xref>) was applied to correct the p-values for all layers and ROIs within each task, across a total of 24 tests.</p><p>For the MEG decoding, since we did not have a strong prior about when feedback information would arrive in the foveal V1, we used the nonparametric cluster-based permutation test (<xref ref-type="bibr" rid="bib41">Maris and Oostenveld, 2007</xref>; <xref ref-type="bibr" rid="bib51">Spaak, 2024</xref>) to correct multiple comparisons over time. For each time point, the sign of the effect (i.e. decoding accuracy vs. chance, Granger causality influence vs. baseline, behavioral correlation vs. zeros) was randomly flipped in each participant for 50,000 times to get the null hypothesis distribution. Then, the cluster-based permutation test was performed, where clusters were defined as continuous significant time points in the time series. The effects in each cluster were summed, and the most significant of them in the time series was used to generate the corrected null hypotheses distribution.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Before the experiments, all participants gave written consent, and the institutional review board of the Institute of Biophysics, Chinese Academy of Sciences approved the protocols (#2017-IRB-004).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103788-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All preprocessed data and code in this study have been uploaded to Figshare: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.25991362.v1">https://doi.org/10.6084/m9.figshare.25991362.v1</ext-link>. The raw data have been deposited in Zenodo: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.17865765">https://doi.org/10.5281/zenodo.17865765</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>W</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>The code and preprocessed data of &quot;Differential destinations, dynamics, and functions of high- and low-order features in the feedback signal during object processing&quot;</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.25991362.v1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>W</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Differential destinations, dynamics, and functions of high- and low-order features in the feedback signal during object processing</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.17865765</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Brain Science and Brain-like Intelligence Technology – National Science and Technology Major Project (Grant Nos. 2021ZD0204200, 2021ZD0203800); Key Research Program of Frontier Sciences, Chinese Academy of Sciences (Grant No. KJZD-SW-L08); and CAS Project for Young Scientists in Basic Research (Grant No. YSBR-071). The authors would like to thank Dr. Peng Zhang and Dr. Chencan Qian for their help during data collection and analysis.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>F</given-names></name><name><surname>Menelaou</surname><given-names>G</given-names></name><name><surname>Warrington</surname><given-names>O</given-names></name><name><surname>Koolschijn</surname><given-names>RS</given-names></name><name><surname>Corbin</surname><given-names>N</given-names></name><name><surname>Callaghan</surname><given-names>MF</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prior expectations evoke stimulus-specific activity in the deep layers of the primary visual cortex</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3001023</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001023</pub-id><pub-id pub-id-type="pmid">33284791</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference</article-title><source>Journal of Neuroscience Methods</source><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.10.018</pub-id><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id><pub-id pub-id-type="pmid">11682119</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergmann</surname><given-names>J</given-names></name><name><surname>Petro</surname><given-names>LS</given-names></name><name><surname>Abbatecola</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>MS</given-names></name><name><surname>Morgan</surname><given-names>AT</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Cortical depth profiles in primary visual cortex for illusory and imaginary experiences</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>1002</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-45065-w</pub-id><pub-id pub-id-type="pmid">38307834</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carricarte</surname><given-names>T</given-names></name><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Trampel</surname><given-names>R</given-names></name><name><surname>Chaimow</surname><given-names>D</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Laminar dissociation of feedforward and feedback in high-level ventral visual cortex during imagery and perception</article-title><source>iScience</source><volume>27</volume><elocation-id>110229</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2024.110229</pub-id><pub-id pub-id-type="pmid">39006482</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Allen</surname><given-names>CPG</given-names></name><name><surname>Maizey</surname><given-names>L</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Is delayed foveal feedback critical for extra-foveal perception?</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>49</volume><fpage>327</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2012.03.007</pub-id><pub-id pub-id-type="pmid">22503283</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research, an International Journal</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Liu</surname><given-names>AK</given-names></name><name><surname>Fischl</surname><given-names>BR</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Lewine</surname><given-names>JD</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title><source>Neuron</source><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81138-1</pub-id><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degutis</surname><given-names>JK</given-names></name><name><surname>Chaimow</surname><given-names>D</given-names></name><name><surname>Haenelt</surname><given-names>D</given-names></name><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name><name><surname>Lorenz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Dynamic layer-specific processing in the prefrontal cortex during working memory</article-title><source>Communications Biology</source><volume>7</volume><elocation-id>1140</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-024-06780-8</pub-id><pub-id pub-id-type="pmid">39277694</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Albright</surname><given-names>TD</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name><name><surname>Bruce</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Stimulus-selective properties of inferior temporal neurons in the macaque</article-title><source>The Journal of Neuroscience</source><volume>4</volume><fpage>2051</fpage><lpage>2062</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.04-08-02051.1984</pub-id><pub-id pub-id-type="pmid">6470767</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shao</surname><given-names>H</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Temporally flexible feedback signal to foveal cortex for peripheral object recognition</article-title><source>PNAS</source><volume>113</volume><fpage>11627</fpage><lpage>11632</lpage><pub-id pub-id-type="doi">10.1073/pnas.1606137113</pub-id><pub-id pub-id-type="pmid">27671651</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Jangraw</surname><given-names>DC</given-names></name><name><surname>Molfese</surname><given-names>PJ</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Layer-dependent activity in human prefrontal cortex during working memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1687</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0487-z</pub-id><pub-id pub-id-type="pmid">31551596</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id><pub-id pub-id-type="pmid">15937014</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Qian</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Adaptation to feedback representation of illusory orientation produced from flash grab effect</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3925</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17786-1</pub-id><pub-id pub-id-type="pmid">32764538</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Top-down influences on visual processing</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>350</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id><pub-id pub-id-type="pmid">23595013</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical connectivity and sensory coding</article-title><source>Nature</source><volume>503</volume><fpage>51</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1038/nature12654</pub-id><pub-id pub-id-type="pmid">24201278</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="software"><person-group person-group-type="author"><collab>herrlich10</collab></person-group><year iso-8601-date="2025">2025</year><data-title>Mripy</data-title><version designator="db1324a">db1324a</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>W</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>The code and preprocessed data of “Differential destinations, dynamics, and functions of high- and low-order features in the feedback signal during object processing</data-title><source>Figshare</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.25991362.v1">https://doi.org/10.6084/m9.figshare.25991362.v1</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Ferrier lecture Functional architecture of macaque monkey visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>198</volume><fpage>1</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1098/rspb.1977.0085</pub-id><pub-id pub-id-type="pmid">20635</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Yakupov</surname><given-names>R</given-names></name><name><surname>Haenelt</surname><given-names>D</given-names></name><name><surname>Sciarra</surname><given-names>A</given-names></name><name><surname>Mattern</surname><given-names>H</given-names></name><name><surname>Luesebrink</surname><given-names>F</given-names></name><name><surname>Duezel</surname><given-names>E</given-names></name><name><surname>Speck</surname><given-names>O</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Perceived and mentally rotated contents are differentially represented in cortical depth of V1</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>1069</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02582-4</pub-id><pub-id pub-id-type="pmid">34521987</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>K</given-names></name><name><surname>Zamboni</surname><given-names>E</given-names></name><name><surname>Kemper</surname><given-names>V</given-names></name><name><surname>Rua</surname><given-names>C</given-names></name><name><surname>Goncalves</surname><given-names>NR</given-names></name><name><surname>Ng</surname><given-names>AKT</given-names></name><name><surname>Rodgers</surname><given-names>CT</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent processing drives perceptual plasticity</article-title><source>Current Biology</source><volume>30</volume><fpage>4177</fpage><lpage>4187</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.08.016</pub-id><pub-id pub-id-type="pmid">32888488</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast recurrent processing via ventrolateral prefrontal cortex is needed by the primate ventral stream for robust core visual object recognition</article-title><source>Neuron</source><volume>109</volume><fpage>164</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.035</pub-id><pub-id pub-id-type="pmid">33080226</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Jeong</surname><given-names>SK</given-names></name><name><surname>Mruczek</surname><given-names>REB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A brief comparative review of primate posterior parietal cortex: A novel hypothesis on the human toolmaker</article-title><source>Neuropsychologia</source><volume>105</volume><fpage>123</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.01.034</pub-id><pub-id pub-id-type="pmid">28159617</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Vizioli</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Margalit</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A critical assessment of data quality and venous effects in sub-millimeter fMRI</article-title><source>NeuroImage</source><volume>189</volume><fpage>847</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.006</pub-id><pub-id pub-id-type="pmid">30731246</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>AJ</given-names></name><name><surname>Roth</surname><given-names>MM</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Feedback generates a second receptive field in neurons of the visual cortex</article-title><source>Nature</source><volume>582</volume><fpage>545</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2319-4</pub-id><pub-id pub-id-type="pmid">32499655</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id><pub-id pub-id-type="pmid">31591217</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirchberger</surname><given-names>L</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Contextual drive of neuronal responses in mouse V1 in the absence of feedforward input</article-title><source>Science Advances</source><volume>9</volume><elocation-id>eadd2498</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.add2498</pub-id><pub-id pub-id-type="pmid">36662858</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Bains</surname><given-names>LJ</given-names></name><name><surname>van Mourik</surname><given-names>T</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Selective activation of the deep layers of the human primary visual cortex by top-down feedback</article-title><source>Current Biology</source><volume>26</volume><fpage>371</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.038</pub-id><pub-id pub-id-type="pmid">26832438</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovács</surname><given-names>G</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Selectivity of macaque inferior temporal neurons for partially occluded shapes</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>1984</fpage><lpage>1997</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-03-01984.1995</pub-id><pub-id pub-id-type="pmid">7891146</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Beyond the feedforward sweep: feedback computations in the visual cortex</article-title><source>Annals of the New York Academy of Sciences</source><volume>1464</volume><fpage>222</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1111/nyas.14320</pub-id><pub-id pub-id-type="pmid">32112444</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(98)80042-1</pub-id><pub-id pub-id-type="pmid">9751656</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>SJD</given-names></name><name><surname>van Mourik</surname><given-names>T</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Koopmans</surname><given-names>PJ</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Laminar organization of working memory signals in human visual cortex</article-title><source>Current Biology</source><volume>28</volume><fpage>3435</fpage><lpage>3440</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.08.043</pub-id><pub-id pub-id-type="pmid">30344121</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>SJ</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissociable laminar profiles of concurrent bottom-up and top-down modulation in the human visual cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e44422</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44422</pub-id><pub-id pub-id-type="pmid">31063127</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Guo</surname><given-names>F</given-names></name><name><surname>Qian</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>DJ</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Layer-dependent multiplicative effects of spatial attention on contrast responses in human early visual cortex</article-title><source>Progress in Neurobiology</source><volume>207</volume><elocation-id>101897</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101897</pub-id><pub-id pub-id-type="pmid">32818495</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The neural decoding toolbox</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00008</pub-id><pub-id pub-id-type="pmid">23734125</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>AT</given-names></name><name><surname>Petro</surname><given-names>LS</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Scene representations conveyed by cortical feedback to early visual cortex can be described by line drawings</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9410</fpage><lpage>9423</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0852-19.2019</pub-id><pub-id pub-id-type="pmid">31611306</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nielsen</surname><given-names>KJ</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Rainer</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dissociation between local field potentials and spiking activity in macaque inferior temporal cortex reveals diagnosticity-based encoding of complex objects</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>9639</fpage><lpage>9645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2273-06.2006</pub-id><pub-id pub-id-type="pmid">16988034</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rockland</surname><given-names>KS</given-names></name><name><surname>Pandya</surname><given-names>DN</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey</article-title><source>Brain Research</source><volume>179</volume><fpage>3</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(79)90485-2</pub-id><pub-id pub-id-type="pmid">116716</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuman</surname><given-names>B</given-names></name><name><surname>Dellal</surname><given-names>S</given-names></name><name><surname>Prönneke</surname><given-names>A</given-names></name><name><surname>Machold</surname><given-names>R</given-names></name><name><surname>Rudy</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neocortical layer 1: an elegant solution to top-down and bottom-up integration</article-title><source>Annual Review of Neuroscience</source><volume>44</volume><fpage>221</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-100520-012117</pub-id><pub-id pub-id-type="pmid">33730511</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Glessner</surname><given-names>M</given-names></name><name><surname>Salat</surname><given-names>D</given-names></name><name><surname>Hahn</surname><given-names>HK</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A hybrid approach to the skull stripping problem in MRI</article-title><source>NeuroImage</source><volume>22</volume><fpage>1060</fpage><lpage>1075</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.032</pub-id><pub-id pub-id-type="pmid">15219578</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A MATLAB toolbox for Granger causal connectivity analysis</article-title><source>Journal of Neuroscience Methods</source><volume>186</volume><fpage>262</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2009.11.020</pub-id><pub-id pub-id-type="pmid">19961876</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shipp</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Structure and function of the cerebral cortex</article-title><source>Current Biology</source><volume>17</volume><fpage>R443</fpage><lpage>R449</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.03.044</pub-id><pub-id pub-id-type="pmid">17580069</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Cluster test: simple cluster-based permutation testing in arbitrary dimensions</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.10877825">https://doi.org/10.5281/zenodo.10877825</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>TS</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Teo</surname><given-names>B</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Complex pattern selectivity in macaque primary visual cortex revealed by large-scale two-photon imaging</article-title><source>Current Biology</source><volume>28</volume><fpage>38</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.039</pub-id><pub-id pub-id-type="pmid">29249660</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Large-scale two-photon imaging revealed super-sparse population codes in the V1 superficial layer of awake monkeys</article-title><source>eLife</source><volume>7</volume><elocation-id>e33370</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33370</pub-id><pub-id pub-id-type="pmid">29697371</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13804</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13804</pub-id><pub-id pub-id-type="pmid">28054544</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The time course of visual processing: from early perception to decision-making</article-title><source>Journal of Cognitive Neuroscience</source><volume>13</volume><fpage>454</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1162/08989290152001880</pub-id><pub-id pub-id-type="pmid">11388919</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Mechler</surname><given-names>F</given-names></name><name><surname>Repucci</surname><given-names>MA</given-names></name><name><surname>Purpura</surname><given-names>KP</given-names></name><name><surname>Sharpee</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Responses of V1 neurons to two-dimensional hermite functions</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>379</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1152/jn.00498.2005</pub-id><pub-id pub-id-type="pmid">16148274</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title><source>Science</source><volume>287</volume><fpage>1273</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1126/science.287.5456.1273</pub-id><pub-id pub-id-type="pmid">10678835</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Shim</surname><given-names>WM</given-names></name><name><surname>Dang</surname><given-names>S</given-names></name><name><surname>Triantafyllou</surname><given-names>C</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Feedback of visual object information to foveal retinotopic cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1439</fpage><lpage>1445</lpage><pub-id pub-id-type="doi">10.1038/nn.2218</pub-id><pub-id pub-id-type="pmid">18978780</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong-Riley</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Reciprocal connections between striate and prestriate cortex in squirrel monkey as demonstrated by combined peroxidase histochemistry and autoradiography</article-title><source>Brain Research</source><volume>147</volume><fpage>159</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(78)90781-3</pub-id><pub-id pub-id-type="pmid">77701</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyatte</surname><given-names>D</given-names></name><name><surname>Jilk</surname><given-names>DJ</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Early recurrent feedback facilitates visual object recognition under challenging conditions</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>674</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00674</pub-id><pub-id pub-id-type="pmid">25071647</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Shim</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modulating foveal representation can influence visual discrimination in the periphery</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1167/16.3.15</pub-id><pub-id pub-id-type="pmid">26885627</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhaoping</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Peripheral vision is mainly for looking rather than seeing</article-title><source>Neuroscience Research</source><volume>201</volume><fpage>18</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2023.11.006</pub-id><pub-id pub-id-type="pmid">38000447</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103788.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kok</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This study reports <bold>important</bold> findings about the nature of feedback to primary visual cortex (V1) during object recognition. The state-of-the-art functional MRI evidence for the main claims is <bold>solid</bold>, and the combination of high-resolution fMRI with MEG yields significant insight into neural mechanisms. The findings presented here are relevant to a number of scientific fields such as object recognition, categorisation and predictive coding.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103788.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This study examines the spatiotemporal properties of feedback signals in the human brain during an object discrimination task. Using 7T fMRI and MEG, the authors show that task-relevant object category information can be decoded from both deep and superficial layers of V1, originating from occipito-temporal and posterior parietal cortices. In contrast, task-irrelevant category feedback does not appear in V1, even when the same objects are foveally presented. Low-level orientation information, however, is decodable from V1 regardless of task relevance and is supported by recurrence with occipito-temporal regions. These findings suggest that category decoding in V1 depends on task-driven feedback rather than feedforward visual features.</p><p>Strengths</p><p>This study leverages two advanced neuroimaging modalities attempting to connect object recognition across cortical layer and whole-brain levels. The revised manuscript strengthens the connection between the fMRI and MEG components.</p><p>It also demonstrates that a peripheral object discrimination task is effective for isolating feedforward and feedback signals using 7T fMRI.</p><p>It is particularly notable that no low-level features were fed back to V1's superficial layers in the peripheral object discrimination task. The authors further show that high- and low-level feedback to the foveal V1 are comparable in strength, supporting the idea that the superficial layer in V1 selectively represents task-relevant content.</p><p>Weaknesses</p><p>One alternative explanation for the absence of task-irrelevant category decoding in the foveal task could be that feedback enhancement may be required to decode complex features from V1 (compared to a coarse orientation feature). It would be informative to test whether the findings hold if the categorical boundary were defined through a low level feature other than orientation (e.g., frequency) (e.g. Ester, Sprague and Serences, 2020).</p><p>I would like to echo the concerns raised by the other reviewer regarding multiple comparisons correction. It is important to apply correction procedures, especially given the number of statistical tests performed across brain regions where strict a priori hypotheses are unlikely. In the case of cluster-based statistics, the manuscript should clearly specify both the cluster-forming threshold and the significance threshold used for comparing true cluster masses to the shuffled distribution.</p><p>Conclusion</p><p>Overall, the results support the study's conclusions. This work addresses a timely question in object categorization and predictive coding-specifically, how feedback signals vary in content and timing across cortical layers.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103788.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This manuscript reports high-resolution functional MRI data and MEG data revealing additional mechanistic information about an established paradigm studying how foveal regions of primary visual cortex (V1) are involved in processing peripheral visual stimuli. Because of the retinotopic organization of V1, peripheral stimuli should not evoke responses in the regions of V1 that represent stimuli in the center of the visual field (the fovea). However, functional MRI responses in foveal regions do reflect the characteristics of peripheral visual stimuli - this is a surprising finding first reported in 2008. The present study uses fMRI data with sub-millimeter resolution to study the how responses at different depths in the foveal gray matter do or don't reflect peripheral object characteristics during 2 different tasks: one in which observers needed to make detailed judgments about object identity, and one in which observers needed to make more coarse judgments about object orientation. FMRI results reveal interesting and informative patterns in these two conditions. A follow-on MEG study yields information about the timing of these responses. Put together, the findings settle some questions in the field and add new information about the nature of visual feedback to V1.</p><p>Strengths:</p><p>(1) Rigorous and appropriate use of &quot;laminar fMRI&quot; techniques.</p><p>(2) The introduction does an excellent job of contextualizing the work.</p><p>(3) Control experiments and analyses are designed and implemented well</p><p>Weaknesses:</p><p>(1) The use of the term &quot;low order&quot; to describe object orientation is potentially confusing. During review, the authors considered this issue and responded that they would continue with the use of the term low-order to describe object orientation because a low-pass spatial frequency filter would provide object orientation information. This is certainly a reasonable perspective; nonetheless, this reviewer thinks spatial frequencies that low are not readily represented by neurons in early visual cortex and it is common to use &quot;low-order&quot; to refer to features extracted in early visual areas, so I think this causes confusion.</p><p>(2) The methods contain a nice description of the methods for &quot;correcting the vascular-related signals&quot;. I'm guessing this is the method that removed, e.g., 22% of foveal voxels (previous paragraph), but it's not entirely clear whether the voxel selection methods described in the &quot;correcting the vascular-related signals&quot; are describing the same processing step referred to in the previous paragraph as &quot;a portion of voxels was removed based on large vein distribution&quot;.</p><p>(3) It is quite difficult to perform laminar analyses across multiple visual areas because distortion compensation is not perfect and registration of functional to anatomical data will always be a bit better in some places and a bit worse in others. An ideal manuscript would include some images showing registration quality in V1, LOC, and IPS regions for a few different participants, or include some kind of quality metric indicating the confidence in depth assignments in different regions.</p><p>(4) For the decoding analysis, it would be helpful to have more information about how samples were defined for each condition -- were the beta values for entire blocks used as samples for each condition, or were separate timepoints during a block used in the SVM as repeated samples for each condition?</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103788.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hou</surname><given-names>Wenhao</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Biophysics, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Sheng</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Biophysics, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jiedong</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>(1.1) The authors argue that low-level features in a feedback format could be decoded only from deep layers of V1 (and not superficial layers) during a perceptual categorization task. However, previous studies (Bergman et al., 2024; Iamshchinina et al., 2021) demonstrated that low-level features in the form of feedback can be decoded from both superficial and deep layers. While this result could be due to perceptual task or highly predictable orientation feature (orientation was kept the same throughout the experimental block), an alternative explanation is a weaker representation of orientation in the feedback (even before splitting by layers there is only a trend towards significance; also granger causality for orientation information in MEG part is lower than that for category in peripheral categorization task), because it is orthogonal to the task demand. It would be helpful if the authors added a statistical comparison of the strength of category and orientation representations in each layer and across the layers.</p></disp-quote><p>We agree that the strength of feedback information is related to task demand. Specifically, we would like to highlight the relationship between task demand and feedback information in the superficial layer. Previous studies have shown that foveal feedback information is observed only when the task requires the identity information of the peripheral objects (Williams et al., 2008; Fan et al., 2016; Yu and Shim, 2016). In this study, we found that the deep layer represented both orientation and categorical feedback information, while the superficial layer only represented categorical information. This suggests that feedback information in the superficial layer may be related to (or enhanced by) the task demands. In other words, if the experimental design required participants to discriminate orientation rather than object identity, we would expect stronger orientation information in foveal V1 and significant decoding performance of orientation feedback information in the superficial layer of foveal V1. This assumption is consistent with the anatomical connections of the superficial layer, which not only receives feedback connections but also sends outputs to higher-level regions for further processing. This is also consistent with Iamshchinina et al.’s observation that, when orientation information had to be mentally rotated and reported (i.e., task-relevant), it was observed in both the superficial and deep layers of V1. Bergmann et al. observed illusory color information in the superficial layer of V1, which may reflect a combination of lateral propagation and feedback mechanisms in the superficial layer that support visual filling-in phenomena. We have revised the discussion in the manuscript: In other words, if the experimental design required participants to discriminate orientation rather than object identity, we would expect stronger orientation information in foveal V1 and significant decoding performance of orientation feedback information in the superficial layer of foveal V1. Recent studies (Iamshchinina et al., 2021; Bergman et al., 2024) have also highlighted the relationship between feedback information and neural representations in V1 superficial layer.</p><p>To further demonstrate the laminar profiles of low- and high-order information, we have re-analyzed the data and added more fine-scale laminar profiles with statistical comparisons in the revised manuscript. The results again showed significant neural decoding performances in the deep layer of both category and orientation information, and only significant decoding performances of category information in the superficial layer.</p><disp-quote content-type="editor-comment"><p>(1.2) The authors argue that category feedback is not driven by low-level confounding features embedded in the stimuli. They demonstrate the ability to decode orientations, particularly well represented by V1, in the absence of category discrimination. However, the orientation is not a category-discriminating feature in this task. It could be that the category-discriminating features cannot be as well decoded from V1 activity patterns as orientations. Also, there are a number of these category discriminating features and it is unclear if it is a variation in their representational strength or merely the absence of the task-driven enhancement that preempts category decoding in V1 during the foveal task. In other words, I am not sure whether, if orientation was a category-specific feature (sharpies are always horizontal and smoothies are vertical), there would still be no category decoding.</p></disp-quote><p>The low-order features mentioned in the manuscript refer to visual information encoded intrinsically in V1, independent of task demands. In the foveal experiment, the task is to discriminate the color of fixation, which is unrelated to the category or orientation of the object stimuli. The results showed that only orientation information could be decoded from foveal V1. This indicates that low-order information, such as orientation, is strongly and automatically encoded in V1, even when it is irrelevant to the task. Meanwhile, category information could not be decoded, indicating that category information relies on feedback signals driven by attention or the task to the objects, both of which are absent in the fixation task. Other evidence indicates that category feedback is not driven by low-level features intrinsically encoded in V1. First, the laminar profiles of these two types of feedback information differ considerably (see response to 1.1). Second, only category feedback information was correlated with behavioral performance (MEG experiment). These findings demonstrate that category feedback information is task-driven and differs from the automatically encoded low-order information in foveal V1. The reviewer expressed some uncertainty that, whether “if orientation was a category-specific feature (sharpies are always horizontal and smoothies are vertical), there would still be no category decoding”. Our data showed that orientation could be automatically decoded in V1, regardless of task demand. Thus, if orientation was a category-specific feature in the foveal task (i.e., sharpies are always horizontal and smoothies are always vertical), category decoding would be successful in V1. However, in this scenario, the orientation and other shape features are not independent, thus preventing us to find out whether non-orientation shape features could be decoded in V1.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>(2.1) While not necessarily a weakness, I do not fully agree with the description of the 2 kinds of feedback information as &quot;low-order&quot; and &quot;high-order&quot;. I understand the motivation to do this - orientation is typically considered a low-level visual feature. But when it's the orientation of an entire object, not a single edge, orientation can only be defined after the elements of the object are grouped. Also, the discrimination between spikies and smoothies requires detecting the orientations of particular edges that form the identifying features. To my mind, it would make more sense to refer to discrimination of object orientation as &quot;coarse&quot; feature discrimination, and orientation of object identity as &quot;fine&quot; feature discrimination. Thus, the sentence on line 83, for example, would read &quot;Interestingly, feedback with fine and coarse feature information exhibits different laminar profiles.&quot;.</p></disp-quote><p>We agree that the object orientation (invariant to object category or identity) is defined on a larger spatial scale than the local orientation features such as local edges, however, in this sense, the object orientation is a coarse feature. In contrast, the category-defining information is mainly contributed by the local shape information (i.e., little cubes vs. bumps), which is more fine-scale information. One way to look at this difference is that the object orientation information is mainly carried by low-spatial frequency information and will survive low-pass filtering, hence “coarse”; while the object category information would largely be lost if the objects underwent low-pass spatial filtering.</p><p>We believe the labeling words “low-order” and “high-order” are consistent with the typical use of these terms in the literature, referring to features intrinsically encoded in early visual cortex vs. in high level object sensitive cortical regions. The more important aspects of our results are in their differential engagement in feedforward vs. feedback processing, with low-order features automatically represented in the early visual cortex during feedforward processing while high-order features represented due to feedback processing. Results from the foveal fMRI experiment (Exp. 2) strongly support this assumption that, when objects were presented at the fovea and the task was a fixation color task irrelevant to object information, foveal V1 could only represent orientation information, not category information. Notably, there was a dramatic difference in decoding performance in foveal V1 between Exp.1 and Exp.2, which ruled out the argument that both orientation and category information were driven by local edge information represented in V1.</p><disp-quote content-type="editor-comment"><p>(2.2) Figure 2 and text on lines 185, and 186: it is difficult to interpret/understand the findings in foveal ROIs for the foveal control task without knowing how big the ROI was. Foveal regions of V1 are grossly expanded by cortical magnification, such that the central half-degree can occupy several centimeters across the cortical surface. Without information on the spatial extent of the foveal ROI compared to the object size, we can't know whether the ROI included voxels whose population receptive fields were expected to include the edges of the objects.</p></disp-quote><p>The ROI of foveal V1 was defined using data from independent localizer runs. In each localizer run, flashing checkerboards of the same size as the objects in the task runs were presented at the fovea or in the periphery. The ROI of foveal V1 was identified as the voxels responsive to the foveal checkerboards. In other words, The ROI of foveal V1 included the voxels whose population receptive fields covered the entire object in the foveal visual field.</p><p>We included a figure in the revised manuscript comparing the activation maps induced by the foveal object stimulus in the task runs with the ROI coverage defined by the localizer runs.</p><disp-quote content-type="editor-comment"><p>(2.3) Line 143 and ROI section of the methods: in order for the reader to understand how robust the responses and analyses are, voxel counts should be provided for the ROIs that were defined, as well as for the number (fraction) of voxels excluded due to either high beta weights or low signal intensity (lines 505-511).</p></disp-quote><p>In the revised manuscript, we have included the number of voxels in each ROI and the criteria for voxel selection:</p><p>For each ROI, the number of voxels depended on the size of the activated region, as estimated from the localizer data. The numbers are as follows: foveal V1, 2185 ± 389; peripheral V1, 1294± 215; LOC, 3451 ± 863; and pIPS, 5154 ± 1517. To avoid the signals of large vessels, a portion of voxels was removed based on the distribution of large vessels: V1 foveal, 22.5% ± 6.6%; V1 peripheral, 6.8% ± 3.9%; LOC, 16.1% ± 8.1% ; and pIPS, 5.1% ± 3.2%. For the decoding analysis, the top 500 responsive voxels in each ROI were selected to balance the voxel numbers across different ROIs for training and testing the decoder.</p><disp-quote content-type="editor-comment"><p>(2.4) I wasn't able to find mention of how multiple-comparisons corrections were performed for either the MEG or fMRI data (except for one Holm-Bonferonni correction in Figure S1), so it's unclear whether the reported p-values are corrected.</p></disp-quote><p>For the fMRI results, there is strong evidence showing that feedback information is sent to the foveal V1 during a peripheral object task (Williams et al., 2008; Fan et al., 2016; Yu and Shim, 2016). In addition, anatomical and functional evidence shows that the superficial and deep layers of V1 receive feedback information during visual processing. Therefore, in the current study, we specifically examined two types of feedback information in the superficial and deep layers of foveal V1, and did not apply multiple-comparison correction to the decoding results.</p><p>Regarding the MEG results, since we did not have a strong prior about when feedback information would arrive in the foveal V1, a cluster-based permutation method was used to correct for multiple comparisons in each time course. Specifically, for each time point, the sign of the effect for each participant was randomly flipped 50000 times to obtain the null hypothesis distribution for each time point. Clusters were defined as continuous significant time points in the real and flipped time series, and the effects in each cluster were summed to create a cluster-based effect. The most significant cluster-based effect in each flipped time series was then used to generate the corrected null hypothesis distribution.</p><p>We included these clarifications in Significance testing part of the revised manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>It would be helpful if the authors could elaborate more on the fMRI decoding results in higher-order visual areas in the Discussion (there are recent studies also investigating higher-order visual areas (Carricarte et al., 2024) and associative areas (Degutis et al., 2024)) and relate it to the MEG information transmission results between the areas overlapping with the regions recorded in the fMRI part of the study.</p></disp-quote><p>We have discussed the fMRI decoding results in the LOC and IPS in the revised manuscript:</p><p>In the current study, fMRI signals from early visual cortex and two high-level brain regions (LOC and pIPS) were recorded. Neural dynamics of these regions were extracted from MEG signals. Decoding analyses based on fMRI and MEG signals consistently showed that object category information could be decoded from both regions. These findings raise an important question: Further Granger causality analysis indicates that the feedback information in foveal V1 was mainly driven by signals from the LOC. Layer-specific analysis showed that category information could be decoded in the middle and superficial layers of the LOC. A reasonable interpretation of this result is that feedforward information from the early visual cortex was received by the LOC’s middle layer, then the category information was generated and fed back to foveal V1 through the LOC’s superficial layer. A recent study (Carricarte et al., 2024) found that, in object selective regions in temporal cortex, the deep layer showed the strongest fMRI responses during an imagery task. Together, the results suggest that the deep and superficial layers correspond to different feedback mechanisms. It is worth noting that other cortical regions may also generate feedback signals to the early visual cortex. The current study did not have simultaneously recorded fMRI signals from the prefrontal cortex, but it has been shown that feedback signals can be traced back to the prefrontal cortex during complex cognitive tasks, such as working memory (Finn et al., 2019; Degutis et al., 2024). Further fMRI studies with submillimeter resolution and whole-brain coverage are needed to test other potential feedback pathways during object processing.</p><disp-quote content-type="editor-comment"><p>The behavioral performance seems quite low (67%), could authors explain the reasons for it?</p></disp-quote><p>We designed the object stimuli to be difficult to distinguish on purpose. Some of our pilot data showed that the more involved the participants were in the peripheral object task, the easier the foveal feedback information was to decoded. It is reasonable to assume that if the peripheral objects were easily distinguishable, the feedback mechanism may not be fully recruited during object processing. Furthermore, since we were decoding category and orientation information rather than identity information, the difficulty of distinguishing two objects from the same category and with the same orientation would not affect the decoding of category and orientation information in the neural signals.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) Line 52: the meaning of the sentence starting with &quot;However, ...&quot; is not entirely clear. Maybe the word &quot;while&quot; is missing after the first comma?</p><p>(2) Line 224. If I'm understanding the rationale for the MEG analysis correctly, it was not possible to localize foveal regions, but the cross-location decoding analysis was used to approximate the strength and timing of feedback information. If this is the case, &quot;neural representations in the foveal region&quot; were not extracted.</p><p>(3) Figure 4. The key information is too small to see. The lines indicating where decoding performance was significant are quite thin but very important, and the text next to them indicating onset times of significant decoding is in such a small font size I needed to zoom in to 300% to read it (yes, my eyes are getting old and tired). Increasing the font size used to represent key information would be nice.</p><p>(4) Figure 4 caption. Line 270 describes the line color in the plots as yellow, but that color is decidedly orange to my eye.</p><p>(5) Line 340/341: Papers that define and describe feedback-receptive fields seem important to cite here:</p><p>Keller, A. J., Roth, M. M., &amp; Scanziani, M. (2020). Feedback generates a second receptive field in neurons of the visual cortex. Nature, 582(7813), 545-549.</p><p>Kirchberger, L., Mukherjee, S., Self, M. W., &amp; Roelfsema, P. R. (2023). Contextual drive of neuronal responses in mouse V1 in the absence of feedforward input. Science advances, 9(3), eadd2498.</p><p>(6) Lines 346-350: this sentence seems to have some missing or misused words, because the syntax isn't intact.</p><p>(7) Line 367: supports should be support.</p></disp-quote><p>We thank the reviewers for the comments and have corrected them in the manuscript.</p></body></sub-article></article>