<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">92119</article-id><article-id pub-id-type="doi">10.7554/eLife.92119</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92119.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Movies reveal the fine-grained organization of infant visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Ellis</surname><given-names>Cameron T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3254-5940</contrib-id><email>cte@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Yates</surname><given-names>Tristan S</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Arcaro</surname><given-names>Michael J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4612-9921</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7519-3001</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Palo Alto</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Psychology, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>Department of Psychology, University of Pennsylvania</institution></institution-wrap><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Psychology, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Wu Tsai Institute, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Dubois</surname><given-names>Jessica</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f82e368</institution-id><institution>Inserm Unité NeuroDiderot, Université Paris Cité</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0336mm561</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>03</month><year>2025</year></pub-date><volume>12</volume><elocation-id>RP92119</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-14"><day>14</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-23"><day>23</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.22.554318"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-19"><day>19</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92119.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-25"><day>25</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92119.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-07"><day>07</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92119.3"/></event></pub-history><permissions><copyright-statement>© 2023, Ellis et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ellis et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-92119-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-92119-figures-v1.pdf"/><abstract><p>Studying infant minds with movies is a promising way to increase engagement relative to traditional tasks. However, the spatial specificity and functional significance of movie-evoked activity in infants remains unclear. Here, we investigated what movies can reveal about the organization of the infant visual system. We collected fMRI data from 15 awake infants and toddlers aged 5–23 months who attentively watched a movie. The activity evoked by the movie reflected the functional profile of visual areas. Namely, homotopic areas from the two hemispheres responded similarly to the movie, whereas distinct areas responded dissimilarly, especially across dorsal and ventral visual cortex. Moreover, visual maps that typically require time-intensive and complicated retinotopic mapping could be predicted, albeit imprecisely, from movie-evoked activity in both data-driven analyses (i.e. independent component analysis) at the individual level and by using functional alignment into a common low-dimensional embedding to generalize across participants. These results suggest that the infant visual system is already structured to process dynamic, naturalistic information and that fine-grained cortical organization can be discovered from movie data.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>How babies see the world is a mystery. They cannot share their experiences, and adults cannot recall this time. Clever experimental methods are needed to understand sensory processing in babies' brains and how variations from adults could cause them to have different experiences.</p><p>However, finding ways to study infant brain structure and function has challenged scientists. Babies cannot complete many cognitive tasks used to assess adult brain activity. It can also be difficult to use imaging tools like magnetic resonance imaging (MRI) that require individuals to lay still for extended periods, which can be challenging for infants who are often wiggly and have short attention spans. As a result, many questions remain unanswered about infant brain organization and function.</p><p>Recent technological advances have made it easier to study infant brain activity. Scientists have developed approaches allowing infants to watch a movie while being comfortably positioned in an MRI machine. Infants and toddlers will often happily watch a film for minutes at a time, enabling scientists to observe how their brains respond to what they see on the screen.</p><p>Ellis et al. used this approach to assess the organization of the visual system in the brains of 15 infants while they watched movies during functional MRI. The researchers compared the infant scans with scans of adult brains who watched the same film, which revealed that babies’ brain activity is surprisingly structured and similar to that of adults. Moreover, the organization of the adult brain could predict the organization of the infant brain.</p><p>Ellis et al. show that scanning infants while they watch movies can be a valuable way to study their brain activity. The experiments reveal important similarities in adult and infant visual processing, helping to identify the foundation on which visual development rests. The movie-watching experiments may also provide a model for scientists to study other types of infant perception and cognition. Movies can help scientists compare brain activity in typically developing infants to those with neurodevelopmental conditions, which could one day help clinicians create new avenues for diagnosis or treatment.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>naturalistic tasks</kwd><kwd>child development</kwd><kwd>fMRI</kwd><kwd>ventral and dorsal visual streams</kwd><kwd>functional alignment</kwd><kwd>retinotopic mapping</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007631</institution-id><institution>Canadian Institute for Advanced Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.37717/2020-1208</award-id><principal-award-recipient><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The visual system of infants has adult-like properties, and these properties can be revealed at an individual level by having infants watch movies during functional magnetic resonance imaging.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Studying the function and organization of the youngest human brains remains a challenge. Despite the recent growth in infant fMRI (<xref ref-type="bibr" rid="bib6">Biagi et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Biagi et al., 2023</xref>; <xref ref-type="bibr" rid="bib12">Cabral et al., 2022</xref>; <xref ref-type="bibr" rid="bib16">Deen et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Kosakowski et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">Truzzi and Cusack, 2023</xref>), one of the most important obstacles facing this research is that infants are unable to maintain focus for long periods of time and struggle to complete traditional cognitive tasks (<xref ref-type="bibr" rid="bib19">Ellis et al., 2020a</xref>). Movies can be a useful tool for studying the developing mind (<xref ref-type="bibr" rid="bib59">Vanderwal et al., 2019</xref>), as has been shown in older children (<xref ref-type="bibr" rid="bib58">Vanderwal et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Richardson et al., 2018</xref>; <xref ref-type="bibr" rid="bib1">Alexander et al., 2017</xref>). The dynamic, continuous, and content-rich nature of movie stimuli (<xref ref-type="bibr" rid="bib45">Nastase et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Finn et al., 2022</xref>) make them effective at capturing infant attention (<xref ref-type="bibr" rid="bib27">Franchak et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">Tran et al., 2017</xref>). Here, we examine what can be revealed about the functional organization of the infant brain during movie-watching.</p><p>We focus on visual cortex because its organization at multiple spatial scales is well understood from traditional, task-based fMRI. The mammalian visual cortex is divided into multiple areas with partially distinct functional roles between areas (<xref ref-type="bibr" rid="bib9">Brodmann, 1909</xref>; <xref ref-type="bibr" rid="bib23">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib57">Ungerleider and Mishkin, 1982</xref>). Within visual areas, there are orderly, topographic representations, or maps, of visual space (<xref ref-type="bibr" rid="bib34">Kaas, 1997</xref>; <xref ref-type="bibr" rid="bib65">White and Fitzpatrick, 2007</xref>). These maps capture information about the location and spatial extent of visual stimuli with respect to fixation. Thus, maps reflect sensitivity to polar angle, measured via alternations between horizontal and vertical meridians that define area boundaries (<xref ref-type="bibr" rid="bib26">Fox et al., 1987</xref>; <xref ref-type="bibr" rid="bib48">Schneider et al., 1993</xref>), and sensitivity to spatial frequency, reflected in gradients of sensitivity to high and low spatial frequencies from foveal to peripheral vision, respectively (<xref ref-type="bibr" rid="bib33">Henriksson et al., 2008</xref>). Previously, we reported that these maps could be revealed by a retinotopy task in infants as young as 5 months of age (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>). However, it remains unclear whether these maps are evoked by more naturalistic task designs.</p><p>The primary goal of the current study is to investigate whether movie-watching data recapitulates the organization of visual cortex. Movies drive strong and naturalistic responses in sensory regions while minimizing task demands (<xref ref-type="bibr" rid="bib41">Loiotile et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Nastase et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Finn et al., 2022</xref>) and thus are a proxy for typical experience. In adults, movies and resting-state data have been used to characterize the visual cortex in a data-driven fashion (<xref ref-type="bibr" rid="bib36">Knapen, 2021</xref>; <xref ref-type="bibr" rid="bib42">Lu et al., 2017</xref>; <xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>). Movies have been useful in awake infant fMRI for studying event segmentation (<xref ref-type="bibr" rid="bib67">Yates et al., 2022</xref>), functional alignment (<xref ref-type="bibr" rid="bib56">Turek et al., 2018</xref>), and brain networks (<xref ref-type="bibr" rid="bib68">Yates et al., 2023</xref>). However, this past work did not address the granularity and specificity of cortical organization that movies evoke. For example, movies evoke similar activity across infants in anatomically aligned visual areas (<xref ref-type="bibr" rid="bib67">Yates et al., 2022</xref>), but it remains unclear whether responses to movie content differ <italic>between</italic> visual areas (e.g. is there more similarity of function within visual areas than between [<xref ref-type="bibr" rid="bib40">Li et al., 2022</xref>]). Moreover, it is unknown whether structure <italic>within</italic> visual areas, namely visual maps, contributes substantially to visual-evoked activity. Additionally, we wish to test whether methods for functional alignment can be used with infants. Functional alignment finds a mapping between participants using functional activity – rather than anatomy – and in adults can improve signal-to-noise, enhance across participant prediction, and enable unique analyses (<xref ref-type="bibr" rid="bib10">Busch et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Kumar et al., 2020b</xref>).</p><p>Nonetheless, there are several reasons for skepticism that movies could evoke detailed, retinotopic organization: Movies may not fully sample the stimulus parameters (e.g. spatial frequencies) or visual functions needed to find topographic maps and areas in visual cortex. Even if movies contain the necessary visual properties, they may unfold at a faster rate than can be detected by fMRI. Additionally, naturalistic stimuli may not drive visual responses as robustly as experimenter-defined stimuli that are designed for retinotopic mapping with discrete onsets and high contrast. Finally, the complexity of movie stimuli may result in variable attention between participants, impeding discovery of reliable visual structure across individuals. If movies do show the fine-grained organization of the infant visual cortex, this suggests that this structure (e.g. visual maps) scaffolds the processing of ongoing visual information.</p><p>We conducted several analyses to probe different kinds of visual granularity in infant movie-watching fMRI data. First, we asked whether distinct areas of the infant visual cortex have different functional profiles. Second, we asked whether the topographic organization of visual areas can be recovered within participants. Third, we asked whether this within-area organization is aligned across participants. These three analyses assess key indicators of the mature visual system: functional specialization between areas, organization within areas, and consistency between individuals.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We performed fMRI in awake, behaving infants and toddlers using a protocol described previously (<xref ref-type="bibr" rid="bib19">Ellis et al., 2020a</xref>). The dataset consisted of 15 sessions of infant participants (4.8–23.1 months of age) who had both movie-watching data and retinotopic mapping data collected in the same session (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). All available movies from each session were included (<xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>), with an average duration of 540.7 s (range: 186–1116 s).</p><p>The retinotopic-mapping data from the same infants (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>) allowed us to generate infant-specific meridian maps (horizontal versus vertical stimulation) and spatial frequency maps (high versus low stimulation). The meridian maps were used to define regions of interest (ROIs) for visual areas V1, V2, V3, V4, and V3A/B.</p><p>As a proof of concept that the analyses we use with infants can identify fine-grained visual organization, we ran the main analyses on an adult sample. These adults (8 participants) had both retinotopic mapping data and movie-watching data. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> demonstrate that applying these analyses to adult movie data reveals similar structure to what we find in infants.</p><sec id="s2-1"><title>Evidence of area organization with homotopic similarity</title><p>To determine what movies can reveal about the organization of areas in visual cortex, we compared activity across left and right hemispheres. Although these analyses cannot define visual maps, they test whether visual areas have different functional signatures. Namely, we correlated time courses of movie-related BOLD activity between retinotopically defined, participant-specific ROIs (7.3 regions per participant per hemisphere, range: 6–8) (<xref ref-type="bibr" rid="bib3">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib11">Butt et al., 2015</xref>; <xref ref-type="bibr" rid="bib40">Li et al., 2022</xref>). Higher correlations between the same (i.e. homotopic) areas than different areas indicate differentiation of function between areas. Moreover, other than V1, homotopic visual areas are anatomically separated across the hemispheres, so similar responses are unlikely to be attributable to spatial autocorrelation.</p><p>Homotopic areas (e.g. left ventral V1 and right ventral V1; diagonal of <xref ref-type="fig" rid="fig1">Figure 1A</xref>) were highly correlated (mean [M]=0.88, range of area means: 0.85–0.90), and more correlated than non-homotopic areas, such as the same visual area across streams (e.g. left ventral V1 and right dorsal V1; <xref ref-type="fig" rid="fig1">Figure 1B</xref>; Δ<sub><italic>Fisher Z</italic></sub> M=0.42, p&lt;0.001). To clarify, we use the term ‘stream’ to liberally distinguish visual regions that are <italic>more</italic> dorsal or <italic>more</italic> ventral, as opposed to the functional definition used in reference to the ‘what’ and ‘where’ streams (<xref ref-type="bibr" rid="bib57">Ungerleider and Mishkin, 1982</xref>). We found no evidence that the variability in movie duration per participant correlated with this difference (r=0.08, p=0.700). Within stream (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), homotopic areas were more correlated than adjacent areas in the visual hierarchy (e.g. left ventral V1 and right ventral V2; Δ<sub><italic>Fisher Z</italic></sub> M=0.09, p&lt;0.001), and adjacent areas were more correlated than distal areas (e.g. left ventral V1 and right ventral V4; Δ<sub><italic>Fisher Z</italic></sub> M=0.20, p&lt;0.001). There was no correlation between movie duration and effect (Same&gt;Adjacent: r=−0.01, p=0.965, Adjacent&gt;Distal: r=−0.09, p=0.740). Additionally, if we control for motion in the correlation between areas – in case motion transients drive consistent activity across areas – then the effects described here are negligibly different (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Hence, movies elicit distinct processing dynamics across areas of infant visual cortex defined independently using retinotopic mapping.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Homotopic correlations between retinotopic areas.</title><p>(<bold>A</bold>) Average correlation of the time course of activity evoked during movie-watching for all areas. This is done for the left and right hemisphere separately, creating a matrix that is not diagonally symmetric. The color triangles overlaid on the corners of the matrix cells indicate which cells contributed to the summary data of different comparisons in subpanels B and C. (<bold>B</bold>) Across-hemisphere similarity of the same visual area from the same stream (e.g. left ventral V1 and right ventral V1) and from different streams (e.g. left ventral V1 and right dorsal V1). (<bold>C</bold>) Across-hemisphere similarity in the same stream when matching the same area (e.g. left ventral V1 and right ventral V1), matching to an adjacent area (e.g. left ventral V1 and right ventral V2), or matching to a distal area (e.g. left ventral V1 and right ventral V4). Gray lines represent individual participants. ***=p&lt;0.001 from bootstrap resampling.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Homotopic correlations between retinotopic areas in the adult sample, akin to <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title><p>(<bold>A</bold>) Average correlation of the time course of activity evoked during movie-watching for all areas. Correlation of homotopic areas: M=0.83 (range: 0.78–0.88). (<bold>B</bold>) Across-hemisphere similarity of the same visual area from the same stream and from different streams. Difference with bootstrap resampling: Δ<sub><italic>Fisher Z</italic></sub> M=0.24, p&lt;0.001. (<bold>C</bold>) Across-hemisphere similarity in the same stream when matching the same area, matching to an adjacent area, or matching to a distal area. Difference with bootstrap resampling: Same&gt;Adjacent Δ<sub><italic>Fisher Z</italic></sub> M=0.10, p&lt;0.001; Adjacent&gt;Distal Δ<sub><italic>Fisher Z</italic></sub> M=0.16, p&lt;0.001. Gray lines represent individual participants. ***=p&lt;0.001 from bootstrap resampling.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Homotopic correlations when controlling for motion.</title><p>In this analysis, we computed correlations for all pairwise comparisons while partialing out our metric of motion: framewise displacement. In other words, if the functional time course in an area was correlated with the motion metric then this would decrease the correlation between that area and others. Subfigures A and B use task-evoked retinotopic definitions of areas (akin to <xref ref-type="fig" rid="fig1">Figure 1</xref>), whereas subfigure C uses anatomical definitions of areas (akin to <xref ref-type="fig" rid="fig2">Figure 2</xref>). Overall the results are qualitatively similar, suggesting that motion does not explain the effect observed here. (<bold>A</bold>) Correlation of the same area and same stream (e.g. left ventral V1 and right ventral V1) versus the same area and different stream (e.g. left ventral V1 and right dorsal V1). Difference with bootstrap resampling: Δ<sub><italic>Fisher Z</italic></sub> M=0.43, p&lt;0.001. (<bold>B</bold>) Correlation within the same stream between the same areas, adjacent areas (e.g. left ventral V1 and right ventral V2), or distal areas (e.g. left ventral V1 and right ventral hV4). Difference with bootstrap resampling: Same&gt;Adjacent Δ<sub><italic>Fisher Z</italic></sub> M=0.09, p&lt;0.001; Adjacent&gt;Distal Δ<sub><italic>Fisher Z</italic></sub> M=0.20, p&lt;0.001. Gray lines represent individual participants. ***=p&lt;0.001 from bootstrap resampling. (<bold>C</bold>) Multi-dimensional scaling of the partial correlation between all anatomically defined areas. The time course of functional activity for each area was extracted and correlated across hemispheres, while partialing out framewise displacement. This matrix was averaged across participants and used to create a Euclidean dissimilarity matrix. MDS captured the structure of this matrix in two dimensions with suitably low stress (0.089). The plot shows a projection that emphasizes the similarity to the brain’s organization.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Homotopic correlations between anatomically defined areas corresponding to the data used in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>(<bold>A</bold>) Average correlation of the time course of activity evoked during movie-watching for ventral and dorsal areas in an anatomical segmentation (<xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref>). This is done for the left and right hemispheres separately, which is why the matrix is not diagonally symmetric. The triangles overlaid on the matrix corner highlight the area-wise comparisons used in B and C. Only areas that we were able to retinotopically map (i.e. those that overlap with <xref ref-type="fig" rid="fig1">Figure 1</xref>) were used for this analysis. (<bold>B</bold>) Correlation of the same area and same stream (e.g. left ventral V1 and right ventral V1) versus the same area and different stream (e.g. left ventral V1 and right dorsal V1). Difference with bootstrap resampling: Δ<sub><italic>Fisher Z</italic></sub> M=0.37, p&lt;0.001. (<bold>C</bold>) Correlation within the same stream between the same areas, adjacent areas (e.g. left ventral V1 and right ventral V2), or distal areas (e.g. left ventral V1 and right ventral hV4). Difference with bootstrap resampling: Same&gt;Adjacent Δ<sub><italic>Fisher Z</italic></sub> M=0.09, p&lt;0.001; Adjacent&gt;Distal Δ<sub><italic>Fisher Z</italic></sub> M=0.18, p&lt;0.001. Gray lines represent individual participants. ***=p&lt;0.001 from bootstrap resampling.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig1-figsupp3-v1.tif"/></fig></fig-group><p>We previously found (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>) that an anatomical segmentation of visual cortex (<xref ref-type="bibr" rid="bib61">Wang et al., 2015</xref>) could identify these same areas reasonably well. Indeed, the results above were replicated when using visual areas defined anatomically (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). However, a key advantage of anatomical segmentation is that it can define visual areas not mapped by a functional retinotopy task. This could help address limitations of the analyses above, namely that there was a variable number of retinotopic areas identified across infants and these areas covered only part of visually responsive cortex. Focusing on broader areas that include portions of the ventral and dorsal stream in the adult visual cortex (<xref ref-type="bibr" rid="bib57">Ungerleider and Mishkin, 1982</xref>; <xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref>), we tested for functional differentiation of these streams in infants. We applied multi-dimensional scaling (MDS) – a data-driven method for assessing the clustering of data – to the average cross-correlation matrix across participants (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>; <xref ref-type="bibr" rid="bib32">Haak and Beckmann, 2018</xref>; <xref ref-type="bibr" rid="bib3">Arcaro and Livingstone, 2017</xref>). The stress of fitting these data with a two-dimensional MDS was in the acceptable range (0.076). Clear organization was present (<xref ref-type="fig" rid="fig2">Figure 2</xref>): areas in the adult-defined ventral stream (e.g. VO, PHC) differentiated from areas in the adult-defined dorsal stream (e.g. V3A/B). Indeed, we see a slight separation between canonical dorsal areas and the recently defined lateral pathway (<xref ref-type="bibr" rid="bib64">Weiner and Gomez, 2021</xref>) (e.g. LO1, hMT), although more evidence is needed to substantiate this distinction. This separation between streams is striking when considering that it happens <italic>despite</italic> differences in visual field representations across areas: while dorsal V1 and ventral V1 represent the lower and upper visual field, respectively, V3A/B and hV4 both have full visual field maps. These visual field representations can be detected in adults (<xref ref-type="bibr" rid="bib31">Haak et al., 2013</xref>), however, they are often not the primary driver of function (<xref ref-type="bibr" rid="bib32">Haak and Beckmann, 2018</xref>). We see that in infants too: hV4 and V3A/B represent the same visual space yet have distinct functional profiles. Again, this organization cannot be attributed to mere spatial autocorrelation within stream because analyses were conducted across hemispheres (at significant anatomical distance) and this pattern is preserved when accounting for motion (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). These results thus provide evidence of a dissociation in the functional profile of anatomically defined ventral and dorsal streams during infant movie-watching.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Multi-dimensional scaling (MDS) of movie-evoked activity in visual cortex.</title><p>(<bold>A</bold>) Anatomically defined areas <xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref> used for this analysis, separated into dorsal (red) and ventral (blue) visual cortex, overlaid on a flatmap of visual cortex. (<bold>B</bold>) The time course of functional activity for each area was extracted and compared across hemispheres (e.g. left V1 was correlated with right V1). This matrix was averaged across participants and used to create a Euclidean dissimilarity matrix. MDS captured the structure of this matrix in two dimensions with suitably low stress. The plot shows a projection that emphasizes the similarity to the brain’s organization.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Multi-dimensional scaling of movie-evoked activity in adult visual cortex, akin to <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>A two-dimensional embedding had inappropriately high stress – 0.87 – whereas a three-dimensional embedding had appropriate stress: 0.105. This three-dimensional scatter depicts the similarity of the functional time course of areas as a function of Euclidean distance. The plot shows a projection that emphasizes the similarity to the brain’s organization.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig2-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Evidence of within-area organization with independent component analysis</title><p>We next explored whether movies can reveal fine-grained organization <italic>within</italic> visual areas by using independent component analysis (ICA) to propose visual maps in individual infant brains (<xref ref-type="bibr" rid="bib3">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib5">Beckmann et al., 2005</xref>; <xref ref-type="bibr" rid="bib36">Knapen, 2021</xref>; <xref ref-type="bibr" rid="bib42">Lu et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Moeller et al., 2009</xref>). ICA is a method for decomposing a source into constituent signals by finding components that account for independent variance. When applied to fMRI data (using MELODIC in FSL), these components have spatial structure that varies in strength over time. Many of these components reflect noise (e.g. motion, breathing) or task-related signals (e.g. face responses), while other components reflect the functional architecture of the brain (e.g. topographic maps) (<xref ref-type="bibr" rid="bib3">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib5">Beckmann et al., 2005</xref>; <xref ref-type="bibr" rid="bib36">Knapen, 2021</xref>; <xref ref-type="bibr" rid="bib42">Lu et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Moeller et al., 2009</xref>). We visually inspected each component and categorized it as a potential spatial frequency map, a potential meridian map, or neither. This process was blind to the ground truth of what the visual maps look like for that participant from the retinotopic mapping task, simulating what would be possible if retinotopy data from the participants were unavailable. Success in this process requires that (1) retinotopic organization accounts for sufficient variance in visual activity to be identified by ICA and (2) experimenters can accurately identify these components.</p><p>Multiple maps could be identified per participant because there were more than one candidate that the experimenter thought was a suitable map. Across infant participants, we identified an average of 2.4 (range: 0–5) components as potential spatial frequency maps and 1.1 (range: 0–4) components as potential meridian maps. To evaluate the quality of these maps, we compared them to the ground truth of that participant’s task-evoked maps (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Spatial frequency and meridian maps are defined by their systematic gradients of intensity across the cortical surface (<xref ref-type="bibr" rid="bib2">Arcaro et al., 2009</xref>). Lines drawn parallel to area boundaries show monotonic gradients on spatial frequency maps, with stronger responses to high spatial frequency at the fovea, and stronger responses to low spatial frequencies in the periphery (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). By contrast, lines drawn perpendicular to the area boundaries show oscillations in sensitivity to horizontal and vertical meridians on meridian maps (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). Using the same manually traced lines from the retinotopy task, we measured the intensity gradients in each component from the movie-watching data. We can then use the gradients of intensity in the retinotopy task-defined maps as a benchmark for comparison with the ICA-derived maps.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Example retinotopic task versus independent component analysis (ICA)-based spatial frequency maps.</title><p>(<bold>A</bold>) Spatial frequency map of a 17.1-month-old toddler. The retinotopic task data are from a prior study (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>). The view is of the flattened occipital cortex with visual areas traced in black. (<bold>B</bold>) Component captured by ICA of movie data from the same participant. This component was chosen as a spatial frequency map in this participant. The sign of ICA is arbitrary so it was flipped here for visualization. (<bold>C</bold>) Gradients in spatial frequency within-area from the task-evoked map in subpanel A. Lines parallel to the area boundaries (emanating from fovea to periphery) were manually traced and used to capture the changes in response to high versus low spatial frequency stimulation. (<bold>D</bold>) Gradients in the component map. We used the same lines that were manually traced on the task-evoked map to assess the change in the component’s response. We found a monotonic trend within area from medial to lateral, just like we see in the ground truth. This is one example result, find all participants in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig3-v1.tif"/></fig><p>To assess the selected component maps, we correlated the gradients (described above) of the task-evoked and component maps. This test uses independent data: the components were defined based on movie data and validated against task-evoked retinotopic maps. <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows the absolute correlations between the task-evoked maps and the manually identified spatial frequency components (M=0.52, range: 0.23–0.85). To evaluate whether movies are a viable method for defining retinotopic maps, we tested whether the task-evoked retinotopic maps were more similar to manually identified components than other components. We identified the best component in 6 of 13 participants (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The percentile of the average manually identified component was high (M=63.8 percentile, range: 26.7–98.1) and significantly above chance (ΔM=13.8, CI=[3.3–24.0], p=0.011). This illustrates that the manually identified components derived from movie-watching data are similar to the spatial frequency maps derived from retinotopic mapping. The fact that this can work also indicates the underlying architecture of the infant visual system influences how movies are processed.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Similarity between visual maps from the retinotopy task and independent component analysis (ICA) applied to movies.</title><p>(<bold>A</bold>) Absolute correlation between the task-evoked and component spatial frequency maps (absolute values used because sign of ICA maps is arbitrary). Each dot is a manually identified component. At least one component was identified in 13 out of 15 participants. The bar plot is the average across participants. The error bar is the standard error across participants. (<bold>B</bold>) Ranked correlations for the manually identified spatial frequency components relative to all components identified by ICA. Bar plot is same as <bold>A</bold>. (<bold>C</bold>) Same as A but for meridian maps. At least one component was identified in 9 out of 15 participants. (<bold>D</bold>) Same as B but for meridian maps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Similarity between visual maps from the adult retinotopy task and independent component analysis (ICA) applied to movies, akin to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>(<bold>A</bold>) Absolute correlation between the task-evoked and component spatial frequency maps (absolute values used because sign of ICA maps is arbitrary). Each dot is a manually identified component. At least one component was identified in 8 out of 8 adult participants. The bar plot is the average across participants. The error bar is the standard error across participants. (<bold>B</bold>) Ranked correlations for the manually identified spatial frequency components relative to all components identified by ICA. Bar plot is same as A. Percentile tests: M=70.6 percentile, range: 26.6–92.3, ΔM from chance = 20.6, CI=[4.2–34.9], p=0.014. (<bold>C</bold>) Same as A but for meridian maps. At least one component was identified in 6 out of 8 participants. (<bold>D</bold>) Same as B but for meridian maps. Percentile tests: M=74.6 percentile, range: 40.3–98.0, ΔM from chance = 24.6, CI=[8.2–39.6], p=0.004.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Gradients for the task-evoked and independent component analysis (ICA)-based spatial frequency maps.</title><p>The gray lines depict the gradients from each chosen IC map, and their scale is indicated by the Y-axis on the left-hand side. The sign of the maps has not been edited, but it is arbitrary. The black line indicates the gradient from the task-evoked map, and their scale is indicated by the Y-axis on the right-hand side. Participants are listed in order of age. Participant data is not reported if no components were chosen for that participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Gradients for the task-evoked and independent component analysis (ICA)-based meridian maps.</title><p>The gray lines depict the gradients from each chosen IC map, and their scale is indicated by the Y-axis on the left-hand side. The sign of the maps has not been edited, but it is arbitrary. The black line indicates the gradient from the task-evoked map, and their scale is indicated by the Y-axis on the right-hand side. Participants are listed in order of age. Participant data is not reported if no components were chosen for that participant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig4-figsupp3-v1.tif"/></fig></fig-group><p>We performed the same analyses on the meridian maps. As noted above, the lines were now traced perpendicular to the boundaries. <xref ref-type="fig" rid="fig4">Figure 4C</xref> shows the correlation between the task-evoked meridian maps and the manually identified components (M=0.46, range: 0.03–0.81). Compared to all possible components identified by ICA, the best possible component was identified for 1 out of 9 participants (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). Although the percentile of the average manually identified component was numerically high (M=67.6 percentile, range: 3.0–100.0), it was not significantly above chance (ΔM=17.6, CI=[–1.8–33.0], p=0.074). This difference in performance compared to spatial frequency is also evident in the fact that fewer components were identified as potential meridian maps, and that several participants had no such maps. Even so, some participants have components that are highly similar to the meridian maps (e.g. s8037_1_2 or s6687_1_5 in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). Because it is possible, albeit less likely, to identify meridian maps from ICA, the structure may be present in the data but more susceptible to noise or gaze variability. Spatial frequency maps have a coarser structure than meridian maps, and are more invariant to fixation, which may explain why they are easier to identify. Equivalent analyses of adult data (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) support this conclusion: meridian maps are found in fewer adult participants.</p><p>Despite the similarity of the identified components to the retinotopic maps, it is possible that the components are noise and this similarity arose by chance. Indeed, given enough patterns of spatially smooth noise, some will resemble retinotopic organization. To test how often components derived from noise are misidentified, we made a version of each component in which the functional data were misaligned with respect to the anatomical data while preserving spatial smoothness. We then intermixed an equal number of these ‘rolled’ components among the original components and randomized the order such that a coder would be blind as to whether any given component was rolled or original. The blind coder manually categorized each component as a spatial frequency component, a meridian component, or neither (identical to the steps above). It was not possible to make the coder blind for some participants whose rolled data contained visible clues because of partial voluming. In the 6 participants without such clues, only 1 of the 14 components labeled as spatial frequency or meridian, from 920 total components, was a rolled component. The fact that 13 of 14 selected components (93%) were original was extremely unlikely to have occurred by chance (binomial test: p=0.002). Thus, our selection procedure rarely identified components as retinotopic in realistic noise.</p></sec><sec id="s2-3"><title>Evidence of within-area organization with shared response modeling</title><p>Finally, we investigated whether the organization of visual cortex in one infant can be predicted from movie-watching data in <italic>other</italic> participants using functional alignment (<xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>). For such functional alignment to work, stimulus-driven responses to the movie must be shared across participants. These analyses also benefit from greater amounts of data, so we expanded the sample in two ways (<xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>): First, we added 71 movie-watching datasets from additional infants who saw the same movies but did not have usable retinotopy data (and thus were not included in the analyses above that compared movie and retinotopy data within participant). Second, we used data from adult participants, including 8 participants who completed the retinotopy task and saw a subset of the movies we showed infants, and 41 datasets from adults who had seen the movies shown to infants but did not have retinotopy data.</p><p>With this expanded dataset, we used shared response modeling (SRM) (<xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>) to predict visual maps from other participants (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Specifically, we held out one participant for testing purposes and used SRM to learn a low-dimensional, shared feature space from the movie-watching data of the remaining participants in a mask of occipital cortex. This shared space represented the responses to that movie in visual cortex that were shared across participants, agnostic to the precise localization of these responses across voxels in each individual (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The number of features in the shared space (K=10) was determined via a cross-validation procedure on movie-watching data in adults (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). The task-evoked retinotopic maps from all but the held-out participant were transformed into this shared space and averaged, separately for each map type (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). We then mapped the held-out participant’s movie data into the learned shared space without changing the shared space (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). In other words, the shared response model was learned and frozen before the held-out participant’s data was considered. This approach has been used and validated in prior SRM studies (<xref ref-type="bibr" rid="bib66">Yates et al., 2021</xref>). Taking the inverse of the held-out participant’s mapping allowed us to transform the averaged shared space representation of visual maps into the held-out participant’s brain space (<xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Pipeline for predicting visual maps from movie data.</title><p>The figure divides the pipeline into four steps. All participants watched the same movie. To predict infant data from other infants (or adults), one participant was held out of the training and used as the test participant. Step <bold>A</bold>: The training participants’ movie data (three color-coded participants shown in this schematic) is masked to include just occipital voxels. The resulting matrix is run through shared response modeling (SRM) (<xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>) to find a lower-dimensional embedding (i.e. a weight matrix) of their shared response. Step <bold>B</bold>: The training participants’ retinotopic maps are transformed into the shared response space using the weight matrices determined in step A. Step <bold>C</bold>: Once steps A and B are finished, the test participant’s movie data are mapped into the shared space that was fixed from step A. This creates a weight matrix for this test participant. Step <bold>D</bold>: The averaged shared response of the retinotopic maps from step B is combined with the test participant’s weight matrix from step C to make a prediction of the retinotopic map in the test participant. This prediction can then be validated against their real map from the retinotopy task. Individual gradients for each participant are shown in <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplements 3</xref>–<xref ref-type="fig" rid="fig6s5">5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Cross-validation of the number of features in shared response modeling (SRM).</title><p>The movie data from all adult participants (<xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>) was split in half, with a 10 TR buffer between sets. The data were masked only to include occipital lobe voxels. The first half of the movie was used for training the SRM in all but one participant. The number of features learned by the SRM was varied across analyses from 1 to 25. The second half of the movie was then used to generate a shared response (i.e. the activity time course in each feature). To test the SRM, the held-out participant’s first half of data is used to learn a mapping of that participant into the SRM space (this mapping does not change the features learned and is not based on the second half of data). The second half of the held-out participant’s data is then mapped into the shared response space, like the other participants. Time-segment matching was performed on the shared response (<xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib56">Turek et al., 2018</xref>). In brief, time-segment matching tests whether a segment of the data (10 TRs) in the held-out participant can be matched to its correct timepoint based on the other participants. This tests whether the SRM succeeds in making the held-out participant similar to the others. This analysis was performed on each participant and movie separately (each has a line). The dashed line is chance for time-segment matching, averaged across all movies and participants. The black solid line at features = 10 reflects the number of features chosen.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig5-figsupp1-v1.tif"/></fig></fig-group><p>This predicted visual organization was compared to the participant’s actual visual map from the retinotopy task using the same methods as for ICA. In other words, the manually traced lines were used to measure the intensity gradients in the predicted maps, and these gradients were compared to the ground truth. Critically, predicting the retinotopic maps used no retinotopy data from the held-out participant. Moreover, it is completely unconstrained anatomically (except for a liberal occipital lobe mask). Hence, the similarity of the SRM-predicted map to the task-evoked map is due to representations of visual space in other participants being mapped into the shared space.</p><p>We trained SRMs on two populations to predict a held-out infant’s maps: (1) other infants and (2) adults. There may be advantages to either approach: infants are likely more similar to each other than adults in terms of how they respond to the movie; however, their data is more contaminated by motion. When using the infants to predict a held-out infant, the spatial frequency map (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and meridian map (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) predictions are moderately correlated with task-evoked retinotopy data (spatial frequency: M=0.46, range: –0.06 to 0.78; meridian: M=0.24, range: –0.12 to 0.78). Some participants were fit well using SRM (e.g. s2077_1_1, and s6687_1_5 for <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Similarity of shared response modeling (SRM)-predicted maps and task-evoked retinotopic maps.</title><p>Correlation between the gradients of the (<bold>A</bold>) spatial frequency maps and (<bold>C</bold>) meridian maps predicted with SRM from other infants and task-evoked retinotopy maps. (<bold>B, D</bold>) Same as A, except using adult participants to train the SRM and predict maps. Dot color indicates the movie used for fitting the SRM. The end of the line indicates the correlation of the task-evoked retinotopy map and the predicted map when using flipped training data for SRM. Hence, lines extending below the dot indicate that the true performance was higher than a baseline fit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Similarity of shared response modeling (SRM)-predicted maps and task-evoked retinotopic maps in adults, akin to <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title><p>Correlation between the gradients of the (<bold>A</bold>) spatial frequency maps and (<bold>C</bold>) meridian maps predicted with SRM from infants and their task-evoked retinotopy maps. Difference between real and flipped SRM fit: Spatial frequency=Δ<sub><italic>Fisher Z</italic></sub> M=0.59, CI=[0.36–0.83], p&lt;0.001. Meridian=Δ<sub><italic>Fisher Z</italic></sub> M=−0.07, CI=[–0.22–0.10], p=0.382. Note: only two infants were used in the prediction with Child Play (red dots), hence why they likely show erratic behavior. (<bold>B, D</bold>) Same as A, except using adult participants to train the SRM and predict maps. Difference between real and flipped SRM fit: Spatial frequency=Δ<sub><italic>Fisher Z</italic></sub> M=1.05, CI=[0.85–1.22], p&lt;0.001. Meridian=Δ<sub><italic>Fisher Z</italic></sub> M=0.49, CI=[0.36–0.64], p&lt;0.001. Dot color indicates the movie used for fitting the SRM. The end of the line indicates the correlation of the task-evoked retinotopy map and the predicted map when using flipped training data for SRM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Gradients for the spatial frequency maps predicted using shared response modeling (SRM) from other infant participants, compared to the task-evoked gradients.</title><p>The colored lines depict the gradients from each chosen movie that could be used, and their scale is indicated by the Y-axis on the left-hand side. The black line indicates the gradient from the task-evoked map, and their scale is indicated by the Y-axis on the right-hand side. Participants are listed in order of age. Participant data is not reported if the participant did not have SRM-compatible movie data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig6-figsupp2-v1.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Gradients for the meridian maps predicted using shared response modeling (SRM) from other infant participants, compared to the task-evoked gradients.</title><p>The colored lines depict the gradients from each chosen movie that could be used, and their scale is indicated by the Y-axis on the left-hand side. The black line indicates the gradient from the task-evoked map, and their scale is indicated by the Y-axis on the right-hand side. Participants are listed in order of age. Participant data is not reported if the participant did not have SRM-compatible movie data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig6-figsupp3-v1.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 4.</label><caption><title>Gradients for the spatial frequency maps predicted using shared response modeling (SRM) from adult participants, compared to the task-evoked gradients.</title><p>The colored lines depict the gradients from each chosen movie that could be used, and their scale is indicated by the Y-axis on the left-hand side. The black line indicates the gradient from the task-evoked map, and their scale is indicated by the Y-axis on the right-hand side. Participants are listed in order of age. Participant data is not reported if the participant did not have SRM-compatible movie data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig6-figsupp4-v1.tif"/></fig><fig id="fig6s5" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 5.</label><caption><title>Gradients for the meridian maps predicted using shared response modeling (SRM) from adult participants, compared to the task-evoked gradients.</title><p>The colored lines depict the gradients from each chosen movie that could be used, and their scale is indicated by the Y-axis on the left-hand side. The black line indicates the gradient from the task-evoked map, and their scale is indicated by the Y-axis on the right-hand side. Participants are listed in order of age. Participant data is not reported if the participant did not have SRM-compatible movie data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92119-fig6-figsupp5-v1.tif"/></fig></fig-group><p>To evaluate whether success was due to fitting the shared response, we flipped the held-out participant’s movie data (i.e. the first timepoint became the last timepoint and vice versa) so that an appropriate fit is not be learnable. The vertical lines for each movie in <xref ref-type="fig" rid="fig6">Figure 6</xref> indicate the change in performance for this baseline. Indeed, flipping significantly worsened prediction of the spatial frequency map (Δ<sub><italic>Fisher Z</italic></sub> M=0.52, CI=[0.24–0.80], p&lt;0.001) and the meridian map (Δ<sub><italic>Fisher Z</italic></sub> M=0.24, CI=[0.02–0.49], p=0.034). Hence, the movie-evoked response enables the mapping of other infants’ retinotopic maps into a held-out infant.</p><p>Using adult data to predict infant data also results in maps similar to task-evoked spatial frequency maps (<xref ref-type="fig" rid="fig6">Figure 6B</xref>; M=0.56, range: 0.17–0.79) and meridian maps (<xref ref-type="fig" rid="fig6">Figure 6D</xref>; M=0.34, range: –0.27–0.64). Some participants were well predicted by these methods (e.g. s8037_1_2, and s6687_1_4 for <xref ref-type="fig" rid="fig6s4">Figure 6—figure supplement 4</xref>, <xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5</xref>). Again, flipping the held-out participants movie data significantly worsened prediction of the held-out participant’s spatial frequency map (Δ<sub><italic>Fisher Z</italic></sub> M=0.40, CI=[0.17–0.65], p&lt;0.001) and meridian map (Δ<sub><italic>Fisher Z</italic></sub> M=0.33, CI=[0.12–0.55], p=0.002). There was no significant difference in SRM performance when using adults versus infants as the training set (spatial frequency: Δ<sub><italic>Fisher Z</italic></sub> M=0.14, CI=[–0.00–0.27], p=0.054; meridian: Δ<sub><italic>Fisher Z</italic></sub> M=0.11, CI=[–0.05–0.28], p=0.179). In sum, SRM could be used to predict visual maps with moderate accuracy. This indicates that functional alignment methods like SRM can partially capture the retinotopic organization of visual cortex from infant movie-watching data.</p><p>We performed an anatomical alignment analog of the functional alignment (SRM) approach. This analysis serves as a benchmark for predicting visual maps using task-based data, rather than movie data, from other participants. For each infant participant, we aggregated all other infant or adult participants as a reference. The retinotopic maps from these reference participants were anatomically aligned to the standard surface template, and then averaged. These averages served as predictions of the maps in the test participant, akin to SRM, and were analyzed equivalently (i.e. correlating the gradients in the predicted map with the gradients in the task-based map). These correlations (<xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>) are significantly higher than for functional alignment (using infants to predict spatial frequency, anatomical alignment&lt;functional alignment: Δ<sub><italic>Fisher Z</italic></sub> M=0.44, CI=[0.32–0.58], p&lt;0.001; using infants to predict meridians, anatomical alignment&lt;functional alignment: Δ<sub><italic>Fisher Z</italic></sub> M=0.61, CI=[0.47–0.74], p&lt;0.001; using adults to predict spatial frequency, anatomical alignment&lt;functional alignment: Δ<sub><italic>Fisher Z</italic></sub> M=0.31, CI=[0.21–0.42], p&lt;0.001; using adults to predict meridians, anatomical alignment&lt;functional alignment: Δ<sub><italic>Fisher Z</italic></sub> M=0.49, CI=[0.39–0.60], p&lt;0.001). This suggests that even if SRM shows that movies can be used to produce retinotopic maps that are significantly similar to a participant, these maps are not as good as those that can be produced by anatomical alignment of the maps from other participants without any movie data.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We present evidence that movies can reveal the organization of infant visual cortex at different spatial scales. We found that movies evoke differential function across areas, topographic organization of function within areas, and this topographic organization is shared across participants.</p><p>We show that the movie-evoked response in a visual area is more similar to the same area in the other hemisphere than to different areas in the other hemisphere. This suggests that visual areas are functionally differentiated in infancy and that this function is shared across hemispheres (<xref ref-type="bibr" rid="bib40">Li et al., 2022</xref>). By comparing across anatomically distant hemispheres, we reduced the impact of spatial autocorrelation and isolated the stimulus-driven signals in the brain activity (<xref ref-type="bibr" rid="bib3">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib40">Li et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Smyser et al., 2010</xref>). The greater across-hemisphere similarity for same versus different areas provides some of the first evidence that visual areas and streams are functionally differentiated in infants as young as 5 months of age. Previous work suggests that functions of the dorsal and ventral streams are detectable in young infants (<xref ref-type="bibr" rid="bib63">Wattam-Bell et al., 2010</xref>) but that the localization of these functions is immature (<xref ref-type="bibr" rid="bib8">Braddick and Atkinson, 2011</xref>). Despite this, we find that the areas of infant visual cortex that will mature into the dorsal and ventral streams have distinct activity profiles during movie-watching.</p><p>Not only do movies evoke differentiated activity in the infant visual cortex between areas, but movies also evoke fine-grained information about the organization of maps within areas. We used a data-driven approach (ICA) to discover maps that are similar to retinotopic maps in the infant visual cortex. We observed components that were highly similar to a spatial frequency map obtained from the same infant in a retinotopy task. This was also true for the meridian maps, to a lesser degree. This means that the retinotopic organization of the infant brain accounts for a detectable amount of variance in visual activity, otherwise components resembling these maps would not be discoverable. Importantly, the components could be identified without knowledge of these ground-truth maps; however, their moderate similarity to the task-defined maps makes them a poor replacement. One caveat for interpreting these results is that although some of the components are <italic>similar</italic> to a spatial frequency map or meridian map, they could reflect a different kind of visual map. For instance, the spatial frequency map is highly correlated with the eccentricity map (<xref ref-type="bibr" rid="bib33">Henriksson et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Smith et al., 2001</xref>; <xref ref-type="bibr" rid="bib51">Srihasam et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Tolhurst and Thompson, 1981</xref>, which itself is related to receptive field size). This means it is inappropriate to make strong claims about the underlying function of the components based on their similarity to visual maps alone. Another limitation is that ICA does not provide a scale to the variation: although we find a correlation between gradients of spatial frequency in the ground truth and the selected component, we cannot use the component alone to infer the spatial frequency selectivity of any part of cortex. In other words, we cannot infer units of spatial frequency sensitivity from the components alone. Nonetheless, these results do show that it is possible to discover approximations of visual maps in infants and toddlers with movie-watching data and ICA.</p><p>We also asked whether functional alignment (<xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib56">Turek et al., 2018</xref>) could be used to detect visual maps in infants. Using a shared response model (<xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>) trained on movie-watching data of infants or adults, we transformed the visual maps of other individuals into a held-out infant’s brain to evaluate the fit to visual maps from a retinotopy task (<xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>). Like ICA, this was more successful for the spatial frequency maps, but it was still possible in some cases with the meridian maps. This is remarkable because the complex pattern of brain activity underlying these visual maps could be ‘compressed’ by SRM into only 10 dimensions in the shared space (i.e. the visual maps were summarized by a vector of 10 values). The weight matrix that ‘decompressed’ visual maps from this low-dimensional space into the held-out infant was learned from their movie-watching data alone. Hence, success with this approach means that visual maps are engaged during infant movie-watching. Furthermore, this result shows that functional alignment is practical for studies in awake infants that produce small amounts of data (<xref ref-type="bibr" rid="bib18">Ellis and Turk-Browne, 2018</xref>). This is initial evidence that functional alignment may be useful for enhancing signal quality, like it has in adults (<xref ref-type="bibr" rid="bib10">Busch et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>), or revealing changing function over development (<xref ref-type="bibr" rid="bib66">Yates et al., 2021</xref>), which may prove especially useful for infant fMRI (<xref ref-type="bibr" rid="bib18">Ellis and Turk-Browne, 2018</xref>). In sum, movies evoke sufficiently reliable activity across infants and adults to find a shared response, and this shared response contains information about the organization of infant visual cortex.</p><p>To be clear, we are not suggesting that movies work well enough to <italic>replace</italic> a retinotopy task when accurate maps are needed. For instance, even though ICA found components that were highly correlated with the spatial frequency map, we also selected some components that turned out to have lower correlations. Without knowing the ground truth from a retinotopy task, there would be no way to weed these out. Additionally, anatomical alignment (i.e. averaging the maps from other participants and anatomically aligning them to a held-out participant) resulted in maps that were highly similar to the ground truth. Indeed, we previously <xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref> found that adult-defined visual areas were moderately similar to infants. While functional alignment with adults can outperform anatomical alignment methods in similar analyses (<xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>), here we find that functional alignment is inferior to anatomical alignment. Thus, if the goal is to define visual areas in an infant that lacks task-based retinotopy, anatomical alignment of other participants’ retinotopic maps is superior to using movie-based analyses, at least as we tested it.</p><p>In conclusion, movies evoke activity in infants and toddlers that recapitulate the organization of the visual cortex. This activity is differentiated across visual areas and contains information about the visual maps at the foundation of visual processing. The work presented here is another demonstration of the power of content-rich, dynamic, and naturalistic stimuli to reveal insights in cognitive neuroscience.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">MATLAB v. 2017a</td><td align="left" valign="bottom">Mathworks, <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">mathworks.com</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Psychtoolbox v. 3</td><td align="left" valign="bottom">Medical Innovations Incubator, <ext-link ext-link-type="uri" xlink:href="https://www.psychtoolbox.net/">psychtoolbox.net/</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_002881">SCR_002881</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Python v. 3.6</td><td align="left" valign="bottom">Python Software Foundation, <ext-link ext-link-type="uri" xlink:href="https://www.python.org/">python.org</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_008394">SCR_008394</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">FSL v. 5.0.9</td><td align="left" valign="bottom">FMRIB, fsl.fmrib.ox.ac.uk/fsl/fslwiki</td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_002823">SCR_002823</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Experiment menu v. 1.1</td><td align="left" valign="bottom">Yale Turk-Browne Lab, (<xref ref-type="bibr" rid="bib20">Ellis et al., 2020b</xref>)</td><td align="left" valign="bottom"/><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/experiment_menu">https://github.com/ntblab/experiment_menu</ext-link></td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Infant neuropipe v. 1.3</td><td align="left" valign="bottom">Yale Turk-Browne Lab, (<xref ref-type="bibr" rid="bib21">Ellis et al., 2020c</xref>)</td><td align="left" valign="bottom"/><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/infant_neuropipe">https://github.com/ntblab/infant_neuropipe</ext-link></td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Participants</title><p>Infant participants with retinotopy data were previously reported in another study (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>). Of those 17 original sessions, 15 had usable movie data collected in the same session and thus could be included in the current study. In this subsample, the age range was 4.8–23.1 months (M=13.0; 12 female; <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). The combinations of movies that infants saw were inconsistent, so the types of comparisons vary across analyses reported here. In brief, all possible infant participant sessions (15) were used in the homotopy analyses and ICA, whereas two of these sessions (ages = 18.5, 23.1 months) could not be used in the SRM analyses. <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> reports demographic information for the infant participants. <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> reports participant information about each of the movies. It also reports the number and age of participants that were used to bolster the SRM analyses.</p><p>An adult sample was collected (N=8, 3 females) and used for validating the analyses and for supporting SRM analyses in infants. Each participant had both retinotopy and movie-watching data. The adult participants saw the five most common movies that were seen by infants in our retinotopy sample. To support the SRM analyses, we also utilized any other available adult data from sessions in which we had shown the main movies in otherwise identical circumstances (<xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>).</p><p>Participants were recruited through fliers, word of mouth, or the Yale Baby School. This study was approved by the Human Subjects Committee at Yale University. Adults provided written informed consent for themselves (if they were the participants) or on behalf of their child (if their child was the participant).</p></sec><sec id="s4-2"><title>Data acquisition</title><p>Data were collected at the Brain Imaging Center (BIC) in the Faculty of Arts and Sciences at Yale University. We used a Siemens Prisma (3T) MRI and only the bottom half of the 20-channel head coil. Functional images were acquired with a whole-brain T2* gradient-echo EPI sequence (TR = 2 s, TE = 30 ms, flip angle = 71, matrix = 64 × 64, slices = 34, resolution = 3 mm iso, interleaved slice acquisition). Anatomical images were acquired with a T1 PETRA sequence for infants (TR1=3.32 ms, TR2=2250 ms, TE = 0.07 ms, flip angle = 6, matrix = 320 × 320, slices = 320, resolution = 0.94 mm iso, radial lines = 30,000) and a T1 MPRAGE sequence for adults, with the top of the head coil attached (TR = 2400 ms, TE = 2.41 ms, TI = 1000 ms, flip angle = 8, iPAT = 2, slices = 176, matrix = 256 × 256, resolution = 1.0 mm iso).</p></sec><sec id="s4-3"><title>Procedure</title><p>Our approach for collecting fMRI data from awake infants has been described in a previous methods paper (<xref ref-type="bibr" rid="bib19">Ellis et al., 2020a</xref>), with important details repeated below. Infants were first brought in for a mock scanning session to acclimate them and their parent to the scanning environment. Scans were scheduled when the infants were typically calm and happy. Participants were carefully screened for metal. We applied hearing protection in three layers for the infants: silicon inner ear putty, over-ear adhesive covers, and ear muffs. For the infants that were played sound (see below), Optoacoustics noise canceling headphones were used instead of the ear muffs. The infant was placed on a vacuum pillow on the bed that comfortably reduced their movement. The top of the head coil was not placed over the infant in order to maintain comfort. Stimuli were projected directly onto the surface of the bore. A video camera (High Resolution camera, MRC systems) recorded the infant’s face during scanning. Adult participants underwent the same procedure with the following exceptions: they did not attend a mock scanning session, hearing protection was only two layers (earplugs and Optoacoustics headphones), and they were not on a vacuum pillow. Some infants participated in additional tasks during their scanning session.</p><p>When the infant was focused, experimental stimuli were shown using Psychtoolbox (<xref ref-type="bibr" rid="bib35">Kleiner et al., 2007</xref>) for MATLAB. The details for the retinotopy task are explained fully elsewhere (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>). In short, we showed two types of blocks. For the meridian mapping blocks, a bow tie cut-out of a colorful, large, flickering checkerboard was presented in either a vertical or horizontal orientation (<xref ref-type="bibr" rid="bib53">Tootell et al., 1995</xref>). For the spatial frequency mapping blocks, the stimuli were grayscale Gaussian random fields of high (1.5 cycles per visual degree) or low (0.05 cycles per visual degree) spatial frequency (<xref ref-type="bibr" rid="bib3">Arcaro and Livingstone, 2017</xref>). For all blocks, a smaller (1.5 visual degree) grayscale movie was played at center to encourage fixation. Each block type contained two phases of stimulation. The first phase consisted of one of the conditions (e.g. horizontal or high) for 20 s, followed immediately by the second phase with the other condition of the same block type (e.g. vertical or low, respectively) for 20 s. At the end of each block there was at least 6 s rest before the start of the next block. Infant participants saw up to 12 blocks of this stimulus, resulting in 24 epochs of stimuli. Adults all saw 12 blocks.</p><p>Participants saw a broad range of movies in this study (<xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>), some of which have been reported previously (<xref ref-type="bibr" rid="bib67">Yates et al., 2022</xref>; <xref ref-type="bibr" rid="bib68">Yates et al., 2023</xref>). The movie titled ‘Child Play’ comprises the concatenation of four silent videos that range in duration from 64 to 143 s and were shown in the same order (with 6 s in-between). They extended 40.8° wide by 25.5° high on the screen. The other movies were stylistically similar, computer-generated animations that each lasted 180 s. These movies extended 45.0° wide by 25.5° high. Some of the movies were collected as part of an unpublished experiment in which we either played the full movie or inserted drops every 10 s (i.e. the screen went blank while the audio continued). We included the ‘Dropped’ movies in the homotopy analyses and ICA (average number of ‘Dropped’ movies per participant: 0.9, range: 0–3); however, we did not include them in the SRM analyses. Moreover, we only included 4 (out of 17) of these movies in the SRM analyses because there were insufficient numbers of infant participants to enable the training of the SRM.</p></sec><sec id="s4-4"><title>Gaze coding</title><p>The infant gaze coding procedure for the retinotopy data was the same as reported previously (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>). The gaze coding for the movies was also the same as reported previously (<xref ref-type="bibr" rid="bib67">Yates et al., 2022</xref>; <xref ref-type="bibr" rid="bib68">Yates et al., 2023</xref>). Participants looked at the screen for an average of 93.7% of the time (range: 78–99) for the movies used in the homotopy analyses and ICA, and 94.5% of the time (range: 82–99) for the movies used in the SRM analyses (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). Adult participants were not gaze-coded, but they were monitored online for inattentiveness. One adult participant was drowsy so they were manually coded. This resulted in the removal of 4 out of the 24 epochs of retinotopy.</p></sec><sec id="s4-5"><title>Preprocessing</title><p>We used FSL’s FEAT analyses with modifications in order to implement infant-specific preprocessing of the data (<xref ref-type="bibr" rid="bib19">Ellis et al., 2020a</xref>). If infants participated in other experiments during the same functional run (14 sessions), the data was split to create a pseudorun. Three burn-in volumes were discarded from the beginning of each run/pseudorun when available. To determine the reference volume for alignment and motion correction, the Euclidean distance between all volumes was calculated and the volume that minimized the distance between all points was chosen as reference (the ‘centroid volume’). Adjacent timepoints with greater than 3 mm of movement were interpolated. To create the brain mask we calculated the SFNR (<xref ref-type="bibr" rid="bib28">Friedman and Glover, 2006</xref>) for each voxel in the centroid volume. This produced a bimodal distribution reflecting the signal properties of brain and non-brain voxels. We thresholded the brain voxels at the trough between these two peaks. We performed Gaussian smoothing (FWHM = 5 mm). Motion correction with 6 degrees of freedom (DOF) was performed using the centroid volume. AFNI’s despiking algorithm attenuated voxels with aberrant timepoints. The data for each movie were <italic>z</italic>-scored in time.</p><p>We registered the centroid volume to a homogenized and skull-stripped anatomical volume from each participant. Initial alignment was performed using FLIRT with a normalized mutual information cost function. This automatic registration was manually inspected and then corrected if necessary using mrAlign from mrTools (<xref ref-type="bibr" rid="bib29">Gardner et al., 2018</xref>).</p><p>The final step common across analyses created a transformation into surface space. Surfaces were reconstructed from iBEAT v2.0 (<xref ref-type="bibr" rid="bib62">Wang et al., 2023</xref>). These surfaces were then aligned into standard Buckner40 standard surface space (<xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref>) using FreeSurfer (<xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref>).</p><p>Additional preprocessing steps were taken for the SRM analyses. For each individual movie (including each movie that makes up ‘Child Play’), the fMRI data was time-shifted by 4 s and the break after the movie finished was cropped. This was done to account for hemodynamic lag, so that the first TR and last TR of the data approximately (<xref ref-type="bibr" rid="bib46">Poppe et al., 2021</xref>) corresponded to the brain’s response to the first and last 2 s of the movie, respectively.</p><p>Occipital masks were aligned to the participant’s native space for the SRM analyses. To produce these, a mapping from native functional space to standard space was determined. This was enabled using non-linear alignment of the anatomical image to standard space using ANTs (<xref ref-type="bibr" rid="bib4">Avants et al., 2011</xref>). For infants, an initial linear alignment with 12 DOF was used to align anatomical data to the age-specific infant template (<xref ref-type="bibr" rid="bib25">Fonov et al., 2011</xref>), followed by non-linear warping using diffeomorphic symmetric normalization. Then, we used a predefined transformation (12 DOF) to linearly align between the infant template and adult standard. For adults, we used the same alignment procedure, except participants were directly aligned to adult standard. We used the occipital mask from the MNI structural atlas (<xref ref-type="bibr" rid="bib43">Mazziotta et al., 2001</xref>) in standard space – defined liberally to include any voxel with an above zero probability of being labeled as the occipital lobe – and used the inverted transform to put it into native functional space.</p></sec><sec id="s4-6"><title>Analysis</title><sec id="s4-6-1"><title>Retinotopy</title><p>For our measure of task-evoked retinotopy in infants, we used the outputs of the retinotopy analyses from our previous paper (<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>) that are publicly released. In brief, we performed separate univariate contrasts between conditions in the study (horizontal&gt;vertical, high spatial frequency&gt;low spatial frequency). We then mapped these contrasts into surface space. Then, in surface space rendered by AFNI (<xref ref-type="bibr" rid="bib14">Cox, 1996</xref>), we demarcated the visual areas V1, V2, V3, V4, and V3A/B using traditional protocols based on the meridian map contrast (<xref ref-type="bibr" rid="bib60">Wandell et al., 2007</xref>). We traced lines perpendicular and parallel to the area boundaries to quantify gradients in the visual areas. The anatomically defined areas of interest <xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref> used in <xref ref-type="fig" rid="fig2">Figure 2</xref> were available in this standard surface space. The adult data were also traced using the same methods as infants (described previously [<xref ref-type="bibr" rid="bib22">Ellis et al., 2021</xref>]) by one of the original infant coders (CE).</p></sec><sec id="s4-6-2"><title>Homotopy</title><p>The homotopy analyses compared the time course of functional activity across visual areas in different hemispheres of each infant. For the participants that had more than one movie in a session (N=9), all the movies were concatenated along with burnout time between the movies (mean number of movies per participant = 2.7, range: 1–6, mean duration of movies = 540.7 s, range: 186–1116). For the areas that were defined with the retinotopy task (average number of areas traced in each hemisphere = 7.3, range: 6.0–8.0), the functional activity was averaged within area and then Pearson correlated between all other areas. The resulting cross-correlation matrix was Fisher Z transformed before different cells were averaged or compared. If infants did not have an area traced then those areas were ignored in the analyses. We grouped visual areas according to stream, where areas that are <italic>more</italic> dorsal of V1 were called ‘dorsal’ stream and areas <italic>more</italic> ventral were called ‘ventral’ stream. To assess the functional similarity of visual areas, Fisher Z correlations between the same areas in the same stream were averaged, and compared to the correlations of approximately equivalent areas from different streams (e.g. dorsal V2 compared with ventral V2). The averages for each of the two conditions (same stream versus different stream) were evaluated statistically using bootstrap resampling (<xref ref-type="bibr" rid="bib17">Efron and Tibshirani, 1986</xref>). Specifically, we computed the mean difference between conditions in a pseudosample, generated by sampling participants with replacement. We created 10,000 such pseudosamples and took the proportion of differences that showed a different sign than the true mean, multiplied by two to get the two-tailed p-value. To evaluate how distance affects similarity, we additionally compared with bootstrap resampling the Fisher Z correlations of areas across hemispheres in the same stream: same area to adjacent areas (e.g. ventral V1 with ventral V2), to distal areas (e.g. ventral V1 with ventral V3). Before reporting the results in the figures, the Fisher Z values were converted back into Pearson correlation values.</p><p>As an additional analysis to the one described above, we used an atlas of anatomically defined visual areas from adults (<xref ref-type="bibr" rid="bib61">Wang et al., 2015</xref>) to define both early and later visual areas. Specifically, we used the areas labeled as part of the ventral and dorsal stream (excluding the intraparietal sulcus and frontal eye fields since they often cluster separately [<xref ref-type="bibr" rid="bib32">Haak and Beckmann, 2018</xref>]), and then averaged the functional response within each area. The functional responses were then correlated across hemispheres, as in the main analysis. MDS was then performed on the cross-correlation matrix, and the dimensionality that fell below the threshold for stress (0.2) was chosen. In this case, that was a dimensionality of 2 (stress = 0.076). We then visualized the resulting output of the data in these two dimensions.</p></sec><sec id="s4-6-3"><title>Independent component analysis</title><p>To conduct ICA, we provided the preprocessed movie data to FSL’s MELODIC (<xref ref-type="bibr" rid="bib5">Beckmann et al., 2005</xref>). Like in the homotopy analyses, we used all of the movie data available per session. The algorithm found a range of components across participants (M=76.4 components, range: 31–167). With this large number of possible components, an individual coder (CE), sorted through them to determine whether each one looked like a meridian map, spatial frequency map, or neither (critically, without referring to the ground truth from the retinotopy task). We initially visually inspected each component in volumetric space, looking for the following features: First, we searched for whether there was a strong weighting of the component in visual cortex. Second, we looked for components that had a symmetrical pattern in visual cortex between the two hemispheres. To identify the spatial frequency maps, we looked for a continuous gradient emanating out from the early visual cortex. For meridian maps, we looked for sharp alternations in the sign of the component, particularly near the midline of the two hemispheres. Based on these criteria, we then chose a small set of components that were further scrutinized in surface space. On the surface, we looked for features that clearly define a visual map topography. Again, this selection process was blind to the task-evoked retinotopic maps, so that a person without retinotopy data could take the same steps and potentially find maps. For the adult participants who were analyzed, the components were selected <italic>before</italic> those participants were retinotopically traced, in order to minimize the potential contamination that could occur when performing these manual steps close in time.</p><p>These components were then tested against that participant’s task-evoked retinotopic maps. If the component was labeled as a potential spatial frequency map, we tested whether there was a monotonic gradient from fovea to periphery. Specifically, we measured the component response along lines drawn parallel to the area boundaries, averaged across these lines, and then correlated this pattern with the same response in the actual map. The absolute correlation was used because the sign of ICA is arbitrary. For each participant, we then ranked the components to ask if the ones that were chosen were the best ones possible out of all those derived from MELODIC. To test whether the identified components were better than the non-identified components, we ranked all the components correlation to the task-evoked maps. This ranking was converted into a percentile, where 100% means it is the best possible component. We took the identified component’s percentile (or averaged the percentiles if there were multiple components chosen) and compared it to chance (50%). This difference from chance was used for bootstrap resampling to evaluate whether the identified components were significantly better than chance. We performed the same kind of analysis for meridian maps, except in this case the lines used for testing were those drawn perpendicular to the areas. In this case, we were testing whether the components showed oscillations in the sign of the intensity.</p><p>To evaluate whether components resembling retinotopic maps arise by chance, we misaligned the functional and anatomical data for a subset of participants and manually relabeled them. If retinotopic components are identified at the same rate in the misaligned data as the original data, this would support the concern that the selection process finds structure where there is none. For each participant, we aligned the components to standard surface space and then flipped the labels for left and right hemispheres. Loading these flipped files as if they were correctly aligned had the effect of rolling the functional signals with respect to the anatomy of the cortical surface. Specifically, because the image files are always read in the same order, but the hemispheres differ in the mosaic alignment of nodes in surface space, this flipping transposed voxels from early visual cortex laterally to the approximate position of the lateral occipital cortex and vice versa, while preserving smoothness. Of the 15 total participants, 9 were excluded from this analysis because they had partial volumes (e.g. missing the superior extent of the parietal lobe) such that their rolled data in surface space contained tell-tale signs (e.g. missing voxels were now in an unrealistic place) that precluded blind coding.</p><p>To set up the blind test for the coder, the rolled components from the 6 remaining participants were intermixed with an equal number of their original components and then the labels (as original or rolled) were hashed. A coder was given all of the original and rolled components with their hashed names and categorized each one as a spatial frequency component, meridian component, or neither. Once completed, these responses were cross-referenced against the unhashed names to determine whether the components the coder selected as retinotopic had been rolled. The proportion of selected components that were original (versus rolled) was compared against chance (50%) with a binomial test.</p></sec><sec id="s4-6-4"><title>Shared response modeling</title><p>We based our SRM analyses on previous approaches using hyperalignment (<xref ref-type="bibr" rid="bib30">Guntupalli et al., 2016</xref>) and adapted them for our sample. SRM embeds the brain activity of multiple individuals viewing a common stimulus into a shared space with a small number of feature dimensions. Each voxel of each participant is assigned a weight for each feature. The weight reflects how much the voxel loads onto that feature. For our study, the SRM was either trained on infant movie-watching data or adult movie-watching data to learn the shared response, and the mapping of the training participants into this shared space. For the infant SRM, we used a leave-one-out approach. We took a movie that the held-out infant saw (e.g. ‘Aeronaut’) and considered all other infant participants that saw that movie (including additional participants without any retinotopy data). We fit an SRM model on all of the participants except the held-out one. This model has 10 features, as was determined based on cross-validation with adult data (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We used an occipital anatomical mask to fit the SRM. Using the learned individual weight matrices, the retinotopic maps from the infants in the training set were then transformed into the shared space and averaged across participants. The held-out participant’s movie data were used to learn a mapping to the learned SRM features. By applying the inverse of this mapping, we transformed the averaged visual maps of the training set in shared space into the brain space of the held-out participant to predict their visual maps. Using the same methods as described for ICA above, we compared the task-evoked and predicted gradient responses. These analysis steps were also followed for the adult SRM, with the difference being that the group of participants used to create the SRM model and to create the averaged visual maps were adults. As with the infant SRM, additional adult participants without retinotopy data were used for training. Across both types of analysis, the held-out participant was completely ignored when fitting the SRM, and no retinotopy data went into training the SRM.</p><p>To test the benefit of SRM, we performed a control analysis in which we scrambled the movie data from the held-out participant before learning their mapping into the shared space. Specifically, we flipped the time course of the data so that the first timepoint became the last, and vice versa. By creating a mismatch in the movie sequence across participants, this procedure should result in meaningless weights for the held-out participant and, in turn, the prediction of visual maps using SRM will fail. We compared ‘real’ and ‘flipped’ SRM procedures by computing the difference in fit (transformed into Fisher Z) for each movie, and then averaging that difference within participant across movies. Those differences were then bootstrap resampled to evaluate significance. We also performed bootstrap resampling to compare the ‘real’ SRM accuracy when using infants versus adults for training.</p></sec><sec id="s4-6-5"><title>Anatomical alignment test</title><p>We performed a second type of between-participant analysis in addition to SRM. Specifically, we anatomically aligned the retinotopic maps from other participants to make a prediction of the map in a held-out participant. To achieve this, we first aligned all spatial frequency and meridian maps from infant and adult participants with retinotopy into the Buckner40 standard surface space (<xref ref-type="bibr" rid="bib15">Dale et al., 1999</xref>). For each infant participant, we composed a map from the average of the <italic>other</italic> participants. The other participants were either all the other infants or all the adult participants. We then used the lines traced parallel to the area boundaries (for spatial frequency) or perpendicular to the area boundaries (for meridian) to extract gradients of response in the average maps. These gradients were then correlated with the ground-truth gradients (i.e. the alternations in sensitivity in the held-out infant using lines traced from that participant). These correlations were then compared to SRM results within participants using bootstrap resampling. If a participant had multiple movies worth of data, then they were averaged prior to this comparison.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This study was approved by the Human Subjects Committee at Yale University (#2000022470). Adults provided written informed consent for themselves (if they were the participants) or on behalf of their child (if their child was the participant). The consent form stated that de-identified data can be published in scientific journals or posted anonymously in data repositories as required by funding agencies and/or scientific journals.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-92119-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Our experiment display code can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/experiment_menu/tree/Movies/">https://github.com/ntblab/experiment_menu/tree/Movies/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/experiment_menu/tree/retinotopy/">https://github.com/ntblab/experiment_menu/tree/retinotopy/</ext-link> (<xref ref-type="bibr" rid="bib20">Ellis et al., 2020b</xref>). The code used to perform the data analyses is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/infant_neuropipe/tree/predict_retinotopy/">https://github.com/ntblab/infant_neuropipe/tree/predict_retinotopy/</ext-link>, (<xref ref-type="bibr" rid="bib21">Ellis et al., 2020c</xref>) this code uses tools from the Brain Imaging Analysis Kit (<xref ref-type="bibr" rid="bib38">Kumar et al., 2020a</xref>); <ext-link ext-link-type="uri" xlink:href="https://brainiak.org/docs/">https://brainiak.org/docs/</ext-link>. Raw and preprocessed functional and anatomical data is available on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.jm63xsjm3">Dryad</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Yates</surname><given-names>T</given-names></name><name><surname>Arcaro</surname><given-names>M</given-names></name><name><surname>Turk-Browne</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data from: Movies reveal the fine-grained organization of infant visual cortex</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.jm63xsjm3</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Yates</surname><given-names>T</given-names></name><name><surname>Skalaban</surname><given-names>L</given-names></name><name><surname>Bejjanki</surname><given-names>V</given-names></name><name><surname>Arcaro</surname><given-names>M</given-names></name><name><surname>Turk-Browne</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Retinotopic organization of visual cortex in human infants</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.7h44j0ztm</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are thankful to the families of infants who participated. We also acknowledge the hard work of the Yale Baby School team, including L Rait, J Daniels, A Letrou, and K Armstrong for recruitment, scheduling, and administration, and L Skalaban, A Bracher, D Choi, and J Trach for help in infant fMRI data collection. Thank you to J Wu, J Fel, and A Klein for help with gaze coding, and R Watts for technical support. We are grateful for internal funding from the Department of Psychology and Faculty of Arts and Sciences at Yale University. NBTB was further supported by the Canadian Institute for Advanced Research and the James S McDonnell Foundation.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>LM</given-names></name><name><surname>Escalera</surname><given-names>J</given-names></name><name><surname>Ai</surname><given-names>L</given-names></name><name><surname>Andreotti</surname><given-names>C</given-names></name><name><surname>Febre</surname><given-names>K</given-names></name><name><surname>Mangone</surname><given-names>A</given-names></name><name><surname>Vega-Potler</surname><given-names>N</given-names></name><name><surname>Langer</surname><given-names>N</given-names></name><name><surname>Alexander</surname><given-names>A</given-names></name><name><surname>Kovacs</surname><given-names>M</given-names></name><name><surname>Litke</surname><given-names>S</given-names></name><name><surname>O’Hagan</surname><given-names>B</given-names></name><name><surname>Andersen</surname><given-names>J</given-names></name><name><surname>Bronstein</surname><given-names>B</given-names></name><name><surname>Bui</surname><given-names>A</given-names></name><name><surname>Bushey</surname><given-names>M</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Castagna</surname><given-names>V</given-names></name><name><surname>Camacho</surname><given-names>N</given-names></name><name><surname>Chan</surname><given-names>E</given-names></name><name><surname>Citera</surname><given-names>D</given-names></name><name><surname>Clucas</surname><given-names>J</given-names></name><name><surname>Cohen</surname><given-names>S</given-names></name><name><surname>Dufek</surname><given-names>S</given-names></name><name><surname>Eaves</surname><given-names>M</given-names></name><name><surname>Fradera</surname><given-names>B</given-names></name><name><surname>Gardner</surname><given-names>J</given-names></name><name><surname>Grant-Villegas</surname><given-names>N</given-names></name><name><surname>Green</surname><given-names>G</given-names></name><name><surname>Gregory</surname><given-names>C</given-names></name><name><surname>Hart</surname><given-names>E</given-names></name><name><surname>Harris</surname><given-names>S</given-names></name><name><surname>Horton</surname><given-names>M</given-names></name><name><surname>Kahn</surname><given-names>D</given-names></name><name><surname>Kabotyanski</surname><given-names>K</given-names></name><name><surname>Karmel</surname><given-names>B</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>Kleinman</surname><given-names>K</given-names></name><name><surname>Koo</surname><given-names>B</given-names></name><name><surname>Kramer</surname><given-names>E</given-names></name><name><surname>Lennon</surname><given-names>E</given-names></name><name><surname>Lord</surname><given-names>C</given-names></name><name><surname>Mantello</surname><given-names>G</given-names></name><name><surname>Margolis</surname><given-names>A</given-names></name><name><surname>Merikangas</surname><given-names>KR</given-names></name><name><surname>Milham</surname><given-names>J</given-names></name><name><surname>Minniti</surname><given-names>G</given-names></name><name><surname>Neuhaus</surname><given-names>R</given-names></name><name><surname>Levine</surname><given-names>A</given-names></name><name><surname>Osman</surname><given-names>Y</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name><name><surname>Pugh</surname><given-names>KR</given-names></name><name><surname>Racanello</surname><given-names>A</given-names></name><name><surname>Restrepo</surname><given-names>A</given-names></name><name><surname>Saltzman</surname><given-names>T</given-names></name><name><surname>Septimus</surname><given-names>B</given-names></name><name><surname>Tobe</surname><given-names>R</given-names></name><name><surname>Waltz</surname><given-names>R</given-names></name><name><surname>Williams</surname><given-names>A</given-names></name><name><surname>Yeo</surname><given-names>A</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Paus</surname><given-names>T</given-names></name><name><surname>Leventhal</surname><given-names>BL</given-names></name><name><surname>Craddock</surname><given-names>RC</given-names></name><name><surname>Koplewicz</surname><given-names>HS</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An open resource for transdiagnostic research in pediatric mental health and learning disorders</article-title><source>Scientific Data</source><volume>4</volume><elocation-id>170181</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2017.181</pub-id><pub-id pub-id-type="pmid">29257126</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>McMains</surname><given-names>SA</given-names></name><name><surname>Singer</surname><given-names>BD</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Retinotopic organization of human ventral visual cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>10638</fpage><lpage>10652</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2807-09.2009</pub-id><pub-id pub-id-type="pmid">19710316</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title><source>eLife</source><volume>6</volume><elocation-id>e26196</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26196</pub-id><pub-id pub-id-type="pmid">28671063</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Song</surname><given-names>G</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title><source>NeuroImage</source><volume>54</volume><fpage>2033</fpage><lpage>2044</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id><pub-id pub-id-type="pmid">20851191</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>DeLuca</surname><given-names>M</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Investigations into resting-state connectivity using independent component analysis</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>360</volume><fpage>1001</fpage><lpage>1013</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1634</pub-id><pub-id pub-id-type="pmid">16087444</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biagi</surname><given-names>L</given-names></name><name><surname>Crespi</surname><given-names>SA</given-names></name><name><surname>Tosetti</surname><given-names>M</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>BOLD response selective to flow-motion in very young infants</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002260</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002260</pub-id><pub-id pub-id-type="pmid">26418729</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biagi</surname><given-names>L</given-names></name><name><surname>Tosetti</surname><given-names>M</given-names></name><name><surname>Crespi</surname><given-names>SA</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Development of BOLD response to motion in human infants</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>3825</fpage><lpage>3837</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0837-22.2023</pub-id><pub-id pub-id-type="pmid">37037605</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braddick</surname><given-names>O</given-names></name><name><surname>Atkinson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Development of human visual function</article-title><source>Vision Research</source><volume>51</volume><fpage>1588</fpage><lpage>1609</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.02.018</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brodmann</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1909">1909</year><source>Vergleichende Lokalisationslehre Der Grosshirnrinde in Ihren Prinzipien Dargestellt Auf Grund Des Zellenbaues</source><publisher-name>Barth</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busch</surname><given-names>EL</given-names></name><name><surname>Slipski</surname><given-names>L</given-names></name><name><surname>Feilong</surname><given-names>M</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>di O Castello</surname><given-names>M</given-names></name><name><surname>Huckins</surname><given-names>JF</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hybrid hyperalignment: A single high-dimensional model of shared information embedded in cortical patterns of response and functional connectivity</article-title><source>NeuroImage</source><volume>233</volume><elocation-id>117975</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117975</pub-id><pub-id pub-id-type="pmid">33762217</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butt</surname><given-names>OH</given-names></name><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Datta</surname><given-names>R</given-names></name><name><surname>Aguirre</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical and homotopic correlations of spontaneous neural activity within the visual cortex of the sighted and blind</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00025</pub-id><pub-id pub-id-type="pmid">25713519</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabral</surname><given-names>L</given-names></name><name><surname>Zubiaurre-Elorza</surname><given-names>L</given-names></name><name><surname>Wild</surname><given-names>CJ</given-names></name><name><surname>Linke</surname><given-names>A</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Anatomical correlates of category-selective visual regions have distinctive signatures of connectivity in neonates</article-title><source>Developmental Cognitive Neuroscience</source><volume>58</volume><elocation-id>101179</elocation-id><pub-id pub-id-type="doi">10.1016/j.dcn.2022.101179</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>PH</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Yeshurun</surname><given-names>Y</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>V.Haxby</surname><given-names>J</given-names></name><name><surname>J.Ramadge</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A reduced-dimension fMRI shared response model</article-title><source>NIPS</source><volume>28</volume><fpage>460</fpage><lpage>468</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research, an International Journal</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deen</surname><given-names>B</given-names></name><name><surname>Richardson</surname><given-names>H</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Organization of high-level visual cortex in human infants</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13995</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13995</pub-id><pub-id pub-id-type="pmid">28072399</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy</article-title><source>Statistical Science</source><volume>1</volume><fpage>54</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1214/ss/1177013815</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Infant fmri: A model system for cognitive neuroscience</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>375</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.01.005</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Córdova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Re-imagining fMRI for awake behaving infants</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4523</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18286-y</pub-id><pub-id pub-id-type="pmid">32908125</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Córdova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020b</year> <data-title>experiment_menu</data-title><version designator="1.1">1.1</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/experiment_menu">https://github.com/ntblab/experiment_menu</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Córdova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020c</year><data-title>Development project analysis pipeline</data-title><version designator="1.3">1.3</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/infant_neuropipe">https://github.com/ntblab/infant_neuropipe</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Retinotopic organization of visual cortex in human infants</article-title><source>Neuron</source><volume>109</volume><fpage>2616</fpage><lpage>2626</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.004</pub-id><pub-id pub-id-type="pmid">34228960</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Vanderwal</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Naturalistic imaging: the use of ecologically valid conditions to study brain function</article-title><source>NeuroImage</source><volume>247</volume><elocation-id>118776</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118776</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>V</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>Botteron</surname><given-names>K</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name><collab>Brain Development Cooperative Group</collab></person-group><year iso-8601-date="2011">2011</year><article-title>Unbiased average age-appropriate atlases for pediatric studies</article-title><source>NeuroImage</source><volume>54</volume><fpage>313</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.033</pub-id><pub-id pub-id-type="pmid">20656036</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Miezin</surname><given-names>FM</given-names></name><name><surname>Allman</surname><given-names>JM</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Retinotopic organization of human visual cortex mapped with positron-emission tomography</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>913</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-03-00913.1987</pub-id><pub-id pub-id-type="pmid">3494107</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franchak</surname><given-names>JM</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Adolph</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Free viewing gaze behavior in infants and adults</article-title><source>Infancy</source><volume>21</volume><fpage>262</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1111/infa.12119</pub-id><pub-id pub-id-type="pmid">27134573</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>L</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Report on a multicenter fMRI quality assurance protocol</article-title><source>Journal of Magnetic Resonance Imaging</source><volume>23</volume><fpage>827</fpage><lpage>839</lpage><pub-id pub-id-type="doi">10.1002/jmri.20583</pub-id><pub-id pub-id-type="pmid">16649196</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>J</given-names></name><name><surname>Merriam</surname><given-names>E</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Besle</surname><given-names>J</given-names></name><name><surname>Heeger</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Mrtools: analysis and visualization package for functional magnetic resonance imaging data</data-title><version designator="01">01</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.1299483">http://doi.org/10.5281/zenodo.1299483</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Ramadge</surname><given-names>PJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A model of representational spaces in human cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2919</fpage><lpage>2934</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw068</pub-id><pub-id pub-id-type="pmid">26980615</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haak</surname><given-names>KV</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Renken</surname><given-names>R</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Cornelissen</surname><given-names>FW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Connective field modeling</article-title><source>NeuroImage</source><volume>66</volume><fpage>376</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.037</pub-id><pub-id pub-id-type="pmid">23110879</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haak</surname><given-names>KV</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Objective analysis of the topological organization of the human cortical visual connectome suggests three visual pathways</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>98</volume><fpage>73</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.03.020</pub-id><pub-id pub-id-type="pmid">28457575</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriksson</surname><given-names>L</given-names></name><name><surname>Nurminen</surname><given-names>L</given-names></name><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Vanni</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatial frequency tuning in human retinotopic visual areas</article-title><source>Journal of Vision</source><volume>8</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.1167/8.10.5</pub-id><pub-id pub-id-type="pmid">19146347</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Topographic maps are fundamental to sensory processing</article-title><source>Brain Research Bulletin</source><volume>44</volume><fpage>107</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/S0361-9230(97)00094-4</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><elocation-id>1</elocation-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Topographic connectivity reveals task-dependent retinotopic processing throughout the human brain</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2017032118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2017032118</pub-id><pub-id pub-id-type="pmid">33372144</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosakowski</surname><given-names>HL</given-names></name><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Selective responses to faces, scenes, and bodies in the ventral visual pathway of infants</article-title><source>Current Biology</source><volume>32</volume><fpage>265</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.10.064</pub-id><pub-id pub-id-type="pmid">34784506</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>M</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Lu</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Capotă</surname><given-names>M</given-names></name><name><surname>Willke</surname><given-names>TL</given-names></name><name><surname>Ramadge</surname><given-names>PJ</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>BrainIAK tutorials: User-friendly learning materials for advanced fMRI analysis</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007549</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007549</pub-id><pub-id pub-id-type="pmid">31940340</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>O’Connell</surname><given-names>TP</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Searching through functional space reveals distributed visual, auditory, and semantic coding in the human brain</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008457</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008457</pub-id><pub-id pub-id-type="pmid">33270655</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Wen</surname><given-names>Q</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Dang</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Development of visual cortex in human neonates is selectively modified by postnatal experience</article-title><source>eLife</source><volume>11</volume><elocation-id>e78733</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.78733</pub-id><pub-id pub-id-type="pmid">36399034</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loiotile</surname><given-names>RE</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Naturalistic audio-movies and narrative synchronize “visual” cortices across congenitally blind but not sighted individuals</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>8940</fpage><lpage>8948</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0298-19.2019</pub-id><pub-id pub-id-type="pmid">31548238</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>KH</given-names></name><name><surname>Jeong</surname><given-names>JY</given-names></name><name><surname>Wen</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spontaneous activity in the visual cortex is organized by visual streams</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>4613</fpage><lpage>4630</lpage><pub-id pub-id-type="doi">10.1002/hbm.23687</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazziotta</surname><given-names>J</given-names></name><name><surname>Toga</surname><given-names>A</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name><name><surname>Fox</surname><given-names>P</given-names></name><name><surname>Lancaster</surname><given-names>J</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Woods</surname><given-names>R</given-names></name><name><surname>Paus</surname><given-names>T</given-names></name><name><surname>Simpson</surname><given-names>G</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name><name><surname>Holmes</surname><given-names>C</given-names></name><name><surname>Collins</surname><given-names>L</given-names></name><name><surname>Thompson</surname><given-names>P</given-names></name><name><surname>MacDonald</surname><given-names>D</given-names></name><name><surname>Iacoboni</surname><given-names>M</given-names></name><name><surname>Schormann</surname><given-names>T</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Palomero-Gallagher</surname><given-names>N</given-names></name><name><surname>Geyer</surname><given-names>S</given-names></name><name><surname>Parsons</surname><given-names>L</given-names></name><name><surname>Narr</surname><given-names>K</given-names></name><name><surname>Kabani</surname><given-names>N</given-names></name><name><surname>Le Goualher</surname><given-names>G</given-names></name><name><surname>Boomsma</surname><given-names>D</given-names></name><name><surname>Cannon</surname><given-names>T</given-names></name><name><surname>Kawashima</surname><given-names>R</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A probabilistic atlas and reference system for the human brain: International Consortium for Brain Mapping (ICBM)</article-title><source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source><volume>356</volume><fpage>1293</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1098/rstb.2001.0915</pub-id><pub-id pub-id-type="pmid">11545704</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Nallasamy</surname><given-names>N</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Functional connectivity of the macaque brain across stimulus and arousal states</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>5897</fpage><lpage>5909</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0220-09.2009</pub-id><pub-id pub-id-type="pmid">19420256</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Keep it real: rethinking the primacy of experimental control in cognitive neuroscience</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117254</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117254</pub-id><pub-id pub-id-type="pmid">32800992</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poppe</surname><given-names>T</given-names></name><name><surname>Willers Moore</surname><given-names>J</given-names></name><name><surname>Arichi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Individual focused studies of functional brain development in early human infancy</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>137</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.04.017</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>H</given-names></name><name><surname>Lisandrelli</surname><given-names>G</given-names></name><name><surname>Riobueno-Naylor</surname><given-names>A</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Development of the social brain from age three to twelve years</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>1027</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-03399-2</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>W</given-names></name><name><surname>Noll</surname><given-names>DC</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Functional topographic mapping of the cortical ribbon in human vision with conventional MRI scanners</article-title><source>Nature</source><volume>365</volume><fpage>150</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1038/365150a0</pub-id><pub-id pub-id-type="pmid">8371756</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AT</given-names></name><name><surname>Singh</surname><given-names>KD</given-names></name><name><surname>Williams</surname><given-names>AL</given-names></name><name><surname>Greenlee</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Estimating receptive field size from fMRI data in human striate and extrastriate visual cortex</article-title><source>Cerebral Cortex</source><volume>11</volume><fpage>1182</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1093/cercor/11.12.1182</pub-id><pub-id pub-id-type="pmid">11709489</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smyser</surname><given-names>CD</given-names></name><name><surname>Inder</surname><given-names>TE</given-names></name><name><surname>Shimony</surname><given-names>JS</given-names></name><name><surname>Hill</surname><given-names>JE</given-names></name><name><surname>Degnan</surname><given-names>AJ</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Neil</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Longitudinal analysis of neural network development in preterm infants</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2852</fpage><lpage>2862</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq035</pub-id><pub-id pub-id-type="pmid">20237243</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Novel domain formation reveals proto-architecture in inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1776</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1038/nn.3855</pub-id><pub-id pub-id-type="pmid">25362472</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolhurst</surname><given-names>D</given-names></name><name><surname>Thompson</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>On the variety of spatial frequency selectivities shown by neurons in area 17 of the cat</article-title><source>Proceedings of the Royal Society of London Series B Biological Sciences</source><volume>213</volume><fpage>183</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1098/rspb.1981.0061</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Born</surname><given-names>RT</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Functional analysis of human MT and related visual cortical areas using magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>3215</fpage><lpage>3230</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-04-03215.1995</pub-id><pub-id pub-id-type="pmid">7722658</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>M</given-names></name><name><surname>Cabral</surname><given-names>L</given-names></name><name><surname>Patel</surname><given-names>R</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Online recruitment and testing of infants with Mechanical Turk</article-title><source>Journal of Experimental Child Psychology</source><volume>156</volume><fpage>168</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.jecp.2016.12.003</pub-id><pub-id pub-id-type="pmid">28088051</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truzzi</surname><given-names>A</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The development of intrinsic timescales: A comparison between the neonate and adult brain</article-title><source>NeuroImage</source><volume>275</volume><elocation-id>120155</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120155</pub-id><pub-id pub-id-type="pmid">37169116</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Turek</surname><given-names>JS</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Willke</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Capturing Shared and Individual Information in fMRI Data</article-title><conf-name>ICASSP 2018 - 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</conf-name><conf-loc>Calgary, AB</conf-loc><fpage>826</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2018.8462175</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1982">1982</year><chapter-title>Two cortical visual systems</chapter-title><person-group person-group-type="editor"><name><surname>Ingle</surname><given-names>DMR</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><source>In Analysis of Visual Behavior</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>549</fpage><lpage>586</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanderwal</surname><given-names>T</given-names></name><name><surname>Kelly</surname><given-names>C</given-names></name><name><surname>Eilbott</surname><given-names>J</given-names></name><name><surname>Mayes</surname><given-names>LC</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inscapes: A movie paradigm to improve compliance in functional magnetic resonance imaging</article-title><source>NeuroImage</source><volume>122</volume><fpage>222</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.069</pub-id><pub-id pub-id-type="pmid">26241683</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanderwal</surname><given-names>T</given-names></name><name><surname>Eilbott</surname><given-names>J</given-names></name><name><surname>X.Castellanos</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Movies in the magnet: Naturalistic paradigms in developmental functional neuroimaging</article-title><source>Developmental Cognitive Neuroscience</source><volume>36</volume><elocation-id>100600</elocation-id><pub-id pub-id-type="doi">10.1016/j.dcn.2018.10.004</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Brewer</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.012</pub-id><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Mruczek</surname><given-names>REB</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>iBEAT V2.0: A multisite-applicable, deep learning-based pipeline for infant cerebral cortical surface reconstruction</article-title><source>Nature Protocols</source><volume>18</volume><fpage>1488</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1038/s41596-023-00806-x</pub-id><pub-id pub-id-type="pmid">36869216</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wattam-Bell</surname><given-names>J</given-names></name><name><surname>Birtles</surname><given-names>D</given-names></name><name><surname>Nyström</surname><given-names>P</given-names></name><name><surname>von Hofsten</surname><given-names>C</given-names></name><name><surname>Rosander</surname><given-names>K</given-names></name><name><surname>Anker</surname><given-names>S</given-names></name><name><surname>Atkinson</surname><given-names>J</given-names></name><name><surname>Braddick</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reorganization of global form and motion processing during human visual development</article-title><source>Current Biology</source><volume>20</volume><fpage>411</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.12.020</pub-id><pub-id pub-id-type="pmid">20171101</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Gomez</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Third visual pathway anatomy, and cognition across species</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>548</fpage><lpage>549</lpage></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>LE</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Vision and cortical map development</article-title><source>Neuron</source><volume>56</volume><fpage>327</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.011</pub-id><pub-id pub-id-type="pmid">17964249</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Emergence and organization of adult brain function throughout child development</article-title><source>NeuroImage</source><volume>226</volume><elocation-id>117606</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117606</pub-id><pub-id pub-id-type="pmid">33271266</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Bracher</surname><given-names>AJ</given-names></name><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural event segmentation of continuous experience in human infants</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2200257119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2200257119</pub-id><pub-id pub-id-type="pmid">36252007</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Functional networks in the infant brain during sleep and wake states</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>10820</fpage><lpage>10835</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhad327</pub-id><pub-id pub-id-type="pmid">37718160</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Demographic and dataset information for infant participants in the study.</title><p>‘Age’ is recorded in months. ‘Sex’ is the assigned sex at birth. ‘Retinotopy areas’ is the number of areas segmented from task-evoked retinotopy, averaged across hemispheres. Information about the movie data is separated based on analysis type: whereas all movie data is used for homotopy analyses and independent component analyses (ICA), a subset of data is used for shared response modeling (SRM). ‘Num.’ is the number of movies used. ‘Length’ is the duration in seconds of the run used for these analyses (includes both movie and rest periods). ‘Drops’ is the number of movies that include dropped periods. ‘Runs’ says how many runs or pseudoruns of movie data there were. ‘Gaze’ is the percentage of the data where the participants were looking at the movie.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">ID</th><th align="left" valign="bottom" rowspan="2">Age</th><th align="left" valign="bottom" rowspan="2">Sex</th><th align="left" valign="bottom" rowspan="2">Retinotopy Areas</th><th align="center" valign="bottom" colspan="5">Homotopy and ICA</th><th align="center" valign="bottom" colspan="2">SRM</th></tr><tr><th align="left" valign="bottom">Num.</th><th align="left" valign="bottom">Length</th><th align="left" valign="bottom">Drops</th><th align="left" valign="bottom">Runs</th><th align="left" valign="bottom">Gaze</th><th align="left" valign="bottom">Num.</th><th align="left" valign="bottom">Gaze</th></tr></thead><tbody><tr><td align="left" valign="bottom">s2077_1_1</td><td align="right" valign="bottom">4.8</td><td align="left" valign="bottom">M</td><td align="right" valign="bottom">6</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">430</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">97</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">97</td></tr><tr><td align="left" valign="bottom">s2097_1_1</td><td align="right" valign="bottom">5.2</td><td align="left" valign="bottom">M</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">186</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">96</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">96</td></tr><tr><td align="left" valign="bottom">s4047_1_1</td><td align="right" valign="bottom">5.5</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7.5</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">186</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">99</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">99</td></tr><tr><td align="left" valign="bottom">s7017_1_3</td><td align="right" valign="bottom">7.2</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7</td><td align="right" valign="bottom">4</td><td align="right" valign="bottom">744</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">97</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">98</td></tr><tr><td align="left" valign="bottom">s7047_1_1</td><td align="right" valign="bottom">9.6</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">432</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">91</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">91</td></tr><tr><td align="left" valign="bottom">s7067_1_4</td><td align="right" valign="bottom">10.6</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7.5</td><td align="right" valign="bottom">6</td><td align="right" valign="bottom">1110</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">98</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">99</td></tr><tr><td align="left" valign="bottom">s8037_1_2</td><td align="right" valign="bottom">12.2</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7.5</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">186</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">95</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">95</td></tr><tr><td align="left" valign="bottom">s4607_1_4</td><td align="right" valign="bottom">13</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">558</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">93</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">90</td></tr><tr><td align="left" valign="bottom">s1607_1_4</td><td align="right" valign="bottom">14.4</td><td align="left" valign="bottom">M</td><td align="right" valign="bottom">6</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">372</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">93</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">93</td></tr><tr><td align="left" valign="bottom">s6687_1_4</td><td align="right" valign="bottom">15.4</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">186</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">82</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">82</td></tr><tr><td align="left" valign="bottom">s8687_1_5</td><td align="right" valign="bottom">17.1</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">186</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">98</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">98</td></tr><tr><td align="left" valign="bottom">s6687_1_5</td><td align="right" valign="bottom">18.1</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">5</td><td align="right" valign="bottom">930</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">94</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">92</td></tr><tr><td align="left" valign="bottom">s4607_1_7</td><td align="right" valign="bottom">18.5</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7.5</td><td align="right" valign="bottom">4</td><td align="right" valign="bottom">744</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">78</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom">NaN</td></tr><tr><td align="left" valign="bottom">s6687_1_6</td><td align="right" valign="bottom">20.1</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">6.5</td><td align="right" valign="bottom">6</td><td align="right" valign="bottom">1116</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">97</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">98</td></tr><tr><td align="left" valign="bottom">s8687_1_8</td><td align="right" valign="bottom">23.1</td><td align="left" valign="bottom">F</td><td align="right" valign="bottom">7.5</td><td align="right" valign="bottom">4</td><td align="right" valign="bottom">744</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">97</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom">NaN</td></tr><tr><td align="left" valign="bottom">Mean</td><td align="right" valign="bottom">13</td><td align="char" char="." valign="bottom">.</td><td align="right" valign="bottom">7.3</td><td align="right" valign="bottom">2.7</td><td align="right" valign="bottom">540.7</td><td align="right" valign="bottom">0.9</td><td align="right" valign="bottom">1.5</td><td align="right" valign="bottom">93.7</td><td align="right" valign="bottom">1.3</td><td align="right" valign="bottom">94.5</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Number of participants per movie.</title><p>The first column is the movie name, where ‘Drop-’ indicates that it was a movie containing alternating epochs of blank screens. ‘SRM’ (shared response modeling) indicates whether the movie is used in SRM analyses. The movies that are not included in SRM are used for homotopy analyses and independent component analyses (ICA). ‘Ret. infants’ and ‘Ret. adults’ refers to the number of participants with retinotopy data that saw this movie. ‘Infant SRM’ and ‘Adult SRM’ refer to the number of additional participants available to use for training the SRM but who did not have retinotopy data. ‘Infant Ages’ is the average age in months of the infant participants included in the SRM, with the range of ages included in parentheses.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Movie name</th><th align="left" valign="bottom">SRM</th><th align="left" valign="bottom">Ret. infants</th><th align="left" valign="bottom">Ret. adults</th><th align="left" valign="bottom">Infant SRM</th><th align="left" valign="bottom">Infant Ages</th><th align="left" valign="bottom">Adult SRM</th></tr></thead><tbody><tr><td align="left" valign="bottom">Child_Play</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">20</td><td align="char" char="." valign="bottom">13.7 (3.3–32.0)</td><td align="right" valign="bottom">9</td></tr><tr><td align="left" valign="bottom">Aeronaut</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">35</td><td align="char" char="." valign="bottom">10.1 (3.6–20.1)</td><td align="right" valign="bottom">32</td></tr><tr><td align="left" valign="bottom">Caterpillar</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">6</td><td align="char" char="." valign="bottom">13.0 (6.6–18.2)</td><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Meerkats</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">4</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">6</td><td align="char" char="." valign="bottom">13.4 (7.2–18.2)</td><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Mouseforsale</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">8</td><td align="right" valign="bottom">4</td><td align="char" char="." valign="bottom">14.7 (7.2–20.1)</td><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Elephant</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">MadeinFrance</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Clocky</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Gopher</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Foxmouse</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Caterpillar</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">4</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Meerkats</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">3</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Mouseforsale</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Elephant</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-MadeinFrance</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Clocky</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">2</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Ballet</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Drop-Foxmouse</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">1</td><td align="right" valign="bottom">0</td><td align="right" valign="bottom">0</td><td align="left" valign="bottom"/><td align="right" valign="bottom">0</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Details for each movie used in this study.</title><p>‘Name’ specifies the movie name. ‘Duration’ specifies the duration of the movie in seconds. Movies were edited to standardize length and remove inappropriate content. ‘Sound’ is whether sound was played during the movie. These sounds include background music, animal noises, and sound effects, but no language. ‘Description’ gives a brief description of the movie, as well as a current link to it when appropriate. All movies are provided in the data release.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Name</th><th align="left" valign="bottom">Duration</th><th align="left" valign="bottom">Sound</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom">Child_Play</td><td align="left" valign="bottom">406</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">Four photo-realistic clips from `Daniel Tiger' showing children playing. The clips showed the following: 1. children playing in an indoor playground (84 s); 2. a family making frozen banana desserts (64 s); 3. a child visiting the doctor (115 s); 4. children helping with indoor and outdoor chores (143 s).</td></tr><tr><td align="left" valign="bottom">Aeronaut</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">A computer-generated segment from a short film titled ”Soar'' (<ext-link ext-link-type="uri" xlink:href="https://vimeo.com/148198462">https://vimeo.com/148198462</ext-link>) <break/>and described here (<xref ref-type="bibr" rid="bib67">Yates et al., 2022</xref>).</td></tr><tr><td align="left" valign="bottom">Caterpillar</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment from a short film titled ”Sweet Cocoon'' (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=yQ1ZcNpbwOA">https://www.youtube.com/watch?v=yQ1ZcNpbwOA</ext-link>). This video depicts a caterpillar trying to fit into its cocoon so it <break/>can become a butterfly.</td></tr><tr><td align="left" valign="bottom">Meerkats</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment from the short film titled ”Catch It'' (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=c88QE6yGhfM">https://www.youtube.com/watch?v=c88QE6yGhfM</ext-link>). It depicts a gang of meerkats who take back a treasured fruit from a vulture.</td></tr><tr><td align="left" valign="bottom">Mouse for Sale</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment from a short film of the same name (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=UB3nKCNUBB4">https://www.youtube.com/watch?v=UB3nKCNUBB4</ext-link>). It shows a mouse in a pet store who is teased for having big ears.</td></tr><tr><td align="left" valign="bottom">Elephant</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment from a short film of the same name (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=h_aC8pGY1aY">https://www.youtube.com/watch?v=h_aC8pGY1aY</ext-link>). It shows an elephant in a china shop.</td></tr><tr><td align="left" valign="bottom">Made in France</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment from a short film of the same name (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=Her3d1DH7yU">https://www.youtube.com/watch?v=Her3d1DH7yU</ext-link>). It shows a mouse making cheese.</td></tr><tr><td align="left" valign="bottom">Clocky</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment from a short film of the same name (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=8VRD5KOFK94">https://www.youtube.com/watch?v=8VRD5KOFK94</ext-link>). It shows a clock preparing to wake up its owner.</td></tr><tr><td align="left" valign="bottom">Gopher</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment named `Gopher broke' (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=tWufIUbXubY">https://www.youtube.com/watch?v=tWufIUbXubY</ext-link>). It shows a gopher collecting food.</td></tr><tr><td align="left" valign="bottom">Foxmouse</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment named `The short story of a fox and a mouse' (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=k6kCwj0Sk4s">https://www.youtube.com/watch?v=k6kCwj0Sk4s</ext-link>). It shows a fox playing with a mouse in the snow.</td></tr><tr><td align="left" valign="bottom">Ballet</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">A computer-generated segment named `The Duet' (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=GuX52wkCIJA">https://www.youtube.com/watch?v=GuX52wkCIJA</ext-link>). <break/>This is an artistic rendition of growing up and falling in love.</td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Correlations between infant gradients and the spatial average of other infants or adults.</title><p>For each participant, all other participants with retinotopy data (adults or infants) were aligned to standard surface space and averaged. The traced lines from the held-out participant were then applied to this average. The resulting gradients were correlated with the held-out participant and the correlation is reported here. This was done separately for meridian maps and spatial frequency maps.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">ID</th><th align="center" valign="bottom" colspan="2">Adults</th><th align="center" valign="bottom" colspan="2">Infants</th></tr><tr><th align="left" valign="bottom">Spatial freq.</th><th align="left" valign="bottom">Meridians</th><th align="left" valign="bottom">Spatial freq.</th><th align="left" valign="bottom">Meridians</th></tr></thead><tbody><tr><td align="left" valign="bottom">s2077_1_1</td><td align="right" valign="bottom">0.85</td><td align="right" valign="bottom">0.77</td><td align="right" valign="bottom">0.89</td><td align="right" valign="bottom">0.81</td></tr><tr><td align="left" valign="bottom">s2097_1_1</td><td align="right" valign="bottom">0.66</td><td align="right" valign="bottom">0.72</td><td align="right" valign="bottom">0.66</td><td align="right" valign="bottom">0.65</td></tr><tr><td align="left" valign="bottom">s4047_1_1</td><td align="right" valign="bottom">0.86</td><td align="right" valign="bottom">0.78</td><td align="right" valign="bottom">0.94</td><td align="right" valign="bottom">0.82</td></tr><tr><td align="left" valign="bottom">s7017_1_3</td><td align="right" valign="bottom">0.9</td><td align="right" valign="bottom">0.92</td><td align="right" valign="bottom">0.93</td><td align="right" valign="bottom">0.92</td></tr><tr><td align="left" valign="bottom">s7047_1_1</td><td align="right" valign="bottom">0.43</td><td align="right" valign="bottom">0.65</td><td align="right" valign="bottom">0.56</td><td align="right" valign="bottom">0.64</td></tr><tr><td align="left" valign="bottom">s7067_1_4</td><td align="right" valign="bottom">0.87</td><td align="right" valign="bottom">0.67</td><td align="right" valign="bottom">0.92</td><td align="right" valign="bottom">0.61</td></tr><tr><td align="left" valign="bottom">s8037_1_2</td><td align="right" valign="bottom">0.92</td><td align="right" valign="bottom">0.73</td><td align="right" valign="bottom">0.93</td><td align="right" valign="bottom">0.83</td></tr><tr><td align="left" valign="bottom">s4607_1_4</td><td align="right" valign="bottom">0.77</td><td align="right" valign="bottom">0.97</td><td align="right" valign="bottom">0.74</td><td align="right" valign="bottom">0.94</td></tr><tr><td align="left" valign="bottom">s1607_1_4</td><td align="right" valign="bottom">0.93</td><td align="right" valign="bottom">0.82</td><td align="right" valign="bottom">0.92</td><td align="right" valign="bottom">0.86</td></tr><tr><td align="left" valign="bottom">s6687_1_4</td><td align="right" valign="bottom">0.87</td><td align="right" valign="bottom">0.9</td><td align="right" valign="bottom">0.93</td><td align="right" valign="bottom">0.93</td></tr><tr><td align="left" valign="bottom">s8687_1_5</td><td align="right" valign="bottom">0.97</td><td align="right" valign="bottom">0.89</td><td align="right" valign="bottom">0.98</td><td align="right" valign="bottom">0.83</td></tr><tr><td align="left" valign="bottom">s6687_1_5</td><td align="right" valign="bottom">0.92</td><td align="right" valign="bottom">0.81</td><td align="right" valign="bottom">0.97</td><td align="right" valign="bottom">0.91</td></tr><tr><td align="left" valign="bottom">s4607_1_7</td><td align="right" valign="bottom">0.85</td><td align="right" valign="bottom">0.91</td><td align="right" valign="bottom">0.8</td><td align="right" valign="bottom">0.86</td></tr><tr><td align="left" valign="bottom">s6687_1_6</td><td align="right" valign="bottom">0.92</td><td align="right" valign="bottom">0.94</td><td align="right" valign="bottom">0.86</td><td align="right" valign="bottom">0.97</td></tr><tr><td align="left" valign="bottom">s8687_1_8</td><td align="right" valign="bottom">0.89</td><td align="right" valign="bottom">0.93</td><td align="right" valign="bottom">0.9</td><td align="right" valign="bottom">0.88</td></tr><tr><td align="left" valign="bottom">Mean</td><td align="right" valign="bottom">0.84</td><td align="right" valign="bottom">0.83</td><td align="right" valign="bottom">0.86</td><td align="right" valign="bottom">0.83</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92119.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dubois</surname><given-names>Jessica</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Inserm Unité NeuroDiderot, Université Paris Cité</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This study presents <bold>valuable</bold> evidence concerning the potential for naturalistic movie-viewing fMRI experiments to reveal some features that are correlated with the functional and topographical organization of the developing visual system in awake infants and toddlers. The data are <bold>compelling</bold> given the difficulty of studying this population, the methodology is original and validated, and the evidence supporting the conclusions is <bold>convincing</bold> and in line with prior research using resting-state and awake task-based fMRI. This study will be of interest to cognitive neuroscientists and developmental psychologists, and in particular those interested in using fMRI to investigate brain organisation in pediatric and clinical populations with limited tolerance to fMRI.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92119.4.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This manuscript reports analyses of fMRI data from infants and toddlers watching naturalistic movies. Visual areas in the infant brain show distinct functions, consistent with previous studies using resting state and awake task-based infant fMRI. The pattern of activity in visual regions contains some features predicted by the regions' retinotopic responses. The revised version of the manuscript provides additional validation of the methodology, and clarifies the claims. As a result, the data provide clear support for the claims.</p><p>Strengths:</p><p>The authors have collected a unique dataset: the same individual infants both watched naturalistic animations and a specific retinotopy task. Using these data position the authors show that activity evoked by movies, in infants' visual areas, is correlated with the regions' retinopic response. The revised manuscript validates this methodology, using adult data. The revised manuscript also shows that an infant's movie watching data is not sufficient or optimal to predict their visual areas' retinotopic responses; anatomical alignment with a group of previous participants provides more accurate prediction of a new participant's retinotopic response.</p><p>Weaknesses:</p><p>A key step in the analysis of the movie-watching data is the selection of independent components of the movie evoked response that resemble retinotopic spatial patterns. While the trained researcher was unlikely to be biased by this infant's own retinotopy, he/she was actively looking for ICs that resemble average patterns of retinotopic response. To show that these ICs didn't arise by chance (i.e. in noise), the authors proposed an additional analysis in the revised manuscript, by misaligning the functional and anatomical data for a subset of participants. This only partially confirms the reliability of the original components, since when the (new) coder tried to be conservative to avoid false components, he/she identified just over half of the 'true' components (13 vs 22 estimated over the group of 6 infants).</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92119.4.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ellis</surname><given-names>Cameron T</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Palo Alto</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Yates</surname><given-names>Tristan S</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Arcaro</surname><given-names>Michael J</given-names></name><role specific-use="author">Author</role><aff><institution>University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name><role specific-use="author">Author</role><aff><institution>Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife Assessment</bold></p><p>This study presents valuable findings on the potential of short-movie viewing fMRI protocol to explore the functional and topographical organization of the visual system in awake infants and toddlers. Although the data are compelling given the difficulty of studying this population, the evidence presented is incomplete and would be strengthened by additional analyses to support the authors' claims. This study will be of interest to cognitive neuroscientists and developmental psychologists, especially those interested in using fMRI to investigate brain organisation in pediatric and clinical populations with limited fMRI tolerance.</p></disp-quote><p>We are grateful for the thorough and thoughtful reviews. We have provided point-bypoint responses to the reviewers’ comments, but first, we summarize the major revisions here. We believe these revisions have substantially improved the clarity of the writing and impact of the results.</p><p>Regarding the framing of the paper, we have made the following major changes in response to the reviews:</p><p>(1) We have clarified that our goal in this paper was to show that movie data contains topographic, fine-grained details of the infant visual cortex. In the revision, we now state clearly that our results should not be taken as evidence that movies could replace retinotopy and have reworded parts of the manuscript that could mislead the reader in this regard.</p><p>(2) We have added extensive details to the (admittedly) complex methods to make them more approachable. An example of this change is that we have reorganized the figure explaining the Shared Response Modelling methods to divide the analytic steps more clearly.</p><p>(3) We have clarified the intermediate products contributing to the results by adding 6 supplementary figures that show the gradients for each IC or SRM movie and each infant participant.</p><p>In response to the reviews, we have conducted several major analyses to support our findings further:</p><p>(1) To verify that our analyses can identify fine-grained organization, we have manually traced and labeled adult data, and then performed the same analyses on them. The results from this additional dataset validate that these analyses can recover fine-grained organization of the visual cortex from movie data.</p><p>(2) To further explore how visual maps derived from movies compare to alternative methods, we performed an anatomical alignment control analysis. We show that high-quality maps can be predicted from other participants using anatomical alignment.</p><p>(3) To test the contribution of motion to the homotopy analyses, we regressed out the motion effects in these analyses. We found qualitatively similar results to our main analyses, suggesting motion did not play a substantial role.</p><p>(4) To test the contribution of data quantity to the homotopy analyses, we correlated the amount of movie data collected from each participant with the homotopy results. We did not find a relationship between data quantity and the homotopy results.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>Ellis et al. investigated the functional and topographical organization of the visual cortex in infants and toddlers, as evidenced by movie-viewing data. They build directly on prior research that revealed topographic maps in infants who completed a retinotopy task, claiming that even a limited amount of rich, naturalistic movie-viewing data is sufficient to reveal this organization, within and across participants. Generating this evidence required methodological innovations to acquire high-quality fMRI data from awake infants (which have been described by this group, and elsewhere) and analytical creativity. The authors provide evidence for structured functional responses in infant visual cortex at multiple levels of analyses; homotopic brain regions (defined based on a retinotopy task) responded more similarly to one another than to other brain regions in visual cortex during movie-viewing; ICA applied to movie-viewing data revealed components that were identifiable as spatial frequency, and to a lesser degree, meridian maps, and shared response modeling analyses suggested that visual cortex responses were similar across infants/toddlers, as well as across infants/toddlers and adults. These results are suggestive of fairly mature functional response profiles in the visual cortex in infants/toddlers and highlight the potential of movie-viewing data for studying finer-grained aspects of functional brain responses, but further evidence is necessary to support their claims and the study motivation needs refining, in light of prior research.</p><p>Strengths:</p><p>- This study links the authors' prior evidence for retinotopic organization of visual cortex in human infants (Ellis et al., 2021) and research by others using movie-viewing fMRI experiments with adults to reveal retinotopic organization (Knapen, 2021).</p><p>- Awake infant fMRI data are rare, time-consuming, and expensive to collect; they are therefore of high value to the community. The raw and preprocessed fMRI and anatomical data analyzed will be made publicly available.</p></disp-quote><p>We are grateful to the reviewer for their clear and thoughtful description of the strengths of the paper, as well as their helpful outlining of areas we could improve.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>- The Methods are at times difficult to understand and in some cases seem inappropriate for the conclusions drawn. For example, I believe that the movie-defined ICA components were validated using independent data from the retinotopy task, but this was a point of confusion among reviewers.</p></disp-quote><p>We acknowledge the complexity of the methods and wish to clarify them as best as possible for the reviewers and the readers. We have extensively revised the methods and results sections to help avoid potential misunderstandings. For instance, we have revamped the figure and caption describing the SRM pipeline (Figure 5).</p><p>To answer the stated confusion directly, the ICA components were derived from the movie data and validated on the (completely independent) retinotopy data. There were no additional tasks. The following text in the paper explains this point:</p><p>“To assess the selected component maps, we correlated the gradients (described above) of the task-evoked and component maps. This test uses independent data: the components were defined based on movie data and validated against task-evoked retinotopic maps.” Pg. 11</p><disp-quote content-type="editor-comment"><p>In either case: more analyses should be done to support the conclusion that the components identified from the movie reproduce retinotopic maps for example, by comparing the performance of movie-viewing maps to available alternatives (anatomical ROIs, group-defined ROIs).</p></disp-quote><p>Before addressing this suggestion, we want to restate our conclusions: features of the retinotopic organization of infant visual cortex could be predicted from movie data. We did not conclude that movie data could ‘reproduce’ retinotopic maps in the sense that they would be a replacement. We recognize that this was not clear in our original manuscript and have clarified this point throughout, including in this section of the discussion:</p><p>“To be clear, we are not suggesting that movies work well enough to replace a retinotopy task when accurate maps are needed. For instance, even though ICA found components that were highly correlated with the spatial frequency map, we also selected some components that turned out to have lower correlations. Without knowing the ground truth from a retinotopy task, there would be no way to weed these out. Additionally, anatomical alignment (i.e., averaging the maps from other participants and anatomically aligning them to a held-out participant) resulted in maps that were highly similar to the ground truth. Indeed, we previously(Henriksson et al., 2008) found that adult-defined visual areas were moderately similar to infants. While functional alignment with adults can outperform anatomical alignment methods in similar analyses (Lu et al., 2017), here we find that functional alignment is inferior to anatomical alignment. Thus, if the goal is to define visual areas in an infant that lacks task-based retinotopy, anatomical alignment of other participants’ retinotopic maps is superior to using movie-based analyses, at least as we tested it.” Pg. 21</p><p>As per the reviewer’s suggestion and alluded to in the paragraph above, we have created anatomically aligned visual maps, providing an analogous test to the betweenparticipant analyses like SRM. We find that these maps are highly similar to the ground truth. We describe this result in a new section of the results:</p><p>“We performed an anatomical alignment analog of the functional alignment (SRM) approach. This analysis serves as a benchmark for predicting visual maps using taskbased data, rather than movie data, from other participants. For each infant participant, we aggregated all other infant or adult participants as a reference. The retinotopic maps from these reference participants were anatomically aligned to the standard surface template, and then averaged. These averages served as predictions of the maps in the test participant, akin to SRM, and were analyzed equivalently (i.e., correlating the gradients in the predicted map with the gradients in the task-based map). These correlations (Table S4) are significantly higher than for functional alignment (using infants to predict spatial frequency, anatomical alignment &gt; functional alignment: ∆<sub>Fisher Z</sub> M=0.44, CI=[0.32–0.58], p&lt;.001; using infants to predict meridians, anatomical alignment &gt; functional alignment: ∆<sub>Fisher Z</sub> M=0.61, CI=[0.47–0.74], p&lt;.001; using adults to predict spatial frequency, anatomical alignment &gt; functional alignment: ∆<sub>Fisher Z</sub> M=0.31, CI=[0.21–0.42], p&lt;.001; using adults to predict meridians, anatomical alignment &gt; functional alignment: ∆<sub>Fisher Z</sub> M=0.49, CI=[0.39–0.60], p&lt;.001). This suggests that even if SRM shows that movies can be used to produce retinotopic maps that are significantly similar to a participant, these maps are not as good as those that can be produced by anatomical alignment of the maps from other participants without any movie data.” Pg. 16–17</p><disp-quote content-type="editor-comment"><p>Also, the ROIs used for the homotopy analyses were defined based on the retinotopic task rather than based on movie-viewing data alone - leaving it unclear whether movie-viewing data alone can be used to recover functionally distinct regions within the visual cortex.</p></disp-quote><p>We agree with the reviewer that our approach does not test whether movie-viewing data alone can be used to recover functionally distinct regions. The goal of the homotopy analyses was to identify whether there was functional differentiation of visual areas in the infant brain while they watch movies. This was a novel question that provides positive evidence that these regions are functionally distinct. In subsequent analyses, we show that when these areas are defined anatomically, rather than functionally, they also show differentiated function (e.g., Figure 2). Nonetheless, our intention was not to use the homotopy analyses to define the regions. We have added text to clarify the goal and novelty of this analysis.</p><p>“Although these analyses cannot define visual maps, they test whether visual areas have different functional signatures.” Pg. 6</p><p>Additionally, even if the goal were to define areas based on homotopy, we believe the power of that analysis would be questionable. We would need to use a large amount of the movie data to define the areas, leaving a low-powered dataset to test whether their function is differentiated by these movie-based areas.</p><disp-quote content-type="editor-comment"><p>- The authors previously reported on retinotopic organization of the visual cortex in human infants (Ellis et al., 2021) and suggest that the feasibility of using movie-viewing experiments to recover these topographic maps is still in question. They point out that movies may not fully sample the stimulus parameters necessary for revealing topographic maps/areas in the visual cortex, or the time-resolution constraints of fMRI might limit the use of movie stimuli, or the rich, uncontrolled nature of movies might make them inferior to stimuli that are designed for retinotopic mapping, or might lead to variable attention between participants that makes measuring the structure of visual responses across individuals challenging. This motivation doesn't sufficiently highlight the importance or value of testing this question in infants. Further, it's unclear if/how this motivation takes into account prior research using movie-viewing fMRI experiments to reveal retinotopic organization in adults (e.g., Knapen, 2021). Given the evidence for retinotopic organization in infants and evidence for the use of movie-viewing experiments in adults, an alternative framing of the novel contribution of this study is that it tests whether retinotopic organization is measurable using a limited amount of movie-viewing data (i.e., a methodological stress test). The study motivation and discussion could be strengthened by more attention to relevant work with adults and/or more explanation of the importance of testing this question in infants (is the reason to test this question in infants purely methodological - i.e., as a way to negate the need for retinotopic tasks in subsequent research, given the time constraints of scanning human infants?).</p></disp-quote><p>We are grateful to the reviewer for giving us the opportunity to clarify the innovations of this research. We believe that this research contributes to our understanding of how infants process dynamic stimuli, demonstrates the viability and utility of movie experiments in infants, and highlights the potential for new movie-based analyses (e.g., SRM). We have now consolidated these motivations in the introduction to more clearly motivate this work:</p><p>“The primary goal of the current study is to investigate whether movie-watching data recapitulates the organization of visual cortex. Movies drive strong and naturalistic responses in sensory regions while minimizing task demands (Nastase et al., 2020; Finn et al., 2022; Ellis et al., 2021) and thus are a proxy for typical experience. In adults, movies and resting-state data have been used to characterize the visual cortex in a data-driven fashion (Loiotile et al., 2019; Knapen, 2021; Lu et al., 2017). Movies have been useful in awake infant fMRI for studying event segmentation (Guntupalli et al., 2016), functional alignment (Yates et al., 2022), and brain networks (Turek et al., 2018). However, this past work did not address the granularity and specificity of cortical organization that movies evoke. For example, movies evoke similar activity in infants in anatomically aligned visual areas (Guntupalli et al., 2016), but it remains unclear whether responses to movie content differ between visual areas (e.g., is there more similarity of function within visual areas than between [Yates et al., 2023]). Moreover, it is unknown whether structure within visual areas, namely visual maps, contributes substantially to visual evoked activity. Additionally, we wish to test whether methods for functional alignment can be used with infants. Functional alignment finds a mapping between participants using functional activity – rather than anatomy – and in adults can improve signal-to-noise, enhance across participant prediction, and enable unique analyses (Lu et al., 2017; Li et al., 2022; Busch et al., 2021; Chen et al., 2025)].” Pg. 3-4</p><p>Furthermore, the introduction culminates in the following statement on what the analyses will tell us about the nature of movie-driven activity in infants:</p><p>“These three analyses assess key indicators of the mature visual system: functional specialization between areas, organization within areas, and consistency between individuals.” Pg. 5</p><p>Furthermore, in the discussion we revisit these motivations and elaborate on them further:</p><p>[Regarding homotopy:] “This suggests that visual areas are functionally differentiated in infancy and that this function is shared across hemispheres (Yates et al., 2023).” Pg. 19</p><p>[Regarding ICA:] “This means that the retinotopic organization of the infant brain accounts for a detectable amount of variance in visual activity, otherwise components resembling these maps would not be discoverable.” Pg. 19–20</p><p>[Regarding SRM:] “This is initial evidence that functional alignment may be useful for enhancing signal quality, like it has in adults (Lu et al., 2017; Li et al., 2022; Busch et al., 2021), or revealing changing function over development (Yates et al., 2021).” Pg. 21</p><p>Additionally, we have expanded our discussion of relevant work that uses similar methods such as the excellent research from Knapen (2021) and others:</p><p>“In adults, movies and resting-state data have been used to characterize the visual cortex in a data-driven fashion (Loiotile et al., 2019; Knapen, 2021; Lu et al., 2017).” Pg. 4</p><p>“We next explored whether movies can reveal fine-grained organization within visual areas by using independent components analysis (ICA) to propose visual maps in individual infant brains (Loiotile et al., 2019; Knapen, 2021; Kumar et al., 2020; Beckmann et al., 2005; Moeller et al., 2009).” Pg. 9</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This manuscript shows evidence from a dataset with awake movie-watching in infants, that the infant brain contains areas with distinct functions, consistent with previous studies using resting state and awake task-based infant fMRI. However, substantial new analyses would be required to support the novel claim that movie-watching data in infants can be used to identify retinotopic areas or to capture within-area functional organization.</p><p>Strengths:</p><p>The authors have collected a unique dataset: the same individual infants both watched naturalistic animations and a specific retinotopy task. These data position the authors to test their novel claim, that movie-watching data in infants can be used to identify retinotopic areas.</p><p>Weaknesses:</p><p>To claim that movie-watching data can identify retinotopic regions, the authors should provide evidence for two claims:</p><p>- Retinotopic areas defined based only on movie-watching data, predict retinotopic responses in independent retinotopy-task-driven data.</p><p>- Defining retinotopic areas based on the infant's own movie-watching response is more accurate than alternative approaches that don't require any movie-watching data, like anatomical parcellations or shared response activation from independent groups of participants.</p></disp-quote><p>We thank the reviewer for their comments. Before addressing their suggestions, we wish to clarify that we do not claim that movie data can be used to identify retinotopic areas, but instead that movie data captures components of the within and between visual area organization as defined by retinotopic mapping. We recognize that this was not clear in our original manuscript and have clarified this point throughout, including in this section of the discussion:</p><p>“To be clear, we are not suggesting that movies work well enough to replace a retinotopy task when accurate maps are needed. For instance, even though ICA found components that were highly correlated with the spatial frequency map, we also selected some components that turned out to have lower correlations. Without knowing the ground truth from a retinotopy task, there would be no way to weed these out. Additionally, anatomical alignment (i.e., averaging the maps from other participants and anatomically aligning them to a held-out participant) resulted in maps that were highly similar to the ground truth. Indeed, we previously (Henriksson et al., 2008) found that adult-defined visual areas were moderately similar to infants. While functional alignment with adults can outperform anatomical alignment methods in similar analyses (Lu et al., 2017), here we find that functional alignment with infants is inferior to anatomical alignment. Thus, if the goal is to define visual areas in an infant that lacks task-based retinotopy, anatomical alignment of other participants’ retinotopic maps is superior to using movie-based analyses, at least as we tested it.” Pg. 21</p><p>In response to the reviewer’s suggestion, we compare the maps identified by SRM to the averaged, anatomically aligned maps from infants. We find that these maps are highly similar to the task-based ground truth and we describe this result in a new section:</p><p>“We performed an anatomical alignment analog of the functional alignment (SRM) approach. This analysis serves as a benchmark for predicting visual maps using taskbased data, rather than movie data, from other participants. For each infant participant, we aggregated all other infant or adult participants as a reference. The retinotopic maps from these reference participants were anatomically aligned to the standard surface template, and then averaged. These averages served as predictions of the maps in the test participant, akin to SRM, and were analyzed equivalently (i.e., correlating the gradients in the predicted map with the gradients in the task-based map). These correlations (Table S4) are significantly higher than for functional alignment (using infants to predict spatial frequency, anatomical alignment &lt; functional alignment: ∆<sub>Fisher Z</sub> M=0.44, CI=[0.32–0.58], p&lt;.001; using infants to predict meridians, anatomical alignment &lt; functional alignment: ∆<sub>Fisher Z</sub> M=0.61, CI=[0.47–0.74], p&lt;.001; using adults to predict spatial frequency, anatomical alignment &lt; functional alignment: ∆<sub>Fisher Z</sub> M=0.31, CI=[0.21–0.42], p&lt;.001; using adults to predict meridians, anatomical alignment &lt; functional alignment: ∆<sub>Fisher Z</sub> M=0.49, CI=[0.39–0.60], p&lt;.001). This suggests that even if SRM shows that movies can be used to produce retinotopic maps that are significantly similar to a participant, these maps are not as good as those that can be produced by anatomical alignment of the maps from other participants without any movie data.” Pg. 16–17</p><p>Note that we do not compare the anatomically aligned maps with the ICA maps statistically. This is because these analyses are not comparable: ICA is run withinparticipant whereas anatomical alignment is necessarily between-participant — either infant or adults. Nonetheless, an interested reader can refer to the Table where we report the results of anatomical alignment and see that anatomical alignment outperforms ICA in terms of the correlation between the predicted and task-based maps.</p><disp-quote content-type="editor-comment"><p>Both of these analyses are possible, using the (valuable!) data that these authors have collected, but these are not the analyses that the authors have done so far. Instead, the authors report the inverse of (1): regions identified by the retinotopy task can be used to predict responses in the movies. The authors report one part of (2), shared responses from other participants can be used to predict individual infants' responses in the movies, but they do not test whether movie data from the same individual infant can be used to make better predictions of the retinotopy task data, than the shared response maps.</p><p>So to be clear, to support the claims of this paper, I recommend that the authors use the retinotopic task responses in each individual infant as the independent &quot;Test&quot; data, and compare the accuracy in predicting those responses, based on:</p><p>- The same infant's movie-watching data, analysed with MELODIC, when blind experimenters select components for the SF and meridian boundaries with no access to the ground-truth retinotopy data.</p><p>- Anatomical parcellations in the same infant.</p><p>- Shared response maps from groups of other infants or adults.</p><p>- (If possible, ICA of resting state data, in the same infant, or from independent groups of infants).</p><p>Or, possibly, combinations of these techniques.</p><p>If the infant's own movie-watching data leads to improved predictions of the infant's retinotopic task-driven response, relative to these existing alternatives that don't require movie-watching data from the same infant, then the authors' main claim will be supported.</p></disp-quote><p>These are excellent suggestions for additional analyses to test the suitability for moviebased maps to replace task-based maps. We hope it is now clear that it was never our intention to claim that movie-based data could replace task-based methods. We want to emphasize that the discoveries made in this paper — that movies evoke fine-grained organization in infant visual cortex — do not rely on movie-based maps being better than alternative methods for producing maps, such as the newly added anatomical alignment.</p><disp-quote content-type="editor-comment"><p>The proposed analysis above solves a critical problem with the analyses presented in the current manuscript: the data used to generate maps is identical to the data used to validate those maps. For the task-evoked maps, the same data are used to draw the lines along gradients and then test for gradient organization. For the component maps, the maps are manually selected to show the clearest gradients among many noisy options, and then the same data are tested for gradient organization. This is a double-dipping error. To fix this problem, the data must be split into independent train and test subsets.</p></disp-quote><p>We appreciate the reviewer’s concern; however, we believe it is a result of a miscommunication in our analytic strategy. We have now provided more details on the analyses to clarify how double-dipping was avoided.</p><p>To summarize, a retinotopy task produced visual maps that were used to trace both area boundaries and gradients across the areas. These data were then fixed and unchanged, and we make no claims about the nature of these maps in this paper, other than to treat them as the ground truth to be used as a benchmark in our analyses. The movie data, which are collected independently from the same infant in the session, used the boundaries from the retinotopy task (in the case of homotopy) or were compared with the maps from the retinotopy task (in the case of ICA and SRM). In other words, the statement that “the data used to generate maps is identical to the data used to validate those maps” is incorrect because we generated the maps with a retinotopy task and validated the maps with the movie data. This means no double dipping occurred.</p><p>Perhaps a cause of the reviewer’s interpretation is that the gradients used in the analysis are not clearly described. We now provide this additional description: “Using the same manually traced lines from the retinotopy task, we measured the intensity gradients in each component from the movie-watching data. We can then use the gradients of intensity in the retinotopy task-defined maps as a benchmark for comparison with the ICA-derived maps.” Pg. 10</p><p>Regarding the SRM analyses, we take great pains to avoid the possibility of data contamination. To emphasize how independent the SRM analysis is, the prediction of the retinotopic map from the test participant does not use their retinotopy data at all; in fact, the predicted maps could be made before that participant’s retinotopy data were ever collected. To make this prediction for a test participant, we need to learn the inversion of the SRM, but this only uses the movie data of the test participant. Hence, there is no double-dipping in the SRM analyses. We have elaborated on this point in the revision, and we remade the figure and its caption to clarify this point:</p><p>We also have updated the description of these results to emphasize how double-dipping was avoided:</p><p>“We then mapped the held-out participant's movie data into the learned shared space without changing the shared space (Figure 5c). In other words, the shared response model was learned and frozen before the held-out participant’s data was considered.</p><p>This approach has been used and validated in prior SRM studies (Yates et al., 2021).” Pg. 14</p><p>The reviewer suggests that manually choosing components from ICA is double-dipping. Although the reviewer is correct that the manual selection of components in ICA means that the components chosen ought to be good candidates, we are testing whether those choices were good by evaluating those components against the task-based maps that were not used for the ICA. Our statistical analyses evaluate whether the components chosen were better than the components that would have been chosen by random chance. Critically: all decisions about selecting the components happen before the components are compared to the retinotopic maps. Hence there is no double-dipping in the selection of components, as the choice of candidate ICA maps is not informed by the ground-truth retinotopic maps. We now clarify what the goal of this process is in the results:</p><p>“Success in this process requires that (1) retinotopic organization accounts for sufficient variance in visual activity to be identified by ICA and (2) experimenters can accurately identify these components.” Pg. 10</p><p>The reviewer also alludes to a concern that the researcher selecting the maps was not blind to the ground-truth retinotopic maps from participants and this could have influenced the results. In such a scenario, the researcher could have selected components that have the gradients of activity in the places that the infant has as ground truth. The researcher who made the selection of components (CTE) is one of the researchers who originally traced the areas in the participants approximately a year prior to the identification of ICs. The researcher selecting the components didn’t use the ground-truth retinotopic maps as reference, nor did they pay attention to the participant IDs when sorting the IC components. Indeed, they weren’t trying to find participant specific maps per se, but rather aimed to find good candidate retinotopic maps in general. In the case of the newly added adult analyses, the ICs were selected before the retinotopic mapping was reviewed or traced; hence, no knowledge about the participant-specific ground truth could have influenced the selection of ICs. Even with this process from adults, we find results of comparable strength as we found in infants, as shown below. Nonetheless, there is a possibility that this researcher’s previous experience of tracing the infant maps could have influenced their choice of components at the participant-specific level. If so, it was a small effect since the components the researcher selected were far from the best possible options (i.e., rankings of the selected components averaged in the 64th percentile for spatial frequency maps and the 68th percentile for meridian maps). We believe all reasonable steps were taken to mitigate bias in the selection of ICs.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>The manuscript reports data collected in awake toddlers recording BOLD while watching videos. The authors analyse the BOLD time series using two different statistical approaches, both very complex but do not require any a priori determination of the movie features or contents to be associated with regressors. The two main messages are that (1) toddlers have occipital visual areas very similar to adults, given that an SRM model derived from adult BOLD is consistent with the infant brains as well; (2) the retinotopic organization and the spatial frequency selectivity of the occipital maps derived by applying correlation analysis are consistent with the maps obtained by standard and conventional mapping.</p><p>Clearly, the data are important, and the author has achieved important and original results. However, the manuscript is totally unclear and very difficult to follow; the figures are not informative; the reader needs to trust the authors because no data to verify the output of the statistical analysis are presented (localization maps with proper statistics) nor so any validation of the statistical analysis provided. Indeed what I think that manuscript means, or better what I understood, may be very far from what the authors want to present, given how obscure the methods and the result presentation are.</p><p>In the present form, this reviewer considers that the manuscript needs to be totally rewritten, the results presented each technique with appropriate validation or comparison that the reader can evaluate.</p></disp-quote><p>We are grateful to the reviewer for the chance to improve the paper. We have broken their review into three parts: clarification of the methods, validation of the analyses, and enhancing the visualization.</p><p>Clarification of the methods</p><p>We acknowledge that the methods we employed are complex and uncommon in many fields of neuroimaging. That said, numerous papers have conducted these analyses on adults (Beckman et al., 2005; Butt et al., 2015; Guntupalli et al., 2016; Haak &amp; Beckman, 2018; Knapen, 2021; Lu et al., 2017) and non-human primates (Arcaro &amp; Livingstone, 2017; Moeller et al., 2009). We have redoubled our efforts in the revision to make the methods as clear as possible, expanding on the original text and providing intuitions where possible. These changes have been added throughout and are too vast in number to repeat here, especially without context, but we hope that readers will have an easier time following the analyses now.</p><p>Additionally, we updated Figures 3 and 5 in which the main ICA and SRM analyses are described. For instance, in Figure 3’s caption we now add details about how the gradient analyses were performed on the components:</p><p>“We used the same lines that were manually traced on the task-evoked map to assess the change in the component’s response. We found a monotonic trend within area from medial to lateral, just like we see in the ground truth.” Pg. 11</p><p>Regarding Figure 5, we reconsidered the best way to explain the SRM analyses and decided it would be helpful to partition the diagram into steps, reflecting the analytic process. These updates have been added to Figure 5, and the caption has been updated accordingly.</p><p>We hope that these changes have improved the clarity of the methods. For readers interested in learning more, we encourage them to either read the methods-focused papers that debut the analyses (e.g., Chen et al., 2015), read the papers applying the methods (e.g., Guntupalli et al., 2016), or read the annotated code we publicly release which implements these pipelines and can be used to replicate the findings.</p><p>Validation of the analyses</p><p>One of the requests the reviewer makes is to validate our analyses. Our initial approach was to lean on papers that have used these methods in adults or primates (e.g., Arcaro, &amp; Livingstone, 2017; Beckman et al., 2005; Butt et al., 2015; Guntupalli et al., 2016; Haak &amp; Beckman, 2018; Knapen, 2021; Moeller et al., 2009) where the underlying organization and neurophysiology is established. However, we have made changes to these methods that differ from their original usage (e.g., we used SRM rather than hyperalignment, we use meridian mapping rather than traveling wave retinotopy, we use movie-watching data rather than rest). Hence, the specifics of our design and pipeline warrant validation.</p><p>To add further validation, we have rerun the main analyses on an adult sample. We collected 8 adult participants who completed the same retinotopy task and a large subset of the movies that infants saw. These participants were run under maximally similar conditions to infants (i.e., scanned using the same parameters and without the top of the head-coil) and were preprocessed using the same pipeline. Given that the relationship between adult visual maps and movie-driven (or resting-state) analyses has been shown in many studies (Beckman et al., 2005; Butt et al., 2015; Guntupalli et al., 2016; Haak &amp; Beckman, 2018; Knapen, 2021; Lu et al., 2017), these adult data serve as a validation of our analysis pipeline. These adult participants were included in the original manuscript; however, they were previously only used to support the SRM analyses (i.e., can adults be used to predict infant visual maps). The adult results are described before any results with infants, as a way to engender confidence. Moreover, we have provided new supplementary figures of the adult results that we hope will be integrated with the article when viewing it online, such that it will be easy to compare infant and adult results, as per the reviewer’s request.</p><p>As per the figures and captions below, the analyses were all successful with the adult participants: (1) Homotopic correlations are higher than correlations between comparable areas in other streams or areas that are more distant within stream. (2) A multidimensional scaling depiction of the data shows that areas in the dorsal and ventral stream are dissimilar. (3) Using independent components analysis on the movie data, we identified components that are highly correlated with the retinotopy task-based spatial frequency and meridian maps. (4) Using shared response modeling on the movie data, we predicted maps that are highly correlated with the retinotopy task-based spatial frequency and meridian maps.</p><p>These supplementary analyses are underpowered for between-group comparisons, so we do not statistically compare the results between infants and adults. Nonetheless, the pattern of adult results is comparable overall to the infant results.</p><p>We believe these adult results provide a useful validation that the infant analyses we performed can recover fine-grained organization.</p><p>Enhancing the visualization</p><p>The reviewer raises an additional concern about the lack of visualization of the results. We recognize that the plots of the summary statistics do not provide information about the intermediate analyses. Indeed, we think the summary statistics can understate the degree of similarity between the components or predicted visual maps and the ground truth. Hence, we have added 6 new supplementary figures showing the intensity gradients for the following analyses: 1. spatial frequency prediction using ICA, 2. meridian prediction using ICA, 3. spatial frequency prediction using infant SRM, 4. meridian prediction using infant SRM, 5. spatial frequency prediction using adult SRM, and 6. meridian prediction using adult SRM.</p><p>We hope that these visualizations are helpful. It is possible that the reviewer wishes us to also visually present the raw maps from the ICA and SRM, akin to what we show in Figure 3A and 3B. We believe this is out of scope of this paper: of the 1140 components that were identified by ICA, we selected 36 for spatial frequency and 17 for meridian maps. We also created 20 predicted maps for spatial frequency and 20 predicted meridian maps using SRM. This would result in the depiction of 93 subfigures, requiring at least 15 new full-page supplementary figures to display with adequate resolution. Instead, we encourage the reader to access this content themselves: we have made the code to recreate the analyses publicly available, as well as both the raw and preprocessed data for these analyses, including the data for each of these selected maps.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) As mentioned in the public review, the authors should consider incorporating relevant adult fMRI research into the Introduction and explain the importance of testing this question in infants.</p></disp-quote><p>Our public response describes the several citations to relevant adult research we have added, and have provided further motivation for the project.</p><disp-quote content-type="editor-comment"><p>(2) The authors should conduct additional analyses to support their conclusion that movie data alone can generate accurate retinotopic maps (i.e., by comparing this approach to other available alternatives).</p></disp-quote><p>We have clarified in our public response that we did not wish to conclude that movie data alone can generate accurate retinotopic maps, and have made substantial edits to the text to emphasize this. Thus, because this claim is already not supported by our analyses, we do not think it is necessary to test it further.</p><disp-quote content-type="editor-comment"><p>(3) The authors should re-do the homotopy analyses using movie-defined ROIs (i.e., by splitting the movie-viewing data into independent folds for functional ROI definition and analyses).</p></disp-quote><p>As stated above, defining ROIs based on the movie content is not the intended goal of this project. Even if that were the general goal, we do not believe that it would be appropriate to run this specific analysis with the data we collected. Firstly, halving the data for ROI definition (e.g., using half the movie data to identify and trace areas, and then use those areas in the homotopy analysis to run on the other half of data) would qualitatively change the power of the analyses described here. Secondly, we would be unable to define areas beyond hV4/V3AB with confidence, since our retinotopic mapping only affords specification of early visual cortex. Thus we could not conduct the MDS analyses shown in Figure 2.</p><disp-quote content-type="editor-comment"><p>(4) If the authors agree that a primary contribution of this study and paper is to showcase what is possible to do with a limited amount of movie-viewing data, then they should make it clearer, sooner, how much usable movie data they have from infants. They could also consider conducting additional analyses to determine the minimum amount of fMRI data necessary to reveal the same detailed characteristics of functional responses in the visual cortex.</p></disp-quote><p>We agree it would be good to highlight the amount of movie data used. When the infant data is first introduced in the results section, we now state the durations:</p><p>“All available movies from each session were included (Table S2), with an average duration of 540.7s (range: 186-1116s).” Pg. 5</p><p>Additionally, we have added a homotopy analysis that describes the contribution of data quantity to the results observed. We compare the amount of data collected with the magnitude of same vs. different stream effect (Figure 1B) and within stream distance effect (Figure 1C). We find no effect of movie duration in the sample we tested, as reported below:</p><p>“We found no evidence that the variability in movie duration per participant correlated with this difference [of same stream vs. different stream] (r=0.08, p=.700).” Pg. 6-7</p><p>“There was no correlation between movie duration and the effect (Same &gt; Adjacent: r=-0.01, p=.965, Adjacent &gt; Distal: r=-0.09, p=.740).” Pg. 7</p><disp-quote content-type="editor-comment"><p>(5) If any of the methodological approaches are novel, the authors should make this clear. In particular, has the approach of visually inspecting and categorizing components generated from ICA and movie data been done before, in adults/other contexts?</p></disp-quote><p>The methods we employed are similar to others, as described in the public review.</p><p>However, changes were necessary to apply them to infant samples. For instance, Guntupalli et al. (2016) used hyperalignment to predict the visual maps of adult participants, whereas we use SRM. SRM and hyperalignment have the same goal — find a maximally aligned representation between participants based on brain function — but their implementation is different. The application of functional alignment to infants is novel, as is their use in movie data that is relatively short by comparison to standard adult data. Indeed, this is the most thorough demonstration that SRM — or any functional alignment procedure — can be usefully applied to infant data, awake or sleeping. We have clarified this point in the discussion.</p><p>“This is initial evidence that functional alignment may be useful for enhancing signal quality, like it has in adults (Lu et al., 2017; Li et al., 2022; Busch et al., 2021), or revealing changing function over development (Yates et al., 2021), which may prove especially useful for infant fMRI (Ellis and Turk-Browne, 2018).” Pg. 21</p><disp-quote content-type="editor-comment"><p>(6) The authors found that meridian maps were less identifiable from ICA and movie data and suggest that this may be because these maps are more susceptible to noise or gaze variability. If this is the case, you might predict that these maps are more identifiable in adult data. The authors could consider running additional analyses with their adult participants to better understand this result.</p></disp-quote><p>As described in the manuscript, we hypothesize that meridian maps are more difficult to identify than spatial frequency maps because meridian maps are a less smooth, more fine-grained map than spatial frequency. Indeed, it has previously been reported (Moeller et al., 2009) that similar procedures can result in meridian maps that are constituted by multiple independent components (e.g., a component sensitive to horizontal orientations, and a separate component sensitive to vertical components). Nonetheless, we have now conducted the ICA procedure on adult participants and again find it is easier to identify spatial frequency components compared to meridian maps, as reported in the public review.</p><disp-quote content-type="editor-comment"><p>Minor corrections:</p><p>(1) Typo: Figure 3 title: &quot;Example retintopic task vs. ICA-based spatial frequency maps.&quot;.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>(2) Given the age range of the participants, consider using &quot;infants and toddlers&quot;? (Not to diminish the results at all; on the contrary, I think it is perhaps even more impressive to obtain awake fMRI data from ~1-2-year-olds). Example: Figure 3 legend: &quot;(A) Spatial frequency map of a 17.1-monthold infant.&quot;.</p></disp-quote><p>We agree with the reviewer that there is disagreement about the age range at which a child starts being considered a toddler. We have changed the terms in places where we refer to a toddler in particular (e.g., the figure caption the reviewer highlights) and added the phrase “infants and toddlers” in places where appropriate. Nonetheless, we have kept “infants” in some places, particularly those where we are comparing the sample to adults. Adding “and toddlers” could imply three samples being compared which would confuse the reader.</p><disp-quote content-type="editor-comment"><p>(3) Figure 6 legend: The following text should be omitted as there is no bar plot in this figure: &quot;The bar plot is the average across participants. The error bar is the standard error across participants.&quot;.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>(4) Table S1 legend: Missing first single quote: Runs'.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>I request that this paper cite more of the existing literature on the fMRI of human infants and toddlers using task-driven and resting-state data. For example, early studies by (first authors) Biagi, Dehaene-Lambertz, Cusack, and Fransson, and more recent studies by Chen, Cabral, Truzzi, Deen, and Kosakowski.</p></disp-quote><p>We have added several new citations of recent task-based and resting state studies to the second sentence of the main text:</p><p>“Despite the recent growth in infant fMRI (Biagi et al., 2015; Biagi et al., 202 3; Cabral et al, 2022; Deen et al., 2017; Kosakowski et al., 2021; Truzzi and Cusack, 2023), one of the most important obstacles facing this research is that infants are unable to maintain focus for long periods of time and struggle to complete traditional cognitive tasks (Ellis et al., 2020).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>In the following, I report some of my main perplexities, but many more may arise when the material is presented more clearly.</p><p>The age of the children varies from 5 months to about 2 years. While the developmental literature suggests that between 1 and 2 years children have a visual system nearly adult-like, below that age some areas may be very immature. I would split the sample and perhaps attempt to validate the adult SRM model with the youngest children (and those can be called infants).</p></disp-quote><p>We recognize the substantial age variability in our sample, which is why we report participant-specific data in our figures. While splitting up the data into age bins might reveal age effects, we do not think we can perform adequately powered null hypothesis testing of the age trend. In order to investigate the contribution of age, larger samples will be needed. That said, we can see from the data that we have reported that any effect of age is likely small. To elaborate: Figures 4 and 6 report the participant-specific data points and order the participants by age. There are no clear linear trends in these plots, thus there are no strong age effects.</p><p>More broadly, we do not think there is a principled way to divide the participants by age. The reviewer suggests that the visual system is immature before the first year of life and mature afterward; however, such claims are the exact motivation for the type of work we are doing here, and the verdict is still out. Indeed, the conclusion of our earlier work reporting retinotopy in infants (Ellis et al., 2021) suggests that the organization of the early visual cortex in infants as young as 5 months — the youngest infant in our sample — is surprisingly adult-like.</p><disp-quote content-type="editor-comment"><p>The title cannot refer to infants given the age span.</p></disp-quote><p>There is disagreement in the field about the age at which it is appropriate to refer to children as infants. In this paper, and in our prior work, we followed the practice of the most attended infant cognition conference and society, the International Congress of Infant Studies (ICIS), which considers infants as those aged between 0-3 years old, for the purposes of their conference. Indeed, we have never received this concern across dozens of prior reviews for previous papers covering a similar age range. That said, we understand the spirit of the reviewer’s comment and now refer to the sample as “infants and toddlers” and to older individuals in our sample as “toddlers” wherever it is appropriate (the younger individuals would fairly be considered “infants” under any definition).</p><disp-quote content-type="editor-comment"><p>Figure 1 is clear and an interesting approach. Please also show the average correlation maps on the cortical surface.</p></disp-quote><p>While we would like to create a figure as requested, we are unsure how to depict an area-by-area correlation map on the cortical surface. One option would be to generate a seed-based map in which we take an area and depict the correlation of that seed (e.g., vV1) with all other voxels. This approach would result in 8 maps for just the task-defined areas, and 17 maps for anatomically-defined areas. Hence, we believe this is out of scope of this paper, but an interested reader could easily generate these maps from the data we have released.</p><disp-quote content-type="editor-comment"><p>Figure 2 results are not easily interpretable. Ventral and dorsal V1-V3 areas represent upper or lower VF respectively. Higher dorsal and ventral areas represent both upper and lower VF, so we should predict an equal distance between the two streams. Again, how can we verify that it is not a result of some artifacts?</p></disp-quote><p>In adults, visual areas differ in their functional response properties along multiple dimensions, including spatial coding. The dorsal/ventral stream hypothesis is derived from the idea that areas in each stream support different functions, independent of spatial coding. The MDS analysis did not attempt to isolate the specific contribution of spatial representations of each area but instead tested the similarity of function that is evoked in naturalistic viewing. Other covariance-based analyses specifically isolate the contribution of spatial representations (Haak et al., 2013); however, they use a much more constrained analysis than what was implemented here. The fact that we find broad differentiation of dorsal and ventral visual areas in infants is consistent with adults (Haak &amp; Beckman, 2018) and neonate non-human primates (Arcaro &amp; Livingstone, 2017).</p><p>Nonetheless, we recognize that we did not mention the differences in visual field properties across areas and what that means. If visual field properties alone drove the functional response then we would expect to see a clustering of areas based on the visual field they represent (e.g., hV4 and V3AB should have similar representations). Since we did not see that, and instead saw organization by visual stream, the result is interesting and thus warrants reporting. We now mention this difference in visual fields in the manuscript to highlight the surprising nature of the result.</p><p>“This separation between streams is striking when considering that it happens <italic>despite</italic> differences in visual field representations across areas: while dorsal V1 and ventral V1 represent the lower and upper visual field, respectively, V3A/B and hV4 both have full visual field maps. These visual field representations can be detected in adults (Haak et al., 2013); however, they are often not the primary driver of function (Haak and Beckmann, 2018). We see that in infants too: hV4 and V3A/B represent the same visual space yet have distinct functional profiles.” Pg. 8</p><p>The reviewer raises a concern that the MDS result may be spurious and caused by noise. Below, we present three reasons why we believe these results are not accounted for by artifacts but instead reflect real functional differentiation in the visual cortex.</p><p>(1) Figure 2 is a visualization of the similarity matrix presented in Figure S1. In Figure S1, we report the significance testing we performed to confirm that the patterns differentiating dorsal and ventral streams — as well as adjacent areas from distal areas — are statistically reliable across participants. If an artifact accounted for the result then it would have to be a kind of systematic noise that is consistent across participants.</p><p>(2) One of the main sources of noise (both systematic and non-systematic) with infant fMRI is motion. Homotopy is a within-participant analysis that could be biased by motion. To assess whether motion accounts for the results, we took a conservative approach of regressing out the framewise motion (i.e., how much movement there is between fMRI volumes) from the comparisons of the functional activity in regions. Although the correlations numerically decreased with this procedure, they were qualitatively similar to the analysis that does not regress out motion:</p><p>“Additionally, if we control for motion in the correlation between areas – in case motion transients drive consistent activity across areas – then the effects described here are negligibly different (Figure S5).” Pg. 7</p><p>(3) We recognize that despite these analyses, it would be helpful to see what this pattern looks like in adults where we know more about the visual field properties and the function of dorsal and ventral streams. This has been done previously (e.g., Haak &amp; Beckman, 2018), but we have now run those analyses on adults in our sample, as described in the public review. As with infants, there are reliable differences in the homotopy between streams (Figure S1). The MDS results show that the adult data was more complex than the infant data, since it was best described by 3 dimensions rather than 2. Nonetheless, there is a rotation of the MDS such that the structure of the ventral and dorsal streams is also dissociable.</p><disp-quote content-type="editor-comment"><p>Figure 3 also raises several alternative interpretations. The spatial frequency component in B has strong activity ONLY at the extreme border of the VF and this is probably the origin of the strong correlation. I understand that it is only one subject, but this brings the need to show all subjects and to report the correlation. Also, it is important to show the putative average ICA for retinotopy and spatial frequencies across subjects and for adults. All methods should be validated on adults where we have clear data for retinotopy and spatial frequency.</p></disp-quote><p>The reviewer notes that the component in Figure 3 shows strong negative response in the periphery. It is often the case, as reported elsewhere (Moeller et al., 2009), that ICA extracts portions of visual maps. To make a full visual map would require combining components into a composite (e.g., a component that has a high response in the periphery and another component that has a high response in the fovea). If we were to claim that this component, or others like it, could replace the need for retinotopic mapping, then we would want to produce these composite maps; however, our conclusion in this project is that the topographic information of retinotopic maps manifest in individual components of ICA. For this purpose, the analysis we perform adequately assesses this topography.</p><p>Regarding the request to show the results for all subjects, we address this in the public response and repeat it here briefly: we have added 6 new figures to show results akin to Figure 3C and D. It is impractical to show the equivalent of Figure 3A and B for all participants, yet we do release the data necessary to see to visualize these maps easily.</p><p>Finally, the reviewer suggests that we validate the analyses on adult participants. As shown in Figure S3 and reported in the public response, we now run these analyses on adult participants and observe qualitatively similar results to infants.</p><disp-quote content-type="editor-comment"><p>How much was the variation in the presumed spatial frequency map? Is it consistent with the acuity range? 5-month-old infants should have an acuity of around 10c/deg, depending on the mean luminance of the scene.</p></disp-quote><p>The reviewer highlights an important weakness of conducting ICA: we cannot put units on the degree of variation we see in components. We now highlight this weakness in the discussion:</p><p>“Another limitation is that ICA does not provide a scale to the variation: although we find a correlation between gradients of spatial frequency in the ground truth and the selected component, we cannot use the component alone to infer the spatial frequency selectivity of any part of cortex. In other words, we cannot infer units of spatial frequency sensitivity from the components alone.” Pg. 20</p><disp-quote content-type="editor-comment"><p>Figure 5 pipeline is totally obscure. I presumed that I understood, but as it is it is useless. All methods should be clearly described, and the intermediate results should be illustrated in figures and appropriately discussed. Using such blind analyses in infants in principle may not be appropriate and this needs to be verified. Overall all these techniques rely on correlation activities that are all biased by head movement, eye movement, and probably the dummy sucking. All those movements need to be estimated and correlated with the variability of the results. It is a strong assumption that the techniques should work in infants, given the presence of movements.</p></disp-quote><p>We recognize that the SRM methods are complex. Given this feedback, we remade Figure 5 with explicit steps for the process and updated the caption (as reported in the public review).</p><p>Regarding the validation of these methods, we have added SRM analyses from adults and find comparable results. This means that using these methods on adults with comparable amounts of data as what we collected from infants can predict maps that are highly similar to the real maps. Even so, it is not a given that these methods are valid in infants. We present two considerations in this regard.</p><p>First, as part of the SRM analyses reported in the manuscript, we show that control analyses are significantly worse than the real analyses (indicated by the lines on Figure 6). To clarify the control analysis: we break the mapping (i.e., flip the order of the data so that it is backwards) between the test participant and the training participants used to create the SRM. The fact that this control analysis is significantly worse indicates that SRM is learning meaningful representations that matter for retinotopy.</p><p>Second, we believe that this paper is a validation of SRM for infants. Infant fMRI is a nascent field and SRM has the potential to increase the signal quality in this population. We hope that readers will see these analyses as a proof of concept that SRM can be used in their work with infants. We have stated this contribution in the paper now.</p><p>“Additionally, we wish to test whether methods for functional alignment can be used with infants. Functional alignment finds a mapping between participants using functional activity – rather than anatomy – and in adults can improve signal-to-noise, enhance across participant prediction, and enable unique analyses (Lu et al., 2017; Li et al., 2022; Busch et al., 2021; Chen et al., 2015).” Pg. 4</p><p>“This is initial evidence that functional alignment may be useful for enhancing signal quality, like it has in adults (Lu et al., 2017; Li et al., 2022; Busch et al., 2021), or revealing changing function over development (Yates et al., 2021).” Pg. 21</p><p>Regarding the reviewer’s concern that motion may bias the results, we wish to emphasize the nature of the analyses being conducted here: we are using data from a group of participants to predict the neural responses in a held-out participant. For motion to explain consistency between participants, the motion would need to be timelocked across participants. Even if motion was time-locked during movie watching, motion will impair the formation of an adequate model that can contain retinotopic information. Thus, motion should only hurt the ability for a shared response to be found that can be used for predicting retinotopic maps. Hence, the results we observed are despite motion and other sources of noise.</p><disp-quote content-type="editor-comment"><p>What is M??? is it simply the mean value??? If not, how it is estimated?</p></disp-quote><p>M is an abbreviation for mean. We have now expanded the abbreviation the first time we use it.</p><p>Figure 6 should be integrated with map activity where the individual area correlation should be illustrated. Probably fitting SMR adult works well for early cortical areas, but not for more ventral and associative, and the correlation should be evaluated for the different masks.</p><p>With the addition of plots showing the gradients for each participant and each movie (Figures S10–S13) we hope we have addressed this concern. We additionally want to clarify that the regions we tested in the analysis in Figure 6 are only the early visual areas V1, V2, V3, V3A/B, and hV4. The adult validation analyses show that SRM works well for predicting the visual maps in these areas. Nonetheless, it is an interesting question for future research with more extensive retinotopic mapping in infants to see if SRM can predict maps beyond extrastriate cortex.</p><disp-quote content-type="editor-comment"><p>Occipital masks have never been described or shown.</p></disp-quote><p>The occipital mask is from the MNI probabilistic structural atlas (Mazziotta et al., 2001), as reported in the original version and is shared with the public data release. We have added the additional detail that the probabilistic atlas is thresholded at 0% in order to be liberally inclusive.</p><p>“We used the occipital mask from the MNI structural atlas (Mazziotta et al., 2001) in standard space – defined liberally to include any voxel with an above zero probability of being labelled as the occipital lobe – and used the inverted transform to put it into native functional space.” Pg. 27–28</p><disp-quote content-type="editor-comment"><p>Methods lack the main explanation of the procedures and software description.</p></disp-quote><p>We hope that the additions we have made to address this reviewer’s concerns have provided better explanations for our procedures. Additionally, as part of the data and code release, we thoroughly explain all of the software needed to recreate the results we have observed here.</p></body></sub-article></article>