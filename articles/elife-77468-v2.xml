<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">77468</article-id><article-id pub-id-type="doi">10.7554/eLife.77468</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural tracking of phrases in spoken language comprehension is automatic and task-dependent</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-205180"><name><surname>ten Oever</surname><given-names>Sanne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7547-5842</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-269484"><name><surname>Carta</surname><given-names>Sara</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-269485"><name><surname>Kaufeld</surname><given-names>Greta</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-171158"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3395-7234</contrib-id><email>Andrea.Martin@mpi.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf3"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Language and Computation in Neural Systems group, Max Planck Institute for Psycholinguistics</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>Language and Computation in Neural Systems group, Donders Centre for Cognitive Neuroimaging</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jz4aj89</institution-id><institution>Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University</institution></institution-wrap><addr-line><named-content content-type="city">Maastricht</named-content></addr-line><country>Netherlands</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>ADAPT Centre, School of Computer Science and Statistics, University of Dublin, Trinity College</institution></institution-wrap><addr-line><named-content content-type="city">Dublin</named-content></addr-line><country>Ireland</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>CIMeC - Center for Mind/Brain Sciences, University of Trento</institution></institution-wrap><addr-line><named-content content-type="city">Trento</named-content></addr-line><country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Washington University in St. Louis</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>07</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e77468</elocation-id><history><date date-type="received" iso-8601-date="2022-01-31"><day>31</day><month>01</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-06-25"><day>25</day><month>06</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-02-10"><day>10</day><month>02</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.02.08.479571"/></event></pub-history><permissions><copyright-statement>Â© 2022, ten Oever et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>ten Oever et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-77468-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-77468-figures-v2.pdf"/><abstract><p>Linguistic phrases are tracked in sentences even though there is no one-to-one acoustic phrase marker in the physical signal. This phenomenon suggests an automatic tracking of abstract linguistic structure that is endogenously generated by the brain. However, all studies investigating linguistic tracking compare conditions where either relevant information at linguistic timescales is available, or where this information is absent altogether (e.g., sentences versus word lists during passive listening). It is therefore unclear whether tracking at phrasal timescales is related to the content of language, or rather, results as a consequence of attending to the timescales that happen to match behaviourally relevant information. To investigate this question, we presented participants with sentences and word lists while recording their brain activity with magnetoencephalography (MEG). Participants performed passive, syllable, word, and word-combination tasks corresponding to attending to four different rates: one they would naturally attend to, syllable-rates, word-rates, and phrasal-rates, respectively. We replicated overall findings of stronger phrasal-rate tracking measured with mutual information for sentences compared to word lists across the classical language network. However, in the inferior frontal gyrus (IFG) we found a task effect suggesting stronger phrasal-rate tracking during the word-combination task independent of the presence of linguistic structure, as well as stronger delta-band connectivity during this task. These results suggest that extracting linguistic information at phrasal rates occurs automatically with or without the presence of an additional task, but also that IFG might be important for temporal integration across various perceptual domains.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>sentence comprehension</kwd><kwd>speech</kwd><kwd>MEG</kwd><kwd>mutual information</kwd><kwd>temporal dynamics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><award-id>Lise Meitner Research Group &quot;Language and Computation in Neural Systems&quot;</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Andrea E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><award-id>Independent Research Group &quot;Language and Computation in Neural Systems&quot;</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Andrea E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.Vidi.188.029</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Andrea E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The brainâs projection of linguistic knowledge onto speech is reflected by the automatic, reflexive neural tracking of phrases, while the distribution of phrasal-rate tracking and connectivity, particularly the involvement of inferior frontal cortex, is subject to task demands.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding spoken language likely requires a multitude of processes (<xref ref-type="bibr" rid="bib11">Friederici, 2011</xref>; <xref ref-type="bibr" rid="bib35">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib17">Halle and Stevens, 1962</xref>). Although not always an exclusively bottom-up affair, acoustic patterns must be segmented and mapped onto internally-stored phonetic and syllabic representations (<xref ref-type="bibr" rid="bib17">Halle and Stevens, 1962</xref>; <xref ref-type="bibr" rid="bib32">Marslen-Wilson and Welsh, 1978</xref>; <xref ref-type="bibr" rid="bib33">Martin, 2016</xref>). Phonemes must be combined and mapped onto words, which in turn form abstract linguistic structures such as phrases (e.g., <xref ref-type="bibr" rid="bib35">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib46">Pinker and Jackendoff, 2005</xref>).In proficient speakers of a language, this process seems to happen so naturally that one might almost forget the complex parallel and hierarchical processing which occurs during natural speech and language comprehension.</p><p>It has been shown that it is essential to track the temporal dynamics of the speech signal in order to understand its meaning (e.g., <xref ref-type="bibr" rid="bib13">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib44">Peelle and Davis, 2012</xref>). In natural speech, syllables follow each other in the theta range (3â8 Hz; <xref ref-type="bibr" rid="bib48">Rosen, 1992</xref>; <xref ref-type="bibr" rid="bib4">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Pellegrino et al., 2011</xref>), while higher-level linguistic features such as words and phrases tend to occur at lower rates (0.5â3 Hz; <xref ref-type="bibr" rid="bib48">Rosen, 1992</xref>; <xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Keitel et al., 2018</xref>). Tracking of syllabic features is stronger when one understands a language (<xref ref-type="bibr" rid="bib31">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib59">Zoefel et al., 2018a</xref>; <xref ref-type="bibr" rid="bib5">Doelling et al., 2014</xref>) and tracking of phrasal rates is more prominent when the signal contains phrasal information (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib3">Ding et al., 2016</xref>; e.g., word lists versus sentences). Importantly, phrasal tracking even occurs when there are no distinct acoustic modulations at the phrasal rate (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib3">Ding et al., 2016</xref>). These results seem to suggest that tracking of relevant temporal timescales is critical for speech understanding.</p><p>An observation one could make regarding these findings is that tracking occurs only at the rates that are meaningful and thereby behaviourally relevant (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Ding et al., 2016</xref>). For example, in word lists, the word rate is the slowest rate that is meaningful during natural listening. Modulations at slower phrasal rates might not be tracked as they do not contain behaviourally relevant information. In contrast, in sentences, phrasal rates contain linguistic information and therefore these slower rates are also tracked. Thus, when listening to speech one automatically tries to extract the meaning, which requires extracting information at the highest linguistic level (<xref ref-type="bibr" rid="bib17">Halle and Stevens, 1962</xref>; <xref ref-type="bibr" rid="bib33">Martin, 2016</xref>). However, it remains unclear if tracking at these slower rates is a unique feature of language processing, or rather is dependent on attention to relevant temporal timescales.</p><p>As understanding language requires a multitude of processes, it is difficult to figure out what participants actually are doing when listening to natural speech. Moreover, designing a task in an experimental setting that does justice to this multitude of processing is difficult. This is probably why tasks in language studies vary vastly. Tasks include passively listening (e.g., <xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>), asking comprehension questions (e.g., <xref ref-type="bibr" rid="bib27">Keitel et al., 2018</xref>), rating intelligibility (e.g., <xref ref-type="bibr" rid="bib31">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib5">Doelling et al., 2014</xref>), working memory tasks (e.g., <xref ref-type="bibr" rid="bib24">Kayser et al., 2015</xref>), or even syllable counting (e.g., <xref ref-type="bibr" rid="bib3">Ding et al., 2016</xref>). It is unclear whether outcomes are dependent on the specifics of the task. There has so far not been a study that investigates if task instructions focusing on extracting information at different temporal rates or timescales have an influence on the tracking that occurs on these timescales. It is therefore not clear whether tracking phrasal timescales is unique for language stimuli which contain phrasal structures, or could also occur for other acoustic materials where participants are instructed to pay attention to information happening at these temporal rates or timescales.</p><p>To answer this question, we designed an experiment in which participants were instructed to pay attention to different temporal modulation rates while listening to the same stimuli. We presented participants with naturally spoken sentences and word lists and asked them to either passively listen, or perform a task on the temporal scales corresponding to syllables, words, or phrases. We recorded brain activity using magnetoencephalography (MEG) while participants performed these tasks and investigated tracking as well as power and connectivity at three nodes that are part of the language network: the superior temporal gyrus (STG), the middle temporal gyrus (MTG), and the inferior frontal gyrus (IFG). We hypothesized that if tracking is purely based on behavioural relevance, it should mostly depend on the task instructions, rather than the nature of the stimuli. In contrast, if there is something automatic and specific about language information, tracking should depend on the level of linguistic information available to the brain.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behaviour</title><p>Overall task performance was above chance and participants complied with task instructions (<xref ref-type="fig" rid="fig1">Figure 1</xref>; see <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref> for individual data). We found a significant interaction between condition and task (<italic>F</italic>(2,72.0) = 11.51, p &lt; 0.001) as well as a main effect of task (<italic>F</italic>(2,19.7) = 44.19, p &lt; 0.001) and condition (<italic>F</italic>(2,72.0) = 29.0, p &lt; 0.001). We found that only for the word-combination (phrasal-level) task, the sentence condition had a significantly higher accuracy than the word list condition (<italic>t</italic>(54.0) = 6.97, p &lt; 0.001). For the other two tasks, no significant condition effect was found (syllable: <italic>t</italic>(54.0) = 0.62, p = 1.000; word list: <italic>t</italic>(54.0) = 1.74, p = 0.176). Investigating the main effect of task indicated a difference between all tasks (phraseâsyllable: <italic>t</italic>(18.0) = 3.71, p = 0.003; phraseâword: <italic>t</italic>(22.4) = â6.34, p &lt; 0.001; syllableâword: <italic>t</italic>(19.2) = â8.67, p &lt; 0.001).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Behavioural results.</title><p>Accuracy for the three different tasks. Double asterisks indicate significance at the 0.01 level using a paired samples t-test (n=19). Box edges indicate the standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 1.</label><caption><title>Behavioural results with individual data.</title><p>Accuracy for the three different tasks. Double asterisks indicate significance at the 0.01 level using a paired samples t-test (n=19).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig1-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Mutual information</title><p>The overall timeâfrequency response in the three different regions of interest (ROI) using the top-20 PCA components was as expected, with an initial evoked response followed by a more sustained response to the ongoing speech (<xref ref-type="fig" rid="fig2">Figure 2</xref>). From these regions-of-interest, we extracted mutual information (MI) in three different frequency bands (phrasal, word, and syllable). Here, we focus on the phrasal band as this is the band that differentiates word lists from sentences and showed the strongest modulation for this contrast in our previous study (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>). MI results for all other bands are reported in the supplementary materials.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Anatomical regions of interests (ROIs).</title><p>(<bold>A</bold>) ROIs displayed on one exemplar participant surface. (<bold>B</bold>) Timeâfrequency response at each ROI. STG = superior temporal gyrus, MTG = medial temporal gyrus, IFG = inferior frontal gyrus.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig2-v2.tif"/></fig><p>For the phrasal timescale in STG, we found significantly higher MI in the sentence compared to the word list condition (<italic>F</italic>(3,126) = 67.39, p &lt; 0.001; <xref ref-type="fig" rid="fig3">Figure 3</xref>; see <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref> for individual data). No other effects were significant (p &gt; 0.1). This finding paralleled the effect found in <xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>. For the MTG, we saw a different picture: Besides the main effect of condition (<italic>F</italic>(3,126) = 50.24, p &lt; 0.001), an interaction between task and condition was found (<italic>F</italic>(3,126) = 2.948, p = 0.035). We next investigated the effect of condition per task and found for all tasks except the passive task a significant effect of condition, with stronger MI for the sentence condition (passive: <italic>t</italic>(126) = 1.07, p = 0.865; syllable: <italic>t</italic>(126) = 4.06, p = 0.003; word: <italic>t</italic>(126) = 5.033, p &lt; 0.001; phrase: <italic>t</italic>(126) = 4.015, p = 0.003). For the IFG, we found a main effect of condition (<italic>F</italic>(3,108) = 21.89, p &lt; 0.001) as well as a main effect of task (<italic>F</italic>(3,108) = 2.74, p = 0.047). The interaction was not significant (<italic>F</italic>(3,108) = 1.49, p = 0.220). Comparing the phrasal task with the other tasks indicated higher MI for the phrasal compared to the word task (<italic>t</italic>(111) = 2.50, p = 0.028). We also found a trend for the comparison between the phrasal and syllable tasks (<italic>t</italic>(111) = 2.17, p = 0.064), as well as the phrasal and passive tasks (<italic>t</italic>(111) = 2.25, p = 0.052).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Mutual information (MI) analysis at the phrasal band (0.8â1.1 Hz) for the three different regions of interests (ROIs).</title><p>Single and double asterisks indicate significance at the 0.05 and 0.01 level using a paired samples t-test (n=19). T indicates trend level significance (p &lt; 0.1). Inset at the top left of the graph indicates whether a main effect of condition was present (with higher MI for sentences versus word lists; this inset does not reflect real data). Averages of conditions are only shown if there was a main task effect without an interaction. Box edges indicate the standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Mutual information (MI) analysis at the phrasal band (0.8â1.1 Hz) for the three different regions of interests (ROIs) with individual data.</title><p>Single and double asterisks indicate significance at the 0.05 and 0.01 level using a paired samples t-test (n=19). T indicates trend level significance (p &lt; 0.1). Inset at the top left of the graph indicates whether a main effect of condition was present (with higher MI for sentences versus word lists; this inset does not reflect real data). Averages of conditions are only shown if there was a main task effect.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 2.</label><caption><title>Mutual information (MI) analysis at the syllable (3.5â5.0 Hz) and word rate (1.9â2.8 Hz) for the three different regions of interests (ROIs).</title><p>Double asterisks indicate significance at the 0.01 level using a paired samples t-test (n=19). Inset at the top left of the graph indicates whether a main effect of condition was present (with higher MI for word lists versus sentences; this inset does not reflect real data).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig3-figsupp2-v2.tif"/></fig></fig-group><p>For the word and syllable frequency bands no interactions were found (all p &gt; 0.1; <xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>). For all six models, there was a significant effect of condition, with stronger MI for word lists compared to sentences (all p &lt; 0.001). The main effect of task was not significant in any of the models (p &gt; 0.1; for the MTG syllable level there was a trend: <italic>F</italic>(3,126) = 2.40, p = 0.071).</p><p>When running the power control analysis, we did not find that significant effects in power differences (also see next section for power in generic bands; mostly due to main effects of condition) influenced our tracking results for any of the bands investigated.</p></sec><sec id="s2-3"><title>Power</title><p>We repeated the linear mixed modelling using power instead of MI to investigate if power changes paralleled the MI effects. For the delta band, we found for the STG a main effect of condition (F(1,18) = 6.11, p = 0.024; <xref ref-type="fig" rid="fig4">Figure 4</xref>. See <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref> for individual data) and task (F(3,108) = 3.069, p = 0.031). For the interaction we found a trend (F(3,108) = 2.620, p = 0.054). Overall sentences had stronger delta power than word lists. We found lower power for the phrase compared to the passive task (t(111) = 2.31, p = 0.045) and lower power for the phrase compared to the syllable task (t(111) = 2.43, p = 0.034). There was no significant difference between the phrase and word task (t(111) = 0.642, p = 1.00).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Power effects for the different regions of interests (ROIs).</title><p>Single and double asterisks indicate significance at the 0.05 and 0.01 level using a paired samples t-test (n=19). T indicates trend significance (p &lt; 0.1) Inset at the right top of the graph indicates whether a main effect of condition was present (with higher activity for sentences versus word lists; this inset does not reflect real data). Averages of conditions are only shown if there was a main task effect. Box edges indicate the standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Power effects for the different regions of interests (ROIs) with individual data.</title><p>Single and double asterisks indicate significance at the 0.05 and 0.01 level using a paired samples t-test (n=19). T indicates trend significance (p &lt; 0.1) Inset at the left top of the graph indicates whether a main effect of condition was present (with higher activity for sentences versus word lists; this inset does not reflect real data). Averages of conditions are only shown if there was a main task effect.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>Power effects for the different regions of interests (ROIs) and different bands.</title><p>Single and double asterisks indicate significance at the 0.05 and 0.01 level using a paired samples t-test (n=19). T indicates trend significance (p &lt; 0.1) Inset at the top left of the graph indicates whether a main effect of condition was present (with higher activity for sentences versus word lists; this inset does not reflect real data).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig4-figsupp2-v2.tif"/></fig></fig-group><p>The MTG delta power effect overall paralleled the STG effects with a significant condition (<italic>F</italic>(1,124.94) = 12.339, p &lt; 0.001) and task effect (<italic>F</italic>(3,124.94) = 4.326, p = 0.006). The interaction was trend significant (<italic>F</italic>(3,124.94) = 2.58, p = 0.056). Pairwise comparisons of the task effect showed significantly stronger power for the phrase compared to the passive task (<italic>t</italic>(128) = 2.98, p = 0.007) and lower power for the phrase compared to the syllable task (<italic>t</italic>(128) = 3.10, p = 0.024). The passiveâword comparison was not significant (<italic>t</italic>(128) = 2.577, p = 0.109). Finally, for the IFG we only found a trend effect for condition (<italic>F</italic>(1,123.27)=4.15, p = 0.057), with stronger delta power in the sentence condition.</p><p>The results for all other bands can be found in the supplementary materials (<xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>). In summary, no interaction effects were found for any of the models (all p &gt; 0.1). In all bands, power was generally higher for sentences than for word lists. Any task effect generally showed stronger power for the lower hierarchical level (e.g. generally higher power for passive versus word-combination tasks).</p></sec><sec id="s2-4"><title>Connectivity</title><p>Overall connectivity patterns showed the strongest connectivity in the delta and alpha frequency band (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In the delta band, we found a main effect of task for the STGâIFG connectivity (<italic>F</italic>(3,122.06) = 4.1078, p = 0.008; <xref ref-type="fig" rid="fig6">Figure 6</xref>; see <xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref> for individual data). Follow-up analysis showed a significant difference between the phrasal and passive tasks with higher connectivity in the phrasal compared to the passive task (<italic>t</italic>(125) = 3.254, p = 0.003). The other comparisons with the phrasal task were not significant. The effect of task remained significant even when correcting for power differences between the passive and phrasal tasks (<italic>F</italic>(1,53.02) = 12.39, p &lt; 0.001; note the change in degrees of freedom as only the passive and phrasal tasks were included in this mixed model as any power correction is done on pairs). Initially, we also found main effects of condition for the delta and beta bands for the MTGâIFG connectivity (stronger connectivity for the sentence compared to the word list condition), however after controlling for power, these effects did not remain significant (<xref ref-type="fig" rid="fig6s2">Figure 6âfigure supplement 2</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Connectivity pattern between anatomical regions of interests (ROIs).</title><p>(<bold>A</bold>) ROI connections displayed on one exemplar participant surface. (<bold>B</bold>) Timeâfrequency weighted phase-lagged index (WPLI) response at each ROI.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig5-v2.tif"/></fig><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Weighted phase lag index (WPLI) effects for the different regions of interests (ROIs).</title><p>Double asterisks indicate significance at the 0.01 level using a paired samples t-test (n=19) after correcting for power differences between the two conditions (we plot the original data, not corrected for power, as we can only perform pairwise power and consequently data will be different for each control). Averages of conditions are only shown if there was a main task effect. Box edges indicate the standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6âfigure supplement 1.</label><caption><title>Weighted phase lag index (WPLI) effects for the different regions of interests (ROIs) with individual data.</title><p>Single and double asterisks indicate significance 0.01 level using a paired samples t-test (n=19) after correcting for power differences between the two conditions (we plot the original data, not corrected for power, as we can only perform pairwise power and consequently data will be different for each control). Averages of conditions are only shown if there was a main task effect.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6âfigure supplement 2.</label><caption><title>Weighted phase lag index (WPLI) effects for the different regions of interests (ROIs) and different bands.</title><p>Connectivity is displayed before correcting for power differences. None of the effects survived correcting for power differences. Inset at the top left of the graph indicates whether a main effect of condition was present (with higher activity for sentences versus word lists; this inset does not reflect real data).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig6-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>MEGâbehavioural performance relation</title><p>We found for the MI analysis a significant effect of accuracy only in the MTG. Here, we found a three-way interaction between accuracy Ã task Ã condition (<italic>F</italic>(2,91.9) = 3.459, p = 0.036). Splitting up for the three different tasks we found only an uncorrected significant effect for the condition Ã accuracy interaction for the phrasal task (<italic>F</italic>(1,24.8) = 5.296, p = 0.03) and not for the other two tasks (p &gt; 0.1). In the phrasal task, we found that when accuracy was high, there was a stronger difference between the sentence and the word list condition compared to when accuracy was low, with stronger accuracy for the sentence condition (<xref ref-type="fig" rid="fig7">Figure 7A</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>MEGâbehavioural performance relation.</title><p>(<bold>A</bold>) Predicted values for the phrasal band MI in the middle temporal gyrus (MTG) for the word-combination task separately for the two conditions. (<bold>B</bold>) Predicted values for the delta-band weighted phase lag index (WPLI) in the superior temporal gyrus (STG)âMTG connection separately for the two conditions. Error bars indicate the 95% confidence interval of the fit. Coloured lines at the bottom indicate individual datapoints.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7âfigure supplement 1.</label><caption><title>Age effects on power estimates.</title><p>(<bold>A</bold>) Predicted values for delta power for the two conditions dependent on age in superior temporal gyrus (STG, left) and middle temporal gyrus (MTG, right). (<bold>B</bold>) Predicted values for delta power for the four tasks dependent on age in MTG. Error bars indicate the 95% confidence interval of the fit. Coloured lines at the bottom indicate individual datapoints.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-fig7-figsupp1-v2.tif"/></fig></fig-group><p>No relation between accuracy and power was found. For the connectivity analysis, we found a significant condition Ã accuracy interaction for the STGâMTG connection (<italic>F</italic>(1,80.23) = 5.19, p = 0.025; <xref ref-type="fig" rid="fig7">Figure 7B</xref>). Independent of task, when accuracy was low the difference between sentence and word lists was stronger with higher weighted phase lag index (WPLI) fits for the sentence condition. After correcting for accuracy there was also a significant task Ã condition interaction (<italic>F</italic>(2,80.01) = 3.348, p = 0.040) and a main effect of condition (<italic>F</italic>(1,80.361) = 5.809, p = 0.018). While overall there was a stronger WPLI for the sentence compared to the word list condition, the interaction seemed to indicate that this was especially the case during the word task (p = 0.005), but not for the other tasks (p &gt; 0.1).</p></sec><sec id="s2-6"><title>Age control</title><p>Adding age to the analysis did not change any of the original findings (all original effects were still significant). We did however find for the power analysis age-specific interactions with condition and task. Specifically, for both the STG and the MTG we found an interaction between age and condition (<italic>F</italic>(1,28.87) = 6.156, p = 0.0192 and <italic>F</italic>(1,31) = 10.31, p = 0.003). In both ROIs, there was a stronger difference between sentences and word lists (higher delta power for sentences) for the younger compared to the older participants (<xref ref-type="fig" rid="fig7s1">Figure 7âfigure supplement 1</xref>). In the MTG, there was also an interaction between task and age (<italic>F</italic>(1,31) = 5.020, p = 0.006). Here, in a follow-up we found that only in the word task there was a correlation between age and power (p = 0.023 uncorrected), but not for the other tasks (p &gt; 0.1).</p></sec><sec id="s2-7"><title>Number of component control</title><p>Overall, the amount of PCA components did not influence any of the qualitative differences in the condition. It did seem however that 10 PCA components were not sufficient to show all original effects with the same power. Specifically, the IFG task and MTG task Ã condition effect were only trend significant for 10 components (p = 0.06 and p = 0.1, respectively). The other effects did remain significant with 10 components. Using 30 components made some of our effects stronger than with 20 components. Here, the IFG task and MTG task Ã condition effects had p values of 0.034 and 0.006, respectively. We conclude that the amount of PCAs components did not qualitative change any of our reported effects.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the current study, we investigated the effects of âadditionalâ tasks on the neural tracking of sentences and word lists at temporal modulations that matched phrasal rates. Different nodes of the language network showed different tracking patterns. In STG, we found stronger tracking of phrase-timed dynamics in sentences compared to word lists, independent of task. However, in MTG we found this sentence-improved tracking only for active tasks. In IFG, we also found an overall increase of tracking for sentences compared to word lists. Additionally, stronger phrasal tracking was found for the phrasal-level word-combination task compared to the other tasks (independent of stimulus type; note that for the syllable and passive comparison we found a trend), which was paralleled with increased IFGâSTG connectivity in the delta band for the word-combination task. Behavioural performance seemed to relate to MI tracking in the MTG and STGâMTG connections. This suggests that tracking at phrasal timescales depends both on the linguistic information present in the signal, and on the specific task that is performed.</p><p>The findings reported in this study are in line with previous results, with overall stronger tracking of low-frequency information in the sentences compared to the word list condition (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>). Crucially, for the stimuli used in our study it has been shown that the condition effects are not due to acoustic differences in the stimuli and also do not occur for reversed speech (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>). It is therefore most likely that our results reflect an automatic inference-based extraction of relevant phrase-level information in sentences, indicating automatic processing in participants as they understand the meaning of the speech they hear using stored, structural linguistic knowledge (<xref ref-type="bibr" rid="bib35">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib3">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Har-Shai Yahav and Zion Golumbic, 2021</xref>). Overall, it did not seem that making participants pay attention to the temporal dynamics at the same hierarchical level through an additional task â instructing them to remember word combinations at the phrasal rate during word list presentation â could counter this main effect of condition.</p><p>Even though there was an overall main effect of condition, task did influence neural responses. Interestingly, the task effects differed for the three ROIs. In the STG, we found no task effects, while in the MTG we found an interaction between task and condition. In the MTG increased phrasal-level tracking for sentences only occurred when participants were specifically instructed to perform an active task on the materials. It therefore seems that in MTG all levels of linguistic information are used to do an active language operation on the stimuli. Importantly, the tracking at the phrasal rate in MTG seemed relevant for behavioural performance when attending to phrasal timescales (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). This is in line with previous theoretical and empirical research suggesting a strong top-down modulatory response of speech processing in which predictions flow from the highest hierarchical levels (e.g., syntax) down to lower levels (e.g., phonemes) to aid language understanding (<xref ref-type="bibr" rid="bib33">Martin, 2016</xref>; <xref ref-type="bibr" rid="bib16">Hagoort, 2017</xref>; <xref ref-type="bibr" rid="bib7">Federmeier, 2007</xref>). As in the word list condition no linguistic information is present at the phrasal rate, this information cannot be used to provide useful feedback for processing lower-level linguistic information. Instead, it could have been expected that the same type of increased tracking should have happened at the word rate rather than the phrasal rate for word lists (i.e., stronger word-rate tracking for word lists for the active tasks versus passive task). This effect was not found; this could either be attributed to different computational operations occurring at different hierarchical levels or to signal-to-noise/signal detection issues.</p><p>We found that across participants both the MI and the connectivity in temporal cortex influenced behavioural performance. Specifically, MTGâSTG connections were, independent of task, related to accuracy. There was higher connectivity between MTG and STG for sentences compared to word lists at low accuracies. At high accuracies, we found that stronger MTG tracking at phrasal rates (measured with MI) for sentences compared to word lists during the word-combination task. These results suggest that indeed tracking of phrasal structure in MTG is relevant to understand sentences compared to word lists. This was reflected in a general increase in delta connectivity differences when the task was difficult (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Participants might compensate for the difficulty using phrasal structure present in the sentence condition. When phrasal structure in sentences are accurately tracked (as measured with MI) performance is better when these rates are relevant (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). These results point to a role for phrasal tracking for accurately understanding the higher-order linguistic structure in sentences, though more research is needed to verify this. It is evident that the connectivity and tracking correlations to behaviour do not explain all variation in the behavioural performance (compare <xref ref-type="fig" rid="fig1">Figure 1</xref> with <xref ref-type="fig" rid="fig3">Figure 3</xref>). Plainly, temporal tracking does not explain everything in language processing. Besides tracking there are many other components important for our designated tasks, such as memory load and semantic context which are not captured by our current analyses.</p><p>It is interesting that MTG, but not STG, showed an interaction effect. Both MTG and STG are strong hubs for language processing and have been involved in many studies which contrasted pseudo-words and words (<xref ref-type="bibr" rid="bib20">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib55">Turken and Dronkers, 2011</xref>; <xref ref-type="bibr" rid="bib56">Vouloumanos et al., 2001</xref>). It is likely that STG does the more lower-level processing of the two regions, as it is earlier in the cortical hierarchy, thereby being more involved in initial segmentation and initial phonetic abstraction rather than a lexical interface (<xref ref-type="bibr" rid="bib20">Hickok and Poeppel, 2007</xref>). This could also explain why STG does not show task-specific tracking effects; STG could be earlier in a workload bottleneck, receiving feedback independent of task, while MTG feedback is recruited only when active linguistic operations are required. Alternatively, it is possible that either small differences in the acoustics are detected by STG (even though this effect was not previously found with the same stimuli, <xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>), or that our blocked designed put participants in a sentence or word list âmodeâ which could have influenced the state of these early hierarchical regions.</p><p>The IFG was the only region that showed an increase in phrasal-rate tracking specifically for the word-combination task. Note, however, that this was a weak effect, as the comparison between the phrase task and the syllable and passive tasks only reached a trend towards significance. Nonetheless, this effect is interesting for understanding the role of IFG in language. Traditionally, IFG has been viewed as a hub for articulatory processing (<xref ref-type="bibr" rid="bib20">Hickok and Poeppel, 2007</xref>), but its role during speech comprehension, specifically in syntactic processing, has also been acknowledged (<xref ref-type="bibr" rid="bib11">Friederici, 2011</xref>; <xref ref-type="bibr" rid="bib16">Hagoort, 2017</xref>; <xref ref-type="bibr" rid="bib38">Nelson et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Zaccarella et al., 2017</xref>). Integrating information across time and relative timing is essential for syntactic processing (<xref ref-type="bibr" rid="bib35">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib2">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Martin and Doumas, 2019</xref>), and IFG feedback has been shown to occur in temporal dynamics at lower (delta) rates during sentence processing (<xref ref-type="bibr" rid="bib43">Park et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Keitel and Gross, 2016</xref>). However, it has also been shown that syntactic-independent verbal working memory chunking tasks recruit the IFG (<xref ref-type="bibr" rid="bib2">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib42">Osaka et al., 2004</xref>; <xref ref-type="bibr" rid="bib8">Fegen et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Koelsch et al., 2009</xref>). This is in line with our findings that show that IFG is involved when we need to integrate across temporal domains either in a language-specific domain (sentences versus word lists) or for language-unspecific tasks (word-combination versus other tasks). We also show increased delta connectivity with STG for the only temporal-integration tasks in our study (i.e., the word-combination task), independent of the linguistic features in the signal. Our results therefore support a role of the IFG as a combinatorial hub integrating information across time (<xref ref-type="bibr" rid="bib12">Gelfand and Bookheimer, 2003</xref>; <xref ref-type="bibr" rid="bib49">Schapiro et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Skipper, 2015</xref>).</p><p>In the current study, we investigated power as a neural readout during language comprehension from speech. This was both to ensure that any tracking effects we found were not due to overall signal-to-noise (SNR) differences, as well as to investigate task-and-condition dependent computations. SNR is better for conditions with higher power, which therefore leads to more reliable phase estimations, critical for computing MI as well as connectivity (<xref ref-type="bibr" rid="bib58">Zar, 1998</xref>). We will therefore discuss the power differences as well as their consequences for the interpretation of the MI and connectivity results. Generally, it seemed that there was stronger power in the sentence condition compared to the word list condition in the delta band. However, the pattern was very different than the MI pattern. For the power, the word list-sentence difference was the biggest in the passive condition. In contrast, for the MI there was either no task difference (in STG) or even a stronger effect for the active tasks (in MTG; note that the power interaction was trend significant STG and MTG). We therefore think it unlikely that our MI effects were purely driven by SNR differences, and our power control analysis is consistent with this interpretation. Instead, power seems to reflect a different computation than the tracking, where more complex tasks generally lead to lower power across almost all tested frequency bands. As most of our frequency bands are on the low side of the spectrum (up to beta), it is expected that more complex tasks reduce the low-frequency power (<xref ref-type="bibr" rid="bib22">Jensen and Mazaheri, 2010</xref>; <xref ref-type="bibr" rid="bib28">Klimesch, 1999</xref>). It is interesting to observe that this did not reduce the connectivity for the delta band between IFG and STG, but rather increased it. It has been suggested that low power can potentially increase the available computational space, as it increases the entropy in the signal (<xref ref-type="bibr" rid="bib18">Hanslmayr et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">ten Oever and Sack, 2015</xref>). Note that even though we found increased connectivity, we did not see a clear power peak in the delta band. This suggests that we might not be looking at an endogenous oscillator, but rather at connections operation at that temporal scale (potentially being non-oscillatory in nature). Finally, in the power comparisons for the theta, alpha, and beta bands we found stronger power for the sentence compared to the word list condition, which could reflect that listening to a natural sentence is generally less effortful than listening to a word list.</p><p>In the current manuscript, we describe tracking of ongoing temporal dynamics. However, the neural origin of this tracking is unknown. While we can be sure that modulations in the phrasal-rate follow changes in the phrasal rate of the acoustic input, it is unclear what the mechanism behind this modulation is. It is possible that there is stronger alignment of neural oscillations with the acoustic input at the phrasal rate (<xref ref-type="bibr" rid="bib30">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Obleser and Kayser, 2019</xref>; <xref ref-type="bibr" rid="bib47">Rimmele et al., 2021</xref>). However, it could as well be that there is a phrasal timescale or slower operation happening while processing the incoming input (which de facto is at the same timescale as the phrasal structure inferred from the input). This operation, in response to stimulus input, could just as well induce the patterns we observe (<xref ref-type="bibr" rid="bib37">Meyer et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Zoefel et al., 2018b</xref>). Finally, it is possible that there are specific responses as a consequence of the syntactic structure, task, or statistical regularities occurring as specific events at phrasal timescales (<xref ref-type="bibr" rid="bib40">Obleser and Kayser, 2019</xref>; <xref ref-type="bibr" rid="bib54">Ten Oever and Martin, 2021</xref>; <xref ref-type="bibr" rid="bib10">Frank and Yang, 2018</xref>).</p><p>It is difficult to decide on the most natural task in an experimental setting, that best reflects how we use language in a natural setting. This is probably why such a vast number of different tasks have been used in the literature. Our study (and many before us) indicates that during passive listening, we naturally attend to all levels of linguistic hierarchy. This is consistent with the widely accepted notion that the meaning of a natural sentence requires composing words in a grammatical structure. For most research questions in language, it therefore is sensible to use a task that mimics this automatic natural understanding of a sentence. Here, we show that automatic understanding of linguistic information, and all the processing that this entails, cannot be countered to substantially change the consequences for neural readout, even when explicitly instructing participants to pay attention to particular timescales.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>In total, 20 Dutch native speakers (16 females; age range: 18â59; mean age = 39.5) participated in the study. All were right handed, reported normal hearing, had normal or corrected-to-normal vision, and did not have any history of dyslexia or other language-related disorders. Participants performed a screening for their eligibility in the MEG and MRI and gave written informed consent. The study was approved by the Ethical Commission for human research Arnhem/Nijmegen (project number CMO2014/288). Participants were reimbursed for their participation. One participant was excluded from the analysis as they did not finish the full session.</p></sec><sec id="s4-2"><title>Materials and design</title><p>Materials were identical to the stimuli used in <xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>. They consisted of naturally spoken sentences or word lists which consisted of 10 words (see <xref ref-type="table" rid="table1">Table 1</xref> for examples). The sentences contained two coordinate clauses with the following structure: [Adj N V N Conj Det Adj N V N]. All words were disyllabic except for the words âdeâ (<italic>the</italic>) and âenâ (<italic>and</italic>). Word lists were word-scrambled versions of the original sentences which always followed the structure [V V Adj Adj Det Conj N N N N] or [N N N N Det Conj V V Adj Adj] to ensure that they were grammatically incorrect. In total, 60 sentences were used. All sentences were presented at a comfortable sound level.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Stimuli and task examples.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom">Sentence</td><td align="left" valign="bottom" colspan="4">[bange helden] [plukken bloemen] en de [bruine vogels] [halen takken]<break/>[<italic>timid heroes</italic>] [<italic>pluck flowers</italic>] <italic>and the</italic> [<italic>brown birds</italic>] [<italic>gather branches</italic>]</td></tr><tr><td align="left" valign="bottom">Word list</td><td align="left" valign="bottom" colspan="4">[helden bloemen] [vogels takken] de en [plukken halen] [bange bruine]<break/>[<italic>heroes flowers</italic>] [<italic>birds branches</italic>] <italic>and the</italic> [<italic>pluck gather</italic>] [<italic>timid brown</italic>]</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="2">Sentence</td><td align="left" valign="bottom" colspan="2">Word list</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Correct</td><td align="left" valign="bottom">Incorrect</td><td align="left" valign="bottom">Correct</td><td align="left" valign="bottom">Incorrect</td></tr><tr><td align="left" valign="bottom">Syllable</td><td align="left" valign="bottom">/bÉ/</td><td align="left" valign="bottom">/lÉ/</td><td align="left" valign="bottom">/bÉ/</td><td align="left" valign="bottom">/lÉ/</td></tr><tr><td align="left" valign="bottom">Word</td><td align="left" valign="bottom">bloemen<break/>[<italic>flowers</italic>]</td><td align="left" valign="bottom">vaders<break/>[<italic>fathers</italic>]</td><td align="left" valign="bottom">bloemen<break/>[<italic>flowers</italic>]</td><td align="left" valign="bottom">vaders<break/>[<italic>fathers</italic>]</td></tr><tr><td align="left" valign="bottom">Word combination</td><td align="left" valign="bottom">bange helden<break/>[<italic>timid heroes</italic>]</td><td align="left" valign="bottom">halen bloemen<break/>[<italic>gather flowers</italic>]</td><td align="left" valign="bottom">helden bloemen<break/>[<italic>heroes flowers</italic>]</td><td align="left" valign="bottom">vogels bloemen<break/>[<italic>birds flowers</italic>]</td></tr></tbody></table><table-wrap-foot><fn><p>For each condition (sentence and word list) one example stimulus (top) and corresponding tasks are shown (bottom).</p></fn></table-wrap-foot></table-wrap><p>Participants were asked to perform four different tasks on these stimuli: a passive task, a syllable task, a word task, and a word-combination task. For the passive task, participants did not need to perform any task other than comprehension â they only needed to press a button to go to the next trial. For the syllable task, participants heard after every sentence two part-of-speech sounds, each consisting of one syllable. The sound fragments were a randomly determined syllable from the previously presented sentence and a random syllable from all other sentences. Participantsâ task was to indicate via a button press which of the two sound fragments was part of the previous sentence. For the word task, two words were displayed on the screen after each trial (a random word from the just presented sentence and one random word from all other sentences excluding âdeâ and âenâ), and participants needed to indicate which of the two words was part of the sentence before. For the word-combination task, participants were presented with two word pairs on the screen. Each of the four words was part of the just presented sentence, but only one of the pairs was in the correct order. Participants needed to indicate which of the two pairs was presented in the sentence before. Presented options for the sentence condition were always a grammatically and semantically plausible combination of words. See <xref ref-type="table" rid="table1">Table 1</xref> for an example of the tasks for each condition (sentences and word lists). The three active tasks required participants to focus on the syllabic (syllable task), word (word task), or phrasal (word combination or also called phrasal task) timescales. Note that different trials within a task were not matched for task difficulty. For example, in the syllable task syllables that make a word are much easier to recognize than syllables that do not make a word. Additionally, trials pertaining to the beginning of the sentence are more difficult than ones related to the end of the sentence due to recency effects.</p></sec><sec id="s4-3"><title>Procedure</title><p>At the beginning of each trial, participants were instructed to look at a fixation cross presented at the middle of the screen on a grey background. Audio recordings were presented after a random interval between 1.5 and 3 s; 1 s after the end of the audio, the task was presented. For the word and word-combination task, this was the presentation of visual stimuli. For the syllable task, this entailed presenting the sound fragments one after each other (with a delay of 0.5 s in between). For the passive task, this was the instruction to press a button to continue. In total, there were eight blocks (two conditions Ã four tasks) each lasting about 8 min. The order of the blocks was pseudo-randomized by independently randomizing the order of the tasks and the conditions. For a single participant, we then always presented the same task twice in a row to avoid task-switching costs. As a consequence, condition was always alternated (a possible order of blocks would be: passive-sentence, passive-word list, word-sentence, word-word list, syllable-sentence, syllable-word list, word-combination-sentence, word-combination-word list). Across participants the starting condition was counterbalanced. After the main experiment, an auditory localizer was collected which consisted of listening to 200 ms sinewave and broadband sounds (centred at 0.5, 1, and 2 kHz; for the broadband at a 10% frequency band) at approximately equal loudness. Each sound had a 50 ms linear on and off ramp and was presented for 30 times (with random inter-stimulus interval between 1 and 2 s).</p><p>At arrival, participants filled out a screening. Electrodes to monitor eye movements and heart beat were placed (left mastoid was used as ground electrode) at an impedance below 15 kÎ©. Participants wore metal free clothes and fitted earmolds on which two of the three head localizers were placed (together with a final head localizer placed at the nasion). They then performed the experiment in the MEG. MEG was recorded using a 275-channel axial gradiometer CTF MEG system at a sampling rate of 1.2 kHz. After every block participants had a break, during which head position was corrected (<xref ref-type="bibr" rid="bib52">Stolk et al., 2013</xref>). After the session, the headshape was collected using Polhemus digitizer (using as fiducials the nasion and the entrance of the ear canals as positioned with the earmolds). For each participant, an MRI was collected with a 3T Siemens Skyra system using the MPRAGE sequence (1 mm isotropic). Also for the MRI acquisition participants wore the earmolds with vitamin pills to optimize the alignment.</p></sec><sec id="s4-4"><title>Behavioural analysis</title><p>We performed a linear mixed model analysis with fixed factors task (syllable, word, and word combination) and condition (sentence and word list) as implemented by lmer in R4.1.0. The dependent variable was accuracy. First, any outliers were removed (values more extreme than median Â± 2.5 IQR). Then, we investigated what the best random model was, including a random intercept or a random slope for one or two of the factors. The models with varying random factors were compared with each other using an analysis of variance. With no significant difference, the model with the lowest number of factors was included (with minimally a random intercept). Finally, lsmeans was used for follow-up tests using the kenward-roger method to calculate the degrees of freedom from the linear mixed model. For significant interactions, we investigated the effect of condition per task. For main effects, we investigated pairwise comparisons. We corrected for multiple comparisons using adjusted Bonferroni corrections unless specified otherwise. For all further reported statistical analyses for the MEG data, we followed the same procedure (except that there was one more level of task, i.e. the passive task). To avoid exploding the amount of comparisons, we a priori decided for any task effects in the MEG analysis to only compare the individual tasks with the phrase task.</p></sec><sec id="s4-5"><title>MEG pre-processing</title><p>First source models from the MRI were made using a surface-based approach in which grid points were defined on the cortical sheet using the automatic segmentation of freesurfer6.0 (<xref ref-type="bibr" rid="bib9">Fischl, 2012</xref>) in combination with pre-processing tools from the HCP workbench1.3.2 (<xref ref-type="bibr" rid="bib14">Glasser et al., 2013</xref>) to down-sample the mesh to 4 k vertices per hemisphere. The MRI was co-registered to the MEG using the previously defined fiducials as well as an automatic alignment of the MRI to the Polhemus headshape using the Fieldtrip20211102 software (<xref ref-type="bibr" rid="bib41">Oostenveld et al., 2011</xref>).</p><p>Pre-processing involved epoching the data between â3 and +7.9 s (+3 relative to the longest sentence of 4.9 s) around sentence onset. We applied a dftfilter at 50, 100, and 150 Hz to remove line noise, a Butterworth bandpass filter between 0.6 and 100 Hz, and performed baseline correction (â0.2 to 0 s baseline). Trials with excessive movements or squid jumps were removed via visual inspection (20.1 Â± 18.5 trials removed; mean Â± standard deviation). Then data were resampled to 300 Hz and we performed ICA decomposition to correct for eye blinks/movement and heart beat artefacts (4.7 Â± 0.99 components removed; mean Â± standard deviation). Trials with remaining artefacts were removed by visual inspection (11.3 Â± 12.4 trials removed; mean Â± standard deviation). Then we applied a lcmv filter to transform the data to have single-trial source space representations. A common filter across all trials was calculated using a fixed orientation and a lambda of 5%. We only extracted time courses for our ROI, STG (<xref ref-type="bibr" rid="bib11">Friederici, 2011</xref>; <xref ref-type="bibr" rid="bib43">Park et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Fegen et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Koelsch et al., 2009</xref>), medial temporal gyrus (<xref ref-type="bibr" rid="bib46">Pinker and Jackendoff, 2005</xref>; <xref ref-type="bibr" rid="bib44">Peelle and Davis, 2012</xref>; <xref ref-type="bibr" rid="bib31">Luo and Poeppel, 2007</xref>), and inferior frontal cortex (<xref ref-type="bibr" rid="bib3">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Kayser et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Har-Shai Yahav and Zion Golumbic, 2021</xref>); numbers correspond to label-coding from the aparc parcellations implemented in Freesurfer. These time courses were baseline corrected (â0.2 to 0 s). To reduce computational load and to ensure that we used relevant data within the ROI, we extracted the top 20 PCA components per ROI for all following analyses based on a PCA using the time window of interest (0.5â3.7 s; 0.5 to ensure that all initial evoked responses were not included and 3.7 as it corresponds to the shortest trials). All following analyses were done per ROI. With enough statistical power one would add ROI as a separate factor in the analyses, but unfortunately, we did not have enough power to find a potential three-way interaction (ROI Ã condition Ã task). We therefore cannot make strong conclusions about one ROI having a stronger effect than another.</p></sec><sec id="s4-6"><title>MI analysis</title><p>First, we extracted the speech envelopes by following previous procedures (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib21">Ince et al., 2017</xref>). The acoustic waveforms (third-order Butterworth filter) were filtered in eight frequency bands (100â8000 Hz) equidistant on the cochlear frequency map (<xref ref-type="bibr" rid="bib51">Smith et al., 2002</xref>). The absolute of the Hilbert transform was computed, we low passed the data at 100 Hz (third order Butterworth) and then down-sampled to 300 Hz (matching the MEG sampling rate). Then, we averaged across all bands.</p><p>MI was calculated between the filtered speech envelopes (using a third-order Butterworth filter) and the filtered MEG data at three different frequency bands corresponding to information content at different linguistic hierarchical levels: phrase (0.8â1.1 Hz), word (1.9â2.8 Hz), and syllable (3.5â5.0 Hz). The frequency bands were extracted based on the rate of the linguistic information in the speech signal. We hypothesized that tracking of relevant information should happen at those respective bands. While in our stimulus set the boundaries of the linguistic levels did not overlap, in natural speech the brain has an even more difficult task as there is no one-to-one match between band and linguistic unit (<xref ref-type="bibr" rid="bib39">Obleser et al., 2012</xref>). Our main analysis focuses on the phrasal band, as that is where our previous study found the strongest effects (<xref ref-type="bibr" rid="bib23">Kaufeld et al., 2020</xref>), but for completeness we also report on the other bands. MI was estimated after the evoked response (0.5 s) until the end of the stimulus at five different delays (60, 80, 100, 120, and 140 ms) and averaged across delays between the phase estimations of the envelopes and MEG data. A single MI value was generated per condition per ROI by concatenating all trials before calculating the MI (MEG and speech). Statistical analysis was performed per ROI per frequency band.</p></sec><sec id="s4-7"><title>Power analysis</title><p>Power analysis was performed to compare the MI results with absolute power changes. On the one hand, we did this analysis as MI differences could be a consequence of signal-to-noise differences in the original data (which would be reflected in power effects). On the other hand, generic delta power has been associated with language processing (<xref ref-type="bibr" rid="bib36">Meyer, 2018</xref>; <xref ref-type="bibr" rid="bib25">Kazanina and Tavano, 2021</xref>). Therefore, we choose to analyse classical frequency bands instead of the stimulus informed ones (as used in the MI analysis) in order to compare these results with other studies. Moreover, as this analysis does not measure tracking to the stimulus (like the MI analysis does) it did not seem appropriate to match the frequency content to the stimulus content. We first extracted the timeâfrequency representation for all conditions and ROIs separately. To do so, we performed a wavelet analysis with a width of 4, with a frequency of interest between 1 and 30 (step size of 1) and time of interest between â0.2 and 3.7 s (step size of 0.05 s). We extracted the logarithm of the power and baseline corrected the data in the frequency domain using a â0.3 and â0.1 s window. For four different frequency bands (delta: 0.5â3.0 Hz; theta: 3.0â8.0 Hz; alpha: 8.0â15.0 Hz; beta: 15.0â25.0 Hz) we extracted the mean power in the 0.5â3.7 s time window per task, condition, and ROI. Again, our main analysis focuses on the delta band, but we also report on the other bands for completeness. For each ROI, we performed the statistical analysis on power as described in the behavioural analysis.</p></sec><sec id="s4-8"><title>Connectivity analysis</title><p>For the connectivity analysis, we repeated all pre-processing as in the power analysis, but separately for the left and right hemispheres (as we did not expect connections for PCA across hemispheres), after which we averaged the connectivity measure across hemispheres (using the Fourier spectrum and not the power spectrum). We used the debiased WPLI for our connectivity measure, which ensures that no zero-lag phase differences are included in the estimation (avoiding effects due to volume conduction). All connections between the three ROIs were investigated for the mean WPLI for the four different frequency bands also used in the power analysis in the 0.5â3.7 s time window. Also in this case, the same statistical analysis was applied. Note that not for all frequency bands we found a clear peak in the power signal (only clearly so for the alpha band), this indicates that the connectivity likely does not reflect endogenous oscillatory activity (<xref ref-type="bibr" rid="bib6">Donoghue et al., 2020</xref>), but might still pertain to connected regions operating at those timescales.</p></sec><sec id="s4-9"><title>MEGâbehavioural performance analysis</title><p>To investigate the relation between the MEG measures and the behavioural performance we repeated the analyses (MI, power, and connectivity) but added accuracy as a factor (together with the interactions with the task and condition factor). As there is no accuracy for the passive task, we removed this task from the analysis. We then followed the same analyse steps as before. Since we reduced our degree of freedom, we could however only create random intercept and not random slope models.</p></sec><sec id="s4-10"><title>Power control analysis</title><p>The reliability of phase estimations is influenced by the signal-to-noise ratio of the signal (<xref ref-type="bibr" rid="bib58">Zar, 1998</xref>). As a consequence, trials with generally high power have more reliable phase estimations compared to low power trials. This could influence any measure relying on this phase estimation, such as MI and connectivity (<xref ref-type="bibr" rid="bib21">Ince et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Bastos and Schoffelen, 2015</xref>). It is therefore possible that power differences between conditions lead to differences between connectivity or MI. To ensure that our reported effects are not due to signal-to-noise effects, we controlled any significant power difference between conditions for the connectivity and MI analysis. To do this, we iteratively removed the highest and lowest power trials between the mean highest and mean lowest of the two relevant conditions (either collapsing trials across tasks/conditions or using individual conditions; for the MI analysis we used power estimated within its respective frequency band). We repeated this until the original condition with the highest power had lower power than the other condition. Then we repeated the analysis and statistics, investigating if the effect of interest was still significant. The control analysis is reported along the main MI and connectivity sections.</p></sec><sec id="s4-11"><title>Other control analysis</title><p>We performed two final control analyses. Firstly, we investigated if age had an influence on any of our primary outcome measures. Secondly, we repeated the analyses using either 10 or 30 PCA components instead of the original 20 components. These controls ensure a robustness check of all the reported results. Note that this study was not intended to investigate age related differences. We therefore only report on interaction effect with our task and condition variables. Any main effect of age is difficult to interpret as it is unclear if the effect pertains to overall age-related differences (or anatomical variation leading to differential MEG responses) or language-related age differences.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf3"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Supervision, Validation, Visualization, Writing - original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Investigation, Methodology, Software, Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Methodology, Software, Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Funding acquisition, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing â review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Participants performed a screening for their eligibility in the MEG and MRI and gave written informed consent. The study was approved by the Ethical Commission for human research Arnhem/Nijmegen (project number CMO2014/288). Participants were reimbursed for their participation.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-77468-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and analysis code are available at <ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/collections/di/dccn/DSC_3027006.01_220">https://data.donders.ru.nl/collections/di/dccn/DSC_3027006.01_220</ext-link> (doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/vjw9-0572">https://doi.org/10.34973/vjw9-0572</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>ten Oever</surname><given-names>S</given-names></name><name><surname>Carta</surname><given-names>S</given-names></name><name><surname>Kaufeld</surname><given-names>G</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Task relevant tracking of hierarchical linguistic structure</data-title><source>Donders Repository</source><pub-id pub-id-type="doi">10.34973/vjw9-0572</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Tutorial Review of Functional Connectivity Analysis Methods and Their Interpretational Pitfalls</article-title><source>Frontiers in Systems Neuroscience</source><volume>9</volume><elocation-id>175</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00175</pub-id><pub-id pub-id-type="pmid">26778976</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The Neural Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees</article-title><source>Neuron</source><volume>88</volume><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.019</pub-id><pub-id pub-id-type="pmid">26447569</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id><pub-id pub-id-type="pmid">26642090</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal modulations in speech and music</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>81</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.02.011</pub-id><pub-id pub-id-type="pmid">28212857</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>NeuroImage</source><volume>85 Pt 2</volume><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id><pub-id pub-id-type="pmid">23791839</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoghue</surname><given-names>T</given-names></name><name><surname>Haller</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>EJ</given-names></name><name><surname>Varma</surname><given-names>P</given-names></name><name><surname>Sebastian</surname><given-names>P</given-names></name><name><surname>Gao</surname><given-names>R</given-names></name><name><surname>Noto</surname><given-names>T</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Shestyuk</surname><given-names>A</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parameterizing neural power spectra into periodic and aperiodic components</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1655</fpage><lpage>1665</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00744-x</pub-id><pub-id pub-id-type="pmid">33230329</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Thinking ahead: the role and roots of prediction in language comprehension</article-title><source>Psychophysiology</source><volume>44</volume><fpage>491</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00531.x</pub-id><pub-id pub-id-type="pmid">17521377</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fegen</surname><given-names>D</given-names></name><name><surname>Buchsbaum</surname><given-names>BR</given-names></name><name><surname>DâEsposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The effect of rehearsal rate and memory load on verbal working memory</article-title><source>NeuroImage</source><volume>105</volume><fpage>120</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.034</pub-id><pub-id pub-id-type="pmid">25467303</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>SL</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Lexical representation explains cortical entrainment during speech comprehension</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0197304</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0197304</pub-id><pub-id pub-id-type="pmid">29771964</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The brain basis of language processing: from structure to function</article-title><source>Physiological Reviews</source><volume>91</volume><fpage>1357</fpage><lpage>1392</lpage><pub-id pub-id-type="doi">10.1152/physrev.00006.2011</pub-id><pub-id pub-id-type="pmid">22013214</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelfand</surname><given-names>JR</given-names></name><name><surname>Bookheimer</surname><given-names>SY</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dissociating neural mechanisms of temporal sequencing and processing phonemes</article-title><source>Neuron</source><volume>38</volume><fpage>831</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00285-x</pub-id><pub-id pub-id-type="pmid">12797966</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Wilson</surname><given-names>JA</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Webster</surname><given-names>M</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title><source>NeuroImage</source><volume>80</volume><fpage>105</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id><pub-id pub-id-type="pmid">23668970</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hoogenboom</surname><given-names>N</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech Rhythms and Multiplexed Oscillatory Sensory Coding in the Human Brain</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001752</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The core and beyond in the language-ready brain</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>81</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.01.048</pub-id><pub-id pub-id-type="pmid">28193452</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halle</surname><given-names>M</given-names></name><name><surname>Stevens</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Speech recognition: A model and A program for research</article-title><source>IEEE Transactions on Information Theory</source><volume>8</volume><fpage>155</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1109/TIT.1962.1057686</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanslmayr</surname><given-names>S</given-names></name><name><surname>Staudigl</surname><given-names>T</given-names></name><name><surname>Fellner</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Oscillatory power decreases and long-term memory: the information via desynchronization hypothesis</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00074</pub-id><pub-id pub-id-type="pmid">22514527</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Har-Shai Yahav</surname><given-names>P</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linguistic processing of task-irrelevant speech at a cocktail party</article-title><source>eLife</source><volume>10</volume><elocation-id>e65096</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65096</pub-id><pub-id pub-id-type="pmid">33942722</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via A gaussian copula</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>1541</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1002/hbm.23471</pub-id><pub-id pub-id-type="pmid">27860095</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Mazaheri</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Shaping functional architecture by oscillatory alpha activity: gating by inhibition</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>186</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00186</pub-id><pub-id pub-id-type="pmid">21119777</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufeld</surname><given-names>G</given-names></name><name><surname>Bosker</surname><given-names>HR</given-names></name><name><surname>Ten Oever</surname><given-names>S</given-names></name><name><surname>Alday</surname><given-names>PM</given-names></name><name><surname>Meyer</surname><given-names>AS</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Linguistic Structure and Meaning Organize Neural Oscillations into a Content-Specific Hierarchy</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>9467</fpage><lpage>9475</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0302-20.2020</pub-id><pub-id pub-id-type="pmid">33097640</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>SJ</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>14691</fpage><lpage>14701</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2243-15.2015</pub-id><pub-id pub-id-type="pmid">26538641</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kazanina</surname><given-names>N</given-names></name><name><surname>Tavano</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What Neural Oscillations Can(Not) Do for Syntactic Structure Building</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/yv2tm</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Individual Human Brain Areas Can Be Identified from Their Characteristic Spectral Activation Fingerprints</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002498</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002498</pub-id><pub-id pub-id-type="pmid">27355236</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id><pub-id pub-id-type="pmid">29529019</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>EEG alpha and theta oscillations reflect cognitive and memory performance: a review and analysis</article-title><source>Brain Research. Brain Research Reviews</source><volume>29</volume><fpage>169</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/s0165-0173(98)00056-3</pub-id><pub-id pub-id-type="pmid">10209231</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Schulze</surname><given-names>K</given-names></name><name><surname>Sammler</surname><given-names>D</given-names></name><name><surname>Fritz</surname><given-names>T</given-names></name><name><surname>MÃ¼ller</surname><given-names>K</given-names></name><name><surname>Gruber</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Functional architecture of verbal and tonal working memory: an FMRI study</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>859</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1002/hbm.20550</pub-id><pub-id pub-id-type="pmid">18330870</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Karmos</surname><given-names>G</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection</article-title><source>Science</source><volume>320</volume><fpage>110</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1126/science.1154735</pub-id><pub-id pub-id-type="pmid">18388295</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id><pub-id pub-id-type="pmid">17582338</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name><name><surname>Welsh</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Processing interactions and lexical access during word recognition in continuous speech</article-title><source>Cognitive Psychology</source><volume>10</volume><fpage>29</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(78)90018-X</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Language Processing as Cue Integration: Grounding the Psychology of Language in Perception and Neurophysiology</article-title><source>Frontiers in Psychology</source><volume>7</volume><elocation-id>120</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.00120</pub-id><pub-id pub-id-type="pmid">26909051</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>AE</given-names></name><name><surname>Doumas</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predicate learning in neural systems: using oscillations to discover latent structure</article-title><source>Current Opinion in Behavioral Sciences</source><volume>29</volume><fpage>77</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.04.008</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Compositional Neural Architecture for Language</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>1407</fpage><lpage>1427</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01552</pub-id><pub-id pub-id-type="pmid">32108553</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The neural oscillations of speech processing and language comprehension: state of the art and emerging mechanisms</article-title><source>The European Journal of Neuroscience</source><volume>48</volume><fpage>2609</fpage><lpage>2621</lpage><pub-id pub-id-type="doi">10.1111/ejn.13748</pub-id><pub-id pub-id-type="pmid">29055058</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Synchronous, but not entrained: exogenous and endogenous cortical rhythms of speech and language processing</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>1089</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1080/23273798.2019.1693050</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>MJ</given-names></name><name><surname>El Karoui</surname><given-names>I</given-names></name><name><surname>Giber</surname><given-names>K</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Koopman</surname><given-names>H</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Hale</surname><given-names>JT</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neurophysiological dynamics of phrase-structure building during sentence processing</article-title><source>PNAS</source><volume>114</volume><fpage>E3669</fpage><lpage>E3678</lpage><pub-id pub-id-type="doi">10.1073/pnas.1701590114</pub-id><pub-id pub-id-type="pmid">28416691</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Herrmann</surname><given-names>B</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural Oscillations in Speech: Donât be Enslaved by the Envelope</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>250</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00250</pub-id><pub-id pub-id-type="pmid">22969717</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural Entrainment and Attentional Selection in the Listening Brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>913</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.08.004</pub-id><pub-id pub-id-type="pmid">31606386</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osaka</surname><given-names>N</given-names></name><name><surname>Osaka</surname><given-names>M</given-names></name><name><surname>Kondo</surname><given-names>H</given-names></name><name><surname>Morishita</surname><given-names>M</given-names></name><name><surname>Fukuyama</surname><given-names>H</given-names></name><name><surname>Shibasaki</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural basis of executive function in working memory: an fMRI study based on individual differences</article-title><source>NeuroImage</source><volume>21</volume><fpage>623</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.09.069</pub-id><pub-id pub-id-type="pmid">14980565</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural Oscillations Carry Speech Rhythm through to Comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pellegrino</surname><given-names>F</given-names></name><name><surname>CoupÃ©</surname><given-names>C</given-names></name><name><surname>Marsico</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Across-Language Perspective on Speech Information Rate</article-title><source>Language</source><volume>87</volume><fpage>539</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1353/lan.2011.0057</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinker</surname><given-names>S</given-names></name><name><surname>Jackendoff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The faculty of language: whatâs special about it?</article-title><source>Cognition</source><volume>95</volume><fpage>201</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2004.08.004</pub-id><pub-id pub-id-type="pmid">15694646</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname><given-names>JM</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Acoustically Driven Cortical Î´ Oscillations Underpin Prosodic Chunking</article-title><source>ENeuro</source><volume>8</volume><elocation-id>ENEURO.0562-20.2021</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0562-20.2021</pub-id><pub-id pub-id-type="pmid">34083380</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Temporal information in speech: acoustic, auditory and linguistic aspects</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>336</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0070</pub-id><pub-id pub-id-type="pmid">1354376</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Cordova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural representations of events arise from temporal community structure</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>486</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1038/nn.3331</pub-id><pub-id pub-id-type="pmid">23416451</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The NOLB model: A model of the natural organization of language and the brain</article-title><source>APA Psycnet</source><volume>1</volume><elocation-id>e6</elocation-id><pub-id pub-id-type="doi">10.1017/CBO9781107323667.006</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>ZM</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title><source>Nature</source><volume>416</volume><fpage>87</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1038/416087a</pub-id><pub-id pub-id-type="pmid">11882898</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Todorovic</surname><given-names>A</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Online and offline tools for head movement compensation in MEG</article-title><source>NeuroImage</source><volume>68</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.11.047</pub-id><pub-id pub-id-type="pmid">23246857</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ten Oever</surname><given-names>S</given-names></name><name><surname>Sack</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Oscillatory phase shapes syllable perception</article-title><source>PNAS</source><volume>112</volume><fpage>15833</fpage><lpage>15837</lpage><pub-id pub-id-type="doi">10.1073/pnas.1517519112</pub-id><pub-id pub-id-type="pmid">26668393</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ten Oever</surname><given-names>S</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An oscillating computational model can track pseudo-rhythmic speech by using linguistic predictions</article-title><source>eLife</source><volume>10</volume><elocation-id>e68066</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.68066</pub-id><pub-id pub-id-type="pmid">34338196</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turken</surname><given-names>AU</given-names></name><name><surname>Dronkers</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neural architecture of the language comprehension network: converging evidence from lesion and connectivity analyses</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00001</pub-id><pub-id pub-id-type="pmid">21347218</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vouloumanos</surname><given-names>A</given-names></name><name><surname>Kiehl</surname><given-names>KA</given-names></name><name><surname>Werker</surname><given-names>JF</given-names></name><name><surname>Liddle</surname><given-names>PF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Detection of sounds in the auditory stream: event-related fMRI evidence for differential activation to speech and nonspeech</article-title><source>Journal of Cognitive Neuroscience</source><volume>13</volume><fpage>994</fpage><lpage>1005</lpage><pub-id pub-id-type="doi">10.1162/089892901753165890</pub-id><pub-id pub-id-type="pmid">11595101</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaccarella</surname><given-names>E</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name><name><surname>Makuuchi</surname><given-names>M</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Building by Syntax: The Neural Basis of Minimal Linguistic Structures</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>411</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv234</pub-id><pub-id pub-id-type="pmid">26464476</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zar</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Biostatistical Analysis</source><edition>4 ed</edition><publisher-loc>Englewood Cliffs, New Jersey</publisher-loc><publisher-name>Prentice Hall</publisher-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname><given-names>B</given-names></name><name><surname>Archer-Boyd</surname><given-names>A</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Phase Entrainment of Brain Oscillations Causally Modulates Neural Responses to Intelligible Speech</article-title><source>Current Biology</source><volume>28</volume><fpage>401</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.071</pub-id><pub-id pub-id-type="pmid">29358073</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname><given-names>B</given-names></name><name><surname>Ten Oever</surname><given-names>S</given-names></name><name><surname>Sack</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>The Involvement of Endogenous Neural Oscillations in the Processing of Rhythmic Input: More Than a Regular Repetition of Evoked Neural Responses</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>95</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00095</pub-id><pub-id pub-id-type="pmid">29563860</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77468.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Washington University in St. Louis</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.02.08.479571" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.02.08.479571"/></front-stub><body><p>This MEG study elegantly assesses human brain responses to spoken language at the syllable, word, and sentence level. Although prior studies have shown significant cortical tracking of the speech signal, the current work uses clever task manipulation to direct attention to different timescales of speech, thus demonstrating tracking mechanisms that are both automatic and task-dependent operate in tandem during spoken language comprehension.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77468.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Washington University in St. Louis</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rimmele</surname><given-names>Johanna</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000rdbk18</institution-id><institution>Max-Planck-Institute for Empirical Aesthetics</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.02.08.479571">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.02.08.479571v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Task-dependent and automatic tracking of hierarchical linguistic structure&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Johanna Rimmele (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1. The relationship of participants' behavior to the observed MEG responses. One clear concern is the degree to which the accuracy differences between word lists and sentences (Figure 1) explain MI differences.</p><p>2. A number of important details regarding analysis choices were missing and should be added, including: justification of the frequency bands used for the MI analysis, task order (were sentences always first, and word lists second), whether ROIs were defined across hemispheres, details on signal filtering, and details regarding the PCA analysis.</p><p>3. The use of &quot;hierarchical linguistic structure&quot; should be clarified or tempered in the context of the current results. Although the attention was directed to syllables, words, and phrases through instructions, the significant MEG results were in the phrasal band (Figure 3) and not the others (supplemental figures).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The participants ranged in age from 18-59, which is a rather broad range. Given age-related changes in both hearing (not assessed) and language processing, it was surprising to not at least see age included in the models (though perhaps there were not enough participants?). Some comment would be useful here.</p><p>Figures: The y axis label seems frequently oddly positioned at the top â maybe consider just centering it on the axis?</p><p>I liked the inset panels for the main effects, but they are rather small. If you can make them any larger that would improve readability.</p><p>In general, I really like showing individual subjects in addition to means. However, it also can make the plots a bit busy. I imagine you've tried other ways of plotting but might be worth exploring ways to highlight the means a bit more (or simplify the main paper figures, and keep these more detailed figures for supplemental)? Totally up to you, but I had a hard time seeing the trends as the plots currently stand.</p><p>On my copy of the PDF, Figures 6 and 4 (p. 14) were overlapping and so really impossible to see.</p><p>Figure 1: In the text, the condition is referred to as &quot;word list&quot;, but in the figure &quot;wordlist&quot;. Having these match isn't strictly necessary but would be a nice touch.</p><p>Regarding accuracy differences: including accuracy in the models are good, but alternately, restricting analyses to only correct responses might also help with this?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>â l. 122: does that mean the sentence was always presented first and the word-list second? The word-list did not contain the same words as the sentence, right? This might have affected differences in tracking and power.</p><p>â l. 197: why were the linear mixed-models for the MEG data performed separately per ROI? Also, were ROIs computed across hemispheres?</p><p>â l. 198: was the procedure described by Ince et al., 2017 using gaussian copula used for the MI analysis, if yes could you cite the reference?</p><p>â l. 191: could you give the details of the filter, what kind of filter etc?</p><p>â l. 191/l. 206: why were the frequency bands for the power analysis (Î´: 0.5-3.0 Hz; theta: 3.0-8.0 Hz; Î±: 8.0-15.0 Hz; Î²: 15.0-25.0 Hz) different than for the mutual information analysis (l. 191: phrase (0.8-1.1 Hz), word (1.9-2.8 Hz), and syllable (3.5- 5.0 Hz)), could you explain the choice of frequency band? Particularly, if the authors want to check signal-to-noise differences in the mutual information analysis by using the power analysis, it seems relevant to match frequency bands.</p><p>â l. 207: why was the data averaged across trials and not the single-trial data fed into the mixed-models, such an analysis might strengthen the findings?</p><p>â l. 213: averaged across hemispheres or across trials?</p><p>â l. 216: all previous analyses were conducted across the hemispheres?</p><p>â l. 217: &quot;for the four different frequency bands&quot; this is referring to the frequency bands chosen for the power analysis, not those in the MI analysis?</p><p>â Figure 1: were only correct trials analyzed in the MEG analysis? If not, this might be a problem as there were more correct trials in the sentence compared to the word-list condition at the phrasal scale task. Could you add a control analysis on the correct trials only, to make sure that this was not a confound?</p><p>â l. 252 ff./Figure 2: As no power peak is observed in the Î´ (and Î±) band, this might result in spurious connectivity findings.</p><p>â l. 304: with higher connectivity in the phrasal compared to the passive task? Could you add this info?</p><p>â Something went wrong with figure 6.</p><p>â l. 403: alignment of neural oscillations with acoustics at phrasal scale, this reference seems relevant here: Rimmele, Poeppel, Ghitza, 2021.</p><p>â Wording: l. 387 &quot;in STG&quot;.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. In procedure settings, I am not sure whether the sentence condition is always presented before the word list condition (the misunderstanding comes from lines 126-128). Please add the necessary details and avoid this misunderstanding.</p><p>2. In the MEG analysis, PCA was performed to reduce computational load and increase the data relevance. However, the PCA procedure is not clear. For example, the authors extracted top *20 PCA components per region. Why 20 components? And why not 10 or 30 components? The details should be clarified.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77468.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The relationship of participants' behavior to the observed MEG responses. One clear concern is the degree to which the accuracy differences between word lists and sentences (Figure 1) explain MI differences.</p><p>2. A number of important details regarding analysis choices were missing and should be added, including: justification of the frequency bands used for the MI analysis, task order (were sentences always first, and word lists second), whether ROIs were defined across hemispheres, details on signal filtering, and details regarding the PCA analysis.</p><p>3. The use of &quot;hierarchical linguistic structure&quot; should be clarified or tempered in the context of the current results. Although the attention was directed to syllables, words, and phrases through instructions, the significant MEG results were in the phrasal band (Figure 3) and not the others (supplemental figures).</p></disp-quote><p>We thank the Editor and the Reviewers for their helpful comments and constructive feedback. We feel that the reviews have substantially improved our manuscript. We have now addressed these three core concerns, and detail at length in individual responses to each reviewer how and what can now additionally be shown. To summarize briefly here:</p><p>1. We now include a new analysis in which we add accuracy as a factor. None of the original statistical patterns changed. We still find that MI/ neural tracking is higher for phrases in sentences than in word lists and that tracking during spoken language processing is a largely automatic response. Task specific effect were still found in in MTG and IFG. Thus, accuracy differences did not explain MI difference and we therefore remain with our core messages of the paper. We would like to note that we do no use tasks in the same way that they are typically used in cognitive neuroscience (say in a working memory paradigm, where only neural activity on correct trials can be associated the cognitive processing in question; instead we use tasks to direct our participantâs attention to syllables, words, and phrases and the timescale that they occur in). This difference in design â in other words, that in our case, that behavioral performance on the syllable task was lower than in the word or phrase tasks, does not mean that the sentence or word list was not heard/comprehended â means that inclusion or exclusion of incorrect trials does not further isolate the cognitive process of spoken language comprehension in question. That said, we agree it is very important to show that MI differences do not stem from behavioral performance differences. In order to show this, we add the above mentioned analysis to the manuscript. As said, this analysis did not change any of our main findings or conclusions, and rather strengthened the argument that tracking of phrases in sentences vs. word lists is stronger. We report now report these results in both our response and the manuscript.</p><p>2. We believe that we have fully addressed these queries and thank the reviewers for encouraging us be more comprehensive and thorough. We note that due to our 2-condition by 4-task design, we did not achieve enough power to include ROI as an independent factor. This is obviously not ideal, but embodies a trade-off between testing these 8 conditions together, which we felt was crucial for the inferences we wanted to make about tracking, and discovering underlying sources. We thus stuck to the well-known ROIs/ sources widely used and defined in the speech and language processing literature.</p><p>3. We regret any confusion this word might have caused; we meant to refer to the fact that our 4 tasks focus participantsâ attention to different timescales and linguistic representations occur on across the linguistic hierarchy. We have changed the title to more clearly reflect this and have removed reference to linguistic hierarchy throughout the paper.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The participants ranged in age from 18-59, which is a rather broad range. Given age-related changes in both hearing (not assessed) and language processing, it was surprising to not at least see age included in the models (though perhaps there were not enough participants?). Some comment would be useful here.</p></disp-quote><p>We indeed had quite a wide range of ages included in our study. Our wide age range was partly due to constraints in the MEG testing during covid times. It was difficult to find participants during the pandemic. On top of this, it is almost standard practice in the Netherlands to put dental wires behind the teeth after having braces and leave them in place for life. These participants cannot participate in MEG studies and therefore we widened the scope of our age range to reach enough participants. However, we also believe that it is good practice to not limit the participants age range to such a restricted range as is common practice in most cognitive neuroscience research. Otherwise, we end up only comparing brains of student populations with each other while we want to make conclusions for a much wider population. It is therefore rather an asset than a drawback that we have this wide age range. Regarding the hearing difficulties, we would like to note that we did ask participants whether they had any hearing problems (which none of them reported), but indeed did not assess did ourselves.</p><p>Nonetheless, we agree that it is valuable to investigate whether age had an influence in any of our results and therefore added age in the models of the main manuscript (i.e. for the lower frequencies). Again, we could only run fixed intercept models due to a reduction in the degrees of freedom.</p><p>For the MI we found no significant effect of age or interactions for any of our models. There was a trend significant effect in MTG for task*age interaction (F(3,123) = 2.5874, p = 0.0561). However, also the task*condition interaction remained significant and thus not change any conclusions.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Age effects on power estimates.</title><p>(<bold>A</bold>) Predicted values for Î´ power for the two conditions dependent on age in STG (left) and MTG (right). (<bold>B</bold>) Predicted values for Î´ power for the four tasks dependent on age in MTG. Error bars indicate the 95% confidence interval of the fit. Colored lines at the bottom indicate individual datapoints.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-sa2-fig1-v2.tif"/></fig><p>For power we found a significant interaction between condition and age in STG (F(1, 28.88), p = 0.0192) and MTG (F(1,31) = 10.31, p = 0.003). In the MTG we additionally found an interaction between task and ages (F(3,31) = 5.02, p = 0.006). In IFG we only found a main effect of age (F(1,43) = 5.067, p = 0.030). For connectivity we found overall a main effect of age for all connections (STG-MTG, F(1,17.12) = 10.09, p = 0.005), (STG-IFG, F(1,16.96) = 17.42, p &lt; 0.001, MTG-IFG, F(1,17.058) = 12.478, p = 0.002). The condition*age interaction in STG and MTG both suggested only for wordlist a change in power with age and not for the sentence condition (follow-up correlation age-MI per condition. STG: p = 0.076 (uncorrected) and MTG: p = 0.023 (uncorrected)). The task*age interaction in MTG showed only for the passive task a significant effect of age (follow-up correlation age-MI per task. p = 0.028 (uncorrected)).</p><p>Generally, we found it difficult to make strong conclusions about the main effect of age. Firstly, we did not have any baseline to assess whether main effects are specific to our language tasks or a more general age-related difference. Secondly, there could be anatomical age-related differences that spuriously drive the main effects of age. Therefore, we only report on the interaction effects. These analyses are now in the manuscript and there is an additional figure in the supplementary materials.</p><p>The results now read:</p><p>âAge control. Adding age to the analysis did not change any of the original findings (all original effects were still significant). We did however find for the power analysis age-specific interactions with condition and task. Specifically, for both the STG and the MTG we found an interaction between age and condition (F(1,28.87) = 6.156, p = 0.0192 and F(1,31) = 10.31, p = 0.003). In both ROIs there was a stronger difference between sentences and word lists (higher Î´ power for sentences) for the younger compared to the older participants (Supplementary Figure 4). In the MTG there was also an interaction between task and age (F(1,31) = 5.020, p = 0.006). Here, in a follow-up we found that only in the word task there was a correlation between age and power (p = 0.023 uncorrected), but not for the other tasks (p&gt;0.1).â.</p><disp-quote content-type="editor-comment"><p>Figures: The y axis label seems frequently oddly positioned at the top â maybe consider just centering it on the axis?</p></disp-quote><p>We changed the y-axes of the figures accordingly and centered them all.</p><disp-quote content-type="editor-comment"><p>I liked the inset panels for the main effects, but they are rather small. If you can make them any larger that would improve readability.</p></disp-quote><p>We increased the size of the insets. Note however that as there is no space to add any y-labels these insets do not reflect real data, but just to quickly show the direction and presence of a main effect. We added this information now in the figure legend to ensure that this is clear.</p><disp-quote content-type="editor-comment"><p>In general, I really like showing individual subjects in addition to means. However, it also can make the plots a bit busy. I imagine you've tried other ways of plotting but might be worth exploring ways to highlight the means a bit more (or simplify the main paper figures, and keep these more detailed figures for supplemental)? Totally up to you, but I had a hard time seeing the trends as the plots currently stand.</p></disp-quote><p>There is a difficult balance to showing everything and highlighting the important bits. We were happy the reviewer pointed out that all trends were difficult to see and now change the figures to only include the mean and SEM. Additionally, when we found a main effect of task, we also added the condition means to highlight the differences on the significant effect more. The original plots are now added in the supplementary materials.</p><disp-quote content-type="editor-comment"><p>On my copy of the PDF, Figures 6 and 4 (p. 14) were overlapping and so really impossible to see.</p></disp-quote><p>We are so sorry about this! We realized this was the case due to the pdf conversion from our own word document. We now double-checked that this wasnât the case.</p><disp-quote content-type="editor-comment"><p>Figure 1: In the text, the condition is referred to as &quot;word list&quot;, but in the figure &quot;wordlist&quot;. Having these match isn't strictly necessary but would be a nice touch.</p></disp-quote><p>We change the condition to word list throughout.</p><disp-quote content-type="editor-comment"><p>Regarding accuracy differences: including accuracy in the models are good, but alternately, restricting analyses to only correct responses might also help with this?</p></disp-quote><p>We have looked into this analysis, but there are some issues with doing the analysis in this way. First, there are clear differences in difficulty level of the trials within a condition. For example, if the target question was related to the last part of the audio fragment, the task was much easier than when it was at the beginning of the audio fragment. In the syllable task, if syllables also were (by chance) a part-word, the trial was also much easier. If we were to split up in correct and incorrect trials we would not really infer solely processes due to accurately processing the speech fragments, but also confounded the analysis by the individual difficulty level of the trials. Second, we end up with a very low trial amount after only looking at incorrect trials. In the worst case, some participants end up with only 22 trials which is too little to do much. We think it is therefore fair to compare the accuracy across participants (as done in the other analysis), but it is difficult to do this within participants.</p><p>To acknowledge this, we added this limitation to the methods. The method now reads:</p><p>âNote that different trials within a task were not matched for task difficulty. For example, in the syllable task syllables that make a word are much easier to recognize than syllables that do not make a word. Additionally, trials pertaining to the beginning of the sentence are more difficult than ones related to the end of the sentence due to recency effects.â</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>â l. 122: does that mean the sentence was always presented first and the word-list second? The word-list did not contain the same words as the sentence, right? This might have affected differences in tracking and power.</p></disp-quote><p>We regret the confusion. We randomized whether the sentence or wordlist would come first across participants, but kept it constant within the participants across the four tasks. So, an individual would always alternate between a sentence and a word list block, but which one is first is counterbalanced across participants. To control for acoustic differences, we have the same words in the wordlists and the sentences (see e.g. Kaufeld et al., 2020). But this cannot have affected the results as the order is counterbalanced across participants. We now made clearer in the text how we assign the blocks.</p><disp-quote content-type="editor-comment"><p>â l. 197: why were the linear mixed-models for the MEG data performed separately per ROI? Also, were ROIs computed across hemispheres?</p></disp-quote><p>We had a clear a-priori idea about choosing these ROIs, therefore we did the analysis separately. Post-hoc, there now is also the simple issue of having too little power to estimate a three-way interaction between ROI, task, and condition. This power situation arises because we needed to test 4 task * 2 condition together in a single experiment in order to be able to make the inferences that we wanted to about the effects on tracking. This unfortunately comes at the cost of the power we can allot to each condition which affects our ability to generate source models with ROIs as a factor. Ideally, this would have been done, but practically this is very difficult to achieve. We cannot do more than acknowledge this in the main text (now in line 195-199). It is simply very difficult to have any design with multiple factors on top of an anatomical constraint (most studies seem to limit themselves to two factors these days to avoid these issues). The ROIs were computed across hemisphere (we had no a-priori hypothesis about hemispheres). Note that the PCAs therefore we also calculated across hemispheres (except for the coherence analysis as cross-hemisphere coherence seemed unlikely to us). Please note that we do not use ROIs that deviate from the established speech and language processing literature.</p><p>We now write:</p><p>âAll following analyses were done per ROI. With enough statistical power one would add ROI as a separate factor in the analyses, but unfortunately, we did not have enough power to find a potential three-way interaction (ROI*condition*task). We therefore cannot make strong conclusions about one ROI having a stronger effect than another.â</p><disp-quote content-type="editor-comment"><p>â l. 198: was the procedure described by Ince et al., 2017 using gaussian copula used for the MI analysis, if yes could you cite the reference?</p></disp-quote><p>We regret not citing it in the main text and do this now.</p><disp-quote content-type="editor-comment"><p>â l. 191: could you give the details of the filter, what kind of filter etc?</p></disp-quote><p>We used a third order (bi-directional) Butterworth filter separately on eight equidistant bands of the speech signal. This information is now added.</p><disp-quote content-type="editor-comment"><p>â l. 191/l. 206: why were the frequency bands for the power analysis (Î´: 0.5-3.0 Hz; theta: 3.0-8.0 Hz; Î±: 8.0-15.0 Hz; Î²: 15.0-25.0 Hz) different than for the mutual information analysis (l. 191: phrase (0.8-1.1 Hz), word (1.9-2.8 Hz), and syllable (3.5- 5.0 Hz)), could you explain the choice of frequency band? Particularly, if the authors want to check signal-to-noise differences in the mutual information analysis by using the power analysis, it seems relevant to match frequency bands.</p></disp-quote><p>The frequency bands of the MI analyses were based on the stimuli. They reflect the syllabic, word, and phrasal rates (calculated in Kaufeld et al., 2020). The power analyses were based on generic frequency bands. We choose for these two different splits as for the tracking one would expect the tracking (or phase alignment) from the exact frequency ranges in the signal relating to linguistic content (Ding et al., 2016; Martin, 2020; Keitel et al., 2018). In contrast, if there is no alignment, it is still possible that oscillatory signals are important in processing language stimuli (Jensen et al., 2010; Benitez-Burraco and Murphy, 2019). As these hypotheses rather pertain to commonly known frequency bands present in the brain we think it is more appropriate here to use generic bands (also to promote comparisons). We therefore stick to the stimulus-driven frequencies for the tracking hypothesis and use for control frequencies the generic ones (as we do not match them with anything in the stimulus). Note however, that when controlling for power in both the MI and the connectivity analysis, we did use the power of the respective band used for that specific analysis.</p><p>We write this reasoning now clearer in the methods:</p><p>âPower analysis was performed to compare the MI results with absolute power changes. On the one hand, we did this analysis as MI differences could be a consequence of signal-to-noise differences in the original data (which would be reflected in power effects). On the other hand, generic Î´ power has been associated with language processing [27, 28]. Therefore, we choose to analyse classical frequency bands instead of the stimulus informed ones (as used in the MI analysis) in order to compare these results with other studies. Moreover, as this analysis does not measure tracking to the stimulus (like the MI analysis does) it did not seem appropriate to match the frequency content to the stimulus content.â</p><disp-quote content-type="editor-comment"><p>â l. 207: why was the data averaged across trials and not the single-trial data fed into the mixed-models, such an analysis might strengthen the findings?</p></disp-quote><p>We could do this for the power analysis, but not for the MI and connectivity analysis which were calculated incorporating all trials (for the MI by concatenating all trials/speech signals, and for the connectivity as connectivity is calculated across trials). This would mean that we would run a different model on the power as on the other two variables of interest. We choose not to do this. Previously, MI analyses have been done by concatenating the data to improve sensitivity of the measure (Keitel et al., 2020; Kaufeld et al., 2018). Theoretically one could calculate the MI per trial, however this would unnecessarily reduce the sensitivity of the analysis.</p><disp-quote content-type="editor-comment"><p>â l. 213: averaged across hemispheres or across trials?</p></disp-quote><p>Across hemispheres. The WPLI is calculated across trials, so cannot be averaged across trials. We now clarified this.</p><disp-quote content-type="editor-comment"><p>â l. 216: all previous analyses were conducted across the hemispheres?</p></disp-quote><p>Yes. The MI and the power used the principles components incorporating both hemispheres, but for the connectivity analysis we calculated the PCAs for the hemispheres separately and averages only after calculating the connectivity across hemispheres.</p><disp-quote content-type="editor-comment"><p>â l. 217: &quot;for the four different frequency bands&quot; this is referring to the frequency bands chosen for the power analysis, not those in the MI analysis?</p></disp-quote><p>Yes. This is now clarified. As stated above there it makes more sense to look at generic frequency bands when we do not directly link the analysis with the speech signal.</p><disp-quote content-type="editor-comment"><p>â Figure 1: were only correct trials analyzed in the MEG analysis? If not, this might be a problem as there were more correct trials in the sentence compared to the word-list condition at the phrasal scale task. Could you add a control analysis on the correct trials only, to make sure that this was not a confound?</p></disp-quote><p>We included all trials in the analysis; we do this for several reasons, first because we not use the tasks in the same way as a traditional working memory task, where only correct trials contain the cognitive process that is being studied. Second, because it is not possible to only include the correct trials as we end up with too few trials per condition to reliably estimate our effect. Moreover, trials are not controlled for difficulty within a condition. However, in order to show that our MI effects are not driven by task performance, we can include accuracy across participants as a factor. We refer to point 1 for the results of this analysis. We have added this information now in the methods:</p><p>âNote that different trials within a task were not matched for task difficulty. For example, in the syllable task syllables that make a word are much easier to recognize than syllables that do not make a word. Additionally, trials pertaining to the beginning of the sentence are more difficult than ones related to the end of the sentence due to recency effects.â</p><disp-quote content-type="editor-comment"><p>â l. 252 ff./Figure 2: As no power peak is observed in the Î´ (and Î±) band, this might result in spurious connectivity findings.</p></disp-quote><p>Indeed, the absence of a peak does result in difficulty to interpret connectivity findings as it is unknown whether we are truly investigating an endogenous oscillation or some other form of connections which could for example be related to stimulus evoked responses. If the latter, we could not speak of âtrueâ connectivity increases, but rather for differential processing of the stimulus. We acknowledge this, but still find also this type of change worth reporting. We therefore made clear cautious note in the Results section that this is the case.</p><p>The results now read:</p><p>âNote that not for all frequency bands we found a clear peak in the power signal (only clearly so for the Î± band), this indicates that the connectivity likely does not reflect endogenous oscillatory activity [29], but might still pertain to connected regions operating at those time scales.â</p><p>The discussion now reads:</p><p>âNote that even though we found increased connectivity, we did not see a clear power peak in the Î´ band. This suggest that we might not be looking at an endogenous oscillator, but rather at connections operation at that temporal scale (potentially being non-oscillatory in nature).â</p><disp-quote content-type="editor-comment"><p>â l. 304: with higher connectivity in the phrasal compared to the passive task? Could you add this info?</p></disp-quote><p>We have added this info.</p><disp-quote content-type="editor-comment"><p>â Something went wrong with figure 6.</p></disp-quote><p>We have realized this and regret it. The new manuscript should have the figures placed correctly.</p><disp-quote content-type="editor-comment"><p>â l. 403: alignment of neural oscillations with acoustics at phrasal scale, this reference seems relevant here: Rimmele, Poeppel, Ghitza, 2021.</p></disp-quote><p>Agreed at we have added this.</p><disp-quote content-type="editor-comment"><p>â Wording: l. 387 &quot;in STG&quot;.</p></disp-quote><p>We have changed this wording accordingly.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. In procedure settings, I am not sure whether the sentence condition is always presented before the word list condition (the misunderstanding comes from lines 126-128). Please add the necessary details and avoid this misunderstanding.</p></disp-quote><p>We regret the confusion. We randomized whether the sentence or wordlist would come first across participants, but kept it constant within the participants across the four tasks. So, an individual would always alternate between a sentence and a word list block, but which one is first is counterbalanced across participants. To control for acoustic differences, we have the same words in the wordlists and the sentences (see e.g. Kaufeld et al., 2020). But this cannot have affected the results as the order is counterbalanced across participants. We now made clearer in the text how we assign the blocks.</p><disp-quote content-type="editor-comment"><p>2. In the MEG analysis, PCA was performed to reduce computational load and increase the data relevance. However, the PCA procedure is not clear. For example, the authors extracted top *20 PCA components per region. Why 20 components? And why not 10 or 30 components? The details should be clarified.</p></disp-quote><p>As the reviewer also mentions, our main aim was to reduce the dimensions of our analysis (to reduce computational load), while keeping in enough variance relevant for the analysis. We are not aware of any established way to determine the amount of PCA components which is best to use and can therefore only look at the explained variance of the components. As for us the goal was to reduce computational load, we wanted to keep as much original variance in the analysis as possible. In <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> one can see the cumulative explained variance of the components. Around 20 components over 99.9 % of the data is explained, which seems sensible amount to keep in.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77468-sa2-fig2-v2.tif"/></fig><p>To ensure that all our results are also robust against changing the exact number of components we repeated the analysis using 10 and 30 components. No big qualitative differences were visible. It did seem that 10 components were not sufficient to show the original effects. The IFG task effect and MTG task*condition effect were only trend significant for 10 components (p = 0.06 and p = 0.1 respectively). The condition effect remained significant for all ROIs. Using 30 components all effects (including the main and interaction; IFG task effect: p = 0.034; MTG task*condition interaction p = 0.0064) effect remained significant. We therefore believe that the results are robust against choosing the exact amount of PCAs, but does require more than 10 components.</p><p>We now discuss the effect of PCA component number in the text. The text now reads:âOverall, the amount of PCA components did not influence any of the qualitative differences in the condition. It did seem however that 10 PCA components were not sufficient to show all original effects with the same power. Specifically, the IFG task and MTG task*condition effect were only trend significant for 10 components (p = 0.06 and p = 0.1 respectively). The other effects did remain significant with 10 components. Using 30 components made some of our effects stronger than with 20 components. Here, the IFG task and MTG task*condition effects had p-values of 0.034 and 0.006 respectively. We conclude that the amount of PCAs components did not qualitative change any of our reported effects.â</p></body></sub-article></article>