<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83139</article-id><article-id pub-id-type="doi">10.7554/eLife.83139</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural assemblies uncovered by generative modeling explain whole-brain activity statistics and reflect structural connectivity</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-291372"><name><surname>van der Plas</surname><given-names>Thijs L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5490-1785</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-115548"><name><surname>Tubiana</surname><given-names>Jérôme</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8878-5620</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-279834"><name><surname>Le Goc</surname><given-names>Guillaume</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6946-1142</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-291373"><name><surname>Migault</surname><given-names>Geoffrey</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-291374"><name><surname>Kunst</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-140155"><name><surname>Baier</surname><given-names>Herwig</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7268-0469</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-107255"><name><surname>Bormuth</surname><given-names>Volker</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-62308"><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9106-0356</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-291375"><name><surname>Debrégeas</surname><given-names>Georges</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3698-4497</contrib-id><email>georges.debregeas@sorbonne-universite.fr</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Computational Neuroscience Lab, Department of Neurophysiology, Donders Center for Neuroscience, Radboud University</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01c2cjg59</institution-id><institution>Sorbonne Université, CNRS, Institut de Biologie Paris-Seine (IBPS), Laboratoire Jean Perrin (LJP)</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Physiology, Anatomy and Genetics, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04mhzgx49</institution-id><institution>Blavatnik School of Computer Science, Tel Aviv University</institution></institution-wrap><addr-line><named-content content-type="city">Tel Aviv</named-content></addr-line><country>Israel</country></aff><aff id="aff5"><label>5</label><institution>Department Genes – Circuits – Behavior, Max Planck Institute for Biological Intelligence</institution><addr-line><named-content content-type="city">Martinsried</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dcv1019</institution-id><institution>Allen Institute for Brain Science</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>17</day><month>01</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e83139</elocation-id><history><date date-type="received" iso-8601-date="2022-09-21"><day>21</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-15"><day>15</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-11-11"><day>11</day><month>11</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.11.09.467900"/></event></pub-history><permissions><copyright-statement>© 2023, van der Plas, Tubiana et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>van der Plas, Tubiana et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83139-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83139-figures-v2.pdf"/><abstract><p>Patterns of endogenous activity in the brain reflect a stochastic exploration of the neuronal state space that is constrained by the underlying assembly organization of neurons. Yet, it remains to be shown that this interplay between neurons and their assembly dynamics indeed suffices to generate whole-brain data statistics. Here, we recorded the activity from ∼40,000 neurons simultaneously in zebrafish larvae, and show that a data-driven generative model of neuron-assembly interactions can accurately reproduce the mean activity and pairwise correlation statistics of their spontaneous activity. This model, the compositional Restricted Boltzmann Machine (cRBM), unveils ∼200 neural assemblies, which compose neurophysiological circuits and whose various combinations form successive brain states. We then performed in silico perturbation experiments to determine the interregional functional connectivity, which is conserved across individual animals and correlates well with structural connectivity. Our results showcase how cRBMs can capture the coarse-grained organization of the zebrafish brain. Notably, this generative model can readily be deployed to parse neural data obtained by other large-scale recording techniques.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>functional imaging</kwd><kwd>zebrafish</kwd><kwd>neural assembly</kwd><kwd>maximum entropy</kwd><kwd>restricted boltzmann machine</kwd><kwd>functional connectivity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/M011224/1</award-id><principal-award-recipient><name><surname>van der Plas</surname><given-names>Thijs L</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Edmond J. Safra Center for Bioinformatics at Tel Aviv University</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Tubiana</surname><given-names>Jérôme</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>LT001058/2019-C</award-id><principal-award-recipient><name><surname>Tubiana</surname><given-names>Jérôme</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>715980</award-id><principal-award-recipient><name><surname>Bormuth</surname><given-names>Volker</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0060/2017</award-id><principal-award-recipient><name><surname>Debrégeas</surname><given-names>Georges</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.VIDI.189.052</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><name><surname>van der Plas</surname><given-names>Thijs L</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A data-driven network model offers an interpretable and physiologically sound description of the whole-brain spontaneous neural activity of zebrafish larvae.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain is a highly connected network, organized across multiple scales, from local circuits involving just a few neurons to extended networks spanning multiple brain regions (<xref ref-type="bibr" rid="bib86">White et al., 1986</xref>; <xref ref-type="bibr" rid="bib67">Song et al., 2005</xref>; <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>). Concurrent with this spatial organization, brain activity exhibits correlated firing among large groups of neurons, often referred to as neural assemblies (<xref ref-type="bibr" rid="bib28">Harris, 2005</xref>). This assembly organization of brain dynamics has been observed in, for example, auditory cortex (<xref ref-type="bibr" rid="bib9">Bathellier et al., 2012</xref>), motor cortex (<xref ref-type="bibr" rid="bib54">Narayanan et al., 2005</xref>), prefrontal cortex (<xref ref-type="bibr" rid="bib69">Tavoni et al., 2017</xref>), hippocampus (<xref ref-type="bibr" rid="bib44">Lin et al., 2005</xref>), retina (<xref ref-type="bibr" rid="bib65">Shlens et al., 2009</xref>), and zebrafish optic tectum (<xref ref-type="bibr" rid="bib62">Romano et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Mölter et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Diana et al., 2019</xref>; <xref ref-type="bibr" rid="bib73">Triplett et al., 2020</xref>). These neural assemblies are thought to form elementary computational units and subserve essential cognitive functions such as short-term memory, sensorimotor computation or decision-making (<xref ref-type="bibr" rid="bib31">Hebb, 1949</xref>; <xref ref-type="bibr" rid="bib27">Gerstein et al., 1989</xref>; <xref ref-type="bibr" rid="bib28">Harris, 2005</xref>; <xref ref-type="bibr" rid="bib14">Buzsáki, 2010</xref>; <xref ref-type="bibr" rid="bib29">Harris, 2012</xref>; <xref ref-type="bibr" rid="bib55">Palm et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Eichenbaum, 2018</xref>). Despite the prevalence of these assemblies across the nervous system and their role in neural computation, it remains an open challenge to extract the assembly organization of a full brain and to show that the assembly activity state, derived from that of the neurons, is sufficient to account for the collective neural dynamics.</p><p>The need to address this challenge is catalyzed by technological advances in light-sheet microscopy, enabling the simultaneous recording of the majority of neurons in the zebrafish brain at single-cell resolution in vivo (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Ahrens et al., 2013</xref>; <xref ref-type="bibr" rid="bib87">Wolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Vanwalleghem et al., 2018</xref>). This neural recording technique opens up new avenues for constructing near-complete models of neural activity, and in particular its assembly organization. Recent attempts have been made to identify assemblies using either clustering (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Triplett et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Mölter et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Bartoszek et al., 2021</xref>), dimensionality reduction approaches (<xref ref-type="bibr" rid="bib45">Lopes-dos-Santos et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Romano et al., 2015</xref>; <xref ref-type="bibr" rid="bib53">Mu et al., 2019</xref>) or latent variable models (<xref ref-type="bibr" rid="bib19">Diana et al., 2019</xref>; <xref ref-type="bibr" rid="bib73">Triplett et al., 2020</xref>), albeit often limited to single brain regions. However, these methods do not explicitly assess to what extent the inferred assemblies could give rise to the observed neural data statistics, which is a crucial property of physiologically meaningful assemblies (<xref ref-type="bibr" rid="bib28">Harris, 2005</xref>). Here, we address this challenge by developing a generative model of neural activity that is explicitly constrained by the assembly organization, thereby quantifying if assemblies indeed suffice to produce the observed neural data statistics.</p><p>Specifically, we formalize neural assemblies using a bipartite network of two connected layers representing the neuronal and the assembly activity, respectively. Together with the maximum entropy principle (<xref ref-type="bibr" rid="bib37">Jaynes, 1957</xref>; <xref ref-type="bibr" rid="bib11">Bialek, 2012</xref>), this architecture defines the Restrictive Boltzmann Machine (RBM) model (<xref ref-type="bibr" rid="bib34">Hinton and Salakhutdinov, 2006</xref>). Here, we use an extension to the classical RBM definition termed compositional RBM (cRBM) that we have recently introduced (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>) and which brings multiple advances to assembly-based network modeling: (1) The maximum entropy principle ensures that neural assemblies are inferred solely from the data statistics. (2) The generative nature of the model, through alternate data sampling of the neuronal and assembly layers, can be leveraged to evaluate its capacity to replicate the empirical data statistics, such as the pairwise co-activation probabilities of all neuron pairs. (3) The cRBM steers the assembly organization to the so-called compositional phase where a small number of assemblies are active at any point in time, making the resulting model highly interpretable as we have shown previously for protein sequence analysis (<xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>).</p><p>Here, we have successfully trained cRBMs to brain-scale, neuron-level recordings of spontaneous activity in larval zebrafish containing 41,000 neurons on average (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). This represents an increase of ∼2 orders of magnitude in number of neurons with respect to previously reported RBM implementations (<xref ref-type="bibr" rid="bib41">Köster et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Gardella et al et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">Volpi et al., 2020</xref>), attained through significant algorithmic and computational enhancements. We found that all cells could be grouped into 100–200 partially overlapping assemblies, which are anatomically localized and together span the entire brain, and accurately replicate the first and second order statistics of the neural activity. These assemblies were found to carry more predictive power than a fully connected model which has orders of magnitude more parameters, validating that assemblies underpin collective neural dynamics. Further, the probabilistic nature of our model allowed us to compute a functional connectivity matrix by quantifying the effect of activity perturbations in silico. This assembly-based functional connectivity is well-conserved across individual fish and consistent with anatomical connectivity at the mesoscale (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>).</p><p>In summary, we present an assembly decomposition spanning the zebrafish brain, which accurately accounts for its activity statistics. Our cRBM model provides a widely applicable tool to the community to construct low-dimensional data representations that are defined by the statistics of the data, in particular for very high-dimensional systems. Its generative capability further allows to produce new (synthetic) activity patterns that are amenable to direct in silico perturbation and ablation studies.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Compositional RBMs construct Hidden Units by grouping neurons into assemblies</title><p>Spontaneous neural activity was recorded from eight zebrafish larvae aged 5–7 days post fertilization expressing the GCaMP6s or GCaMP6f calcium reporters using light-sheet microscopy (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). Each data set contained the activity of a large fraction of the neurons in the brain (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mn>40709</mml:mn><mml:mo>±</mml:mo><mml:mn>13854</mml:mn></mml:mrow></mml:math></inline-formula>; mean ± standard deviation), which, after cell segmentation, were registered onto the ZBrain atlas (<xref ref-type="bibr" rid="bib60">Randlett et al., 2015</xref>) and mapzebrain atlas (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>). Individual neuronal fluorescence traces were deconvolved to binarized spike trains using blind sparse deconvolution (<xref ref-type="bibr" rid="bib78">Tubiana et al., 2020</xref>). This data acquisition process is depicted in <xref ref-type="fig" rid="fig1">Figure 1A</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>cRBMs construct Hidden Units by grouping neurons into assemblies.</title><p>(<bold>A</bold>) The neural activity of zebrafish larvae was imaged using light-sheet microscopy (left), which resulted in brain-scale, single-cell resolution data sets (middle, microscopy image of a single plane shown for fish #1). Calcium activity <inline-formula><mml:math id="inf2"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> was deconvolved to binarized spike traces for each segmented cell (right, example neuron). (<bold>B</bold>) cRBM sparsely connects neurons (left) to Hidden Units (HUs, right). The neurons that connect to a given HU (and thus belong to the associated assembly), are depicted by the corresponding color labeling (right panel). Data sets typically consist of <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> neurons and <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> HUs. The activity of 500 randomly chosen example neurons (raster plot, left) and HUs 99, 26, 115 (activity traces, right) of the same time excerpt is shown. HU activity is continuous and is determined by transforming the neural activity of its assembly. (<bold>C</bold>) The neural assemblies of an example data set (fish #3) are shown by coloring each neuron according to its strongest-connecting HU. 7 assemblies are highlighted (starting rostrally at the green forebrain assembly, going clockwise: HU 177, 187, 7, 156, 124, 64, 178), by showing their neurons with a connection <inline-formula><mml:math id="inf5"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>. See <xref ref-type="fig" rid="fig3">Figure 3</xref> for more anatomical details of assemblies. R: Right, L: Left, Ro: Rostral, C: Caudal.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig1-v2.tif"/></fig><p>We trained compositional Restricted Boltzmann Machine (cRBM) models to capture the activity statistics of these neural recordings. cRBMs are maximum entropy models, that is, the maximally unconstrained solution that fits model-specific data statistics (<xref ref-type="bibr" rid="bib34">Hinton and Salakhutdinov, 2006</xref>; <xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib25">Gardella et al., 2019</xref>), and critically extend the classical RBM formulation. Its architecture consists of a bipartite graph where the high-dimensional layer of neurons <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (named ‘visible units’ in RBM terminology) is connected to the low-dimensional layer of latent components, termed Hidden Units (HUs) <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Their interaction is characterized by a weight matrix <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that is regularized to be sparse. The collection of neurons that have non-zero interactions with a particular HU, noted <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> (i.e. with <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), define its corresponding neural assembly μ (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). This weight matrix, together with the neuron weight vector <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">g</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and HU potential <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, defines the transformation from the binarized neural activity <inline-formula><mml:math id="inf13"><mml:mrow><mml:mtext>v</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the continuous HU activity <inline-formula><mml:math id="inf14"><mml:mrow><mml:mtext>h</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows all recorded neurons of a zebrafish brain, color-labeled according to their strongest-connecting HU, illustrating that cRBM-inferred assemblies (hereafter, neural assemblies for conciseness) are typically densely localized in space and together span the entire brain.</p><p>Beyond its architecture (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), the model is defined by the probability function <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of any data configuration <inline-formula><mml:math id="inf16"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (see Materials and methods ‘Restricted Boltzmann Machines’ and ‘Compositional Restricted Boltzmann Machine’ for details):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf17"><mml:mi>Z</mml:mi></mml:math></inline-formula> is the partition function that normalizes <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> and <inline-formula><mml:math id="inf18"><mml:mi>E</mml:mi></mml:math></inline-formula> is the following energy function:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>HU activity h is obtained by sampling from the conditional probability function <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>h</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Conversely, neural activity is obtained from HU activity through:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ3 equ4">Equations 3 and 4</xref> mathematically reflect the dual relationship between neural and assembly states: the Hidden Units <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> drive ‘visible’ neural activity <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, expressed as <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>v</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, while the stochastic assembly activity <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> itself is defined as a function of the activity of the neurons: <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>h</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Importantly, the model does not include direct connections between neurons, hence neural correlations <inline-formula><mml:math id="inf25"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> can arise solely from shared assemblies. Moreover, this bipartite architecture ensures that the conditional distributions factorize, leading to a sampling procedure where all neurons or all HUs can be sampled in parallel. The cRBM leverages this property to efficiently generate new data by Monte Carlo sampling alternately from <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>h</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>v</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>cRBM is optimized to accurately replicate data statistics.</title><p>(<bold>A</bold>) Schematic of the cRBM architecture, with neurons <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> on the left, HUs <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> on the right, connected by weights <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. (<bold>B</bold>) Schematic depicting how cRBMs generate new data. The HU activity <inline-formula><mml:math id="inf31"><mml:mrow><mml:mtext>h</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is sampled from the visible unit (i.e. neuron) configuration <inline-formula><mml:math id="inf32"><mml:mrow><mml:mtext>v</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, after which the new visible unit configuration <inline-formula><mml:math id="inf33"><mml:mrow><mml:mtext>v</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is sampled and so forth. (<bold>C</bold>) cRBM-predicted and experimental mean neural activity <inline-formula><mml:math id="inf34"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> were highly correlated (Pearson correlation <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.91</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>307</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and had low error (<inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.11</mml:mn></mml:mrow></mml:math></inline-formula>, normalized Root Mean Square Error, see Materials and methods - ‘Calculating the normalized Root Mean Square Error’ ). Data displayed as 2D probability density function (PDF), scaled logarithmically (base 10). (<bold>D</bold>) cRBM-predicted and experimental mean Hidden Unit (HU) activity <inline-formula><mml:math id="inf38"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> also correlated very strongly (<inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>86</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and had low <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula> (other details as in C) (<bold>E</bold>) cRBM-predicted and experimental average pairwise neuron-HU interactions <inline-formula><mml:math id="inf42"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> correlated strongly (<inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.74</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>307</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and had a low error (<inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>F</bold>) cRBM-predicted and experimental average pairwise neuron-neuron interactions <inline-formula><mml:math id="inf46"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> correlated well (<inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.58</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>307</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and had a low error (<inline-formula><mml:math id="inf49"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, where the negative nRMSE value means that cRBM-predictions match the test data slightly better than the train data). Pairwise interactions were corrected for naive correlations due to their mean activity by subtracting <inline-formula><mml:math id="inf50"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>G</bold>) cRBM-predicted and experimental average pairwise HU-HU interactions <inline-formula><mml:math id="inf51"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> correlated strongly (<inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.73</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>307</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and had a low error (<inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>H</bold>) The low-dimensional cRBM bottleneck reconstructs most neurons above chance level (purple), quantified by the normalized log-likelihood (nLLH) between neural test data <italic>v</italic><sub><italic>i</italic></sub> and the reconstruction after being transformed to HU activity (see Materials and methods - ‘Reconstruction quality’). Median normalized = <inline-formula><mml:math id="inf55"><mml:msub><mml:mtext>nLLH</mml:mtext><mml:mtext>cRBM</mml:mtext></mml:msub></mml:math></inline-formula> 0.24. Reconstruction quality was also determined for a fully connected Generalized Linear Model (GLM) that attempted to reconstruct the activity of a neuron <italic>v</italic><sub><italic>i</italic></sub> using all other neurons <inline-formula><mml:math id="inf56"><mml:msub><mml:mtext>v</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see Materials and methods - ‘Generalized Linear Model’). The distribution of 5000 randomly chosen neurons is shown (blue), with median <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mtext>nLLH</mml:mtext><mml:mtext>GLM</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.20</mml:mn></mml:mrow></mml:math></inline-formula>. The cRBM distribution is stochastically greater than the GLM distribution (one-sided Mann Whitney U test, <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>42</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). (<bold>I</bold>) cRBM (purple) had a sparse weight distribution, but exhibited a greater proportion of large weights <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> than PCA (yellow), both for positive and negative weights, displayed in log-probability. (<bold>J</bold>) Distribution of above-threshold absolute weights <inline-formula><mml:math id="inf60"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> per neuron <italic>v</italic><sub><italic>i</italic></sub> (dark purple), indicating that more neurons strongly connect to the cRBM hidden layer than expected by shuffling the weight matrix of the same cRBM (light purple). The threshold <inline-formula><mml:math id="inf61"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math></inline-formula> was set such that the expected number of above-threshold weights per neuron <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">#</mml:mi><mml:msub><mml:mrow><mml:mtext mathvariant="bold">w</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>K</bold>) Corresponding distribution as in (<bold>J</bold>) for PCA (dark yellow) and its shuffled weight matrix (light yellow), indicating a predominance of small weights in PCA for most neurons <italic>v</italic><sub><italic>i</italic></sub>. All panels of this figure show the data statistics of the cRBM with parameters <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula> (best choice after cross-validation, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) of example fish #3, comparing the experimental test data test and model-generated data after cRBM training converged.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>cRBM free parameter optimization by cross-validation.</title><p>(<bold>A</bold>) Model performance, as quantified by the normalized RMSE (nRMSE, see Materials and methods - ‘Calculating the normalized root mean square error’) of the mean neural activity <inline-formula><mml:math id="inf65"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, as a function of the sparsity regularization parameter <inline-formula><mml:math id="inf66"><mml:mi>λ</mml:mi></mml:math></inline-formula> (x-axis) and the number of HUs <inline-formula><mml:math id="inf67"><mml:mi>M</mml:mi></mml:math></inline-formula> (y-axis). cRBM models were evaluated after training converged on withheld testing data. <inline-formula><mml:math id="inf68"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>nRMSE</mml:mtext></mml:mrow></mml:math></inline-formula> values are shown to enable optimization by maximization in panel G. Values below 0 were set to 0 for panels A-F. (<bold>B</bold>) Equivalent figure for the mean HU activity <inline-formula><mml:math id="inf69"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Equivalent figure for average pairwise neuron-HU interactions <inline-formula><mml:math id="inf70"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>D</bold>) Equivalent figure for average pairwise neuron-neuron interactions <inline-formula><mml:math id="inf71"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>E</bold>) Equivalent figure for average pairwise HU-HU interactions <inline-formula><mml:math id="inf72"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>F</bold>) Equivalent figure for the reconstruction quality of the low-dimensional cRBM bottleneck, quantified by the median normalized LLH. (<bold>G</bold>) The cRBM free parameters were optimized by maximizing the element-wise minimum of panels A-F. Using the element-wise minimum to compare the six statistics ensures that the model performs performs well on all aspects. First, in order to compare panels A-E with panel F, the values of panel F were scaled to 1 by dividing all elements of panel F by its maximum value (=0.29). Next, for each <inline-formula><mml:math id="inf73"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> combination the minimum value was determined from all 6 evaluation criteria. These are shown in this panel, with the black arrow indicating the resulting maximum of 0.83 at <inline-formula><mml:math id="inf74"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Generalized Linear Model (GLM) parameter optimization.</title><p>(<bold>A</bold>) Schematic of the pairwise GLM model. Connections were estimated with logistic regression and <italic>L</italic><sub>2</sub> sparsity regularization. (<bold>B</bold>) The sparsity regularization parameter <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>λ</mml:mi><mml:mtext>GLM</mml:mtext></mml:msub></mml:math></inline-formula> was optimized using cross-validation of 6 parameter configurations, using 1000 randomly sampled neurons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>cRBMs are in the compositional phase after convergence.</title><p>(<bold>A</bold>) Distribution of <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the number of effective active HUs per time point <inline-formula><mml:math id="inf77"><mml:mi>t</mml:mi></mml:math></inline-formula>, calculated on the test data of example fish #3. <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as <inline-formula><mml:math id="inf79"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>PR</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>h</mml:mtext><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where PR is the Participation Ratio (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>). Median <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.27</mml:mn><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>54</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) The distribution of median <inline-formula><mml:math id="inf81"><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:mfrac></mml:math></inline-formula> values of the entire recordings of all cRBMs used for the connectivity analyses. The average <inline-formula><mml:math id="inf82"><mml:mfrac><mml:mrow><mml:mi>median</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:mfrac></mml:math></inline-formula> across all cRBMs is 0.26. The compositional phase is characterized by <inline-formula><mml:math id="inf83"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≪</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> which occurs for all cRBMs. The three phases are indicated. Here, the ferromagnetic phase upper bound was manually set at 5%, and the spin-glass phase lower bound was determined by computing the PR of normally distributed activity (with mean and standard deviation of the test data of example fish #3 of panel A).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Neurons can be embedded in multiple assemblies.</title><p>cRBM allow neurons to be embedded in multiple assemblies, which we quantified by measuring the number of above-threshold weights <inline-formula><mml:math id="inf84"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mtext>threshold</mml:mtext></mml:mrow></mml:math></inline-formula> per neuron.Threshold values of 0.001 (left), 0.01 (middle), and 0.1 (right) were used. For each threshold value, we counted the fraction of neurons that had at least 0, 1, 3, 5, 7, or 10 above-threshold weights (i.e. were embedded in at least that many assemblies), as denoted on the x-axis of each panel. Changing the threshold from 0.001 to 0.01 did not cause a large shift in the distribution of embedded assemblies (left vs middle), indicating that there were few connections in the range <inline-formula><mml:math id="inf85"><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>. Increasing the threshold to 0.1 strongly decreased the number of neurons with many (<italic>gt</italic><sub>3</sub>) assembly embeddings, but not the number of neurons with at least 1 assembly embedding, indicating that most neurons have at least 1 strong assembly connection (in accordance with <xref ref-type="fig" rid="fig2">Figure 2J</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Influence of number of HUs <inline-formula><mml:math id="inf86"><mml:mi>M</mml:mi></mml:math></inline-formula> and sparsity regularization <inline-formula><mml:math id="inf87"><mml:mi>λ</mml:mi></mml:math></inline-formula> on cRBM properties.</title><p>(<bold>A–F</bold>) The distribution of assembly sizes is shown as a function of number of HUs <inline-formula><mml:math id="inf88"><mml:mi>M</mml:mi></mml:math></inline-formula> (x-axis) and sparsity regularization parameter <inline-formula><mml:math id="inf89"><mml:mi>λ</mml:mi></mml:math></inline-formula> (panels). Assembly size was determined by computing <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for each HU μ. Two-way ANOVA determined that both <inline-formula><mml:math id="inf91"><mml:mi>M</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) and <inline-formula><mml:math id="inf93"><mml:mi>λ</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), as well as their interaction (<inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) significantly explained the mean assembly size. (<bold>G–L</bold>) The distribution of HU dynamics time scales is shown as a function of <inline-formula><mml:math id="inf96"><mml:mi>M</mml:mi></mml:math></inline-formula> (x-axis) and <inline-formula><mml:math id="inf97"><mml:mi>λ</mml:mi></mml:math></inline-formula> (panels). Time scales were calculated as in <xref ref-type="fig" rid="fig2">Figure 2F</xref>. There are 2 extra out-of-panel outliers for <inline-formula><mml:math id="inf98"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>150</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> at 70 s and <inline-formula><mml:math id="inf99"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> at 36 s. Two-way ANOVA determined that only <inline-formula><mml:math id="inf100"><mml:mi>M</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) strongly significantly explained the mean time scale, while <inline-formula><mml:math id="inf102"><mml:mi>λ</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>) did not and their interaction (<inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>) only weakly. M-O Three example assemblies are shown, corresponding to the mean assembly size of <inline-formula><mml:math id="inf105"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> (panel M), <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (<bold>N</bold>) and <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> (<bold>O</bold>) cRBM models (all with <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula>). Assembly sizes are 0.27 (<bold>M</bold>), 0.07 (<bold>N</bold>), 0.01 (<bold>O</bold>), respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>cRBM assemblies are sparse and spatially localized.</title><p>(<bold>A</bold>) For each neural assembly (y-axis), the occupancy of ZBrain Atlas (<xref ref-type="bibr" rid="bib60">Randlett et al., 2015</xref>) brain regions (x-axis, sorted alphabetically) was determined, by quantifying the number of anatomical regions that each HU connects to (i.e. the overlap between cRBM assemblies and anatomical regions, see Materials and methods - ‘Regional occupancy’). ZBrain atlas regions that were not imaged were excluded, yielding <inline-formula><mml:math id="inf109"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>ZBA</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>206</mml:mn></mml:mrow></mml:math></inline-formula> regions in total. Occupancy was defined as the dot product between the binary ZBrain-label matrix (size <inline-formula><mml:math id="inf110"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>ZBA</mml:mtext></mml:msub><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, where 1 indicates that a neuron is embedded in an anatomical region, and 0 vice versa) and the cRBM weight matrix (size <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>), and was normalized to 100% for each assembly to account for different assembly sizes. Assemblies were sorted by their dynamics (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, Materials and methods). ZBrain Atlas regions are abbreviated by: Diencephalon (Di), Ganglia (Ga), Mesencephalon (Me), Rhombencephalon (Rh), Spinal Cord (SC) and Telencephalon (Te). (<bold>B</bold>) Equivalent figure for Principal Axes of PCA (i.e. the PCA Eigenvectors). Principal Axes were sorted by Eigenvalue. Panels A and B share the same color scale (right). (<bold>C</bold>) Distributions of the effective number of ZBrain Atlas regions per assembly (Principal Axis) of the occupancy metric of panel A (<bold>B</bold>) for cRBM (PCA), respectively. The effective number of regions was determined by calculating the Participation Ratio per assembly (Principal Axis), multiplied with the total number of regions <inline-formula><mml:math id="inf112"><mml:msub><mml:mi>N</mml:mi><mml:mtext>ZBA</mml:mtext></mml:msub></mml:math></inline-formula> (see Materials and methods - ‘Regional occupancy’). cRBM assemblies occupy a median of 3 regions (interquartile range: 2–6 regions), while PCA assemblies occupy a median of 160 regions (interquartile range: 140–168 regions).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 7.</label><caption><title>Variational Autoencoder (VAE) models do not fit the second-order statistics of the neural data.</title><p>(<bold>A</bold>) VAE model performance, quantified by its ELBO objective on withheld test data. The optimal VAE was found with parameters <inline-formula><mml:math id="inf113"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and this optimum was very close to the performance of <inline-formula><mml:math id="inf114"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (the cRBM optimal parameters, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For ease of comparison, in the other panels we consider the latter VAE model, but also quantify the performance of the first in panels B and C. (<bold>B</bold>) VAE-predicted and experimental mean neural activity <inline-formula><mml:math id="inf115"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> were strongly correlated Pearson correlation (<inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.84</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>307</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) but still had fairly high error (<inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.61</mml:mn></mml:mrow></mml:math></inline-formula>), presumably because the model appeared to underestimate the activity of most neurons (linear regression determined the optimal slope to be 0.52). The <inline-formula><mml:math id="inf119"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (optimal) VAE model had a similar <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.54</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) VAE-predicted and experimental average pairwise neuron-neuron interactions <inline-formula><mml:math id="inf121"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> had a very high error of <inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.23</mml:mn></mml:mrow></mml:math></inline-formula>, demonstrating that the VAE did not capture the second-order statistics of the data at all. The <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (optimal) VAE model had a similar <inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.22</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>D</bold>) Reconstruction quality of VAE was also very low, performing almost equal to an independent model. (<bold>E</bold>) VAE weights were similarly sparsely distributed to cRBM weights. (VAE weights were also sign-swapped for consistency with cRBM, see Materials and methods - ‘Choice of HU potential’) (<bold>F</bold>) The distribution of above-threshold absolute weights was also similar to cRBM (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). (<bold>G</bold>) Fraction of disconnected HUs of cRBM and VAE (defined for each HU μ by <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.01</mml:mn><mml:mspace width="thickmathspace"/><mml:mi mathvariant="normal">∀</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>H</bold>) Distribution of assembly sizes (determined by <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), excluding all disconnected HUs (1/200 for cRBM, 122/200 for VAE).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig2-figsupp7-v2.tif"/></fig></fig-group><p>The cRBM differs from the classical RBM formulation (<xref ref-type="bibr" rid="bib34">Hinton and Salakhutdinov, 2006</xref>) through the introduction of double Rectified Linear Unit (dReLU) potentials <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, weight sparsity regularization and normalized HU activity (further detailed in Methods). We have previously demonstrated in theory and application (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>) that this new formulation steers the model into the so-called compositional phase, which makes the latent representation highly interpretable. This phase occurs when a limited number <inline-formula><mml:math id="inf128"><mml:mi>m</mml:mi></mml:math></inline-formula> of HUs co-activate such that <inline-formula><mml:math id="inf129"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≪</mml:mo><mml:mi>m</mml:mi><mml:mo>≪</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf130"><mml:mi>M</mml:mi></mml:math></inline-formula> is the total number of HUs. Thus, each visible configuration is mapped to a specific combination of activated HUs. This contrasts with the ferromagnetic phase (<inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) where each HU encodes one specific activity pattern, thus severely limiting the possible number of encoded patterns, or the spin-glass phase (<inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∼</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) where all HUs activate simultaneously, yielding a very complex assembly patchwork (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>). Therefore, the compositional phase can provide the right level of granularity for a meaningful interpretation of the cRBM neural assemblies by decomposing the overall activity as a time-dependent co-activation of different assemblies of interpretable size and extent.</p></sec><sec id="s2-2"><title>Trained cRBMs accurately replicate data statistics</title><p>cRBM models are trained to maximize the <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> log-likelihood of the zebrafish data recordings, which is achieved by matching the model-generated statistics <inline-formula><mml:math id="inf134"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf135"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf136"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> (the mean neuronal activity, mean HU activity and their correlations, respectively) to the empirical data statistics (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>). In order to optimize the two free parameters of the cRBM model – the sparsity regularization parameter <inline-formula><mml:math id="inf137"><mml:mi>λ</mml:mi></mml:math></inline-formula> and the total number of HUs <inline-formula><mml:math id="inf138"><mml:mi>M</mml:mi></mml:math></inline-formula> – we assessed the cRBM performance for a grid of <inline-formula><mml:math id="inf139"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-values for one data set (fish #3). This analysis yielded an optimum for <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). These values were subsequently used for all recordings, where <inline-formula><mml:math id="inf142"><mml:mi>M</mml:mi></mml:math></inline-formula> was scaled with the number of neurons <inline-formula><mml:math id="inf143"><mml:mi>N</mml:mi></mml:math></inline-formula>.</p><p>We trained cRBMs on 70% of the recording length, and compared the statistics of model-generated data to the withheld test data set (the remaining 30% of recording, see Materials and methods ‘Train / test data split’ and ‘Assessment of data statistics’ for details). After convergence, the cRBM generated data that replicated the training statistics accurately, with normalized Root Mean Square Error (nRMSE) values of <inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.11</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2C-E</xref>). Here, nRMSE is normalized such that 1 corresponds to shuffled data statistics and 0 corresponds to the best possible RMSE, i.e., between train and test data.</p><p>We further evaluated cRBM performance to assess its ability to capture data statistics that the cRBM was not explicitly trained to replicate: the pairwise correlations between neurons <inline-formula><mml:math id="inf147"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and the pairwise correlations between HUs <inline-formula><mml:math id="inf148"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. We found that these statistics were also accurately replicated by model-generated data, with <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (meaning that the model slightly outperformed the train-test data difference) and <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2F, G</xref>). The fact that cRBM also accurately replicated neural correlations <inline-formula><mml:math id="inf151"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2F</xref>) is of particular relevance, since this indicates that (1) the assumption that neural correlations can be explained by their shared assemblies is justified and (2) cRBMs may provide an efficient mean to model neural interactions of such large systems (<inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) where directly modeling all <inline-formula><mml:math id="inf153"><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> interactions would be computationally infeasible or not sufficiently constrained by the available data.</p><p>Next, we assessed the reconstruction quality after neural data was compressed by the cRBM low-dimensional bottleneck. The reconstruction quality is defined as the log-likelihood of reconstructed neural data <inline-formula><mml:math id="inf154"><mml:msub><mml:mtext>v</mml:mtext><mml:mtext>recon</mml:mtext></mml:msub></mml:math></inline-formula> (i.e. <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that is first transformed to the low-dimensional <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and then back again to the high-dimensional <inline-formula><mml:math id="inf157"><mml:msub><mml:mtext>v</mml:mtext><mml:mtext>recon</mml:mtext></mml:msub></mml:math></inline-formula>, see Materials and methods - ‘Reconstruction quality’). This is important to prevent trivial, undesired solutions like <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+2.8pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="5.3pt">∀</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> which would directly lead to <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (potentially because of strong sparsity regularization). <xref ref-type="fig" rid="fig2">Figure 2H</xref> shows the distribution of cRBM reconstruction quality of all neurons (in purple), quantified by the normalized log-likelihood (nLLH) such that 0 corresponds to an independent model <inline-formula><mml:math id="inf160"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and 1 corresponds to perfect reconstruction (non-normalized <inline-formula><mml:math id="inf161"><mml:mrow><mml:mtext>LLH</mml:mtext><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). For comparison, we also reconstructed the neural activity using a fully connected Generalized Linear Model (GLM, see Materials and methods - ‘Generalized Linear Model’ and <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2H</xref>, blue). The cRBM nLLH distribution is significantly greater than the GLM nLLH distribution (one-sided Mann Whitney U test, <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>42</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), with medians <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mtext>LLH</mml:mtext><mml:mtext>cRBM</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.24</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mtext>LLH</mml:mtext><mml:mtext>GLM</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.20</mml:mn></mml:mrow></mml:math></inline-formula>. Hence, projecting the neural data onto the low-dimensional representation of the HUs does not compromise the ability to explain the neural activity. In fact, reconstruction quality of the cRBM slightly outperforms the GLM, possibly due to the suppression of noise in the cRBM estimate. The optimal <inline-formula><mml:math id="inf165"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> choice of free parameters was selected by cross-validating the median of the cRBM reconstruction quality, together with the normalized RMSE of the five previously described statistics (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>Lastly, we confirmed that the cRBM indeed resides in the compositional phase, characterized by <inline-formula><mml:math id="inf166"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≪</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of HUs active at time point <inline-formula><mml:math id="inf168"><mml:mi>t</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>). This property is a consequence of the sparse weight matrix <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, indicated by its heavy-tail log-distribution (<xref ref-type="fig" rid="fig2">Figure 2I</xref>, purple). The compositional phase is the norm for the presently estimated cRBMs, evidenced by the distribution of median <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values for all recordings (average <inline-formula><mml:math id="inf171"><mml:mfrac><mml:mrow><mml:mi>median</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:mfrac></mml:math></inline-formula> is 0.26, see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3B</xref>). Importantly, the sparse weight matrix does not automatically imply that only a small subset of neurons is connected to the cRBM hidden layer. We validated this by observing that more neurons strongly connect to the hidden layer than expected by shuffling the weight matrix (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). Further, we quantified the number of assemblies that each neuron was embedded in, which showed that increasing the embedding threshold did not notably affect the fraction of neurons embedded in at least 1 assembly (93–94%, see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). To assess the influence of <inline-formula><mml:math id="inf172"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf173"><mml:mi>λ</mml:mi></mml:math></inline-formula> on the inferred assemblies, we computed, for all cRBM models trained during the optimization of <inline-formula><mml:math id="inf174"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:mi>λ</mml:mi></mml:math></inline-formula>, the distribution of assembly sizes (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5A-F</xref>). We found that <inline-formula><mml:math id="inf176"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf177"><mml:mi>λ</mml:mi></mml:math></inline-formula> controlled the distribution of assembly sizes in a consistent manner: assembly size was a gradually decreasing function of both <inline-formula><mml:math id="inf178"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf179"><mml:mi>λ</mml:mi></mml:math></inline-formula> (two-way ANOVA, both <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). Furthermore, for <inline-formula><mml:math id="inf181"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mi>λ</mml:mi></mml:math></inline-formula> values close to the optimal parameter-setting (<inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula>), the changes in assembly size were very small and gradual. This showcases the robustness of the cRBM to slight changes in parameter choice.</p><p>Sparsity facilitated that each assembly only connects to a handful of anatomical regions, as we quantified by calculating the overlap between cRBM assemblies and anatomical regions (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). We found that cRBM assemblies connect to a median of three regions (interquartile range: 2–6 regions). Importantly, the cRBM has no information about the locations of neurons during training, so the localization to a limited set of anatomical areas that we observe is extracted from the neural co-activation properties alone. For comparison, Principal Component Analysis (PCA), a commonly used non-sparse dimensionality reduction method that shares the cRBM architecture, naturally converged to a non-sparse weight matrix (<xref ref-type="fig" rid="fig2">Figure 2I</xref>, yellow), with fewer connected neurons than expected by shuffling its weight matrix (<xref ref-type="fig" rid="fig2">Figure 2K</xref>). This led to unspecific assemblies that are difficult to interpret by anatomy (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). As a result, sparsity, a cRBM property shared with some other dimensionality reduction techniques, is crucial to interpret the assemblies by anatomy as we demonstrate in the next section.</p><p>We next asked whether sparsity alone was sufficient for a generative model to accurately recapitulate the neural recording statistics. To address this question, we trained sparse linear Variational Autoencoders (VAEs) using the same parameter-optimization protocol (<xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7A</xref>). Like cRBMs, linear VAEs are generative models that learn a latent representation of a dataset (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). We observed that VAEs were not able to replicate the second-order statistics, and therefore were not able to reconstruct neural activity from latent representation (<xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7B-D</xref>), even though they also obtained sparse representations (<xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7E, F</xref>). Other clustering or dimensionality reduction methods, such as k-means, PCA and non-negative matrix factorization, have been used previously to cluster neurons in the zebrafish brain (<xref ref-type="bibr" rid="bib15">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Mu et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Marques et al., 2020</xref>). However, because these methods cannot generate artificial neural data using their inferred assemblies, their quality cannot be quantitatively assessed as we have done for the cRBM (but see <xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref> for other comparisons).</p></sec><sec id="s2-3"><title>cRBM assemblies compose functional circuits and anatomical structures</title><p>Above, we have shown that cRBMs converge to sparse weight matrix solutions. This property enables us to visualize the cRBM-inferred neural assemblies as the collection of significantly connected neurons to an HU. Neurons from a given neural assembly display concerted dynamics, and so one may expect their spatial organization to reflect the neuroanatomy and functional organization of the brain. We here highlight a selection of salient examples of neural assemblies, illustrating that assemblies match well with anatomical structures and functional circuits, while the complete set of neural assemblies is presented in <xref ref-type="video" rid="video1">Video 1</xref>. In particular, we identified assemblies that together compose a neural circuit, are neurotransmitter-specific, encompass a long-range pathway, or can be identified by anatomy. The examples shown here are from a single fish (#3), but results from other fish were comparable.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-83139-video1.mp4" id="video1"><label>Video 1.</label><caption><title>All neural assemblies of one example fish All 200 inferred assemblies of the example fish #2 of <xref ref-type="fig" rid="fig3">Figure 3</xref> are shown in sequence.</title><p>Top: neural assembly. Bottom: HU activity of test data.</p></caption></media><p>First, we identified six assemblies that together span the hindbrain circuit that drives eye and tail movements (<xref ref-type="bibr" rid="bib20">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2018</xref>). We find two neural assemblies in rhombomere 2 which align with the anterior rhombencephalic turning region (ARTR, <xref ref-type="bibr" rid="bib3">Ahrens et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>, <xref ref-type="fig" rid="fig3">Figure 3A, B</xref>). Each assembly primarily comprises neurons of either the left or right side of the ARTR, but also includes a small subset of contralateral neurons with weights of opposite sign in line with the established mutual inhibition between both subpopulations. Two other symmetric assemblies (<xref ref-type="fig" rid="fig3">Figure 3C, D</xref>) together encompass the oculomotor nucleus (nIII) and the contralateral abducens nucleus (nVI, in rhombomere 6), two regions engaged in ocular saccades (<xref ref-type="bibr" rid="bib46">Ma et al., 2014</xref>) and under the control of the ARTR (<xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>). Additionally, we observed two symmetric assemblies (<xref ref-type="fig" rid="fig3">Figure 3E, F</xref>) in the posterior hindbrain (in rhombomere 7), in a region known to drive unilateral tail movements (<xref ref-type="bibr" rid="bib15">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Marques et al., 2020</xref>) and whose antiphasic activation is also controlled by the ARTR activity (<xref ref-type="bibr" rid="bib20">Dunn et al., 2016</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>cRBM assemblies compose functional circuits and anatomical structures.</title><p>(<bold>A–I</bold>) Individual example assemblies μ are shown by coloring each neuron <inline-formula><mml:math id="inf185"><mml:mi>i</mml:mi></mml:math></inline-formula> with its connectivity weight value <inline-formula><mml:math id="inf186"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see color bar at the right hand side). The assembly index μ is stated at the bottom of each panel. The orientation and scale are given in panel A (Ro: rostral, C: caudal, R: right, L: left, D: dorsal, V: ventral). Anatomical regions of interest, defined by the ZBrain Atlas (<xref ref-type="bibr" rid="bib60">Randlett et al., 2015</xref>), are shown in each panel (Rh: rhombomere, nMLF: nucleus of the medial longitudinal fascicle; nIII: oculomotor nucleus nIII, Cl: cluster; Str: stripe, P. nV TG: Posterior cluster of nV trigeminal motorneurons; Pa: pallium; Hb: habenula; IPN: interpeduncular nucleus). (<bold>J–L</bold>) Groups of example assemblies that lie in the same anatomical region are shown for cerebellum (Cb), torus semicircularis (TSC), and optic tectum (OT). Neurons i were defined to be in an assembly μ when <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and colored accordingly. If neurons were in multiple assemblies shown, they were colored according to their strongest-connecting assembly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig3-v2.tif"/></fig><p>Next, we observed assemblies that correspond to particular neurotransmitter expressions in the ZBrain atlas (<xref ref-type="bibr" rid="bib60">Randlett et al., 2015</xref>), such as the excitatory Vglut2 (<xref ref-type="fig" rid="fig3">Figure 3G</xref>) and inhibitory Gad1b (<xref ref-type="fig" rid="fig3">Figure 3H</xref>) neurotransmitters. These assemblies consist of multiple dense loci that sparsely populate the entire brain, confirming that cRBMs are able to capture a large morphological diversity of neural assemblies. <xref ref-type="fig" rid="fig3">Figure 3I</xref> depicts another sparse, brain-wide assembly that encompasses the pallium, habenula (Hb) and interpeduncular nucleus (IPN), and thus captures the Hb-IPN pathway that connects to other regions such as the pallium (<xref ref-type="bibr" rid="bib10">Beretta et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Bartoszek et al., 2021</xref>).</p><p>Larger nuclei or circuits were often composed of a small number of distinct neural assemblies with some overlap. For example, the cerebellum was decomposed into multiple, bilateral assemblies (<xref ref-type="fig" rid="fig3">Figure 3J</xref>) whereas neurons in the torus semicircularis were grouped per brain hemisphere (<xref ref-type="fig" rid="fig3">Figure 3K</xref>). As a last example, the optic tectum was composed of a larger set of approximately 18 neural assemblies, which spatially tiled the volume of the optic tectum (<xref ref-type="fig" rid="fig3">Figure 3L</xref>). This particular organization is suggestive of spatially localized interactions within the optic tectum, and aligns with the morphology of previously inferred assemblies in this specific region (<xref ref-type="bibr" rid="bib62">Romano et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Diana et al., 2019</xref>; <xref ref-type="bibr" rid="bib73">Triplett et al., 2020</xref>). However, <xref ref-type="fig" rid="fig3">Figure 3</xref> altogether demonstrates that the typical assembly morphology of the optic tectum identified by our and these previous analyses does not readily generalize to other brain regions, where a large range of different assembly morphologies compose neural circuits.</p><p>Overall, the clear alignment of cRBM-based neural assemblies with anatomical regions and circuits suggests that cRBMs are able to identify anatomical structures from dynamical activity alone, which enables them to break down the overall activity into parts that are interpretable by physiologists in the context of previous, more local studies.</p></sec><sec id="s2-4"><title>HU dynamics cluster into groups and display slower dynamics than neurons</title><p>HU activity, defined as the expected value of <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>h</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), exhibits a rich variety of dynamical patterns (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). HUs can activate very transiently, slowly modulate their activity, or display periods of active and inactive states of comparable duration. <xref ref-type="fig" rid="fig4">Figure 4B</xref> highlights a few HU activity traces that illustrate this diversity of HU dynamics. The top three panels of <xref ref-type="fig" rid="fig4">Figure 4B</xref> show the dynamics of the assemblies of <xref ref-type="fig" rid="fig3">Figure 3A-F</xref> which encompass the ARTR hindbrain circuit that controls saccadic eye movements and directional tail flips. HUs 99 and 161 drive the left and right ARTR and display antiphasic activity with long dwell times of ∼15s, in accordance with previous studies (<xref ref-type="bibr" rid="bib3">Ahrens et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>). HU 102 and 163 correspond to the oculomotor neurons in the nuclei nIII and nVI that together drive the horizontal saccades. Their temporal dynamics are locked to that of the ARTR units in line with the previously identified role of ARTR as a pacemaker for the eye saccades (<xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>). HUs 95 and 135, which drive directional tail flips, display transient activations that only occur when the ipsilateral ARTR-associated HU is active. This is consistent with the previous finding that the ARTR alternating activation pattern sets the orientation of successive tail flips accordingly (<xref ref-type="bibr" rid="bib20">Dunn et al., 2016</xref>). The fourth panel shows the traces of the brain-wide assemblies of <xref ref-type="fig" rid="fig3">Figure 3G, I</xref>, displaying slow tonic modulation of their activity. Finally, the bottom panel, which corresponds to the collective dynamics of assembly 122 (<xref ref-type="fig" rid="fig3">Figure 3H</xref>), comprises short transient activity that likely corresponds to fictive swimming events.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>HU dynamics are bimodal and activate slower than neurons.</title><p>(<bold>A</bold>) HU dynamics are diverse and are partially shared across HUs. The bimodality transition point of each HU was determined and subtracted individually, such that positive values correspond to HU activation (see Materials and methods - ‘Time constant calculation’6.12). The test data consisted of three blocks, with a discontinuity in time between the first and second block (Materials and methods). (<bold>B</bold>) Highlighted example traces from panel A. HU indices are denoted on the right of each trace, colored according to their cluster from panel D. The corresponding cellular assemblies of these HU are shown in <xref ref-type="fig" rid="fig3">Figure 3A-I</xref>. (<bold>C</bold>) Top: Pearson correlation matrix of the dynamic activity of panel A. Bottom: Hierarchical clustering of the Pearson correlation matrix. Clusters (as defined by the colors) were annotated manually. This sorting of HUs is maintained throughout the manuscript. OT: Optic Tectum, Di: Diencephalon, ARTR: ARTR-related, Misc.: Miscellaneous, L: Left, R: Right. (<bold>D</bold>) A Principal Component Analysis (PCA) of the HU dynamics of panel A shows that much of the HU dynamics variance can be captured with a few PCs. The first 3 PCs captured 52%, the first 10 PCs captured 73% and the first 25 PCs captured 85% of the explained variance. (<bold>E</bold>) The distribution of all HU activity values of panel A shows that HU activity is bimodal and sparsely activated (because the positive peak is smaller than the negative peak). PDF: Probability Density Function. (<bold>F</bold>) Distribution of the time constants of HUs (black) and neurons (grey). Time constants are defined as the median oscillation period, for both HUs and neurons. An HU oscillation is defined as a consecutive negative and positive activity interval. A neuron oscillation is defined as a consecutive interspike-interval and spike-interval (which can last for multiple time steps, for example see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The time constant distribution of HUs is greater than the neuron distribution (Mann Whitney U test, <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig4-v2.tif"/></fig><p>Some HUs regularly co-activate, leading to strong correlations between different HUs. This is quantified by their Pearson correlation matrix shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref> (top), which reveals clusters of correlated HUs. These were grouped using hierarchical clustering (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom), and we then manually identified their main anatomical location (top labels). These clusters of HUs with strongly correlated activity suggest that much of the HU variance could be captured using only a small number of variables. We quantified this by performing PCA on the HU dynamics, finding that indeed 52% of the variance was captured by the first three PCs, and 85% by the first 20 PCs (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). We further observed that HU activity is bimodal, as evidenced by the distribution of all HU activity traces in <xref ref-type="fig" rid="fig4">Figure 4E</xref>. This bimodality can emerge because the dReLU potentials <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>) can learn to take different shapes, including a double-well potential that leads to bimodal dynamics (see Materials and methods - ‘Choice of HU potential’). This allows us to effectively describe HU activity as a two-state system, where <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> increases the probability to spike (<inline-formula><mml:math id="inf192"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) for its positively connected neurons, and <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> decreases their probability to spike. The binarized neuron activity is also a two-state system (spiking or not spiking), which enabled us to compare the time constants of neuron and HU state changes, quantified by the median time between successive onsets of activity. We find that HUs, which represent the concerted dynamics of neuronal assemblies, operate on a slower time scale than individual neurons (<xref ref-type="fig" rid="fig4">Figure 4F</xref>, <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5G-L</xref>). This observation aligns with the expected difference between cellular and circuit-level time scales.</p></sec><sec id="s2-5"><title>cRBM embodies functional connectivity that is strongly correlated across individuals</title><p>The probabilistic nature of cRBMs uniquely enables in silico perturbation experiments to estimate the functional connection <inline-formula><mml:math id="inf194"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between pairs of neurons, where <inline-formula><mml:math id="inf195"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is quantified by directly perturbing the activity of neuron <inline-formula><mml:math id="inf196"><mml:mi>j</mml:mi></mml:math></inline-formula> and observing the change in probability to spike of neuron <inline-formula><mml:math id="inf197"><mml:mi>i</mml:mi></mml:math></inline-formula>. We first defined the generic, symmetric functional connection <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> using <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>) and then used <inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ12">Equation 12</xref>) to derive the cRBM-specific <inline-formula><mml:math id="inf201"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>, see Materials and methods - ‘Effective connectivity matrix’). Using this definition of <inline-formula><mml:math id="inf202"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we constructed a full neuron-to-neuron effective connectivity matrix for each zebrafish recording. We then asked whether this cRBM-inferred connectivity matrix was robust across individuals. For this purpose, we calculated the functional connections between anatomical regions, given by the assemblies that occupy each region, because neuronal identities can vary across individual specimen. We aggregated neurons using the <italic>L</italic><sub>1</sub> norm for each pair of anatomical regions to determine the functional connection between regions (see Materials and methods - ‘From inter-neuron to inter-region connectivity’). For this purpose, we considered anatomical regions as defined by the mapzebrain atlas (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>) for which a regional-scale structural connectivity matrix exists to which we will compare our functional connectivity matrix.</p><p>This led to a symmetrical functional connectivity matrix for each animal, three of which are shown in <xref ref-type="fig" rid="fig5">Figure 5A-C</xref> (where non-imaged regions are left blank, and all eight animals are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). The strength of functional connections is distributed approximately log-normal (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), similar to the distribution of structural region-to-region connections (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>). To quantify the similarity between individual fish, we computed the Pearson correlation between each pair of fish. Functional connectivity matrices correlate strongly across individuals, with an average Pearson correlation of 0.69 (<xref ref-type="fig" rid="fig5">Figure 5E and F</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>cRBM gives rise to functional connectivity that is strongly correlated across individuals.</title><p>(<bold>A</bold>) The functional connectivity matrix between anatomical regions of the mapzebrain atlas (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>) of example fish #2 is shown. Functional connections between two anatomical regions were determined by the similarity of the HUs to which neurons from both regions connect to (Materials and methods). Mapzebrain atlas regions with less than five imaged neurons were excluded, yielding <inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> regions in total. See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for region name abbreviations. The matrix is shown in <italic>log</italic><sub>10</sub> scale, because functional connections are distributed approximately log-normal (see panel D). (<bold>B</bold>) Equivalent figure for example fish #3 (example fish of prior figures). (<bold>C</bold>) Equivalent figure for example fish #4. Panels A-C share the same <italic>log</italic><sub>10</sub> color scale (right). (<bold>D</bold>) Functional connections are distributed approximately log-normal. (Mutual information with a log-normal fit (black dashed line) is 3.83, while the mutual information with a normal fit is 0.13). All connections of all eight fish are shown, in <italic>log</italic><sub>10</sub> scale (purple). (<bold>E</bold>) Functional connections of different fish correlate well, exemplified by the three example fish of panels A-C. All non-zero functional connections (x-axis and y-axis) are shown, in <italic>log</italic><sub>10</sub> scale. Pearson correlation <inline-formula><mml:math id="inf204"><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub></mml:math></inline-formula> between pairs: <inline-formula><mml:math id="inf205"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.73</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf206"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.73</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf207"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.78</mml:mn></mml:mrow></mml:math></inline-formula>. All correlation p values <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (two-sided t-test). (<bold>F</bold>) Pearson correlations <inline-formula><mml:math id="inf209"><mml:msub><mml:mi>r</mml:mi><mml:mtext>P</mml:mtext></mml:msub></mml:math></inline-formula> of region-to-region functional connections between all pairs of 8 fish. For each pair, regions with less than five neurons in either fish were excluded. All p values <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (two-sided t-test), and average correlation value is 0.69.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Functional connectivity matrices of all fish.</title><p>8 panels. (<bold>A–H</bold>) showing the individual cRBM functional connectivity matrices of all zebrafish recordings. Panels B, C, and D correspond to <xref ref-type="fig" rid="fig3">Figure 3A–C</xref>. All matrices share the same <italic>log</italic><sub>10</sub> color scale (bottom right). Regions that contained less than five neurons were left blank, for each fish individually.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig5-figsupp1-v2.tif"/></fig></fig-group><p>We conclude that similar functional circuits spontaneously activate across individuals, despite the limited duration of neural recordings (∼25 minutes), which can be identified across fish using independently estimated cRBMs. In the next section, we aggregate these individual matrices to a general functional connectivity matrix for comparison with the zebrafish structural connectivity matrix.</p></sec><sec id="s2-6"><title>cRBM-inferred functional connectivity reflects structural connectivity</title><p>In the previous section we have determined the functional connections between anatomical regions using the cRBM assembly organization. Although functional connectivity stems from the structural (i.e. biophysical) connections between neurons, it can reflect correlations that arise through indirect network interactions (<xref ref-type="bibr" rid="bib8">Bassett and Sporns, 2017</xref>; <xref ref-type="bibr" rid="bib18">Das and Fiete, 2020</xref>). Using recently published structural connectivity data of the zebrafish brain (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>), we are now able to quantify the overlap between a structurally defined connectivity matrix and our functional connectivity matrix estimated through neural dynamics. <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref> determined a zebrafish structural connectivity matrix between 72 anatomical regions using structural imaging data from thousands of individually Green Fluorescent Protein (GFP)-labeled neurons from multiple animals. We slightly extended this matrix by using the most recent data, filtering indirect connections and accounting for the resulting sampling bias (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, regions that were not imaged in our light-sheet microscopy experiments were excluded). Next, we aggregated the functional connectivity matrices of all our calcium imaging recordings to one grand average functional connectivity matrix (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>cRBM-inferred functional connectivity reflects structural connectivity.</title><p>(<bold>A</bold>) Structural connectivity matrix is shown in <italic>log</italic><sub>10</sub> scale, updated from Figure 8C of <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>. Regions that were not imaged in our experiments were excluded (such that <inline-formula><mml:math id="inf211"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> out of 72 regions remain). Regions (x-axis and y-axis) were sorted according to <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>. Compared to Figure 8C of <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref> additional structural data was added and the normalization procedure was updated to include within-region connectivity (see Materials and methods - ‘Extensions of the structural connectivity matrix’). See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for region name abbreviations. (<bold>B</bold>) Average functional connectivity matrix is shown in <italic>log</italic><sub>10</sub> scale, as determined by averaging the cRBM functional connectivity matrices of all 8 fish (see Materials and methods - ‘Specimen averaging of connectivity matrices’). The same regions (x-axis and y-axis) are shown as in panel A. (<bold>C</bold>) The average functional and structural connectivity of panels A and B correlate well, with Spearman correlation <inline-formula><mml:math id="inf212"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>S</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.39</mml:mn></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, two-sided t-test). Each data point corresponds to one region-to-region pair. Data points for which the structural connection was exactly 0 were excluded (see panel D for their analysis). (<bold>D</bold>) The distribution of average functional connections of region pairs with non-zero structural connections is greater than functional connections corresponding to region pairs without structural connections (<inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, two-sided Kolmogorov-Smirnov test). The bottom panel shows the evidence for inferring either non-zero or zero structural connections, defined as the fraction between the PDFs of the top panel (fitted Gaussian distributions were used for denoising).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>cRBM functional connectivity compared to baseline methods.</title><p>(<bold>A</bold>) Average functional connectivity matrix of the covariance baseline method (see Materials and methods - ‘Connectivity inference baselines’) in <italic>log</italic><sub>10</sub> scale. (<bold>B</bold>) Average functional connectivity matrix of the Pearson correlations baseline method (see Materials and methods - ‘Connectivity inference baselines’) in <italic>log</italic><sub>10</sub> scale. (<bold>C, D</bold>) Equivalent figure to <xref ref-type="fig" rid="fig4">Figure 4C</xref> for functional connections as determined by the covariance (<bold>C</bold>) and correlations (<bold>D</bold>) between neurons (see Materials and methods - ‘Correlation analysis of connectivity matrices’). The Spearman correlation coefficient between functional and structural connections for covariance is <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.18</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and for correlation <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>E–F</bold>) Equivalent figure to <xref ref-type="fig" rid="fig4">Figure 4D</xref> for functional connections as determined by the covariance (<bold>E</bold>) and correlations (<bold>F</bold>) between neurons. Non-zero and zero distributions are significantly different (all p values <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, two-sided Kolmogorov-Smirnov tests).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83139-fig6-figsupp1-v2.tif"/></fig></fig-group><p>For comparison, we also calculated the connectivity matrices defined by either covariance or Pearson correlation (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). The cRBM functional connectivity spans a larger range of values than either of these methods, leading to a more fine-grained connectivity matrix akin to the structural connectivity map (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). This greater visual resemblance was statistically confirmed by calculating the Spearman correlation between structural and functional connectivity, which is greater for cRBM (<inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>S</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.39</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig6">Figure 6C</xref>), than for covariance-based connectivity (<inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>S</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.18</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> left) or correlation-based connectivity (<inline-formula><mml:math id="inf220"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>S</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> right). Hence, using recordings of ∼25 min on average, cRBMs were able to identify functional connections that resemble the anatomical connectivity between brain regions. Strong or weak functional connections are predictive of present or absent structural connections respectively (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), and could thus potentially be used for inference in systems where the structural connectivity pattern is unknown.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have developed a cRBM model that accurately replicated the data statistics of brain-scale zebrafish recordings, thereby forming neural assemblies that spanned the entire brain. The objective of our study was threefold: first, to show that the cRBM model can be applied to high-dimensional data, such as whole-brain recordings, second, to prove that an assembly-based model is sufficient to generate whole-brain neural data statistics, and third, to describe the physiological properties of the assembly organization in the zebrafish brain and use it to create a functional connectivity map. We have shown that, after convergence, the cRBM-generated data not only replicated the data statistics that it was constrained to fit, but also extrapolated to fit the pairwise correlation statistics of neurons and HUs, leading to a better reconstruction of neural data than a fully connected GLM (<xref ref-type="fig" rid="fig2">Figure 2</xref>). These results thereby quantify how neural assemblies play a major role in determining the collective dynamics of the brain. To achieve this, cRBMs formed sparsely localized assemblies that spanned the entire brain, facilitating their biological interpretation (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>, <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). Further, the probabilistic nature of the cRBM model allowed us to create a mesoscale functional connectivity map that was largely conserved across individual fish and correlated well with structural connectivity (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p><p>The maximum entropy principle underlying the cRBM definition has been a popular method for inferring pairwise effective connections between neurons or assemblies of co-activating cells (<xref ref-type="bibr" rid="bib64">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib69">Tavoni et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Ferrari et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Meshulam et al., 2017</xref>; <xref ref-type="bibr" rid="bib58">Posani et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Chen et al., 2019</xref>). However, its computational cost has limited this pairwise connectivity analysis to typically <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi/><mml:mo rspace="0.8pt">∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> neurons. The two-layer cRBM model that we used here alleviates this burden, because the large number of neuron-to-neuron connections are no longer explicitly optimized, which enables a fast data sampling procedure (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). However, we have shown that these connections are still estimated indirectly with high accuracy via the assemblies they connect to (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). We have thus shown that the cRBM is able to infer the <inline-formula><mml:math id="inf222"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>≈</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (symmetric) pairwise connections through its assembly structure, a feat that is computationally infeasible for many other methods. By implementing various algorithmic optimizations (Materials and methods - ‘Algorithmic Implementation’), cRBM models converged in approximately 8–12 hr on high-end desktop computers (also see Materials and methods - ‘Computational limitations’).</p><p>Previously, we have extensively compared cRBM performance to other dimensionality reduction techniques, including Principal Component Analysis (PCA), Independent Component Analysis (ICA), Variational Autoencoders (VAEs) and their sparse variants, using protein sequence data as a benchmark (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). Briefly put, we showed that PCA and ICA could not accurately model the system due to their deterministic nature, putting too much emphasis on low-probability high-variance states, while VAEs were unable to capture all features of data due to the unrealistic assumption of independent, Gaussian-distributed latent variables. In this study, we repeated this comparison with sparse linear VAEs, and reached similar conclusions: VAEs trained using the same protocol as cRBMs failed to reproduce second-order data statistics and to reconstruct neural activity via the latent layer, while the learnt assemblies were of substantially lower quality (indicated by a large fraction of disconnected HUs, as well as a highly variable assembly size; <xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref>). Additionally, while PCA has previously been successful in describing zebrafish neural dynamics in terms of their main covariances modes (<xref ref-type="bibr" rid="bib2">Ahrens et al., 2012</xref>; <xref ref-type="bibr" rid="bib47">Marques et al., 2020</xref>), we show here that it is not appropriate for assembly extraction due to the absence of both a compositional and stochastic nature (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). Furthermore, we have shown that the generative component of cRBM models is essential for quantitatively assessing that the assembly organization is sufficient for reproducing neural statistics (<xref ref-type="fig" rid="fig2">Figure 2</xref>), moving beyond deterministic clustering analyses such as k-means (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2018</xref>), similarity graph clustering (<xref ref-type="bibr" rid="bib51">Mölter et al., 2018</xref>) or non-negative matrix factorization (<xref ref-type="bibr" rid="bib53">Mu et al., 2019</xref>) (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>After having quantitatively validated the resultant assemblies, we moved to discussing the biological implications of our findings. Previous studies of the zebrafish optic tectum have identified neural assemblies that were spatially organized into single dense clusters of cells (<xref ref-type="bibr" rid="bib62">Romano et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Diana et al., 2019</xref>; <xref ref-type="bibr" rid="bib73">Triplett et al., 2020</xref>). We have replicated these findings by observing the distinct organization of ball-shaped assemblies in the optic tectum (<xref ref-type="fig" rid="fig3">Figure 3L</xref>). However, our data extends to many other anatomical regions in the brain, where we found that assemblies can be much more dispersed, albeit still locally dense, consisting of multiple clusters of neurons (<xref ref-type="fig" rid="fig3">Figure 3</xref>). In sum, cRBM-inferred cell assemblies display many properties that one expects from physiological cell assemblies: they are anatomically localized, can overlap, encompass functionally identified neuronal circuits and underpin the collective neural dynamics (<xref ref-type="bibr" rid="bib28">Harris, 2005</xref>; <xref ref-type="bibr" rid="bib29">Harris, 2012</xref>; <xref ref-type="bibr" rid="bib21">Eichenbaum, 2018</xref>). Yet, the cRBM bipartite architecture lacks many of the traits of neurophysiological circuits. In particular, cRBMs lack direct neuron-to-neuron connections, asymmetry in the connectivity weights and a hierarchical organization of functional dependencies beyond one hidden layer. Therefore, to what extent cRBM-inferred assemblies identify to neurophysiological cell assemblies, as postulated by <xref ref-type="bibr" rid="bib31">Hebb, 1949</xref> and others, remains an open question.</p><p>cRBM allowed us to compute the effective, functional connections between each pair of neurons, aggregated to functional connections between each pair of regions, by perturbing neural activity in silico. Importantly, we found that this region-scale connectivity is well-conserved across specimen. This observation is non-trivial because each recording only lasted ∼25 min, which represents a short trajectory across accessible brain states. It suggests that, although each individual brain may be unique at the neuronal scale, the functional organization could be highly stereotyped at a sufficiently coarse-grained level.</p><p>It would be naive to assume that these functional connections equate biophysical, structural connections (<xref ref-type="bibr" rid="bib18">Das and Fiete, 2020</xref>). Both represent different, yet interdependent aspects of the brain organization. Indeed, we found that structural connectivity is well-correlated to functional connectivity, confirming that functional links are tied to the structural blueprint of brain connectivity (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Furthermore, strong (weak) functional connections are predictive of present (absent) structural connections between brain regions, although intermediate values are ambiguous.</p><p>It will be crucial to synergistically merge structural and dynamic information of the brain to truly comprehend brain-wide functioning (<xref ref-type="bibr" rid="bib6">Bargmann and Marder, 2013</xref>; <xref ref-type="bibr" rid="bib40">Kopell et al., 2014</xref>). Small brain organisms are becoming an essential means to this end, providing access to a relatively large fraction of cells (<xref ref-type="bibr" rid="bib4">Ahrens and Engert, 2015</xref>). To generate new scientific insights it is thus essential to develop analytical methods that can scale with the rapidly growing size of both structural and dynamic data (<xref ref-type="bibr" rid="bib32">Helmstaedter, 2015</xref>; <xref ref-type="bibr" rid="bib5">Ahrens, 2019</xref>). In this study, we have established that the cRBM can model high-dimensional data accurately, and that its application to zebrafish recordings was crucial to unveil their brain-scale assembly organization. In future studies, cRBMs could be used to generate artificial data whose statistics replicate those of the zebrafish brain. This could be used for further in silico ablation and perturbation studies with strong physiological footing, crucial for developing hypotheses for future experimental work (<xref ref-type="bibr" rid="bib38">Jazayeri and Afraz, 2017</xref>; <xref ref-type="bibr" rid="bib18">Das and Fiete, 2020</xref>). Lastly, the application of cRBMs is not specific to calcium imaging data, and can therefore be readily applied to high-dimensional neural data obtained by other recording techniques.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">cRBM algorithm</td><td align="left" valign="bottom">This paper and <xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/jertubiana/PGM">github.com/jertubiana/PGM</ext-link></td><td align="left" valign="bottom">Materials and methods - ‘Restricted Boltzmann Machines’ , ‘Compositional Restricted Boltzmann Machine’ and Algorithmic Implementation</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Fishualizer</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/benglitz/fishualizer_public">bitbucket.org/benglitz/fishualizer_public</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Blind Sparse Deconvolution</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib78">Tubiana et al., 2020</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/jertubiana/BSD">github.com/jertubiana/BSD</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">ZBrain Atlas</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib60">Randlett et al., 2015</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://engertlab.fas.harvard.edu/Z-Brain">engertlab.fas.harvard.edu/Z-Brain</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">mapzebrain atlas</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://fishatlas.neuro.mpg.de/">fishatlas.neuro.mpg.de</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">MATLAB (data preprocessing)</td><td align="left" valign="bottom">MathWorks</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/products/matlab.html">mathworks.com/products/matlab.html</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Computational Morphometry Toolkit (CMTK)</td><td align="left" valign="bottom">NITRC</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/cmtk">nitrc.org/projects/cmtk</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Python</td><td align="left" valign="bottom">Python Software Foundation</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.python.org/">python.org</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Danio rerio, nacre mutant</italic>)</td><td align="left" valign="bottom">Tg(elavl3:H2B-GCaMP6f)</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib59">Quirin et al., 2016</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Danio rerio, nacre mutant</italic>)</td><td align="left" valign="bottom">Tg(elavl3:H2B-GCaMP6s)</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib83">Vladimirov et al., 2014</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Data and code availability</title><p>The cRBM model has been developed in Python 3.7 and is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/jertubiana/PGM">https://github.com/jertubiana/PGM</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a671999516b1e9eddb70b706752e9ed2a636ca78;origin=https://github.com/jertubiana/PGM;visit=swh:1:snp:b8c1e3bccb8d255065fee4aa971f297265b59ef9;anchor=swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe">swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe</ext-link>; <xref ref-type="bibr" rid="bib79">Tubiana and van der Plas, 2023</xref>). An extensive example notebook that implements this model is provided <ext-link ext-link-type="uri" xlink:href="https://github.com/jertubiana/PGM/blob/master/examples/Neural%5C%20recordings%5C%20of%5C%20larval%5C%20zebrafish%5C%20spontaneous%5C%20activity.ipynb">here</ext-link>.</p><p>Calcium imaging data pre-processing was performed in MATLAB (Mathworks) using previously published protocols and software (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>; <xref ref-type="bibr" rid="bib78">Tubiana et al., 2020</xref>). The functional data recordings, the trained cRBM models and the structural and functional connectivity matrix are available at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data">https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data</ext-link>.</p><p>Figures of neural assemblies or neurons (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig3">3</xref>) were made using the Fishualizer, which is a 4D (space +time) data visualization software package that we have previously published (<xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>), available at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/benglitz/fishualizer_public">https://bitbucket.org/benglitz/fishualizer_public</ext-link>. Minor updates were implemented to tailor the Fishualizer for viewing assemblies, which can be found at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/benglitz/fishualizer_public/src/assembly_viewer">https://bitbucket.org/benglitz/fishualizer_public/src/assembly_viewer</ext-link>.</p><p>All other data analysis and visualization was performed in Python 3.7 using standard packages (numpy <xref ref-type="bibr" rid="bib30">Harris et al., 2020</xref>), scipy (<xref ref-type="bibr" rid="bib82">Virtanen et al., 2020</xref>), scikit-learn (<xref ref-type="bibr" rid="bib57">Pedregosa, 2011</xref>), matplotlib (<xref ref-type="bibr" rid="bib36">Hunter, 2007</xref>), pandas (<xref ref-type="bibr" rid="bib48">McKinney, 2010</xref>), seaborn (<xref ref-type="bibr" rid="bib85">Waskom, 2021</xref>), h5py. The corresponding code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vdplasthijs/zf-rbm">https://github.com/vdplasthijs/zf-rbm</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f6167f75be922f03a1b1c52e7ff94a3705a69b85;origin=https://github.com/vdplasthijs/zf-rbm;visit=swh:1:snp:d6ea653d3d9faf01f8e678067f4f52716ff32f10;anchor=swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444">swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444</ext-link>; <xref ref-type="bibr" rid="bib80">van der Plas, 2023</xref>).</p></sec><sec id="s4-2"><title>Zebrafish larvae</title><p>Experiments were conducted on nacre mutants, aged 5–7 days post-fertilization (dpf). Larvae were reared in Petri dishes at 28 °C in embryo medium (E3) on a 14/10 hr light/dark cycle, and were fed powdered nursery food every day from 6 dpf. They were expressing either the calcium reporter GCaMP6s (fish 1–4, 6, and 8) or GCaMP6f (fish 5 and 7) under the control of the nearly pan-neuronal promoter elavl3 expressed in the nucleus <italic>Tg(elavl3:H2B-GCaMP6</italic>). Both lines were provided by Misha Ahrens and published by <xref ref-type="bibr" rid="bib83">Vladimirov et al., 2014</xref> (H2B-GCaMP6s) and <xref ref-type="bibr" rid="bib59">Quirin et al., 2016</xref> (H2B-GCaMP6f). Experiments were approved by Le Comité d’Éthique pour l’Experimentation Animale Charles Darwin C2EA-05 (02601.01).</p></sec><sec id="s4-3"><title>Light-sheet microscopy of zebrafish larvae</title><p>Spontaneous neural activity (i.e. in the absence of sensory stimulation) was recorded in larval zebrafish using light-sheet microscopy, which acquires brain-scale scans by imaging multiple <inline-formula><mml:math id="inf223"><mml:mi>z</mml:mi></mml:math></inline-formula>-planes sequentially (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). Larvae were placed in 2% low melting point agarose (Sigma-Aldrich), drawn tail-first into a glass capillary tube with 1 mm inner diameter via a piston and placed in chamber filled with E3 in the microscope. Recordings were of length 1514 ± 238 seconds (mean ± standard deviation), with a brain volume imaging frequency of 3.9 ± 0.8 Hz.</p><p>The following imaging pre-processing steps were performed offline using MATLAB, in line with previously reported protocols (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). Automated cell segmentation was performed using a watershed algorithm (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>) and fluorescence values of pixels belonging to the same neuron was averaged to obtain cell measurements. The fluorescence intensity values <inline-formula><mml:math id="inf224"><mml:mi>F</mml:mi></mml:math></inline-formula> were normalized to <inline-formula><mml:math id="inf225"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf226"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is the baseline signal per neuron and <italic>F</italic><sub>0</sub> is the overall background intensity (<xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). The <inline-formula><mml:math id="inf227"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> activity of different imaging planes was subsequently temporally aligned using interpolation (because of the time delay between imaging planes; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>) and deconvolved to binarized spike traces using Blind Sparse Deconvolution (BSD) (<xref ref-type="bibr" rid="bib78">Tubiana et al., 2020</xref>). BSD estimates the most likely binary spike trace by minimizing the <italic>L</italic><sub>2</sub> norm of the difference between the estimated spike trace convolved with an exponential kernel and the ground-truth calcium data, using <italic>L</italic><sub>1</sub> sparsity regularization and online hyperparameter optimization. Calcium kernel time constants used for deconvolution were inferred using BSD on the spontaneous activity of three different fish (approximately 5000 neurons per fish, recorded at 10 Hz, previously reported by <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). For the GCaMP6s line, we used a rise time of 0.2 s and a decay time of 3.55 s; for the GCaMP6f line, we used 0.15 s and 1.6 s, respectively.</p><p>Brain activity was recorded of 15 animals in total. Of these recordings, 1 was discarded because of poor image quality and 6 were discarded because neurons were inactive (defined by less than 0.02 spikes/(neurons × time points)), hence leaving 8 data sets for further analysis. The recorded brains were then registered onto the ZBrain Atlas (<xref ref-type="bibr" rid="bib60">Randlett et al., 2015</xref>) and the mapzebrain atlas (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>) for anatomical labeling of neurons (<xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>). The ZBrain Atlas was used in <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig4">4</xref> because of its detailed region descriptions (outlining 294 regions in total). However, we also registered our data to the mapzebrain atlas (72 regions in total) in order to compare our results with the structural connectivity matrix which was defined for this atlas only (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>). Only neurons that were registered to at least 1 ZBrain region were used for analysis (to filter imaging artefacts). This resulted in <inline-formula><mml:math id="inf228"><mml:mrow><mml:mn>40709</mml:mn><mml:mo>±</mml:mo><mml:mn>13854</mml:mn></mml:mrow></mml:math></inline-formula> neurons per recording (mean ± standard deviation, minimum = 23446, maximum = 65517).</p></sec><sec id="s4-4"><title>Maximum entropy principle</title><p>Here, we provide in brief the general derivation of the class of maximum entropy probabilistic models. Restricted Boltzmann Machines are an instance of this model, which is detailed in the following sections. The maximum entropy principle is used to create probabilistic models <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf230"><mml:mi>x</mml:mi></mml:math></inline-formula> denotes one data configuration sample) that replicate particular data statistics <italic>f</italic><sub><italic>k</italic></sub>, but are otherwise most random, and therefore least assumptive, by maximizing their entropy <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib25">Gardella et al., 2019</xref>). The goal of the model is to match its model statistics <inline-formula><mml:math id="inf232"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>model</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to the empirical data statistics <inline-formula><mml:math id="inf233"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This is done using Lagrange multipliers <inline-formula><mml:math id="inf234"><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which yields, when <inline-formula><mml:math id="inf235"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> is maximized with respect to <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the Boltzmann distribution (see, e.g., <xref ref-type="bibr" rid="bib11">Bialek, 2012</xref> for a full derivation):<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf237"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the resulting energy function. Importantly, the data dependency (<inline-formula><mml:math id="inf238"><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>) disappears when going from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> to <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. Hence, the maximum entropy principle only defines the shape of the distribution <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but not its specific parameters <inline-formula><mml:math id="inf240"><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib11">Bialek, 2012</xref>). In the case of RBM, these are then optimized using maximum likelihood estimation, as detailed in the sections below.</p><sec id="s4-4-1"><title>Motivation for choice of statistics</title><p>The derivation above describes the general maximum entropy model for a set of statistics <inline-formula><mml:math id="inf241"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The objective of this study is to extract the assembly structure from neural data, therefore creating two layers: a visible (neural data) layer <inline-formula><mml:math id="inf242"><mml:mrow><mml:mtext>v</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a hidden (latent) layer <inline-formula><mml:math id="inf243"><mml:mrow><mml:mtext>h</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The model should capture the mean activity of each neuron <inline-formula><mml:math id="inf244"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, their pairwise correlations <inline-formula><mml:math id="inf245"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, the neuron-HU interactions <inline-formula><mml:math id="inf246"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and a function of <inline-formula><mml:math id="inf247"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>. The latter is determined by the potential <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which we set to be a double Rectified Linear Unit (dReLU), as motivated in the following sections. Fitting all <inline-formula><mml:math id="inf249"><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> pairwise interactions <inline-formula><mml:math id="inf250"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is computationally infeasible, but under the cell assembly hypothesis we assume that this should not be necessary because collective neural behavior is expected to be explained by membership to similar assemblies via <inline-formula><mml:math id="inf251"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, and can therefore be excluded. We later show that pairwise correlations are indeed optimized implicitly (<xref ref-type="fig" rid="fig2">Figure 2</xref>). All other statistics are included and therefore explicitly optimized, also see <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>.</p></sec></sec><sec id="s4-5"><title>Restricted Boltzmann machines</title><p>A Restricted Boltzmann Machine (RBM) is an undirected graphical model defined on a bipartite graph (<xref ref-type="bibr" rid="bib66">Smolensky, 1986</xref>; <xref ref-type="bibr" rid="bib33">Hinton, 2002</xref>; <xref ref-type="bibr" rid="bib34">Hinton and Salakhutdinov, 2006</xref>), see <xref ref-type="fig" rid="fig2">Figure 2A</xref>. RBMs are constituted by two layers of random variables, neurons <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and Hidden Units (HUs) <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which are coupled by a weight matrix <inline-formula><mml:math id="inf254"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula>. There are no direct couplings between pairs of units within the same layer. Here, each visible unit <italic>v</italic><sub><italic>i</italic></sub> corresponds to a single recorded neuron with binary (spike-deconvolved) activity (<inline-formula><mml:math id="inf255"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). Each Hidden Unit (HU) <inline-formula><mml:math id="inf256"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> corresponds to the (weighted) activity of its neural assembly and is chosen to be real-valued. The joint probability distribution <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> writes (<xref ref-type="bibr" rid="bib34">Hinton and Salakhutdinov, 2006</xref>; <xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf258"><mml:mi>E</mml:mi></mml:math></inline-formula> is the energy function and <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="bold">v</mml:mi></mml:msub><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:msub></mml:mpadded><mml:mrow><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>v</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the partition function. The weights <italic>g</italic><sub><italic>i</italic></sub> and potentials <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> control the activity level of the visible units and the marginal distributions of the HUs respectively, and the weights <inline-formula><mml:math id="inf261"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> couple the two layers. Note that while <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is directly observed from the neural recordings, <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is by definition unobserved (i.e. hidden) and is sampled from the observed <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> values instead.</p><sec id="s4-5-1"><title>From data to features</title><p>Given a visible layer configuration <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, a HU <inline-formula><mml:math id="inf266"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> receives the input <inline-formula><mml:math id="inf267"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold">w</mml:mi><mml:mi>μ</mml:mi><mml:none/><mml:none/><mml:mi>T</mml:mi></mml:mmultiscripts><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and, owing to the bipartite architecture, the conditional distribution <inline-formula><mml:math id="inf268"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>h</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> factorizes as:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>h</mml:mi><mml:mo>⋅</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the cumulant generating function associated to the potential <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that normalizes <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> (<xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>). The average activity of HU <inline-formula><mml:math id="inf271"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> associated to a visible configuration <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by a linear-nonlinear transformation (as defined by the properties of the cumulant generating function):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Throughout the manuscript, we use this definition to compute HU activity <inline-formula><mml:math id="inf273"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (e.g., in <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></sec><sec id="s4-5-2"><title>From features to data</title><p>Conversely, given a hidden layer configuration <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, a visible unit <italic>v</italic><sub><italic>i</italic></sub> receives the input <inline-formula><mml:math id="inf275"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:none/><mml:none/><mml:mi>T</mml:mi></mml:mmultiscripts><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the conditional distribution factorizes as:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:munder><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and the average sampled <italic>v</italic><sub><italic>i</italic></sub> activity is given by:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf276"><mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the logistic function. Hence, a sampled visible layer configuration <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained by a weighted combination of the HU activity followed by Bernoulli sampling. RBMs are generative models, in the sense that they can generate new, artificial data using <xref ref-type="disp-formula" rid="equ8 equ1">Equations 8 and 10</xref>. <xref ref-type="fig" rid="fig2">Figure 2B</xref> illustrates this Markov Chain Monte Carlo (MCMC) process, by recursively sampling from <inline-formula><mml:math id="inf278"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which converges at equilibrium to <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-5-3"><title>Marginal distributions</title><p>The marginal distribution <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a closed-form expression because of the factorized conditional distribution of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>):<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>d</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For a quadratic potential <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the cumulant generating function would also be quadratic and <inline-formula><mml:math id="inf283"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would reduce to a Hopfield model, that is, a pairwise model with an interaction matrix <inline-formula><mml:math id="inf284"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). Otherwise, <inline-formula><mml:math id="inf285"><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> is not quadratic, yielding high-order effective interaction terms between visible units and allowing RBMs to express more complex distributions. Importantly, the number of parameters remains limited, controlled by <inline-formula><mml:math id="inf286"><mml:mi>M</mml:mi></mml:math></inline-formula> and does not scale as <inline-formula><mml:math id="inf287"><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (unlike pairwise models).</p></sec><sec id="s4-5-4"><title>Choice of HU potential</title><p>The choice of HU potential determines three related properties: the HU conditional distribution <inline-formula><mml:math id="inf288"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the transfer function of the HUs and the parametric form of the marginal distribution <inline-formula><mml:math id="inf289"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Hereafter we use the double-Rectified Linear Unit (dReLU) potential:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Varying the parameters <inline-formula><mml:math id="inf290"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> allows the potential to take a variety of shapes, including quadratic potentials (<inline-formula><mml:math id="inf291"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf292"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), ReLU potentials <inline-formula><mml:math id="inf293"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and double-well potentials (<xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>). The associated cumulant generating function <inline-formula><mml:math id="inf294"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is non-quadratic in general, and depending on the parameters, the transfer function can be linear, ReLU-like (asymmetric slope and thresholding) or logistic-like (strong local slopes for binarizing inputs). Closed-form expressions of <inline-formula><mml:math id="inf295"><mml:mi mathvariant="normal">Γ</mml:mi></mml:math></inline-formula> are detailed in <xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>, and its derivatives are also detailed in <xref ref-type="bibr" rid="bib75">Tubiana, 2018</xref>, p49-50. Note that the dReLU potential <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and distribution <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are invariant to the sign swap transformation <inline-formula><mml:math id="inf298"><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>⟺</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf299"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⟺</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="5.3pt">∀</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (leading to <inline-formula><mml:math id="inf300"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⟺</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). For visual clarity, we perform this sign swap transformation after training on all HUs with predominantly negative weights (defined by <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Subsequently all HUs are positively activated if the group of neurons to which it connects is strongly active.</p></sec><sec id="s4-5-5"><title>RBM training</title><p>The RBM is trained by maximizing the average log-likelihood of the empirical data configurations <inline-formula><mml:math id="inf302"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>, using stochastic gradient descent methods. The gradient update steps are derived by calculating the derivative of <inline-formula><mml:math id="inf303"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:math></inline-formula>, using <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, with respect to the model parameters (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>):<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>model</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>model</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>±</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>model</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>±</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>±</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mo>±</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>model</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Each gradient of <inline-formula><mml:math id="inf304"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:math></inline-formula> is thus the difference between a data statistic <inline-formula><mml:math id="inf305"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub></mml:math></inline-formula> and a model statistic <inline-formula><mml:math id="inf306"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>model</mml:mtext></mml:msub></mml:math></inline-formula>. Hence the model learns to match these statistics to the training data. Importantly, model statistics <inline-formula><mml:math id="inf307"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>model</mml:mtext></mml:msub></mml:math></inline-formula> cannot be evaluated exactly due to the exponentially large number of data configurations (e.g. <inline-formula><mml:math id="inf308"><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> visible configurations). Therefore they are approximated by computing the statistics of model-generated data using the MCMC sampling scheme defined with <xref ref-type="disp-formula" rid="equ8 equ1">Equations 8 and 10</xref> (see Materials and methods - ‘Matching data statistics to model statistics’ for more detail). MCMC sampling of a Boltzmann distribution in such high-dimensional space is in general very challenging owing to the exponentially long time to reach equilibrium. We use the persistent contrastive divergence approximation (<xref ref-type="bibr" rid="bib70">Tieleman, 2008</xref>) and discuss its validity below.</p></sec></sec><sec id="s4-6"><title>Compositional restricted boltzmann machine</title><p>In the previous sections, we have described the general properties of RBMs. We now motivate the specific RBM model choices that we have implemented, such as the dReLU potential and sparsity regularization, by discussing their impact on the properties of RBM-generated data.</p><p>Directed graphical models, for example, PCA, ICA, sparse dictionaries or variational autoencoders, prescribe a priori statistical constraints for their data representations, such as orthogonality/independence or specific marginal distributions such as Gaussian/sparse distributions. In contrast, the statistical properties of the representation of the data learned by RBMs are unknown a priori by construction (because of the maximum entropy principle). Instead, they emerge from the structure of the weight matrix, the potentials and the recursive back-and-forth sampling procedure described above. We have therefore previously studied the properties of typical samples of RBM with random weights as a function of the visible and hidden unit potentials and properties of the weight matrix using statistical mechanics tools (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>; <xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). We have identified the three following typical behaviors, or phases.</p><p>In the ferromagnetic phase, a typical sample from <inline-formula><mml:math id="inf309"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a single strongly activated HU (<inline-formula><mml:math id="inf310"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf311"><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of activated HUs at time <inline-formula><mml:math id="inf312"><mml:mi>t</mml:mi></mml:math></inline-formula>), whereas the others are not or merely weakly activated. The corresponding active visible units <italic>v</italic><sub><italic>i</italic></sub> are defined by the weight vector <inline-formula><mml:math id="inf313"><mml:msub><mml:mtext>w</mml:mtext><mml:msup><mml:mi>μ</mml:mi><mml:mo>⋆</mml:mo></mml:msup></mml:msub></mml:math></inline-formula> associated to the active HU <inline-formula><mml:math id="inf314"><mml:msub><mml:mi>h</mml:mi><mml:msup><mml:mi>μ</mml:mi><mml:mo>⋆</mml:mo></mml:msup></mml:msub></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ1">Equation 10</xref>).</p><p>In the spin-glass phase, a typical sample does not have any relatively strongly activated HUs, but instead many moderately activated ones (<inline-formula><mml:math id="inf315"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>). They interfere in a complex fashion to produce different visible unit configurations and there is no clear correspondence between the weight matrix and a typical data configuration.</p><p>Finally, in the compositional phase, a typical sample from <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a small number of strongly activated HUs (<inline-formula><mml:math id="inf317"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≪</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) whereas the others are weak or silent. Their weights are linearly combined through <xref ref-type="disp-formula" rid="equ1">Equation 10</xref> to produce the corresponding visible layer configuration. The compositional phase is desirable because, firstly, there exists a simple link between the weight matrix and typical data configurations (they are obtained by combining a few weights), which facilitates interpretation of biological systems (<xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>). Secondly, the corresponding neural activity distribution is rich, as different choices of HU subsets yield a combinatorial diversity of visible layer configurations. Moreover, the modular nature of the compositional phase facilitates the assembly organization of neural dynamics, as motivated in the Introduction.</p><p>A set of sufficient conditions for the emergence of the compositional phase are (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>):</p><list list-type="order"><list-item><p>The HUs are unbounded and real-valued with a non-linear, ReLU-like transfer function.</p></list-item><list-item><p>The weight matrix <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is sparse.</p></list-item><list-item><p>The columns <inline-formula><mml:math id="inf319"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> of the weight matrix have similar norm. (If a weight column associated to one HU is much larger than the others, visible configurations are solely aligned to it according to <xref ref-type="disp-formula" rid="equ1">Equation 10</xref>.)</p></list-item></list><p>The first condition is satisfied by the dReLU potential (but not by quadratic potentials or binary-valued HUs). The second condition is enforced in practice by adding a <italic>L</italic><sub>1</sub> sparse penalty term <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to the log-likelihood cost function. In our experiments, the optimal sparsity parameter <inline-formula><mml:math id="inf321"><mml:mi>λ</mml:mi></mml:math></inline-formula> was determined to be <inline-formula><mml:math id="inf322"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula> by cross-validation (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The final condition is achieved by enforcing that <inline-formula><mml:math id="inf323"><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf324"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="5.3pt">∀</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This is done by an appropriate reparameterization of the HU potential of <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> and a batch-norm–like procedure, described in detail in <xref ref-type="bibr" rid="bib75">Tubiana, 2018</xref>. This normalization promotes homogeneity among HU importance, preventing some units from being disconnected or others from dominating. In addition, ensuring that <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> irrespective of the visible layer size (as opposed to e.g., <inline-formula><mml:math id="inf326"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> which yields <inline-formula><mml:math id="inf327"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>) avoids the problem of ill-conditioned Hessians that was previously described by <xref ref-type="bibr" rid="bib35">Hinton, 2012</xref>.</p><p>To emphasise the departure from the classic RBM formulation in this study, we name our model compositional RBM (cRBM).</p></sec><sec id="s4-7"><title>Algorithmic implementation</title><p>In the previous sections, we have described the cRBM model in full mathematical detail. The corresponding algorithmic implementation was adapted from <xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>. In addition, we have made several major implementation and algorithmic changes to accommodate the large data size of the zebrafish neural recordings. We provide the code open-source, and describe the code improvements and hyperparameter settings in this section. The following improvements were made, leading to a substantial reduction of computation time:</p><list list-type="bullet"><list-item><p>Python 3 and numba (<xref ref-type="bibr" rid="bib43">Lam et al., 2015</xref>) were used to compile custom functions, enabling SIMD vectorization and multicore parallelism.</p></list-item><list-item><p>The sampling of <inline-formula><mml:math id="inf328"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and evaluating its cumulant generating function <inline-formula><mml:math id="inf329"><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and various moments requires repeated and costly evaluation of error functions erf and related functions (<xref ref-type="bibr" rid="bib75">Tubiana, 2018</xref>, p49-50). Fast numerical approximations of these functions were implemented based on <xref ref-type="bibr" rid="bib1">Abramowitz et al., 1988</xref> (p299).</p></list-item><list-item><p>The number of memory allocation operations was minimized.</p></list-item><list-item><p>The optimization algorithm was changed from stochastic gradient ascent to RMSprop (i.e. ADAM without momentum) with learning rate <inline-formula><mml:math id="inf330"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf331"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf332"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf333"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf334"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, see <xref ref-type="bibr" rid="bib39">Kingma and Ba, 2014</xref> for a definition of the parameters. Compared to the original stochastic gradient ascent, the adaptive learning rates of RMSprop/ADAM yield larger updates for the weights attached to neurons with very sparse activity, resulting in substantially faster convergence.</p></list-item></list><sec id="s4-7-1"><title>Hyperparameter settings</title><p>The following hyperparameters were used in the experiments of this manuscript:</p><list list-type="bullet"><list-item><p>Number of hidden unit <inline-formula><mml:math id="inf335"><mml:mi>M</mml:mi></mml:math></inline-formula>: 200. This value was determined by cross-validation (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) on one data set (example fish #3). Because this cross-validation procedure was computationally expensive, the same value was used for all other data sets, except for 3 data sets which used <inline-formula><mml:math id="inf336"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> because their <inline-formula><mml:math id="inf337"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Sparse regularization penalty <inline-formula><mml:math id="inf338"><mml:mi>λ</mml:mi></mml:math></inline-formula>: 0.02 (determined by cross-validation).</p></list-item><list-item><p>Batch size: 100, 200, or 400. Larger batch sizes yield longer training time but more stable training; batch size was increased if training failed to converge.</p></list-item><list-item><p>Number of Monte Carlo chains: 100.</p></list-item><list-item><p>Number of gradient updates: <inline-formula><mml:math id="inf339"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Number of Monte Carlo steps between each gradient update: 15.</p></list-item><list-item><p>Initial learning rate <inline-formula><mml:math id="inf340"><mml:mi>η</mml:mi></mml:math></inline-formula>: between <inline-formula><mml:math id="inf341"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf342"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We used <inline-formula><mml:math id="inf343"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> by default and if weight divergence was observed, the learning was reinitialized with a reduced learning rate. This occurred notably for high-<inline-formula><mml:math id="inf344"><mml:mi>M</mml:mi></mml:math></inline-formula> and low-<inline-formula><mml:math id="inf345"><mml:mi>λ</mml:mi></mml:math></inline-formula> models during the cross-validation procedure of <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></list-item><list-item><p>Learning rate annealing scheme: the learning rate geometrically decayed during training, starting after 25% of the gradient update steps, from its initial value <inline-formula><mml:math id="inf346"><mml:mi>η</mml:mi></mml:math></inline-formula> to a final value of <inline-formula><mml:math id="inf347"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p></list-item><list-item><p>Number of training data samples: 70% of frames of each recording (=4086 training data samples on average), see section ‘Train / test data split’ for details.</p></list-item></list></sec><sec id="s4-7-2"><title>Computational limitations</title><p>We found that 57.5% (<inline-formula><mml:math id="inf348"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mn>23</mml:mn><mml:mo>/</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) of cRBMs with optimal <inline-formula><mml:math id="inf349"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> settings successfully converged. cRBM models of these zebrafish data sets could be estimated in approximately 8–12 hr using 16 CPU threads (Intel Xeon Phi processor). The <inline-formula><mml:math id="inf350"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-cross-validation was therefore completed in three weeks using two desktop computers. Previously, we observed that this model requires a fixed number of gradient updates to converge, rather than a fixed number of epochs (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>; <xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>; <xref ref-type="bibr" rid="bib13">Bravi et al., 2021</xref>). Hence, in principle, runtime does not strictly depend on the recording length, as the number of epochs can be reduced for longer recordings (assuming that the data distribution remains statistically stationary).</p></sec></sec><sec id="s4-8"><title>Validity of the persistent contrastive divergence algorithm</title><p>Training RBM requires extensive MCMC sampling which is notoriously difficult for high-dimensional data sets. We resolve this by using Persistent Contrastive Divergence (PCD) to approximate the gradients (<xref ref-type="bibr" rid="bib70">Tieleman, 2008</xref>). In this section, we discuss why this worked to successfully converge, despite the very large data size.</p><p>The typical number of Monte Carlo steps required to transition from one energy minimum to another through an energy barrier <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> follows the Arrhenius law, scaling as <inline-formula><mml:math id="inf352"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. In the thermodynamic limit (<inline-formula><mml:math id="inf353"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="inf354"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> scales as the system size <inline-formula><mml:math id="inf355"><mml:mi>N</mml:mi></mml:math></inline-formula> multiplied by the typical energy required to flip a single visible unit, corresponding here to the inputs received from the hidden layer <inline-formula><mml:math id="inf356"><mml:mi>I</mml:mi></mml:math></inline-formula>. In contrast, for PCD only a limited number of MC steps (here, 15) are applied between each gradient update. Three factors explain why reasonably successful convergence was achieved in the trainings presented here.</p><p>Firstly, the use of the <italic>L</italic><sub>1</sub> regularization limits the magnitude of the weights and therefore limits the input scale <inline-formula><mml:math id="inf357"><mml:mi>I</mml:mi></mml:math></inline-formula>. Secondly, in the compositional phase, the energy barriers do not scale as the full system size <inline-formula><mml:math id="inf358"><mml:mi>N</mml:mi></mml:math></inline-formula> but rather as the size of one assembly <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf360"><mml:mi>p</mml:mi></mml:math></inline-formula> is the fraction of non-zero weights (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>). Indeed, transitioning from one energy minimum, characterized by a subset of strongly activated HUs, to another minimum, characterized by another set of strongly activated HUs, is done by gradually morphing the first set into the second (<xref ref-type="bibr" rid="bib63">Roussel et al., 2021</xref>). Compared to a direct transition, such a path is favored because the intermediate states are thermodynamically stable and energy barriers are smaller as each HU flip has an energy cost <inline-formula><mml:math id="inf361"><mml:mrow><mml:mi/><mml:mo rspace="0.8pt">∼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Lastly, throughout PCD training, MCMC sampling is not performed at thermal equilibrium and the model updates of the parameters of the distribution promote mixing (<xref ref-type="bibr" rid="bib71">Tieleman and Hinton, 2009</xref>). This is seen from <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>: the log-likelihood gradient is the difference between the gradient of the energy averaged over the empirical data and the energy averaged over MCMC samples. Ascending the gradient amounts to pushing down the energy of data configurations and pushing up the energy of MCMC samples, thereby promoting mixing of the Markov chains.</p><p>Overall, combining small learning rates (and large number of gradient updates), large regularization, large number of Markov Chains and Monte Carlo steps has allowed convergence to be reached for the majority of cRBM training sessions.</p></sec><sec id="s4-9"><title>Functional connectivity inference</title><sec id="s4-9-1"><title>Effective connectivity matrix</title><p>In this section, we present a derivation of the effective coupling matrix between neurons from the marginal distribution <inline-formula><mml:math id="inf362"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using cRBMs. This is achieved by perturbing the activity of each neuron individually and quantifying the effect on other neurons. We first define the local coupling <inline-formula><mml:math id="inf363"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between two neurons <italic>v</italic><sub><italic>i</italic></sub> and <italic>v</italic><sub><italic>j</italic></sub> for a generic probability distribution <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, given a data configuration <inline-formula><mml:math id="inf365"><mml:mi mathvariant="bold">v</mml:mi></mml:math></inline-formula>:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In other words, <inline-formula><mml:math id="inf366"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as the impact of the state of neuron <inline-formula><mml:math id="inf367"><mml:mi>j</mml:mi></mml:math></inline-formula> on neuron <inline-formula><mml:math id="inf368"><mml:mi>i</mml:mi></mml:math></inline-formula> in the context of activity pattern <inline-formula><mml:math id="inf369"><mml:mi mathvariant="bold">v</mml:mi></mml:math></inline-formula>. Hence, the effective connectivity matrix <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">J</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> mathematically defines the functional connections, which can only be done using a probabilistic model <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A positive (negative) coupling <inline-formula><mml:math id="inf372"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicates correlated (anti-correlated) collective behavior of neurons <inline-formula><mml:math id="inf373"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>. This effective coupling value is symmetric (because of Bayes’ rule): <inline-formula><mml:math id="inf374"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For context, note that <inline-formula><mml:math id="inf375"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is uniformly zero for an independent model of the form <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and that for a maximum entropy pairwise (Ising) model, with <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, the <inline-formula><mml:math id="inf378"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> matrix exactly identifies with the coupling matrix <inline-formula><mml:math id="inf379"><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ising</mml:mi></mml:msubsup></mml:math></inline-formula>, and does not depend on the data configuration <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (so <inline-formula><mml:math id="inf381"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>).</p><p>However, in general, and for RBMs in particular, <inline-formula><mml:math id="inf382"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depends on the data set <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and an overall coupling matrix can be derived by taking its average over all data configurations:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mtext mathvariant="bold">data</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Although <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> has a closed-form solution for RBMs (by inserting <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>), a naive evaluation requires <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> operations where <inline-formula><mml:math id="inf385"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of data samples. However, a fast and intuitive approximation can be derived by performing a second-order Taylor expansion of <inline-formula><mml:math id="inf386"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ17">Equation 17</xref> is exact for quadratic potential and in general justified as the contribution of neurons <inline-formula><mml:math id="inf387"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> is small compared to the scale of variation of <inline-formula><mml:math id="inf388"><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf390"><mml:mi>p</mml:mi></mml:math></inline-formula> is the fraction of non-zero couplings. In conclusion, we have mathematically derived the effective coupling between any two neurons <inline-formula><mml:math id="inf391"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf392"><mml:mi>j</mml:mi></mml:math></inline-formula>. Intuitively, two neurons <inline-formula><mml:math id="inf393"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> are effectively connected if they are connected to the same HUs (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>).</p></sec><sec id="s4-9-2"><title>From inter-neuron to inter-region connectivity</title><p>In the above section, we have derived the inter-neuronal connectivity matrix <inline-formula><mml:math id="inf394"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">J</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This matrix is then aggregated to an inter-regional connectivity matrix <inline-formula><mml:math id="inf395"><mml:msup><mml:mtext>J</mml:mtext><mml:mi class="ltx_font_mathcaligraphic">ℛ</mml:mi></mml:msup></mml:math></inline-formula> by taking the normalised <italic>L</italic><sub>1</sub>-norm of the corresponding <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">J</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix block elements (i.e., <inline-formula><mml:math id="inf397"><mml:mrow><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℛ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf398"><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is the set of neurons in region <inline-formula><mml:math id="inf399"><mml:mi>k</mml:mi></mml:math></inline-formula>).</p><p>Next, to derive the average connectivity matrix across multiple recordings, we used a weighted average of the individual recordings, with a region-pair specific weight equal to the length of the recording multiplied by the sum of the number of neurons in both regions (also see Section - ‘Specimen averaging of connectivity matrices’). Compared to a naive average, this weighted average accounts for the variable number of neurons per region between recordings.</p></sec><sec id="s4-9-3"><title>Training cRBM models for connectivity estimates</title><p>Constructing the functional connectivity matrix of a cRBM does not require test data, but just the estimated weight matrix <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (as explained above). Therefore we trained new cRBMs using the entire recordings (100% of data) to fully use the information available. cRBM training is stochastic, and to mitigate the possible variability that could arise we trained five cRBMs for each recording. Then, to assess convergence, we selected all cRBMs with <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>&lt;</mml:mo><mml:mtext>std</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">w</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, where std denotes standard deviation, for further functional connectivity analysis (yielding 23 cRBMs for 8 data sets in total). Connectivity estimates of multiple cRBM models per data sets were averaged.</p></sec><sec id="s4-9-4"><title>Connectivity inference baselines</title><p>We considered four additional connectivity inference baseline methods:</p><list list-type="bullet"><list-item><p>The covariance matrix.</p></list-item><list-item><p>The Pearson correlation matrix.</p></list-item><list-item><p>The sparse inverse covariance matrix inferred by graphical LASSO (<xref ref-type="bibr" rid="bib24">Friedman et al., 2008</xref>) (as implemented in scikit-learn with default settings <xref ref-type="bibr" rid="bib57">Pedregosa, 2011</xref>). Graphical LASSO is an efficient method for inference of large scale connectivity. Unfortunately, the implementation available failed to converge in reasonable time due to the high dimensionality of the data.</p></list-item><list-item><p>The Ising model with pseudo-likelihood maximization (PLM) inference (<xref ref-type="bibr" rid="bib61">Ravikumar et al., 2010</xref>).</p></list-item></list><p>Results obtained with the covariance and correlation matrices are presented in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The connectivity matrices obtained by the PLM Ising model (not shown) correctly identified the diagonal entries of the region-region matrix, but not the off-diagonal coefficients and had a weaker correlation with the structural connectivity matrix than the covariance and correlation matrices (<inline-formula><mml:math id="inf402"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></inline-formula> using 4 fish).</p></sec></sec><sec id="s4-10"><title>Optimizing the free parameters of cRBM</title><p>We set the free parameters <inline-formula><mml:math id="inf403"><mml:mi>λ</mml:mi></mml:math></inline-formula> (sparsity regularization parameter) and <inline-formula><mml:math id="inf404"><mml:mi>M</mml:mi></mml:math></inline-formula> (number of HUs) by cross-validating a large range of <inline-formula><mml:math id="inf405"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> values for one data set (fish #3). This was done by training cRBMs on 70% of the data, and evaluating model performance on the remaining test data, as detailed below. The resulting optimal values could then be used for all data sets (where <inline-formula><mml:math id="inf406"><mml:mi>M</mml:mi></mml:math></inline-formula> was scaled with the number of neurons <inline-formula><mml:math id="inf407"><mml:mi>N</mml:mi></mml:math></inline-formula>). Importantly, the <inline-formula><mml:math id="inf408"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> parameters implicitly tune the average assembly size. Increasing the number of HUs and/or increasing the regularization strength decreases the average number of neurons per assembly (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). Intuitively, assemblies that are too small do not have the capacity to capture high-order correlations, while assemblies that are too large would fail to account for local co-activations. Hence, the <inline-formula><mml:math id="inf409"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-cross-validation effectively identifies the optimal assembly sizes that fit the data statistics.</p><sec id="s4-10-1"><title>Train / test data split</title><p>We split up one recording (fish #3) into training data (70% of recording) and withheld test data (30% of recording) for the free parameter (<inline-formula><mml:math id="inf410"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) optimization procedure. This enabled us to assess whether the cRBMs learned to model the data statistics (as described in the main text, <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), while ensuring that the cRBMs are not overfitted to the specific training data configurations. Importantly, this assumes that the test data comes from the same statistical distribution as the training data (while consisting of different data configurations). To ensure this, we split up the recording of example fish #3 (used for parameter optimization) in training and test splits as follows (before training the cRBMs): We divided the recording of length <inline-formula><mml:math id="inf411"><mml:mi>T</mml:mi></mml:math></inline-formula> in 10 chronological segments of equal length (so that segment 1 has time points <inline-formula><mml:math id="inf412"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> et cetera), with the rationale that by maintaining temporal order within each segment we would later be able to conduct dynamic activity analysis. This yielded <inline-formula><mml:math id="inf413"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac linethickness="0pt"><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula> possible training/test splits of the neural data. We then evaluated the statistical similarity between the training and test split of each combination, by assessing the mean neural activity <inline-formula><mml:math id="inf414"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and pairwise neural correlations <inline-formula><mml:math id="inf415"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> statistics. We quantified the similarity between training and test statistics by calculating the Root Mean Square Error (<inline-formula><mml:math id="inf416"><mml:mrow><mml:mrow><mml:mtext>RMSE</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>). The most similar split is defined by the lowest RMSE, but to show that cRBM are not dependent on picking the best possible split, but rather on avoiding the bad splits, we then chose to use the split with the <inline-formula><mml:math id="inf417"><mml:msup><mml:mn>10</mml:mn><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula>-percentile ranking RMSE. We hope that this aids future studies, where a potentially high number of possible splits prevents researchers from evaluating all possible splits, but a good split may nevertheless be found efficiently.</p></sec><sec id="s4-10-2"><title>Assessment of data statistics</title><p>Please note that the loss function, the log-likelihood, is computationally intractable and therefore cannot be readily used to monitor convergence or goodness-of-fit after training (<xref ref-type="bibr" rid="bib23">Fischer &amp; Igel and Igel, 2012</xref>). Moreover, approximations of the log-likelihood based on annealed importance sampling were found to be unreliable due to the large system size. However, because (c)RBM learn to match data statistics to model statistics (see Materials and methods–RBM training), we can directly compare these to assess model performance. Therefore, we assessed the following quantities.</p><sec id="s4-10-2-1"><title>Matching data statistics to model statistics</title><p>Firstly, we evaluated three statistics that cRBMs are trained to optimize: the mean activity of neurons <inline-formula><mml:math id="inf418"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, the mean activity of HUs <inline-formula><mml:math id="inf419"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and their pairwise interactions <inline-formula><mml:math id="inf420"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. Additionally, second order statistics of pairwise neuron-neuron interactions <inline-formula><mml:math id="inf421"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, HU-HU interactions <inline-formula><mml:math id="inf422"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and the reconstruction quality were evaluated, which the cRBM was not constrained to fit. Monitoring HU single and pairwise statistics <inline-formula><mml:math id="inf423"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf424"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> served two purposes: (i) validation of model convergence and (ii) assessing whether correlations between assemblies can be captured by this bipartite model (i.e., without direct couplings between hidden units or an additional hidden layer). For each statistic <inline-formula><mml:math id="inf425"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, we computed its value based on empirical data <inline-formula><mml:math id="inf426"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub></mml:math></inline-formula> and on the model <inline-formula><mml:math id="inf427"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>model</mml:mtext></mml:msub></mml:math></inline-formula>, which we then quantitatively compared to assess model performance.</p><p>Data statistics <inline-formula><mml:math id="inf428"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub></mml:math></inline-formula> were calculated on withheld test data (30% of recording). Naturally, the neural recordings consisted only of neural data <inline-formula><mml:math id="inf429"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and not of HU data <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We therefore computed the expected value of <inline-formula><mml:math id="inf431"><mml:msub><mml:mtext>h</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> at each time point <inline-formula><mml:math id="inf432"><mml:mi>t</mml:mi></mml:math></inline-formula> conditioned on the empirical data <inline-formula><mml:math id="inf433"><mml:msub><mml:mtext>v</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, as further detailed in Methods - ‘From data to features’.</p><p>Model statistics <inline-formula><mml:math id="inf434"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>data</mml:mtext></mml:msub></mml:math></inline-formula> cannot be calculated exactly, because that would require one to sample all possible states <inline-formula><mml:math id="inf435"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>v</mml:mtext><mml:mo>,</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and were therefore approximated by evaluating cRBM-generated data. Here, 300 Monte Carlo chains were each initiated on random training data configurations and 50 configurations were sampled consecutively for each chain, with 20 sampling steps between saved configurations, after a burn-in period of 100 effective sampling configurations.</p><p>The <inline-formula><mml:math id="inf436"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> statistic (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) was corrected for the sparsity regularization, by adding the sparsity regularization parameter <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>:</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mtext>model</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mtext>model-generated data</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Furthermore, <inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> pairs with exactly <inline-formula><mml:math id="inf440"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> were excluded from analysis (5% of total for optimal cRBM in <xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>The pairwise neuron-neuron and HU-HU statistics (<inline-formula><mml:math id="inf441"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf442"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>) were corrected for their (trivially) expected correlation due to their mean activities (by subtraction of <inline-formula><mml:math id="inf443"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf444"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> respectively), so that only true correlations were assessed.</p></sec><sec id="s4-10-2-2"><title>Calculating the normalized Root Mean Square Error</title><p>Goodness of fit was quantified by computing the normalized Root Mean Square Error (nRMSE) for each statistic (shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The RMSE between two vectors <inline-formula><mml:math id="inf445"><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of length <inline-formula><mml:math id="inf446"><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> is defined as <inline-formula><mml:math id="inf447"><mml:mrow><mml:mtext>RMSE</mml:mtext><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. Ordinary RMSE was normalized so that different statistics could be compared, where 1 corresponds to <inline-formula><mml:math id="inf448"><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mtext>shuffled</mml:mtext></mml:msub></mml:math></inline-formula>, where both data and model statistics were randomly shuffled, and 0 corresponds to <inline-formula><mml:math id="inf449"><mml:msub><mml:mtext>nRMSE</mml:mtext><mml:mtext>optimal</mml:mtext></mml:msub></mml:math></inline-formula> which is the RMSE between the training data and test data (by <inline-formula><mml:math id="inf450"><mml:mrow><mml:mtext>nRMSE</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mtext>RMSE</mml:mtext><mml:mtext>ordinary</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mtext>RMSE</mml:mtext><mml:mtext>shuffled</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mtext>RMSE</mml:mtext><mml:mtext>optimal</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mtext>RMSE</mml:mtext><mml:mtext>shuffled</mml:mtext></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-10-2-3"><title>Reconstruction quality</title><p>Additionally, we assessed the reconstruction quality of the test data. Here, the log-likelihood (LLH) between the test data <inline-formula><mml:math id="inf451"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and its reconstruction <inline-formula><mml:math id="inf452"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mtext>recon</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>v</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mtext>h</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mtext>v</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was computed. Because <inline-formula><mml:math id="inf453"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the LLH is defined as<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>LLH</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mrow><mml:mtext>recon</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mrow><mml:mtext>recon</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="bold">v</mml:mtext></mml:mrow><mml:mrow><mml:mtext>recon</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The resulting LLH was normalized (nLLH) such that 0 corresponds to an independent model (i.e., fitting neural activity with <inline-formula><mml:math id="inf454"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mtext>recon</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo rspace="5.3pt" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="5.3pt">∀</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) and 1 to optimal performance (which is <inline-formula><mml:math id="inf455"><mml:mrow><mml:msub><mml:mtext>LLH</mml:mtext><mml:mtext>optimal</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), by <inline-formula><mml:math id="inf456"><mml:mrow><mml:mtext>nLLH</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mtext>LLH</mml:mtext><mml:mtext>ordinary</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mtext>LLH</mml:mtext><mml:mtext>independent</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mtext>LLH</mml:mtext><mml:mtext>independent</mml:mtext></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p></sec></sec></sec><sec id="s4-11"><title>Generalized Linear Model</title><p>We used logistic regression, a Generalized Linear Model (GLM), to quantify the reconstruction quality of a fully connected model (i.e., with neuron-to-neuron connections, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). Logistic regression makes a probabilistic binary prediction (<xref ref-type="bibr" rid="bib12">Bishop, 2006</xref>), hence allowing direct comparison to the probabilistic estimates of neural activity by the cRBM. In logistic regression, for a neuron <inline-formula><mml:math id="inf457"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at time <inline-formula><mml:math id="inf458"><mml:mi>t</mml:mi></mml:math></inline-formula>, the activity of all other neurons <inline-formula><mml:math id="inf459"><mml:mrow><mml:msub><mml:mtext>v</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at time <inline-formula><mml:math id="inf460"><mml:mi>t</mml:mi></mml:math></inline-formula> was used to predict <inline-formula><mml:math id="inf461"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mtext>w</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf462"><mml:msub><mml:mtext>w</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the estimated weight vector. This was implemented with scikit-learn (<xref ref-type="bibr" rid="bib57">Pedregosa, 2011</xref>), using <italic>L</italic><sub>2</sub> regularization. <italic>L</italic><sub>2</sub> regularization was favored over <italic>L</italic><sub>1</sub> as it typically yields higher reconstruction performance; in the related context of protein contact map prediction, <italic>L</italic><sub>2</sub>-regularized GLMs also better reconstructed contacts than <italic>L</italic><sub>1</sub>-regularized GLMs (<xref ref-type="bibr" rid="bib52">Morcos et al., 2011</xref>). The parameter <inline-formula><mml:math id="inf463"><mml:msub><mml:mi>λ</mml:mi><mml:mtext>GLM</mml:mtext></mml:msub></mml:math></inline-formula> was optimized to <inline-formula><mml:math id="inf464"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mtext>GLM</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> using cross-validation (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>). This is a computationally intensive model to compute because of the large number of regressor neurons <inline-formula><mml:math id="inf465"><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>: only <inline-formula><mml:math id="inf466"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> matrix rows could be inferred in 1 day on a 16-thread CPU desktop computer. Therefore, we performed the cross-validation of <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref> on 2% of all neurons (=1050 neurons) and computed the final distribution of <xref ref-type="fig" rid="fig2">Figure 2H</xref> on 10% of all neurons (=5252 neurons). GLMs were trained on the same train data as cRBMs, and evaluated on the same withheld test data as cRBMs (as described above).</p></sec><sec id="s4-12"><title>Variational Autoencoders</title><p>Variational Autoencoders (VAEs) were implemented in Tensorflow (2.1.10) using Keras (<xref ref-type="bibr" rid="bib17">Chollet, 2015</xref>). For the encoder, we used a two-layer perceptron with intermediate layer size equal to the dimension of the latent space, a ReLU non-linearity for the intermediate layer and no non-linearity for the conditional mean and log-variance outputs. Batch normalization was used after each dense layer of the encoder. For the decoder, we used a dense layer with a sigmoid non-linearity (no batch normalization). To obtain sparse assemblies, a L1 penalty on the weights of the decoder was added, such that the latent variables correspond to sparse neural assemblies at generation time. Models were trained by ELBO maximization using a very similar protocol as cRBMs: 200 K updates using the Adam optimizer (initial learning rate <inline-formula><mml:math id="inf467"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf468"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf469"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math></inline-formula> batch size: 100), with geometric decay schedule of learning rate after 50% of the training to a final learning rate of <inline-formula><mml:math id="inf470"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. We tested the same hyperparameter range as for cRBMs, and selected the optimal model based on the held-out ELBO values (10 Gaussian samples per data configuration were used to compute the ELBO). The optimal hyperparameters were <inline-formula><mml:math id="inf471"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf472"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, but several values were very close to optimal (<xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7A</xref>), including the value used for cRBMs (<inline-formula><mml:math id="inf473"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf474"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula>). We chose the latter for comparison to cRBM for the sake of simplicity, although we also included performance metrics of the <inline-formula><mml:math id="inf475"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> VAE model (<xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7B-H</xref>).</p></sec><sec id="s4-13"><title>Regional occupancy</title><p>We determined the anatomical region labels of each neuron by registering our recordings to the ZBrain Atlas (as described previously). This yields a matrix <inline-formula><mml:math id="inf476"><mml:mi>L</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf477"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>ZBA</mml:mtext></mml:msub><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, which elements are <inline-formula><mml:math id="inf478"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if neuron <inline-formula><mml:math id="inf479"><mml:mi>i</mml:mi></mml:math></inline-formula> is embedded in region <inline-formula><mml:math id="inf480"><mml:mi>r</mml:mi></mml:math></inline-formula> and 0 if it is not. A cRBM neural assembly of HU μ is defined by its weight vector <inline-formula><mml:math id="inf481"><mml:msub><mml:mtext>w</mml:mtext><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> (of size <inline-formula><mml:math id="inf482"><mml:mi>N</mml:mi></mml:math></inline-formula>). Because cRBMs converge to sparse solutions, most of the weight elements will be very close to 0. To determine which anatomical regions are occupied by the assembly neurons with significantly nonzero weights, we computed the dot product between the weight vector <inline-formula><mml:math id="inf483"><mml:msub><mml:mtext>w</mml:mtext><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and matrix <inline-formula><mml:math id="inf484"><mml:mi>L</mml:mi></mml:math></inline-formula>, leading to a weighted region label vector (of size <inline-formula><mml:math id="inf485"><mml:msub><mml:mi>N</mml:mi><mml:mtext>ZBA</mml:mtext></mml:msub></mml:math></inline-formula>) for each HU. The matrix of all <inline-formula><mml:math id="inf486"><mml:mi>M</mml:mi></mml:math></inline-formula> weighted region label vectors is shown in <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6A</xref> for cRBM and <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6B</xref> for PCA.</p><p>The effective number of anatomical regions that one cRBM/PCA assembly is embedded in was then calculated using the Participation Ratio (PR) of each HU/Principal Axis. PRs are used to estimate the effective number of nonzero elements in a vector, without using a threshold (<xref ref-type="bibr" rid="bib74">Tubiana and Monasson, 2017</xref>). The PR of a vector <inline-formula><mml:math id="inf487"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined by:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>PR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>PR varies from <inline-formula><mml:math id="inf488"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:math></inline-formula> when only 1 element of <inline-formula><mml:math id="inf489"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is nonzero and <inline-formula><mml:math id="inf490"><mml:mi>n</mml:mi></mml:math></inline-formula> when all elements are equal. We therefore estimated the effective number of regions by multiplying PR of the weighted region label vectors with the total number of regions <inline-formula><mml:math id="inf491"><mml:msub><mml:mi>N</mml:mi><mml:mtext>ZBA</mml:mtext></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6C</xref>.</p></sec><sec id="s4-14"><title>Time constant calculation</title><p>The dReLU potential <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> can learn to take a variety of shapes, including a double-well potential (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). HUs generally converged to this shape, giving rise to bimodal HU activity distributions (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We determined the positions of the two peaks per HU using Gaussian Mixture Models fitted with two Gaussians. The bimodality transition point was then defined as the average between the two peaks (which was approximately 0 for most HUs). To calculate the time constant of state changes between the two activity modes, we subtracted the bimodality transition point from each HU activity <inline-formula><mml:math id="inf493"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> individually. For clarity, all dynamic activity traces shown (e.g. <xref ref-type="fig" rid="fig4">Figure 4</xref>) are thus bimodality transition point subtracted. The time constant of an activity trace was then defined as the period of a (two-state) oscillation. A HU oscillation is defined as a consecutive negative and positive activity interval (because the bimodality now occurs at 0). A neuron oscillation is defined as a consecutive interspike-interval and spike-interval (which can last for multiple time steps, for example see <xref ref-type="fig" rid="fig1">Figure 1A</xref>, right panel).</p></sec><sec id="s4-15"><title>Sorting of HUs</title><p>HUs were sorted by hierarchical clustering of the Pearson correlation matrix of their dynamic activity (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Hierarchical clustering was performed using the Ward variance minimization algorithm that defines the distance between clusters (<xref ref-type="bibr" rid="bib82">Virtanen et al., 2020</xref>). This sorting of HUs (and thus assemblies) is maintained throughout the manuscript for the sake of consistency.</p></sec><sec id="s4-16"><title>Validating that the cRBM is in the compositional phase</title><p>To validate that the cRBMs converged to the compositional phase (see section - ‘Compositional restricted boltzmann machine’, compositional RBM formulation), we calculated the effective number of active HUs per data configuration (i.e., time step) <inline-formula><mml:math id="inf494"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>PR</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>h</mml:mtext><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> where PR is the participation ratio (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>), <inline-formula><mml:math id="inf495"><mml:mi>M</mml:mi></mml:math></inline-formula> the number of HUs and <inline-formula><mml:math id="inf496"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mtext>inactive</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf497"><mml:msub><mml:mi>h</mml:mi><mml:mtext>inactive</mml:mtext></mml:msub></mml:math></inline-formula> is the inactive peak as calculated with the Gaussian Mixture Models (see section - ‘Time constant calculation’), because PR assumes that inactive elements are approximately zero (<xref ref-type="bibr" rid="bib76">Tubiana et al., 2019a</xref>). A cRBM is said to be in the compositional phase if <inline-formula><mml:math id="inf498"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≪</mml:mo><mml:mrow><mml:mtext>median</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>, which is true for all cRBMs (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p></sec><sec id="s4-17"><title>Extensions of the structural connectivity matrix</title><p>The inter-region structural connectivity matrix was derived from the single cell zebrafish brain atlas (<xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref>). We used the post-publication updated data set from <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref> (timestamp: 28 October 2019). The data set consists of <inline-formula><mml:math id="inf499"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3098</mml:mn></mml:mrow></mml:math></inline-formula> neurons, each characterized by the 3D coordinates of the soma center and of its neurites; there is no distinction between dendrites and axons. The brain is subdivided into <inline-formula><mml:math id="inf500"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>72</mml:mn></mml:mrow></mml:math></inline-formula> regions and each neuron is duplicated by left/right hemisphere symmetry. We aim to estimate <inline-formula><mml:math id="inf501"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">’</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, the average strength of the connection between two neurons belonging to regions <inline-formula><mml:math id="inf502"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For each neuron <inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we determine, using region masks, the region <inline-formula><mml:math id="inf504"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where its soma is located and the cumulative length of the intersection between all its neurites and each region <inline-formula><mml:math id="inf505"><mml:mrow><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Under the assumptions that (i) the linear density of dendritic spines / axon presynaptic boutons is constant and (ii) the volumetric density of neurons is identical throughout regions, <inline-formula><mml:math id="inf506"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is proportional to the volume <inline-formula><mml:math id="inf507"><mml:msub><mml:mi>V</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> of region <inline-formula><mml:math id="inf508"><mml:mi>r</mml:mi></mml:math></inline-formula> times the average (bidirectional) connection strength between neuron <inline-formula><mml:math id="inf509"><mml:mi>n</mml:mi></mml:math></inline-formula> and any neuron of region <inline-formula><mml:math id="inf510"><mml:mi>r</mml:mi></mml:math></inline-formula>. Aggregating over all neurons and symmetrizing, we obtain the following estimator for <inline-formula><mml:math id="inf511"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">’</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">z</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf512"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if neuron <inline-formula><mml:math id="inf513"><mml:mi>n</mml:mi></mml:math></inline-formula> has its soma in region <inline-formula><mml:math id="inf514"><mml:mi>r</mml:mi></mml:math></inline-formula> and 0 if not. Using the same notations, the formula previously used in <xref ref-type="bibr" rid="bib42">Kunst et al., 2019</xref> is:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ20">Equation 20</xref> differs from <xref ref-type="disp-formula" rid="equ21">Equation 21</xref> in three aspects:</p><list list-type="order"><list-item><p>It discriminates between direct and indirect connections. Previously, a structural connection between region <inline-formula><mml:math id="inf515"><mml:mi>r</mml:mi></mml:math></inline-formula> and region <inline-formula><mml:math id="inf516"><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> was established if a neuron had neurites with either tips or its soma within both regions. This may however result in indirect connections between <inline-formula><mml:math id="inf517"><mml:mi>r</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf518"><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>, in cases where the neuron soma resides in another region <inline-formula><mml:math id="inf519"><mml:msup><mml:mi>r</mml:mi><mml:mo>′′</mml:mo></mml:msup></mml:math></inline-formula>. Here, we only account for direct connections, resulting in an overall slightly sparser connectivity matrix.</p></list-item><list-item><p>It is well-defined along the diagonal, i.e., for intra-region connections, whereas in <xref ref-type="disp-formula" rid="equ21">Equation 21</xref>, each neurite would be counted as a self-connection.</p></list-item><list-item><p>The denominator corrects for non-uniform sampling of the <italic>traced</italic> neurons throughout regions. Note that this issue only arose in the post-publication data set as non-uniform sampling was used to fill missing entries of the matrix.</p></list-item></list></sec><sec id="s4-18"><title>Specimen averaging of connectivity matrices</title><p>The number of neurons in a particular brain region can vary across recordings from different specimen. Since the entries of the connectivity matrix are expected to be more accurate for well-sampled regions, we computed the weighted average of region-to-region connections <inline-formula><mml:math id="inf520"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> as follows:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">F</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">F</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf521"><mml:msup><mml:mi>T</mml:mi><mml:mi>F</mml:mi></mml:msup></mml:math></inline-formula> is the recording length and <inline-formula><mml:math id="inf522"><mml:msubsup><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mi>F</mml:mi></mml:msubsup></mml:math></inline-formula> is the number of neurons in region <inline-formula><mml:math id="inf523"><mml:mi>r</mml:mi></mml:math></inline-formula> of fish <inline-formula><mml:math id="inf524"><mml:mi>F</mml:mi></mml:math></inline-formula> that were recorded.</p></sec><sec id="s4-19"><title>Correlation analysis of connectivity matrices</title><p>Pearson correlation was used to assess the similarity between cRBM functional connectivity matrices of different individual animals (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Spearman correlation was used to compare structural connectivity versus functional connectivity (<xref ref-type="fig" rid="fig6">Figure 6</xref>), because these two metrics do not necessarily scale linearly. All correlation analyses, and the Kilmogorov-Smirnov test of <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref>, performed on symmetric matrices excluded one off-diagonal triangle (of symmetrical values) to avoid duplicates.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Validation, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Validation, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Experiments were approved by Le Comité d’Éthique pour l’Experimentation Animale Charles Darwin C2EA-05 (02601.01).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Table of abbreviations of mapzebrain atlas region names (used for interregional connectivity analyses).</title></caption><media xlink:href="elife-83139-supp1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Properties (relevant to this study) of common-used methods for analysing neural recordings.</title><p>Abbreviations and example studies: Principal Component Analysis (PCA, <xref ref-type="bibr" rid="bib2">Ahrens et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Lopes-dos-Santos et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Marques et al., 2020</xref>), Independent Component Analysis (ICA, <xref ref-type="bibr" rid="bib45">Lopes-dos-Santos et al., 2013</xref>), k-means based algorithms (k-means, <xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Stringer et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Bartoszek et al., 2021</xref>), Non-Negative Matrix Factorization (NNMF, <xref ref-type="bibr" rid="bib53">Mu et al., 2019</xref>), Variational Auto-Encoder (VAE, <xref ref-type="bibr" rid="bib77">Tubiana et al., 2019b</xref>), Generalized Linear Model (GLM, <xref ref-type="bibr" rid="bib12">Bishop, 2006</xref>), Boltzmann Machine (BM, <xref ref-type="bibr" rid="bib64">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib49">Meshulam et al., 2017</xref>), compositional Restricted Boltzmann Machine (cRBM, this study). Question marks denote tasks that are in principle feasible but computationally expensive and/or not demonstrated.</p></caption><media xlink:href="elife-83139-supp2-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83139-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The cRBM model has been developed in Python 3.7 and is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/jertubiana/PGM">https://github.com/jertubiana/PGM</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a671999516b1e9eddb70b706752e9ed2a636ca78;origin=https://github.com/jertubiana/PGM;visit=swh:1:snp:b8c1e3bccb8d255065fee4aa971f297265b59ef9;anchor=swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe">swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe</ext-link>). An extensive example notebook that implements this model is also provided. Calcium imaging data pre-processing was performed in MATLAB (Mathworks) using previously published protocols and software (<xref ref-type="bibr" rid="bib56">Panier et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Wolf et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Migault et al., 2018</xref>; <xref ref-type="bibr" rid="bib78">Tubiana et al., 2020</xref>). The functional data recordings, the trained cRBM models and the structural and functional connectivity matrix are available at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data">https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data</ext-link>. Figures of neural assemblies or neurons (Figure 1, 3) were made using the Fishualizer, which is a 4D (space + time) data visualization software package that we have previously published (Migault et al., 2018), available at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/benglitz/fishualizer_public">https://bitbucket.org/benglitz/fishualizer_public</ext-link> . Minor updates were implemented to tailor the Fishualizer for viewing assemblies, which can be found at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/benglitz/fishualizer_public/src/assembly_viewer/">https://bitbucket.org/benglitz/fishualizer_public/src/assembly_viewer/</ext-link>. All other data analysis and visualization was performed in Python 3.7 using standard packages (numpy [<xref ref-type="bibr" rid="bib30">Harris et al., 2020</xref>], scipy [<xref ref-type="bibr" rid="bib82">Virtanen et al., 2020</xref>], scikit-learn [<xref ref-type="bibr" rid="bib57">Pedregosa, 2011</xref>], matplotlib [<xref ref-type="bibr" rid="bib36">Hunter, 2007</xref>], pandas [<xref ref-type="bibr" rid="bib48">McKinney, 2010</xref>], seaborn [<xref ref-type="bibr" rid="bib85">Waskom, 2021</xref>], h5py). The corresponding code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vdplasthijs/zf-rbm">https://github.com/vdplasthijs/zf-rbm</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f6167f75be922f03a1b1c52e7ff94a3705a69b85;origin=https://github.com/vdplasthijs/zf-rbm;visit=swh:1:snp:d6ea653d3d9faf01f8e678067f4f52716ff32f10;anchor=swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444">swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>van der Plas</surname><given-names>TL</given-names></name><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Le Goc</surname><given-names>G</given-names></name><name><surname>Migault</surname><given-names>G</given-names></name><name><surname>Kunst</surname><given-names>M</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name><name><surname>Bormuth</surname><given-names>V</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Debregeas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data from: Neural assemblies uncovered by generative modeling explain whole-brain activity statistics and reflect structural connectivity</data-title><source>GIN</source><pub-id pub-id-type="accession" xlink:href="https://gin.g-node.org/vdplasthijs/cRBM_zebrafish_spontaneous_data">cRBM_zebrafish_spontaneous_data</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>TLvdP had an Erasmus +fellowship (European Union) and acknowledges support from the Biotechnology and Biological Sciences Research Council (BBSRC, grant No. BB/M011224/1). JT acknowledges support from the Edmond J Safra Center for Bioinformatics at Tel Aviv University and from the Human Frontier Science Program (cross-disciplinary postdoctoral fellowship LT001058/2019 C). GM was funded by a PhD fellowship from the Doctoral School in Physics, Ile de France (EDPIF). GLG had a PhD fellowship from the Systems Biology Network of Sorbonne Université. BE and TLvdP. were supported by an NWO-VIDI Grant. Funding sources: European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program grant agreement number 715980. Human Frontier Science Program (RGP0060/2017). The French Research National Agency under grant No. ANR-16-CE16-0017. Dutch Institute for Scientific Research NWO (Nederlandse Organisatie voor Wetenschappelijk Onderzoek) grant No. 016.VIDI.189.052. We thank the IBPS fish facility staff for the fish maintenance, in particular Stéphane Tronche and Alex Bois. We are grateful to Carounagarane Dore for his contribution to the design of the experimental setup. We thank Misha Ahrens for providing the GCaMP lines. We are also grateful to Rémi Monasson for very fruitful discussions and his comments on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abramowitz</surname><given-names>M</given-names></name><name><surname>Stegun</surname><given-names>IA</given-names></name><name><surname>Romer</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Handbook of mathematical functions with formulas, graphs, and mathematical tables</article-title><source>American Journal of Physics</source><volume>56</volume><elocation-id>958</elocation-id><pub-id pub-id-type="doi">10.1119/1.15378</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title><source>Nature</source><volume>485</volume><fpage>471</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1038/nature11057</pub-id><pub-id pub-id-type="pmid">22622571</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title><source>Nature Methods</source><volume>10</volume><fpage>413</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2434</pub-id><pub-id pub-id-type="pmid">23524393</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Large-scale imaging in small brains</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>78</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.01.007</pub-id><pub-id pub-id-type="pmid">25636154</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Zebrafish neuroscience: using artificial neural networks to help understand brains</article-title><source>Current Biology</source><volume>29</volume><fpage>R1138</fpage><lpage>R1140</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.09.039</pub-id><pub-id pub-id-type="pmid">31689401</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bargmann</surname><given-names>CI</given-names></name><name><surname>Marder</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From the connectome to brain function</article-title><source>Nature Methods</source><volume>10</volume><fpage>483</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2451</pub-id><pub-id pub-id-type="pmid">23866325</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartoszek</surname><given-names>EM</given-names></name><name><surname>Ostenrath</surname><given-names>AM</given-names></name><name><surname>Jetti</surname><given-names>SK</given-names></name><name><surname>Serneels</surname><given-names>B</given-names></name><name><surname>Mutlu</surname><given-names>AK</given-names></name><name><surname>Chau</surname><given-names>KTP</given-names></name><name><surname>Yaksi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Ongoing habenular activity is driven by forebrain networks and modulated by olfactory stimuli</article-title><source>Current Biology</source><volume>31</volume><fpage>3861</fpage><lpage>3874</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.08.021</pub-id><pub-id pub-id-type="pmid">34416179</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Network neuroscience</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>353</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1038/nn.4502</pub-id><pub-id pub-id-type="pmid">28230844</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bathellier</surname><given-names>B</given-names></name><name><surname>Ushakova</surname><given-names>L</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Discrete neocortical dynamics predict behavioral categorization of sounds</article-title><source>Neuron</source><volume>76</volume><fpage>435</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.07.008</pub-id><pub-id pub-id-type="pmid">23083744</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beretta</surname><given-names>CA</given-names></name><name><surname>Dross</surname><given-names>N</given-names></name><name><surname>Guiterrez-Triana</surname><given-names>JA</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Carl</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Habenula circuit development: past, present, and future</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>51</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00051</pub-id><pub-id pub-id-type="pmid">22536170</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Biophysics: Searching for Principles</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-name>springer</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bravi</surname><given-names>B</given-names></name><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Walczak</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>RBM-mhc: a semi-supervised machine-learning method for sample-specific prediction of antigen presentation by hla-i alleles</article-title><source>Cell Systems</source><volume>12</volume><fpage>195</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2020.11.005</pub-id><pub-id pub-id-type="pmid">33338400</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural SYNTAX: cell assemblies, synapsembles, and readers</article-title><source>Neuron</source><volume>68</volume><fpage>362</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.023</pub-id><pub-id pub-id-type="pmid">21040841</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Kuan</surname><given-names>AT</given-names></name><name><surname>Nikitchenko</surname><given-names>M</given-names></name><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Chen</surname><given-names>AB</given-names></name><name><surname>Gavornik</surname><given-names>JP</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-Wide organization of neuronal activity and convergent sensorimotor transformations in larval zebrafish</article-title><source>Neuron</source><volume>100</volume><fpage>876</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.042</pub-id><pub-id pub-id-type="pmid">30473013</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Randi</surname><given-names>F</given-names></name><name><surname>Leifer</surname><given-names>AM</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Searching for collective behavior in a small brain</article-title><source>Physical Review. E</source><volume>99</volume><elocation-id>052418</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.99.052418</pub-id><pub-id pub-id-type="pmid">31212571</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Keras</data-title><publisher-name>keras.io</publisher-name><ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Systematic errors in connectivity inferred from activity in strongly recurrent networks</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1286</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0699-2</pub-id><pub-id pub-id-type="pmid">32895567</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diana</surname><given-names>G</given-names></name><name><surname>Sainsbury</surname><given-names>TTJ</given-names></name><name><surname>Meyer</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian inference of neuronal assemblies</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007481</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007481</pub-id><pub-id pub-id-type="pmid">31671090</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>TW</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Narayan</surname><given-names>S</given-names></name><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Naumann</surname><given-names>EA</given-names></name><name><surname>Yang</surname><given-names>C-T</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain-Wide mapping of neural activity controlling zebrafish exploratory locomotion</article-title><source>eLife</source><volume>5</volume><elocation-id>e12741</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12741</pub-id><pub-id pub-id-type="pmid">27003593</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Barlow versus Hebb: when is it time to abandon the notion of feature detectors and adopt the cell assembly as the unit of cognition?</article-title><source>Neuroscience Letters</source><volume>680</volume><fpage>88</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2017.04.006</pub-id><pub-id pub-id-type="pmid">28389238</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrari</surname><given-names>U</given-names></name><name><surname>Obuchi</surname><given-names>T</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Random versus maximum entropy models of neural population activity</article-title><source>Physical Review. E</source><volume>95</volume><elocation-id>042321</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.95.042321</pub-id><pub-id pub-id-type="pmid">28505742</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fischer &amp; Igel</surname><given-names>A</given-names></name><name><surname>Igel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Iberoamerican Congress on Pattern Recognition</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-33275-3</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sparse inverse covariance estimation with the graphical LASSO</article-title><source>Biostatistics</source><volume>9</volume><fpage>432</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxm045</pub-id><pub-id pub-id-type="pmid">18079126</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardella</surname><given-names>C</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modeling the correlated activity of neural populations: a review</article-title><source>Neural Computation</source><volume>31</volume><fpage>233</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01154</pub-id><pub-id pub-id-type="pmid">30576613</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gardella et al</surname><given-names>C</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name></person-group><article-title>Restricted boltzmann machines provide an accurate metric for retinal responses to visual stimuli</article-title><conf-name>5th International Conference on Learning Representations, ICLR 2017</conf-name><year iso-8601-date="2017">2017</year><fpage>24</fpage><lpage>26</lpage><ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=Sk1OOnNFx">https://openreview.net/forum?id=Sk1OOnNFx</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstein</surname><given-names>GL</given-names></name><name><surname>Bedenbaugh</surname><given-names>P</given-names></name><name><surname>Aertsen</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Neuronal assemblies</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>36</volume><fpage>4</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1109/10.16444</pub-id><pub-id pub-id-type="pmid">2646211</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural signatures of cell assembly organization</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>399</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1038/nrn1669</pub-id><pub-id pub-id-type="pmid">15861182</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cell assemblies of the superficial cortex</article-title><source>Neuron</source><volume>76</volume><fpage>263</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.007</pub-id><pub-id pub-id-type="pmid">23083730</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with numpy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><source>The Organization of Behavior: A Neuropsychological Theory</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The mutual inspirations of machine learning and neuroscience</article-title><source>Neuron</source><volume>86</volume><fpage>25</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.031</pub-id><pub-id pub-id-type="pmid">25856482</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Training products of experts by minimizing contrastive divergence</article-title><source>Neural Computation</source><volume>14</volume><fpage>1771</fpage><lpage>1800</lpage><pub-id pub-id-type="doi">10.1162/089976602760128018</pub-id><pub-id pub-id-type="pmid">12180402</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G E</given-names></name><name><surname>Salakhutdinov</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reducing the dimensionality of data with neural networks</article-title><source>Science</source><volume>313</volume><fpage>504</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1126/science.1127647</pub-id><pub-id pub-id-type="pmid">16873662</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Neural Networks: Tricks of the Trade</source><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: a 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaynes</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Information theory and statistical mechanics</article-title><source>Physical Review</source><volume>106</volume><fpage>620</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1103/PhysRev.106.620</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Afraz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Navigating the neural space in search of the neural code</article-title><source>Neuron</source><volume>93</volume><fpage>1003</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.019</pub-id><pub-id pub-id-type="pmid">28279349</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>j</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopell</surname><given-names>NJ</given-names></name><name><surname>Gritton</surname><given-names>HJ</given-names></name><name><surname>Whittington</surname><given-names>MA</given-names></name><name><surname>Kramer</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Beyond the connectome: the dynome</article-title><source>Neuron</source><volume>83</volume><fpage>1319</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.016</pub-id><pub-id pub-id-type="pmid">25233314</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Köster</surname><given-names>U</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>CM</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Modeling higher-order correlations within cortical microcolumns</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003684</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003684</pub-id><pub-id pub-id-type="pmid">24991969</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kunst</surname><given-names>M</given-names></name><name><surname>Laurell</surname><given-names>E</given-names></name><name><surname>Mokayes</surname><given-names>N</given-names></name><name><surname>Kramer</surname><given-names>A</given-names></name><name><surname>Kubo</surname><given-names>F</given-names></name><name><surname>Fernandes</surname><given-names>AM</given-names></name><name><surname>Förster</surname><given-names>D</given-names></name><name><surname>Dal Maschio</surname><given-names>M</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A cellular-resolution atlas of the larval zebrafish brain</article-title><source>Neuron</source><volume>103</volume><fpage>21</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.034</pub-id><pub-id pub-id-type="pmid">31147152</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lam</surname><given-names>SK</given-names></name><name><surname>Pitrou</surname><given-names>A</given-names></name><name><surname>Seibert</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Numba: A llvm-based python jit compiler</article-title><conf-name>Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</conf-name><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>L</given-names></name><name><surname>Osan</surname><given-names>R</given-names></name><name><surname>Shoham</surname><given-names>S</given-names></name><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Zuo</surname><given-names>W</given-names></name><name><surname>Tsien</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Identification of network-level coding units for real-time representation of episodic experiences in the hippocampus</article-title><source>PNAS</source><volume>102</volume><fpage>6125</fpage><lpage>6130</lpage><pub-id pub-id-type="doi">10.1073/pnas.0408233102</pub-id><pub-id pub-id-type="pmid">15833817</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes-dos-Santos</surname><given-names>V</given-names></name><name><surname>Ribeiro</surname><given-names>S</given-names></name><name><surname>Tort</surname><given-names>ABL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Detecting cell assemblies in large neuronal populations</article-title><source>Journal of Neuroscience Methods</source><volume>220</volume><fpage>149</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.04.010</pub-id><pub-id pub-id-type="pmid">23639919</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>L-H</given-names></name><name><surname>Grove</surname><given-names>CL</given-names></name><name><surname>Baker</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Development of oculomotor circuitry independent of hox3 genes</article-title><source>Nature Communications</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/ncomms5221</pub-id><pub-id pub-id-type="pmid">24964400</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname><given-names>JC</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Schaak</surname><given-names>D</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Internal state dynamics shape brainwide activity and foraging behaviour</article-title><source>Nature</source><volume>577</volume><fpage>239</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1858-z</pub-id><pub-id pub-id-type="pmid">31853063</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Data Structures for Statistical Computing in Python</article-title><conf-name>Python in Science Conference</conf-name><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meshulam</surname><given-names>L</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Collective behavior of place and non-place neurons in the hippocampal network</article-title><source>Neuron</source><volume>96</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.10.027</pub-id><pub-id pub-id-type="pmid">29154129</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Migault</surname><given-names>G</given-names></name><name><surname>van der Plas</surname><given-names>TL</given-names></name><name><surname>Trentesaux</surname><given-names>H</given-names></name><name><surname>Panier</surname><given-names>T</given-names></name><name><surname>Candelier</surname><given-names>R</given-names></name><name><surname>Proville</surname><given-names>R</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Debrégeas</surname><given-names>G</given-names></name><name><surname>Bormuth</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Whole-Brain calcium imaging during physiological vestibular stimulation in larval zebrafish</article-title><source>Current Biology</source><volume>28</volume><fpage>3723</fpage><lpage>3735</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.017</pub-id><pub-id pub-id-type="pmid">30449666</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mölter</surname><given-names>J</given-names></name><name><surname>Avitan</surname><given-names>L</given-names></name><name><surname>Goodhill</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Detecting neural assemblies in calcium imaging data</article-title><source>BMC Biology</source><volume>16</volume><elocation-id>143</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-018-0606-4</pub-id><pub-id pub-id-type="pmid">30486809</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Pagnani</surname><given-names>A</given-names></name><name><surname>Lunt</surname><given-names>B</given-names></name><name><surname>Bertolino</surname><given-names>A</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Zecchina</surname><given-names>R</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title><source>PNAS</source><volume>108</volume><fpage>E1293</fpage><lpage>E1301</lpage><pub-id pub-id-type="doi">10.1073/pnas.1111471108</pub-id><pub-id pub-id-type="pmid">22106262</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Bennett</surname><given-names>DV</given-names></name><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Narayan</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>C-T</given-names></name><name><surname>Tanimoto</surname><given-names>M</given-names></name><name><surname>Mensh</surname><given-names>BD</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Glia accumulate evidence that actions are futile and suppress unsuccessful behavior</article-title><source>Cell</source><volume>178</volume><fpage>27</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.05.050</pub-id><pub-id pub-id-type="pmid">31230713</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narayanan</surname><given-names>NS</given-names></name><name><surname>Kimchi</surname><given-names>EY</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Redundancy and synergy of neuronal ensembles in motor cortex</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>4207</fpage><lpage>4216</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4697-04.2005</pub-id><pub-id pub-id-type="pmid">15858046</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palm</surname><given-names>G</given-names></name><name><surname>Knoblauch</surname><given-names>A</given-names></name><name><surname>Hauser</surname><given-names>F</given-names></name><name><surname>Schüz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cell assemblies in the cerebral cortex</article-title><source>Biol Cybern</source><volume>108</volume><fpage>559</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1007/s00422-014-0596-4</pub-id><pub-id pub-id-type="pmid">24692024</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panier</surname><given-names>T</given-names></name><name><surname>Romano</surname><given-names>SA</given-names></name><name><surname>Olive</surname><given-names>R</given-names></name><name><surname>Pietri</surname><given-names>T</given-names></name><name><surname>Sumbre</surname><given-names>G</given-names></name><name><surname>Candelier</surname><given-names>R</given-names></name><name><surname>Debrégeas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast functional imaging of multiple brain regions in intact zebrafish larvae using selective plane illumination microscopy</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>65</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00065</pub-id><pub-id pub-id-type="pmid">23576959</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posani</surname><given-names>L</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integration and multiplexing of positional and contextual information by the hippocampal network</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006320</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006320</pub-id><pub-id pub-id-type="pmid">30106966</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quirin</surname><given-names>S</given-names></name><name><surname>Vladimirov</surname><given-names>N</given-names></name><name><surname>Yang</surname><given-names>CT</given-names></name><name><surname>Peterka</surname><given-names>DS</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name><name><surname>B. Ahrens</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Calcium imaging of neural circuits with extended depth-of-field light-sheet microscopy</article-title><source>Opt Lett</source><volume>41</volume><elocation-id>855</elocation-id><pub-id pub-id-type="doi">10.1364/OL.41.000855</pub-id><pub-id pub-id-type="pmid">26974063</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Wee</surname><given-names>CL</given-names></name><name><surname>Naumann</surname><given-names>EA</given-names></name><name><surname>Nnaemeka</surname><given-names>O</given-names></name><name><surname>Schoppik</surname><given-names>D</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name><name><surname>Lacoste</surname><given-names>AMB</given-names></name><name><surname>Riegler</surname><given-names>C</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Whole-Brain activity mapping onto a zebrafish brain atlas</article-title><source>Nature Methods</source><volume>12</volume><fpage>1039</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3581</pub-id><pub-id pub-id-type="pmid">26778924</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravikumar</surname><given-names>P</given-names></name><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Lafferty</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>High-dimensional ising model selection using ℓ1-regularized logistic regression</article-title><source>The Annals of Statistics</source><volume>38</volume><fpage>1287</fpage><lpage>1319</lpage><pub-id pub-id-type="doi">10.1214/09-AOS691</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romano</surname><given-names>SA</given-names></name><name><surname>Pietri</surname><given-names>T</given-names></name><name><surname>Pérez-Schuster</surname><given-names>V</given-names></name><name><surname>Jouary</surname><given-names>A</given-names></name><name><surname>Haudrechy</surname><given-names>M</given-names></name><name><surname>Sumbre</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spontaneous neuronal network dynamics reveal circuit’s functional adaptations for behavior</article-title><source>Neuron</source><volume>85</volume><fpage>1070</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.01.027</pub-id><pub-id pub-id-type="pmid">25704948</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Roussel</surname><given-names>C</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Barriers and Dynamical Paths in Alternating Gibbs Sampling of Restricted Boltzmann Machines</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2107.06013">https://arxiv.org/abs/2107.06013</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title><source>Nature</source><volume>440</volume><fpage>1007</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1038/nature04701</pub-id><pub-id pub-id-type="pmid">16625187</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The structure of large-scale synchronized firing in primate retina</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>5022</fpage><lpage>5031</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5187-08.2009</pub-id><pub-id pub-id-type="pmid">19369571</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smolensky</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1986">1986</year><chapter-title>Information processing in dynamical systems: foundations of harmony theory</chapter-title><person-group person-group-type="editor"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>McClellan</surname><given-names>JL</given-names></name></person-group><source>Parallel Distributed Processing Chapter 6</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>194</fpage><lpage>281</lpage></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Reigl</surname><given-names>M</given-names></name><name><surname>Nelson</surname><given-names>S</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e68</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030068</pub-id><pub-id pub-id-type="pmid">15737062</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavoni</surname><given-names>G</given-names></name><name><surname>Ferrari</surname><given-names>U</given-names></name><name><surname>Battaglia</surname><given-names>FP</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Functional coupling networks inferred from prefrontal cortex activity show experience-related effective plasticity</article-title><source>Network Neuroscience</source><volume>1</volume><fpage>275</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1162/NETN_a_00014</pub-id><pub-id pub-id-type="pmid">29855621</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tieleman</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Training restricted Boltzmann machines using approximations to the likelihood gradient</article-title><conf-name>25th international conference</conf-name><fpage>1064</fpage><lpage>1071</lpage><pub-id pub-id-type="doi">10.1145/1390156.1390290</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tieleman</surname><given-names>T</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Using fast weights to improve persistent contrastive divergence</article-title><conf-name>Proceedings of the 26th annual international conference on machine learning</conf-name><fpage>1033</fpage><lpage>1040</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Triplett</surname><given-names>MA</given-names></name><name><surname>Avitan</surname><given-names>L</given-names></name><name><surname>Goodhill</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of spontaneous assembly activity in developing neural networks without afferent input</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006421</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006421</pub-id><pub-id pub-id-type="pmid">30265665</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Triplett</surname><given-names>MA</given-names></name><name><surname>Pujic</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>B</given-names></name><name><surname>Avitan</surname><given-names>L</given-names></name><name><surname>Goodhill</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Model-based decoupling of evoked and spontaneous neural activity in calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008330</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008330</pub-id><pub-id pub-id-type="pmid">33253161</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of compositional representations in restricted Boltzmann machines</article-title><source>Physical Review Letters</source><volume>118</volume><elocation-id>138301</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.118.138301</pub-id><pub-id pub-id-type="pmid">28409983</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Restricted Boltzmann Machines: From Compositional Representations to Protein Sequence Analysis</source><publisher-loc>Paris, France</publisher-loc><publisher-name>PSL Research University</publisher-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Learning compositional representations of interacting systems with restricted boltzmann machines: comparative study of lattice proteins</article-title><source>Neural Computation</source><volume>31</volume><fpage>1671</fpage><lpage>1717</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01210</pub-id><pub-id pub-id-type="pmid">31260391</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Learning protein constitutive motifs from sequence data</article-title><source>eLife</source><volume>8</volume><elocation-id>e39397</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.39397</pub-id><pub-id pub-id-type="pmid">30857591</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Wolf</surname><given-names>S</given-names></name><name><surname>Panier</surname><given-names>T</given-names></name><name><surname>Debregeas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Blind deconvolution for spike inference from fluorescence recordings</article-title><source>Journal of Neuroscience Methods</source><volume>342</volume><elocation-id>108763</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108763</pub-id><pub-id pub-id-type="pmid">32479972</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>van der Plas</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Probabilistic graphical models (PGM</data-title><version designator="swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe">swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a671999516b1e9eddb70b706752e9ed2a636ca78;origin=https://github.com/jertubiana/PGM;visit=swh:1:snp:b8c1e3bccb8d255065fee4aa971f297265b59ef9;anchor=swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe">https://archive.softwareheritage.org/swh:1:dir:a671999516b1e9eddb70b706752e9ed2a636ca78;origin=https://github.com/jertubiana/PGM;visit=swh:1:snp:b8c1e3bccb8d255065fee4aa971f297265b59ef9;anchor=swh:1:rev:caf1d9fc545120f7f1bc1420135f980d5fd6c1fe</ext-link></element-citation></ref><ref id="bib80"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>van der Plas</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Zf-rbm</data-title><version designator="swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444">swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f6167f75be922f03a1b1c52e7ff94a3705a69b85;origin=https://github.com/vdplasthijs/zf-rbm;visit=swh:1:snp:d6ea653d3d9faf01f8e678067f4f52716ff32f10;anchor=swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444">https://archive.softwareheritage.org/swh:1:dir:f6167f75be922f03a1b1c52e7ff94a3705a69b85;origin=https://github.com/vdplasthijs/zf-rbm;visit=swh:1:snp:d6ea653d3d9faf01f8e678067f4f52716ff32f10;anchor=swh:1:rev:b5df4e37434c0b18120485b8d856596db0b92444</ext-link></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanwalleghem</surname><given-names>GC</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Scott</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integrative whole-brain neuroscience in larval zebrafish</article-title><source>Current Opinion in Neurobiology</source><volume>50</volume><fpage>136</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.02.004</pub-id><pub-id pub-id-type="pmid">29486425</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vladimirov</surname><given-names>N</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Kawashima</surname><given-names>T</given-names></name><name><surname>Bennett</surname><given-names>DV</given-names></name><name><surname>Yang</surname><given-names>C-T</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Light-Sheet functional imaging in fictively behaving zebrafish</article-title><source>Nature Methods</source><volume>11</volume><fpage>883</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3040</pub-id><pub-id pub-id-type="pmid">25068735</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volpi</surname><given-names>R</given-names></name><name><surname>Zanotto</surname><given-names>M</given-names></name><name><surname>Maccione</surname><given-names>A</given-names></name><name><surname>Di Marco</surname><given-names>S</given-names></name><name><surname>Berdondini</surname><given-names>L</given-names></name><name><surname>Sona</surname><given-names>D</given-names></name><name><surname>Murino</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling a population of retinal ganglion cells with restricted boltzmann machines</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>16549</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-73691-z</pub-id><pub-id pub-id-type="pmid">33024225</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Seaborn: statistical data visualization</article-title><source>Journal of Open Source Software</source><volume>6</volume><elocation-id>3021</elocation-id><pub-id pub-id-type="doi">10.21105/joss.03021</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>JG</given-names></name><name><surname>Southgate</surname><given-names>E</given-names></name><name><surname>Thomson</surname><given-names>JN</given-names></name><name><surname>Brenner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The structure of the nervous system of the nematode <italic>Caenorhabditis elegans</italic></article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>314</volume><fpage>1</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1098/rstb.1986.0056</pub-id><pub-id pub-id-type="pmid">22462104</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>S</given-names></name><name><surname>Supatto</surname><given-names>W</given-names></name><name><surname>Debrégeas</surname><given-names>G</given-names></name><name><surname>Mahou</surname><given-names>P</given-names></name><name><surname>Kruglik</surname><given-names>SG</given-names></name><name><surname>Sintes</surname><given-names>J-M</given-names></name><name><surname>Beaurepaire</surname><given-names>E</given-names></name><name><surname>Candelier</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Whole-Brain functional imaging with two-photon light-sheet microscopy</article-title><source>Nature Methods</source><volume>12</volume><fpage>379</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3371</pub-id><pub-id pub-id-type="pmid">25924070</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>S</given-names></name><name><surname>Dubreuil</surname><given-names>AM</given-names></name><name><surname>Bertoni</surname><given-names>T</given-names></name><name><surname>Böhm</surname><given-names>UL</given-names></name><name><surname>Bormuth</surname><given-names>V</given-names></name><name><surname>Candelier</surname><given-names>R</given-names></name><name><surname>Karpenko</surname><given-names>S</given-names></name><name><surname>Hildebrand</surname><given-names>DGC</given-names></name><name><surname>Bianco</surname><given-names>IH</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name><name><surname>Debrégeas</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sensorimotor computation underlying phototaxis in zebrafish</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>651</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00310-3</pub-id><pub-id pub-id-type="pmid">28935857</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83139.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>Large scale recordings, sometimes involving 10s of thousands of neurons, are becoming increasingly common. Making sense of these recordings, however, is not easy. This paper introduces a new method, the compositional Restricted Boltzmann Machine, that overcomes this problem -- it can find structure in data, including both &quot;cell assemblies&quot; and structural connectivity, without inordinate computing resources (data from 40,000 neurons recorded from zebrafish can be analyzed in less than a day). This is a valuable contribution, both to those interested in data analysis, and to those interested in zebrafish.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83139.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>[Editors' note: this paper was reviewed by <ext-link ext-link-type="uri" xlink:href="https://www.reviewcommons.org/">Review Commons</ext-link>.]</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83139.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Evidence, reproducibility and clarity (Required)):</p><p>Summary:</p><p>In the present manuscript, van der Plas et al. compellingly illustrated a novel technique for engendering a wholebrain functional connectivity map from single-unit activities sampled through a large-scale neuroimaging technique. With some clever tweaks to the restricted Boltzmann Machine, the cRBM network is able to learn a low-dimensional representation of population activities, without relying on constrained priors found in some traditional methods. Notably, using some 200 hidden layer neurons, the employed model was able to capture the dynamics of over 40,000 simultaneously imaged neurons with a high degree of accuracy. The extracted features both illustrate the anatomical topography/connectivities and capture the temporal dynamics in the evolution of brain states. The illustrated technique has the potential for wide-spread applications spanning diverse recording techniques and animal species. Furthermore, the prospectives of modeling whole-brain network dynamics in 'neural trajectory' space and of generating artificial data in silico make for very enticing reasons to adopt cRBM.</p><p>Major comments:</p><p>1. Line 164. The authors claim that conventional methods &quot;such as k-means, PCA and non-negative matrix factorization&quot; cannot be quantitatively assessed for quality on the basis that they are unable to generate new artificial data. Though partly true, in most neuroscience applications, this is hardly cause for concern. Most dimensionality reduction methods (with few exceptions such as t-sne) allow new data points to be embedded into the reduced space. As such, quality of encoding can be assessed by cross-validation much in the same way as the authors described, and quantified using traditional metrics such as percentage explained variance. The authors should directly compare the performance of their proposed model against that of NNMF and variational auto-encoders. Doing so would offer a more compelling argument for the advantage of their proposed method over more widely-used methods in neuroscience applications. Furthermore, a direct comparison with rastermap, developed by Stringer lab at Janelia (https://github.com/MouseLand/rastermap), would be a nice addition. This method presents itself as a direct competitor to cRBM. Additionally, the use of GLM doesn't do complete justice to the comparison point used, since a smaller fraction of data were used for calculating performance using GLM, understandably due to its computationally intensive nature.</p></disp-quote><p>We thank the reviewer for the comment, and certainly agree that there are multiple methods for unsupervised feature extraction from data and that they can be validated for encoding quality by cross-validation. Below, we follow the reviewers suggestion to directly compare with VAEs, but argue first that a comparison with NNMF and rastermap is not appropriate. Specifically, we would like to stress that reconstructing through a low-dimensional, continuous bottleneck is a different (and arguably, easier) task, than generating whole distributions as the cRBM does. Reconstruction delineates the manifold of <italic>possible</italic> configurations, whereas generative modeling must <italic>weigh</italic> such configurations adequately. Moreover, none of the methodologies mentioned can perform the same tasks as cRBMs. For instance, NNMF learns localized assemblies, but cannot faithfully model inhibitory connections since, by definition, only nonnegative weights are learnt. Also, the connection between the learnt assemblies and the underlying connectivity is unclear. Similarly, rastermap is an algorithm for robustly i) sorting neurons along a set number of dimensions (typically 1 or 2) such that neighboring neurons are highly correlated, and ii) performing dimensionality reduction by clustering along these dimensions. Because Rastermap uses kmeans as the basis for grouping together neurons, it does not quantify connections between neurons and assemblies, nor assign neurons to multiple assemblies. Moreover, it is not a generative model, and thus cannot predict perturbation experiments, infer connectivities or assign probabilities to configurations. Therefore, we do not believe that NNMF or Rastermap would be a suitable alternative for cRBM in our study. We nonetheless appreciate the reviewer’s suggestions and agree that we should motivate more clearly why these methods are not applicable for our purposes. Therefore, to emphasize the relative merit of cRBM with respect to other unsupervised algorithms, we now provide a table (Supplementary Table 2) that lists their specific characteristics. We stress that we do not claim that cRBM are consistently better than these classical tools for dimensionality reduction, but focus only on the properties relevant to our study.</p><p>Regarding VAEs, we agree that these are close competitors of cRBMs, as they also jointly learn a representation and distribution of the data,. In <italic>Tubiana et al. Neural Computation 2019,</italic> we previously compared sparse VAEs with cRBMs for protein sequence modeling, and found that RBMs consistently outperformed VAEs. In the revised manuscript, we repeated the comparison with VAEs for Zebrafish neural recordings, and reached similar conclusions. Specifically, we found that for sparse linear VAEs trained using a similar protocol as cRBMs (ELBO loss minimization using ADAM optimizer, sparsity regularization, hyperparameter search using held-out validation set): i) the generated samples failed to replicate the second-order statistics of the data ii) the VAE could not reconstruct accurately neural spikes from the latent representation and iii) the majority (~60%) of the latent variables were completely disconnected from the neurons, and the remaining ones had highly variable size. This analysis shows that cRBMs consistently outperform VAE in terms of both interpretability and performance. The comparison between cRBM and VAE performance is now provided in the Manuscript (Section 7.10.4), and illustrated in Supplementary Figure S7 (shown below).</p><p>Results 2.2, last paragraph:</p><p>“We next asked whether sparsity alone was sufficient for a generative model to accurately recapitulate the neural recording statistics. To address this question, we trained sparse linear Variational Autoencoders (VAEs) using the same parameter-optimization protocol (Figure S7A). Like cRBMs, linear VAEs are generative models that learn a latent representation of a dataset (Tubiana et al., 2019a). We observed that VAEs were not able to replicate the second-order statistics, and therefore were not able to reconstruct neural activity from latent representation (Figure S7B-D), even though they also obtained sparse representations (Figure S7E, F).”</p><p>Discussion, 3rd paragraph</p><p>“In this study we repeated this comparison with sparse linear VAEs, and reached similar conclusions: VAEs trained using the same protocol as cRBMs failed to reproduce second-order data statistics and to reconstruct neural activity via the latent layer, while the learnt assemblies were of substantially lower quality (indicated by a large fraction of disconnected HUs, as well as a highly variable assembly size) (Figure S7).”</p><p>As for GLM, it is true that the comparison involved subsampling of the neurons (10%, i.e. 5252 neurons, due to the very high computational cost of GLM, where we could estimate the connectivity of ~1000 neurons per day). This was already denoted in the relevant figure caption, as the reviewer has seen, but we have now also clarified this point in Methods 7.10.3.</p><p>We think that, because this is a large and randomly selected sample of neurons, these 5252 neurons represent the full data set, and the GLM distribution of reconstruction likelihood (Figure 2H) will not change qualitatively if a larger sample would be used.</p><p>In contrast to GLM, optimized cRBM models converged learning of the entire data set in half a day, emphasizing their ability to handle very large datasets, such as the presently used zebrafish whole-brain recordings, which is crucial for any model to be applied in practice.</p><disp-quote content-type="editor-comment"><p>2. Line 26. The authors describe their model architecture as a formalization of cell assemblies. Cell assemblies, as originally formulated by Hebb, pertains to a set of neurons whose connectivity matrix is neither necessarily complete nor symmetric. Critically, in the physiological brain, the interactions between the individual neurons that are part of an assembly would occur over multiple orders of dependencies. In a restricted Boltzmann machine, neurons are not connected within the same layer. Instead, visible layer neurons are grouped into &quot;assemblies&quot; indirectly via a shared connection with a hidden layer neuron. Furthermore, a symmetrical weight matrix connects the bipartite graph, where no recurrent connectivities are made. As such, the proposed model still only elaborates symmetric connections pertaining to first-order interactions (as illustrated in Figure 4C). Such a network may not be likened with the concept of cell assemblies. The authors should refrain from detailing this analogy (of which there are multiple instances of throughout the text). It is true that many authors today refer to cell assemblies as any set of temporally-correlated neurons. However, saying &quot;something could be a cell assembly&quot; is not the same as saying &quot;something is a cell assembly&quot;. How about sticking with cRBM-based cell assemblies (as used in section 2.3) and defining it beforehand?</p></disp-quote><p>We thank the reviewer for this excellent question. We agree that there is, in general, a discrepancy between computationally-defined assemblies and conceptual/neurophysiological definition of cell assemblies. We have added a clarification in Results 2.1 to clarify the use of this term when it first occurs in Results. However, we still believe that our work contributes to narrowing the gap. Indeed, our RBM-defined assemblies are i) localized, ii) overlapping, iii) rooted in connectivity patterns (both excitatory and inhibitory), and iv) cannot be reduced to a simple partitioning of the brain with full and uniform connectivity within and between partitions. This is unlike previous work based on clustering (no overlaps or heterogeneous weights), NNMF (no inhibition) or correlation network analysis (no low-dimensional representation).</p><p>Regarding the specific comments pointed here, we stress that:</p><p>– Effective interactions between neurons are not purely pairwise (“First order”), due to the usage of the non-quadratic potential. (see Equation 12-13). If the reviewer means by “First-order” interactions the lack of hierarchical organization, we agree, to some extent: in the current formulation, correlations between assemblies are mediated by overlaps between their weights. Fully hierarchical organization, e.g. by using Deep Boltzmann Machines or pairwise connections within the hidden layer is an interesting future direction, but on the other hand may make it hard to clearly identify assemblies as they might be spread out over multiple layers</p><p>– Neurons that participate in a given assembly (as defined by a specific hidden unit) are not all connected with one another with equal strength. Indeed, these neurons may participate in other assemblies, resulting in heterogeneity of connections (see Equation. 15-17) and interactions between assemblies.</p><p>– We acknowledge that the constraint of symmetrical connections is a core limitation of our method. Arguably, asymmetric connections are critical for predicting temporal evolution but less important for inferring a steady-state distribution from data, as we do here.</p><p>In the revised submission, we added a new paragraph in the Discussion section (lines 350-356) in which these limitations are discussed, including the imposed symmetry of the connections and the lack of hierarchical structures, copied below. We trust that this addresses the reviewer’s criticism:</p><p>In sum, cRBM-inferred cell assemblies display many properties that one expects from physiological cell assemblies: they are anatomically localized, can overlap, encompass functionally identified neuronal circuits and underpin the collective neural dynamics (Harris, 2005, 2012; Eichenbaum, 2018). Yet, the cRBM bipartite architecture lacks many of the traits of neurophysiological circuits. In particular, cRBMs lack direct neuron-to-neuron connections, asymmetry in the connectivity weights and a hierarchical organization of functional dependencies beyond one hidden layer. Therefore, to what extent cRBM-inferred assemblies identify to neurophysiological cell assemblies, as postulated by Hebb (1949) and others, remains an open question.</p><disp-quote content-type="editor-comment"><p>3. I would strongly recommend adding a paragraph discussing the limitation of using the cRBM, things future researchers need to keep in mind before using this method. One such recommendation is moving the runtimerelated discussion for cRBM, i.e. 8-12 hrs using 16 CPU from Methods to Discussion, since it's relevant for an algorithm like this. Additionally, a statement mentioning how this runtime will increase with the length of recordings and/or with the number of neurons might be helpful. What if the recordings were an hour-long rather than 25mins. This would help readers decide if they can easily use a method like this.</p></disp-quote><p>We thank the reviewer for the suggestion, and agree that it is important to cover the computational cost in the main text. Regarding the runtime for longer recordings, the general rule of thumb is that the model requires a fixed number of gradient updates to converge (20-80k depending on the data dimensionality) rather than a fixed number of epochs. Thus, runtime should not depend on recording length, as the number of epochs can be reduced for longer recordings. While we did not verify this rule for neural recordings, this is what we previously observed when modeling protein/DNA sequence data sets, whose size range from few hundreds to hundreds of thousands of samples (Tubiana et al., 2019, <italic>eLife</italic>; Tubiana et al. 2019, Neural Computation; Bravi et al. Cell Systems 2021; Bravi et al. PLOS CB 2021; Fernandez de Cossio Diaz et al. Arxiv 2022 Di Gioacchino et al. BiorXiv 2022). We have now added a summary of these points in Methods 7.7.2, also refer to this with explicit mention of the runtime in the Discussion, end of 2nd paragraph:</p><p>By implementing various algorithmic optimizations (Methods 7.7), cRBM models converged in approximately 812 hours on high-end desktop computers (also see Methods 7.7.2).</p><disp-quote content-type="editor-comment"><p>4. Line 515. A core feature of the proposed compositional RBM is the addition of a soft sparsity penalty over the weight matrix in the likelihood function. The authors claim that &quot;directed graphical models&quot; are limited by the a priori constraints that they impose on the data structure. Meanwhile, a more accurate statistical solution can be obtained using a RBM-based model, as outlined by the maximum entropy principle. The problem with this argument is that the maximum entropy principle no longer applies to the proposed model with the addition of the penalty term. In fact, the λ regularization term, which was estimated from a set of data statistics motivated by the experimenter's research goals (Figure S1), serves to constrict the prior probability. Moreover, in Figure S1F, we clearly see that reconstruction quality suffers with a higher penalty, suggesting that the principle had indeed been violated. That being said, RBMs are notoriously hard to train, possibly due to the unconstrained nature of the optimization. I believe that cRBM can help bring RBM into wider practical applications. The authors could test their model on a few values of the free parameter and report this as a supplementary. I believe that different parameters of λ could elaborate on different anatomical clusters and temporal dynamics. Readers who would like to implement this method for their own analysis would also benefit tremendously from an understanding of the effects of λ on the interpretation of their data. Item (1) on line 35 (and other instances throughout the text) should be corrected to reflect that cRBM replaces the hard constraints found in many popular methods with a soft penalty term, which allows for more accurate statistical models to be obtained.</p></disp-quote><p>We thank the reviewer for their analysis and suggestion. Indeed, adding the regularization term – not present in the classical formulation of the RBM (Hinton and Salakhutdinov, 2006, Science) – was critical for significantly enhancing its performance, which allowed us to implement this model on our large scale datasets (~50K visible units). We agree that providing more information on the effect of the regularization term will benefit readers who would like to use this method. We have now included an extensive analysis of the effects of λ in the revised manuscript, which is detailed in our answer to a similar question from reviewer 2. We therefore refer to our answer in response to the first question of reviewer 2.</p><p>The reviewer’s comment on the Maximum Entropy issue calls for some clarification. The maximum entropy principle is a recipe for finding the least constrained model that reproduces specified data-dependent moments. However, it cannot determine which moments are statistically meaningful in a finite-sized data set. A general practice is to only include low-order moments (1st and 2nd), but this is sometimes already too much for biological data. Regularization provides a practical means to select stable moments to be fitted and others to be ignored. This can be seen from the optimality condition, which writes, e.g., for the weights w<sub>i,mu</sub>:</p><p>| &lt; v<sub>i</sub> h<sub>,mu</sub>&gt;data – &lt; v<sub>i</sub> h<sub>,mu</sub>&gt;<sub>model</sub> | &lt; λ if w<sub>i,mu</sub> = 0.</p><p>&lt; v<sub>i</sub> h<sub>,mu</sub>&gt;<sub>data</sub> – &lt; v<sub>i</sub> h<sub>,mu</sub>&gt;<sub>model</sub> | = λ(w<sub>i,mu</sub>) if |w<sub>i,mu</sub>| &gt; 0.</p><p>Essentially, this lets the training decide which subset of the constraints should actually be used. Thus, regularized models are closer to the uniform distribution (g=w=0), and actually have higher entropy than unregularized one (see, e.g., Fanthomme et al. Journal of Statistical Mechanics, 2022). Therefore, we believe that a regularized maximum entropy model can still be considered a <italic>bona fide</italic> MaxEnt model. This formulation should not be confused with another formulation (that perhaps the reviewer has in mind) where a weighted sum of the entropy and the regularization term is maximized under the same moment-matching constraints. In this case, we agree that maximum entropy principle (MaxEnt) would be violated.</p><p>The choice of regularization value should be dictated by bias-variance trade-off considerations. Ideally, we would use the same criterion as for training, i.e., maximization of log-likelihood for the held-out test set, but it is intractable. Thus, we used a consensus between several tractable performance metrics as a surrogate; we believe this consensus to be principally independent of the research goal. While the reconstruction error indeed increases for large regularization values, this is simply because too few constraints are retained at high regularizations.</p><p>Essentially, the parameters selected by likelihood maximization find the finest assembly scale that can be accommodated by the data presented. Thus, the number and size of the assemblies are not specified by the complexity of the data set alone. Rather, the temporal resolution and length of the recordings play a key role; higher resolution recordings will allow the inference of a larger number of smaller assemblies, and enable the study of their hierarchical organization.</p><p>That being said, we fully agree that the regularization strength and number of hidden units have a strong impact on the nature of the representation learnt. In the revised manuscript, we follow the reviewer’s suggestion and provide additional insights on the effect of these parameters on the representation learnt (please see p9-10).</p><disp-quote content-type="editor-comment"><p>Minor comments:</p><p>5. From a neuroscience point of view, it might be interesting to show what results are achieved using different values of M (say 100 or 300), rather than M=200, while still maintaining the compositional phase. Is there any similarity between the cRBM-based cell assemblies generated at different values of M? Is there a higher chance of capturing certain dynamics either functional or structural using cRBM? For example, did certain cRBM-based cell assemblies pop up more frequently than others at all values of M (100,200,300)?</p></disp-quote><p>Please find the answer p9-10, in response to a similar question raised by Reviewer #2, Q1.</p><disp-quote content-type="editor-comment"><p>6. The authors have mentioned that this approach can be readily applied to data obtained in other animal models and using different recording techniques. It might be nice to see a demonstration of that.</p></disp-quote><p>We agree that showing additional data analysis would be interesting, but we feel that it would overburden the supplementary section of the manuscript, which is already lengthy. In previous works, we and collaborators have used cRBMs for analyzing MNIST data (Tubiana and Monasson, 2017, PRL; Roussel et al. 2022 PRE), protein sequence data (Tubiana et al., 2019, <italic>eLife</italic>; Tubiana et al. 2019, Neural Computation; Bravi et al. Cell Systems 2021; Bravi et al. PLOS CB 2021; Fernandez de Cossio Diaz et al. Arxiv 2022), DNA sequences (Di Gioacchino et al. BiorXiv 2022), spin systems (Harsh et al. J. Phys. A 2020), etc. Many are included as example notebooks – next to the zebrafish data – in the linked code repository. For neural data, we have recently shared our code with another research group working on mice auditory cortex (2-photon, few thousands of neurons, Léger and Bourdieu). Preliminary results are encouraging, but not ready for publication yet.</p><disp-quote content-type="editor-comment"><p>7. Line 237. The justification for employing a dReLU transfer function as opposed to ReLU is unclear, at least within the context of neurobiology. Given that this gives rise to a bimodal distribution for the activity of HUs, the rationale should be clearly outlined to facilitate interpretability.</p></disp-quote><p>We thank the reviewer for the question. As we detail in the manuscript (Methods), the dReLU potential is one of the sufficient requirements for the RBM to achieve the compositional phase. The compositional phase is characterized by localized assemblies that co-activate to generate the whole-brain neural dynamics. This property reflects neurobiological systems (Harris, 2005, Neuron), which is one of the reasons why we employed compositional RBMs for this study.</p><p>As the reviewer points out, the HUs that we infer exhibit bimodal activity (Figure 4). Importantly, the HU activity is not constrained by the model to take this shape, as dReLU potentials allow for several activity distributions (see Methods 7.5.4; “Choice of HU potential”). In fact, ReLU potentials are a special case of dReLU (by <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="normal">∖</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>), so our model allows HU potentials to behave like ReLUs, but in practice they converge to a double-well potential for almost all HUs, leading to bimodal activity distributions.</p><p>Following the suggestion of the reviewer, we have now added this detail for clarity in Methods 7.5.4 and referenced this Methods section at line 237.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Significance (Required)):</p><p>van der Plas et al. highlighted a novel dimensionality reduction technique that can be used with success for discerning functional connectivities in large-scale single-unit recordings. The proposed model belongs to a large collection of dimensionality reduction techniques (for review, Cunningham, J., Yu, B. Dimensionality reduction for large-scale neural recordings. Nat Neurosci 17, 1500-1509 (2014). https://doi.org/10.1038/nn.3776; Paninski, L., and Cunningham, J. P. (2018). Neural data science: accelerating the experiment-analysis-theory cycle in largescale neuroscience. Current opinion in neurobiology, 50, 232-241.). The authors themselves highlighted some of the key methods, such as PCA, ICA, NNMF, variational auto-encoders, etc. The proposed cRBM model has also been published a few times by the same authors in previous works, although specifically pertaining to protein sequences. The use of RBM-like methods in uncovering functional connectivities is not novel either (see Hjelm RD, Calhoun VD, Salakhutdinov R, Allen EA, Adali T, Plis SM. Restricted Boltzmann machines for neuroimaging: an application in identifying intrinsic networks. Neuroimage. 2014 Aug 1;96:245-60. doi: 10.1016/j.neuroimage.2014.03.048.). However, given that the authors make a substantial improvement on the RBM network and have demonstrated the value of their model using physiological data, I believe that this paper would present itself as an attractive alternative to all readers who are seeking better solutions to interpret their data. However, as I mentioned in my comments, I would like to see more definitive evidence that the proposed solution has a serious advantage over other equivalent methods.</p><p>Reviewer's expertise:</p><p>This review was conducted jointly by three researchers whose combined expertise includes single-unit electrophysiology and two-photon calcium imaging, using which our lab studies the neurobiology of learning and memory and spatial navigation. We also have extensive experience in computational neuroscience, artificial neural network models, and machine learning methods for the analysis of neurobiological data. We are however limited in our knowledge of mathematics and engineering principles. Therefore, our combined expertise is insufficient to evaluate the correctness of the mathematical developments.</p><p>Reviewer #2 (Evidence, reproducibility and clarity (Required)):</p><p>In their manuscript, van der Plas et al. present a generative model of neuron-assembly interaction. The model is a restricted Boltzmann machine with its visible units corresponding to neurons and hidden units to neural assemblies. After fitting their model to whole-brain neural activity data from larval zebrafish, the authors demonstrate that their model is able to replicate several data statistics. In particular, it was able to replicate the pairwise correlations between neurons as well as assemblies that it was not trained on. Moreover, the model allows the authors to extract neural assemblies that govern the population activity and compose functional circuits and can be assigned to anatomical structures. Finally, the authors construct functional connectivity maps from their model that are then shown to correlate with established structural connectivity maps.</p><p>Overall, the authors present convincing evidence for their claims. Furthermore, the authors state that their code to train their restricted Boltzmann machine models is already available on GitHub and that the data underlying the results presented in this manuscript will be made publicly available upon publication, which will allow people to reproduce the results and apply the methods to their data.</p><p>One thing the authors could maybe discuss a bit more is the &quot;right&quot; parameter value M, especially since they used the optimal value of 200 found for one sample also for all the others. More specifically, how sensitive are the results to this value?</p></disp-quote><p>In the following we jointly address three of the reviewers’ questions (2 from reviewer 1, and 1 from reviewer 2).</p><p>Shortly summarized, the cRBM model has 2 free parameters; the number of hidden units M and the regularization parameter λ. In figures 2 and S1 we optimize their values through cross-validation, and then perform the subsequent analyses on models with these optimal values. The reviewers ask us to examine the outcome of the model for slightly different values of both parameters, in particular in relation to the sensitivity of the cRBM results to selecting the optimal parameters and the change in inferred assemblies and their dynamics.</p><p>We thank the reviewers for these questions and appreciate their curiosity to understand the effects of changing either of these two free parameters.</p><p>To assess the influence of both the number of hidden units M and the sparsity regularization parameter λ, we computed, for all cRBMs models trained in the hyperparameter search procedure, two metrics: the distribution of assembly sizes, and the distribution of HU dynamics time scales (as in Figure 4). We have now added a supplementary Figure S5 that shows these two distributions for the grid of M, λ values of Supplementary Figure S1, and have performed two-way ANOVA tests to determine the significance of M and λ on these two metrics. We found that M and λ controlled the distribution of assembly sizes in a consistent manner: assembly size was a gradually decreasing function of both M and λ (Figure S5A-F). Further, M, but not λ, similarly controlled the distribution of time scales (Figure S5G-L). However, for M and λ values close to the optimal parameter-setting (M=200, λ=0.01, determined by model selection), the changes in assembly size and time scale distributions were very gradual and minimal. This showcases the robustness of the cRBM to slight changes in parameter choice.</p><p>For illustration purposes, we also added panels M-O of the median-sized assemblies of 3 very different models (M=5, M=20, M=200).</p><p>Results 2.2:</p><p>To assess the influence of M and λ on the inferred assemblies, we computed, for all cRBM models trained during the optimization of M and λ, the distribution of assembly sizes (Figure S5 A-F). We found that <italic>M</italic> and λ controlled the distribution of assembly sizes in a consistent manner: assembly size was a gradually decreasing function of both M and λ(twoway ANOVA, both P &lt; 10<sup>-3</sup>). Furthermore, for M and λ values close to the optimal parameter-setting (M=200, λ=0.02), the changes in assembly size were very small and gradual. This showcases the robustness of the cRBM to slight changes in parameter choice.</p><disp-quote content-type="editor-comment"><p>And, what happens if one would successively increase that number, would the number of assemblies (in the sense of hidden units that strongly couple to some of the visible units) eventually saturate?</p></disp-quote><p>This point will be addressed by inspecting models at different M values (see #1). We would like to further answer this question by referring to past work. In Tubiana et al., 2019, <italic>eLife</italic> (Appendix 1) we have done this analysis, and the result is consistent with the reviewer’s intuition. Because of the sparsity regularization, if M becomes larger than its optimum, the assemblies further sparsify without benefiting model performance, and eventually new assemblies duplicate previous assemblies or become totally sparse (i.e., all weights = 0) to not further induce a sparsity penalty in the loss function. So the ‘effective’ number of assemblies indeed saturates for high M.</p><disp-quote content-type="editor-comment"><p>Moreover, regarding the presentation, I have a few minor suggestions and comments that the authors also might want to consider:</p><p>– In Figure 6C, instead of logarithmic axes, it might be better to put the logarithmic connectivity on a linear axis. This way the axes can be directly related to the colour bars in Figures 6A and B.</p></disp-quote><p>We agree and thank you for the suggestion. We have changed this accordingly (and also in the equivalent plots in figure S6).</p><disp-quote content-type="editor-comment"><p>– In Equation (8), instead of <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">∖</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>\it should be <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></p></disp-quote><p>Done, thank you.</p><disp-quote content-type="editor-comment"><p>– In Section 7.0.5, it might make sense to have the subsection about the marginal distributions before the ones about the conditional distributions. The reason would be that if one wants to confirm Equation (8) one necessarily has to compute the marginal distribution in Equation (12) first.</p></disp-quote><p>We thank the reviewer for the suggestion, but respectfully propose to leave the section ordering as is. We understand what the reviewer means, but Equation (8) can also be obtained by factorizing P(v,h) Equation (7) and removing the v_i dependency. In Equation (8), \Γ can then be obtained by normalization. We believe this flow aligns better with the main text (where conditionals come first, when used for sampling, followed by the marginal of P(v) used for the functional connectivity inference).</p><disp-quote content-type="editor-comment"><p>– In Line 647f, the operation the authors are referring to is strictly speaking not an L1-norm of the matrix block. It might be better to refer to that e.g. as a normalised L1-norm of the matrix block elements.</p></disp-quote><p>Done, thank you.</p><disp-quote content-type="editor-comment"><p>– In Line 22, when mentioning dimensionality reduction methods to identify assemblies, it might make sense to also reference the work by Lopes-dos-Santos et al. (2013, J. Neurosci. Methods 220).</p></disp-quote><p>Done, thank you for the suggestion.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Significance (Required)):</p><p>The work presented in this manuscript is very interesting for two reasons. First, it has long been suggested that assemblies are a fundamental part of neural activity and this work seems to support that by showing that one can generate realistic whole-brain population activity imposing underlying assembly dynamics. Second, in recent years much work has been devoted to developing methods to find and extract neural assemblies from data and this work and the modelling approach can also be seen as a new method to achieve that. As such, I believe this work is relevant for anyone interested in neural population activity and specifically neural assemblies and certainly merits publication.</p><p>Regarding my field of expertise, I used to work on data analysis of neural population activity and in particular on the question of how one can extract neural assemblies from data. I have to say that I have not much experience with fitting statistical models to data, so I can't provide any in-depth comments on that part of the work, although what has been done seems plausible.</p><p>Reviewer #3 (Evidence, reproducibility and clarity (Required)):</p><p>Summary: Understanding the organization of neural population activities in the brain is one of the most important questions in neuroscience. Recent technique advance has enabled researchers to record a large number of neurons and some times the whole brain. Interpreting and extracting meaningful insights from such data sets are challenging. van der Plas et al applied a generative model called compositional Restricted Boltzmann Machine (cRBM) to discover neuron assemblies from spontaneous activities of zebra fish brain. They found that neurons can be grouped into around 200 assemblies. Many of them have clear neurophysiological meaning, for example, they are anatomically localized and overlapped with known neural circuits. The authors also inferred a coarse-grained functional connectivity which is similar to known structural connectivity.</p><p>The structure of the paper is well organized, the conclusion seems well supported by their numerical results. While this study provides a compelling demonstration that cRBM can be used to uncover meaningful structures from large neural recordings, the following issues limit my enthusiasm.</p><p>Major:</p><p>1) The overall implication is not clear to me. Although the authors mentioned this briefly in the discussion. It is not clear what else do we learn from discovered assemblies beyond stating that they are consist with previous study. For example, the author could have more analysis of the assembly dynamics, such as whether there are low dimensional structure etc.</p></disp-quote><p>First, we will comment on our analysis of the assemblies, before we continue to discuss the main implications of our work, which we believe are the inferred <italic>generative</italic> model of the zebrafish brain and the perturbation-based connectivity matrix that we discovered. Further, we have implemented the reviewer’s suggestion of analyzing the low-dimensional structure of the hidden unit activity, as further detailed in Question 7.</p><p>Indeed, the example assemblies that we show in Figure 3 have been thoroughly characterized in previous studies, which is why we chose to showcase these examples. Previous studies (including our own) typically focused on particular behaviors or sensory modalities, and aimed at identifying the involved neural circuit. Here, we demonstrate that by using cRBM on spontaneous activity recordings, one can simultaneously identify many of those circuits. In other words, these functional circuits/assemblies activate spontaneously, but in many different combinations and perhaps infrequently, so that it is very difficult to infer them from the full neural dynamics that they generate. cRBM has been able to do so, and Figure 3 (and supplementary video 1) serve to illustrate the variety of (known) circuits and assemblies that it inferred, some of which may represent true but not yet characterized circuits, which thus provide hypotheses for subsequent studies.</p><p>Further, we believe that the implication of our study goes beyond the properties of the assemblies we’ve identified, in several ways.</p><p>We demonstrate the power of cRBM’s generative capacity for inferring low-dimensional representations in neuroscience. Unlike standard dimensional reductionality methods, generative models can be assessed by comparing the statistics of experimental vs in-silico generated data. This is a powerful approach to validate a model, rarely used in neuroscience because of the scarcity of generative models compatible with large-scale data, and we hope that our study will inspire the use of this method in the field. We have made our cRBM code available, including notebook tutorials, to facilitate this.</p><p>The generative aspect of our model allowed us to predict the effect of single-neuron perturbations between all ~ 10<sup>9</sup> pairs of neurons per fish, resulting in a functional connectivity matrix. We believe that the functional connectivity matrix is a major result for the field, similar to the structural connectivity matrix from Kunst et al., 2019, Neuron. The relation between functional and structural connectivity is unknown and of strong interest to the community (e.g., Das and Fiete, 2020, Nature Neuroscience). Our results allowed for a direct comparison of whole-brain region-by-region structural and functional connectivity. We were thus able to quantify the similarity between these two maps, and to identify specific region-pair matches and non-matches of functional and structural connectivity – which will be of particular interest to the zebrafish neuroscience community for developing future research questions.</p><p>Further, using these trained models – that will be made public upon publication – anyone can perform any type of <italic>in silico</italic> perturbation experiments, or generate endless artificial data with matching data statistics to the in vivo neural recordings.</p><p>We hope that this may convince the reviewer of the multiple directions of impact of our study. We will further address their comment on analysis of assembly dynamics below (question 7).</p><disp-quote content-type="editor-comment"><p>2) The learning algorithm of cRBM can be interpreted as matching certain statistics between the model and the experiment. For a general audience, it is not easy to understand <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since these are not directly calculated from experimental observed activities v<sub>i</sub>, but rather the average is conditioned on the empirical distribution of p(v<sub>i</sub>). For example, the meaning of  <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> means <inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>l</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where S is the set of all observed neural activities: <inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The authors should explain this in the main text or method, since they are heavily loaded in figure 2.</p></disp-quote><p>We thank the reviewer for their suggestion, and have now implemented this. Their mathematics are correct; and we agree that it is not easy to understand without going through the full derivation of the (c)RBM. At the same time, we have tried not to alienate readers who might be more interested in the neuroscience findings than in understanding the computational method used. Therefore, we have kept mathematical details in the main text to a minimum (and have used schematics to indicate the statistics in Figures 2C-G), while explaining it in detail in Methods.</p><p>Accordingly, we have now extended section 7.10.2 (“Assessment of data statistics”) that explains how the data statistics were computed in Methods (and have referenced this in Results and in Methods 7.5.5), using the fact that we already explain the process of conditioning on <bold>v</bold> in Methods 7.5.1. The following sentences were added:</p><p>“[...]However, because (c)RBM learn to match data statistics to model statistics (see Methods 7.5.5), we can directly compare these to assess model performance. […]”</p><p>[..]</p><p>“For each statistic ⟨<italic>fk</italic>⟩ we computed its value based on empirical data ⟨<italic>fk</italic>⟩<sub>data</sub> and on the model ⟨<italic>fk</italic>⟩<sub>model</sub>, which we then quantitatively compared to assess model performance. Data statistics ⟨<italic>fk</italic>⟩<sub>data</sub> were calculated on withheld test data (30% of recording). Naturally, the neural recordings consisted only of neural data v and not of HU data <bold>h</bold>. We therefore computed the expected value of <bold>h</bold><sub>t</sub> at each time point t conditioned on the empirical data <bold>v</bold><sub>t</sub>, as further detailed in Methods 7.5.1.”</p><p>[...]</p><disp-quote content-type="editor-comment"><p>3) As a modeling paper, it would be great to have some testable predictions.</p></disp-quote><p>We thank the reviewer for the enthusiasm and suggestion. We agree, and that is why we have included this in the form of functional connectivity matrices in Figures 5 and 6. To achieve this, we leveraged the generative aspect of the cRBM to perform <italic>in silico</italic> single-neuron perturbation experiments, which we aggregated to connectivity matrices. In other words, we have used our model to predict the functional connectivity between brain regions using the influence of single-neuron perturbations.</p><p>Obtaining a measure of functional connectivity/influence using single-neuron perturbations is also possible using state-of-the-art neuro-imaging experiments (e.g., Chettih and Harvey, 2019, Nature), though not at the scale of our <italic>in silico</italic> experiments. We therefore verify our predictions using structural data from Kunst et al., 2019, which we have extended substantially. We provide our functional connectivity result in full, and hope that this can inspire future zebrafish research by predicting which regions are functionally connected, which includes many pairs of regions that have not yet directly been studied in vivo.</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>1) The assembly is defined by the neurons that are strongly connected with a given hidden unit. Thus, some neurons may enter different assemblies. A statistics of such overlap would be helpful. For example, a Venn diagram in figure 1 that shows how many of them assigned to 1, 2, etc assemblies.</p></disp-quote><p>We thank the reviewer for this excellent suggestion. Indeed, neurons can be embedded in multiple assemblies. This is an important property of cRBMs, which deserves to be quantified in the manuscript. We have now added this analysis as a new supplementary figure 4. Neurons are embedded in an assembly if their connecting weight <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is ‘significantly’ non-zero, depending on what threshold one uses. We have therefore shown this statistic for 3 values of the threshold (0.001, 0.01 and 0.1) – demonstrating that most neurons are strongly embedded in at least 1 assembly and that many neurons connect to more than 1 assembly.</p><p>Updated text in Results:</p><p>“Further, we quantified the number of assemblies that each neuron was embedded in, which showed that increasing the embedding threshold did not notably affect the fraction of neurons embedded in at least 1 assembly (93% to 94%, see Figure S4).”</p><disp-quote content-type="editor-comment"><p>2) What does the link between hidden units in Figure 1B right panel mean?</p></disp-quote><p>Thank you for the question, and we apologize for the confusion: if we understand the question right, the reviewer asks why the colored circles under the title ‘Neuronal assemblies of Hidden Units’ are linked. This schematic shows the same network of neurons as shown in gray at the left side of Figure 1B, but now colored by the assembly ‘membership’ of each neuron. Hence, the circles shown are still neurons (and not HUs), and their links still represent synaptic connections between neurons. We apologize for the confusion, and have updated the caption of Figure 1B to explain this better:</p><p>“[..] The neurons that connect to a given HU (and thus belong to the associated assembly), are depicted by the corresponding color labeling (right panel).[..]”.</p><disp-quote content-type="editor-comment"><p>3) A side-by-side comparison of neural activity predicted by model and the experimentally recorded activities would help the readers to appreciate the performance of the model. Such comparison can be done at both single neuron level or assembly level.</p></disp-quote><p>We thank the reviewer for this suggestion. The cRBM model is a statistical model, meaning that it fits the statistics of the data, and not the dynamics. The data that it generates therefore (should) adhere to the statistics of the training data, but does not reflect their dynamics. We believe that showing generated activity side-by-side of empirical activity is therefore not a meaningful example of generated data, as this would exemplify the dynamics, which this model is not designed to capture. Instead, in Figure 2, we show the statistics of the generated data versus the statistics of the empirical data (e.g., Figure 2C for the mean activity of all neurons). We believe that this is a better example representation of the generative performance of the model.</p><disp-quote content-type="editor-comment"><p>4) Definition of reconstruction quality in line 130.</p></disp-quote><p>We thank the reviewer for the suggestion, and have added the following sentence after line 130:</p><p>“The reconstruction quality is defined as the log-likelihood of reconstructed neural data v<sub>recon</sub> (i.e., v that is first transformed to the low-dimensional h, and then back again to the high-dimensional v<sub>recon</sub>, see Methods 7.10.2).”</p><p>Further, please note that Methods describes the definition in detail (Eq 18 of the submitted manuscript), although we agree with the reviewer that more detail was required in the Results section at line 130.</p><disp-quote content-type="editor-comment"><p>5) Line 165. If PCA is compared with cRBM, why other dimensionality reduction methods, such as k-means and non-negative matrix factorization, can not be compared in terms of the sparsity?</p></disp-quote><p>Please see answer to question 1 from Reviewer 1.</p><disp-quote content-type="editor-comment"><p>6) Line 260, please provide minimum information about how the functional connectivity is defined based on assemblies discovered by cRBM.</p></disp-quote><p>We apologize if this was not clear. The first paragraph of this section (lines 248-259) of the submitted manuscript, provides the detail that the reviewer asks for, and we realize that the sentence of line 260 is better placed in the first paragraph, as it has come across as a very minimal explanation of how functional connectivity is defined.</p><p>We have now moved this sentence to the preceding paragraph, as well as specified the Method references (as suggested by this reviewer below), for additional clarity. We thank the reviewer for pointing out this sentence.</p><disp-quote content-type="editor-comment"><p>7) Some analysis of the hidden units population activities. Such as whether or not there are interesting low dimensional structure from figure 4A.</p></disp-quote><p>We thank the reviewer for their suggestion. In our manuscript we have used the cRBM model to create a lowdimensional (M=200) representation of zebrafish neural recordings (N=50,000). The richness of this model owes to possible overlaps between HUs/assemblies that can result in significant correlation in their activities. The latter is illustrated in Figure 4A-C: the activity of some HUs can be strongly correlated.</p><p>The reviewer’s suggestion is similar; to perform some form of dimensionality reduction on the low-dimensional HU activity shown in Figure 4. We have now added a PCA analysis to Figure 4 to quantify the degree of lowdimensional structure in the HU dynamics, and show the results in a new panel Figure 4D.</p><p>The following text has been added to the Results section:</p><p>These clusters of HUs with strongly correlated activity suggest that much of the HU variance could be captured using only a small number of variables. We quantified this by performing PCA on the HU dynamics, finding that indeed 52% of the variance was captured by the first 3 PCs, and 85% by the first 20 PCs (Figure 4D).</p><p>We believe that further visualization of these results, such as plotting the PC trajectories, would not further benefit the manuscript. The manuscript focuses on cRBM, and the assemblies/HUs it infers. Unlike PCA, these are not ranked/quantified by how much variance they explain individually, but rather they together ‘compose’ the entire system and explain its (co)variance (Figure 2). Breaking up a dominant activity mode (as found by PCA), such as the ARTR dynamics, into multiple HUs/assemblies, allows for some variation in activity of individual parts of the ARTR circuit (such as tail movement and eye movement generation), even though at most times the activity of these HUs is coordinated. We hope the reviewer agrees with our motivation to keep the manuscript focused on the nature of cRBM-inferred HUs.</p><disp-quote content-type="editor-comment"><p>8) Figure 4B right panel, how did the authors annotate the cluster manually? As certain assembly may overlap with several different brain regions, for example, figure 4D.</p></disp-quote><p>We thank the reviewer for this question, and we presume they meant to reference figure 3D as an example? For figure 4, as well as Figure 3, we used the ZBrain Atlas (Randlett et al., 2015) for definition of brain regions. This atlas presents a hierarchy of brain regions: for example, many brain regions are part of the rhombencephalon/hindbrain. This is what we used for midbrain/hindbrain/diencephalon. Further, many assemblies are solely confined to Optic Tectum (see Figure 3L), which we therefore used (split by hemisphere). Then, many brain regions are (partly) connected to the ARTR circuit, such as the example assembly of Figure 3D that the reviewer mentions. These we have all labeled as ARTR (left or right), though technically only part of their assembly <italic>is</italic> the ARTR. These two clusters therefore rather mean ‘ARTR-related’, in particular because their activity is locked to the rhythm of the ARTR (see Figure 4A). The final category is ‘miscellaneous’ (like Figure 3G).</p><p>However we agree that this wasn’t clear from the manuscript text, so we have changed the figure 4C caption to mention that ‘ARTR’ stands for ARTR-related assemblies, which we hope clarifies that ARTR-clustered assemblies can exist of multiple, disjoint groups of neurons, which relate to the ARTR circuit.</p><disp-quote content-type="editor-comment"><p>9) Better reference of the methods cited in the main text. The method part is quite long, it would be helpful to cite the section number when referring it in the main text.</p></disp-quote><p>We thank the reviewer for this helpful suggestion, we agree that it would benefit the manuscript to reference specific sections of the Methods. We have now changed all references to Methods to incorporate this.</p><disp-quote content-type="editor-comment"><p>10) Some discussion about the limitation of cRBM would be great.</p></disp-quote><p>We thank the reviewer for this suggestion, and have now included this. As Reviewer 1 had the same suggestion, we refer our answer to questions 2 and 3 from Reviewer 1 for more detail.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Significance (Required)):</p><p>This work provides a timely new technique to extract meaningful neural assemblies from large scale recordings. This study should be interested to both researchers doing either experiments and computation/theory. I am a computational neuroscientist.</p><p>Description of analyses that authors prefer not to carry out.</p><p>Please include a point-by-point response explaining why some of the requested data or additional analyses might not be necessary or cannot be provided within the scope of a revision. This can be due to time or resource limitations or in case of disagreement about the necessity of such additional data given the scope of the study. Please leave empty if not applicable.</p></disp-quote><p>We propose to limit the comparison to other assembly inference techniques to generative models.</p></body></sub-article></article>