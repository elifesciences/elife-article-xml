<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">101371</article-id><article-id pub-id-type="doi">10.7554/eLife.101371</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101371.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Balancing safety and efficiency in human decision-making</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Mahajan</surname><given-names>Pranav</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-2507-5450</contrib-id><email>pranav.mahajan@ndcn.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tong</surname><given-names>Shuangyi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-4985-6600</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Sang Wan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6266-9613</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Seymour</surname><given-names>Ben</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1724-5832</contrib-id><email>ben.seymour@ndcn.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0172mzb45</institution-id><institution>Wellcome Centre for Integrative Neuroimaging, FMRIB, Nuffield Department of Clinical Neurosciences, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Institute of Biomedical Engineering, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05apxxy63</institution-id><institution>Department of Brain and Cognitive Sciences, Korea Advanced Institute of Science and Technology (KAIST)</institution></institution-wrap><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05apxxy63</institution-id><institution>Kim Jaechul Graduate School of AI, KAIST</institution></institution-wrap><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05apxxy63</institution-id><institution>KAIST Center for Neuroscience-inspired Artificial Intelligence</institution></institution-wrap><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cools</surname><given-names>Roshan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>10</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP101371</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-07-11"><day>11</day><month>07</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-07-04"><day>04</day><month>07</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.23.576678"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-18"><day>18</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101371.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-12"><day>12</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101371.2"/></event></pub-history><permissions><copyright-statement>© 2024, Mahajan et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Mahajan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-101371-v1.pdf"/><abstract><p>The safety-efficiency dilemma describes the problem of maintaining safety during efficient exploration and is a special case of the exploration-exploitation dilemma in the face of potential dangers. Conventional exploration-exploitation solutions collapse punishment and reward into a single feedback signal, whereby early losses can be overcome by later gains. However, the brain has a separate system for Pavlovian fear learning, suggesting a possible computational advantage to maintaining a specific fear memory during exploratory decision-making. In a series of simulations, we show this promotes safe but efficient learning and is optimised by arbitrating Pavlovian avoidance of instrumental decision-making according to uncertainty. We provide a basic test of this model in a simple human approach-withdrawal experiment in virtual reality and show that this flexible avoidance model captures choice and reaction times. These results show that the Pavlovian fear system has a more sophisticated role in decision-making than previously thought, by shaping flexible exploratory behaviour in a computationally precise manner.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Animals need to navigate a complex world where they gain rewards, such as finding food and shelter, while avoiding danger on a daily basis. This creates a ‘safety-efficiency dilemma’ in how to pursue rewards efficiently without increasing the risk of harm.</p><p>The human brain relies on two key systems to manage this challenge. The instrumental system learns from the consequences of our actions, weighing costs and benefits to maximize overall outcomes. In contrast, the Pavlovian fear system generates automatic defensive responses. For example, it learns to associate certain cues with danger and triggers instinctual reactions, like pulling the hand away from a hot stove. Sometimes these systems conflict—fear urges withdrawal even when careful action might lead to a greater reward—and the brain must resolve this tension. Mahajan et al. set out to explore how the brain flexibly manages the influence of this automatic fear system depending on how uncertain a situation is.</p><p>Is it computationally advantageous for the brain to adjust the impact of Pavlovian fear based on outcome uncertainty? And is there evidence that humans actually use this strategy? These questions are important because the influence of fear has often been treated as fixed. Recognizing its flexibility offers a more nuanced understanding of how we balance safety and efficiency in decision-making.</p><p>Mahajan et al. found that a flexible Pavlovian fear system, guided by uncertainty, promotes safer decisions without substantially sacrificing efficiency. The researchers first used computer simulations to show that a model in which fear’s influence decreases as the environment becomes more predictable resolves the safety–efficiency trade-off more effectively than models with a fixed fear response.</p><p>They then tested this prediction in a virtual reality experiment, where human participants decided whether to approach or withdraw from stimuli that could lead to a mild electric shock. The results showed that the flexible model best explained both people’s choices and their reaction times.</p><p>These findings may eventually benefit individuals with anxiety disorders or chronic pain, conditions often marked by excessive avoidance behaviors. The results suggest the core issue may not simply be an overactive fear system, but rather a difficulty in flexibly reducing its influence when situations become predictable. This perspective could inspire therapies that aim to help patients to better distinguish between controllable and uncontrollable threats, making their defensive responses more adaptable – a concept already present in some forms of cognitive behavioral therapy.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>exploration-exploitation trade-off</kwd><kwd>Pavlovian bias</kwd><kwd>safety-efficiency trade-off</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029chgv08</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/214251</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029chgv08</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/203139</award-id><principal-award-recipient><name><surname>Mahajan</surname><given-names>Pranav</given-names></name><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01g0hqq23</institution-id><institution>Institute for Information and Communications Technology Promotion</institution></institution-wrap></funding-source><award-id>MSIT 2019-0-01371</award-id><principal-award-recipient><name><surname>Lee</surname><given-names>Sang Wan</given-names></name><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hhkn466</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>22H04998</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>NIHR Oxford Health Biomedical Research Centre</institution></institution-wrap></funding-source><award-id>NIHR203316</award-id><principal-award-recipient><name><surname>Seymour</surname><given-names>Ben</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A separate fear system, flexibly gated by outcome uncertainty, guides decision-making to be both safe and efficient during exploration.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans and animals inhabit a complex and dynamic world where they need to find essential rewards such as food, water, and shelter, whilst avoiding a multitude of threats and dangers which can cause injury, disability, or even death. This illustrates a tension at the heart of learning and decision-making systems: on the one hand, one wants to minimise environmental interactions required to learn to acquire rewards (be sample efficient), but on the other hand, it is important not to accrue excessive damage in the process - which is particularly important if you only get one chance at life. This safety-efficiency dilemma is related to the exploration-exploitation dilemma, in which the long-term benefits of information acquisition are balanced against the short-term costs of avoiding otherwise valuable options. Most solutions to the exploration-exploitation dilemma consider things only from the point of view of a single currency of reward, and hence, early losses can be overcome by later gains. Thus, many engineering solutions involve transitioning from exploratory strategies to more exploitative strategies over time as an agent gets more familiar with the environment. However, such solutions could be insufficient if some outcomes are incommensurable with others; for instance, damage accrues to the point that cannot be overcome, or worse still, leads to system failure ‘death’ before you ever get the chance to benefit through exploitation, emphasising the need for safe (early) exploration. Safe learning (<xref ref-type="bibr" rid="bib21">Garcıa and Fernández, 2015</xref>) is an emerging topic in artificial intelligence and robotics, with the advent of adaptive autonomous control systems that learn primarily from experience: e.g., robots intended to explore the world without damaging or destroying themselves (or others) - the same concern animals and humans have.</p><p>A biological solution to this problem may be to have distinct systems for learning, for instance, having Pavlovian reward and punishment systems in addition to an instrumental system, which can then be integrated together to make a decision (<xref ref-type="bibr" rid="bib4">Bach and Dayan, 2017</xref>; <xref ref-type="bibr" rid="bib14">Elfwing and Seymour, 2017</xref>). A dissociable punishment system could then allow, for example, setting a lower bound on losses which must not be crossed during early learning. The brain seems likely to adopt a strategy like this since we know that Pavlovian fear processes influence instrumental reward acquisition processes (e.g. in paradigms such as conditioned suppression [<xref ref-type="bibr" rid="bib31">Kamin et al., 1963</xref>] and Pavlovian-instrumental transfer [<xref ref-type="bibr" rid="bib60">Prévost et al., 2012</xref>; <xref ref-type="bibr" rid="bib69">Talmi et al., 2008</xref>]). However, it is not clear if this exists as a static system, with a constant Pavlovian influence over instrumental decisions, or a flexible system in which the Pavlovian influence is gated by information or experience. Computationally, it implies a multi-attribute architecture involving modular systems that separately learn different components of feedback (rewards, punishments) with the responses or actions to each then combined.</p><p>In this paper, we ask two central questions: (1) whether it is computationally (normatively) adaptive to have a flexible system that titrates the influence of ‘fear’ based on uncertainty, i.e., reduces the impact of fear after exploration and (2) whether there is any evidence that humans use this sort of flexible meta-control strategy. We first describe a computational model of how Pavlovian (state-based) responses shape instrumental (action-based control) processes and show how this translates to a multi-attribute reinforcement learning (RL) framework at an algorithmic level. We propose how Pavlovian-instrumental transfer may be flexibly guided by an estimate of outcome uncertainty (<xref ref-type="bibr" rid="bib3">Bach and Dolan, 2012</xref>; <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>) - which effectively acts as a measure of uncontrollability. We use Pearce-Hall associability (<xref ref-type="bibr" rid="bib34">Krugel et al., 2009</xref>), which is an implementationally simple and direct measure of uncertainty that has been shown to correlate well with both fear behaviour (skin conductance) and brain (amygdala) activity in fear learning studies (<xref ref-type="bibr" rid="bib38">Li et al., 2011</xref>; <xref ref-type="bibr" rid="bib81">Zhang et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Zhang et al., 2018</xref>). Below, we demonstrate the safety-efficiency trade-off in a range of simulation environments and show how it can be solved with a flexible Pavlovian fear bias. Consequently, we then test basic experimental predictions of the model in a virtual reality-based approach-withdrawal task involving pain, which builds upon previous Go-No Go studies studying Pavlovian-instrumental transfer (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>). The virtual-reality approach confers a greater ecological validity, and the immersive nature may contribute better to fear conditioning, making it easier to distinguish the aversive components.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Pavlovian avoidance learning model</title><p>Our model consists of a Pavlovian punishment (fear) learning system and an integrated instrumental learning system (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The standard (rational) RL system is modelled as the instrumental learning system. The additional Pavlovian fear system biases the withdrawal actions to aid in safe exploration, in line with our hypothesis. The Pavlovian system learns punishment expectations for each stimulus/state, with the corresponding Pavlovian responses manifest as action propensities to withdraw. For simplicity, we don’t include a Pavlovian reward system, or other types of Pavlovian fear response (<xref ref-type="bibr" rid="bib7">Bolles, 1970</xref>). The instrumental system learns combined reward and punishment expectations for each stimulus-action or state-action pair and also converts these into action propensities. Both systems learn using a basic temporal difference updating rule (or in instances, its special case, the Rescorla-Wagner [RW] rule). The ultimate decision that the system reaches is based on integrating these two action propensities, according to a linear weight, <italic>ω</italic>. Below, we consider fixed and flexible implementations of this parameter, and test whether a flexible <italic>ω</italic> confers an advantage. We implement the flexible <italic>ω</italic> using Pearce-Hall associability (see <xref ref-type="disp-formula" rid="equ15">Equation 15</xref> in Materials and methods). The Pearce-Hall associability maintains a running average of absolute temporal difference errors (<italic>δ</italic>) as per <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>. This acts as a crude but easy-to-compute metric for outcome uncertainty which gates the influence of the Pavlovian fear system, in line with our hypothesis. This implies that the higher the outcome uncertainty, as is the case in early exploration, the more cautious our agent will be, resulting in safer exploration. For simulations, we use standard grid world-like environments, which provide a didactic tool for understanding Pavlovian-instrumental interactions (<xref ref-type="bibr" rid="bib11">Dayan et al., 2006</xref>). Since Pavlovian biases influence not only choices but also reaction times (RTs), we extend our model to reinforcement learning diffusion decision-making (RLDDM) models (<xref ref-type="bibr" rid="bib17">Fengler et al., 2022</xref>; <xref ref-type="bibr" rid="bib19">Fontanesi et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">Pedersen et al., 2017</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>An illustration of Pavlovian Avoidance Learning (PAL) model.</title><p>Pavlovian and instrumental valuations are combined to arrive at action propensities used for (softmax) action selection. The Pavlovian bias influences protective behaviours through safer (Boltzmann) exploration, and the arbitration between the Pavlovian and instrumental systems is performed using the parameter <italic>ω</italic>. Here, <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> denotes the feedback signal which can take both positive values (in the case of rewards) and negative values (in the case of punishments). Please see Materials and methods for technical details; notations for the illustration follow <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Experiment 1: A simulated flexible fear-commissioning model balances safety and efficiency</title><p>We consider a simple fully-observable grid world environment with stochastic state transitions and fixed starting state and fixed rewarding goal state. <xref ref-type="fig" rid="fig2">Figure 2A</xref> illustrates how the misalignment of Pavlovian bias and instrumental action can lead to a safety-efficiency dilemma. The Pavlovian action is assumed to be an evolutionarily acquired simple withdrawal response (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and the Pavlovian state value is learned during the episode and shapes instrumental policy and value acquisition (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows value plots for the instrumental policy with and without a Pavlovian bias. All plots show values and policy at the end of 1000 episodes of learning. These heatmaps denote value, i.e., the expectation of cumulative long-term rewards <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> (including any punishments) and the arrows show the policy, i.e., actions that maximise this value. Additionally, the learned punishment value <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$V_{p}$\end{document}</tex-math></alternatives></inline-formula> of the Pavlovian bias is also shown along with the Pavlovian avoidance learning (PAL) policy. The PAL value function and policy shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref> utilises the flexible <italic>ω</italic> scheme utilised below.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Demonstration of safety-efficiency trade-off and the flexible arbitration scheme in a grid world environment.</title><p>(<bold>A</bold>) Grid world environment with starting state in the top-right corner and rewarding goal state (<inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$R=+1$\end{document}</tex-math></alternatives></inline-formula>) in the bottom-left corner and the red states are painful (<inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$R=-0.1$\end{document}</tex-math></alternatives></inline-formula>). The grid world layout follows <xref ref-type="bibr" rid="bib23">Gehring and Precup, 2013</xref>. Inset provides a didactic example of misalignment between Pavlovian bias and instrumental action. (<bold>B</bold>) Stochastic policy of pre-trained withdrawal action subset <inline-formula><alternatives><mml:math id="inf6"><mml:msub><mml:mi>A</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><tex-math id="inft6">\begin{document}$A_{p}$\end{document}</tex-math></alternatives></inline-formula>, which is biased with Pavlovian punishment values in the Pavlovian avoidance learning (PAL) agent. (<bold>C</bold>) The learned instrumental values and Pavlovian fear bias <inline-formula><alternatives><mml:math id="inf7"><mml:msub><mml:mi>V</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><tex-math id="inft7">\begin{document}$V_{p}$\end{document}</tex-math></alternatives></inline-formula>(heatmap) and policy (arrows) are learned by the instrumental and flexible <italic>ω</italic> agent by the end of the learning duration. The value functions plotted are computed in an on-policy manner. (<bold>D</bold>) Cumulative pain accrued by fixed and flexible <italic>ω</italic> agents whilst learning over 1000 episodes as a measure of safety averaged over 10 runs. (<bold>E</bold>) Cumulative steps required to reach the fixed goal by fixed and flexible <italic>ω</italic> agents whilst learning over 1000 episodes as a measure of sample efficiency, averaged over 10 runs (<bold>F</bold>) Plot of flexibly modulated <italic>ω</italic> arbitration parameter over the learning duration averaged over 10 runs. This shows a transition from a higher Pavlovian bias to a more instrumental agent over episodes as learning about the environment reduces uncertainty. (<bold>G</bold>) Comparison of different agents using a trade-off metric and to be used only for didactic purposes (using <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> and more details in Materials and methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-fig2-v1.tif"/></fig><p><xref ref-type="fig" rid="fig2">Figure 2D</xref> plots cumulative pain accrued over multiple episodes during learning and is our measure of safety. <xref ref-type="fig" rid="fig2">Figure 2E</xref> plots cumulative steps or environment interactions over episodes and is our measure of sample efficiency. Here, sampling efficiency is represented by the total number of environment interactions or samples required to reach the rewarding goal which terminates the episode. Simply, if an agent requires more samples to reinforce and acquire the rewarding goal, it is less efficient.</p><p>The simulation results with a fixed Pavlovian influence (<xref ref-type="fig" rid="fig2">Figure 2D and E</xref>) show that adding a Pavlovian fear system to the instrumental system makes it safer in the sense that it achieves the goal of solving the environment while accruing lesser cumulative pain over episodes. However, we observe that as the influence of the Pavlovian fear system increases, with an increase in <italic>ω</italic>, it achieves safety at the expense of sample efficiency (within reasonable bounds such as until <italic>ω</italic>=0.5). Whereas under very high Pavlovian fear influence (<italic>ω</italic>=0.9), the agent loses sight of the rewarding goal and performs poorly in terms of both safety and efficiency as the episode doesn’t terminate until it finds the goal.</p><p>However, the flexible omega policy (with <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$\alpha_{\Omega}= 0.6$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>6.5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$\kappa = 6.5$\end{document}</tex-math></alternatives></inline-formula>) achieves safety almost comparable to <italic>ω</italic>=0.5, which is the safest fixed ω policy amongst <italic>ω</italic>=0.1, 0.5, 0.9 at a much higher efficiency than <italic>ω</italic>=0.5, 0.9, thus improving the safety-efficiency trade-offs (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). In this way, the PAL model encourages cautious exploration early on when uncertainty is higher and reduces the Pavlovian biases as the uncertainty is resolved (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). The flexible <italic>ω</italic> value at convergence depends on the environment statistics: transition probabilities and reward/punishment magnitudes. We utilise a simple linear scaling of associability clipped at 1 to arrive at arbitrator <italic>ω</italic> (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>) instead of another alternative such as sigmoid to avoid additional unnecessary meta parameters (i.e. bias shift) to be tuned. In this environment, the value at convergence is <italic>ω</italic>=0.42, due to some irreducible uncertainty in state transitions (10% chance of incorrect transition). The differences in learned instrumental value functions between PAL and a purely instrumental agent are visible in <xref ref-type="fig" rid="fig2">Figure 2C</xref> showing how the Pavlovian bias sculpts instrumental value acquisition.</p><p>In the Appendix, we provide additional simulations that show the robustness of these results with respect to metaparameters <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> and <italic>κ</italic> (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), environments in which the reward locations vary (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), and other grid world environments (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>).</p></sec><sec id="s2-3"><title>Experiment 2: Constant Pavlovian bias introduces sampling asymmetry and affects instrumental value propagation</title><p>Observing the differences in the on-policy value functions with and without the Pavlovian influence (<xref ref-type="fig" rid="fig2">Figure 2</xref>) prompted us to further tease apart the effect of constant Pavlovian bias on sampling asymmetry, and consequent differences in instrumental value discovery and value propagation through the states. We investigated how different fixed values of <italic>ω</italic> can lead to sampling asymmetry, which refers to exploration where certain states are visited or sampled unevenly compared to others. In this set of results, we wish to qualitatively tease apart the role of a Pavlovian bias in shaping and sculpting the instrumental value and also provide more insight into the resulting safety-efficiency trade-off. Having shown the benefits of a flexible <italic>ω</italic> in the previous section, here we only vary the fixed <italic>ω</italic> to illustrate the effect of a constant bias and are not concerned with the flexible bias in this experiment.</p><p>We tested agents with different fixed <italic>ω</italic> in two simulated environments: (1) A T-maze and (2) a three-route task. The T-maze task environment (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) has asymmetric rewards (<inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$R=+0.1$\end{document}</tex-math></alternatives></inline-formula> on the left, whereas <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$R=+1$\end{document}</tex-math></alternatives></inline-formula> on the right). However, the agent will have to walk through a patch of painful states to reach the larger goal on the right; even the safest path will incur a damage of at least <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$R=-0.5$\end{document}</tex-math></alternatives></inline-formula> or worse. Taking discounting into account, the goal on the right is marginally better than the one on the left, and the instrumental agent achieves both of the goals nearly an equal number of times (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Comparing the instrumental agent with other agents in <xref ref-type="fig" rid="fig3">Figure 3C</xref> shows diminished positive (reward) value propagation leading to the <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$R=1$\end{document}</tex-math></alternatives></inline-formula> goal on the right as the constant Pavlovian bias increases, showing how such sampling asymmetry can prevent value discovery of states leading to <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$R=1$\end{document}</tex-math></alternatives></inline-formula> goal. The safety efficiency trade-off can also be observed through <xref ref-type="fig" rid="fig3">Figure 3B</xref>. This illustrates one of the main tenets of our model - that having a Pavlovian fear system ensures a separate ‘un-erasable’ fear/punishment memory which makes the agent more avoidant to punishments. This is helpful in softly ensuring an upper bound on losses, by (conservatively) foregoing decisions resulting in immediate loss, but followed by much larger rewards. This is where the safety-efficiency trade-off marks a clear distinction from the exploration-exploitation trade-off, in which earlier losses can be overcome by gains later on.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Demonstration of sampling asymmetry due to constant Pavlovian bias.</title><p>(<bold>A</bold>) T-maze grid world environment with annotated rewards and punishments. (<bold>B</bold>) Proportion of the rewarding goal chosen by the agent. (<bold>C</bold>) Value function plots for <italic>ω</italic>=0,0.1,0.5,0.9 show diminished value propagation from the reward on the right. (<bold>D</bold>) Grid world environment with three routes with varying pain. (<bold>E</bold>) Cumulative steps required to reach the goal vs cumulative pain accrued by fixed <italic>ω</italic> agents ranging from <italic>ω</italic>=10 to <italic>ω</italic>=0.9. (<bold>F</bold>) State visit count plots for <italic>ω</italic>=0, 0.5, 0.9, i.e., instrumental and constant Pavlovian bias agents. (<bold>F</bold>) Value function plots for <italic>ω</italic>=0, 0.5, 0.9.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-fig3-v1.tif"/></fig><p>The three-route task simulation includes three routes with varying degrees of punishments (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), inspired by previous manipulandum tasks (<xref ref-type="bibr" rid="bib25">Glogan et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Meulders et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">van Vliet et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">van Vliet et al., 2021</xref>). We observe that increasing the constant Pavlovian bias up until <italic>ω</italic>=0.7 leads to increased safety (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Beyond <italic>ω</italic>=0.7, a high fixed Pavlovian bias may incur unnecessarily high cumulative pain and steps as its reward value propagation is diminished (<xref ref-type="fig" rid="fig3">Figure 3G</xref>) and attempts to restrict itself to pain-free states (<xref ref-type="fig" rid="fig3">Figure 3F</xref>) whilst searching for reward (despite stochastic transitions which may lead to slightly more painful encounters in the long run). Comparing the cumulative state visit plots of <xref ref-type="fig" rid="fig3">Figure 3F</xref>, the instrumental agent with an agent with high constant Pavlovian bias <italic>ω</italic>=0.9, we observe that the latter showed an increased sampling of the states on the longest route with no punishments. Comparing the value function plots (<xref ref-type="fig" rid="fig3">Figure 3G</xref>), we observe that a high constant Pavlovian bias impairs the value propagation (it is more diffused) of the rewarding goal in comparison to an instrumental agent. Such high levels of constant Pavlovian bias can be a model of maladaptive anxious behaviour.</p><p>In conclusion, the simulations with this environment show that the Pavlovian fear system can assist in avoidance acquisition; however, a constant Pavlovian bias, depending on the degree of bias, leads to sampling asymmetry and impaired value propagation.</p><p><xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref> includes the performance comparison of agents with a suitable flexible <italic>ω</italic> and with fixed <italic>ω</italic> values on the three-route task. <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> shows the results of a human experiment with subjects navigating a three-route virtual reality maze similar to the simulated one.</p></sec><sec id="s2-4"><title>Experiment 3: Human approach-withdrawal conditioning is modulated by outcome uncertainty</title><p>Our first experiment showed the benefit of having an outcome uncertainty-based flexible <italic>ω</italic> arbitration scheme in balancing safety and efficiency, in a series of grid worlds. In this next experiment, we aimed to find basic evidence that humans employ such a flexible fear commissioning scheme. This is not intended as an exhaustive test of all predictions of the model, but to show in principle that there are situations in which a flexible, rather than fixed Pavlovian influence, provides a good fit to real behavioural data. In line with our grid world simulations, we expected a Pavlovian bias in choices, but in addition to it, we also expected a Pavlovian bias in RTs.</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> describes the trial protocol (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), block protocol (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), and experimental setup (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). We conducted a VR-based approach-avoidance task (28 healthy subjects, of which 14 females and average age 27.96 years) inspired by previous Go-No Go task studies for isolating Pavlovian bias, especially its contributions to misbehaviour (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>; <xref ref-type="bibr" rid="bib24">Gershman et al., 2021</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>; <xref ref-type="bibr" rid="bib47">Mkrtchian et al., 2017a</xref>; <xref ref-type="bibr" rid="bib48">Mkrtchian et al., 2017b</xref>). The subject’s goal was to make a correct approach or withdrawal decision to avoid pain, with four different cues associated with different probabilities of neutral or painful outcomes. We expected the Pavlovian misbehaviour to cause incorrect withdrawal choices for cues where the correct response would be to approach. And in terms of RTs, we expected the bias to slow down correct approach responses and speed up correct withdrawal responses. We explicitly attempted to change the outcome uncertainty or controllability, in a similar way to previous demonstrations (<xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>), but with controllability changing <italic>within</italic> the task. To do this, we set up two of the four cues to be uncontrollable in the first half (i.e. outcome is painful 50% of the times regardless of the choice), but which then become controllable in the second half (i.e. the correct choice will avoid the pain 80% of the times). We anticipated that the Pavlovian bias in choice and RTs would be modulated along with the change in uncontrollability. The virtual reality environment improves ecological validity (<xref ref-type="bibr" rid="bib57">Parsons, 2015</xref>) and introduces gamification, which is known to also improve reliability of studies (<xref ref-type="bibr" rid="bib35">Kucina et al., 2023</xref>; <xref ref-type="bibr" rid="bib62">Sailer et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">Zorowitz et al., 2023</xref>), which is important in attempts to uncover potentially subtle biases.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>An illustration of the VR Approach-Withdrawal task and trial and block protocols.</title><p>(<bold>A</bold>) Trial protocol: The participant is expected to take either an approach action (touch the jellyfish) or withdrawal action (withdraw the hand towards oneself) within the next 2.5 s once the jellyfish changes colour. The participant was requested to bring their hand at the centre of a bubble located halfway between the participant and the jellyfish to initiate the next trial where a new jellyfish would emerge (<ext-link ext-link-type="uri" xlink:href="https://youtu.be/2J9994gCwH8">video</ext-link>). (<bold>B</bold>) Block protocol: First half of the trials had two uncontrollable cues and two controllable cues, and the second half had all controllable cues with aforementioned contingencies. The main experiment 240 trials were preceded by 10 practice trials which do not count towards the results. (<bold>C</bold>) Illustration of experimental setup. VR: virtual reality, WASP: surface electrode for electrodermal stimulation, DS-5: constant current stimulator, GSR: galvanic skin response sensors, HR: heart rate sensor, EMG: electromyography sensors, EEG: electroencephalogram electrodes, Liveamp: wireless amplifier for mobile EEG.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-fig4-v1.tif"/></fig><p>We observe that all subjects learn to solve the task well and solve it better than chance (i.e. fewer than 120 shocks in 240 trials). Out of 240 trials, they receive 88.96 shocks on average (std. deviation = 12.62). We first attempted to test our hypotheses using behavioural metrics of Pavlovian withdrawal bias in choices and RTs. However, our behavioural choice-based metrics cannot distinguish a random exploratory action from Pavlovian misbehaviour. Further, it cannot account for effects of a non-Pavlovian baseline bias <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>. Thus, we did not find any statistically significant result due to noisy behavioural metrics, results, and more information provided in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>.</p><p>We next aimed to test our hypotheses by model comparison of RL models (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and RLDDM models (<xref ref-type="fig" rid="fig5">Figure 5E</xref>) which guides our results below. We used a hierarchical Bayesian estimation of model parameters to increase the reliability across tasks. We found that the baseline action bias <inline-formula><alternatives><mml:math id="inf17"><mml:mi>b</mml:mi></mml:math><tex-math id="inft17">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>, instrumental learning, and the Pavlovian withdrawal bias competed for behavioural control, as observed in previous studies (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>) (parameter distribution plots in <xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref>). However, unlike previous studies that have treated Pavlovian bias as fixed, we found that the flexible Pavlovian bias better explained the behavioural data; please see <xref ref-type="fig" rid="fig5">Figure 5B and F</xref>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>RL and RLDDM model fitting results on VR Approach-Withdrawal task.</title><p>The left panels show choice model fit results using reinforcement learning (RL) models. The right panels show choice and reaction times (RTs) model fit results using reinforcement learning diffusion decision-making (RLDDM) models. (<bold>A</bold>) Simplified RL model from <xref ref-type="fig" rid="fig1">Figure 1</xref> for the approach-withdrawal task. (<bold>B</bold>) Model comparison shows that the model with flexible Pavlovian bias fits best to choices in terms of leave-one-out information criteria (LOOIC). (<bold>C</bold>) Flexible <italic>ω</italic> from the RL model over 240 trials for 28 participants. (<bold>D</bold>) Number of approaches aggregated over all subjects and all trials in data and model predictions by the RL model with flexible.<italic>ω</italic> (normalised to 1). (<bold>E</bold>) Simplified illustration of RLDDM for the approach-withdrawal task, where the baseline bias <inline-formula><alternatives><mml:math id="inf18"><mml:mi>b</mml:mi></mml:math><tex-math id="inft18">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> and the Pavlovian bias <inline-formula><alternatives><mml:math id="inf19"><mml:mrow><mml:mi>ω</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math><tex-math id="inft19">\begin{document}$\omega V_{p}(s)$\end{document}</tex-math></alternatives></inline-formula> is also included in the drift rate. (The base figure is reproduced from Figure 8 from <xref ref-type="bibr" rid="bib12">Desch et al., 2022</xref> with modifications.) (<bold>F</bold>) Model comparison shows that the model with flexible Pavlovian bias fits best to choices and RTs in terms of LOOIC. (<bold>G</bold>) Flexible <inline-formula><alternatives><mml:math id="inf20"><mml:mi>ω</mml:mi></mml:math><tex-math id="inft20">\begin{document}$\omega$\end{document}</tex-math></alternatives></inline-formula> from the RLDDM over 240 trials for 28 participants. (<bold>H</bold>) Distribution of approach and withdrawal RTs aggregated over all subjects and trials in data and model predictions by the RLDDM with flexible <italic>ω</italic>. The bump in RTs at 2.5 s is because of timeout (inactive approaches and withdrawals, please see <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-fig5-v1.tif"/></fig><p>Similar to <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>, the simple RW learning model represented the base model. RW+bias includes a baseline bias <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> that can take any positive or negative value; positive value denoting a baseline bias for approach and negative denoting a baseline bias for withdrawal. From group-level and subject-level parameter distribution plots (<xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref>), we observe that this baseline bias is for approach for most subjects. This is in line with previous studies (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>) and as suggested by our data showing a significant baseline difference in the number of approaches and withdrawal actions across all subjects and trials (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5A</xref>). Note that here, this baseline bias is not learned as it is with a Pavlovian bias. RW+bias+Pavlovian (fixed) model includes a fixed Pavlovian bias and is most similar to models by <xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>, which also used reward and punishment sensitivities for the instrumental learning but did not scale the instrumental values by <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$(1-\omega)$\end{document}</tex-math></alternatives></inline-formula> as done in our model. Our models do not have reward and punishment sensitivities. From group-level and subject-level parameter distribution plots (<xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref>), we observe that the distribution of fixed <italic>ω</italic> is significantly positive and non-zero. RW+bias+Pavlovian (flexible) model includes a flexible Pavlovian bias as per our proposed associability-based arbitration scheme. We found that the flexible <italic>ω</italic> model fits significantly better than the fixed <inline-formula><alternatives><mml:math id="inf23"><mml:mi>ω</mml:mi></mml:math><tex-math id="inft23">\begin{document}$\omega$\end{document}</tex-math></alternatives></inline-formula> model (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), i.e., the flexible <italic>ω</italic> model has the lowest leave-one-out information criteria (LOOIC) score amongst models compared. By comparing incremental improvements in LOOIC, we observe that adding the baseline bias term leads to the most improvement in model fit, followed by changing the fixed <italic>ω</italic> to a flexible <italic>ω</italic> scheme. Here, we plot LOOIC for model comparison, but <xref ref-type="table" rid="app1table1 app1table2">Appendix 1—tables 1 and 2</xref> include both LOOIC and Watanabe-Aikake information criterion (WAIC) scores, showing the same result. Further, it can be seen that <italic>ω</italic> tracks associability, which decreases over the trials (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) (which also resembles <xref ref-type="fig" rid="fig2">Figure 2E</xref>). <xref ref-type="fig" rid="fig5">Figure 5D</xref> shows a plot comparing the number of approaches (normalised to 1), aggregated over all subjects and trials by cue types for data and the best fitting model predictions. We observe qualitatively that the subjects learn to perform the correct actions for each cue and that the model predictions qualitatively reproduce the data.</p><p>We then extend the model fitting to also incorporate RTs, using an RLDDM (<xref ref-type="bibr" rid="bib12">Desch et al., 2022</xref>; <xref ref-type="bibr" rid="bib19">Fontanesi et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">Pedersen et al., 2017</xref>). The propensities calculated using the RL model are now used as drift rates in the DDM, and the RTs are calculated using a Wiener distribution for a diffusion-to-bound process. Thus, the drift rate is proportional to the difference in propensities between approach and withdrawal action. Since Pavlovian bias is also dependent on punishment value, similar to instrumental values, we included the Pavlovian bias and the baseline bias in the drift rate. Thus, the best propensity for an action in choice selection in RL models drives the drift rate in our RLDDM models. We found that the RLDDM replicates the results for model fitting (<xref ref-type="fig" rid="fig5">Figure 5E</xref>) and flexible <italic>ω</italic> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). <xref ref-type="fig" rid="fig5">Figure 5H</xref> shows the distribution for approach and withdrawal RTs aggregated over all subjects, over all trials, in data and model predictions. The data shows that the withdrawal RTs are slightly faster than approach RTs (<xref ref-type="fig" rid="fig5">Figure 5H</xref>) and that the best fitting model captures the withdrawal RT distribution well, but can be improved in the future to capture approach RT distribution better.</p><p><xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref> includes behavioural results for the experiment data. <xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref> include group-level and subject-level (hierarchically fitted) model parameter distributions. <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> and <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> mention model parameters with LOOIC and WAIC values for all RL models and RLDDM models. We observed that all Rhat values were strictly less than 1.05 (most parameters were less than 1.01 and generally close to 1), indicating that the models had converged.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In summary, this paper shows that addition of a fear-learning system, implemented as a Pavlovian controller in a multi-attribute RL architecture, improves safe exploratory decision-making with little cost of sample efficiency. Employing a flexible arbitration scheme where Pavlovian responses are gated by outcome uncertainty (<xref ref-type="bibr" rid="bib3">Bach and Dolan, 2012</xref>) provides a neurally plausible approach to solving the safety-efficiency dilemma. Our experimental results support the hypothesis of such a flexible fear commissioning scheme and suggest that inflexible Pavlovian bias can explain certain aspects of maladaptive ‘anxious’ behaviour (please see <xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>). This can be helpful in making novel predictions in clinical conditions, including maladaptive persistent avoidance in chronic pain in which it may be difficult to ‘unlearn’ an injury.</p><p>Broadly, our model sits amidst the landscape of safe RL (<xref ref-type="bibr" rid="bib21">Garcıa and Fernández, 2015</xref>). In principle, it can be viewed through the lens of constrained Markov decision processes (<xref ref-type="bibr" rid="bib2">Altman, 1999</xref>), where the Pavlovian fear system is dedicated towards keeping constraint violations at a minimum. In the realm of safe learning, there exists a dichotomy: one can either apply computer science-driven approaches to model human and animal behaviour, as seen in optimising worst-case scenarios (<xref ref-type="bibr" rid="bib27">Heger, 1994</xref>) and employing beta-pessimistic <italic>Q</italic>-learning (<xref ref-type="bibr" rid="bib22">Gaskett, 2003</xref>) for modelling anxious behaviour (<xref ref-type="bibr" rid="bib83">Zorowitz et al., 2020</xref>), or opt for neuro-inspired algorithms and demonstrate their utility in safe learning. Our model falls into the latter category, draws inspiration from the extensive literature on Pavlovian-instrumental interactions (<xref ref-type="bibr" rid="bib8">Brown and Jenkins, 1968</xref>; <xref ref-type="bibr" rid="bib31">Kamin et al., 1963</xref>; <xref ref-type="bibr" rid="bib40">Mackintosh, 1983</xref>; <xref ref-type="bibr" rid="bib42">Maia, 2010</xref>; <xref ref-type="bibr" rid="bib53">Mowrer, 1960</xref>; <xref ref-type="bibr" rid="bib52">Mowrer, 1951</xref>; <xref ref-type="bibr" rid="bib60">Prévost et al., 2012</xref>; <xref ref-type="bibr" rid="bib69">Talmi et al., 2008</xref>), fear conditioning (<xref ref-type="bibr" rid="bib36">LaBar et al., 1998</xref>), and punishment-specific prediction errors (<xref ref-type="bibr" rid="bib6">Berg et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Elfwing and Seymour, 2017</xref>; <xref ref-type="bibr" rid="bib59">Pessiglione et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Roy et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Seymour et al., 2007</xref>; <xref ref-type="bibr" rid="bib64">Seymour et al., 2012</xref>; <xref ref-type="bibr" rid="bib79">Watabe-Uchida and Uchida, 2018</xref>), and elucidates a safety-efficiency trade-off. Classical theories of avoidance such as two-factor theory (<xref ref-type="bibr" rid="bib52">Mowrer, 1951</xref>), and indeed actor-critic models (<xref ref-type="bibr" rid="bib42">Maia, 2010</xref>), intrinsically invoke Pavlovian mechanisms in control, although primarily to act as a teaching signal for instrumental control as opposed to directly biasing action propensities such as in our case or (<xref ref-type="bibr" rid="bib11">Dayan et al., 2006</xref>). Recent studies in computer science, particularly those employing policy optimisation (gradient-based) RL under CMDPs (<xref ref-type="bibr" rid="bib2">Altman, 1999</xref>), have also observed a similar safety-efficiency trade-off (<xref ref-type="bibr" rid="bib51">Moskovitz et al., 2023</xref>). Additionally, the fundamental trade-off demonstrated by <xref ref-type="bibr" rid="bib16">Fei et al., 2020</xref>, between risk sensitivity (with exponential utility) and sample efficiency in positive rewards aligns with our perspective on the safety-efficiency trade-off, especially when broadening our definition of safety beyond cumulative pain to include risk considerations. Safety-efficiency trade-offs may also have a close relationship with maladaptive avoidance (<xref ref-type="bibr" rid="bib5">Ball and Gunaydin, 2022</xref>) often measured in clinical anxiety, and our work provides insights into the maladaptive avoidance via the heightened threat appraisal pathway. Similar safe exploration behaviour in choices could be achievable using a risk-sensitive criterion such as conditional value at risk (CVaR) that relies only on the instrumental systems without needing a Pavlovian system. However, these work in different ways. Embracing the decision-theoretic psychiatry framework by <xref ref-type="bibr" rid="bib29">Huys et al., 2015</xref>, which attempts to categorise dysfunctions as the agent either solving the problem with a wrong solution or solving the wrong problem correctly, or solving the right problem correctly but in an unfortunate or wrong environment, then we see the following. CVaR provides the correct solution to the wrong problem (an objective that only maximises the lower tail of the distribution of outcomes). In contrast, the Pavlovian bias provides the wrong solution to the correct problem (normative objective) (<xref ref-type="bibr" rid="bib29">Huys et al., 2015</xref>). Further, approaches such as CVaR might not be the best approach to capture the Pavlovian withdrawal bias effect we find in RTs.</p><p>When it comes to our experiments, both the simulation and VR experiment models are related and derived from the same theoretical framework, maintaining an algebraic mapping. They differ only in task-specific adaptations, i.e., differ in action sets and differ in temporal difference learning rules - multi-step decisions in the grid world vs RW rule for single-step decisions in the VR task. This is also true for <xref ref-type="bibr" rid="bib11">Dayan et al., 2006</xref>, who bridge Pavlovian bias in a Go-No Go task (negative auto-maintenance pecking task) and a grid world task. A further minor difference between the simulation and VR experiment models is the use of a baseline bias in the human experiment’s RL and the RLDDM model, where we also model RTs with drift rates, which is not a behaviour often simulated in the grid world simulations. As mentioned previously, we use the grid world tasks for didactic purposes, similar to <xref ref-type="bibr" rid="bib11">Dayan et al., 2006</xref>, and common to test-beds for algorithms in RL (<xref ref-type="bibr" rid="bib67">Sutton and Barto, 1998</xref>). The main focus of our work is on Pavlovian fear bias in safe exploration and learning, rather than on its role in complex navigational decisions. Future work can focus on capturing more sophisticated safe behaviours, such as escapes (<xref ref-type="bibr" rid="bib15">Evans et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Sporrer et al., 2023</xref>) and model-based planning, which span different aspects of the threat-imminence continuum (<xref ref-type="bibr" rid="bib49">Mobbs et al., 2020</xref>).</p><p>In our simulation experiments, we assume the coexistence of the Pavlovian fear system and the instrumental system to demonstrate the emergent safety-efficiency trade-off from their interaction. It is possible that similar behaviours could be modelled using an instrumental system alone, with higher punishment sensitivity; therefore, we do not argue for the necessity for the Pavlovian fear system here. Instead, the Pavlovian fear system itself could be a potential biologically plausible implementation of punishment sensitivity. Unlike punishment sensitivity (scaling of the punishments), which has not been robustly mapped to neural substrates in fMRI studies, the neural substrates for the Pavlovian fear system are well known (e.g. the limbic loop and amygdala, further see <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>). Additionally, the Pavlovian fear system provides a separate punishment memory that cannot be erased by greater rewards like (<xref ref-type="bibr" rid="bib14">Elfwing and Seymour, 2017</xref>; <xref ref-type="bibr" rid="bib78">Wang et al., 2018</xref>). This fundamental point can be observed in our simple T-maze simulations, where the Pavlovian fear system encourages avoidance behaviour and the agent chooses the smaller reward instead of the greater reward. We next discuss the plausibility of pre-training to select the hardwired actions. In the human experiment, the withdrawal action is straightforwardly biased, as noted, while in the grid world, we assume a hardwired encoding of withdrawal actions for each state/grid. This innate encoding of withdrawal actions could be represented in the dPAG (<xref ref-type="bibr" rid="bib32">Kim et al., 2013</xref>). We implement this bias using pre-training, which we assume would be a product of evolution. Alternatively, this could be interpreted as deriving from an appropriate value initialisation where the gradient over initialised values determines the action bias. Such aversive value initialisation, driving avoidance of novel and threatening stimuli, has been observed in the tail of the striatum in mice, which is hypothesised to function as a Pavlovian fear/threat learning system (<xref ref-type="bibr" rid="bib43">Menegas et al., 2018</xref>).</p><p>We illustrate that a high Pavlovian impetus is characterised by reduced sample efficiency in learning, worsened/weakened (instrumental) value propagation and impervious rigidity in the policy, and misbehaviour due to misalignment of bias with the instrumental action. This way, it also further promotes short-term safer, smaller rewards opposed to long-term higher rewards, echoing the idea of Pavlovian pruning of decision trees (<xref ref-type="bibr" rid="bib28">Huys et al., 2012</xref>). The idea of alignment between the Pavlovian and instrumental actions leading to harm-avoiding safe behaviours and misalignment being the root of maladaptive behaviours was proposed by <xref ref-type="bibr" rid="bib48">Mkrtchian et al., 2017b</xref>, through a Go-No Go task with human subjects and the threat of shock responsible for the Pavlovian instrumental transfer. Recently, <xref ref-type="bibr" rid="bib80">Yamamori et al., 2023</xref>, have developed a restless bandit-based approach-avoidance task to capture anxiety-related avoidance, by using the ratio of reward and punishment sensitivities as a computational measure of approach-avoidance conflict. We show in our simulations that misalignment can also lead to safe behaviours, but at the cost of efficiency. But having a flexible fear commissioning alleviates the majority of Pavlovian misbehaviour and in turn makes the agent more cautious in the face of uncertainty and catastrophe, contrasting with ‘optimism bias’ observed in humans (<xref ref-type="bibr" rid="bib65">Sharot, 2011</xref>). A limitation of our work would be that we do not model the endogenous modulation of pain and stress-induced analgesia which may have the opposite effect of the proposed uncertainty-based fear commissioning scheme. A limitation of our VR experiment is that we only consider uncertainty decrease from first half to second half. This was motivated to make it similar to the grid world simulations as well as to help with behavioural tests (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>), as this would keep all of the reducible and irreducible uncertainty in the first half and none in the second half. However, a stringent test would also require a balanced case, where the outcomes of cues 3 and 4 are more certain in the first half and more uncertain in the second half, or consider differentiating uncertainty and volatility.</p><p>While our flexible <italic>ω</italic> scheme, rooted in associability, shares motivation with <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>, to track uncontrollability, our approach differs. Unlike <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>, which employs a Bayesian arbitrator emphasising the most useful predictor (Pavlovian or instrumental), our Pearce-Hall associability-based measure provides a direct and separate controllability assessment. This distinction allows our measure to scale effectively to complex tasks, including grid world environments, maintaining stability throughout experiments. In contrast, the measure by <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>, exhibits notable variability, even when the controllability of the cue-outcome pair remains constant throughout the task. Previous fMRI studies have associated associability signals with the amygdala (<xref ref-type="bibr" rid="bib81">Zhang et al., 2016</xref>) and pgACC (<xref ref-type="bibr" rid="bib82">Zhang et al., 2018</xref>) control. Additionally, outcome uncertainty computation could possibly be performed within the basal ganglia using scaled prediction errors (<xref ref-type="bibr" rid="bib46">Mikhael and Bogacz, 2016</xref>; <xref ref-type="bibr" rid="bib50">Möller et al., 2022</xref>) and is encoded in the firing rates of orbitofrontal cortex neurons and possibly in slow ramping activity in dopaminergic midbrain neurons (<xref ref-type="bibr" rid="bib3">Bach and Dolan, 2012</xref>; <xref ref-type="bibr" rid="bib18">Fiorillo et al., 2003</xref>; <xref ref-type="bibr" rid="bib56">O’Neill and Schultz, 2010</xref>). Associability as a measure of outcome uncertainty, though very practical and useful on an implementational level, cannot distinguish between various kinds of uncertainties. Further, future work can help differentiate between controllability and predictability. <xref ref-type="bibr" rid="bib39">Ligneul et al., 2022</xref>, suggest that controllability and not predictability may arbitrate the dominance of Pavlovian vs instrumental control. Future work could also use a formal account of uncertainty, which could fit the fear-conditioned skin-conductance response better than Pearce-Hall associability (<xref ref-type="bibr" rid="bib72">Tzovara et al., 2018</xref>). Additionally, <xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>, demonstrated that theta-band oscillatory power in the frontal cortex tracks and overrides Pavlovian biases, later suggesting its connection to inferred controllability (<xref ref-type="bibr" rid="bib24">Gershman et al., 2021</xref>). Notably, <xref ref-type="bibr" rid="bib33">Kim et al., 2023</xref>, revealed that upregulation of the dorsolateral prefrontal cortex (dlPFC) through anodal transcranial direct current stimulation induces behavioural suppression or changes in Pavlovian bias in the punishment domain, implying a causal role of the dlPFC in Pavlovian-instrumental arbitration.</p><p>A natural clinical application (<xref ref-type="bibr" rid="bib20">Fullana et al., 2020</xref>) of this model is towards mechanistic models of anxiety and chronic pain. Quite simply, both have been considered as reflecting excessive Pavlovian punishment learning systems. In the case of anxiety disorder, this equates a strong influence of Pavlovian control with subjectively experienced anxiety symptomatology, leading to excessively defensive behaviour and avoidance of anxiogenic environments (<xref ref-type="bibr" rid="bib55">Norton and Paulus, 2017</xref>). In the case of chronic pain, the idea is that failure to overcome a Pavlovian incentive to avoid moving results in failure to discover that pain escape is possible (the fear avoidance model) (<xref ref-type="bibr" rid="bib77">Vlaeyen and Linton, 2000</xref>). In both cases, the pathological state can be considered a failure to turn down the Pavlovian system when the environment becomes more predictable (i.e. less uncertain). This illustrates a subtle distinction between existing theories that simply propose a constant excess Pavlovian influence, from the possibility they might result from a deficit in the flexible commission of Pavlovian control. This distinction can therefore be experimentally tested in clinical studies. Furthermore, accruing evidence also indicates a role of excessive Pavlovian punishment learning in models of depression (<xref ref-type="bibr" rid="bib30">Huys et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">Nord et al., 2018</xref>), suggesting that this may be a common mechanistic factor in comorbidity between chronic pain, anxiety, and depression. Recent experiments and perspectives also suggest a psychological mechanism of how avoidance in humans can lead to growth of anxiety (increased belief of threats) (<xref ref-type="bibr" rid="bib73">Urcelay, 2024</xref>). A key distinctive prediction of our model for an intervention is that we should help patient groups reduce Pavlovian bias not by training to reduce the bias, but rather by attempting to make the arbitration more flexible. This could potentially be done via some sort of controllability discrimination paradigm, i.e., helping distinguish between what is controllable and what is not - this is something also found in stoicism-based approaches to cognitive behavioural therapy (<xref ref-type="bibr" rid="bib70">Thorn and Dixon, 2007</xref>; <xref ref-type="bibr" rid="bib71">Turk and Rudy, 1992</xref>).</p><p>In conclusion, we outline how the Pavlovian fear system provides an important and computationally precise mechanism to shape or sculpt instrumental decision-making. This role for the Pavlovian fear system extends its utility far beyond merely representing the evolutionary vestiges of a primitive defence system, as sometimes portrayed. This opens avenues for future research in basic science of safe self-preserving behaviour (including in artificial systems), and clinical applications for mechanistic models of anxiety and chronic pain.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Instrumental learning and Pavlovian fear learning</title><p>We consider a standard RL setting in an environment containing reward and punishments (pain). In each time step <inline-formula><alternatives><mml:math id="inf24"><mml:mi>t</mml:mi></mml:math><tex-math id="inft24">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>, the agent observes a state <inline-formula><alternatives><mml:math id="inf25"><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><tex-math id="inft25">\begin{document}$s_{t}$\end{document}</tex-math></alternatives></inline-formula> and selects an action <inline-formula><alternatives><mml:math id="inf26"><mml:mstyle><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$a_{t}$\end{document}</tex-math></alternatives></inline-formula> according to its stochastic policy <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$\pi_{t}(s_{t}, a_{t})$\end{document}</tex-math></alternatives></inline-formula> (i.e. the probability of selecting action <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$a_{t}= a$\end{document}</tex-math></alternatives></inline-formula> in state <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$s_{t}= s$\end{document}</tex-math></alternatives></inline-formula>). The environment then makes a state transition from the current state <inline-formula><alternatives><mml:math id="inf30"><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><tex-math id="inft30">\begin{document}$s_{t}$\end{document}</tex-math></alternatives></inline-formula> to the next state <inline-formula><alternatives><mml:math id="inf31"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft31">\begin{document}$s_{t+1}$\end{document}</tex-math></alternatives></inline-formula> and the agent receives a scalar reward <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$R_{t+1}\in (-\infty, +\infty)$\end{document}</tex-math></alternatives></inline-formula>. This represents that the scalar reward includes both positive rewards and negative rewards or punishments. We use the standard notation used by <xref ref-type="bibr" rid="bib68">Sutton, 2018</xref>.</p><p>In the instrumental system, we define the value of taking action <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> in state <inline-formula><alternatives><mml:math id="inf34"><mml:mi>s</mml:mi></mml:math><tex-math id="inft34">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> under a policy <italic>π</italic>, denoted as the action-value function <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$Q^{\pi}(s, a)$\end{document}</tex-math></alternatives></inline-formula><italic>,</italic> as the expected return starting from <inline-formula><alternatives><mml:math id="inf36"><mml:mi>s</mml:mi></mml:math><tex-math id="inft36">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, taking the action <inline-formula><alternatives><mml:math id="inf37"><mml:mi>a</mml:mi></mml:math><tex-math id="inft37">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula>, and thereafter following policy <italic>π</italic>:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="italic">R</mml:mi><mml:mrow><mml:mi mathvariant="italic">t</mml:mi><mml:mo mathvariant="italic">+</mml:mo><mml:mi mathvariant="italic">k</mml:mi><mml:mo mathvariant="italic">+</mml:mo><mml:mn mathvariant="italic">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo mathvariant="italic" stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="italic">s</mml:mi><mml:mi mathvariant="italic">t</mml:mi></mml:msub><mml:mo mathvariant="italic">=</mml:mo><mml:mi mathvariant="italic">s</mml:mi><mml:mo mathvariant="italic">,</mml:mo><mml:msub><mml:mi mathvariant="italic">a</mml:mi><mml:mi mathvariant="italic">t</mml:mi></mml:msub><mml:mo mathvariant="italic">=</mml:mo><mml:mi mathvariant="italic">a</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}\it{R_{t+k+1}|s_t = s, a_t = a}\right],$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$R_{t+k+1}$\end{document}</tex-math></alternatives></inline-formula> is the scalar reward received <inline-formula><alternatives><mml:math id="inf39"><mml:mi>k</mml:mi></mml:math><tex-math id="inft39">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> time steps in the future, when evaluating the <italic>Q</italic>-values at time steps <inline-formula><alternatives><mml:math id="inf40"><mml:mi>t</mml:mi></mml:math><tex-math id="inft40">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>. The discount factor is <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$\gamma$\end{document}</tex-math></alternatives></inline-formula> and the reward <inline-formula><alternatives><mml:math id="inf42"><mml:mi>k</mml:mi></mml:math><tex-math id="inft42">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> time steps into the future is discounted by <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$\gamma^{k}$\end{document}</tex-math></alternatives></inline-formula>. The optimal action-value function is defined as <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$Q^{*}(s,a) = \max_{\pi}Q^{\pi}(s,a)$\end{document}</tex-math></alternatives></inline-formula>. Note that these are purely instrumental <italic>Q</italic>-values and do not include the Pavlovian bias.</p><p>In addition to the instrumental system, we define a Pavlovian fear (i.e. punishment/pain) system over and above the instrumental system which makes it safer. The Pavlovian fear systems aim to increase the impetus of the pain-avoidance actions that minimise pain. For that, we split the standard reward <inline-formula><alternatives><mml:math id="inf45"><mml:mi>R</mml:mi></mml:math><tex-math id="inft45">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> and only extract the punishment feedback signal <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$p \geq 0$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle p = -\min(R,0), $$\end{document}</tex-math></alternatives></disp-formula></p><p>We can similarly define a Pavlovian reward system trained on <inline-formula><alternatives><mml:math id="inf47"><mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$max(R,0)$\end{document}</tex-math></alternatives></inline-formula>, however, that’s not relevant to the questions of this study, so we will only focus on the arbitration between the instrumental (state-action based) model-free and Pavlovian (state-based) fear system. And we define the pain state-value <inline-formula><alternatives><mml:math id="inf48"><mml:mstyle><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$V_{p}(s)$\end{document}</tex-math></alternatives></inline-formula> of the Pavlovian fear system as follows:<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="italic">p</mml:mi><mml:mrow><mml:mi mathvariant="italic">t</mml:mi><mml:mo mathvariant="italic">+</mml:mo><mml:mi mathvariant="italic">k</mml:mi><mml:mo mathvariant="italic">+</mml:mo><mml:mn mathvariant="italic">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo mathvariant="italic" stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="italic">s</mml:mi><mml:mi mathvariant="italic">t</mml:mi></mml:msub><mml:mo mathvariant="italic">=</mml:mo><mml:mi mathvariant="italic">s</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  V_{p}^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}\it{p_{t+k+1}|s_t = s}\right],$$\end{document}</tex-math></alternatives></disp-formula></p><p>The subset of actions with the Pavlovian bias <inline-formula><alternatives><mml:math id="inf49"><mml:mstyle><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$A_{p}$\end{document}</tex-math></alternatives></inline-formula> are arrived at using a pre-training in the same environment with only punishments and random starting points. <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$V_{p}(s)$\end{document}</tex-math></alternatives></inline-formula> then biases this pre-trained subset of actions <inline-formula><alternatives><mml:math id="inf51"><mml:mstyle><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$A_{p}$\end{document}</tex-math></alternatives></inline-formula> according to <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>.</p><p>Here onwards, we will drop the time subscript for simplicity and write value update equations considering the state transitions from <inline-formula><alternatives><mml:math id="inf52"><mml:mi>s</mml:mi></mml:math><tex-math id="inft52">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf53"><mml:msup><mml:mi>s</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math><tex-math id="inft53">\begin{document}$s'$\end{document}</tex-math></alternatives></inline-formula>. The Pavlovian fear state-value functions are updated as follows:<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle V_{p}(s) := V_{p}(s) + \alpha (p + \gamma V_{p}(s') - V_{p}(s)).$$\end{document}</tex-math></alternatives></disp-formula></p><p>The instrumental value function for qualitative value plots is updated in an on-policy manner as follows (but is not used in the PAL algorithm):<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle V(s) := V(s) + \alpha (R + \gamma V(s') - V(s)).$$\end{document}</tex-math></alternatives></disp-formula></p><p>And the instrumental action-value functions are updated as follows:<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle Q(s,a) := Q(s,a) + \alpha (\delta),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>α</italic> is the learning rate and while using off-policy <italic>Q</italic>-learning (sarsamax) algorithm, the TD-errors are calculated as follows:<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle \delta = R + \gamma Q(s', \text{argmax}_{a'}(Q(s',a'))) - Q(s,a) .$$\end{document}</tex-math></alternatives></disp-formula></p><p>The equations above are valid for a general case and are used in grid world simulations. For model-fitting purposes for the VR approach-withdrawal task, there is no next state <inline-formula><alternatives><mml:math id="inf54"><mml:msup><mml:mi>s</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math><tex-math id="inft54">\begin{document}$s'$\end{document}</tex-math></alternatives></inline-formula>, thus the equations reduce to a simpler form of the RW learning rule.</p></sec><sec id="s4-2"><title>Action selection</title><p>Let <inline-formula><alternatives><mml:math id="inf55"><mml:mstyle><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> be the action set. In the purely instrumental case, propensities <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$\rho(s,a)$\end{document}</tex-math></alternatives></inline-formula> of actions <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}$a \in A$\end{document}</tex-math></alternatives></inline-formula> in state <inline-formula><alternatives><mml:math id="inf58"><mml:mi>s</mml:mi></mml:math><tex-math id="inft58">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> are the advantages of taking action <inline-formula><alternatives><mml:math id="inf59"><mml:mi>a</mml:mi></mml:math><tex-math id="inft59">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> in state <inline-formula><alternatives><mml:math id="inf60"><mml:mi>s</mml:mi></mml:math><tex-math id="inft60">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle \rho(s,a) = Q(s,a).$$\end{document}</tex-math></alternatives></disp-formula></p><p>And thus using softmax action selection with a Boltzmann distribution, the stochastic policy <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$\pi (a|s)$\end{document}</tex-math></alternatives></inline-formula> (probability of taking action <inline-formula><alternatives><mml:math id="inf62"><mml:mi>a</mml:mi></mml:math><tex-math id="inft62">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> in state <inline-formula><alternatives><mml:math id="inf63"><mml:mi>s</mml:mi></mml:math><tex-math id="inft63">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>) as follows:<disp-formula id="equ9"><label>(9)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  \pi (a|s) = \dfrac{e^{(\rho(s,a)/\tau)}}{\sum_{a' \in A} {e^{(\rho(s,a')/\tau)}}},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>τ</italic> is the temperature that controls the trade-off between exploration and exploitation. For grid world simulations, we use hyperbolic annealing of the temperature, where the temperature decreases after every episode <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ10"><label>(10)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>τ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>i</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle \tau (i) = \dfrac{\tau_0}{1+\tau_k i}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf65"><mml:mstyle><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$\tau_{0}$\end{document}</tex-math></alternatives></inline-formula> is the initial temperature and <inline-formula><alternatives><mml:math id="inf66"><mml:mstyle><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft66">\begin{document}$\tau_{k}$\end{document}</tex-math></alternatives></inline-formula> controls the rate of annealing. This is to ensure the policy converges in large state spaces like a grid world and follows previous studies (<xref ref-type="bibr" rid="bib14">Elfwing and Seymour, 2017</xref>; <xref ref-type="bibr" rid="bib78">Wang et al., 2018</xref>). For model fitting of the VR approach-withdrawal task, we do not anneal it and keep it as a free parameter (inverse temperature <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$\beta = 1/\tau$\end{document}</tex-math></alternatives></inline-formula>) to be fitted to each participant. This is also consistent with previous literature (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>; <xref ref-type="bibr" rid="bib24">Gershman et al., 2021</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>) and several other works modelling Go-No Go tasks.</p><p>In the case of a Pavlovian pain/fear system, let <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$A_{p}$\end{document}</tex-math></alternatives></inline-formula> be the subset of actions in state <inline-formula><alternatives><mml:math id="inf69"><mml:mi>s</mml:mi></mml:math><tex-math id="inft69">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> which has the Pavlovian pain urges or impetus associated with it. These are usually a small set of species-specific defensive reactions. In the VR approach-withdrawal task, we assume, or rather propose, it is the bias to withdraw from potentially harmful stimuli (in our case jellyfish). For the purpose of the grid world simulations, these can either be hardcoded geographical controller moving away from harmful states or through a pre-trained value-based controller (<xref ref-type="bibr" rid="bib11">Dayan et al., 2006</xref>). This work does not delve into the evolutionary acquisition of these biases, but one can derive the action subset <inline-formula><alternatives><mml:math id="inf70"><mml:mstyle><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft70">\begin{document}$A_{p}$\end{document}</tex-math></alternatives></inline-formula> from evolutionarily acquired value initialisations which may also help avoid novel stimuli and is a direction for future work.</p><p>Thus, after adding a Pavlovian fear system over and above the instrumental system, the propensities for actions are modified as follows:<disp-formula id="equ11"><label>(11)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>; where </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∖</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \rho(s,a_{n}) = (1- \omega)Q(s,a_{n}) \text{; where }a_{n}\in A_{n}= A \setminus A_{p}.$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ12"><label>(12)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>; where </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle \rho(s,a_{p}) = (1-\omega)Q(s,a_{p}) + \omega (V_{p}(s)) \text{; where }a_{p}\in A_{p}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The same can be compactly written as mentioned in the illustration (<xref ref-type="fig" rid="fig1">Figure 1</xref>):<disp-formula id="equ13"><label>(13)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>; where </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle \rho(s,a) = (1- \omega)Q(s,a) + \omega (V_{p}(s, a)) \text{; where }V_{p}(s, a) = \mathbb{I}[a=a_{p}] V_{p}(s). $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>ω</italic> is the parameter responsible for Pavlovian-instrumental transfer. These equations are constructed following the preceding framework by <xref ref-type="bibr" rid="bib11">Dayan et al., 2006</xref>, which laid out the foundation for interplay between Pavlovian reward system and the instrumental system. <inline-formula><alternatives><mml:math id="inf71"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">l</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi mathvariant="normal">∀</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft71">\begin{document}$\mathbb{l} [\cdot]=1 \forall a_{p}\in A_{p}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∀</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∖</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$\mathbb{I}[\cdot] = 0 \forall a_{n}\in A_{n}= A \setminus A_{p}$\end{document}</tex-math></alternatives></inline-formula> following the succinct vectorised notation by <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>.</p><p>We have referred to this algorithm as the PAL algorithm in this paper. The equations above assume only a Pavlovian fear system in addition to an instrumental system, and the given equations would vary depending on if we add a Pavlovian reward system too. After this modification, the action selection probabilities are calculated in a similar fashion as described in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>.</p></sec><sec id="s4-3"><title>Uncertainty-based modulation of <italic>ω</italic></title><p>We further modulate the parameter <italic>ω</italic> which is responsible for Pavlovian-instrumental transfer using perceived uncertainty in rewards. We use Pearce-Hall associability for this uncertainty estimation based on unsigned prediction errors (<xref ref-type="bibr" rid="bib34">Krugel et al., 2009</xref>; <xref ref-type="bibr" rid="bib81">Zhang et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Zhang et al., 2018</xref>). We maintain a running average of absolute TD-errors <italic>δ</italic> (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) at each state using the following update rule:<disp-formula id="equ14"><label>(14)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle \Omega_{t+1}= (1-\alpha_{\Omega}* \alpha) \Omega_{t}+ \alpha_{\Omega}* \alpha |\delta |.$$\end{document}</tex-math></alternatives></disp-formula></p><p>where Ω is the absolute TD-error estimator, <italic>α</italic> is the learning rate for <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$Q(s,a)$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$V(s)$\end{document}</tex-math></alternatives></inline-formula> values as mentioned earlier and <inline-formula><alternatives><mml:math id="inf75"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft75">\begin{document}$\alpha_{\Omega}\in [0,1]$\end{document}</tex-math></alternatives></inline-formula> is the scalar multiplier for the learning rate used for running average of TD-error. To obtain parameter <inline-formula><alternatives><mml:math id="inf76"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0,1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math><tex-math id="inft76">\begin{document}$\omega \in [0,1]$\end{document}</tex-math></alternatives></inline-formula> from this absolute TD-error estimator <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$\Omega \in [0, \infty)$\end{document}</tex-math></alternatives></inline-formula>, we scale it linearly using scalar <italic>κ</italic> and clip it between [0,1] as follows:<disp-formula id="equ15"><label>(15)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle \omega_{t}= \min(\kappa \Omega_{t}, 1) .$$\end{document}</tex-math></alternatives></disp-formula></p><p>We note that the range values Ω takes largely depends on the underlying reward function in the environment and <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula>. Thus, we choose a suitable value of <italic>κ</italic> for <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> using grid search in each grid world environment simulation to ensure that the Pavlovian system dominates in cases of high uncertainty and that the instrumental system starts to take control as uncertainty reduces. We aim to show that this flexible <italic>ω</italic> scheme is a viable candidate for arbitration between the two systems and addresses the safety-efficiency dilemma wherever it arises. The initial associability <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$\Omega_{0}$\end{document}</tex-math></alternatives></inline-formula> is set to 0 in grid world simulations as there is no principled way to set it. In the case of model fitting for the VR approach-withdrawal task, <inline-formula><alternatives><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:math><tex-math id="inft81">\begin{document}$\Omega_{0}$\end{document}</tex-math></alternatives></inline-formula>, <italic>κ</italic>, and <inline-formula><alternatives><mml:math id="inf82"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft82">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> are set as free parameters fitted to each participant and instead of the TD-errors, we have the RW rule equivalent - punishment prediction errors without any next state <inline-formula><alternatives><mml:math id="inf83"><mml:msup><mml:mi>s</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math><tex-math id="inft83">\begin{document}$s'$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-4"><title>Grid world simulation details</title><p>We consider a series of painful grid world-based navigational tasks, including moderate sources of pain (more variations in the Appendix with catastrophic and dynamic sources of pain). In the grid worlds, the goal is to navigate from starting position (in blue) to goal position (in green) while avoiding the static moderately painful states (in red). The agent receives a positive reward of 1 for reaching the goal and pain of 0.1 for moderately painful states (red). The pain is encoded as a negative reward of –0.1 in the case of standard RL. Four actions move the agent one step north, south, east, or west (or choose not to move, allowed only in certain environments). If the agent hits a wall, it stays and remains in its current state. All simulation environments have the following stochastic transition probabilities: 0.9 probability of correct (desired) state transition, whereas with 0.05 probability, the agent’s state transitions to the position perpendicular to action taken (right or left). We test the PAL algorithm for varying <inline-formula><alternatives><mml:math id="inf84"><mml:mstyle><mml:mrow><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft84">\begin{document}$\omega = 0.1,0.5,0.9$\end{document}</tex-math></alternatives></inline-formula> and for uncertainty-based modulation of flexible <italic>ω</italic> and compare the performance with standard instrumental policy (<italic>Q</italic>-learning). The following meta parameters are fixed for all our tabular grid world simulations - learning rate <inline-formula><alternatives><mml:math id="inf85"><mml:mstyle><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft85">\begin{document}$\alpha = 0.1$\end{document}</tex-math></alternatives></inline-formula>, discount factor <inline-formula><alternatives><mml:math id="inf86"><mml:mstyle><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft86">\begin{document}$\gamma = 0.99$\end{document}</tex-math></alternatives></inline-formula>, temperature annealing meta parameters <inline-formula><alternatives><mml:math id="inf87"><mml:mstyle><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.025</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft87">\begin{document}$\tau_{0}= 1, \tau_{k}= 0.025$\end{document}</tex-math></alternatives></inline-formula>. The meta parameters <inline-formula><alternatives><mml:math id="inf88"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft88">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> and <italic>κ</italic> are tuned using grid search on the safety-efficiency trade-off metrics for each environment. This is necessary, as different environments have different underlying reward distributions leading to different distributions of TD-errors, thus its running average needs to be appropriately scaled to map it to <inline-formula><alternatives><mml:math id="inf89"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0,1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math><tex-math id="inft89">\begin{document}$\omega \in [0,1]$\end{document}</tex-math></alternatives></inline-formula>. Due to this meta-parameter tuning, the claim in the simulation experiments is a modest one that there exists a <inline-formula><alternatives><mml:math id="inf90"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft90">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> and <italic>κ</italic> that mitigate the trade-off as opposed to the trade-off being mitigated by every possible combination of <inline-formula><alternatives><mml:math id="inf91"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft91">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> and <italic>κ</italic>. This resembles the model-fitting procedure in other experimental tasks, where <inline-formula><alternatives><mml:math id="inf92"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft92">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> and <italic>κ</italic> are fit in a hierarchical Bayesian manner, suggesting that humans perform this tuning to varying degrees to the best of their ability. The <inline-formula><alternatives><mml:math id="inf93"><mml:mstyle><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft93">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula>-tables and <inline-formula><alternatives><mml:math id="inf94"><mml:mstyle><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft94">\begin{document}$V_{p}$\end{document}</tex-math></alternatives></inline-formula>-tables are initialised with zeros. Plots are averaged over 10 runs with different seed values.</p><p>We quantify safety using cumulative pain accrued by the agent, and sample efficiency using the cumulative steps (or environment interactions or samples) taken by the agent across all the episodes in the learning process. The lesser the cumulative pain accrued over episodes, the safer is the learning; and the lesser the cumulative steps (or environment interactions), the more efficient is the learning in terms of reward seeking and task completion in each episode. Furthermore, we also construct a trade-off metric to measure how well the safety-efficiency trade-off is improved. We define the safety-efficiency trade-off metrics as follows, which is maximised when both cumulative pain and cumulative steps are independently minimised:<disp-formula id="equ16"><label>(16)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>Trade-off metric</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle \text{Trade-off metric}= \frac{1}{CP_{n}^{2}+ CS_{n}^{2}},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$CP_{n}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf96"><mml:mstyle><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft96">\begin{document}$CS_{n}$\end{document}</tex-math></alternatives></inline-formula> are cumulative pain and cumulative steps normalised by dividing the maximum cumulative pain and steps achieved (usually by fixed <inline-formula><alternatives><mml:math id="inf97"><mml:mstyle><mml:mrow><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft97">\begin{document}$\omega=0$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf98"><mml:mstyle><mml:mrow><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft98">\begin{document}$\omega=0.9$\end{document}</tex-math></alternatives></inline-formula>) in that run. We acknowledge that this normalisation can make the metric favour improvements in either safety or efficiency unequally to an extent, as it weighs the improvements in safety or efficiency relative to worst performance in each of them. This metric can be further weight-adjusted to give more priority to either CP or CS as required, but we don’t do that in the current instance. Thus, this metric should only be used as a didactic tool and not an absolute metric of performance, and one should instead draw conclusions by observing the cumulative pain accrued and steps taken over multiple episodes.</p></sec><sec id="s4-5"><title>Approach-withdrawal conditioning task: experimental design</title><sec id="s4-5-1"><title>Participant recruitment and process</title><p>30 adults participated in the experiment (15 females, 15 males; age: min=18, max=60, mean=30.5, standard deviation=12.44). Healthy participants from ages 18–60 were allowed to participate in the study (pre-established inclusion criteria). All subjects provided written informed consent for the experiment, which was approved by the local ethics board - University of Oxford Central University Research Ethics Committee (CUREC2 R58778/RE002). One participant withdrew and did not complete the study, and one participant turned out to be a fibromyalgia patient upon arrival, thus was excluded. The rest of the 28 healthy subjects’ (14 female, average age 27.96 years) data was used for the analysis.</p><p>Participants filled a short demographic form upon arrival, followed by a pain tolerance calibration procedure, followed by putting on all of the sensors, followed by a re-calibration of their pain tolerance before starting the practice session and the main experiment. All of this was usually completed within 2 hr and participants were paid £30 for their participation (and were adequately compensated for any unexpected overtime and reasonable travel reimbursements). They were free to withdraw from the experiment at any time.</p></sec><sec id="s4-5-2"><title>Trial protocol</title><p>We use a trial-based approach with a withdrawal task; however, the subjects had complete control over when to start the next trial. Each trial consisted of four events: a choice to initiate the trial, a coloured jellyfish cue, an approach or withdrawal motor response, and a probabilistic outcome. The timeline is displayed in <xref ref-type="fig" rid="fig1">Figure 1</xref>. In each trial, subjects will initiate the trial by bringing in their hand inside a hovering bubble in front of them. Then a jellyfish will emerge and fade in (gradually decreasing transparency) within the next 0.5 s and then stay in front of the subject for another 1 s, making the total fixation segment 1.5 s long. Throughout the fixation segment, the jellyfish colour will remain greyish-black. After this fixation segment terminates, the jellyfish takes one of the four colours with associated pain outcome contingencies. This is the stimulus phase and the subject is required to perform either an approach or a withdrawal response within the next 2 s. The approach response involved reaching out their hand and touching the jellyfish, whereas the withdrawal response involved withdrawing the hand away from the jellyfish and towards one’s own self. The subjects practised these two actions in the practice session before the main experiment and were instructed to perform either of these two actions. The stimulus ended as soon as an action was successfully completed and was followed by the probabilistic outcome phase. In the rare case that the 2 s time window completed before the subject could successfully perform either of these two actions, then for the purpose of the probabilistic outcome segment, the action was decided based on the hand-distance from the jellyfish (i.e. whether it was closer to an approach or a withdrawal action). The possible outcomes were either a painful electric shock (along with some shock animation visualisations around the jellyfish) or a neutral outcome (along with bubble animations from the jellyfish). The outcomes were presented depending on the action taken and the contingencies for each cue, as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. After the outcome segment which lasted for 1.5 s, the jellyfish proceeded to fade out (become more transparent gradually) for the next 0.75 s and then the subject could start the next trial by again bringing their hand within the bubble in front of them.</p><p>Subjects were instructed to try to keep their hand inside the bubble during this fixation segment and only move the hand after the jellyfish changes colour. The bubble was placed halfway between the subject and the jellyfish and was placed slightly to the right for right-handed and slightly to the left for left-handed subjects. The subjects performed the task with their dominant hand.</p></sec><sec id="s4-5-3"><title>Block protocol</title><p>Prior to the main task and the practice session, we perform a calibration of the intensity of pain stimulation used for the experiment according to each individual’s pain tolerance. To do this, we start with the minimum stimulation value and gradually increase the value using the ‘staircase’ procedure. We will record a ‘threshold’ value (typically rated as 3/10 on Likert scale), which is identified as the participant first reports pain sensation. We will record a second ‘maximum’ value, which the participant reports as the maximum pain sensation that the participant would be comfortable to tolerate for the complete experiment (typically rated 8/10 on the Likert scale). We then use 80% of that maximum value for stimulation throughout the experiment.</p><p>Before the main task, the subjects had to go through a short practice session to get acquainted with approach and withdrawal motions and the speed requirements. Subjects had one attempt at each of the two actions with no painful outcomes and no timeouts followed by a short practice session with two jellyfish (five trials each, randomised) and with 80% painful outcome contingencies for approach and withdrawal, respectively. They were informed as to which of these two jellyfish likes to be touched and which does not, during the practice session but not for the main experiment. The colours of the jellyfish for the practice session were different from those used for the main experiment. The four colours of the jellyfish cues for the main experiment were chosen so as to be colourblind-friendly. The main experiment had a total of 240 trials, 60 trials for each of the four jellyfish which was balanced across each quarter of the block, i.e., 15 trials per jellyfish per quarter block. The jellyfish 1 was the approach-to-avoid type and jellyfish 2 was the withdraw-to-avoid type throughout 240 trials. The jellyfish 3 was uncontrollable for the first half of the block (first 120 trials) and then was approach-to-avoid type for the rest of the block. The jellyfish 4 was uncontrollable for the first half of the block (first 120 trials) and then was the withdraw-to-avoid type for the rest of the block. Approach-to-avoid type means that the outcome would be a neutral outcome 80% of the times (and shock, 20% of the times) if the ‘correct’ approach action was performed, or else the outcome would be a shock 80% of the times (and neutral, 20% of the times) if the ‘incorrect’ withdrawal action was performed. Withdraw-to-avoid type means that the outcome would be a neutral outcome 80% of the times (and shock, 20% of the times) if the ‘correct’ withdraw action was performed, or else the outcome would be a shock 80% of the times (and neutral, 20% of the times) if the ‘incorrect’ approach action was performed. Uncontrollable type means that the outcome could be shock or neutral with 50% probability each, regardless of the actions performed. After each quarter of the block, the subjects were informed of their progress through the block with a 10 s rest.</p></sec><sec id="s4-5-4"><title>Analysis</title><p>The choices and RTs were extracted and used for model fitting. The EEG, electromyography (EMG) and skin conductance data were acquired but found to be too corrupted by movement artefacts and noise to allow reliable analysis.</p></sec></sec><sec id="s4-6"><title>Hierarchical Bayesian model-fitting choices and RTs</title><p>For both RL model fitting to choices and RLDDM model fitting to choices and RTs, we built four models each: RW (i.e. RW learning rule model), RW+bias (i.e. RW model with baseline bias), RW+bias+Pavlovian (fixed) (i.e. RW+bias model with a fixed Pavlovian withdrawal bias) and RW+bias+Pavlovian (flexible) (i.e. RW+bias model with a flexible Pavlovian withdrawal bias) similar to <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>.</p><p>The action selection for RL models was performed using a softmax as per <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> with free parameter <inline-formula><alternatives><mml:math id="inf99"><mml:mstyle><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft99">\begin{document}$\beta = 1/\tau$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf100"><mml:mstyle><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft100">\begin{document}$\beta\gt0$\end{document}</tex-math></alternatives></inline-formula>. The learning rule for RW models was:<disp-formula id="equ17"><label>(17)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle Q(s,a) := Q(s,a) + \alpha (R - Q(s,a)),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf101"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft101">\begin{document}$R= -1$\end{document}</tex-math></alternatives></inline-formula> in case of electric shocks or <inline-formula><alternatives><mml:math id="inf102"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft102">\begin{document}$R= 0$\end{document}</tex-math></alternatives></inline-formula> in case of neutral outcome. Punishment <inline-formula><alternatives><mml:math id="inf103"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft103">\begin{document}$p$\end{document}</tex-math></alternatives></inline-formula> can be defined as per <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and thus <inline-formula><alternatives><mml:math id="inf104"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft104">\begin{document}$p= 1$\end{document}</tex-math></alternatives></inline-formula> in case of electric shocks or <inline-formula><alternatives><mml:math id="inf105"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft105">\begin{document}$p= 0$\end{document}</tex-math></alternatives></inline-formula>, and the Pavlovian punishment value is calculated as per:<disp-formula id="equ18"><label>(18)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle V_{p}(s) := V_{p}(s) + \alpha (p - V_{p}(s))$$\end{document}</tex-math></alternatives></disp-formula></p><p><inline-formula><alternatives><mml:math id="inf106"><mml:mstyle><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft106">\begin{document}$\alpha\gt0$\end{document}</tex-math></alternatives></inline-formula> is the learning rate and fitted as a free parameter and note that here <inline-formula><alternatives><mml:math id="inf107"><mml:mstyle><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft107">\begin{document}$V_{p}$\end{document}</tex-math></alternatives></inline-formula> is always positive.</p><p>For RW+bias model,<disp-formula id="equ19"><label>(19)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mtext>; if </mml:mtext><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mtext>Approach</mml:mtext></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle \rho(s,a) = Q(s,a) + b \text{; if }a = \text{Approach}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ20"><label>(20)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>; else</mml:mtext><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle \rho(s,a) = Q(s,a) \text{; else}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf108"><mml:mstyle><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft108">\begin{document}$b \in (-\infty, +\infty)$\end{document}</tex-math></alternatives></inline-formula> is the baseline bias, which if positive represents a baseline approach bias and if negative represents baseline negative bias and is not Pavlovian in nature.</p><p>For RW+bias+Pavlovian (fixed) and RW+bias+Pavlovian (flexible) models,<disp-formula id="equ21"><label>(21)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mtext>; if </mml:mtext><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mtext>Approach</mml:mtext></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle \rho(s,a) = (1-\omega) Q(s,a) + b \text{; if }a = \text{Approach}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ22"><label>.(22)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>; if </mml:mtext><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mtext>Withdrawal</mml:mtext></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle \rho(s,a) = (1-\omega) Q(s,a) + \omega (V_{p}(s)) \text{; if }a = \text{Withdrawal}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf109"><mml:mstyle><mml:mrow><mml:mi>ω</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft109">\begin{document}$\omega \in [0,1]$\end{document}</tex-math></alternatives></inline-formula> is a free parameter for the RW+bias+Pavlovian (fixed) model. <italic>ω</italic> is not a free parameter for the RW+bias+Pavlovian (flexible) model, but computed as per <xref ref-type="disp-formula" rid="equ14 equ15">Equations 14 and 15</xref> with free parameters <inline-formula><alternatives><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:math><tex-math id="inft110">\begin{document}$\Omega_{0}$\end{document}</tex-math></alternatives></inline-formula> (initial associability), <italic>κ</italic> (scaling factor for <italic>ω</italic>), and <inline-formula><alternatives><mml:math id="inf111"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft111">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula> (learning rate multiplier for associability).</p><p>For RLDDM models, it is assumed that within a trial, the evidence is accumulated using a drift-diffusion process with parameters drift rate (<inline-formula><alternatives><mml:math id="inf112"><mml:mi>v</mml:mi></mml:math><tex-math id="inft112">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula>), non-decision time (ndt), threshold, and starting point. Non-decision time and threshold were kept as free parameters, and the starting point was kept constant and equal to half the threshold (making it equally likely starting point for both approach and avoidance actions). The drift rate <inline-formula><alternatives><mml:math id="inf113"><mml:mi>v</mml:mi></mml:math><tex-math id="inft113">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula> was set according to the difference in action propensities between the choices as follows:<disp-formula id="equ23"><label>(23)</label><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mtext>Approach</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mtext>Withdrawal</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle v = \rho(s,a=\text{Approach}) - \rho(s,a=\text{Withdrawal}).$$\end{document}</tex-math></alternatives></disp-formula></p><p>Thus, the baseline bias and the Pavlovian biases were also included in the drift rate.</p><p>For model fitting, we used a hierarchical Bayesian modelling approach; all models were fit using Stan. They were fit using both custom code in PyStan, as well as using the hBayesDM package (<xref ref-type="bibr" rid="bib1">Ahn et al., 2017</xref>), and final plots of group-level and subject-level parameter distributions were generated using the plotting functions in hBayesDM. Four parallel chains were run for all models. To assess the predictive accuracy of the models, we computed the LOOIC and WAIC (<xref ref-type="bibr" rid="bib76">Vehtari et al., 2017</xref>).</p></sec><sec id="s4-7"><title>Software and hardware setup</title><p>We used the HTC Vive Pro Eye for the virtual reality (VR) with Alienware PC setup and the experiment was designed in Unity (game engine). The pain stimulator used was DS5 with WASP electrodes for the VR approach-withdrawal task and Silver-Silver Chloride (Ag/AgCl) Cup Electrodes for the VR maze task. We also collected galvanic skin response, heart rate (HR), EMG signals, wireless EEG using Brainproducts LiveAmp and Vive tracker movement signals and eye-tracking inside the VR headset.</p><p>The pain stimulator electrodes were attached to the ring finger, between the ring and the middle finger. The GSR sensors were attached to the middle and the index fingers, and the EMG sensors were attached to the brachioradialis muscle of the active hand used in the task with the ground electrode on the elbow. The HR sensor was attached to the index finger of the opposite hand.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Software</p></fn><fn fn-type="con" id="con3"><p>Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All human subjects in the study provided written informed consent for the experiment, which was approved by the local ethics board - University of Oxford Central University Research Ethics Committee (CUREC2 R58778/RE002).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-101371-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The code and data used for generating the results are provided in <ext-link ext-link-type="uri" xlink:href="https://github.com/PranavMahajan25/Safety-Efficiency-Trade-off">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib41">Mahajan, 2025</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>PM would like to thank Michael Browning, Rafal Bogacz, Suyi Zhang, Charlie Yan, Maryna Alves Rosa Reges, Danielle Hewitt, Katja Wiech, the anonymous COSYNE 2022 reviewers, CCN 2023 reviewers, and Science Advances reviewers for their feedback on earlier draft(s) of the subsections/extended abstracts of the manuscript. PM would like to thank Simon Desch for feedback on RLDDM fitting and Danielle Hewitt for suggestions and guidance on EEG data analysis. The work was funded by Wellcome Trust (214251/Z/18/Z, 203139/Z/16/Z and 203139/A/16/Z), IITP (MSIT 2019-0-01371), and JSPS (22H04998). This research was also partly supported by the NIHR Oxford Health Biomedical Research Centre (NIHR203316). The views expressed are those of the author(s) and not necessarily those of the NIHR or the Department of Health and Social Care. For the purpose of open access, the authors have applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahn</surname><given-names>WY</given-names></name><name><surname>Haines</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package</article-title><source>Computational Psychiatry</source><volume>1</volume><fpage>24</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1162/CPSY_a_00002</pub-id><pub-id pub-id-type="pmid">29601060</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Altman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Constrained Markov Decision Processes</source><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Knowing how much you don’t know: a neural organization of uncertainty estimates</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>572</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1038/nrn3289</pub-id><pub-id pub-id-type="pmid">22781958</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Algorithms for survival: a comparative perspective on emotions</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>311</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.35</pub-id><pub-id pub-id-type="pmid">28360419</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>TM</given-names></name><name><surname>Gunaydin</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Measuring maladaptive avoidance: from animal models to clinical anxiety</article-title><source>Neuropsychopharmacology</source><volume>47</volume><fpage>978</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1038/s41386-021-01263-4</pub-id><pub-id pub-id-type="pmid">35034097</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>BA</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>McDannald</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dorsal raphe nucleus is integral to negative prediction errors in Pavlovian fear</article-title><source>The European Journal of Neuroscience</source><volume>40</volume><fpage>3096</fpage><lpage>3101</lpage><pub-id pub-id-type="doi">10.1111/ejn.12676</pub-id><pub-id pub-id-type="pmid">25041165</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolles</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Species-specific defense reactions and avoidance learning</article-title><source>Psychological Review</source><volume>77</volume><fpage>32</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1037/h0028589</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>PL</given-names></name><name><surname>Jenkins</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Auto-shaping of the pigeon’s key-peck</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>11</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1901/jeab.1968.11-1</pub-id><pub-id pub-id-type="pmid">5636851</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>JF</given-names></name><name><surname>Eisenberg</surname><given-names>I</given-names></name><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Huys</surname><given-names>Q</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Frontal theta overrides pavlovian learning biases</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>8541</fpage><lpage>8548</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5754-12.2013</pub-id><pub-id pub-id-type="pmid">23658191</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crombez</surname><given-names>G</given-names></name><name><surname>Eccleston</surname><given-names>C</given-names></name><name><surname>Van Damme</surname><given-names>S</given-names></name><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name><name><surname>Karoly</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fear-avoidance model of chronic pain: the next generation</article-title><source>The Clinical Journal of Pain</source><volume>28</volume><fpage>475</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1097/AJP.0b013e3182385392</pub-id><pub-id pub-id-type="pmid">22673479</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The misbehavior of value and the discipline of the will</article-title><source>Neural Networks</source><volume>19</volume><fpage>1153</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2006.03.002</pub-id><pub-id pub-id-type="pmid">16938432</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Desch</surname><given-names>S</given-names></name><name><surname>Schweinhardt</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Flor</surname><given-names>H</given-names></name><name><surname>Becker</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Endogenous modulation of pain relief: evidence for dopaminergic but not opioidergic involvement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.07.10.499477</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorfman</surname><given-names>HM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Controllability governs the balance between Pavlovian and instrumental action selection</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5826</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13737-7</pub-id><pub-id pub-id-type="pmid">31862876</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elfwing</surname><given-names>S</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallel reward and punishment control in humans and robots: Safe reinforcement learning using the MaxPain algorithm</article-title><conf-name>2017 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</conf-name><conf-loc>Lisbon</conf-loc><fpage>140</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1109/DEVLRN.2017.8329799</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>DA</given-names></name><name><surname>Stempel</surname><given-names>AV</given-names></name><name><surname>Vale</surname><given-names>R</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cognitive control of escape behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>334</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.012</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fei</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Xie</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Risk-sensitive reinforcement learning: near-optimal risk-sample tradeoff in regret</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>22384</fpage><lpage>22395</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fengler</surname><given-names>A</given-names></name><name><surname>Bera</surname><given-names>K</given-names></name><name><surname>Pedersen</surname><given-names>ML</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Beyond drift diffusion models: fitting a broad class of decision and RL models with HDDM</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.06.19.496747</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Discrete coding of reward probability and uncertainty by dopamine neurons</article-title><source>Science</source><volume>299</volume><fpage>1898</fpage><lpage>1902</lpage><pub-id pub-id-type="doi">10.1126/science.1077349</pub-id><pub-id pub-id-type="pmid">12649484</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontanesi</surname><given-names>L</given-names></name><name><surname>Gluth</surname><given-names>S</given-names></name><name><surname>Spektor</surname><given-names>MS</given-names></name><name><surname>Rieskamp</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A reinforcement learning diffusion decision model for value-based decisions</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>26</volume><fpage>1099</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1554-2</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fullana</surname><given-names>MA</given-names></name><name><surname>Dunsmoor</surname><given-names>JE</given-names></name><name><surname>Schruers</surname><given-names>KRJ</given-names></name><name><surname>Savage</surname><given-names>HS</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Harrison</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Human fear conditioning: From neuroscience to the clinic</article-title><source>Behaviour Research and Therapy</source><volume>124</volume><elocation-id>103528</elocation-id><pub-id pub-id-type="doi">10.1016/j.brat.2019.103528</pub-id><pub-id pub-id-type="pmid">31835072</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcıa</surname><given-names>J</given-names></name><name><surname>Fernández</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A comprehensive survey on safe reinforcement learning</article-title><source>Journal of Machine Learning Research</source><volume>16</volume><fpage>1437</fpage><lpage>1480</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gaskett</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Reinforcement Learning under Circumstances beyond Its Control</source><publisher-name>ATR International</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gehring</surname><given-names>C</given-names></name><name><surname>Precup</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Smart exploration in reinforcement learning using absolute temporal difference errors</article-title><conf-name>Proceedings of the 2013 international conference on autonomous agents and multi-agent systems</conf-name><fpage>1037</fpage><lpage>1044</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural signatures of arbitration between Pavlovian and instrumental action selection</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008553</pub-id><pub-id pub-id-type="pmid">33566831</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glogan</surname><given-names>E</given-names></name><name><surname>Vandael</surname><given-names>K</given-names></name><name><surname>Gatzounis</surname><given-names>R</given-names></name><name><surname>Meulders</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>When do we not face our fears? investigating the boundary conditions of costly pain-related avoidance generalization</article-title><source>The Journal of Pain</source><volume>22</volume><fpage>1221</fpage><lpage>1232</lpage><pub-id pub-id-type="doi">10.1016/j.jpain.2021.03.149</pub-id><pub-id pub-id-type="pmid">33852945</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Fuentemilla</surname><given-names>L</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Go and no-go learning in reward and punishment: interactions between affect and effect</article-title><source>NeuroImage</source><volume>62</volume><fpage>154</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.024</pub-id><pub-id pub-id-type="pmid">22548809</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><chapter-title>Consideration of risk in reinforcement learning</chapter-title><person-group person-group-type="editor"><name><surname>Heger</surname><given-names>M</given-names></name></person-group><source>Machine Learning Proceedings 1994</source><publisher-name>Elsevier</publisher-name><fpage>105</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/B978-1-55860-335-6.50021-0</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>O’Nions</surname><given-names>E</given-names></name><name><surname>Sheridan</surname><given-names>L</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bonsai trees in your head: how the pavlovian system sculpts goal-directed choices by pruning decision trees</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002410</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002410</pub-id><pub-id pub-id-type="pmid">22412360</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decision-theoretic psychiatry</article-title><source>Clinical Psychological Science</source><volume>3</volume><fpage>400</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1177/2167702614562040</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Gölzer</surname><given-names>M</given-names></name><name><surname>Friedel</surname><given-names>E</given-names></name><name><surname>Heinz</surname><given-names>A</given-names></name><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The specificity of Pavlovian regulation is associated with recovery from depression</article-title><source>Psychological Medicine</source><volume>46</volume><fpage>1027</fpage><lpage>1035</lpage><pub-id pub-id-type="doi">10.1017/S0033291715002597</pub-id><pub-id pub-id-type="pmid">26841896</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamin</surname><given-names>LJ</given-names></name><name><surname>Brimer</surname><given-names>CJ</given-names></name><name><surname>Black</surname><given-names>AH</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Conditioned suppression as a monitor of fear of the CS in the course of avoidance training</article-title><source>Journal of Comparative and Physiological Psychology</source><volume>56</volume><fpage>497</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1037/h0047966</pub-id><pub-id pub-id-type="pmid">13962088</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>EJ</given-names></name><name><surname>Horovitz</surname><given-names>O</given-names></name><name><surname>Pellman</surname><given-names>BA</given-names></name><name><surname>Tan</surname><given-names>LM</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Richter-Levin</surname><given-names>G</given-names></name><name><surname>Kim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dorsal periaqueductal gray-amygdala pathway conveys both innate and learned fear responses in rats</article-title><source>PNAS</source><volume>110</volume><fpage>14795</fpage><lpage>14800</lpage><pub-id pub-id-type="doi">10.1073/pnas.1310845110</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Hur</surname><given-names>JK</given-names></name><name><surname>Kwon</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Zoh</surname><given-names>Y</given-names></name><name><surname>Ahn</surname><given-names>WY</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Causal role of the dorsolateral prefrontal cortex in modulating the balance between Pavlovian and instrumental systems in the punishment domain</article-title><source>PLOS ONE</source><volume>18</volume><elocation-id>e0286632</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0286632</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krugel</surname><given-names>LK</given-names></name><name><surname>Biele</surname><given-names>G</given-names></name><name><surname>Mohr</surname><given-names>PNC</given-names></name><name><surname>Li</surname><given-names>SC</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Genetic variation in dopaminergic neuromodulation influences the ability to rapidly and flexibly adapt decisions</article-title><source>PNAS</source><volume>106</volume><fpage>17951</fpage><lpage>17956</lpage><pub-id pub-id-type="doi">10.1073/pnas.0905191106</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kucina</surname><given-names>T</given-names></name><name><surname>Wells</surname><given-names>L</given-names></name><name><surname>Lewis</surname><given-names>I</given-names></name><name><surname>de Salas</surname><given-names>K</given-names></name><name><surname>Kohl</surname><given-names>A</given-names></name><name><surname>Palmer</surname><given-names>MA</given-names></name><name><surname>Sauer</surname><given-names>JD</given-names></name><name><surname>Matzke</surname><given-names>D</given-names></name><name><surname>Aidman</surname><given-names>E</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Calibration of cognitive tests to address the reliability paradox for decision-conflict tasks</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>2234</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-37777-2</pub-id><pub-id pub-id-type="pmid">37076456</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LaBar</surname><given-names>KS</given-names></name><name><surname>Gatenby</surname><given-names>JC</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name><name><surname>LeDoux</surname><given-names>JE</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Human amygdala activation during conditioned fear acquisition and extinction: a mixed-trial fMRI study</article-title><source>Neuron</source><volume>20</volume><fpage>937</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80475-4</pub-id><pub-id pub-id-type="pmid">9620698</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>LC</given-names></name><name><surname>Moloney</surname><given-names>DM</given-names></name><name><surname>Samels</surname><given-names>SB</given-names></name><name><surname>Sears</surname><given-names>RM</given-names></name><name><surname>Cain</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reducing shock imminence eliminates poor avoidance in rats</article-title><source>Learning &amp; Memory</source><volume>27</volume><fpage>270</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1101/lm.051557.120</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Schiller</surname><given-names>D</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differential roles of human striatum and amygdala in associative learning</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1250</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1038/nn.2904</pub-id><pub-id pub-id-type="pmid">21909088</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ligneul</surname><given-names>R</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Ly</surname><given-names>V</given-names></name><name><surname>Cools</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Stress-sensitive inference of task controllability</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>812</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01306-w</pub-id><pub-id pub-id-type="pmid">35273354</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mackintosh</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1983">1983</year><source>Conditioning and Associative Learning</source><publisher-name>Clarendon Press</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mahajan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Safety-efficiency-trade-off</data-title><version designator="swh:1:rev:90f8010ec214ca0346e65786db6227f72dc2e741">swh:1:rev:90f8010ec214ca0346e65786db6227f72dc2e741</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e4233cfb6e63fd4b16a186133a6e1ab55137b493;origin=https://github.com/PranavMahajan25/Safety-Efficiency-Trade-off;visit=swh:1:snp:e053f625b2700a9d6a966f3489763204df02336b;anchor=swh:1:rev:90f8010ec214ca0346e65786db6227f72dc2e741">https://archive.softwareheritage.org/swh:1:dir:e4233cfb6e63fd4b16a186133a6e1ab55137b493;origin=https://github.com/PranavMahajan25/Safety-Efficiency-Trade-off;visit=swh:1:snp:e053f625b2700a9d6a966f3489763204df02336b;anchor=swh:1:rev:90f8010ec214ca0346e65786db6227f72dc2e741</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maia</surname><given-names>TV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two-factor theory, the actor--critic model, and conditioned avoidance</article-title><source>Learning &amp; Behavior</source><volume>38</volume><fpage>50</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.3758/LB.38.1.50</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Akiti</surname><given-names>K</given-names></name><name><surname>Amo</surname><given-names>R</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dopamine neurons projecting to the posterior striatum reinforce avoidance of threatening stimuli</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1421</fpage><lpage>1430</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0222-1</pub-id><pub-id pub-id-type="pmid">30177795</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meulders</surname><given-names>A</given-names></name><name><surname>Vansteenwegen</surname><given-names>D</given-names></name><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The acquisition of fear of movement-related pain and associative learning: a novel pain-relevant human fear conditioning paradigm</article-title><source>Pain</source><volume>152</volume><fpage>2460</fpage><lpage>2469</lpage><pub-id pub-id-type="doi">10.1016/j.pain.2011.05.015</pub-id><pub-id pub-id-type="pmid">21723664</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meulders</surname><given-names>A</given-names></name><name><surname>Franssen</surname><given-names>M</given-names></name><name><surname>Fonteyne</surname><given-names>R</given-names></name><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Acquisition and extinction of operant pain-related avoidance behavior using a 3 degrees-of-freedom robotic arm</article-title><source>Pain</source><volume>157</volume><fpage>1094</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1097/j.pain.0000000000000483</pub-id><pub-id pub-id-type="pmid">26761388</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning reward uncertainty in the Basal Ganglia</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005062</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005062</pub-id><pub-id pub-id-type="pmid">27589489</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mkrtchian</surname><given-names>A</given-names></name><name><surname>Aylward</surname><given-names>J</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Modeling avoidance in mood and anxiety disorders using reinforcement learning</article-title><source>Biological Psychiatry</source><volume>82</volume><fpage>532</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2017.01.017</pub-id><pub-id pub-id-type="pmid">28343697</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mkrtchian</surname><given-names>A</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Threat of shock and aversive inhibition: Induced anxiety modulates Pavlovian-instrumental interactions</article-title><source>Journal of Experimental Psychology. General</source><volume>146</volume><fpage>1694</fpage><lpage>1704</lpage><pub-id pub-id-type="doi">10.1037/xge0000363</pub-id><pub-id pub-id-type="pmid">28910125</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobbs</surname><given-names>D</given-names></name><name><surname>Headley</surname><given-names>DB</given-names></name><name><surname>Ding</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Space, time, and fear: survival computations along defensive circuits</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>228</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.12.016</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Möller</surname><given-names>M</given-names></name><name><surname>Manohar</surname><given-names>S</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Uncertainty-guided learning with scaled prediction errors in the basal ganglia</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009816</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009816</pub-id><pub-id pub-id-type="pmid">35622863</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moskovitz</surname><given-names>T</given-names></name><name><surname>O’Donoghue</surname><given-names>B</given-names></name><name><surname>Veeriah</surname><given-names>V</given-names></name><name><surname>Flennerhag</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Zahavy</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Reload: reinforcement learning with optimistic ascent-descent for last-iterate convergence in constrained mdps</article-title><conf-name>International conference on machine learning</conf-name><fpage>25303</fpage><lpage>25336</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mowrer</surname><given-names>OH</given-names></name></person-group><year iso-8601-date="1951">1951</year><article-title>Two-factor learning theory: summary and comment</article-title><source>Psychological Review</source><volume>58</volume><fpage>350</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1037/h0058956</pub-id><pub-id pub-id-type="pmid">14883248</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mowrer</surname><given-names>OH</given-names></name></person-group><year iso-8601-date="1960">1960</year><source>Learning Theory and Behavior</source><publisher-name>Wiley</publisher-name><pub-id pub-id-type="doi">10.1037/10802-000</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nord</surname><given-names>CL</given-names></name><name><surname>Lawson</surname><given-names>RP</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Pilling</surname><given-names>S</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Depression is associated with enhanced aversive Pavlovian control over instrumental behaviour</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>12582</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-30828-5</pub-id><pub-id pub-id-type="pmid">30135491</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norton</surname><given-names>PJ</given-names></name><name><surname>Paulus</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Transdiagnostic models of anxiety disorder: Theoretical and empirical underpinnings</article-title><source>Clinical Psychology Review</source><volume>56</volume><fpage>122</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1016/j.cpr.2017.03.004</pub-id><pub-id pub-id-type="pmid">28450042</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Neill</surname><given-names>M</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Coding of reward risk by orbitofrontal neurons is mostly distinct from coding of reward value</article-title><source>Neuron</source><volume>68</volume><fpage>789</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.031</pub-id><pub-id pub-id-type="pmid">21092866</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parsons</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Virtual reality for enhanced ecological validity and experimental control in the clinical, affective and social neurosciences</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>660</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00660</pub-id><pub-id pub-id-type="pmid">26696869</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedersen</surname><given-names>ML</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Biele</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The drift diffusion model as the choice rule in reinforcement learning</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>1234</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1199-y</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname><given-names>M</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</article-title><source>Nature</source><volume>442</volume><fpage>1042</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1038/nature05051</pub-id><pub-id pub-id-type="pmid">16929307</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prévost</surname><given-names>C</given-names></name><name><surname>Liljeholm</surname><given-names>M</given-names></name><name><surname>Tyszka</surname><given-names>JM</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural correlates of specific and general Pavlovian-to-Instrumental Transfer within human amygdalar subregions: a high-resolution fMRI study</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>8383</fpage><lpage>8390</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6237-11.2012</pub-id><pub-id pub-id-type="pmid">22699918</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>M</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name><name><surname>Jepma</surname><given-names>M</given-names></name><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Representation of aversive prediction errors in the human periaqueductal gray</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1607</fpage><lpage>1612</lpage><pub-id pub-id-type="doi">10.1038/nn.3832</pub-id><pub-id pub-id-type="pmid">25282614</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sailer</surname><given-names>M</given-names></name><name><surname>Hense</surname><given-names>JU</given-names></name><name><surname>Mayr</surname><given-names>SK</given-names></name><name><surname>Mandl</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How gamification motivates: An experimental study of the effects of specific game design elements on psychological need satisfaction</article-title><source>Computers in Human Behavior</source><volume>69</volume><fpage>371</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.chb.2016.12.033</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Singer</surname><given-names>T</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differential encoding of losses and gains in the human striatum</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>4826</fpage><lpage>4831</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0400-07.2007</pub-id><pub-id pub-id-type="pmid">17475790</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Serotonin selectively modulates reward value in human decision-making</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>5833</fpage><lpage>5842</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0053-12.2012</pub-id><pub-id pub-id-type="pmid">22539845</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharot</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The optimism bias</article-title><source>Current Biology</source><volume>21</volume><fpage>R941</fpage><lpage>R5</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.10.030</pub-id><pub-id pub-id-type="pmid">22153158</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporrer</surname><given-names>JK</given-names></name><name><surname>Brookes</surname><given-names>J</given-names></name><name><surname>Hall</surname><given-names>S</given-names></name><name><surname>Zabbah</surname><given-names>S</given-names></name><name><surname>Serratos Hernandez</surname><given-names>UD</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Functional sophistication in human escape</article-title><source>iScience</source><volume>26</volume><elocation-id>108240</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2023.108240</pub-id><pub-id pub-id-type="pmid">38026199</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Reinforcement Learning: An Introduction</article-title><conf-name>IEEE Transactions on Neural Networks</conf-name><pub-id pub-id-type="doi">10.1109/TNN.1998.712192</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talmi</surname><given-names>D</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Human pavlovian-instrumental transfer</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>360</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4028-07.2008</pub-id><pub-id pub-id-type="pmid">18184778</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thorn</surname><given-names>BE</given-names></name><name><surname>Dixon</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>Coping with chronic pain: a stress-appraisal coping model</chapter-title><person-group person-group-type="editor"><name><surname>Thorn</surname><given-names>BE</given-names></name></person-group><source>Coping with Chronic Illness and Disability: Theoretical, Empirical, and Clinical Aspects</source><publisher-name>Springer</publisher-name><fpage>313</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-48670-3_15</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk</surname><given-names>DC</given-names></name><name><surname>Rudy</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Cognitive factors and persistent pain: A glimpse into pandora’s box</article-title><source>Cognitive Therapy and Research</source><volume>16</volume><fpage>99</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1007/BF01173484</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzovara</surname><given-names>A</given-names></name><name><surname>Korn</surname><given-names>CW</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human Pavlovian fear conditioning conforms to probabilistic learning</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006243</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006243</pub-id><pub-id pub-id-type="pmid">30169519</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urcelay</surname><given-names>GP</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A psychological mechanism for the growth of anxiety</article-title><source>Behavioral Neuroscience</source><volume>138</volume><fpage>281</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.31234/osf.io/mvpuh</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vliet</surname><given-names>CM</given-names></name><name><surname>Meulders</surname><given-names>A</given-names></name><name><surname>Vancleef</surname><given-names>LMG</given-names></name><name><surname>Meyers</surname><given-names>E</given-names></name><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Changes in pain-related fear and pain when avoidance behavior is no longer effective</article-title><source>The Journal of Pain</source><volume>21</volume><fpage>494</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1016/j.jpain.2019.09.002</pub-id><pub-id pub-id-type="pmid">31541718</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vliet</surname><given-names>CM</given-names></name><name><surname>Meulders</surname><given-names>A</given-names></name><name><surname>Vancleef</surname><given-names>LMG</given-names></name><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Avoidance behaviour performed in the context of a novel, ambiguous movement increases threat and pain-related fear</article-title><source>Pain</source><volume>162</volume><fpage>875</fpage><lpage>885</lpage><pub-id pub-id-type="doi">10.1097/j.pain.0000000000002079</pub-id><pub-id pub-id-type="pmid">32947543</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Gabry</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title><source>Statistics and Computing</source><volume>27</volume><fpage>1413</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlaeyen</surname><given-names>JWS</given-names></name><name><surname>Linton</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Fear-avoidance and its consequences in chronic musculoskeletal pain: a state of the art</article-title><source>Pain</source><volume>85</volume><fpage>317</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1016/S0304-3959(99)00242-0</pub-id><pub-id pub-id-type="pmid">10781906</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Elfwing</surname><given-names>S</given-names></name><name><surname>Uchibe</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep Reinforcement Learning by Parallelizing Reward and Punishment using the MaxPain Architecture</article-title><conf-name>2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</conf-name><fpage>175</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1109/DEVLRN.2018.8761044</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Multiple dopamine systems: weal and woe of dopamine</chapter-title><person-group person-group-type="editor"><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><source>Cold Spring Harbor Symposia on Quantitative Biology</source><publisher-name>Cold Spring Harbor Laboratory Press</publisher-name><fpage>83</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1101/sqb.2018.83.037648</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yamamori</surname><given-names>Y</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.04.04.535526</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Mano</surname><given-names>H</given-names></name><name><surname>Ganesh</surname><given-names>G</given-names></name><name><surname>Robbins</surname><given-names>T</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociable learning processes underlie human pain conditioning</article-title><source>Current Biology</source><volume>26</volume><fpage>52</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.10.066</pub-id><pub-id pub-id-type="pmid">26711494</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Mano</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Yoshida</surname><given-names>W</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The control of tonic pain by active relief learning</article-title><source>eLife</source><volume>7</volume><elocation-id>e31949</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31949</pub-id><pub-id pub-id-type="pmid">29482716</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zorowitz</surname><given-names>S</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Anxiety, avoidance, and sequential evaluation</article-title><source>Computational Psychiatry</source><volume>4</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1162/cpsy_a_00026</pub-id><pub-id pub-id-type="pmid">34036174</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zorowitz</surname><given-names>S</given-names></name><name><surname>Karni</surname><given-names>G</given-names></name><name><surname>Paredes</surname><given-names>N</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Improving the reliability of the pavlovian go/no-go task</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/eb697</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Robustness of the associability-based <inline-formula><alternatives><mml:math id="inf114"><mml:mi>ω</mml:mi></mml:math><tex-math id="inft114">\begin{document}$\omega$\end{document}</tex-math></alternatives></inline-formula> in gridworld simulations</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Robustness of PAL in gridworld simulation.</title><p>This figure shows the robustness of grid search for tuning the meta parameters for the associability-based <italic>ω</italic> in grid world simulations. We show that the results hold for a range of values close to the chosen meta-parameters. (<bold>A–C</bold>) Grid search results for the environment in <xref ref-type="fig" rid="fig2">Figure 2</xref> for varying <italic>κ</italic> and <inline-formula><alternatives><mml:math id="inf115"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft115">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula>. (<bold>D–G</bold>) Results for another set of meta-parameters.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig1-v1.tif"/></fig></sec><sec sec-type="appendix" id="s9"><title>Flexible <inline-formula><alternatives><mml:math id="inf116"><mml:mi>ω</mml:mi></mml:math><tex-math id="inft116">\begin{document}$\omega$\end{document}</tex-math></alternatives></inline-formula> agent better adapts to reward relocation than a fixed <inline-formula><alternatives><mml:math id="inf117"><mml:mi>ω</mml:mi></mml:math><tex-math id="inft117">\begin{document}$\omega$\end{document}</tex-math></alternatives></inline-formula> agent</title><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Additional reward relocation experiments with PAL.</title><p>This figure shows cumulative state visit plots and value function plots of the flexible <italic>ω</italic> and fixed <italic>ω</italic> agents at the end of 1000 episodes when we relocate the reward goal from the bottom-left corner (<xref ref-type="fig" rid="fig2">Figure 2</xref>) to the bottom-right corner on episode 500. Comparing state visit plots (<bold>A, B</bold>) and comparing value function plots (<bold>C, D</bold>), we observe that persistent Pavlovian influence leads to persistent rigidity, while the flexible fear commissioning scheme is able to efficiently locate the goal. We observe that, unlike flexible <italic>ω</italic>, constant <italic>ω</italic>=0.5 leads to diminished value propagation of the rewarding value (<bold>C, D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig2-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10"><title>Solving the safety-efficiency trade-off in a range of grid world environments</title><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Flexible arbitration solves safety-efficiency trade-off in a range of gridworlds.</title><p>In this figure, we show the performance of fixed <italic>ω</italic>=0.1,0.5,0.9 and flexible <italic>ω</italic> agents on a range of grid world environments, namely (<bold>A</bold>) the three-route environment from <xref ref-type="fig" rid="fig3">Figure 3</xref>, (<bold>D</bold>) an environment with a moving predator on routine path and (<bold>G</bold>) wall maze grid world from <xref ref-type="bibr" rid="bib14">Elfwing and Seymour, 2017</xref>. Colliding with the predator results in a negative reward of –1 and catastrophic death (episode terminates). Otherwise, colliding with the walls results in moderate pain of 0.1, and the agent’s state remains unchanged. The latter two are completely deterministic environments, unlike the previous environments in the main paper. (<bold>B, C, E, F, H and I</bold>) show the safety-efficiency trade-off that arises in these three environments as well, and there is a separate optimal fixed <italic>ω</italic> for each environment. Alternatively, there exists a flexible <italic>ω</italic> scheme for each environment that can solve the trade-off, suggesting that the brain may be calibrating <italic>ω</italic> flexibly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig3-v1.tif"/></fig></sec><sec sec-type="appendix" id="s11"><title>Human three-route virtual reality maze results</title><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Additional results on human three-route VR maze task.</title><p>(<bold>A</bold>) Top-view of virtual reality (VR) maze with painful regions annotated by highlighted borders. (<bold>B</bold>) Cumulative steps required to reach the goal vs cumulative pain acquired by participants over 20 episodes in the VR maze task. In this figure, we show the results of a VR maze replicating the three-route grid world environment from simulation results; however, it had fewer states and the participants were instructed to reach the goal which was visible to them as a black cube with ‘GOAL’ written on it. In order to move inside the maze, participants had to physically rotate in the direction they wanted to move and then press a button on the joystick to move forward in the virtual space. Thus, the participant did not actually walk in the physical space but did rotate up to 360 degrees in physical space. The painful regions were not known to the participants, but they were aware that some regions of the maze may give them painful shocks with some unknown probability. Walking over the painful states in the VR maze, demarcated by grid borders (see A), potentially shocked them with 75% probability while ensuring 2 s of pain-free interval between two consecutive shocks. Participants were not given shocks with 100% probability as that would be too painful for participants due to the temporal summation effects of pain. The participants engaged in 20 episodes of trials and were aware of this before starting the task and were free to withdraw from the experiment at any point. 16 participants (11 female, average age 30.25 years) were recruited and were compensated adequately for their time. The pain tolerance was acquired similarly to the approach-withdrawal task. (<bold>B</bold>) All participant trajectories inside the maze were discretised into an 8×9 (horizontal × vertical) grid. Entering a 1×1 grid section counted incremented the cumulative steps (CS) count. Upon receiving the shocks, the cumulative pain (CP) count was incremented. CP and CS over 20 episodes were plotted against each other to observe the trade-off. A limitation of this experiment is that it reflects the constraints of the grid world, and future experiments are necessary to show the trade-off in a range of environments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig4-v1.tif"/></fig></sec><sec sec-type="appendix" id="s12"><title>Behavioural results from approach-withdrawal VR task</title><p>We observe a baseline approach bias through significant asymmetry in average number of cumulative approaches and withdrawals (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5A</xref>) and we consider the few inactive approaches and inactive withdrawals due to timeout as approaches and withdrawals, respectively (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5B</xref>).</p><p>We consider a couple of model-free metrics of Pavlovian withdrawal bias prior to model fitting. The withdrawal bias metric on choices for two cues (say, cue X and cue Y) is calculated as follows:<disp-formula id="equ24"><label>(24)</label><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>Choice bias metric(cue X, cue Y)</mml:mtext><mml:mo>=</mml:mo><mml:mi mathvariant="normal">%</mml:mi><mml:mtext>withdrawal choices on `cue X`</mml:mtext><mml:mo>−</mml:mo><mml:mi mathvariant="normal">%</mml:mi><mml:mtext>approach choices on `cue Y`</mml:mtext></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle \text{Choice bias metric(cue X, cue Y)} = \% \text{withdrawal choices on `cue X'} - \% \text{approach choices on `cue Y'}$$\end{document}</tex-math></alternatives></disp-formula></p><p>and the metric for withdrawal bias in RTs is simply the subtraction of average withdrawal times from average approach times in a half (60 trials) or the quarter block (30 trials) under consideration. This choice metric is an extension of the metric used by <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>, to punishment bias, and its logic is as follows. Consider the choice bias metric (cue2, cue1) - As Pavlovian withdrawals will increase %withdrawal choices i.e., (correct choice) for cue2 and decrease %approach choices, i.e., (correct choice) for cue1. Similarly, it will also make sense for metric (cue4, cue3) in the second half, albeit the bias would be lesser as they will be exploiting the optimal actions. It makes less sense for (cue4, cue3) in the first half as there is no optimal action; however, it helps act as a control and quantify a baseline approach bias. Unfortunately, this metric cannot differentiate an action due to random exploration from an action due to Pavlovian misbehaviour, leading to noisy estimates. Further, it cannot capture baseline approach bias <inline-formula><alternatives><mml:math id="inf118"><mml:mi>b</mml:mi></mml:math><tex-math id="inft118">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> at all, because the model by <xref ref-type="bibr" rid="bib13">Dorfman and Gershman, 2019</xref>, does not consider this parameter, unlike (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Guitart-Masip et al., 2012</xref>). However, we show that including baseline bias contributes the most to an incremental improvement in model fit.</p><p>We expect this bias to be largest in the first half with uncontrollable cues, and especially in the second quarter as opposed to the first quarter, by when enough punishment value would have been accrued for each of the cues and there would be a significant drop in random exploration. The Pavlovian withdrawal bias in choices is measured using the controllable cues 1 and 2 and thus computing the same quantity in uncontrollable cues 3 and 4 acts as control (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5C</xref>). Likewise, we also hypothesised that there would exist a Pavlovian bias in RTs which speed up all withdrawals and slow down all approaches regardless of the cue (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5E</xref>). For our second hypothesis, the Pavlovian bias should decrease with a decrease in outcome uncertainty, i.e., it would be higher in the second quarter as opposed to the fourth quarter (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5D and F</xref>). We compare the quarters rather than the first and second halves to minimise the noise through random exploration in the first quarter. However, the differences we observe are not statistically significant.</p><p>In addition to these results from behavioural metrics in choices and RTs, we further observe certain change-of-mind-like patterns in motor responses. It is unclear if these are due to a Pavlovian bias or due to other factors and can be investigated in future studies.</p><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Behavioural results from Approach-Withdrawal VR task.</title><p>(<bold>A</bold>) Asymmetry in average approach and withdrawal responses over all subjects showing a baseline approach bias - Mann-Whitney U test (statistic=597.0, p-value=0.0004). (<bold>B</bold>) Incomplete approaches and incomplete withdrawals that were counted as approaches and withdrawals, respectively. (<bold>C</bold>) Withdrawal bias in choices in the first half with uncontrollable cues - Mann-Whitney U test (statistic = 492.5, p-value = 0.0504). (<bold>D</bold>) Decrease in withdrawal bias in choice with decrease in uncontrollability - Mann-Whitney U test (statistic = 350.5, p-value = 0.7611). (<bold>E</bold>) Withdrawal bias in reaction times in the first half with uncontrollable cues - Mann-Whitney U test (statistic = 475.0, p-value = 0.0882). (<bold>F</bold>) Decrease in withdrawal bias in reaction times with decrease in uncontrollability - Mann-Whitney U test (statistic = 456.0, p-value = 0.1490). (<bold>G</bold>) Change-of-mind trials observed in motor data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig5-v1.tif"/></fig></sec><sec sec-type="appendix" id="s13"><title>Group and subject-level parameter distributions of RL and RLDDM models</title><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Group-level parameter distributions from RL models.</title><p>Group-level parameter distributions from (<bold>A</bold>) the reinforcement learning (RL) model (M3) with fixed <italic>ω</italic> and (<bold>B</bold>) the RL model (M4) with flexible <italic>ω</italic>. Shaded red regions denote 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig6-v1.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Group-level parameter distributions from RLDDM models.</title><p>Group-level parameter distributions from (<bold>A</bold>) the reinforcement learning diffusion decision-making (RLDDM) model (M3) with fixed <italic>ω</italic> and (<bold>B</bold>) the RLDDM model (M4) with flexible <italic>ω</italic>. Shaded red regions denote 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Subject-level parameter distributions from RL models.</title><p>Subject-level parameter distributions from (<bold>A</bold>) the reinforcement learning (RL) model (M3) with fixed <italic>ω</italic> and (<bold>B</bold>) the RL model (M4) with flexible <italic>ω</italic>. Shaded red regions denote 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig8-v1.tif"/></fig><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Subject-level parameter distributions from RLDDM models.</title><p>Subject-level parameter distributions from (<bold>A</bold>) the reinforcement learning diffusion decision-making (RLDDM) model (M3) with fixed <italic>ω</italic> and (<bold>B</bold>) the RLDDM model (M4) with flexible <italic>ω</italic>. Shaded red regions denote 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig9-v1.tif"/></fig></sec><sec sec-type="appendix" id="s14"><title>RL and RLDDM model parameters and model comparison tables</title><p>Please refer to <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> and <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Model comparison results for reinforcement learning (RL) models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model</th><th align="left" valign="top">Free parameters</th><th align="left" valign="top">LOOIC</th><th align="left" valign="top">WAIC</th></tr></thead><tbody><tr><td align="left" valign="top">M1</td><td align="left" valign="top"><italic>α</italic>,<italic>β</italic></td><td align="left" valign="top">8201.53</td><td align="left" valign="top">8182.15</td></tr><tr><td align="left" valign="top">M2</td><td align="left" valign="top"><italic>α</italic>,<italic>β</italic>,<italic>b</italic></td><td align="left" valign="top">7960.00</td><td align="left" valign="top">7926.73</td></tr><tr><td align="left" valign="top">M3</td><td align="left" valign="top"><italic>α</italic>,<italic>β</italic>,<italic>b</italic>,<italic>ω</italic></td><td align="left" valign="top">7947.84</td><td align="left" valign="top">7918.20</td></tr><tr><td align="left" valign="top">M4</td><td align="left" valign="top"><italic>α</italic>,<italic>β</italic>,<italic>b</italic>,<inline-formula><alternatives><mml:math id="inf119"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$\alpha_{\Omega}$\end{document}</tex-math></alternatives></inline-formula>, <italic>κ</italic>,<inline-formula><alternatives><mml:math id="inf120"><mml:mstyle><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$\Omega_0$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="top">7863.79</td><td align="left" valign="top">7830.18</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Model comparison results for reinforcement learning diffusion decision-making (RLDDM) model.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model</th><th align="left" valign="top">Free parameters</th><th align="left" valign="top">LOOIC</th><th align="left" valign="top">WAIC</th></tr></thead><tbody><tr><td align="left" valign="top">M1</td><td align="left" valign="top">ndt, threshold,<italic>α</italic>,<italic>β</italic></td><td align="left" valign="top">12539.14</td><td align="left" valign="top">12495.44</td></tr><tr><td align="left" valign="top">M2</td><td align="left" valign="top">ndt, threshold,<italic>α</italic>,<italic>β</italic>,<italic>b</italic></td><td align="left" valign="top">12303.00</td><td align="left" valign="top">12247.63</td></tr><tr><td align="left" valign="top">M3</td><td align="left" valign="top">ndt, threshold,<italic>α</italic>,<italic>β</italic>,<italic>b</italic>,<italic>ω</italic></td><td align="left" valign="top">12296.38</td><td align="left" valign="top">12247.22</td></tr><tr><td align="left" valign="top">M4</td><td align="left" valign="top">ndt, threshold,<italic>α</italic>,<italic>β</italic>,<italic>b</italic>,<inline-formula><alternatives><mml:math id="inf121"><mml:mstyle><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>κ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft121">\begin{document}$\alpha_{\Omega},\kappa,\Omega_0$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="top">12205.52</td><td align="left" valign="top">12164.14</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s15"><title>Model predictions: adapting fear responses in a chronic pain grid world</title><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>Model predictions: Adapting fear responses in a chronic pain gridworld.</title><p>Pavlovian-instrumental interactions are invoked in a popular model of chronic pain, in which excessive Pavlovian fear of movement is self-punitive in a context in which active avoidance would reduce pain (<xref ref-type="bibr" rid="bib10">Crombez et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Meulders et al., 2011</xref>). (<bold>A</bold>) Grid world with a start at the centre (blue) and goal at the left end (green) operationalises this. We augment the action set to include an additional ‘immobilise’ action, to the action set, resulting in no state change and repeated rewards. An upper bound of 100 steps per episode is set; exceeding it leads to a painless death and episode restart. (<bold>B</bold>) Cumulative failures to reach the goal as a measure of efficiency. With a constant Pavlovian fear influence, the agent struggles to complete episodes, resembling effects seen in rodent models of anxiety (<xref ref-type="bibr" rid="bib39">Ligneul et al., 2022</xref>). (<bold>C</bold>) Cumulative pain accrued as a measure of safety. In clinical terms, the agent remains stuck in a painful state, contrasting with an instrumental system that can seek and consume rewards despite pain. Flexible parameter <italic>ω</italic> (<inline-formula><alternatives><mml:math id="inf122"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft122">\begin{document}$\kappa=3, \alpha_{\Omega}=0.01$\end{document}</tex-math></alternatives></inline-formula>) allows the agent to overcome fear and complete episodes efficiently, demonstrating a safety-efficiency dilemma. The flexible <italic>ω</italic> policy outperforms fixed variants, emphasising the benefits of adapting fear responses for task completion. (<bold>D</bold>) Results from <xref ref-type="bibr" rid="bib37">Laughlin et al., 2020</xref> show that 25% of the (anxious) rats fail the signalled active avoidance task due to freezing. GIFs for different configurations: pure instrumental agent, adaptively safe agent (flexible <italic>ω</italic>), and maladaptively safe agent (constant <italic>ω</italic>) can be found <ext-link ext-link-type="uri" xlink:href="https://docs.google.com/presentation/d/1FJwsnsH6ZQ46g07gm0sOZMRQRT4FS95EW5q2WvnLiFE/present#slide=id.gf5961c859c_0_0">here</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig10-v1.tif"/></fig></sec><sec sec-type="appendix" id="s16"><title>Neurobiology of Pavlovian contributions to bias avoidance behaviour</title><fig id="app1fig11" position="float"><label>Appendix 1—figure 11.</label><caption><title>An overview of neurobiological substrates for the proposed Pavlovian avoidance learning (PAL) model based on relevant prior literature.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101371-app1-fig11-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101371.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cools</surname><given-names>Roshan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> work describes results from a set of simulation and empirical studies of a set-up assessing exploratory behavior in a potentially rewarding environment that contains danger. The core idea is that an instrumental agent can be helped to be both effective and safe, thus avoiding excessive danger, during exploratory behavior, if the influence of an independent Pavlovian fear is flexibly gated based on uncertainty. This work is grounded in previous foundational work on Pavlovian control of instrumental choice, and significantly extends prior work showing that the impact of Pavlovian reward biases can be flexibly gated. The conclusion that safe but effective exploration can be achieved based on a flexibly weighted combination of a Pavlovian and an instrumental agent is <bold>convincing</bold>.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101371.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper provides a computational model of a synthetic task in which an agent needs to find a trajectory to a rewarding goal in a 2D-grid world, in which certain grid blocks incur a punishment. In a completely unrelated setup without explicit rewards, they then provide a model that explains data from an approach-avoidance experiment in which an agent needs to decide whether to approach, or withdraw from, a jellyfish, in order to avoid a pain stimulus, with no explicit rewards. Both models include components that are labelled as &quot;Pavlovian&quot;; hence the authors argue that their data show that the brain uses a &quot;Pavlovian&quot; fear system in complex navigational and approach-avoid decisions.</p><p>In the first setup, they simulate a model in which a &quot;Pavlovian&quot; component learns about punishment in each grid block, where as a Q-learner learns about the optimal path to the goal, using a scalar loss function for rewards and punishments. &quot;Pavlovian&quot; and Q-learning components are then weighed at each step to produce an action. Unsurprisingly, the authors find that including the &quot;Pavlovian&quot; component into the model reduces the cumulative punishment incurred, and this increases as the weight of the &quot;Pavlovian&quot; system increases. The paper does not explore to what extent increasing the punishment loss (while keeping reward loss constant) would lead to the same outcomes with a simpler model architecture.</p><p>In the second setup, an agent learns about punishments alone. So-called &quot;Pavlovian biases&quot; have previously been demonstrated in this task (i.e. an over avoidance when the correct decision is to approach). The authors explore several models to account for the Pavlovian biases.</p><p>Strengths:</p><p>Overall, the modelling exercises are interesting and relevant and incrementally expand the space of existing models.</p><p>Weaknesses:</p><p>For the first task, the simulation results are not compared to a simple Q-learning model. The second task is somewhat artificial, a problem compounded by the virtual reality setup. According to the cover story, participants get &quot;stung by a jellyfish&quot; on average 88 times during the experiment. In one condition, withdrawal from a jelly fish lead to a sting.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101371.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors tested the efficiency of a model combining Pavlovian fear valuation and instrumental valuation. This model is amenable to many behavioral decision and learning setups - some of which have been or will be designed to test differences in patients with mental disorders (e.g., anxiety disorder, OCD, etc.).</p><p>Strengths:</p><p>(1) Simplicity of the model which can at the same time model rather complex environments.</p><p>(2) Introduction of a flexible omega parameter.</p><p>(3) Direct application to a rather advanced VR task.</p><p>(4) The paper is extremely well written. It was a joy to read.</p><p>Weaknesses:</p><p>Almost none! In very few cases, the explanations could be a bit better.</p><p>Comments on revised version:</p><p>No further comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101371.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper aims to address the problem of exploring potentially rewarding environments that contain danger, based on the assumption that an independent Pavlovian fear learning system can help guide an agent during exploratory behaviour such that it avoids severe danger. This is important given that otherwise later gains seem to outweigh early threats, and agents may end up putting themselves in danger when it is advisable not to do so.</p><p>The authors develop a computational model of exploratory behaviour that accounts for both instrumental and Pavlovian influences, combining the two according to uncertainty in the rewards. The result is that Pavlovian avoidance has a greater influence when the agent is uncertain about rewards.</p><p>Strengths:</p><p>The study does a thorough job of testing this model using both simulations and data from human participants performing an avoidance task. Simulations demonstrate that the model can produce &quot;safe&quot; behaviour, where the agent may not necessarily achieve the highest possible reward but ensures that losses are limited. Interestingly, the model appears to describe human avoidance behaviour in a task that tests for Pavlovian avoidance influences better than a model that doesn't adapt the balance between Pavlovian and instrumental based on uncertainty. The methods are robust, and generally there is little to criticise about the study.</p><p>Weaknesses:</p><p>The methods are robust, and generally there is little to criticise about the study. The extent of the testing in human participants is fairly limited, but goes far enough to demonstrate that the model can account for human behaviour in an exemplar task. There are, however, some elements of the model that are unrealistic (for example, the fact that pre-training is required to select actions with a Pavlovian bias would require the agent to explore the environment initially and encounter a vast amount of danger in order to learn how to avoid the danger later), although this could simply reflect a lengthy evolutionary process.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101371.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mahajan</surname><given-names>Pranav</given-names></name><role specific-use="author">Author</role><aff><institution>Wellcome Centre for Integrative Neuroimaging</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Tong</surname><given-names>Shuangyi</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Sang Wan</given-names></name><role specific-use="author">Author</role><aff><institution>Korea Advanced Institute of Science and Technology</institution><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Seymour</surname><given-names>Ben</given-names></name><role specific-use="author">Author</role><aff><institution>John Radcliffe Hospital</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>This paper provides a computational model of a synthetic task in which an agent needs to find a trajectory to a rewarding goal in a 2D-grid world, in which certain grid blocks incur a punishment. In a completely unrelated setup without explicit rewards, they then provide a model that explains data from an approach-avoidance experiment in which an agent needs to decide whether to approach or withdraw from, a jellyfish, in order to avoid a pain stimulus, with no explicit rewards. Both models include components that are labelled as Pavlovian; hence the authors argue that their data show that the brain uses a Pavlovian fear system in complex navigational and approach-avoid decisions.</p></disp-quote><p>Thanks to the reviewer’s comments, we have now added the following text to our Discussion section (Lines 290-302):</p><p>“When it comes to our experiments, both the simulation and VR experiment models are related and derived from the same theoretical framework maintaining an algebraic mapping. They differ only in task-specific adaptations i.e. differ in action sets and differ in temporal difference learning rules - multi-step decisions in the grid world vs. Rescorla-Wagner rule for single-step decisions in the VR task. This is also true for Dayan et al. [2006] who bridge Pavlovian bias in a Go-No Go task (negative auto-maintenance pecking task) and a grid world task. A further minor difference between the simulation and VR experiment models is the use of a baseline bias in the human experiment's RL and the RLDDM model, where we also model reaction times with drift rates which is not a behaviour often simulated in the grid world simulations. As mentioned previously, we use the grid world tasks for didactic purposes, similar to Dayan et al. [2006] and common to test-beds for algorithms in reinforcement learning [Sutton et al., 1998]. The main focus of our work is on Pavlovian fear bias in safe exploration and learning, rather than on its role in complex navigational decisions. Future work can focus on capturing more sophisticated safe behaviours, such as escapes [Evans et al., 2019, Sporrer et. al., 2023] and model-based planning, which span different aspects of the threat-imminence continuum [Mobbs et al., 2020].”</p><disp-quote content-type="editor-comment"><p>In the first setup, they simulate a model in which a component they label as Pavlovian learns about punishment in each grid block, whereas a Q-learner learns about the optimal path to the goal, using a scalar loss function for rewards and punishments. Pavlovian and Q-learning components are then weighed at each step to produce an action. Unsurprisingly, the authors find that including the Pavlovian component in the model reduces the cumulative punishment incurred, and this increases as the weight of the Pavlovian system increases. The paper does not explore to what extent increasing the punishment loss (while keeping reward loss constant) would lead to the same outcomes with a simpler model architecture, so any claim that the Pavlovian component is required for such a result is not justified by the modelling.</p></disp-quote><p>Thanks to the reviewer’s comments, we have now added the following text to our Discussion section (Line 303-313):</p><p>“In our simulation experiments, we assume the coexistence of the Pavlovian fear system and the instrumental system to demonstrate the emergent safety-efficiency trade-off from their interaction. It is possible that similar behaviours could be modelled using an instrumental system alone, with higher punishment sensitivity, therefore we do not argue for the necessity for the Pavlovian fear system here. Instead, the Pavlovian fear system itself could be a potential biologically plausible implementation of punishment sensitivity. Unlike punishment sensitivity (scaling of the punishments), which has not been robustly mapped to neural substrates in fMRI studies; the neural substrates for the Pavlovian fear system are well known (e.g., the limbic loop and amygdala, further see Supplementary Fig. 16). Additionally, Pavlovian fear system provides a separate punishment memory that cannot be erased by greater rewards like [Elfwing and Seymour, 2017, Wang et al., 2018]. This fundamental point can be observed in our simple T-maze simulations, where the Pavlovian fear system encourages avoidance behaviour and the agent chooses the smaller reward instead of the greater reward.”</p><disp-quote content-type="editor-comment"><p>In the second setup, an agent learns about punishments alone. &quot;Pavlovian biases&quot; have previously been demonstrated in this task (i.e. an overavoidance when the correct decision is to approach). The authors explore several models (all of which are dissimilar to the ones used in the first setup) to account for the Pavlovian biases.</p></disp-quote><p>Thanks to the reviewer’s comments, we have now added a paragraph in our Discussion section (Line 290-302) explaining the similarity of our models and their integrated interpretation. We hope this addresses the reviewer’s concerns.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>Overall, the modelling exercises are interesting and relevant and incrementally expand the space of existing models.</p><p>Weaknesses:</p><p>I find the conclusions misleading, as they are not supported by the data.</p><p>First, the similarity between the models used in the two setups appears to be more semantic than computational or biological. So it is unclear to me how the results can be integrated.</p></disp-quote><p>Thanks to the reviewer’s comments, we have now added a paragraph in our Discussion section (Line 290-302 onwards) explaining the similarity of our models and their integrated interpretation. We hope this addresses the reviewer’s concerns.</p><disp-quote content-type="editor-comment"><p>Secondly, the authors do not show &quot;a computational advantage to maintaining a specific fear memory during exploratory decision-making&quot; (as they claim in the abstract). Making such a claim would require showing an advantage in the first place. For the first setup, the simulation results will likely be replicated by a simple Q-learning model when scaling up the loss incurred for punishments, in which case the more complex model architecture would not confer an advantage. The second setup, in contrast, is so excessively artificial that even if a particular model conferred an advantage here, this is highly unlikely to translate into any real-world advantage for a biological agent. The experimental setup was developed to demonstrate the existence of Pavlovian biases, but it is not designed to conclusively investigate how they come about. In a nutshell, who in their right mind would touch a stinging jellyfish 88 times in a short period of time, as the subjects do on average in this task? Furthermore, in which real-life environment does withdrawal from a jellyfish lead to a sting, as in this task?</p><p>Crucially, simplistic models such as the present ones can easily solve specifically designed lab tasks with low dimensionality but they will fail in higher-dimensional settings. Biological behaviour in the face of threat is utterly complex and goes far beyond simplistic fight-flight-freeze distinctions (Evans et al., 2019). It would take a leap of faith to assume that human decision-making can be broken down into oversimplified sub-tasks of this sort (and if that were the case, this would require a meta-controller arbitrating the systems for all the sub-tasks, and this meta-controller would then struggle with the dimensionality j).</p></disp-quote><p>Thanks to the reviewer’s comments, we have now mentioned this point in Lines 299-302.</p><disp-quote content-type="editor-comment"><p>On the face of it, the VR task provides higher &quot;ecological validity&quot; than previous screen-based tasks. However, in fact, it is only the visual stimulation that differs from a standard screen-based task, whereas the action space is exactly the same. As such, the benefit of VR does not become apparent, and its full potential is foregone.</p><p>If the authors are convinced that their model can - then data from naturalistic approach-avoidance VR tasks is publicly available, e.g. (Sporrer et al., 2023), so this should be rather easy to prove or disprove. In summary, I am doubtful that the models have any relevance for real-life human decision-making.</p><p>Finally, the authors seem to make much broader claims that their models can solve safety-efficiency dilemmas. However, a combination of a Pavlovian bias and an instrumental learner (study 1) via a fixed linear weighting does not seem to be &quot;safe&quot; in any strict sense. This will lead to the agent making decisions leading to death when the promised reward is large enough (outside perhaps a very specific region of the parameter space). Would it not be more helpful to prune the decision tree according to a fixed threshold (Huys et al., 2012)? So, in a way, the model is useful for avoiding cumulatively excessive pain but not instantaneous destruction. As such, it is not clear what real-life situation is modelled here.</p></disp-quote><p>We hope our additions to the Discussion section, from Line 290 to Line 313 address the reviewer’s concerns.</p><disp-quote content-type="editor-comment"><p>A final caveat regarding Study 1 is the use of a PH associability term as a surrogate for uncertainty. The authors argue that this term provides a good fit to fear-conditioned SCR but that is only true in comparison to simpler RW-type models. Literature using a broader model space suggests that a formal account of uncertainty could fit this conditioned response even better (Tzovara et al., 2018).</p></disp-quote><p>We have now added a line discussing this. (Line 356-358)</p><p>“Future work could also use a formal account of uncertainty which could fit the fear-conditioned skin-conductance response better than Pearce-Hall associability [Tzovara et al., 2018].”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The authors tested the efficiency of a model combining Pavlovian fear valuation and instrumental valuation. This model is amenable to many behavioral decision and learning setups - some of which have been or will be designed to test differences in patients with mental disorders (e.g., anxiety disorder, OCD, etc.).</p><p>Strengths:</p><p>(1) Simplicity of the model which can at the same time model rather complex environments.</p><p>(2) Introduction of a flexible omega parameter.</p><p>(3) Direct application to a rather advanced VR task.</p><p>(4) The paper is extremely well written. It was a joy to read.</p><p>Weaknesses:</p><p>Almost none! In very few cases, the explanations could be a bit better.</p></disp-quote><p>Thank you, we have added further explanations in the discussion section. We have further improved the writing in abstract, introduction and Methods section taking into account recommendations from reviewer #2 and #3.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) Why is there no flexible omega in Figures 3B and 3C? Did I miss this?</p></disp-quote><p>Thank you. We have now added additional text to explain our motivation in Experiment 2, which only varies the fixed omega and omits the flexible omega (Lines 136-140).</p><p>“In this set of results, we wish to qualitatively tease apart the role of a Pavlovian bias in shaping and sculpting the instrumental value and also provide more insight into the resulting safety-efficiency trade-off. Having shown the benefits of a flexible ω in the previous section, here we only vary the fixed ω to illustrate the effect of a constant bias and are not concerned with the flexible bias in this experiment.”</p><p>We encourage the reader to consider this akin to an additional study that will explain how Pavlovian bias to withdraw can play a role in avoiding punishments similar to that of punishment sensitivity. This is particularly important as we do have neural correlates for Pavlovian biases but lack a clear neural correlation for punishment sensitivity so far, as mentioned in our new additions to the Discussion section (Lines 303-313).</p><disp-quote content-type="editor-comment"><p>(2) The introduction of the flexible omega and the PAL agent in the results is a bit sudden. Some more details are needed to understand this during the first read of this passage.</p></disp-quote><p>We thank reviewer #2 for bringing this to our notice. We have attempted to refine our passage by including sentences like -</p><p>“The standard (rational) reinforcement learning system is modelled as the instrumental learning system. The additional Pavlovian fear system biases the withdrawal actions to aid in safe exploration, in line with our hypothesis.”</p><p>“Both systems learn using a basic temporal difference updating rule (or in instances, its special case, the Rescorla-Wagner rule)”</p><p>“We implement the flexible ω using Pearce-Hall associability (see equation 15 in Methods). The Pearce-Hall associability maintains a running average of absolute temporal difference errors (δ) as per equation 14. This acts as a crude but easy-to-compute metric for outcome uncertainty which gates the influence of the Pavlovian fear system, in line with our hypothesis. This implies that higher the outcome uncertainty, as is the case in early exploration, the more cautious our agent will be, resulting in safer exploration”</p><disp-quote content-type="editor-comment"><p>(3) In my view, the possibility of modeling moving predators is extremely interesting. I would include Figure 8D and the corresponding explanation in the main text.</p></disp-quote><p>Response with revision: We thank the reviewer for finding our simulation on moving predators extremely interesting. Unfortunately, since our instrumental system is not model-based, and especially is not explicitly modelling the predator dynamics, our simulation might not be a very accurate representation of real moving predator environments. As pointed out by Reviewer #1, perhaps several other systems other than Pavlovian fear responses are necessary for safe behaviour in such environments and we hope to address these in future studies. Thanks again for taking an interest in our simulations.</p><disp-quote content-type="editor-comment"><p>(4) The VR experiment should be mentioned more clearly in the abstract and the introduction. It should be mentioned a bit more clearly why VR was helpful and why the authors did not use a simple bird's eye grid world task.</p><p>I cannot assess the RLDDM and I did not check the code.</p></disp-quote><p>Thank you, we have now mentioned the VR experiment more clearly in the abstract and the introduction. We also now further mention that the VR experiment “builds upon previous Go-No Go studies studying Pavlovian-Instrumental transfer (Guitart-Masip et al, 2012; Cavanagh et al, 2013). The virtual-reality approach confers a greater ecological validity and the immersive nature may contribute better fear conditioning, making it easier to distinguish the aversive components.”</p><p>A bird’s eye grid world may not invoke a strong withdrawal response, as seen in these immersive approach-withdrawal tasks where we can clearly distinguish a Pavlovian fear-based withdrawal response. We did include immersive VR maze results in the supplementary materials, but future work is needed to isolate the different systems at play in such a complex behaviour.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>This paper aims to address the problem of exploring potentially rewarding environments that contain the danger, based on the assumption that an independent Pavlovian fear learning system can help guide an agent during exploratory behaviour such that it avoids severe danger. This is important given that otherwise later gains seem to outweigh early threats, and agents may end up putting themselves in danger when it is advisable not to do so.</p><p>The authors develop a computational model of exploratory behaviour that accounts for both instrumental and Pavlovian influences, combining the two according to uncertainty in the rewards. The result is that Pavlovian avoidance has a greater influence when the agent is uncertain about rewards.</p><p>Strengths:</p><p>The study does a thorough job of testing this model using both simulations and data from human participants performing an avoidance task. Simulations demonstrate that the model can produce &quot;safe&quot; behaviour, where the agent may not necessarily achieve the highest possible reward but ensures that losses are limited. Interestingly, the model appears to describe human avoidance behaviour in a task that tests for Pavlovian avoidance influences better than a model that doesn't adapt the balance between Pavlovian and instrumental based on uncertainty. The methods are robust, and generally, there is little to criticise about the study.</p><p>Weaknesses:</p><p>The extent of the testing in human participants is fairly limited but goes far enough to demonstrate that the model can account for human behaviour in an exemplar task. There are, however, some elements of the model that are unrealistic (for example, the fact that pre-training is required to select actions with a Pavlovian bias would require the agent to explore the environment initially and encounter a vast amount of danger in order to learn how to avoid the danger later). The description of the models is also a little difficult to parse.</p></disp-quote><p>Thank you, we have now attempted to clarify these points in the Discussion section by adding the following text (Lines 313-321):</p><p>“ We next discuss the plausibility of pre-training to select the hardwired actions In the human experiment, the withdrawal action is straightforwardly biased, as noted, while in the grid world, we assume a hardwired encoding of withdrawal actions for each state/grid. This innate encoding of withdrawal actions could be represented in the dPAG [Kim et al., 2013]. We implement this bias using pre-training, which we assume would be a product of evolution. Alternatively, this could be interpreted as deriving from an appropriate value initialization where the gradient over initialized values determines the action bias. Such aversive value initialization, driving avoidance of novel and threatening stimuli, has been observed in the tail of the striatum in mice, which is hypothesised to function as a Pavlovian fear/threat learning system [Menegas et al., 2018].”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>I have relatively little to suggest, as in my view the paper is robust, thorough, and creative, and does enough to support the primary argument being made at the most fundamental level. My suggestions for improvement are as follows:</p><p>(1) Some aspects of the model are potentially unrealistic (as described in the public review), and the paper may benefit from some discussion of these issues or attempts to make the model more realistic - i.e., to what extent is this plausible in explaining more complex avoidance behaviour? Primarily, the fact that pre-training is required to identify actions subject to Pavlovian bias seems unlikely to be effective in real-world situations - is there a better way to achieve this in cases where there isn't necessarily an instinctual Pavlovian response?</p></disp-quote><p>Thank you, we agree that the advantage of Pavlovian bias is restricted to the bias/instinctual Pavlovian response conferred by evolution. Future work is needed to model more complex avoidance behaviour such as escapes. We hope to have made this more clear with our edits to the Discussion (Lines 299-302) in our response to Reviewer #1’s comments, specifically:</p><p>“The main focus of our work is on Pavlovian fear bias in safe exploration and learning, rather than on its role in complex navigational decisions. Future work can focus on capturing more sophisticated safe behaviours, such as escapes [Evans et al., 2019, Sporrer et. al., 2023] and model-based planning which span different aspects of the threat-imminence continuum [Mobbs et al., 2020]”</p><disp-quote content-type="editor-comment"><p>(2) The description of the model in the method can be a little hard to follow and would benefit from further explanation of certain parameters. In general, it would be good to ensure that all terms mentioned in equations are described clearly in the text (for example, in Equation1 it isn't clear what k refers to).</p></disp-quote><p>Thank you, we have now added further information on all of the parameters in Equation 1 and overall improved the Methods section writing, for instance using time subscript for less confusion while introducing the parameters. We use the standard notation used in Sutton and Barto textbook. k refers to the timesteps into the future, and is now explained better in the Methods section.</p><disp-quote content-type="editor-comment"><p>(3) Another point of clarification in Equation 1 - does the policy account for the Pavlovian influence or is this purely instrumental?</p></disp-quote><p>Thank you, Equation 1 is purely instrumental. We have now specifically mentioned this. The Pavlovian influence follows later. They are combined into propensities for action as per equations 11-13.</p><disp-quote content-type="editor-comment"><p>(4) I was curious whether similar outcomes could be achieved by more complex instrumental models without the need for Pavlovian influences. For example, could different risk-sensitive decision rules (e.g., conditional value at risk) that rely only on the instrumental system afford safe behaviour without the need for an additional Pavlovian system?</p></disp-quote><p>Thank you for your comment. Yes, CVaR can achieve safe exploration/cautious behaviour in choices similar to Pavlovian avoidance learning. But we think both differ in the following ways:</p><p>(1) CVaR provides the correct solution to the wrong problem (objective that only maximises the lower tail of the distribution of outcomes)</p><p>(2) Pavlovian bias provides the wrong solution to the right problem (normative objective, but a Pavlovian bias which may be vestige of evolution)</p><p>Here we use the “wrong problem, wrong solution, wrong environment” categorisation terminology from Huys et al. 2015.</p><p>Huys, Q. J., Guitart-Masip, M., Dolan, R. J., &amp; Dayan, P. (2015). Decision-theoretic psychiatry. Clinical Psychological Science, 3(3), 400-421.</p><p>Secondly, we find an effect of Pavlovian bias on reaction times - slowing down of approach responses and faster withdrawal responses. We do not think this can be best explained in a CVaR type model and is a direction for future work. We think such model-based methods are slower to compute, but Pavlovian withdrawal bias is quicker response.</p><p>We have now included this in brief in Lines 280-288.</p><disp-quote content-type="editor-comment"><p>(5) Figure 5 would benefit from a clearer caption as it is not necessarily clear from the current one that the left panels refer to choices and the right panels to reaction times.</p></disp-quote><p>Thank you, we have improved the caption for Fig. 5.</p><disp-quote content-type="editor-comment"><p>(6) It would be good to include some indication of the quality of the model fits for the human behavioural study (i.e., diagnostics such as R-hat) to ensure that differences in model fit between models are not due to convergence issues with different models. This would be especially helpful for the RLDDM models as these can be difficult to fit successfully.</p></disp-quote><p>Thank you, we observed that all Rhat values were strictly less than 1.05 (most parameters were less than 1.01 and generally close to 1), indicating that the models converged. We have now added this line to the results (Line 246-248). Thanks to the reviewer’s comments, we have now added the following text to our Discussion section (Lines 290-302): “When it comes to our experiments, both the simulation and VR experiment models are related and derived from the same theoretical framework maintaining an algebraic mapping. They differ only in task-specific adaptations i.e. differ in action sets and differ in temporal difference learning rules - multi-step decisions in the grid world vs. Rescorla-Wagner rule for single-step decisions in the VR task. This is also true for Dayan et al. [2006] who bridge Pavlovian bias in a Go-No Go task (negative auto-maintenance pecking task) and a grid world task. A further minor difference between the simulation and VR experiment models is the use of a baseline bias in the human experiment's RL and the RLDDM model, where we also model reaction times with drift rates which is not a behaviour often simulated in the grid world simulations. As mentioned previously, we use the grid world tasks for didactic purposes, similar to Dayan et al. [2006] and common to test-beds for algorithms in reinforcement learning [Sutton et al., 1998]. The main focus of our work is on Pavlovian fear bias in safe exploration and learning, rather than on its role in complex navigational decisions. Future work can focus on capturing more sophisticated safe behaviours, such as escapes [Evans et al., 2019, Sporrer et. al., 2023] and model-based planning, which span different aspects of the threat-imminence continuum [Mobbs et al., 2020].” In the first setup, they simulate a model in which a component they label as Pavlovian learns about punishment in each grid block, whereas a Q-learner learns about the optimal path to the goal, using a scalar loss function for rewards and punishments. Pavlovian and Q-learning components are then weighed at each step to produce an action. Unsurprisingly, the authors find that including the Pavlovian component in the model reduces the cumulative punishment incurred, and this increases as the weight of the Pavlovian system increases. The paper does not explore to what extent increasing the punishment loss (while keeping reward loss constant) would lead to the same outcomes with a simpler model architecture, so any claim that the Pavlovian component is required for such a result is not justified by the modelling.</p></body></sub-article></article>