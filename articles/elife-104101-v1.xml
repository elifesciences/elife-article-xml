<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">104101</article-id><article-id pub-id-type="doi">10.7554/eLife.104101</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104101.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Online reinforcement learning of state representation in recurrent network supported by the power of random feedback and biological constraints</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tsurumi</surname><given-names>Takayuki</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kato</surname><given-names>Ayaka</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6306-6600</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kumar</surname><given-names>Arvind</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8044-9195</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Morita</surname><given-names>Kenji</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2192-4248</contrib-id><email>morita@p.u-tokyo.ac.jp</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057zh3y96</institution-id><institution>Physical and Health Education, Graduate School of Education, The University of Tokyo</institution></institution-wrap><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qg15b79</institution-id><institution>Theoretical Sciences Visiting Program, Okinawa Institute of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Okinawa</named-content></addr-line><country>Japan</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Department of Psychiatry, Icahn School of Medicine at Mount Sinai</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026vcq606</institution-id><institution>Division of Computational Science and Technology, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Stockholm</named-content></addr-line><country>Sweden</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04ev03g22</institution-id><institution>Science for Life Laboratory</institution></institution-wrap><addr-line><named-content content-type="city">Solna, Stockholm</named-content></addr-line><country>Sweden</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057zh3y96</institution-id><institution>International Research Center for Neurointelligence (WPI-IRCN), The University of Tokyo</institution></institution-wrap><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Naud</surname><given-names>Richard</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>University of Ottawa</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>24</day><month>09</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP104101</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-11-05"><day>05</day><month>11</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-10-04"><day>04</day><month>10</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.22.609100"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-14"><day>14</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104101.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-30"><day>30</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104101.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-07-22"><day>22</day><month>07</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104101.3"/></event></pub-history><permissions><copyright-statement>© 2025, Tsurumi et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Tsurumi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-104101-v1.pdf"/><abstract><p>Representation of external and internal states in the brain plays a critical role in enabling suitable behavior. Recent studies suggest that state representation and state value can be simultaneously learned through Temporal-Difference-Reinforcement-Learning (TDRL) and Backpropagation-Through-Time (BPTT) in recurrent neural networks (RNNs) and their readout. However, neural implementation of such learning remains unclear as BPTT requires offline update using transported downstream weights, which is suggested to be biologically implausible. We demonstrate that simple online training of RNNs using TD reward prediction error and random feedback, without additional memory or eligibility trace, can still learn the structure of tasks with cue–reward delay and timing variability. This is because TD learning itself is a solution for temporal credit assignment, and feedback alignment, a mechanism originally proposed for supervised learning, enables gradient approximation without weight transport. Furthermore, we show that biologically constraining downstream weights and random feedback to be non-negative not only preserves learning but may even enhance it because the non-negative constraint ensures loose alignment—allowing the downstream and feedback weights to roughly align from the beginning. These results provide insights into the neural mechanisms underlying the learning of state representation and value, highlighting the potential of random feedback and biological constraints.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>dopamine</kwd><kwd>corticostriatal</kwd><kwd>reinforcement learning</kwd><kwd>state representation</kwd><kwd>feedback alignment</kwd><kwd>biological constraints</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hhkn466</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>23H03295</award-id><principal-award-recipient><name><surname>Morita</surname><given-names>Kenji</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hhkn466</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>23K27985</award-id><principal-award-recipient><name><surname>Morita</surname><given-names>Kenji</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hhkn466</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>25H02594</award-id><principal-award-recipient><name><surname>Morita</surname><given-names>Kenji</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s016q17</institution-id><institution>Naito Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Morita</surname><given-names>Kenji</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hhkn466</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>Overseas Research Fellowships</award-id><principal-award-recipient><name><surname>Kato</surname><given-names>Ayaka</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02ztymy03</institution-id><institution>Digital Futures</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kumar</surname><given-names>Arvind</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>StratNeuro SRA</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kumar</surname><given-names>Arvind</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Recurrent neural network and its readout (cortex–striatum) can learn state representation and value using online random-weight feedback of temporal-difference reward-prediction-error (dopamine) through feedback alignment or biological non-negative-weight constraint-induced loose alignment.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Multiple lines of studies have suggested that Temporal-Difference-Reinforcement-Learning (TDRL) is implemented in the cortico-basal ganglia-dopamine (DA) circuits where DA encodes TD reward-prediction-error (RPE) (<xref ref-type="bibr" rid="bib84">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib115">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib94">Niv and Schoenbaum, 2008</xref>; <xref ref-type="bibr" rid="bib20">Cohen et al., 2012</xref>; <xref ref-type="bibr" rid="bib126">Steinberg et al., 2013</xref>; <xref ref-type="bibr" rid="bib60">Kim et al., 2020</xref>) and DA-dependent plasticity of cortico-striatal synapses corresponds to TD-RPE-dependent update of state/action values (<xref ref-type="bibr" rid="bib105">Reynolds et al., 2001</xref>; <xref ref-type="bibr" rid="bib116">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="bib143">Yagishita et al., 2014</xref>). Traditionally, TDRL in the cortico-basal ganglia-DA circuits was considered to serve only for relatively simple behavior. However, subsequent studies suggested that more sophisticated, apparently goal-directed/model-based behavior can also be achieved by TDRL if states are appropriately represented (<xref ref-type="bibr" rid="bib108">Russek et al., 2017</xref>; <xref ref-type="bibr" rid="bib122">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>) and that DA signals indeed reflect model-based predictions (<xref ref-type="bibr" rid="bib64">Langdon et al., 2018</xref>; <xref ref-type="bibr" rid="bib59">Keiflin et al., 2019</xref>). Conversely, impairments in state representation may relate to behavioral or mental health problems (<xref ref-type="bibr" rid="bib104">Redish et al., 2007</xref>; <xref ref-type="bibr" rid="bib41">Gershman et al., 2013</xref>; <xref ref-type="bibr" rid="bib117">Shimomura et al., 2021</xref>; <xref ref-type="bibr" rid="bib34">Feng et al., 2021</xref>; <xref ref-type="bibr" rid="bib112">Sato et al., 2023</xref>). Early modeling studies treated state representations appropriate to the situation/task as given (‘handcrafted’ by the authors), but representation itself should be learned in the brain (<xref ref-type="bibr" rid="bib40">Gershman and Niv, 2010</xref>; <xref ref-type="bibr" rid="bib95">Niv, 2019</xref>; <xref ref-type="bibr" rid="bib38">George et al., 2023</xref>; <xref ref-type="bibr" rid="bib11">Bono et al., 2023</xref>; <xref ref-type="bibr" rid="bib32">Fang et al., 2023</xref>; <xref ref-type="bibr" rid="bib22">Cone and Clopath, 2024</xref>). Recently, it was shown that appropriate state representation can be learned through TDRL in a recurrent neural network (RNN) by minimizing squared TD value-error without explicit target (<xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>; <xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>), while state value can be simultaneously learned in connections downstream of the RNN.</p><p>However, whether such a learning—referred to as the value-RNN (<xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>), can be implemented in the brain remains unclear. This is because the value-RNN (<xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>) used the Backpropagation-Through-Time (BPTT) (<xref ref-type="bibr" rid="bib107">Rumelhart et al., 1986b</xref>), which applies gradient-descent error-‘backpropagation‘ (hereafter referred to as backprop) (<xref ref-type="bibr" rid="bib2">Amari, 1967</xref>; <xref ref-type="bibr" rid="bib106">Rumelhart et al., 1986a</xref>) to a temporally unfolded RNNs. BPTT has been argued to be biologically implausible mainly due to problems with feedback and causality. Regarding feedback, updating upstream connections requires feedback whose weights are transported from downstream forward connections, but such weight transportation is difficult to implement biologically (<xref ref-type="bibr" rid="bib44">Grossberg, 1987</xref>; <xref ref-type="bibr" rid="bib26">Crick, 1989</xref>). In the case of the value-RNN, if the state-representing RNN and the value-encoding readout are implemented by the intra-cortical circuit and the striatal neurons, respectively, as generally assumed (<xref ref-type="bibr" rid="bib84">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib28">Doya, 2000</xref>; <xref ref-type="bibr" rid="bib97">O’Doherty et al., 2004</xref>), this weight transportation means that the update (plasticity) rule for intra-cortical connections involves the downstream cortico-striatal synaptic strengths, which are not accessible from the cortex. Regarding the problem of causality in BPTT, the error needs to be incrementally accumulated in the temporally backward order, but such an acausal offline update is biologically implausible (<xref ref-type="bibr" rid="bib93">Murray, 2019</xref>; <xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>).</p><p>Recently, a potential solution for the feedback problem of backprop has been proposed (<xref ref-type="bibr" rid="bib70">Lillicrap et al., 2016</xref>; see also <xref ref-type="bibr" rid="bib45">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="bib109">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="bib141">Whittington and Bogacz, 2019</xref>; <xref ref-type="bibr" rid="bib71">Lillicrap et al., 2020</xref>; <xref ref-type="bibr" rid="bib100">Payeur et al., 2021</xref>; <xref ref-type="bibr" rid="bib43">Greedy et al., 2022</xref>; <xref ref-type="bibr" rid="bib121">Song et al., 2024</xref>; <xref ref-type="bibr" rid="bib99">Pagkalos et al., 2024</xref> for other approaches). Specifically, in supervised learning of feed-forward networks, it was shown that when the transported downstream weights used for updating upstream connections were replaced with fixed random strengths, comparable learning performance was still achieved (<xref ref-type="bibr" rid="bib70">Lillicrap et al., 2016</xref>). This was suggested to be because the information of the random strengths transferred to the upstream connections and then to the downstream feed-forward connections so that these feed-forward connections became aligned to the random feedback and thereby the random feedback could work similarly to the feedback with transported downstream weights in backprop. This mechanism was named the ‘feedback alignment’ (FA) (<xref ref-type="bibr" rid="bib70">Lillicrap et al., 2016</xref>), and was subsequently shown to work also in online supervised learning of RNN (algorithm named RFLO (random feedback local online)) (<xref ref-type="bibr" rid="bib93">Murray, 2019</xref>) and proposed to be ‘neurally’ implemented (<xref ref-type="bibr" rid="bib139">Wärnberg and Kumar, 2023</xref>) (in a different way from the present study as we discuss in the Discussion).</p><p>The value-RNN (<xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>; <xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>) differs from supervised learning considered in these previous FA studies in two ways: (1) it is TD learning, that is, it approximates the true error by the TD-RPE because the true error, or true state value, is unknown, and (2) it uses a scalar error (TD-RPE) rather than a vector error. Scalar reward-based online learning of RNN with random feedback was actually shown to work in a different study (<xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>) (their Supplementary Figure 5), but TD-RPE was not introduced in that setup, while the same study also examined another setup with TD-RPE, but the result with random feedback was not shown for this latter setup (their Figures 4 and 5). Therefore, it was non-trivial whether the value-RNN could be modified to incorporate online update using random feedback. Here, we demonstrate that such a modified value-RNN could still work and provide a mechanistic insight into how it works.</p><p>Next, we address other biological plausibility issues with FA-based rule. Specifically, we imposed biological constraints that the downstream (cortico-striatal) weights and the fixed random feedback, as well as the activities of neurons in the RNN, were all non-negative. Moreover, we also remedied the non-monotonic dependence of the update of RNN connection strength on post-synaptic neural activity. We found that the non-negative constraint appeared to aid, rather than degrade, the learning by ensuring that the downstream weights and the fixed random feedback were loosely aligned from the beginning. These results suggest how learning of state representation and value could be implemented via DA-dependent synaptic plasticity in cortical and striatal circuits, where DA encodes TD-RPE.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Online value-RNN with fixed random feedback</title><p>We considered an online value-RNN in the cortico-basal ganglia circuits (<xref ref-type="fig" rid="fig1">Figure 1</xref>). In this model, a cortical region/population represents information of sensory observation (<italic>o</italic>) and sends it to another cortical region/population which estimates a state (<italic>x</italic>) given the sensory inputs. We approximate this cortical population as an RNN (number of RNN units was varied between 5 and 40). Neurons in the RNN learn to represent states by updating the strengths of recurrent connections <bold>A</bold> and feed-forward connections <bold>B</bold>. The activity of a population of striatal neurons that receive inputs from the RNN is supposed to learn to represent the state values (<italic>v</italic>), by learning the weights (<italic>w</italic>) of cortico-striatal connections. DA neurons in the ventral tegmental area (VTA) receive information about the value and reward (<italic>r</italic>) from the striatum (both direct and indirect pathways) and other structures, and the activity of the DA neurons, as well as released DA, represents TD-RPE (<italic>δ</italic>). The TD-RPE-representing DA is released in the striatum and also in the cortical RNN through mesocorticolimbic projections and used for modifying <bold>A</bold>, <bold>B</bold>, and <italic>w</italic> (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Implementation of the online value-recurrent neural network (RNN) in the cortico-basal ganglia-DA circuits.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig1-v1.tif"/></fig><p>The original value-RNN (<xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>; <xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>) adopted BPTT (<xref ref-type="bibr" rid="bib107">Rumelhart et al., 1986b</xref>) as an update rule for the connections onto the RNN (<bold>A</bold> and <bold>B</bold>), which requires the (gradually changing) value weights (<italic>w</italic>), but this is biologically implausible because the cortico-striatal synaptic weights are not available in the cortex as mentioned above. Therefore, we examined a model (agent) in which these weights were replaced with fixed random strengths (<italic>c</italic>), in comparison with a model that used these weights. Because the temporally acausal offline update used in BPTT is also biologically implausible, we used an online learning rule, which considers only the influence of the recurrent weights at the previous time step (see the Methods for details and equations). We refer to these models (agents) as the online value-RNN or oVRNN.</p><p>We assumed that a single RNN unit corresponds to a small population of neurons that intrinsically share inputs and outputs, for genetic or developmental reasons, and the activity of each unit represents the (relative) firing rate of the population. Cortical population activity is suggested to be sustained not only by fast synaptic transmission and spiking but also, even predominantly, by slower synaptic neurochemical dynamics (<xref ref-type="bibr" rid="bib83">Mongillo et al., 2008</xref>) such as short-term facilitation, whose time constant can be around 500 ms (<xref ref-type="bibr" rid="bib85">Morishima et al., 2011</xref>). Therefore, we assumed that a single time step of our rate-based (rather than spike-based) model corresponds to 500 ms.</p><p>In each simulation, the recurrent (<bold>A</bold>) and feed-forward connection (<bold>B</bold>) weights onto the RNN units were initialized to pseudo standard normal random numbers. As a negative control, we also conducted simulations in which these connections were not updated from initial values, referring to as the case with ‘untrained (fixed) RNN’. Notably, the value weights <italic>w</italic> (i.e., connection weights from the RNN to the striatal value unit) were still trained in the models with untrained RNN. The oVRNN models, and the model with untrained RNN, were continuously trained across trials in each task, because we considered that it was ecologically more plausible than episodic training of separate trials.</p></sec><sec id="s2-2"><title>Simulation of a Pavlovian cue–reward association task with variable inter-trial intervals</title><p>First, we took a small RNN with 7 units to represent state in the cortex and simulated a Pavlovian cue–reward association task, in which a cue was followed by a reward three time steps later, and inter-trial interval (ITI, i.e., reward to next cue) was randomly chosen from 4, 5, 6, or 7 time steps (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Given that a single time step corresponds to 500 ms as mentioned above, three time steps from cue to reward correspond to 1.5 s, which matches the delay in the conditioning task used by <xref ref-type="bibr" rid="bib115">Schultz et al., 1997</xref>. In this task, states after receiving cue information can be defined by time steps from the cue, and the state values of these states can be estimated by calculating the expected cumulative discounted future rewards (<xref ref-type="bibr" rid="bib128">Sutton and Barto, 2018</xref>) through simulations; we refer to them as ‘estimated true state values’ (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, black line). Expected TD-RPE can be calculated from these estimated true values (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, red line).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Simulation of a Pavlovian cue–reward association task.</title><p>(<bold>A</bold>) Simulated task with variable inter-trial intervals (ITIs). (<bold>B</bold>) Black line: Estimated true values of states/timings through simulations according to the definition of state value, that is, expected cumulative discounted future rewards, taking into account the effect of probabilistic ITI. Red line: TD-RPEs calculated from the estimated true state/timing values. (<bold>C–G</bold>) State values (black lines) and TD-RPEs (red lines) at 1000th trial, averaged across 100 simulations (error bars indicating mean ± SEM across simulations; same applied to the followings unless otherwise mentioned), in different types of agent: (<bold>C</bold>) TD-RL agent having punctate state representation and state values without continuation between trials (i.e., the value of the last state in a trial was not updated by TD-RPE upon entering the next trial); (<bold>D</bold>) TD-RL agent having punctate state representation and continuously updated state values across trials; (<bold>E</bold>) Online value-recurrent neural network (RNN) with backprop (oVRNNbp). The number of RNN units was 7 (same applied to <bold>F, G</bold>); (<bold>F</bold>) Online value-RNN with fixed random feedback (oVRNNrf); (<bold>G</bold>) Agent with untrained RNN. (<bold>H</bold>) State values at 1000th trial in individual simulations of oVRNNbp (top), oVRNNrf (middle), and untrained RNN (bottom). (<bold>I</bold>) Histograms of the value of the pre-reward state (i.e., the state one time step before the reward state) at 1000th trial in individual simulations of the three models. The vertical black dashed lines indicate the true value of the pre-reward state (estimated through simulations). (<bold>J</bold>) <italic>Left</italic>: Mean of the squares of differences between the state values developed by each agent and the estimated true state values between cue and reward (referred to as the mean squared value-error) at 1000th trial in oVRNNbp (red line), oVRNNrf (blue line), and the model with untrained RNN (gray line) when the number of RNN units (<italic>n</italic>) was varied from 5 to 40. Learning rate for value weights was normalized by dividing by <italic>n</italic>/7 (same applied to the followings unless otherwise mentioned). <italic>Right</italic>: Mean squared value-error in oVRNNrf (blue line: same data as in the left panel) and oVRNN with uniform feedback (green line). (<bold>K</bold>) Log contribution ratios of the principal components of the time series (for 1000 trials) of RNN activities in each model with 20 RNN units. (<bold>L</bold>) Mean squared value-error in each model with 20 RNN units across trials. (<bold>M</bold>) Mean squared value-error in each model at 3000th trial in the cases where the cue–reward delay was 3, 4, 5, or 6 time steps (top to bottom panels).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig2-v1.tif"/></fig><p>First, for comparison, we examined the traditional TD-RL agent with punctate state representation (without using the RNN), in which each state (time step from a cue) was represented in a punctate manner, that is, by a one-hot vector such as (1, 0,..., 0), (0, 1,..., 0), and so on. We examined two cases: one in which training was done in an episodic manner without continuation between trials (i.e., the value of the last state in a trial was not updated by TD-RPE upon entering the next trial) and the other in which training was done continuously across trials, as in the cases of agents using the RNN. The former agent developed positive values between cue and reward, and abrupt TD-RPE upon cue (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), whereas the latter agent developed positive values also for states in the ITI (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), looking similar to the true values (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>We then examined our oVRNN agents, with backprop-type transported downstream weights (oVRNNbp: <xref ref-type="fig" rid="fig2">Figure 2E</xref>) or with fixed random feedback (oVRNNrf: <xref ref-type="fig" rid="fig2">Figure 2F</xref>), in comparison with the agent with untrained fixed RNN (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). As shown in the figures, oVRNNbp successfully learned the values of states between cue and reward, and oVRNNrf also learned these values, although to a somewhat smaller degree on average. On the other hand, the agent with untrained RNN developed the smallest state values on average among the three agents. This inferiority of untrained RNN may sound odd because there were only four states from cue to reward while random RNN with enough units is expected to be able to represent many different states (cf., <xref ref-type="bibr" rid="bib103">Rajan and Abbott, 2006</xref>) and the effectiveness of training of only the readout weights has been shown in reservoir computing studies (<xref ref-type="bibr" rid="bib27">Dominey, 1995</xref>; <xref ref-type="bibr" rid="bib75">Maass et al., 2002</xref>; <xref ref-type="bibr" rid="bib55">Jaeger, 2007</xref>; <xref ref-type="bibr" rid="bib132">Tanaka et al., 2019</xref>). However, there was a difficulty stemming from the continuous training across trials (rather than episodic training of separate trials): the activity of untrained RNN upon cue presentation generally differed from trial to trial, and so it is non-trivial that cue presentation in different trials should be regarded as the same single state, even if it could eventually be dealt with at the readout level if the number of units increases.</p><p>The results above indicate that value-RNN could be trained online by fixed random feedback at least to a certain extent, although somewhat less effectively than by backprop-type feedback. Results of individual simulations shown in <xref ref-type="fig" rid="fig2">Figure 2H, I</xref> indicate that state values developed in oVRNNrf were largely comparable to those developed in oVRNNbp once they were successfully learned, but the success rate was smaller than oVRNNbp while still larger than the untrained RNN.</p><sec id="s2-2-1"><title>Systematic simulations and analyses</title><p>Next, we tested whether learning performance of oVRNNbp, oVRNNrf, and the agent with untrained fixed RNN depends on the number of RNN units (<italic>n</italic>). For a valid comparison with the previously shown cases with 7 RNN units, the learning rate for the value weights was normalized by dividing by <italic>n</italic>/7. Learning performance was measured by the mean of squares of differences between the state values developed by each of these three types of agents and the estimated true state values (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) between cue and reward at 1000th trial. As shown in the left panel of <xref ref-type="fig" rid="fig2">Figure 2J</xref>, on average across simulations, oVRNNbp and oVRNNrf exhibited largely comparable performance and always outperformed the untrained RNN (p &lt; 0.00022 in Wilcoxon rank sum test for oVRNNbp or oVRNNrf vs untrained for each number of RNN units), although oVRNNbp somewhat outperformed or underperformed oVRNNrf when the number of RNN units was small (≤10 (p &lt; 0.049)) or large (≥25 (p &lt; 0.045)), respectively. As the number of RNN units increased from 5 to 15 or 20, all three agents improved their performance. Additional increase of RNN units did not largely change the mean performance in oVRNNrf, while moderately decreasing it in oVRNNbp and untrained RNN. The green line in <xref ref-type="fig" rid="fig2">Figure 2J</xref>, right shows the performance of a special case where the random feedback in oVRNNrf was fixed to the direction of (1, 1,..., 1)<sup>T</sup> (i.e., uniform feedback) with a random coefficient, which was largely comparable to, but somewhat worse than, that for the general oVRNNrf (blue line).</p><p>In order to examine the dimensionality of RNN dynamics, we conducted principal component analysis (PCA) of the time series (for 1000 trials) of RNN activities and calculated the contribution ratios of PCs in the cases of oVRNNbp, oVRNNrf, and untrained RNN with 20 RNN units. <xref ref-type="fig" rid="fig2">Figure 2K</xref> shows a log of contribution ratios of 20 PCs in each case. Compared with the case of untrained RNN, in oVRNNbp and oVRNNrf, initial component(s) had smaller contributions (PC1 (<italic>t</italic>-test p = 0.00018 in oVRNNbp; p = 0.0058 in oVRNNrf) and PC2 (p = 0.080 in oVRNNbp; p = 0.0026 in oVRNNrf)) while later components had larger contributions (PC3–10, 15–20, p &lt; 0.041 in oVRNNbp; PC5–20, p &lt; 0.0017 in oVRNNrf) on average, and this is considered to underlie their superior learning performance. We noticed that late components had larger contributions in oVRNNrf than in oVRNNbp, although these two models with 20 RNN units were comparable in terms of cue–reward state values (<xref ref-type="fig" rid="fig2">Figure 2J</xref>, left).</p><p>We examined how learning proceeded across trials in the models with 20 RNN units. As shown in <xref ref-type="fig" rid="fig2">Figure 2L</xref>, learning became largely converged by the 1000th trial, although slight improvement continued afterward. We further examined the cases with longer cue–reward delays. As shown in <xref ref-type="fig" rid="fig2">Figure 2M</xref>, as the delay increased, the mean squared error of state values (at 3000-th trial) increased, but the relative superiority of oVRNNbp and oVRNNrf over the model with untrained RNN remained to hold, except for cases with small number of RNN units (5) and long delay (5 or 6) (p &lt; 0.0025 in Wilcoxon rank sum test for oVRNNbp or oVRNNrf vs untrained for each number of RNN units for each delay).</p></sec><sec id="s2-2-2"><title>Occurrence of FA and an intuitive understanding of its mechanism</title><p>Next, we questioned how FA contributes to the learnability of oVRNNrf. To address this question, we used an RNN with 7 units and examined whether the value weight vector <italic>w</italic> became aligned to the random feedback vector <italic>c</italic> in oVRNNrf, by looking at the changes in the angle between these two vectors across trials. As shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, this angle, averaged across simulations, decreased over trials, indicating that the value weight <italic>w</italic> indeed tended to become aligned to the random feedback <italic>c</italic>. We then examined whether better alignment of <italic>w</italic> to <italic>c</italic> related to better development of state value by looking at the relation between the angle between <italic>w</italic> and <italic>c</italic> and the value of the pre-reward state at 1000th trial. As shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, there was a negative correlation such that the smaller the angle (i.e., more aligned), the larger the state value (<italic>r</italic> = −0.288, p = 0.00362), consistent with our expectation. These results indicate that the mechanism of FA, previously shown to work for supervised learning, also worked for TD learning of value weights and recurrent/feed-forward connections.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Occurrence of feedback alignment and an intuitive understanding of its mechanism.</title><p>(<bold>A</bold>) Over-trial changes in the angle between the value-weight vector <italic>w</italic> and the fixed random feedback vector <italic>c</italic> in the simulations of oVRNNrf (7 recurrent neural network [RNN] units). The solid line and the dashed lines indicate the mean ± SD across 100 simulations, respectively. (<bold>B</bold>) Negative correlation (<italic>r</italic> = −0.288, p = 0.00362) between the angle between <italic>w</italic> and <italic>c</italic> (horizontal axis) and the value of the pre-reward state (vertical axis) at 1000th trial. The dots indicate the results of individual simulations, and the line indicates the regression line. (<bold>C</bold>) Angle between the hypothetical change in <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">B</mml:mi></mml:mrow><mml:mi>o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$x\left (t\right)=f\left (Ax\left (t- 1\right),{\rm B}o\left (t- 1\right)\right)$\end{document}</tex-math></alternatives></inline-formula> in cases <bold>A</bold> and <bold>B</bold> were replaced with their updated ones, multiplied with the sign of TD-RPE (sign(<italic>δ</italic>(<italic>t</italic>))), and the fixed random feedback vector <italic>c</italic> across time steps. The black thick line and the gray lines indicate the mean ± SD across 100 simulations, respectively (same applied to (<bold>D</bold>)). (<bold>D</bold>) Multiplication of TD-RPEs in successive trials at individual states (top: cue, fourth from the top: reward). Positive or negative value indicates that TD-RPEs in successive trials have the same or different signs, respectively. (<bold>E</bold>) <italic>Left</italic>: RNN trajectories mapped onto the primary and secondary principal components (horizontal and vertical axes, respectively) in three successive trials (red, blue, and green lines (heavily overlapped)) at different phases in an example simulation (10th to 12th, 300th to 302nd, 600th to 602nd, and 900th to 902nd trials from top to bottom). The crosses and circles indicate the cue and reward states, respectively. <italic>Right</italic>: State values (black lines) and TD-RPEs (red lines) at 11th, 301st, 601st, and 901st trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig3-v1.tif"/></fig><p>How did the FA mechanistically occur? We made an attempt to obtain an intuitive understanding. Assume that positive TD-RPE <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$(\delta (t)\gt 0)$\end{document}</tex-math></alternatives></inline-formula> is generated in a state, <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$S(=\boldsymbol x(t))$\end{document}</tex-math></alternatives></inline-formula> in a trial. Because of the update rule for <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$\boldsymbol w (\boldsymbol w\leftarrow \boldsymbol w+a\delta (t)\boldsymbol x (t))$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 1.9</xref> in the Methods), <italic>w</italic> is updated in the direction of <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$\boldsymbol x (t)$\end{document}</tex-math></alternatives></inline-formula>. Next, what is the effect of updates of recurrent/feed-forward connections (<bold>A</bold> and <bold>B</bold>) on <italic>x</italic>? For simplicity, here we consider the case where observation is null (<italic>o</italic> = <bold>0</bold>) and so <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$\boldsymbol x(t)=\boldsymbol f({\boldsymbol {\rm A}\boldsymbol x}(t- 1))$\end{document}</tex-math></alternatives></inline-formula> holds (but a similar argument can be done in the case where observation is not null). If <bold>A</bold> is replaced with its updated one, it can be calculated that the <italic>i</italic>th element of <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$\boldsymbol {Ax}\left (t- 1\right)$\end{document}</tex-math></alternatives></inline-formula> will hypothetically change by <italic>c<sub>i</sub></italic> × (a positive value) (technical note: the value is <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>a</mml:mi><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$a\delta (t) \{\Sigma _{j}x_{j}(t- 1)^{2} \} (0.5+x_{i} (t)) (0.5- x_{i}(t))$\end{document}</tex-math></alternatives></inline-formula> (cf., <xref ref-type="disp-formula" rid="equ11">Equation 1.10</xref>) which is positive unless <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$x(t- 1)=\boldsymbol 0$\end{document}</tex-math></alternatives></inline-formula>), and therefore the vector <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$\boldsymbol {{\rm A}x}(t- 1)$\end{document}</tex-math></alternatives></inline-formula> as a whole will hypothetically change by a vector that is in a relatively close angle with <italic>c</italic>, or more specifically, is in the same quadrant as (and thus within at maximum 90° from) <italic>c</italic> (e.g, [<italic>c</italic><sub>1</sub> <italic>c</italic><sub>2</sub> <italic>c</italic><sub>3</sub>]<sup>T</sup> and [0.5<italic>c</italic><sub>1</sub> 1.2<italic>c</italic><sub>2</sub> 0.8<italic>c</italic><sub>3</sub>]<sup>T</sup>). Then, because <italic>f</italic> is a monotonically increasing sigmoidal function, <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$\boldsymbol x\left (t\right)=f\left (\boldsymbol {Ax}\left (t- 1\right)\right)$\end{document}</tex-math></alternatives></inline-formula> will also hypothetically change by a vector that is in a relatively close angle with <italic>c</italic>. This was indeed the case in our simulations as shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, which plotted the angle between the hypothetical change in <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$\boldsymbol x(t)=\boldsymbol f (\boldsymbol {Ax} (t- 1),\boldsymbol {Bo} (t- 1))$\end{document}</tex-math></alternatives></inline-formula> in cases <bold>A</bold> and <bold>B</bold> were replaced with their updated ones, multiplied with the sign of TD-RPE (sign(<italic>δ</italic>(<italic>t</italic>))), and the fixed random feedback vector <italic>c</italic> across time steps.</p><p>In this way, at state <italic>S</italic> where TD-RPE is positive, <italic>w</italic> is updated in the direction of <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> will hypothetically change by a vector that is in a relatively close angle with <italic>c</italic> if <bold>A</bold> is replaced with its updated one. Then, if the update of <italic>w</italic> and the hypothetical change in <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> due to the update of <bold>A</bold> could be integrated, <italic>w</italic> would become aligned to <italic>c</italic> (if TD-RPE is instead negative, <italic>w</italic> is updated in the opposite direction of <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> will hypothetically change by a vector that is in a relatively close angle with −<italic>c</italic>, and so the same story holds in the end).</p><p>There is, however, a caveat regarding how the update of <italic>w</italic> and the hypothetical change in <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> can be integrated. Although technical, here we briefly describe the caveat and a possible solution for it. The updates of <italic>w</italic> and <bold>A</bold> use TD-RPE, which are calculated based on <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$v(t)=\boldsymbol w^{T}\boldsymbol x (t)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$v (t+1)=w^{T}\boldsymbol x (t+1)$\end{document}</tex-math></alternatives></inline-formula>, and so <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$\boldsymbol x(t)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$\boldsymbol x(t+1)$\end{document}</tex-math></alternatives></inline-formula> should already be determined beforehand. Therefore, the hypothetical change in <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$\boldsymbol x(t)$\end{document}</tex-math></alternatives></inline-formula> due to the update of <bold>A</bold>, described in the above, does not actually occur (this was why we mentioned ‘hypothetical’) and thus cannot be integrated with the update of <italic>w</italic>. Nevertheless, integration could still occur across successive trials, at least to a certain extent. Specifically, although TD-RPEs at <italic>S</italic> in successive trials would generally differ from each other, they would still tend to have the same sign, as was indeed the case in our simulations (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Also, although the trajectories of RNN activity (<italic>x</italic>) in successive trials would differ, we could expect a certain level of similarity because the RNN is entrained by observation-representing inputs, again as was indeed the case in our example simulation (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Then, the hypothetical change in <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$\boldsymbol x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> due to the update of <bold>A</bold>, considered above, could become a reality in the next trial, to a certain extent, and could thus be integrated into the update of <italic>w</italic>, explaining the occurrence of FA.</p></sec><sec id="s2-2-3"><title>Simulation of tasks with probabilistic structures of reward timing/existence</title><p>Previous work (<xref ref-type="bibr" rid="bib124">Starkweather et al., 2017</xref>) examined the response of DA neurons in cue–reward association tasks in which reward timing was probabilistically determined (early in some trials but late in other trials). There were two tasks, which were largely similar, but there was a key difference that reward was given in all the trials in one task, whereas reward was omitted in some randomly determined trials in another task. <xref ref-type="bibr" rid="bib124">Starkweather et al., 2017</xref> found that the DA response to later reward was smaller than the response to earlier reward in the former task, presumably reflecting the animal’s belief that delayed reward will surely come, but the opposite was the case in the latter task, presumably because the animal suspected that reward was omitted in that trial. <xref ref-type="bibr" rid="bib124">Starkweather et al., 2017</xref> then showed that such response patterns could be explained if DA encoded TD-RPE under particular state representations that incorporated the probabilistic structures of the task (called the ‘belief state’). In that study, such state representations were ‘handcrafted’ by the authors, but the subsequent work (<xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>) showed that the original value-RNN with backprop (BPTT) could develop similar representations and reproduce the experimentally observed DA patterns.</p><p>In order to examine if our online value-RNN with fixed random feedback could also explain those experimental results, we simulated two tasks (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) that were qualitatively similar to (though simpler than) the two tasks examined in the experiments (<xref ref-type="bibr" rid="bib124">Starkweather et al., 2017</xref>). In our task 1, a cue was always followed by a reward either two or four time steps later with equal probabilities. Task 2 was the same as task 1 except that reward was omitted with 40% probability. In task 1, if reward was not given at the early timing (i.e., two steps later than cue), agent could predict that reward would be given at the late timing (i.e., four steps later than cue), and thus TD-RPE upon reward at the late timing is expected to be smaller than TD-RPE upon reward at the early timing (if agent perfectly learned the task structure, TD-RPE upon reward at the late timing should be 0). By contrast, in task 2, if reward was not given at the early timing, it might indicate that reward would be given at the late timing but might instead indicate that reward would be omitted in that trial, and thus TD-RPE upon reward at the late timing is expected to exist and can even be larger than TD-RPE upon reward at the early timing.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Simulation of two tasks having probabilistic structures, which were qualitatively similar to the two tasks examined in experiments (<xref ref-type="bibr" rid="bib124">Starkweather et al., 2017</xref>) and modeled by the original value-recurrent neural network (RNN) with Backpropagation-Through-Time (BPTT) (<xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>).</title><p>(<bold>A</bold>) Simulated two tasks, in which reward was given at the early or the late timing with equal probabilities in all the trials (task 1) or 60% of trials (task 2). (<bold>B</bold>) (<bold>a</bold>) <italic>Top</italic>: Trial types. Two trial types (with early reward and with late reward) in task 1 and three trial types (with early reward, with late reward, and without reward) in task 2. <italic>Bottom</italic>: Value of each timing in each trial type estimated through simulations. (<bold>b</bold>) Agent’s probabilistic belief about the current trial type, in the case where agent was in fact in the trial with early reward (top row), the trial with late reward (second row), or the trial without reward (third row in task 2). (<bold>c</bold>) <italic>Top</italic>: States defined by considering the probabilistic beliefs at each timing from cue. <italic>Bottom</italic>: True state/timing values calculated by taking (mathematical) expected value of the estimated value of each timing in each trial type. (<bold>C</bold>) Expected TD-RPE calculated from the estimated true values of the states/timings for task 1 (left) and task 2 (right). Red lines: case where reward was given at the early timing, blue lines: case where reward was given at the late timing. It is expected that TD-RPE at early reward is larger than TD-RPE at late reward in task 1, whereas the opposite is the case in task 2, as indicated by the inequality signs. (<bold>D–H</bold>) TD-RPEs at the latest trial within 1000 trials in which reward was given at the early timing (red lines) or the late timing (blue lines), averaged across 100 simulations (error bars indicating ± SEM across simulations), in the different types of agent: TD-RL agent having punctate state representation and state values without (<bold>D</bold>) or with (<bold>E</bold>) continuation between trials; (<bold>F</bold>) oVRNNbp. The number of RNN units was 12 (same applied to (<bold>G, H</bold>)); (<bold>G</bold>) oVRNNrf; (<bold>H</bold>) agent with untrained RNN. The p values are for paired <italic>t</italic>-test between TD-RPE at early reward and TD-RPE at late reward (100 pairs, two-tailed), and the <italic>d</italic> values are Cohen’s <italic>d</italic> using an average variance and their signs are with respect to the expected patterns shown in (<bold>C</bold>) (same applied to Figures 8 and 9C).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig4-v1.tif"/></fig><p>In these tasks, states can be defined in the following way. There were two types of trials, with early or late reward, in task 1, and additionally one more type of trial, without reward, in task 2 (<xref ref-type="fig" rid="fig4">Figure 4Ba</xref>, top). For each timing after receival of cue information in each of these trial types, its value can be estimated through simulations (<xref ref-type="fig" rid="fig4">Figure 4Ba</xref>, bottom). The agent could not know the current trial type until receiving reward at the early timing or the late timing or receiving no reward at both timings. Until these timings, the agent could have probabilistic belief about the current trial type, for example, 50% in the trial with early reward and 50% in the trial with late reward (in task 1) or 30% in the trial with early reward, 30% in the trial with late reward, and 40% in the trial without reward (in task 2) (<xref ref-type="fig" rid="fig4">Figure 4Bb</xref>). States of the timings after receival of cue information can be defined by incorporating these probabilistic beliefs at each timing (<xref ref-type="fig" rid="fig4">Figure 4Bc</xref>, top). Their true values can be calculated by taking (mathematical) expected value of the estimated values of each timing in each trial type (<xref ref-type="fig" rid="fig4">Figure 4Bc</xref>, bottom). Expected TD-RPE calculated from the estimated true values (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) exhibited features that matched the conjecture mentioned above: in task 1, TD-RPE upon reception of late reward, which was actually 0, was smaller than TD-RPE upon reception of early reward, whereas in task 2, TD-RPE upon reception of late reward was larger than TD-RPE upon reception of early reward (as indicated by the inequality signs on <xref ref-type="fig" rid="fig4">Figure 4C</xref>).</p><p>As mentioned above, the previous work (<xref ref-type="bibr" rid="bib124">Starkweather et al., 2017</xref>) has shown that VTA DA neurons exhibited similar activity patterns to the abovementioned TD-RPE patterns, and the subsequent work (<xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>) has shown that the original value-RNN with backprop (BPTT) could reproduce such TD-RPE patterns. We examined how our oVRNNbp and oVRNNrf (with 12 RNN units) behaved in our simulated two tasks. oVRNNbp developed the expected TD-RPE patterns, that is, smaller TD-RPE upon late than early timing in task 1 but opposite pattern in task 2 (<xref ref-type="fig" rid="fig4">Figure 4F</xref>), and oVRNNrf also developed such patterns, although the effect size for task 1 was small (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). These results indicate that online value-RNN could learn the probabilistic structures of the tasks even with fixed random feedback. By contrast, agents with punctate state representation without or with continuous value update across trials (<xref ref-type="fig" rid="fig4">Figure 4D, E</xref>), as well as agents with untrained fixed RNN (<xref ref-type="fig" rid="fig4">Figure 4H</xref>), could not develop such patterns well.</p></sec></sec><sec id="s2-3"><title>Online value-RNN with further biological constraints</title><p>So far, the activities of neurons in the RNN (<italic>x</italic>) were initialized to pseudo standard normal random numbers, and thereafter took numbers in the range between −0.5 and 0.5 that was the range of the sigmoidal input-output function. The value weights (<italic>w</italic>) could also take both positive and negative values since no constraint was imposed. The fixed random feedback in oVRNNrf (<italic>c</italic>) was generated by pseudo standard normal random numbers, and so could also be positive or negative. Negativity of the neurons’ activities and the value weights could potentially be regarded as inhibitory or smaller-than-baseline quantities. However, because neuronal firing rate is non-negative and cortico-striatal projections are excitatory, it would be biologically more plausible to assume that the activities of neurons in the RNN and the value weights are non-negative. As for the fixed random feedback, if it is negative, the update rule becomes anti-Hebbian under positive TD-RPE, and so assuming non-negativity would be plausible since Hebbian property has been suggested for rapid plasticity of cortical synapses (<xref ref-type="bibr" rid="bib33">Feldman, 2009</xref>) (see Appendix 1.2 for possible consideration of behavioral time-scale synaptic plasticity (BTSP) in our models). Regarding the connection weights in/onto the RNN, here we keep the original assumption that they could be positive or negative because it could be an approximate description of recurrent neuronal network with both recurrent excitation and inhibition. Later (the ‘Models with excitatory and inhibitory units’ section) we will examine extended models that incorporate excitatory and inhibitory units and conform to Dale’s law.</p><p>Other than the sign of connection weights, there was another biological plausibility issue in the update rule for recurrent and feed-forward connections that were derived from the gradient descent. Specifically, the dependence on the post-synaptic activity was non-monotonic, maximized at the middle of the range of activity. It would be more biologically plausible to assume a monotonic increase (while an <italic>opposite</italic> shape of non-monotonicity, once decrease and thereafter increase, called the BCM (Bienenstock–Cooper–Munro) rule has actually been suggested; <xref ref-type="bibr" rid="bib9">Bienenstock et al., 1982</xref>; <xref ref-type="bibr" rid="bib42">Gjorgjieva et al., 2011</xref>; <xref ref-type="bibr" rid="bib119">Shouval, 2011</xref>).</p><p>In order to address these issues, we considered revised models. We first considered a revised oVRNNbp (with backprop-type transported weights), referred to as oVRNNbp-rev, in which the RNN activities and the value weights were constrained to be non-negative, while the non-monotonic dependence of the update rule on the post-synaptic activity remained unchanged (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We then considered a revised oVRNNrf, referred to as oVRNNrf-bio, in which the fixed random feedback, as well as the RNN activities and the value weights, were constrained to be non-negative, and also the update rule was modified so that the dependence on the post-synaptic activity became monotonic (with saturation) (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Revised online value-recurrent neural network (RNN) models with further biological constraints.</title><p>(<bold>A</bold>) oVRNNbp-rev: oVRNNbp (online value-RNN with backprop) was modified so that the activities of neurons in the RNN (<italic>x</italic>) and the value weights (<italic>w</italic>) became non-negative. (<bold>B</bold>) oVRNNrf-bio: oVRNNrf (online value-RNN with fixed random feedback) was modified so that <italic>x</italic> and <italic>w</italic>, as well as the fixed random feedback (<italic>c</italic>), became non-negative and also the dependence of the update rules for recurrent/feed-forward connections (<bold>A, B</bold>) on post-synaptic activity became monotonic + saturation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig5-v1.tif"/></fig><p>We examined how these revised models, in comparison with agents with untrained RNNs that also had non-negative constraints for <italic>x</italic> and <italic>w</italic>, performed in the Pavlovian cue–reward association task examined above (the numbers of RNN units and trials were set to 12 and 1500, respectively). oVRNNbp-rev well developed state values toward reward (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). oVRNNrf-bio also developed state values to a largely comparable extent (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). By contrast, the agent with an untrained RNN could not develop such a pattern of state values (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This, however, could be because initially set recurrent/feed-forward connections were far from those learned in the online value-RNNs. Therefore, as a more strict control, we conducted simulations of agents with untrained RNN with non-negative <italic>x</italic> and <italic>w</italic>, where in each simulation the recurrent/feed-forward connections were set to be those shuffled from the learned connections in a simulation of oVRNNrf-bio (hereafter we refer to these two types of untrained RNN as ‘naive untrained RNN’ and ‘shuffled untrained RNN’). The model with shuffled untrained RNN developed state values somewhat better than the naive untrained RNN case (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), but still worse than oVRNNbp-rev and oVRNNrf-bio.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Performances of the revised online value-recurrent neural network (RNN) models in the cue–reward association task, in comparison with models with untrained RNN that also had the non-negative constraint.</title><p>State values (black lines) and TD-RPEs (red lines) at the 1500th trial in oVRNNbp-rev (<bold>A</bold>), oVRNNrf-bio (<bold>B</bold>), agent with naive untrained RNN (i.e., randomly initialized RNN) with <italic>x</italic> and <italic>w</italic> constrained to be non-negative (<bold>C</bold>), and agent with untrained RNN with connections shuffled from those learned in oVRNNrf-bio and also with non-negative <italic>x</italic> and <italic>w</italic> (<bold>D</bold>). The number of RNN units was 12 in all the cases. Error bars indicate mean ± SEM across 100 simulations; same applied to the followings unless otherwise mentioned. The right histograms show the across-simulation distribution of the value of the pre-reward state in each model. The vertical black dashed lines in the histograms indicate the true value of the pre-reward state (estimated through simulations). (<bold>E</bold>) <italic>Left</italic>: Mean squared value-error at the 1500th trial in oVRNNbp-rev (red line), oVRNNrf-bio (blue line), agent with naive untrained RNN (gray solid line: partly out of view), and agent with shuffled untrained RNN (gray dotted line) when the number of RNN units (<bold>n</bold>) was varied from 5 to 40. Learning rate for value weights was normalized by dividing by <italic>n</italic>/12 (same applied to the followings). <italic>Right</italic>: Mean squared value-error in oVRNNrf-bio (blue line: same data as in the left panel), oVRNN-bio with random-magnitude uniform feedback (green line), oVRNN-bio with fixed-magnitude (0.5) uniform feedback (light blue line), and oVRNNrf-rev where the update rule of oVRNNrf-bio was changed back to the original one (blue dotted line). (<bold>F</bold>) <italic>Left</italic>: Mean of the elements of the recurrent and feed-forward connections (at 1500th trial) of oVRNNbp-rev (red line), oVRNNrf-bio (blue line), and naive untrained RNN (gray solid line). <italic>Right</italic>: Mean of the elements of the recurrent and feed-forward connections of oVRNNrf-bio (blue line: same data as in the left panel), oVRNN-bio with random-magnitude uniform feedback (green line), oVRNN-bio with fixed-magnitude (0.5) uniform feedback (light blue line), and oVRNNrf-rev (blue dotted line). (<bold>G</bold>) Learned state values (left panel) and TD-RPEs (right panel) in oVRNNbp-rev (red lines) and oVRNNrf-bio (blue lines) in the cases with 40 RNN units, compared to the estimated true values (black lines). (<bold>H</bold>) Log of contribution ratios of the principal components of the time series (for 1500 trials) of RNN activities in each model with 20 RNN units. (<bold>I</bold>) Mean squared value-error in each model with 20 RNN units across trials. (<bold>J</bold>) Mean squared value-error at 3000th trial in each model in the cases where the cue–reward delay was 3, 4, 5, or 6 time steps (top to bottom panels). Left and right panels show the results with default learning rates and halved learning rates, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig6-v1.tif"/></fig><sec id="s2-3-1"><title>Systematic simulations and analyses</title><p>We varied the number of RNN units (<italic>n</italic>), with the learning rate for value weights normalized by dividing by <italic>n</italic>/12, and compared the performance (mean of squared errors of state values between cue and reward at 1500th trial) of oVRNNbp-rev and oVRNNrf-bio, in comparison with models with naive or shuffled untrained RNN. As shown in the left panel of <xref ref-type="fig" rid="fig6">Figure 6E</xref>, oVRNNbp-rev and oVRNNrf-bio exhibited largely comparable performance and always outperformed the models with untrained RNN (p &lt; 2.5 × 10<sup>−12</sup> in Wilcoxon rank sum test for oVRNNbp-rev or oVRNNrf-bio vs naive or shuffled untrained for each number of RNN units), although oVRNNbp-rev somewhat outperformed or underperformed oVRNNrf-bio when the number of RNN units was small (≤10 (p &lt; 0.00029)) or large (≥25 (p &lt; 3.7 × 10<sup>−6</sup>)), respectively (<xref ref-type="fig" rid="fig6">Figure 6G</xref> shows the learned state values and TD-RPEs in oVRNNbp-rev and oVRNNrf-bio in the cases with 40 RNN units, compared to the estimated true values). Remarkably, oVRNNrf-bio generally achieved better performance than both oVRNNbp and oVRNNrf, which did not have the non-negative constraint (Wilcoxon rank sum test, vs oVRNNbp: p &lt; 7.8 × 10<sup>−6</sup> for 5 or ≥25 RNN units; vs oVRNNrf: p &lt; 0.021 for ≤10 or≥20 RNN units).</p><p>The left panel of <xref ref-type="fig" rid="fig6">Figure 6F</xref> shows the mean of the elements of the recurrent and feed-forward connections at the 1500th trial in the different models. As shown in this figure, these connections (initialized to pseudo standard normal random numbers) were learned to become negative on average, in oVRNNbp-rev and oVRNNrf-bio. This learned negative-dominance (inhibition-dominance) could possibly be related, for example, through prevention of excessive activity, to the good performance of oVRNNrf-bio and also the better performance of the shuffled untrained RNN than the naive untrained RNN. The green and light blue lines in the right panels of <xref ref-type="fig" rid="fig6">Figure 6E, F</xref> show the results for special cases where the random feedback in oVRNNrf-bio was fixed to the direction of (1, 1,..., 1)<sup>T</sup> (i.e., uniform feedback) with a random non-negative magnitude (green line) or a fixed magnitude of 0.5 (light blue line). The performance of these special cases, especially the former (with random magnitude), was somewhat worse than that of oVRNNrf-bio, but still better than that of the models with untrained RNN. The blue dotted lines in the right panels of <xref ref-type="fig" rid="fig6">Figure 6E, F</xref> show the results where the modified update rule of oVRNNrf-bio was changed back to the original rule with non-monotonic dependence on the post-synaptic activity (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The performance of this model was somewhat worse than oVRNNrf-bio, indicating that the biologically motivated modification of the update rule in fact improved the performance.</p><p><xref ref-type="fig" rid="fig6">Figure 6H</xref> shows contribution ratios of PCs of the time series of RNN activities in each model with 20 RNN units. Compared with the cases with naive/shuffled untrained RNN, in oVRNNbp-rev and oVRNNrf-bio, later components had relatively high contributions (PC5–20, p &lt; 1.4 × 10<sup>−6</sup> (<italic>t</italic>-test vs naive) or &lt;0.014 (vs shuffled) in oVRNNbp-rev; PC6–20, p &lt; 2.0 × 10<sup>−7</sup> (vs naive) or PC7–20, p &lt; 5.9 × 10<sup>−14</sup> (vs shuffled) in oVRNNrf-bio), explaining their superior value-learning performance. <xref ref-type="fig" rid="fig6">Figure 6I</xref> shows how learning proceeded across trials in the models with 20 RNN units. While oVRNNbp-rev and oVRNNrf-bio eventually reached a comparable level of errors, oVRNNrf-bio outperformed oVRNNbp-rev in early trials (at 200, 300, 400, or 500 trials; p &lt; 0.049 in Wilcoxon rank sum test for each). This is presumably because the value weights did not develop well in early trials, and so the backprop-type feedback, which was the same as the value weights, did not work well, while the non-negative fixed random feedback worked finely from the beginning. <xref ref-type="fig" rid="fig6">Figure 6J</xref> shows the cases with longer cue–reward delays, with default or halved learning rates. As the delay increased, the mean squared error of state values (at 3000th trial) increased, but the relative superiority of oVRNNbp-rev and oVRNNrf-bio over the models with untrained RNN remained to hold, except for a few cases with 5 RNN units (5 delay oVRNNrf-bio vs shuffled with default learning rate, 6 delay oVRNNrf-bio vs naive or shuffled with halved learning rate) (p &lt; 0.047 in Wilcoxon rank sum test for oVRNNbp-rev or oVRNNrf-bio vs naive or shuffled untrained for each number of RNN units for each delay). We further examined how the revised online value-RNN models performed in the two tasks with probabilistic structures examined above. The models with 12 RNN units appeared not able to produce the expected different patterns of TD-RPEs in the two tasks (TD-RPE at early reward &gt; TD RPE at late reward in task 1 and opposite pattern in task 2), and we increased the number of RNN units to 20. Then, both oVRNNbp-rev and oVRNNrf-bio produced such TD-RPE patterns (<xref ref-type="fig" rid="fig7">Figure 7A, B</xref>), whereas the models with untrained RNN could not (<xref ref-type="fig" rid="fig7">Figure 7C, D</xref>). This indicates that the online value-RNN with random feedback and further biological constraints could learn the differential characteristics of the tasks.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Performances of the revised online value-recurrent neural network (RNN) models with further biological constraints in the two tasks having probabilistic structures, in comparison with models with untrained RNN.</title><p>TD-RPEs at the latest trial within 2000 trials in which reward was given at the early timing (red lines) or the late timing (blue lines) in task 1 (left) and task 2 (right), averaged across 100 simulations (error bars indicating ± SEM across simulations), are shown for the four types of agent: (<bold>A</bold>) oVRNNbp-rev; (<bold>B</bold>) oVRNNrf-bio; (<bold>C</bold>) agent with naive untrained RNN; (<bold>D</bold>) agent with untrained RNN with connections shuffled from those learnt in oVRNNrf-bio. The number of RNN units was 20 for all the cases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig7-v1.tif"/></fig></sec><sec id="s2-3-2"><title>Loose alignment and FA</title><p>Coming back to the original cue–reward association task, we examined how the angle between the value weights (<italic>w</italic>) and the random feedback (<italic>c</italic>) changed across trials in oVRNNrf-bio with 12 RNN units. As shown in <xref ref-type="fig" rid="fig8">Figure 8A</xref>, the angle was on average smaller than 90°, which was the chance-level angle in the case without non-negative constraint, from the beginning, while there was no further alignment over trials. This could be understood as follows. Because both the value weights (<italic>w</italic>) and the random feedback (<italic>c</italic>) were now constrained to be non-negative, these two vectors were ensured to be in a relatively close angle (i.e., in the same quadrant) from the beginning. By virtue of this loose alignment, the random feedback could act similarly to backprop-type transported-weight feedback, even without further alignment.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Loose alignment of the value weights (<italic>w</italic>) and the random feedback (<italic>c</italic>) in oVRNNrf-bio (with 12 recurrent neural network [RNN] units).</title><p>(<bold>A</bold>) Over-trial changes in the angle between the value weights <italic>w</italic> and the fixed random feedback <italic>c</italic>. The solid line and the dashed lines indicate the mean ± SD across 100 simulations, respectively. (<bold>B</bold>) No correlation between the <italic>w–c</italic> angle (horizontal axis) and the value of the pre-reward state (vertical axis) at 1500th trial (<italic>r</italic> = 0.0117, p = 0.908). The dots indicate the results of individual simulations. (<bold>C</bold>) Correlation between the <italic>w–c</italic> angle at <italic>k</italic>th trial (horizontal axis) and the value of the cue, post-cue, pre-reward, or reward state (top-bottom panels) at 500th trial across 1000 simulations. The solid lines indicate the correlation coefficient, and the short vertical bars at the top of each panel indicate the cases in which p-value was less than 0.05. (<bold>D</bold>) Distribution of the angle between two 12-dimensional vectors when the elements of both vectors were drawn from [0 1] uniform pseudo-random numbers (<bold>a</bold>) or when one of the vectors was replaced with [1 0 0... 0] (i.e., on the edge of the non-negative quadrant) (<bold>b</bold>) or [1 1 0... 0] (i.e., on the boundary of the non-negative quadrant) (<bold>c</bold>). (<bold>E</bold>) Across-simulations histograms of elements of <italic>w</italic> in oVRNNrf-bio with 12 RNN units ordered from the largest to smallest ones after 1500 trials when there was no value-weight-decay (<bold>a</bold>) or there was value-weight-decay with decay rate (per time step) of 0.001 (<bold>b</bold>) or 0.002 (<bold>c</bold>). The error bars indicate the mean ± SEM across 100 simulations. (<bold>F</bold>) Over-trial changes in the angle between the value weights <italic>w</italic> and the fixed random feedback <italic>c</italic> when there was value-weight decay with decay rate (per time step) of 0.001 (top panel) or 0.002 (bottom panel). Notations are the same as those in (<bold>A</bold>). (<bold>G</bold>) Mean squared value-error at the 1500th trial in oVRNNrf-bio with 12 RNN units with the rate of value-weight decay varied (horizontal axis). The error bars indicate the mean ± SEM across 100 simulations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig8-v1.tif"/></fig><p>We examined if the angle between the value weights (<italic>w</italic>) and the random feedback (<italic>c</italic>) at the 1500th trial was associated with the developed value of pre-reward state across simulations, but found no association (<italic>r</italic> = 0.0117, p = 0.908) (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). We then examined if the <italic>w–c</italic> angle at earlier trials (2nd to 500th trials) was associated with the developed values at 500th trial, with the number of simulations increased to 1000 so that small correlation could be detected. We found that the <italic>w–c</italic> angle at initial trials (2nd to around 10th trials) was negatively correlated with the developed values of the reward state and preceding states at 500th trial (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). As for the reward state, negative correlation at around 100th to 300th trial was also observed. These results suggest that better alignment of <italic>w</italic> and <italic>c</italic> at initial and early timings was associated with better development of state values, in line with the conjecture that loose alignment of <italic>w</italic> and <italic>c</italic> coming from the non-negative constraint supported learning. It should be noted, however, that there were cases where positive (although small) correlation was observed. Its exact reason is not sure, but it could be related to the fact that the largeness of developed values or the speed of value development does not necessarily mean good learning.</p><p>As mentioned above, while the angle between <italic>w</italic> and <italic>c</italic> was on average smaller than 90° from the beginning, there was no further alignment over trials. This seemed mysterious because the mechanism for FA that we derived for the models without non-negative constraint was expected to work also for the models with non-negative constraint. As a possible reason for the non-occurrence of FA, we guessed that one or a few element(s) of <italic>w</italic> grew prominently during learning, and so <italic>w</italic> became close to an edge or boundary of the non-negative quadrant and thereby the angle between <italic>w</italic> and other vector became generally large (as illustrated in <xref ref-type="fig" rid="fig8">Figure 8D</xref>). <xref ref-type="fig" rid="fig8">Figure 8Ea</xref> shows the mean ± SEM of the elements of <italic>w</italic> ordered from the largest to smallest ones after 1500 trials. As conjectured above, a few elements indeed grew prominently.</p><p>We considered that if a slight decay (forgetting) of value weights (cf., <xref ref-type="bibr" rid="bib89">Morita and Kato, 2014</xref>; <xref ref-type="bibr" rid="bib57">Kato and Morita, 2016</xref>; <xref ref-type="bibr" rid="bib58">Kato and Morita, 2025</xref>) was assumed, such a prominent growth of a few elements of <italic>w</italic> may be mitigated and alignment of <italic>w</italic> to <italic>c</italic>, beyond the initial loose alignment because of the non-negative constraint, may occur. These conjectures were indeed confirmed by simulations (<xref ref-type="fig" rid="fig8">Figure 8Eb, c, F</xref>). The mean squared value-error slightly increased when the value-weight decay was assumed (<xref ref-type="fig" rid="fig8">Figure 8G</xref>); however, presumably reflecting a decrease in developed values and a deterioration of learning because of the decay.</p></sec></sec><sec id="s2-4"><title>Models with excitatory and inhibitory units</title><p>As mentioned above, in oVRNNbp-rev and oVRNNrf-bio, the connection weights in/onto the RNN could be both positive and negative, against Dale’s law. Recent studies started to examine neural networks incorporating Dale’s law (<xref ref-type="bibr" rid="bib24">Cornford et al., 2021</xref>; <xref ref-type="bibr" rid="bib69">Li et al., 2023</xref>) or other connectivity features (<xref ref-type="bibr" rid="bib78">Mastrogiuseppe and Ostojic, 2018</xref>). So we examined extended models, named oVRNNbp-rev-ei and oVRNNrf-bio-ei, which incorporated excitatory E-units, modeling pyramidal cells, and inhibitory I-units, modeling fast-spiking (FS) cells (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Cortical excitation can operate slowly due to slow synaptic dynamics (<xref ref-type="bibr" rid="bib83">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib85">Morishima et al., 2011</xref>) (see the description about the time step in the Methods for details). In contrast, inhibition from FS cells to pyramidal cells may operate more quickly, since it was shown (<xref ref-type="bibr" rid="bib87">Morita et al., 2008</xref>) that observed phases of regular-spiking (RS, putatively pyramidal) cells’ and FS cells’ spikes (<xref ref-type="bibr" rid="bib46">Hasenstaub et al., 2005</xref>) could be explained by fast FS → RS inhibition and temporally distributed recurrent excitation.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>oVRNNbp-rev-ei and oVRNNrf-bio-ei models incorporating excitatory E-units and inhibitory I-units.</title><p>(<bold>A</bold>) Schematic illustration of the models’ architecture. For ease of viewing, only limited parts of units and connections are drawn. (<bold>B</bold>) Mean squared value-error at the 1500th trial in the cue–reward association task in oVRNNbp-rev-ei (red line), oVRNNrf-bio-ei (blue line), and E-/I-units-incorporated models with naive untrained recurrent neural network (RNN) (i.e., randomly initialized RNN) (gray solid line) or untrained RNN with connections shuffled from those learned in oVRNNrf-bio-ei (gray dotted line). (<bold>C</bold>) Patterns of TD-RPE in the tasks with probabilistic structures generated in the four models with E-/I-units. Simulation conditions and notations are the same as those in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig9-v1.tif"/></fig><p>Given these, we assumed that excitation from E-units to E- and I-units took one time step whereas I → E inhibition operated within a time step, and also that each E-unit received inputs from all the E-units and a particular I-unit (although this assumption could be supported by the abovementioned suggestions, its validity remains largely open). Chemical and electrical connections between FS cells exist and are suggested to serve for synchronization or oscillation (<xref ref-type="bibr" rid="bib138">Wang, 2010</xref>; <xref ref-type="bibr" rid="bib16">Buzsáki and Wang, 2012</xref>), but we omitted I → I connections because our models did not describe fast spike dynamics. Since FS cells can fire at high frequencies, we assumed that the activation function for I-units was not saturating but linear. Lastly, we did not assume plasticity for connections from/to I-units. The connection weights onto the E- and I-units from the observation units and E-units were non-negatively initialized, specifically, initialized to pseudo normal random numbers with mean = 3 and SD = 1 and rectified to 0 when becoming negative.</p><p>We examined how these extended models behaved in the Pavlovian task and the probabilistic tasks. As shown in <xref ref-type="fig" rid="fig9">Figure 9B</xref>, oVRNNbp-rev-ei and oVRNNrf-bio-ei learned the state values in the Pavlovian task much more accurately than the models with naive untrained RNN or untrained RNN whose connections from the observation and E-units to E- and I-units were shuffled from oVRNNrf-bio-ei (p &lt; 5.2 × 10<sup>−12</sup> in Wilcoxon rank sum test for oVRNNbp-rev-ei or oVRNNrf-bio-ei vs naive or shuffled untrained for each number of RNN units). oVRNNbp-rev-ei somewhat outperformed or underperformed oVRNNrf-bio-ei when the number of RNN units was small (≤10 (p &lt; 0.0091)) or relatively large (15–35 (p &lt; 0.027)), respectively. Also, as shown in <xref ref-type="fig" rid="fig9">Figure 9C</xref>, oVRNNbp-rev-ei and oVRNNrf-bio-ei with 20 E-units and 20 I-units generated the expected different patterns of TD-RPEs in the two tasks (TD-RPE at early reward &gt; TD-RPE at late reward in task 1 and opposite pattern in task 2), although the effect size for task 2 in oVRNNrf-bio-ei was small, while the models with untrained RNN did not.</p><p>As such, the extended models with E- and I-units showed largely similar behaviors to those of the original oVRNNbp-rev and oVRNNrf-bio with mixed positive and negative RNN weights. This is actually reasonable, because combining the update equations for I- and E-units in the extended models (top two equations in <xref ref-type="fig" rid="fig9">Figure 9A</xref>) results in an equation largely similar to the update equation for RNN units in the original models (top equation in <xref ref-type="fig" rid="fig1">Figure 1</xref>). In other words, the original models with mixed positive and negative RNN weights could be regarded as a simplified description of the models with E- and I-units under the abovementioned assumptions. Therefore, for simplicity, we will return to mixed positive and negative RNN weights in the following.</p><sec id="s2-4-1"><title>Task with distractor cue</title><p>So far, we have examined situations where there existed a reward and a cue associated with the reward. However, in real environments, it is likely that there exist both reward-associated and non-associated (distractor) cues, and the agent does not initially know which cue is associated with reward and which is not. Learning cue–reward association in such distractor-existing environments is generally not easy for biologically constrained models, and it has been addressed by only a few previous works (<xref ref-type="bibr" rid="bib23">Cone et al., 2024</xref>). We examined whether our biologically constrained oVRNNrf-bio, as well as oVRNNbp-rev, could learn the cue–reward association under the presence of distractor cue. We considered a simple case where there existed a distractor cue, which was presented to the agent with a certain probability at every time step, between cue–reward duration or reward–cue duration (i.e., ITI) or simultaneously with cue or reward. As for the agent’s models, we assumed that the observation inputs had an additional element (dimension), which was set to 1 when the distractor was presented and 0 when not (<xref ref-type="fig" rid="fig10">Figure 10A</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Cue–reward association task with distractor cue.</title><p>(<bold>A</bold>) Modification of oVRNNbp-rev and oVRNNrf-bio to incorporate possible existence of distractor cue. The observation units <italic>o</italic> had an additional element (leftmost circle labeled as ‘Dist’), which was 1 at the time steps where distractor cue was present and 0 otherwise. Results of the cases where the probability of the presence of distractor cue at every time step was 0 (<bold>B</bold>), 0.1 (<bold>C</bold>), 0.2 (<bold>D</bold>), and 0.3 (<bold>E</bold>). <italic>Left panels</italic>: Examples of the presence of distractor (‘D’), reward-associated cue (‘C’), and reward (‘R’) over 100 time steps. <italic>Middle panels</italic>: Mean squared value-error at the 1500th trial in oVRNNbp-rev (red line), oVRNNrf-bio (blue line), and the models with naive or shuffled untrained recurrent neural network (RNN) (gray solid or dotted line). <italic>Right panels</italic>: Results for the models with E- and I-units (oVRNNbp-rev-ei: red line, oVRNNrf-bio-ei: blue line, models with naive or shuffled untrained RNN: gray solid or dotted line), which were modified to incorporate possible existence of distractor cue in the same manner as in (<bold>A</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig10-v1.tif"/></fig><p>We examined how oVRNNbp-rev, oVRNNrf-bio, and the models with untrained RNN behaved in the modified Pavlovian task with a distractor cue, which was presented with probability 0, 0.1, 0.2, or 0.3 at every time step (<xref ref-type="fig" rid="fig10">Figure 10B–E</xref>, left panels). As a result, even when there was such a distractor cue, oVRNNbp-rev and oVRNNrf-bio could still learn the state values better than the models with naive or shuffled untrained RNN (p &lt; 1.7 × 10<sup>−10</sup> in Wilcoxon rank sum test for oVRNNbp-rev or VRNNrf-bio vs naive or shuffled untrained for each number of RNN units for each level of distractor probability) (<xref ref-type="fig" rid="fig10">Figure 10C–E</xref>, middle panels), although the accuracy moderately decreased compared with the case without distractor cue (<xref ref-type="fig" rid="fig10">Figure 10B</xref>, middle panel). These results suggest robustness of the learning ability of oVRNNrf-bio against distractor in realistic situations. We further examined how the models with E- and I-units behaved in the task with a distractor cue and confirmed that even in the presence of a distractor cue, oVRNNbp-rev-ei and oVRNNrf-bio-ei could learn the state values better than the models with naive or shuffled untrained RNN (p &lt; 3.2 × 10<sup>−8</sup> in Wilcoxon rank sum test for oVRNNbp-rev-ei or oVRNNrf-bio-ei vs naive or shuffled untrained for each number of RNN units for each level of distractor probability) (<xref ref-type="fig" rid="fig10">Figure 10B–E</xref>, right panels).</p></sec><sec id="s2-4-2"><title>Incorporation of action selection</title><p>Ultimate purpose of animals and RL agents is to optimize their policy, that is, probability of action selection at each state to maximize rewards. Therefore, we examined if our models could be extended to incorporate action selection. In reference to the proposals that algorithms akin to the actor-critic method may be implemented in the brain (<xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>; <xref ref-type="bibr" rid="bib127">Sutton and Barto, 1998</xref>; <xref ref-type="bibr" rid="bib129">Takahashi et al., 2008</xref>), we considered extended models oVRNNbp-rev-ac and oVRNNrf-bio-ac, which incorporated an actor-critic architecture (<xref ref-type="fig" rid="fig11">Figure 11A</xref>). Specifically, each RNN unit was assumed to connect to not only the state-value (<italic>v</italic>)-representing unit in the ventral striatum but also the action-value (<italic>q<sub>k</sub></italic>)-representing units in the dorsal striatum. Their non-negative weights (<italic>u<sub>kj</sub></italic>) represented (as a vector) action preferences, which slightly decayed with time so as not to unboundedly increase.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Incorporation of action selection.</title><p>(<bold>A</bold>) Schematic illustration of the models incorporating an actor-critic architecture. (<bold>B</bold>) Two-alternative choice task. (<bold>a</bold>) Task diagram. (<bold>b, c</bold>) Proportion of Action 1 selection in 2901–3000th trials in oVRNNbp-rev-as (red line), oVRNNrf-bio-as (blue line), and the models with naive untrained recurrent neural network (RNN) (i.e., randomly initialized RNN) (gray solid line) or untrained RNN with connections shuffled from those learnt in VRNNrf-bio-as (gray dotted line). Error bars indicate the mean ± SEM over 100 simulations. The inverse temperature was set to 1 (<bold>b</bold>) or 2 (<bold>c</bold>). (<bold>C</bold>) Inter-temporal choice task. The notations are the same as those in (<bold>B</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-fig11-v1.tif"/></fig><p>It has been suggested that action is selected through competition of neural populations in the striatum–thalamus–cortex(-striatum) circuit, which represent or receive action values, in the presence of noise (<xref ref-type="bibr" rid="bib137">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib72">Lo and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib53">Hunt et al., 2012</xref>). However, because our models did not describe fast neural dynamics (see the description about the time step in the Methods), we assumed a soft-max function for action selection, i.e., assumed that an action was selected according to a soft-max probability determined by the difference in the action values when there were two action candidates. We then assumed that there is a cortical region, which contains action-representing neural populations, implemented as ‘action units’ in the model (<xref ref-type="fig" rid="fig11">Figure 11A</xref>). Action unit was assumed to become active (i.e., = 1) when the corresponding action was selected and inactive (i.e., = 0) otherwise, and these action units were assumed to send inputs to the RNN, similarly to the observation units informing the presence of cue and reward.</p><p>We examined how these models (oVRNNbp-rev-ac and oVRNNrf-bio-ac) and control models with naive or shuffled untrained RNNs behaved in two-alternative choice tasks. In the first choice task (<xref ref-type="fig" rid="fig11">Figure 11Ba</xref>), taking action 1 led to a large (size 2) reward two time steps later whereas taking action 2 led to a small (size 1) reward two time steps later. oVRNNbp-rev-ac and oVRNNrf-bio-ac, after training, successfully selected the better action, action 1, in most cases, whereas the models with untrained RNN did not develop strong tendency to select action 1 (<xref ref-type="fig" rid="fig11">Figure 11Bb, c</xref>). Next, in the second choice task (<xref ref-type="fig" rid="fig11">Figure 11Ca</xref>), taking action 1 led to a large (size 2) reward two time steps later whereas taking action 2 led to a small (size 1) reward one time step later. So this task imposed inter-temporal choice between delayed large reward and sooner small reward. oVRNNbp-rev-ac, after training, tended to select action 1, which was better than action 2 even when presumed temporal discounting (0.8 per time step) was taken into account (<xref ref-type="fig" rid="fig11">Figure 11Cb, c</xref>). oVRNNrf-bio-ac also tended to select action 1 when the number of RNN units was not small. On the other hand, the models with untrained RNNs tended to select small rewards sooner. These results suggest that oVRNNrf-bio-ac, as well as oVRNNbp-rev-ac, could learn to select advantageous action to a certain extent, even in the case of inter-temporal choice.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have shown that state representation and value can be learned online in the RNN and its readout by using random feedback instead of biologically unavailable downstream weights. This was achieved through FA, and we have presented an intuitive understanding of its mechanism. We have further shown that the non-negative constraint realizes loose alignment of the forward weights and feedback from the beginning, which appeared to support learning.</p><sec id="s3-1"><title>Roles of DA</title><p>Midbrain DA neurons project to both striatum and cortex, including the prefrontal cortex (<xref ref-type="bibr" rid="bib142">Williams and Goldman-Rakic, 1998</xref>) and the hippocampus (<xref ref-type="bibr" rid="bib13">Broussard et al., 2016</xref>). As for striatal DA, DA-dependent cortico-striatal plasticity is considered to implement TD-RPE-based value update (<xref ref-type="bibr" rid="bib105">Reynolds et al., 2001</xref>; <xref ref-type="bibr" rid="bib111">Samejima et al., 2005</xref>). By contrast, while cortical DA has been implicated in working memory (<xref ref-type="bibr" rid="bib14">Brozoski et al., 1979</xref>; <xref ref-type="bibr" rid="bib113">Sawaguchi and Goldman-Rakic, 1991</xref>; <xref ref-type="bibr" rid="bib30">Durstewitz et al., 2000</xref>; <xref ref-type="bibr" rid="bib15">Brunel and Wang, 2001</xref>), decision making (<xref ref-type="bibr" rid="bib36">Floresco and Magyar, 2006</xref>), and aversive memory (<xref ref-type="bibr" rid="bib134">Tsetsenis et al., 2021</xref>), its role as TD-RPE remains unclear, despite findings suggesting that cortical DA does encode (TD-)RPE (<xref ref-type="bibr" rid="bib96">O’Doherty et al., 2003</xref>; <xref ref-type="bibr" rid="bib130">Takahashi et al., 2011</xref>; <xref ref-type="bibr" rid="bib125">Starkweather et al., 2018</xref>; <xref ref-type="bibr" rid="bib131">Takahashi et al., 2023</xref>) and modulate plasticity (<xref ref-type="bibr" rid="bib98">Otani et al., 2003</xref>; <xref ref-type="bibr" rid="bib114">Sayegh et al., 2024</xref>). Learnability of our biologically constrained online value-RNN suggests that TD-RPE-encoding cortical DA modulates plasticity of RNN so that appropriate state representation can be learned.</p><p>Many studies reported heterogeneities of DA signals, which may come from encoding prediction errors other than RPE or feature-specific components of RPE (<xref ref-type="bibr" rid="bib66">Lee et al., 2024b</xref>). Referring to a result (<xref ref-type="bibr" rid="bib3">Avvisati et al., 2024</xref>) indicating DA’s encoding of non-reward PEs and the fact that DA neurons receive inputs from the cerebellum (<xref ref-type="bibr" rid="bib140">Watabe-Uchida et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Carta et al., 2019</xref>), which presumably implements supervised learning (<xref ref-type="bibr" rid="bib77">Marr, 1969</xref>), a recent work (<xref ref-type="bibr" rid="bib139">Wärnberg and Kumar, 2023</xref>) proposed that DA encodes vector-valued errors used for supervised learning of actions in continuous space. In contrast, we assumed DA’s encoding of scalar TD-RPE, which can be consistent with the heterogeneity due to encoding of feature-specific RPE components (<xref ref-type="bibr" rid="bib66">Lee et al., 2024b</xref>). The previous model (<xref ref-type="bibr" rid="bib139">Wärnberg and Kumar, 2023</xref>) and our model can coexist, with different DA neuronal populations encoding different types of errors, or a single DA neuron switching its encoding depending on context/inputs.</p><p>VTA DA neurons also project to the basolateral amygdala (BLA) (<xref ref-type="bibr" rid="bib6">Beier et al., 2015</xref>), and DA also regulates plasticity there (<xref ref-type="bibr" rid="bib68">Li and Rainnie, 2014</xref>). Moreover, VTA → BLA DA entailed properties of TD-RPE, although increased also upon aversive event and was not itself reinforcing but crucial for the formation of environmental model (<xref ref-type="bibr" rid="bib120">Sias et al., 2024</xref>). BLA has recurrent connections (<xref ref-type="bibr" rid="bib47">Headley et al., 2021</xref>), projects to the striatum (<xref ref-type="bibr" rid="bib12">Britt et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Lee et al., 2024a</xref>), and engages in abstract context representation (<xref ref-type="bibr" rid="bib110">Saez et al., 2015</xref>). Thus, given that goal-directed-like behavior could be achieved through sophisticated state representation (<xref ref-type="bibr" rid="bib108">Russek et al., 2017</xref>; <xref ref-type="bibr" rid="bib122">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>), it could potentially be learned by value-RNN-like mechanism in the BLA. Whether such sophisticated representation can be learned, however, remains open, and it might require multidimensional error (<xref ref-type="bibr" rid="bib123">Stalnaker et al., 2019</xref>) beyond TD-RPE.</p><p>There are many DA-related mechanisms that were not incorporated into our models, including the distinctions of D1-direct and D2-indirect pathways (<xref ref-type="bibr" rid="bib39">Gerfen and Surmeier, 2011</xref>; <xref ref-type="bibr" rid="bib21">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib82">Mikhael and Bogacz, 2016</xref>; <xref ref-type="bibr" rid="bib90">Morita and Kawaguchi, 2018</xref>; <xref ref-type="bibr" rid="bib73">Lowet et al., 2025</xref>) and cortical projections to them (<xref ref-type="bibr" rid="bib136">Wall et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Morita, 2014</xref>; <xref ref-type="bibr" rid="bib52">Hooks et al., 2018</xref>; <xref ref-type="bibr" rid="bib91">Morita et al., 2019</xref>), as well as mechanisms underlying TD-RPE encoding (cf., <xref ref-type="bibr" rid="bib90">Morita and Kawaguchi, 2018</xref>; <xref ref-type="bibr" rid="bib133">Tian et al., 2016</xref>) or learning for it (cf., <xref ref-type="bibr" rid="bib23">Cone et al., 2024</xref>). Future studies are expected to incorporate these.</p></sec><sec id="s3-2"><title>Predictions and implications of our models</title><p>oVRNNrf predicts that the feedback vector <italic>c</italic> and the value-weight vector <italic>w</italic> become gradually aligned, while oVRNNrf-bio predicts that <italic>c</italic> and <italic>w</italic> are loosely aligned from the beginning. Element of <italic>c</italic> could be measured as the magnitude of pyramidal cell’s response to DA stimulation. The element of <italic>w</italic> corresponding to a given pyramidal cell could be measured, if striatal neuron that receives input from that pyramidal cell can be identified (although technically demanding), as the magnitude of response of the striatal neuron to activation of the pyramidal cell. Then, the abovementioned predictions could be tested by (1) identifying cortical, striatal, and VTA regions that are connected, (2) identifying pairs of cortical pyramidal cells and striatal neurons that are connected, (3) measuring the responses of identified pyramidal cells to DA stimulation, as well as the responses of identified striatal neurons to activation of the connected pyramidal cells, and (4) testing whether DA → pyramidal responses and pyramidal → striatal responses are associated across pyramidal cells, and whether such associations develop through learning.</p><p>Testing this prediction, however, would be technically quite demanding, as mentioned above. An alternative way of testing our model is to manipulate the cortical DA feedback and see if it will cause (re-)alignment of value weights (i.e., cortical striatal strengths). Specifically, our model predicts that if DA projection to a particular cortical locus is silenced, the effect of the activity of that locus on the value-encoding striatal activity will become diminished.</p><p>We have shown that oVRNNrf and oVRNNrf-bio could work even when the random feedback was uniform, that is, fixed to the direction of (1, 1,..., 1)<sup>T</sup>, although the performance was somewhat worse. This is reasonable because uniform feedback can still encode scalar TD-RPE that drives our models, in contrast to a previous study (<xref ref-type="bibr" rid="bib139">Wärnberg and Kumar, 2023</xref>), which considered DA’s encoding of vector error and thus regarded uniform feedback as a negative control. If oVRNNrf/oVRNNrf-bio-like mechanism indeed operates in the brain and the feedback is near uniform, alignment of the value weights <italic>w</italic> to near (1, 1,..., 1) is expected to occur. This means that states are (learned to be) represented in such a way that simple summation of cortical neuronal activity approximates value, thereby potentially explaining why value is often correlated with regional activation (fMRI BOLD signal) of cortical regions (<xref ref-type="bibr" rid="bib67">Levy and Glimcher, 2012</xref>). Notably, uniform feedback coupled with positive forward weights was shown to be effective also in supervised learning of one-dimensional output in feed-forward networks (<xref ref-type="bibr" rid="bib61">Konishi et al., 2023</xref>), and we guess that loose alignment may underlie it.</p></sec><sec id="s3-3"><title>On the RNN unit</title><p>In our oVRNNbp without non-negative constraint, as the number of RNN units increased, the squared error initially decreased but then increased (<xref ref-type="fig" rid="fig2">Figure 2J</xref>) (while intriguingly it was not the case for the models with non-negative constraint (<xref ref-type="fig" rid="fig6">Figure 6E</xref>)). In contrast, in the original value-RNN (<xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>; <xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>), the ability to develop belief-state-like representation was reported to improve as the number of RNN units increased to 100 or 50. There are at least two possible reasons for this difference, other than the difference in the performance measures. The first one is the difference in the update rules. As mentioned earlier, the original value-RNN used BPTT (<xref ref-type="bibr" rid="bib107">Rumelhart et al., 1986b</xref>) whereas our oVRNNbp used an online learning rule, which only considered the influence of the recurrent weights at the previous time step.</p><p>The second one is a difference in the RNN unit. Specifically, the original value-RNN used the Gated Recurrent Unit (GRU) cell (<xref ref-type="bibr" rid="bib19">Cho et al., 2014</xref>) whereas we used a simple sigmoidal function. RNN with simple nonlinear unit is known to have the vanishing gradient problem (<xref ref-type="bibr" rid="bib50">Hochreiter, 1998</xref>), which could be alleviated by using memorable/gated RNN such as the Long Short-Term Memory (LSTM) unit (<xref ref-type="bibr" rid="bib49">Hochreiter and Schmidhuber, 1997</xref>) or the GRU cell (<xref ref-type="bibr" rid="bib19">Cho et al., 2014</xref>). We used the simple sigmoidal unit because the biological plausibility of the GRU cell appeared elusive. However, a gated unit similar to the LSTM unit has actually been proposed to be implemented in cortical microcircuits (<xref ref-type="bibr" rid="bib25">Costa et al., 2017</xref>), and incorporation of such biologically plausible gated unit into our online value-RNN would be a hopeful direction.</p><p>From a bottom-up viewpoint, our RNN unit did not incorporate spiking (<xref ref-type="bibr" rid="bib100">Payeur et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Gjorgjieva et al., 2011</xref>; <xref ref-type="bibr" rid="bib118">Shouval et al., 2010</xref>) nor nonlinear dendritic computations (<xref ref-type="bibr" rid="bib99">Pagkalos et al., 2024</xref>; <xref ref-type="bibr" rid="bib101">Poirazi et al., 2003</xref>; <xref ref-type="bibr" rid="bib86">Morita, 2008</xref>). Recent studies suggest that dendritic mechanisms (<xref ref-type="bibr" rid="bib45">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="bib109">Sacramento et al., 2018</xref>), possibly in combination with burst-dependent plasticity (<xref ref-type="bibr" rid="bib100">Payeur et al., 2021</xref>; <xref ref-type="bibr" rid="bib43">Greedy et al., 2022</xref>), can realize credit assignment without backprop in supervised and unsupervised learning (<xref ref-type="bibr" rid="bib62">Körding and König, 2001</xref>; <xref ref-type="bibr" rid="bib54">Illing et al., 2021</xref>). Also, a recent model of hippocampus <xref ref-type="bibr" rid="bib22">Cone and Clopath, 2024</xref> has shown that a network of multi-compartment units could learn complex representations. Having dendritic mechanisms is different from just increasing the number of neural-network layers because of their own specific features/constraints, and it was argued (<xref ref-type="bibr" rid="bib99">Pagkalos et al., 2024</xref>) that adding such biological constraints enables learning in deep neural networks. Therefore, incorporation of biological details into RNN unit in our models would be hopeful also from the bottom-up viewpoint.</p></sec><sec id="s3-4"><title>Comparison to other algorithms</title><p>As an alternative to backprop in hierarchical network, aside from FA (<xref ref-type="bibr" rid="bib70">Lillicrap et al., 2016</xref>), Associative Reward–Penalty (A<sub>R–P</sub>) algorithm has been proposed (<xref ref-type="bibr" rid="bib5">Barto and Jordan, 1987</xref>; <xref ref-type="bibr" rid="bib80">Mazzoni et al., 1991a</xref>; <xref ref-type="bibr" rid="bib81">Mazzoni et al., 1991b</xref>). In A<sub>R–P</sub>, the hidden units behave stochastically, allowing the gradient to be estimated via stochastic sampling. Recent work by <xref ref-type="bibr" rid="bib79">Max et al., 2024</xref> has proposed Phaseless Alignment Learning, in which high-frequency noise-induced learning of feedback projections proceeds simultaneously with learning of forward projections using the feedback in a lower frequency. Noise-induced learning of the weights on readout neurons from untrained RNN by reward-modulated Hebbian plasticity has also been demonstrated (<xref ref-type="bibr" rid="bib51">Hoerzer et al., 2014</xref>). Such noise- or perturbation-based (<xref ref-type="bibr" rid="bib71">Lillicrap et al., 2020</xref>) mechanisms are biologically plausible because neurons and neural networks can exhibit noisy or chaotic behavior (<xref ref-type="bibr" rid="bib31">Faisal et al., 2008</xref>; <xref ref-type="bibr" rid="bib1">Aihara and Matsumoto, 1986</xref>; <xref ref-type="bibr" rid="bib135">van Vreeswijk and Sompolinsky, 1996</xref>), and might improve the performance of value-RNN if implemented.</p><p>Regarding the learning of RNN, ‘e-prop’ (<xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>) was proposed as a locally learnable online approximation of BPTT (<xref ref-type="bibr" rid="bib107">Rumelhart et al., 1986b</xref>), which was used in the original value-RNN (<xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>). In e-prop, neuron-specific learning signal is combined with weight-specific locally updatable ‘eligibility trace’. Reward-based e-prop was also shown to work (<xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>), both in a setup not introducing TD-RPE with symmetric or random feedback (their Figure S5) and in another setup introducing TD-RPE with symmetric feedback (their Figures 4 and 5). Compared to these, our models differ in multiple ways.</p><p>First, we have shown that alignment to random feedback occurs in the models driven by TD-RPE. Second, our models do not have ‘eligibility trace’ (nor memorable/gated unit, different from the original value-RNN <xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>; see Appendix 1.1 for possible incorporation of eligibility trace into our models), but could still solve temporal credit assignment to a certain extent because TD learning is by itself a solution for it (notably, recent work showed that combination of TD(0) and model-based RL well explained rat’s choice and DA patterns; <xref ref-type="bibr" rid="bib63">Krausz et al., 2023</xref>). However, as mentioned before, a single time step in our models was assumed to correspond to hundreds of milliseconds, incorporating slow synaptic dynamics, whereas e-prop is an algorithm for spiking neuron models with a much finer time scale. From this aspect, our models could be seen as a coarse-time-scale approximation of e-prop. On top of these, our results point to a potential computational benefit of biological non-negative constraint, which could effectively limit the parameter space and promote learning.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Online value-RNN with backprop (oVRNNbp)</title><p>We constructed an online value-RNN model based on the previous proposals (<xref ref-type="bibr" rid="bib102">Qian et al., 2025</xref>; <xref ref-type="bibr" rid="bib48">Hennig et al., 2023</xref>) but with several differences. We assumed that the activities of neurons in the RNN at time <italic>t</italic> + 1 were determined by the activities of these neurons and neurons representing observation (cue, reward, or nothing) at time <italic>t</italic>:<disp-formula id="equ1"><label>(1.1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle \boldsymbol x\left (t+1\right)=f\left (\boldsymbol {Ax}\left (t\right)+\boldsymbol {Bo}\left (t\right)\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where</p><p><italic> </italic><inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$\boldsymbol x=\left (x_{j}\right)$\end{document}</tex-math></alternatives></inline-formula>: activity of <italic>j</italic>th neuron in the RNN (<inline-formula><alternatives><mml:math id="inf26"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:math><tex-math id="inft26">\begin{document}$j=1,..,n$\end{document}</tex-math></alternatives></inline-formula>)</p><p><italic> </italic><inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$\boldsymbol o=\left (o_{k}\right)$\end{document}</tex-math></alternatives></inline-formula>: activity of <italic>k</italic>th neuron in the observation layer (<italic>k</italic> = 1, 2)</p><p>  if there was a cue at <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$t,\boldsymbol o\left (t\right)=\left (1\,0\right)^{T}$\end{document}</tex-math></alternatives></inline-formula>,</p><p>  if there was a reward at <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$t,\boldsymbol o\left (t\right)=\left (0\,1\right)^{T}$\end{document}</tex-math></alternatives></inline-formula>,</p><p>  and otherwise, <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$\boldsymbol o\left (t\right)=\left (0\,0\right)^{T}$\end{document}</tex-math></alternatives></inline-formula></p><p><bold> </bold><inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$\boldsymbol A=\left (A_{ij}\right)$\end{document}</tex-math></alternatives></inline-formula>: recurrent connection strength from <inline-formula><alternatives><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft32">\begin{document}$x_{j}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft33">\begin{document}$x_{i}$\end{document}</tex-math></alternatives></inline-formula></p><p><bold> </bold><inline-formula><alternatives><mml:math id="inf34"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><tex-math id="inft34">\begin{document}$B=\left (B_{ik}\right)$\end{document}</tex-math></alternatives></inline-formula>: feed-forward connection strength from <inline-formula><alternatives><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft35">\begin{document}$o_{k}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft36">\begin{document}$x_{i}$\end{document}</tex-math></alternatives></inline-formula><disp-formula id="equ2"><label>(1.2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo>:</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle f\left (z\right)={1}/{(1+exp\left (- z\right)})- 0.5:$$\end{document}</tex-math></alternatives></disp-formula></p><p>  sigmoidal function representing neuronal input–output relation.</p><p>The estimated value of the state at <italic>t</italic> was calculated as<disp-formula id="equ3"><label>(1.3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle v(t)=\boldsymbol w^{\rm T} \boldsymbol x(t)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ4"><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle \boldsymbol w=(w_{j})$$\end{document}</tex-math></alternatives></disp-formula></p><p>were the value weights. The error between this estimated value and the true value, <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$v_{true}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, was defined as<disp-formula id="equ5"><label>(1.4)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle \varepsilon (t)=v_{true}(t)- v(t)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ6"><label>(1.5)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mn>2</mml:mn><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  &amp;{-\partial \left (\varepsilon \left (t\right)^{2}\right)}/{\partial _{wj}}\\&amp;={2\varepsilon \left (t\right)\partial \varepsilon \left (t\right)}/{\partial w_{j}}\\ &amp;=- {2\varepsilon \left (t\right)\partial \left (v_{true}\left (t\right)- w^{T}x\left (t\right)\right)}/{\partial _{wj}}\\ &amp;=- 2\varepsilon \left (t\right)\left (- x_{j}\left (t\right)\right)\\ &amp;=2\varepsilon \left (t\right)x_{j}\left (t\right)\\ &amp;\approx 2\delta \left (t\right)x_{j}\left (t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>In the last line, since <inline-formula><alternatives><mml:math id="inf38"><mml:mi>ε</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft38">\begin{document}$\varepsilon \left (t\right)$\end{document}</tex-math></alternatives></inline-formula> was unavailable as <inline-formula><alternatives><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft39">\begin{document}$v_{true}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> was unknown, it was approximated by the TD-RPE:<disp-formula id="equ7"><label>(1.6)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle \delta \left (t\right)=r\left (t\right)+\gamma v\left (t+1\right)- v\left (t\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p><inline-formula><alternatives><mml:math id="inf40"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}${- \partial \left (\varepsilon \left (t\right)^{2}\right)}/{\partial A_{ij}}$\end{document}</tex-math></alternatives></inline-formula> was calculated as follows:<disp-formula id="equ8"><label>(1.7)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  &amp;{- \partial \left (\varepsilon \left (t\right)^{2}\right)}/{\partial A_{ij}}\\ &amp;=-{2\varepsilon \left (t\right)\partial \left (v_{true}\left (t\right)- w^{T}x\left (t\right)\right)}/{\partial A_{ij}}\\ &amp;\approx {2\delta \left (t\right)\partial \left (w^{T}f\left (Ax\left (t- 1\right)+Bo\left (t- 1\right)\right)\right)}/{\partial A_{ij}}\\ &amp;=2\delta \left (t\right)x_{i}\left (t- 1\right)\left (0.5+x_{i}\left (t\right)\right)\left (0.5- xi\left (t\right)\right)w_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Similarly, <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$- \partial (\varepsilon (t)^{2})/\partial B_{ik}$\end{document}</tex-math></alternatives></inline-formula> was calculated as follows:<disp-formula id="equ9"><label>(1.8)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.9em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mn>2</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>o</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  &amp;-{\partial \left( \varepsilon(t)^{2} \right)}/{\partial B_{ik}} \\[6pt] &amp; \approx 2 \, \delta(t) \, o_k(t-1) \, \left(0.5 + x_i(t)\right) \, \left(0.5 - x_i(t)\right) \, w_i $$\end{document}</tex-math></alternatives></disp-formula></p><p>According to these, the online update rule for the value-RNN was determined as follows:<disp-formula id="equ10"><label>(1.9)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle w_{j}\leftarrow w_{j}+a_{\rm value}\delta \left (t\right)x_{j}\left (t\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ11"><label>(1.10)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle A_{ij}\leftarrow A_{ij}+a_{\rm RNN}\delta \left (t\right)x_{j}\left (t- 1\right)\left (0.5+x_{i}\left (t\right)\right)\left (0.5- x_{i}\left (t\right)\right)w_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ12"><label>(1.11)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle B_{ik}\leftarrow B_{ik}+a_{\rm RNN}\delta \left (t\right)o_{k}\left (t- 1\right)\left (0.5+x_{i}\left (t\right)\right)\left (0.5- x_{i}\left (t\right)\right)w_{i},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>a</italic><sub>value</sub> and <italic>a</italic><sub>RNN</sub> were the learning rates. In each simulation, the elements of <bold>A</bold> and <bold>B</bold>, as well as the elements of <italic>x</italic>, were initialized to pseudo standard normal random numbers, and the elements of <italic>w</italic> were initialized to 0.</p></sec><sec id="s4-2"><title>Online value-RNN with fixed random feedback (oVRNNrf)</title><p>We considered an implementation of the online value-RNN described above in the cortico-basal ganglia-DA system (<xref ref-type="fig" rid="fig1">Figure 1</xref>):</p><list list-type="simple" id="list1"><list-item><p> <italic>x</italic>: activities of neurons in a cortical region with rich recurrent connections</p></list-item><list-item><p> <bold>A</bold>: recurrent connection strengths among <italic>x</italic></p></list-item><list-item><p> <italic>o</italic>: activities of neurons in a cortical region processing sensory inputs</p></list-item><list-item><p> <bold>B</bold>: feed-forward connection strengths from <italic>o</italic> to <italic>x</italic></p></list-item><list-item><p> <italic>f</italic>: sigmoidal relationship from the input to the output of the cortical neurons</p></list-item><list-item><p> <italic>w</italic>: connection strengths from cortical neurons <italic>x</italic> to a group of striatal neurons</p></list-item><list-item><p> <italic>v</italic>: activity of the group of striatal neurons</p></list-item><list-item><p> <italic>δ</italic>: activity of a group of DA neurons/released DA</p></list-item></list><p>The update rule for <italic>w</italic> (<xref ref-type="disp-formula" rid="equ10">Equation 1.9</xref>):<disp-formula id="equ13"><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle w_{j}\leftarrow w_{j}+a_{\rm value}\delta \left (t\right)x_{j}\left (t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>could be naturally implemented as cortico-striatal synaptic plasticity, which depends on DA (<italic>δ</italic>(<italic>t</italic>)) and pre-synaptic (cortical) neuronal activity (<italic>x<sub>j</sub></italic>(<italic>t</italic>)). However, an issue emerged in the implementation of the update rules for <bold>A</bold> and <bold>B</bold> (<xref ref-type="disp-formula" rid="equ11 equ12">Equations 1.10 and 1.11</xref>):<disp-formula id="equ14"><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle A_{ij}\leftarrow A_{ij}+a_{\rm RNN}\delta \left (t\right)x_{j}\left (t- 1\right)\left (0.5+x_{i}\left (t\right)\right)\left (0.5- x_{i}\left (t\right)\right)w_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ15"><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle B_{ik}\leftarrow B_{ik}+a_{\rm RNN}\delta \left (t\right)o_{k}\left (t- 1\right)\left (0.5+x_{i}\left (t\right)\right)\left (0.5- x_{i}\left (t\right)\right)w_{i},$$\end{document}</tex-math></alternatives></disp-formula></p><p>Specifically, <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$w_{i}$\end{document}</tex-math></alternatives></inline-formula> included in the rightmost of these update rules (for the strengths of cortico-cortical synapses <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$A_{ij}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$B_{ik}$\end{document}</tex-math></alternatives></inline-formula>) is the connection strength from cortical neuron <inline-formula><alternatives><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft45">\begin{document}$x_{i}$\end{document}</tex-math></alternatives></inline-formula> to striatal neurons, that is, the strength of the cortico-striatal synapses (located within the striatum), which is considered to be unavailable at the cortico-cortical synapses (located within the cortex).</p><p>As mentioned in the Introduction, this is an example of the long-standing difficulty in biological implementation of backprop, and recently a potential solution for this difficulty, that is, replacement of the downstream connection strengths in the update rule for upstream connections with fixed random strengths, has been demonstrated in supervised learning of feed-forward and recurrent networks (<xref ref-type="bibr" rid="bib93">Murray, 2019</xref>; <xref ref-type="bibr" rid="bib70">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib139">Wärnberg and Kumar, 2023</xref>). The online value-RNN, which we considered here, differed from supervised learning considered in these previous studies in two ways: (1) it was TD learning, apparent in the approximation of the true error <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$\varepsilon \left (t\right)$\end{document}</tex-math></alternatives></inline-formula> by the TD-RPE <inline-formula><alternatives><mml:math id="inf47"><mml:mi>δ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft47">\begin{document}$\delta \left (t\right)$\end{document}</tex-math></alternatives></inline-formula> in the derivation described above, and (2) it used a scalar error (TD-RPE) rather than a vector error. But we expected that the FA mechanism could still work at least to some extent and explored it in this study. Specifically, we examined a modified online value-RNN with fixed random feedback (oVRNNrf), in which the update rules for <bold>A</bold> and <bold>B</bold> were modified as follows:<disp-formula id="equ16"><label>(2.1)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle  A_{ij} \leftarrow A_{ij} + a_{\mathrm{RNN}} \, \delta(t) \, x_{j}(t-1) \, \left(0.5 + x_{i}(t)\right) \, \left(0.5 - x_{i}(t)\right) \, c_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ17"><label>(2.2)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle B_{ik}\leftarrow B_{ik}+a_{\rm RNN}\delta \left (t\right)o_{k}\left (t- 1\right)\left (0.5+x_{i}\left (t\right)\right)\left (0.5- x_{i}\left (t\right)\right)c_{i},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft48">\begin{document}$w_{i}$\end{document}</tex-math></alternatives></inline-formula> in the update rules of the online value-RNN with backprop (oVRNNbp) was replaced with a fixed random parameter <inline-formula><alternatives><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft49">\begin{document}$c_{i}$\end{document}</tex-math></alternatives></inline-formula>. Notably, these modified update rules for the cortico-cortical connections <bold>A</bold> and <bold>B</bold> required only pre-synaptic activities (<inline-formula><alternatives><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math><tex-math id="inft50">\begin{document}$x_{j}\left (t- 1\right)$\end{document}</tex-math></alternatives></inline-formula>,<inline-formula><alternatives><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math><tex-math id="inft51">\begin{document}$o_{k}\left (t- 1\right)$\end{document}</tex-math></alternatives></inline-formula>), post-synaptic activities <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$(x_{i}\left (t\right))$\end{document}</tex-math></alternatives></inline-formula>, TD-RPE-representing DA <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$(\delta \left (t\right))$\end{document}</tex-math></alternatives></inline-formula>, and fixed random strengths (<inline-formula><alternatives><mml:math id="inf54"><mml:mstyle><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$c_{i}$\end{document}</tex-math></alternatives></inline-formula>), which would all be available at the cortico-cortical synapses given that VTA DA neurons project not only to the striatum but also to the cortex and random <inline-formula><alternatives><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft55">\begin{document}$c_{i}$\end{document}</tex-math></alternatives></inline-formula> could be provided by intrinsic heterogeneity. In each simulation, the elements of <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> were initialized to pseudo standard normal random numbers.</p></sec><sec id="s4-3"><title>Revised online value-RNN models with further biological constraints</title><p>In the later part of this study, we examined revised online value-RNN models with further biological constraints. Specifically, we considered models in which the value weights and the activities of neurons in the RNN were constrained to be non-negative. In order to do so, the update rule for <italic>w</italic> was modified to:<disp-formula id="equ18"><label>(3.1)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle w_{j}\leftarrow \max\left (0,w_{j}+a_{\rm value}\delta \left (t\right)x_{j}\left (t\right)\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where max(<italic>q</italic><sub>1</sub>, <italic>q</italic><sub>2</sub>) returned the maximum of <italic>q</italic><sub>1</sub> and <italic>q</italic><sub>2</sub>. Also, the sigmoidal input–output function was replaced with<disp-formula id="equ19"><label>(3.2)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle f\left (z\right)={1}/{(1+ \exp\left (- z\right))},$$\end{document}</tex-math></alternatives></disp-formula></p><p>and the elements of <italic>x</italic> were initialized to pseudo uniform [0 1] random numbers. The backprop-based update rules for <bold>A</bold> and <bold>B</bold> in oVRNNbp were replaced with<disp-formula id="equ20"><label>(3.3)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle A_{ij} \leftarrow A_{ij} + a_{\mathrm{RNN}} \, \delta(t) \, x_{j}(t-1) \, x_{i}(t) \, \bigl(1 - x_{i}(t)\bigr) \, w_{i}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ21"><label>(3.4)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle  B_{ik} \leftarrow B_{ik} + a_{\mathrm{RNN}} \, \delta(t) \, o_{k}(t-1) \, x_{i}(t) \, \bigl(1 - x_{i}(t)\bigr) \, w_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>We referred to the model with these modifications to oVRNNbp as oVRNNbp-rev.</p><p>As a revised online value-RNN with fixed random feedback (oVRNNrf), in addition to the abovementioned modifications of the update of <italic>w</italic>, the sigmoidal input-output function, and the initialization of <italic>x</italic>, the fixed random feedback <italic>c</italic> was assumed to be non-negative. Specifically, the elements of <italic>c</italic> were set to pseudo uniform [0 1] random numbers. Moreover, the update rules for <bold>A</bold> and <bold>B</bold> were replaced with</p><p> (when <inline-formula><alternatives><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:math><tex-math id="inft57">\begin{document}$x_{i}\left (t\right)\leq 0.5$\end{document}</tex-math></alternatives></inline-formula>)<disp-formula id="equ22"><label>(3.5)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle  A_{ij} \leftarrow A_{ij} + a_{\mathrm{RNN}} \, \delta(t) \, x_{j}(t-1) \, x_{i}(t) \, \bigl(1 - x_{i}(t)\bigr) \, c_{i} $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ23"><label>(3.6)</label><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle  B_{ik} \leftarrow B_{ik} + a_{\mathrm{RNN}} \, \delta(t) \, o_{k}(t-1) \, x_{i}(t) \, \bigl(1 - x_{i}(t)\bigr) \, c_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p> (when <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$x_{i}\left (t\right)\gt 0.5$\end{document}</tex-math></alternatives></inline-formula>)<disp-formula id="equ24"><label>(3.7)</label><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle  A_{ij} \leftarrow A_{ij} + 0.25 \, a_{\mathrm{RNN}} \, \delta(t) \, x_{j}(t-1) \, c_{i} $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ25"><label>(3.8)</label><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle B_{ik} \leftarrow B_{ik} + 0.25 \, a_{\mathrm{RNN}} \, \delta(t) \, o_{k}(t-1) \, c_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>so that the originally non-monotonic dependence on <italic>x<sub>i</sub></italic>(<italic>t</italic>) (post-synaptic activity) became monotonic + saturation (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). These update rules with non-negative <italic>c<sub>i</sub></italic> could be said to be Hebbian with additional modulation by TD-RPE (Hebbian under positive TD-RPE) (see Appendix 1.2 for possible consideration of behavioral time-scale synaptic plasticity (BTSP) in our models). We referred to the model with these modifications to oVRNNrf as oVRNNrf-bio. In the right panels of <xref ref-type="fig" rid="fig6">Figure 6E, F</xref>, we also examined the model where the modified update rules of oVRNNrf-bio were changed back to the original ones, referred to as oVRNNrf-rev. In some simulations in <xref ref-type="fig" rid="fig8">Figure 8E–G</xref>, we examined a modified oVRNNrf-bio with a slight decay (forgetting) of value weights, in which each element of <italic>w</italic> decayed at every time step:<disp-formula id="equ26"><label>(3.9)</label><alternatives><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t26">\begin{document}$$\displaystyle  w_{i} \leftarrow (1 - dr) \, w_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>dr</italic> was the decay rate per time step and was set to 0.001 or 0.002.</p><p>We further examined extensions of oVRNNbp-rev and oVRNNrf-bio, referred to as oVRNNbp-rev-ei and oVRNNrf-bio-ei, which incorporated excitatory E-units and inhibitory I-units (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Based on biological suggestions (see the Results), we made the following assumptions. Each E-unit received inputs from the observation units <italic>o</italic> (connections: <bold>B<sub>E</sub></bold>), all the E-units (connections: <bold>A<sub>E</sub></bold>), and a particular I-unit (with a strength <italic>h</italic>), and projected to the striatal value unit (connections: <italic>w</italic>). Each I-unit received inputs from the observation units <italic>o</italic> (connections: <bold>B<sub>I</sub></bold>) and all the E-units (connections: <bold>A<sub>I</sub></bold>). Excitation from the observation units and E-units to E- and I-units took one time step, whereas I → E inhibition operated within a time step. The activation function for E-unit and plasticity rules for connections from/to E-units were the same as those for the RNN unit in the original models. I-unit had a linear activation function, and there was no plasticity for connections from/to I-units. The update rule for <italic>w</italic> was the same as the original one with the activity of the RNN units replaced with the activity of E-units. Equations for the activities of E-units and I-units, <italic>x</italic><bold><sub>E</sub></bold> and <italic>x</italic><bold><sub>I</sub></bold>, are given as follows:<disp-formula id="equ27"><label>(3.10)</label><alternatives><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="t27">\begin{document}$$\displaystyle \boldsymbol x_{\rm I}\left (t+1\right)=\boldsymbol {A_{\rm I}x_{\rm E}}\left (t\right)+\boldsymbol {B_{\rm I}o}\left (t\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ28"><label>(3.11)</label><alternatives><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t28">\begin{document}$$\displaystyle \boldsymbol {x_{\rm E}}\left (t+1\right)=\boldsymbol f\left (\boldsymbol {A_{\rm E}x}\left (t\right)+\boldsymbol {B_{\rm E}o}\left (t\right)- h\boldsymbol {x_{\rm I}}\left (t+1\right)\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>h</italic> was set to 1. The elements of <bold>A<sub>I</sub></bold>, <bold>B<sub>I</sub></bold>, <bold>A<sub>E</sub></bold>, and <bold>B<sub>E</sub></bold> were initialized to be non-negative:<inline-formula><alternatives><mml:math id="inf59"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="italic">z</mml:mi><mml:mo mathvariant="italic" stretchy="false">)</mml:mo><mml:mo mathvariant="italic">,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$\rm {max} (0,3+\it z),$\end{document}</tex-math></alternatives></inline-formula></p><p>where <italic>z</italic> was a pseudo standard normal random number. The elements of <italic>x</italic><bold><sub>E</sub></bold> were initialized to pseudo uniform [0 1] random numbers, and the initial values of <italic>x</italic><bold><sub>I</sub></bold> were determined according to the abovementioned equation.</p></sec><sec id="s4-4"><title>Incorporation of action selection</title><p>We considered extensions of oVRNNbp-rev and oVRNNrf-bio that incorporated an actor-critic architecture, referred to as oVRNNbp-rev-ac and oVRNNrf-bio-ac (<xref ref-type="fig" rid="fig11">Figure 11A</xref>). Each RNN unit additionally connected to additional two units representing the action-values of action 1 and action 2 (<italic>q</italic><sub>1</sub> and <italic>q</italic><sub>2</sub>):<disp-formula id="equ29"><label>(4.1)</label><alternatives><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">U</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t29">\begin{document}$$\displaystyle \boldsymbol q\left (t\right)=\boldsymbol {\rm U}x\left (t\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$U=\left (u_{kj}\right)$\end{document}</tex-math></alternatives></inline-formula> consisted of two row vectors that represented the preferences of the two actions. At the time step next to cue presentation, one action was selected in a soft-max manner based on the action values. Specifically, action <italic>k</italic> was selected with the probability of<disp-formula id="equ30"><label>(4.2)</label><alternatives><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t30">\begin{document}$$\displaystyle {\exp (\beta q_{k})}/{(\exp (\beta q_{1})+\exp (\beta q_{2}))},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>β</italic> was the inverse temperature parameter, set to 1 or 2, representing the degree of exploitation over exploration. The selected action was then informed to the RNN units. Specifically, the observation layer had two additional elements (<italic>o'</italic> in <xref ref-type="fig" rid="fig11">Figure 11A</xref>) corresponding to the two actions. These elements became 1 when the corresponding action was selected and 0 otherwise. The preference of selected action <italic>k</italic> was updated by using the TD-RPE <italic>δ</italic>(<italic>t</italic>) as follows:<disp-formula id="equ31"><label>(4.3)</label><alternatives><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t31">\begin{document}$$\displaystyle u_{kj}\leftarrow u_{kj}+a_{\rm pref}\delta \left (t\right)x_{j}\left (t\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>a</italic><sub>pref</sub> was the learning rate. In order to prevent unbounded increase of action preference, we assumed a slight decay of all the action preferences at every time step:<disp-formula id="equ32"><label>(4.4)</label><alternatives><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t32">\begin{document}$$\displaystyle  u_{kj} \leftarrow (1 - dr) \, u_{kj} $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>dr</italic> was the decay rate per time step and was set to 0.001. <italic>u<sub>kj</sub></italic> (i.e., the elements of <bold>U</bold>) were initialized to 0. The connection weights from the action-observation units <italic>o'</italic>, as well as from <italic>o</italic> and the RNN units, onto the RNN units were initialized to pseudo standard normal random numbers.</p></sec><sec id="s4-5"><title>Simulation of the tasks</title><p>In the Pavlovian cue–reward association task, at time 1 of each trial, cue observation was received by the RNN, and at time 4, reward observation was received. The trial was pseudo-randomly ended at time 7, 8, 9, or 10, and the next trial started from the next time step (i.e., ITI was 4, 5, 6, or 7 time steps with equal probabilities). Reward size was <italic>r</italic>=1. We also conducted simulations with longer cue–reward delays, in which reward was given at time 5, 6, or 7, and the end of trial was shifted accordingly. The tasks with probabilistic structures (tasks 1 and 2) were implemented in the same way except that reward timing was not time 4 but time 3 or 5 with equal probabilities, specifically, 50% and 50% in task 1 and 30% and 30% in task 2, and there was no reward in the remaining 40% of trials in task 2. The tasks with action selection were also implemented in the same way except that size 2 reward was received, that is, the reward term in the TD-RPE calculation as well as the reward-corresponding element of the observation inputs were set to 2, at time 4 when action 1 was selected, whereas size 1 reward was received at time 4 (in the first choice task) or time 3 (in the second choice task) when action 2 was selected.</p><p>The cue or reward state/timing, mentioned in the text and marked in the figures, was defined to be the timing when the RNN received the cue or reward observation, respectively. Specifically, if <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$\boldsymbol o\left (t\right)=\left (1\,0\right)^{T}$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$\boldsymbol o\left (t\right)=\left (0\,1\right)^{T}$\end{document}</tex-math></alternatives></inline-formula> at time <italic>t</italic>, <italic>t</italic> + 1 was defined to be a cue or reward timing, respectively. For the agents with punctate state representation, which is also referred to as the complete serial compound representation (<xref ref-type="bibr" rid="bib84">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib128">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib74">Ludvig et al., 2012</xref>), each timing from a cue in the tasks was represented by a 10-dimensional one-hot vector, starting from (1 0 0... 0)<sup>T</sup> for the cue state, with the next state (0 1 0... 0)<sup>T</sup> and so on.</p><p>In the simulations of the cue–reward association task with distractor cue, the observation units <italic>o</italic> = (<italic>o<sub>k</sub></italic>) had an additional element <italic>o</italic><sub>3</sub> (<xref ref-type="fig" rid="fig10">Figure 10A</xref>), which was 1 at the time steps where distractor cue was present and 0 otherwise. We examined four cases, in which the probability of the presence of distractor cue at every time step throughout the task was 0, 0.1, 0.2, and 0.3 (<xref ref-type="fig" rid="fig10">Figure 10B–E</xref>).</p><p>Learning rates were set as follows. For the models with punctate state representation, <italic>a</italic><sub>value</sub> = 0.1. For oVRNNbp, oVRNNrf, and the model with untrained RNN compared with these two, <italic>a</italic><sub>RNN</sub> = 0.1 and <italic>a</italic><sub>value</sub> = 0.1/(<italic>n</italic>/7). For oVRNNbp-rev, oVRNNrf-bio, and the models with untrained RNN compared with these two, <italic>a</italic><sub>RNN</sub> = 0.1 and <italic>a</italic><sub>value</sub> = <italic>a</italic><sub>pref</sub> = 0.1/(<italic>n</italic>/12) except for the right panels of <xref ref-type="fig" rid="fig6">Figure 6J</xref>, and <italic>a</italic><sub>RNN</sub> = 0.05 and <italic>a</italic><sub>value</sub> = 0.05/(<italic>n</italic>/12) for the right panels of <xref ref-type="fig" rid="fig6">Figure 6J</xref>. Time discount factor (<italic>γ</italic>) was set to 0.8.</p></sec><sec id="s4-6"><title>Estimation of true state/timing values</title><p>As for the Pavlovian cue–reward association task, we defined states after agent’s receival of cue information by relative timings from the cue and estimated their (true) values by simulations according to the definition of state value. We generated a sequence of cues and rewards corresponding to 1000 trials with the ITI after the first trial, ITI<sub>1</sub>, fixed to one of the possible lengths (4, 5, 6, or 7 time steps), and calculated cumulative discounted future rewards within the sequence:<disp-formula id="equ33"><alternatives><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mtext>rew </mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mtext>rew</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t33">\begin{document}$$\displaystyle  \Sigma_{t_{-} \text {rew }}(r \gamma^{t_{-} \text{rew}}),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$t_{rew}$\end{document}</tex-math></alternatives></inline-formula> denotes the time step of each reward counted from the starting state, starting from +1,..., and +3 + ITI<sub>1</sub> time steps from a cue (the last one corresponded to the cue timing of the next trial). For each case where ITI<sub>1</sub> = 4, 5, 6, or 7, we repeated this 1000 times, generating 1000 sequences (i.e., 1000 simulations of 1000 trials), with different sets of pseudo-random numbers, and calculated the average over these 1000 sequences (we refer to these as ITI<sub>1</sub>-specific values). We estimated the value of each state of +1,..., and +7 time steps from cue (i.e., −2,...,+4 time steps from reward) by taking the average of the ITI<sub>1</sub>-specific values for four possible ITI<sub>1</sub>.</p><p>We also estimated the true values of the cue timing and one and two timing(s) before it in the following way; these values could not be estimated in the abovementioned way because the agent should not know the length of ITI (i.e., when ITI ends) until receiving cue information at the cue timing. In the case where ITI is in fact 4 time steps, until receiving the next cue, the agent should think that ITI can be 4, 5, 6, or 7 time steps with equal probabilities (1/4 for each). Thus, the value of next cue timing and one and two timing(s) before it should be the average of the four ITI<sub>1</sub>-specific values of +4, +3, and +2 time steps from reward. Similarly, in the case where ITI is in fact 5 time steps, until the previous time step of the next cue, the agent should think that ITI can be 4, 5, 6, or 7 time steps with equal probabilities (1/4 for each). Thus, the value of one and two timing(s) before next cue should be the average of the four ITI<sub>1</sub>-specific values of +4 and +3 time steps from reward. On the other hand, at the timing of the next cue, the agent should think that ITI can be 5, 6, or 7 (but not 4) time steps with equal probabilities (1/3 for each). Thus, the value of next cue timing should be the average of the three ITI<sub>1</sub>-specific values (for ITI<sub>1</sub> = 5, 6, or 7) of +5 time steps from reward. Similar considerations can be made for the cases where ITI is in fact 6 or 7 time steps. And then, the ‘true’ value of (next) cue timing can be calculated as the average of the values of next cue timing in the cases where ITI is in fact 4, 5, 6, or 7 time steps. Using these estimated true state values, we calculated TD-RPE at each state/timing (−2, –1, ..., and +5 time steps from cue). True state/timing values in the cases where the cue–reward delay was 4, 5, or 6 time steps were estimated in the same way.</p><p>We also estimated true state/timing values for tasks 1 and 2 that had probabilistic structures. As for task 1, we first estimated the values of each timing in each of the trial types (<xref ref-type="fig" rid="fig4">Figure 4Ba</xref>, left), in which reward was given at early (2 time steps after cue) or late (4 time steps after cue) timing, in the same manner (but using 10000 rather than 1000 simulations for each condition) as done for the cue–reward association task mentioned above (values of the cue timing and the one and two timing(s) before cue after each trial type were also estimated). Then, based on the agent’s belief about trial types (<xref ref-type="fig" rid="fig4">Figure 4Bb</xref>, left), we defined the following states: +1 and +2 time steps from cue (i.e., states visited [entered] before knowing whether reward was given at the early timing [= +2 time step from cue]), +3, 4, 5, and 6 time steps from cue after reception of reward at the early timing, and +3, 4, 5, and 6 time steps from cue after no reception of reward at the early timing (<xref ref-type="fig" rid="fig4">Figure 4Bc</xref>, left-top). We calculated the true values of these states and also of the cue timing and one and two timing(s) before cue (<xref ref-type="fig" rid="fig4">Figure 4Bc</xref>, left-bottom) by taking (mathematical) expected value of the abovementioned estimated value of each timing in each trial type. Using these true values, we calculated TD-RPE (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, left).</p><p>As for task 2, we first estimated the values of each timing in each of the trial types (<xref ref-type="fig" rid="fig4">Figure 4Ba</xref>, right), in which reward was given at early (2 time steps after cue) or late (4 time steps after cue) timing or was not given. Then, based on the agent’s belief about trial types (<xref ref-type="fig" rid="fig4">Figure 4Bb</xref>, right), we defined the following states: +1 and +2 time steps from cue (i.e., states visited [entered] before knowing whether reward was given at the early timing), +3, 4, 5, and 6 time steps from cue after reception of reward at the early timing, +3 and 4 time steps from cue after no reception of reward at the early timing (states visited [entered] before knowing whether reward was given at the late timing [= +4 time step from cue]), +5 and 6 time steps from cue after reception of reward at the late timing, and +5 and 6 time steps from cue after no reception of reward at both early and late timings (<xref ref-type="fig" rid="fig4">Figure 4Bc</xref>, right-top). We estimated the true values of these states and also of the cue timing and one and two timing(s) before cue (<xref ref-type="fig" rid="fig4">Figure 4Bc</xref>, right-bottom) in the same manner as for task 1, and using these true values, we calculated TD-RPE (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right).</p></sec><sec id="s4-7"><title>Analyses, software, and code availability</title><p>SEM was approximated by SD/√<italic>N</italic> (number of samples). Cohen’s <italic>d</italic> using an average variance was calculated as (difference in the means)/(square root of the average of variances). Linear regression, PCA, Wilcoxon rank sum test, and <italic>t</italic>-tests were conducted by using R (functions lm, prcomp, wilcox.exact (in package exactRankTests), and t.test). The difference in the Wilcoxon rank sum test and <italic>t</italic>-tests was reported when p &lt; 0.05. Simulations were conducted by using MATLAB, and pseudo-random numbers were implemented by using rand, randn, and randperm functions. The codes for simulations and analyses are available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/kenjimoritagithub/oVRNN1">https://github.com/kenjimoritagithub/oVRNN1</ext-link>, copy archived at <xref ref-type="bibr" rid="bib92">Morita, 2025</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Formal analysis, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Investigation, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-104101-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The codes for simulations and analyses are available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/kenjimoritagithub/oVRNN1">https://github.com/kenjimoritagithub/oVRNN1</ext-link>, copy archived at <xref ref-type="bibr" rid="bib92">Morita, 2025</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank Dr. Kenji Doya for valuable suggestions. KM was supported by Grants-in-Aid for Scientific Research 23H03295, 23K27985, and 25H02594 from Japan Society for the Promotion of Science (JSPS) and the Naito Foundation. AyK was supported by JSPS Overseas Research Fellowships. ArK was partially funded by Digital Futures (KTH) grant and StratNeuro SRA.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aihara</surname><given-names>K</given-names></name><name><surname>Matsumoto</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1986">1986</year><chapter-title>Chaotic oscillations and bifurcations in squid giant axons</chapter-title><person-group person-group-type="editor"><name><surname>Holden</surname><given-names>AV</given-names></name></person-group><source>Chaos</source><publisher-name>Princeton University Press</publisher-name><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1515/9781400858156.257</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>A theory of adaptive pattern classifiers</article-title><source>IEEE Transactions on Electronic Computers</source><volume>EC-16</volume><fpage>299</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1109/PGEC.1967.264666</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avvisati</surname><given-names>R</given-names></name><name><surname>Kaufmann</surname><given-names>A-K</given-names></name><name><surname>Young</surname><given-names>CJ</given-names></name><name><surname>Portlock</surname><given-names>GE</given-names></name><name><surname>Cancemi</surname><given-names>S</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Magill</surname><given-names>PJ</given-names></name><name><surname>Dodson</surname><given-names>PD</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Distributional coding of associative learning in discrete populations of midbrain dopamine neurons</article-title><source>Cell Reports</source><volume>43</volume><elocation-id>114080</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2024.114080</pub-id><pub-id pub-id-type="pmid">38581677</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bailey</surname><given-names>D</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Predecessor features</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2206.003</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Barto</surname><given-names>AG</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Gradient following without back-propagation in layered networks</article-title><conf-name>Proceedings of the First Annual International Conference on Neural Networks</conf-name><fpage>629</fpage><lpage>636</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beier</surname><given-names>KT</given-names></name><name><surname>Steinberg</surname><given-names>EE</given-names></name><name><surname>DeLoach</surname><given-names>KE</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Miyamichi</surname><given-names>K</given-names></name><name><surname>Schwarz</surname><given-names>L</given-names></name><name><surname>Gao</surname><given-names>XJ</given-names></name><name><surname>Kremer</surname><given-names>EJ</given-names></name><name><surname>Malenka</surname><given-names>RC</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Circuit architecture of VTA dopamine neurons revealed by systematic input-output mapping</article-title><source>Cell</source><volume>162</volume><fpage>622</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.07.015</pub-id><pub-id pub-id-type="pmid">26232228</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Scherr</surname><given-names>F</given-names></name><name><surname>Subramoney</surname><given-names>A</given-names></name><name><surname>Hajek</surname><given-names>E</given-names></name><name><surname>Salaj</surname><given-names>D</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A solution to the learning dilemma for recurrent networks of spiking neurons</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3625</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17236-y</pub-id><pub-id pub-id-type="pmid">32681001</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>GQ</given-names></name><name><surname>Poo</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>10464</fpage><lpage>10472</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-24-10464.1998</pub-id><pub-id pub-id-type="pmid">9852584</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienenstock</surname><given-names>EL</given-names></name><name><surname>Cooper</surname><given-names>LN</given-names></name><name><surname>Munro</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>32</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-01-00032.1982</pub-id><pub-id pub-id-type="pmid">7054394</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title><source>Science</source><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id><pub-id pub-id-type="pmid">28883072</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bono</surname><given-names>J</given-names></name><name><surname>Zannone</surname><given-names>S</given-names></name><name><surname>Pedrosa</surname><given-names>V</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning predictive cognitive maps with spiking neurons during behavior and replays</article-title><source>eLife</source><volume>12</volume><elocation-id>e80671</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80671</pub-id><pub-id pub-id-type="pmid">36927625</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britt</surname><given-names>JP</given-names></name><name><surname>Benaliouad</surname><given-names>F</given-names></name><name><surname>McDevitt</surname><given-names>RA</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name><name><surname>Wise</surname><given-names>RA</given-names></name><name><surname>Bonci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Synaptic and behavioral profile of multiple glutamatergic inputs to the nucleus accumbens</article-title><source>Neuron</source><volume>76</volume><fpage>790</fpage><lpage>803</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.040</pub-id><pub-id pub-id-type="pmid">23177963</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broussard</surname><given-names>JI</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Levine</surname><given-names>AT</given-names></name><name><surname>Tsetsenis</surname><given-names>T</given-names></name><name><surname>Jenson</surname><given-names>D</given-names></name><name><surname>Cao</surname><given-names>F</given-names></name><name><surname>Garcia</surname><given-names>I</given-names></name><name><surname>Arenkiel</surname><given-names>BR</given-names></name><name><surname>Zhou</surname><given-names>FM</given-names></name><name><surname>De Biasi</surname><given-names>M</given-names></name><name><surname>Dani</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine regulates aversive contextual learning and associated in vivo synaptic plasticity in the hippocampus</article-title><source>Cell Reports</source><volume>14</volume><fpage>1930</fpage><lpage>1939</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.01.070</pub-id><pub-id pub-id-type="pmid">26904943</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brozoski</surname><given-names>TJ</given-names></name><name><surname>Brown</surname><given-names>RM</given-names></name><name><surname>Rosvold</surname><given-names>HE</given-names></name><name><surname>Goldman</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Cognitive deficit caused by regional depletion of dopamine in prefrontal cortex of rhesus monkey</article-title><source>Science</source><volume>205</volume><fpage>929</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1126/science.112679</pub-id><pub-id pub-id-type="pmid">112679</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Effects of neuromodulation in a cortical network model of object working memory dominated by recurrent inhibition</article-title><source>Journal of Computational Neuroscience</source><volume>11</volume><fpage>63</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1023/a:1011204814320</pub-id><pub-id pub-id-type="pmid">11524578</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of gamma oscillations</article-title><source>Annual Review of Neuroscience</source><volume>35</volume><fpage>203</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150444</pub-id><pub-id pub-id-type="pmid">22443509</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carta</surname><given-names>I</given-names></name><name><surname>Chen</surname><given-names>CH</given-names></name><name><surname>Schott</surname><given-names>AL</given-names></name><name><surname>Dorizan</surname><given-names>S</given-names></name><name><surname>Khodakhah</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cerebellar modulation of the reward circuitry and social behavior</article-title><source>Science</source><volume>363</volume><elocation-id>eaav0581</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav0581</pub-id><pub-id pub-id-type="pmid">30655412</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Caya-Bissonnette</surname><given-names>L</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Béïque</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Cellular substrate of eligibility traces</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.06.29.547097</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>van Merrienboer</surname><given-names>B</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Bougares</surname><given-names>F</given-names></name><name><surname>Schwenk</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</article-title><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name><pub-id pub-id-type="doi">10.3115/v1/D14-1179</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Vong</surname><given-names>L</given-names></name><name><surname>Lowell</surname><given-names>BB</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title><source>Nature</source><volume>482</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/nature10754</pub-id><pub-id pub-id-type="pmid">22258508</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Opponent actor learning (OpAL): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title><source>Psychological Review</source><volume>121</volume><fpage>337</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1037/a0037015</pub-id><pub-id pub-id-type="pmid">25090423</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cone</surname><given-names>I</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Latent representations in hippocampal network model co-evolve with behavioral exploration of task structure</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>687</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-44871-6</pub-id><pub-id pub-id-type="pmid">38263408</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cone</surname><given-names>I</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Shouval</surname><given-names>HZ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Learning to express reward prediction error-like dopaminergic activity requires plastic representations of time</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>5856</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-50205-3</pub-id><pub-id pub-id-type="pmid">38997276</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cornford</surname><given-names>J</given-names></name><name><surname>Kalajdzievski</surname><given-names>D</given-names></name><name><surname>Leite</surname><given-names>M</given-names></name><name><surname>Lamarquette</surname><given-names>A</given-names></name><name><surname>Kullmann</surname><given-names>DM</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning to Live with Dale’s Principle: ANNs with separate excitatory and inhibitory units</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.02.364968</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Assael</surname><given-names>YM</given-names></name><name><surname>Shillingford</surname><given-names>B</given-names></name><name><surname>Freitas</surname><given-names>N</given-names></name><name><surname>Vogels</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cortical microcircuits as gated-recurrent neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crick</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The recent excitement about neural networks</article-title><source>Nature</source><volume>337</volume><fpage>129</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1038/337129a0</pub-id><pub-id pub-id-type="pmid">2911347</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dominey</surname><given-names>PF</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning</article-title><source>Biological Cybernetics</source><volume>73</volume><fpage>265</fpage><lpage>274</lpage><pub-id pub-id-type="pmid">7548314</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Complementary roles of basal ganglia and cerebellum in learning and motor control</article-title><source>Current Opinion in Neurobiology</source><volume>10</volume><fpage>732</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(00)00153-7</pub-id><pub-id pub-id-type="pmid">11240282</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drew</surname><given-names>PJ</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Extending the effects of spike-timing-dependent plasticity to behavioral timescales</article-title><source>PNAS</source><volume>103</volume><fpage>8876</fpage><lpage>8881</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600676103</pub-id><pub-id pub-id-type="pmid">16731625</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dopamine-mediated stabilization of delay-period activity in a network model of prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>1733</fpage><lpage>1750</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.83.3.1733</pub-id><pub-id pub-id-type="pmid">10712493</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Selen</surname><given-names>LPJ</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Noise in the nervous system</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id><pub-id pub-id-type="pmid">18319728</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>C</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Mackevicius</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title><source>eLife</source><volume>12</volume><elocation-id>e80680</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80680</pub-id><pub-id pub-id-type="pmid">36928104</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Synaptic mechanisms for plasticity in neocortex</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>33</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135516</pub-id><pub-id pub-id-type="pmid">19400721</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Nagase</surname><given-names>AM</given-names></name><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A reinforcement learning approach to understanding procrastination: does inaccurate value approximation cause irrational postponing of a task?</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>660595</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.660595</pub-id><pub-id pub-id-type="pmid">34602962</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floeder</surname><given-names>JR</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Mohebi</surname><given-names>A</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Mesolimbic dopamine ramps reflect environmental timescales</article-title><source>eLife</source><volume>01</volume><elocation-id>e666.1</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.98666.1</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floresco</surname><given-names>SB</given-names></name><name><surname>Magyar</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mesocortical dopamine modulation of executive functions: beyond working memory</article-title><source>Psychopharmacology</source><volume>188</volume><fpage>567</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1007/s00213-006-0404-5</pub-id><pub-id pub-id-type="pmid">16670842</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garr</surname><given-names>E</given-names></name><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Brooke</surname><given-names>S</given-names></name><name><surname>Castell</surname><given-names>L</given-names></name><name><surname>Bal</surname><given-names>A</given-names></name><name><surname>Magnard</surname><given-names>R</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Mesostriatal dopamine is sensitive to changes in specific cue-reward contingencies</article-title><source>Science Advances</source><volume>10</volume><elocation-id>eadn4203</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.adn4203</pub-id><pub-id pub-id-type="pmid">38809978</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>George</surname><given-names>TM</given-names></name><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Rapid learning of predictive maps with STDP and theta phase precession</article-title><source>eLife</source><volume>12</volume><elocation-id>e80663</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80663</pub-id><pub-id pub-id-type="pmid">36927826</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Surmeier</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modulation of striatal projection systems by dopamine</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>441</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-061010-113641</pub-id><pub-id pub-id-type="pmid">21469956</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Learning latent structure: carving nature at its joints</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>251</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.008</pub-id><pub-id pub-id-type="pmid">20227271</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Jones</surname><given-names>CE</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Monfils</surname><given-names>MH</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Gradual extinction prevents the return of fear: implications for the discovery of state</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>7</volume><elocation-id>164</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2013.00164</pub-id><pub-id pub-id-type="pmid">24302899</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gjorgjieva</surname><given-names>J</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Audet</surname><given-names>J</given-names></name><name><surname>Pfister</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-order spatiotemporal correlations</article-title><source>PNAS</source><volume>108</volume><fpage>19383</fpage><lpage>19388</lpage><pub-id pub-id-type="doi">10.1073/pnas.1105933108</pub-id><pub-id pub-id-type="pmid">22080608</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Greedy</surname><given-names>W</given-names></name><name><surname>Zhu</surname><given-names>HW</given-names></name><name><surname>Pemberton</surname><given-names>J</given-names></name><name><surname>Mellor</surname><given-names>J</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Single-phase deep learning in cortico-cortical networks</article-title><conf-name>Advances in Neural Information Processing Systems 35</conf-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Competitive learning: from interactive activation to adaptive resonance</article-title><source>Cognitive Science</source><volume>11</volume><fpage>23</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1111/j.1551-6708.1987.tb00862.x</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards deep learning with segregated dendrites</article-title><source>eLife</source><volume>6</volume><elocation-id>e22901</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22901</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasenstaub</surname><given-names>A</given-names></name><name><surname>Shu</surname><given-names>Y</given-names></name><name><surname>Haider</surname><given-names>B</given-names></name><name><surname>Kraushaar</surname><given-names>U</given-names></name><name><surname>Duque</surname><given-names>A</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Inhibitory postsynaptic potentials carry synchronized frequency information in active cortical networks</article-title><source>Neuron</source><volume>47</volume><fpage>423</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.06.016</pub-id><pub-id pub-id-type="pmid">16055065</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Headley</surname><given-names>DB</given-names></name><name><surname>Kyriazi</surname><given-names>P</given-names></name><name><surname>Feng</surname><given-names>F</given-names></name><name><surname>Nair</surname><given-names>SS</given-names></name><name><surname>Pare</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Gamma oscillations in the basolateral amygdala: localization, microcircuitry, and behavioral correlates</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>6087</fpage><lpage>6101</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3159-20.2021</pub-id><pub-id pub-id-type="pmid">34088799</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Romero Pinto</surname><given-names>SA</given-names></name><name><surname>Yamaguchi</surname><given-names>T</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Emergence of belief-like representations through reinforcement learning</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011067</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011067</pub-id><pub-id pub-id-type="pmid">37695776</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The vanishing gradient problem during learning recurrent neural nets and problem solutions</article-title><source>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</source><volume>06</volume><fpage>107</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1142/S0218488598000094</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoerzer</surname><given-names>GM</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>677</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs348</pub-id><pub-id pub-id-type="pmid">23146969</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hooks</surname><given-names>BM</given-names></name><name><surname>Papale</surname><given-names>AE</given-names></name><name><surname>Paletzki</surname><given-names>RF</given-names></name><name><surname>Feroze</surname><given-names>MW</given-names></name><name><surname>Eastwood</surname><given-names>BS</given-names></name><name><surname>Couey</surname><given-names>JJ</given-names></name><name><surname>Winnubst</surname><given-names>J</given-names></name><name><surname>Chandrashekar</surname><given-names>J</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Topographic precision in sensory and motor corticostriatal projections varies across cell type and cortical area</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3549</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05780-7</pub-id><pub-id pub-id-type="pmid">30177709</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms underlying cortical activity during value-guided choice</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>470</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nn.3017</pub-id><pub-id pub-id-type="pmid">22231429</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Illing</surname><given-names>B</given-names></name><name><surname>Ventura</surname><given-names>J</given-names></name><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Local plasticity rules can learn deep representations using self-supervised contrastive predictions</article-title><conf-name>Advances in Neural Information Processing Systems 34 NeurIPS</conf-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Echo state network</article-title><source>Scholarpedia</source><volume>2</volume><elocation-id>2330</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.2330</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Taylor</surname><given-names>A</given-names></name><name><surname>Floeder</surname><given-names>JR</given-names></name><name><surname>Lohmann</surname><given-names>M</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Burke</surname><given-names>DA</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mesolimbic dopamine release conveys causal associations</article-title><source>Science</source><volume>378</volume><elocation-id>eabq6740</elocation-id><pub-id pub-id-type="doi">10.1126/science.abq6740</pub-id><pub-id pub-id-type="pmid">36480599</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname><given-names>A</given-names></name><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Forgetting in reinforcement learning links sustained dopamine signals to motivation</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005145</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005145</pub-id><pub-id pub-id-type="pmid">27736881</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname><given-names>A</given-names></name><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Striatal gradient in value-decay explains regional differences in dopamine patterns and reinforcement learning computations</article-title><source>The Journal of Neuroscience</source><volume>45</volume><elocation-id>e0170252025</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0170-25.2025</pub-id><pub-id pub-id-type="pmid">40681344</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keiflin</surname><given-names>R</given-names></name><name><surname>Pribut</surname><given-names>HJ</given-names></name><name><surname>Shah</surname><given-names>NB</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ventral tegmental dopamine neurons participate in reward identity predictions</article-title><source>Current Biology</source><volume>29</volume><fpage>93</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.050</pub-id><pub-id pub-id-type="pmid">30581025</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Malik</surname><given-names>AN</given-names></name><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Bech</surname><given-names>P</given-names></name><name><surname>Tsutsui-Kimura</surname><given-names>I</given-names></name><name><surname>Sun</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A unified framework for dopamine signals across timescales</article-title><source>Cell</source><volume>183</volume><fpage>1600</fpage><lpage>1616</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.11.013</pub-id><pub-id pub-id-type="pmid">33248024</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konishi</surname><given-names>M</given-names></name><name><surname>Igarashi</surname><given-names>KM</given-names></name><name><surname>Miura</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Biologically plausible local synaptic learning rules robustly implement deep supervised learning</article-title><source>Frontiers in Neuroscience</source><volume>17</volume><elocation-id>1160899</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2023.1160899</pub-id><pub-id pub-id-type="pmid">37886676</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Supervised and unsupervised learning with two sites of synaptic integration</article-title><source>Journal of Computational Neuroscience</source><volume>11</volume><fpage>207</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1023/a:1013776130161</pub-id><pub-id pub-id-type="pmid">11796938</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krausz</surname><given-names>TA</given-names></name><name><surname>Comrie</surname><given-names>AE</given-names></name><name><surname>Kahn</surname><given-names>AE</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Berke</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dual credit assignment processes underlie dopamine signals in a complex spatial environment</article-title><source>Neuron</source><volume>111</volume><fpage>3465</fpage><lpage>3478</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.07.017</pub-id><pub-id pub-id-type="pmid">37611585</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langdon</surname><given-names>AJ</given-names></name><name><surname>Sharpe</surname><given-names>MJ</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Model-based predictions for dopamine</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.10.006</pub-id><pub-id pub-id-type="pmid">29096115</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>IB</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Han</surname><given-names>N-E</given-names></name><name><surname>Slavuj</surname><given-names>M</given-names></name><name><surname>Hwang</surname><given-names>JW</given-names></name><name><surname>Lee</surname><given-names>A</given-names></name><name><surname>Sun</surname><given-names>T</given-names></name><name><surname>Jeong</surname><given-names>Y</given-names></name><name><surname>Baik</surname><given-names>J-H</given-names></name><name><surname>Park</surname><given-names>J-Y</given-names></name><name><surname>Choi</surname><given-names>S-Y</given-names></name><name><surname>Kwag</surname><given-names>J</given-names></name><name><surname>Yoon</surname><given-names>B-J</given-names></name></person-group><year iso-8601-date="2024">2024a</year><article-title>Persistent enhancement of basolateral amygdala-dorsomedial striatum synapses causes compulsive-like behaviors in mice</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>219</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-44322-8</pub-id><pub-id pub-id-type="pmid">38191518</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RS</given-names></name><name><surname>Sagiv</surname><given-names>Y</given-names></name><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2024">2024b</year><article-title>A feature-specific prediction error model explains dopaminergic heterogeneity</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>1574</fpage><lpage>1586</lpage><pub-id pub-id-type="doi">10.1038/s41593-024-01689-1</pub-id><pub-id pub-id-type="pmid">38961229</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>DJ</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The root of all value: a neural common currency for choice</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>1027</fpage><lpage>1038</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.06.001</pub-id><pub-id pub-id-type="pmid">22766486</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Rainnie</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bidirectional regulation of synaptic plasticity in the basolateral amygdala induced by the D1-like family of dopamine receptors and group II metabotropic glutamate receptors</article-title><source>The Journal of Physiology</source><volume>592</volume><fpage>4329</fpage><lpage>4351</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2014.277715</pub-id><pub-id pub-id-type="pmid">25107924</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Cornford</surname><given-names>J</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning better with dale’s law: a spectral perspective</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.06.28.546924</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Cownden</surname><given-names>D</given-names></name><name><surname>Tweed</surname><given-names>DB</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Marris</surname><given-names>L</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Backpropagation and the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>335</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id><pub-id pub-id-type="pmid">32303713</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lo</surname><given-names>C-C</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortico-basal ganglia circuit mechanism for a decision threshold in reaction time tasks</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>956</fpage><lpage>963</lpage><pub-id pub-id-type="doi">10.1038/nn1722</pub-id><pub-id pub-id-type="pmid">16767089</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowet</surname><given-names>AS</given-names></name><name><surname>Zheng</surname><given-names>Q</given-names></name><name><surname>Meng</surname><given-names>M</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>An opponent striatal circuit for distributional reinforcement learning</article-title><source>Nature</source><volume>639</volume><fpage>717</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-08488-5</pub-id><pub-id pub-id-type="pmid">39972123</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Kehoe</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Evaluating the TD model of classical conditioning</article-title><source>Learning &amp; Behavior</source><volume>40</volume><fpage>305</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.3758/s13420-012-0082-6</pub-id><pub-id pub-id-type="pmid">22927003</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Lübke</surname><given-names>J</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title><source>Science</source><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>A theory of cerebellar cortex</article-title><source>The Journal of Physiology</source><volume>202</volume><fpage>437</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1969.sp008820</pub-id><pub-id pub-id-type="pmid">5784296</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Linking connectivity, dynamics, and computations in low-rank recurrent neural networks</article-title><source>Neuron</source><volume>99</volume><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.003</pub-id><pub-id pub-id-type="pmid">30057201</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Max</surname><given-names>K</given-names></name><name><surname>Kriener</surname><given-names>L</given-names></name><name><surname>Pineda García</surname><given-names>G</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name><name><surname>Jaras</surname><given-names>I</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Petrovici</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Learning efficient backprojections across cortical hierarchies in real time</article-title><source>Nature Machine Intelligence</source><volume>6</volume><fpage>619</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1038/s42256-024-00845-3</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzoni</surname><given-names>P</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1991">1991a</year><article-title>A more biologically plausible learning rule for neural networks</article-title><source>PNAS</source><volume>88</volume><fpage>4433</fpage><lpage>4437</lpage><pub-id pub-id-type="doi">10.1073/pnas.88.10.4433</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzoni</surname><given-names>P</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1991">1991b</year><article-title>A more biologically plausible learning rule than backpropagation applied to a network model of cortical area 7a</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>293</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.4.293</pub-id><pub-id pub-id-type="pmid">1822737</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning reward uncertainty in the basal ganglia</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005062</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005062</pub-id><pub-id pub-id-type="pmid">27589489</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname><given-names>PR</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id><pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morishima</surname><given-names>M</given-names></name><name><surname>Morita</surname><given-names>K</given-names></name><name><surname>Kubota</surname><given-names>Y</given-names></name><name><surname>Kawaguchi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Highly differentiated projection-specific cortical subnetworks</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10380</fpage><lpage>10391</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0772-11.2011</pub-id><pub-id pub-id-type="pmid">21753015</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Possible role of dendritic compartmentalization in the spatial working memory circuit</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>7699</fpage><lpage>7724</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0059-08.2008</pub-id><pub-id pub-id-type="pmid">18650346</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name><name><surname>Kalra</surname><given-names>R</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name><name><surname>Robinson</surname><given-names>HPC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Recurrent synaptic input and the timing of gamma-frequency-modulated firing of pyramidal cells during neocortical “UP” states</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>1871</fpage><lpage>1881</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3948-07.2008</pub-id><pub-id pub-id-type="pmid">18287504</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differential cortical activation of the striatal direct and indirect pathway cells: reconciling the anatomical and optogenetic results by using a computational method</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>120</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1152/jn.00625.2013</pub-id><pub-id pub-id-type="pmid">24598515</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name><name><surname>Kato</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Striatal dopamine ramping may indicate flexible reinforcement learning with forgetting in the cortico-basal ganglia circuits</article-title><source>Frontiers in Neural Circuits</source><volume>8</volume><elocation-id>36</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2014.00036</pub-id><pub-id pub-id-type="pmid">24782717</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name><name><surname>Kawaguchi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dual role hypothesis of the cortico-basal-ganglia pathways: opponency and temporal difference through dopamine and adenosine</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><elocation-id>111</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00111</pub-id><pub-id pub-id-type="pmid">30687019</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name><name><surname>Im</surname><given-names>S</given-names></name><name><surname>Kawaguchi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Differential striatal axonal arborizations of the intratelencephalic and pyramidal-tract neurons: analysis of the data in the mouselight database</article-title><source>Frontiers in Neural Circuits</source><volume>13</volume><elocation-id>71</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2019.00071</pub-id><pub-id pub-id-type="pmid">31803027</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>OVRNN1</data-title><version designator="swh:1:rev:090f3bee166d07ca8f9f06aa98a2a48a4073de0c">swh:1:rev:090f3bee166d07ca8f9f06aa98a2a48a4073de0c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:91430f8c49958a81379fd801952c0b3de08f42ee;origin=https://github.com/kenjimoritagithub/oVRNN1;visit=swh:1:snp:f80ce5aeb567b53301eae4dff328701f13cf1f91;anchor=swh:1:rev:090f3bee166d07ca8f9f06aa98a2a48a4073de0c">https://archive.softwareheritage.org/swh:1:dir:91430f8c49958a81379fd801952c0b3de08f42ee;origin=https://github.com/kenjimoritagithub/oVRNN1;visit=swh:1:snp:f80ce5aeb567b53301eae4dff328701f13cf1f91;anchor=swh:1:rev:090f3bee166d07ca8f9f06aa98a2a48a4073de0c</ext-link></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local online learning in recurrent networks with random feedback</article-title><source>eLife</source><volume>8</volume><elocation-id>e43299</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43299</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dialogues on prediction errors</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>265</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.03.006</pub-id><pub-id pub-id-type="pmid">18567531</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning task-state representations</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1544</fpage><lpage>1553</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0470-8</pub-id><pub-id pub-id-type="pmid">31551597</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Critchley</surname><given-names>H</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Temporal difference models and reward-related learning in the human brain</article-title><source>Neuron</source><volume>38</volume><fpage>329</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00169-7</pub-id><pub-id pub-id-type="pmid">12718865</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>J</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title><source>Science</source><volume>304</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1126/science.1094285</pub-id><pub-id pub-id-type="pmid">15087550</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otani</surname><given-names>S</given-names></name><name><surname>Daniel</surname><given-names>H</given-names></name><name><surname>Roisin</surname><given-names>MP</given-names></name><name><surname>Crepel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dopaminergic modulation of long-term synaptic plasticity in rat prefrontal neurons</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1251</fpage><lpage>1256</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg092</pub-id><pub-id pub-id-type="pmid">14576216</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagkalos</surname><given-names>M</given-names></name><name><surname>Makarov</surname><given-names>R</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Leveraging dendritic properties to advance machine learning and neuro-inspired computing</article-title><source>Current Opinion in Neurobiology</source><volume>85</volume><elocation-id>102853</elocation-id><pub-id pub-id-type="doi">10.1016/j.conb.2024.102853</pub-id><pub-id pub-id-type="pmid">38394956</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1010</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00857-x</pub-id><pub-id pub-id-type="pmid">33986551</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Brannon</surname><given-names>T</given-names></name><name><surname>Mel</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pyramidal neuron as two-layer neural network</article-title><source>Neuron</source><volume>37</volume><fpage>989</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00149-1</pub-id><pub-id pub-id-type="pmid">12670427</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>L</given-names></name><name><surname>Burrell</surname><given-names>M</given-names></name><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Prospective contingency explains behavior and dopamine signals during associative learning</article-title><source>Nature Neuroscience</source><volume>28</volume><fpage>1280</fpage><lpage>1292</lpage><pub-id pub-id-type="doi">10.1038/s41593-025-01915-4</pub-id><pub-id pub-id-type="pmid">40102680</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Eigenvalue spectra of random matrices for neural networks</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>188104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.188104</pub-id><pub-id pub-id-type="pmid">17155583</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Jensen</surname><given-names>S</given-names></name><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling</article-title><source>Psychological Review</source><volume>114</volume><fpage>784</fpage><lpage>805</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.3.784</pub-id><pub-id pub-id-type="pmid">17638506</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JN</given-names></name><name><surname>Hyland</surname><given-names>BI</given-names></name><name><surname>Wickens</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cellular mechanism of reward-related learning</article-title><source>Nature</source><volume>413</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/35092560</pub-id><pub-id pub-id-type="pmid">11544526</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986a</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986b</year><chapter-title>Learning internal representations by error propagation</chapter-title><person-group person-group-type="editor"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</source><publisher-name>MIT Press Direct</publisher-name><fpage>318</fpage><lpage>362</lpage></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005768</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005768</pub-id><pub-id pub-id-type="pmid">28945743</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title><conf-name>Advances in Neural Information Processing Systems 31 NeurIPS</conf-name></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saez</surname><given-names>A</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Abstract context representations in primate amygdala and prefrontal cortex</article-title><source>Neuron</source><volume>87</volume><fpage>869</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.024</pub-id><pub-id pub-id-type="pmid">26291167</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname><given-names>K</given-names></name><name><surname>Ueda</surname><given-names>Y</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Kimura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Representation of action-specific reward values in the striatum</article-title><source>Science</source><volume>310</volume><fpage>1337</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1126/science.1115270</pub-id><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>R</given-names></name><name><surname>Shimomura</surname><given-names>K</given-names></name><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Opponent learning with different representations in the cortico-basal ganglia pathways can develop obsession-compulsion cycle</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011206</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011206</pub-id><pub-id pub-id-type="pmid">37319256</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawaguchi</surname><given-names>T</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>D1 dopamine receptors in prefrontal cortex: involvement in working memory</article-title><source>Science</source><volume>251</volume><fpage>947</fpage><lpage>950</lpage><pub-id pub-id-type="doi">10.1126/science.1825731</pub-id><pub-id pub-id-type="pmid">1825731</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sayegh</surname><given-names>FJP</given-names></name><name><surname>Mouledous</surname><given-names>L</given-names></name><name><surname>Macri</surname><given-names>C</given-names></name><name><surname>Pi Macedo</surname><given-names>J</given-names></name><name><surname>Lejards</surname><given-names>C</given-names></name><name><surname>Rampon</surname><given-names>C</given-names></name><name><surname>Verret</surname><given-names>L</given-names></name><name><surname>Dahan</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Ventral tegmental area dopamine projections to the hippocampus trigger long-term potentiation and contextual learning</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>4100</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-47481-4</pub-id><pub-id pub-id-type="pmid">38773091</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>W</given-names></name><name><surname>Flajolet</surname><given-names>M</given-names></name><name><surname>Greengard</surname><given-names>P</given-names></name><name><surname>Surmeier</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title><source>Science</source><volume>321</volume><fpage>848</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1160575</pub-id><pub-id pub-id-type="pmid">18687967</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimomura</surname><given-names>K</given-names></name><name><surname>Kato</surname><given-names>A</given-names></name><name><surname>Morita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rigid reduced successor representation as a potential mechanism for addiction</article-title><source>The European Journal of Neuroscience</source><volume>53</volume><fpage>3768</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.1111/ejn.15227</pub-id><pub-id pub-id-type="pmid">33840120</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shouval</surname><given-names>HZ</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Wittenberg</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Spike timing dependent plasticity: a consequence of more fundamental learning rules</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00019</pub-id><pub-id pub-id-type="pmid">20725599</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shouval</surname><given-names>HZ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What is the appropriate description level for synaptic plasticity?</article-title><source>PNAS</source><volume>108</volume><fpage>19103</fpage><lpage>19104</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117027108</pub-id><pub-id pub-id-type="pmid">22089234</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sias</surname><given-names>AC</given-names></name><name><surname>Jafar</surname><given-names>Y</given-names></name><name><surname>Goodpaster</surname><given-names>CM</given-names></name><name><surname>Ramírez-Armenta</surname><given-names>K</given-names></name><name><surname>Wrenn</surname><given-names>TM</given-names></name><name><surname>Griffin</surname><given-names>NK</given-names></name><name><surname>Patel</surname><given-names>K</given-names></name><name><surname>Lamparelli</surname><given-names>AC</given-names></name><name><surname>Sharpe</surname><given-names>MJ</given-names></name><name><surname>Wassum</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Dopamine projections to the basolateral amygdala drive the encoding of identity-specific reward memories</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>728</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1038/s41593-024-01586-7</pub-id><pub-id pub-id-type="pmid">38396258</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Millidge</surname><given-names>B</given-names></name><name><surname>Salvatori</surname><given-names>T</given-names></name><name><surname>Lukasiewicz</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Inferring neural activity before plasticity as a foundation for learning beyond backpropagation</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>348</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01514-1</pub-id><pub-id pub-id-type="pmid">38172438</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Howard</surname><given-names>JD</given-names></name><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Kahnt</surname><given-names>T</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dopamine neuron ensembles signal the content of sensory prediction errors</article-title><source>eLife</source><volume>8</volume><elocation-id>e49315</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49315</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starkweather</surname><given-names>CK</given-names></name><name><surname>Babayan</surname><given-names>BM</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dopamine reward prediction errors reflect hidden-state inference across time</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>581</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/nn.4520</pub-id><pub-id pub-id-type="pmid">28263301</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starkweather</surname><given-names>CK</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The medial prefrontal cortex shapes dopamine reward prediction errors under state uncertainty</article-title><source>Neuron</source><volume>98</volume><fpage>616</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.036</pub-id><pub-id pub-id-type="pmid">29656872</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinberg</surname><given-names>EE</given-names></name><name><surname>Keiflin</surname><given-names>R</given-names></name><name><surname>Boivin</surname><given-names>JR</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A causal link between prediction errors, dopamine neurons and learning</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>966</fpage><lpage>973</lpage><pub-id pub-id-type="doi">10.1038/nn.3413</pub-id><pub-id pub-id-type="pmid">23708143</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>R</given-names></name><name><surname>Barto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.1016/S1474-6670(17)38315-5</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><edition>2nd Ed</edition><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>Y</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Silencing the critics: understanding the effects of cocaine sensitization on dorsolateral and ventral striatum in the context of an actor/critic model</article-title><source>Frontiers in Neuroscience</source><volume>2</volume><fpage>86</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.014.2008</pub-id><pub-id pub-id-type="pmid">18982111</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Toreson</surname><given-names>K</given-names></name><name><surname>O’Donnell</surname><given-names>P</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Expectancy-related changes in firing of dopamine neurons depend on orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1590</fpage><lpage>1597</lpage><pub-id pub-id-type="doi">10.1038/nn.2957</pub-id><pub-id pub-id-type="pmid">22037501</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Montesinos-Cartegena</surname><given-names>M</given-names></name><name><surname>Kahnt</surname><given-names>T</given-names></name><name><surname>Langdon</surname><given-names>AJ</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Expectancy-related changes in firing of dopamine neurons depend on hippocampus</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.19.549728</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>G</given-names></name><name><surname>Yamane</surname><given-names>T</given-names></name><name><surname>Héroux</surname><given-names>JB</given-names></name><name><surname>Nakane</surname><given-names>R</given-names></name><name><surname>Kanazawa</surname><given-names>N</given-names></name><name><surname>Takeda</surname><given-names>S</given-names></name><name><surname>Numata</surname><given-names>H</given-names></name><name><surname>Nakano</surname><given-names>D</given-names></name><name><surname>Hirose</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recent advances in physical reservoir computing: A review</article-title><source>Neural Networks</source><volume>115</volume><fpage>100</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2019.03.005</pub-id><pub-id pub-id-type="pmid">30981085</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>R</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Osakada</surname><given-names>F</given-names></name><name><surname>Kobak</surname><given-names>D</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distributed and mixed information in monosynaptic inputs to dopamine neurons</article-title><source>Neuron</source><volume>91</volume><fpage>1374</fpage><lpage>1389</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.018</pub-id><pub-id pub-id-type="pmid">27618675</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsetsenis</surname><given-names>T</given-names></name><name><surname>Badyna</surname><given-names>JK</given-names></name><name><surname>Wilson</surname><given-names>JA</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Krizman</surname><given-names>EN</given-names></name><name><surname>Subramaniyan</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Thomas</surname><given-names>SA</given-names></name><name><surname>Dani</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Midbrain dopaminergic innervation of the hippocampus is sufficient to modulate formation of aversive memories</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2111069118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2111069118</pub-id><pub-id pub-id-type="pmid">34580198</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wall</surname><given-names>NR</given-names></name><name><surname>De La Parra</surname><given-names>M</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name><name><surname>Kreitzer</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Differential innervation of direct- and indirect-pathway striatal projection neurons</article-title><source>Neuron</source><volume>79</volume><fpage>347</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.05.014</pub-id><pub-id pub-id-type="pmid">23810541</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neurophysiological and computational principles of cortical rhythms in cognition</article-title><source>Physiological Reviews</source><volume>90</volume><fpage>1195</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1152/physrev.00035.2008</pub-id><pub-id pub-id-type="pmid">20664082</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wärnberg</surname><given-names>E</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Feasibility of dopamine as a vector-valued feedback signal in the basal ganglia</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2221994120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2221994120</pub-id><pub-id pub-id-type="pmid">37527344</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><name><surname>Ogawa</surname><given-names>SK</given-names></name><name><surname>Vamanrao</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Whole-brain mapping of direct inputs to midbrain dopamine neurons</article-title><source>Neuron</source><volume>74</volume><fpage>858</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.017</pub-id><pub-id pub-id-type="pmid">22681690</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theories of error back-propagation in the brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>235</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.12.005</pub-id><pub-id pub-id-type="pmid">30704969</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>SM</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Widespread origin of the primate mesofrontal dopamine system</article-title><source>Cerebral Cortex</source><volume>8</volume><fpage>321</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1093/cercor/8.4.321</pub-id><pub-id pub-id-type="pmid">9651129</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yagishita</surname><given-names>S</given-names></name><name><surname>Hayashi-Takagi</surname><given-names>A</given-names></name><name><surname>Ellis-Davies</surname><given-names>GCR</given-names></name><name><surname>Urakubo</surname><given-names>H</given-names></name><name><surname>Ishii</surname><given-names>S</given-names></name><name><surname>Kasai</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title><source>Science</source><volume>345</volume><fpage>1616</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1126/science.1255514</pub-id><pub-id pub-id-type="pmid">25258080</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Performance (mean squared value-error from cue to reward) of modified oVRNNbp-rev (red lines) and oVRNNrf-bio (blue lines) incorporating eligibility trace in the Pavlovian cue–reward association task with 6 time steps cue–reward delay, with the default learning rates (left panel) or halved learning rates (right panel) used in <xref ref-type="fig" rid="fig6">Figure 6J</xref>, left and right, respectively.</title><p>The eligibility-trace parameter <italic>η</italic> was varied as indicated by the horizontal axis. The data at <italic>η</italic> = 0 are the results for the original models without eligibility trace (the same data as those shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>), for which 100 simulations were executed. For the other cases, we executed 1000 simulations in order to precisely examine changes in the performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Performance (mean squared value-error from cue to reward) of modified oVRNNrf-bio incorporating behavioral time-scale synaptic plasticity (BTSP) in the Pavlovian cue–reward association task with 3 time steps cue–reward delay.</title><p>In the modified model, the update rule for the weights on the RNN units was modified so that the update could depend on both pre → post and post → pre activity pairings. The proportion of update induced by pre → post activity pairing (parameter <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$k_{\rm pre\rightarrow post}$\end{document}</tex-math></alternatives></inline-formula>) was varied to 1 (corresponding to the original oVRNNrf-bio), 0.75, 2/3(=0.66..), 0.6, or 0.5, indicated by solid lines with different colors. The gray dotted line indicates the performance of an agent having an untrained RNN with connections shuffled from learned original oVRNNrf-bio. Other configurations are the same as those for <xref ref-type="fig" rid="fig6">Figure 6E</xref>, left. Other parameters were set as follows. Learning rates: aRNN = 0.1 and avalue = 0.1/(<italic>n</italic>/12). Time discount factor: <italic>γ</italic> = 0.8. Number of trials: 1500. Number of simulations: 100 for each condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104101-app1-fig2-v1.tif"/></fig><sec sec-type="appendix" id="s8"><title>1.1 Eligibility trace</title><p>As mentioned in the Introduction and Discussion, our model has relations to other models, including the algorithms named RFLO (random feedback local online) (<xref ref-type="bibr" rid="bib93">Murray, 2019</xref>) and e-prop (<xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>). In the derivation of RFLO (<xref ref-type="bibr" rid="bib93">Murray, 2019</xref>), initially, the partial derivative of the across-time sum of squared errors with respect to each RNN weight was taken (as in the exact RTRL (Real-Time Recurrent Learning)), and then in order to make the update rule local, the parts depending on the other weights were omitted, while the local ‘eligibility traces’ remained. e-prop (<xref ref-type="bibr" rid="bib7">Bellec et al., 2020</xref>) also resulted in the terms interpreted as eligibility traces. By contrast, we started from the partial derivative of the squared error at the current time rather than its across-time sum, and the resulting update rule does not contain eligibility trace. Our model could still solve credit assignment at least to some extent because we use TD error and TD learning itself (even TD(0) without eligibility trace) is a solution for credit assignment as we mentioned in the Discussion.</p><p>Based on these considerations, we conjectured that adding eligibility trace to our update rule could possibly improve the learnability of our model, especially in the case with long cue–reward delay, and we tested it. We considered modified models, in which the terms depending on the pre-synaptic and post-synaptic activities in the update rules for the connections on the RNN units were replaced with the terms including eligibility traces that were locally updated and exponentially decayed. Specifically, we examined modified oVRNNbp-rev and oVRNNrf-bio, in which the following part in the update rules for elements of <bold>A</bold> and <bold>B</bold>:</p><p><inline-formula><alternatives><mml:math id="inf65"><mml:mstyle><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$X_{ij}=x_{j}\left (t- 1\right)x_{i}\left (t\right)\left (1- x_{i}\left (t\right)\right)$\end{document}</tex-math></alternatives></inline-formula> in oVRNNbp-rev, or <inline-formula><alternatives><mml:math id="inf66"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft66">\begin{document}$X_{ij}=x_{j}\left (t- 1\right)x_{i}\left (t\right)\left (1- x_{i}\left (t\right)\right)\left (x_{i}\left (t\right)\leq 0.5\right)$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mn>0.25</mml:mn><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$0.25x_{j}\left (t- 1\right)\left (x_{i}\left (t\right)\gt 0.5\right)$\end{document}</tex-math></alternatives></inline-formula> in oVRNNrf-bio was replaced with (1−η)Zij, where Zij was updated at every time step as follows:</p><p><inline-formula><alternatives><mml:math id="inf68"><mml:mstyle><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$Z_{ij}\leftarrow \eta Z_{ij}+x_{j}\left (t- 1\right)x_{i}\left (t\right)\left (1- x_{i}\left (t\right)\right)$\end{document}</tex-math></alternatives></inline-formula> in oVRNNbp-rev, or <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$Z_{ij}\leftarrow \eta Z_{ij}+x_{j}\left (t- 1\right)x_{i}\left (t\right)\left (1- x_{i}\left (t\right)\right)\left (x_{i}\left (t\right)\leq 0.5\right)$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf70"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mn>0.25</mml:mn><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft70">\begin{document}$0.25x_{j}\left (t- 1\right)\left (x_{i}\left (t\right)\gt 0.5\right)$\end{document}</tex-math></alternatives></inline-formula> in oVRNNrf-bio was a parameter indicating the length (time scale) of eligibility trace.</p><p>We simulated the Pavlovian cue–reward association task with 6 time steps cue–reward delay, varying the parameter <italic>η</italic>. As a result, in the model with random feedback, oVRNNrf-bio, with 40 RNN units and the default learning rates that we used, eligibility trace indeed improved the performance, though not drastically (the blue line in the left panel of <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), while in the model with symmetric feedback (oVRNNbp-rev), there was no improvement (the red line in the left panel). In the cases with the learning rates halved (used in <xref ref-type="fig" rid="fig6">Figure 6J</xref>, right), the results look more complicated (the right panel).</p></sec><sec sec-type="appendix" id="s9"><title>1.2 Behavioral time-scale synaptic plasticity</title><p>We originally assumed that update of the connections onto the RNN units is implemented by Hebbian synaptic plasticity. However, considering this point further, we have realized that Hebbian plasticity, or at least a typical form of it, namely, the spike-timing-dependent-plasticity (STDP) (<xref ref-type="bibr" rid="bib76">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib8">Bi and Poo, 1998</xref>) may not be able to implement the update of the RNN connections in our model. This is because while the single time step in our model was assumed to correspond to several hundreds of milliseconds, the time scale of STDP is much shorter, up to several tens of milliseconds. Besides, measurable degrees of STDP would be caused only after repetitive pairing of pre- and post-synaptic spikes, whereas our non-spiking rate-based model has no description of such repetitive pairing. There have been proposals on how such a gap between the time scale of STDP and behavioral time scale could be bridged (<xref ref-type="bibr" rid="bib38">George et al., 2023</xref>; <xref ref-type="bibr" rid="bib11">Bono et al., 2023</xref>; <xref ref-type="bibr" rid="bib29">Drew and Abbott, 2006</xref>), and with such mechanisms, STDP could still potentially implement the update of the RNN connections in our model. However, it may be more likely to be implemented by a different form of synaptic plasticity named the behavioral time-scale synaptic plasticity (BTSP) (<xref ref-type="bibr" rid="bib10">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Caya-Bissonnette et al., 2023</xref>).</p><p>BTSP, originally found in hippocampal CA1 synapses (<xref ref-type="bibr" rid="bib10">Bittner et al., 2017</xref>), is induced by pairing of pre- and post-synaptic activity, similar to STDP. However, different from STDP, the time scale of BTSP is much longer, up to seconds, and also BTSP can be induced with only a few, or even a single, pairing. These features could better fit the update of the RNN connections in our model. Recently, <xref ref-type="bibr" rid="bib18">Caya-Bissonnette et al., 2023</xref>, BTSP was found also in the neocortex, in particular, the prefrontal cortex, which receives rich DA projections and is thus a primarily hypothesized brain region for the RNN in our model. Compared to hippocampal BTSP, prefrontal BTSP has somewhat shorter time scale, several hundreds of millisecond difference between pre- and post-synaptic activity, for potentiation, and also differs in that pairing within a time window of 100–300 ms does not cause potentiation (instead causing slight depression) (<xref ref-type="bibr" rid="bib18">Caya-Bissonnette et al., 2023</xref>).</p><p>In terms of time scale, BTSP appears to be good for implementing the update of the RNN connections in our model. However, two points need consideration. First, bidirectional modulation of BTSP by positive/negative DA signals will be required, but it has not yet been experimentally shown. Second, in both hippocampal (<xref ref-type="bibr" rid="bib10">Bittner et al., 2017</xref>) and prefrontal (<xref ref-type="bibr" rid="bib18">Caya-Bissonnette et al., 2023</xref>) BTSP, potentiation is caused not only by the pre → post order but also by the post → pre order of activity pairing, different from STDP. As for hippocampal BTSP, there is still an asymmetry, that is, potentiation occurs more/longer for pre → post pairing than for post → pre pairing (Figure 3D of <xref ref-type="bibr" rid="bib10">Bittner et al., 2017</xref>). Similar asymmetry was not reported for prefrontal BTSP (<xref ref-type="bibr" rid="bib18">Caya-Bissonnette et al., 2023</xref>), although asymmetry could still possibly appear due to additional factors in vivo. The update rule for the RNN connections in our model, inherited from the original derivation according to the gradient descent, assumes potentiation for pre → post pairing but not for post → pre pairing under positive TD-RPE. It thus remains elusive whether learning could still occur in our model when not only pre → post pairing but also post → pre pairing induces potentiation under positive TD-RPE.</p><p>We examined this by simulations of the Pavlovian cue–reward association task with 3 time steps cue–reward delay with modified oVRNNrf-bio, in which update of the connections on the RNN units was induced by pre → post and post → pre activity pairings with <inline-formula><alternatives><mml:math id="inf71"><mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft71">\begin{document}$k_{pre\rightarrow post}\colon 1- k_{pre\rightarrow post}$\end{document}</tex-math></alternatives></inline-formula> ratio, with <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$k_{pre\rightarrow post}$\end{document}</tex-math></alternatives></inline-formula> varied from 1 (corresponding to the original oVRNNrf-bio), 0.75 (3:1 ratio), 0.66 (2:1 ratio), 0.6 (3:2 ratio), to 0.5 (1:1 ratio, i.e., symmetric). More specifically, we examined modified oVRNNrf-bio, in which the update rules for <bold>A</bold> and <bold>B</bold> (<xref ref-type="disp-formula" rid="equ22 equ23 equ24 equ25">Equations 3.5–3.8</xref>) were replaced with the followings:<disp-formula id="equ34"><alternatives><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>0.25</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t34">\begin{document}$$\displaystyle  \Delta A_{ij_{\rm pre}\rightarrow \rm post}= &amp; \, a_{\rm RNN}\delta \left (t\right)x_{j}\left (t- 1\right)x_{i}\left (t\right)\left (1- x_{i}\left (t\right)\right)c_{i}\left (x_{i}\left (t\right)\leq 0.5\right) \\ &amp; {\rm or}\,\, 0.25a_{\rm RNN}\delta \left (t\right)x_{j}\left (t- 1\right)c_{i}\left (x_{i}\left (t\right)\gt 0.5\right) $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ35"><alternatives><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>0.25</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t35">\begin{document}$$\displaystyle  \Delta B_{ik_{\rm pre}\rightarrow \rm post}= &amp; \, a_{\rm RNN}\delta \left (t\right)o_{k}\left (t- 1\right)x_{i}\left (t\right)\left (1- x_{i}\left (t\right)\right)c_{i}\left (x_{i}\left (t\right)\leq 0.5\right) \\ &amp;{\rm or}\,\, 0.25a_{\rm RNN}\delta \left (t\right)o_{k}\left (t- 1\right)c_{i}\left (x_{i}\left (t\right)\gt 0.5\right) $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ36"><alternatives><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mtext>post</mml:mtext><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>0.25</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t36">\begin{document}$$\displaystyle  \Delta A_{ij_{-} \text {post}\rightarrow \rm pre }= &amp; \, a_{\rm RNN}\delta \left (t\right)x_{j}\left (t\right)x_{i}\left (t- 1\right)\left (1- x_{i}\left (t- 1\right)\right)c_{i}\left (x_{i}\left (t- 1\right)\leq 0.5\right) \\ &amp;{\rm or}\,\, 0.25a_{\rm RNN}\delta \left (t\right)x_{j}\left (t\right)c_{i}\left (x_{i}\left (t- 1\right)\gt 0.5\right) $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ37"><alternatives><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mtext>post</mml:mtext><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>0.25</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t37">\begin{document}$$\displaystyle  \Delta B_{ik_{-} \text {post}\rightarrow \rm pre }= &amp; \, a_{\rm RNN}\delta \left (t\right)o_{k}\left (t\right)x_{i}\left (t- 1\right)\left (1- x_{i}\left (t- 1\right)\right)c_{i}\left (x_{i}\left (t- 1\right)\leq 0.5\right) \\ &amp;{\rm or}\,\, 0.25a_{\rm RNN}\delta \left (t\right)o_{k}\left (t\right)c_{i}\left (x_{i}\left (t- 1\right)\gt 0.5\right) $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ38"><alternatives><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mtext>post</mml:mtext><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t38">\begin{document}$$\displaystyle  A_{ij}\leftarrow A_{ij}+k_{\rm pre\rightarrow post}\Delta A_{ij_{\rm pre}\rightarrow \rm post}+\left (1- k_{\rm pre\rightarrow post}\right)\Delta A_{ij_{-} \text {post}\rightarrow \rm pre}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ39"><alternatives><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mtext>post</mml:mtext><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t39">\begin{document}$$\displaystyle  B_{ij}\leftarrow B_{ij}+k_{\rm pre\rightarrow post}\Delta B_{ij_{\rm pre}\rightarrow \rm post}+\left (1- k_{\rm pre\rightarrow post}\right)\Delta B_{ij_{-} \text {post}\rightarrow \rm pre}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$k_{\rm pre\rightarrow post}$\end{document}</tex-math></alternatives></inline-formula> is a parameter indicating the proportion of update induced by pre → post activity pairing and was set to 1 (corresponding to the original oVRNNrf-bio), 0.75, 2/3(=0.66..), 0.6, or 0.5.</p><p>The results, mean squared errors in the state values from cue to reward, are shown in Figure A2. As shown in the figure, the model performed better than the untrained control with connections shuffled from learned original oVRNNrf-bio even when the update of the RNN connections was induced by not only pre → post but also post → pre activity pairings if there was at least a modest level of asymmetry (3:2) and the number of RNN units was not small (25 or above).</p><p>#Addition of synaptic update by post → pre pairing up to a certain extent thus does not totally ruin the performance of our model. But it still causes a degradation, rather than an improvement, in terms of this performance measure, as would be expected. However, the addition of synaptic update by post → pre pairing could potentially provide the model with other features. Specifically, given that potentiation by pre → post pairing could cause self-organization of the successor representation (SR) (<xref ref-type="bibr" rid="bib38">George et al., 2023</xref>; <xref ref-type="bibr" rid="bib11">Bono et al., 2023</xref>), potentiation by post → pre pairing could potentially cause self-organization of the opposite, predecessor representation (PR) (cf., <xref ref-type="bibr" rid="bib4">Bailey and Mattar, 2022</xref>). Then, having both pre → post potentiation and post → pre potentiation could achieve a formation of state representation that has features of both SR and PR. Intriguingly, recent studies (<xref ref-type="bibr" rid="bib56">Jeong et al., 2022</xref>; <xref ref-type="bibr" rid="bib37">Garr et al., 2024</xref>; <xref ref-type="bibr" rid="bib35">Floeder et al., 2024</xref>) suggest that mesolimbic DA signals ‘adjusted net contingency for causal relations (ANCCR)’, which #combines SR and PR. Whether BTSP with both pre → post potentiation and post → pre potentiation contributes to its mechanism may be interesting to address in future studies.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104101.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Naud</surname><given-names>Richard</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Ottawa</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>In this <bold>important</bold> study, the authors model reinforcement-learning experiments using a recurrent neural network. The work examines if the detailed credit assignment necessary for back-propagation through time can be replaced with random feedback. The authors provide <bold>solid</bold> evidence that the solution is adequate within relatively simple tasks.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104101.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Can a plastic RNN serve as a basis function for learning to estimate value. In previous work this was shown to be the case, with a similar architecture to that proposed here. The learning rule in previous work was back-prop with an objective function that was the TD error function (delta) squared. Such a learning rule is non-local as the changes in weights within the RNN, and from inputs to the RNN depends on the weights from the RNN to the output, which estimates value. This is non-local, and in addition, these weights themselves change over learning. The main idea in this paper is to examine if replacing the values of these non-local changing weights, used for credit assignment, with random fixed weights can still produce similar results to those obtained with complete bp. This random feedback approach is motivated by a similar approach used for deep feed-forward neural networks.</p><p>This work shows that this random feedback in credit assignment performs well but is not as well as the precise gradient-based approach. When more constraints due to biological plausibility are imposed performance degrades. These results are consistent with previous results on random feedback.</p><p>Strengths:</p><p>The authors show that random feedback can approximate well a model trained with detailed credit assignment.</p><p>The authors simulate several experiments including some with probabilistic reward schedules and show results similar to those obtained with detailed credit assignments as well as in experiments.</p><p>The paper examines the impact of more biologically realistic learning rules and the results are still quite similar to the detailed back-prop model.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104101.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Tsurumi et al. show that recurrent neural networks can learn state and value representations in simple reinforcement learning tasks when trained with random feedback weights. The traditional method of learning for recurrent network in such tasks (backpropogation through time) requires feedback weights which are a transposed copy of the feed-forward weights, a biologically implausible assumption. This manuscript builds on previous work regarding &quot;random feedback alignment&quot; and &quot;value-RNNs&quot;, and extends them to a reinforcement learning context. The authors also demonstrate that certain non-negative constraints can enforce a &quot;loose alignment&quot; of feedback weights. The author's results suggest that random feedback may be a powerful tool of learning in biological networks, even in reinforcement learning tasks.</p><p>Strengths:</p><p>The authors describe well the issues regarding biologically plausible learning in recurrent networks and in reinforcement learning tasks. They take care to propose networks which might be implemented in biological systems and compare their proposed learning rules to those already existing in literature. Further, they use small networks on relatively simple tasks, which allows for easier intuition into the learning dynamics.</p><p>Weaknesses:</p><p>The principles discovered by the authors in these smaller networks are not applied to larger networks or more complicated tasks with long temporal delays (&gt;100 timesteps), so it remains unclear to what degree these methods can scale or can be used more generally.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104101.4.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The paper studies learning rules in a simple sigmoidal recurrent neural network setting. The recurrent network has a single layer of 10 to 40 units. It is first confirmed that feedback alignment (FA) can learn a value function in this setting. Then so-called bio-plausible constraints are added: (1) when value weights (readout) is non-negative, (2) when the activity is non-negative (normal sigmoid rather than downscaled between -0.5 and 0.5), (3) when the feedback weights are non-negative, (4) when the learning rule is revised to be monotic: the weights are not downregulated. In the simple task considered all four biological features do not appear to impair totally the learning.</p><p>Strengths:</p><p>(1) The learning rules are implemented in a low-level fashion of the form: (pre-synaptic-activity) x (post-synaptic-activity) x feedback x RPE. Which is therefore interpretable in terms of measurable quantities in the wet-lab.</p><p>(2) I find that non-negative FA (FA with non negative c and w) is the most valuable theoretical insight of this paper: I understand why the alignment between w and c is automatically better at initialization.</p><p>(3) The task choice is relevant, since it connects with experimental settings of reward conditioning with possible plasticity measurements.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104101.4.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tsurumi</surname><given-names>Takayuki</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Tokyo</institution><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff></contrib><contrib contrib-type="author"><name><surname>Kato</surname><given-names>Ayaka</given-names></name><role specific-use="author">Author</role><aff><institution>Icahn School of Medicine at Mount Sinai</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Kumar</surname><given-names>Arvind</given-names></name><role specific-use="author">Author</role><aff><institution>KTH Royal Institute of Technology</institution><addr-line><named-content content-type="city">Stockholm</named-content></addr-line><country>Sweden</country></aff></contrib><contrib contrib-type="author"><name><surname>Morita</surname><given-names>Kenji</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Tokyo</institution><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p></disp-quote><p>We thank the reviewer for the positive feedback on the work. The reviewer has raised two weaknesses and in the following we discuss how those can be addressed.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The impact of the article is limited by using a network with discrete time- steps, and only a small number of time steps from stimulus to reward. They assume that each time step is on the order of hundreds of ms. They justify this by pointing to some slow intrinsic mechanisms, but they do not implement these slow mechanisms is a network with short time steps, instead they assume without demonstration that these could work as suggested. This is a reasonable first approximation, but its validity should be explicitly tested.</p></disp-quote><p>Our goal here was to give a proof of concept that online random feedback is sufficient to train an RNN to estimate value. Indeed, it is important to show that the idea works in a model where the slow mechanisms are explicitly implemented. However, this is a non-trivial task and desired to be addressed in future works.</p><disp-quote content-type="editor-comment"><p>As the delay between cue and reward increases the performance decreases. This is not surprising given the proposed mechanism, but is still a limitation, especially given that we do not really know what a is the reasonable value of a single time step.</p></disp-quote><p>In reply to this comment and the other reviewer's related comment, we have conducted two sets of additional simulations, one for examining incorporation of eligibility traces, and the other for considering (though not mechanistically implementing) behavioral time-scale synaptic plasticity (BTSP). We have added their results to the revised manuscript as Appendix. We think that the results addressed this point to some extent while how longer cue-reward delay can be learnt by elaboration of the model remains as a future issue.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p></disp-quote><p>We thank the reviewer for the positive feedback on the work. The reviewer gave comments on our revisions, and here we discuss how those can be addressed.</p><disp-quote content-type="editor-comment"><p>Comments on revisions: I would still want to see how well the network learns tasks with longer time delays (on the order of 100 or even 1000 timesteps). Previous work has shown that random feedback struggles to encode longer timescales (see Murray 2019, Figure 2), so I would be interested to see how that translates to the RL context in your model.</p></disp-quote><p>We would like to note that in Murray et al 2019 the random feedback per se appeared not to be primarily responsible for the difficulty in encoding longer timesclaes. In the Figure 2d (Murray 2019), the author compared his RFLO (random feedback local online) and BPTT with two intermediate algorithms, which incorporated either one of the two approximations made in RFLO: (i) random feedback instead of symmetric feedback, and (ii) omittance of non-local effect (i.e., dependence of the derivative of the loss with respect to a given weight on the other weights). The performance difference between RFLO and BPTT was actually mostly explained by (ii), as the author mentioned &quot;The results show that the local approximation is essentially fully responsible for the performance difference between RFLO and BPTT, while there is no significant loss in performance due to the random feedback alone. (Line 6-8, page 7 of Murray, 2019, eLife)&quot;.</p><p>Meanwhile, regarding the difference in the performance of the model with random feedback vs the model with symmetric feedback in our settings, actually it appeared (already) in the case with 6 time-steps or less (the biologically constrained model with random feedback performed worse: Fig. 6J, left).</p><p>In practice, our model, either with random or symmetric feedback, would not be able to learn the cases with very long delays. This is indeed a limitation of our model. However, our model is critically different from the model of Murray 2019 in that we use RL rather than supervised learning and we use a scalar bootstrapped (TD) reward-prediction-error rather than the true output error. We would think that these differences may be major reasons for the limited learning ability of our model.</p><p>Regarding the feasibility of the model when tasks involve longer time delays: Indeed this is a problem and the other reviewers have also raised the same point. Our model can be extended by incorporating either a kind of eligibility trace (similar one to those contained in RFLO and e-prop) or behavioral time-scale synaptic plasticity (BTSP), and we have added the results of simulations incorporating each to the revised manuscript as Appendix. But how longer cue-reward delay can be learnt by elaboration of the model remains as a future issue.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Comments on revisions: Thank you for addressing all my comments in your reply.</p></disp-quote><p>We are happy to learn that all concerns raised by the reviewer in the previous round were addressed adequately. We agree with the reviewer that there are several ways the work can be improved.</p><p>The various points raised by the reviewers at weaknesses are desired to be taken up in future works.</p></body></sub-article></article>