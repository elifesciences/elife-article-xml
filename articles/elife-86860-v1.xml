<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86860</article-id><article-id pub-id-type="doi">10.7554/eLife.86860</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A chromatic feature detector in the retina signals visual context changes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Höfling</surname><given-names>Larissa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2459-0706</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Szatko</surname><given-names>Klaudia P</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Behrens</surname><given-names>Christian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3623-352X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Yuyao</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Qiu</surname><given-names>Yongrong</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Klindt</surname><given-names>David Alexander</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="pa3">§</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Jessen</surname><given-names>Zachary</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Schwartz</surname><given-names>Gregory W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8909-4397</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Bethge</surname><given-names>Matthias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6417-7812</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Berens</surname><given-names>Philipp</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0199-4727</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Franke</surname><given-names>Katrin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8649-4835</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2392-5105</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Euler</surname><given-names>Thomas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4567-6966</contrib-id><email>thomas.euler@cin.uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Institute for Ophthalmic Research, University of Tübingen</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Centre for Integrative Neuroscience, University of Tübingen</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>SLAC National Accelerator Laboratory, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Menlo Park</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Feinberg School of Medicine, Department of Ophthalmology, Northwestern University</institution></institution-wrap><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Tübingen AI Center, University of Tübingen</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution>Hertie Institute for AI in Brain Health</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Institute of Computer Science and Campus Institute Data Science, University of Göttingen</institution></institution-wrap><addr-line><named-content content-type="city">Göttingen</named-content></addr-line><country>Germany</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0087djs12</institution-id><institution>Max Planck Institute for Dynamics and Self-Organization</institution></institution-wrap><addr-line><named-content content-type="city">Göttingen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Smith</surname><given-names>Lois EH</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dvg7y05</institution-id><institution>Boston Children's Hospital</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Department of Ophthalmology, Byers Eye Institute, Stanford University School of Medicine, Stanford, United States</p></fn><fn fn-type="present-address" id="pa3"><label>§</label><p>Cold Spring Harbor Laboratory, Cold Spring Harbor, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>10</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e86860</elocation-id><history><date date-type="received" iso-8601-date="2023-02-09"><day>09</day><month>02</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2024-08-25"><day>25</day><month>08</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-12-01"><day>01</day><month>12</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.30.518492"/></event></pub-history><permissions><copyright-statement>© 2024, Höfling et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Höfling et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86860-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86860-figures-v1.pdf"/><abstract><p>The retina transforms patterns of light into visual feature representations supporting behaviour. These representations are distributed across various types of retinal ganglion cells (RGCs), whose spatial and temporal tuning properties have been studied extensively in many model organisms, including the mouse. However, it has been difficult to link the potentially nonlinear retinal transformations of natural visual inputs to specific ethological purposes. Here, we discover a nonlinear selectivity to chromatic contrast in an RGC type that allows the detection of changes in visual context. We trained a convolutional neural network (CNN) model on large-scale functional recordings of RGC responses to natural mouse movies, and then used this model to search in silico for stimuli that maximally excite distinct types of RGCs. This procedure predicted centre colour opponency in transient suppressed-by-contrast (tSbC) RGCs, a cell type whose function is being debated. We confirmed experimentally that these cells indeed responded very selectively to Green-OFF, UV-ON contrasts. This type of chromatic contrast was characteristic of transitions from ground to sky in the visual scene, as might be elicited by head or eye movements across the horizon. Because tSbC cells performed best among all RGC types at reliably detecting these transitions, we suggest a role for this RGC type in providing contextual information (i.e. sky or ground) necessary for the selection of appropriate behavioural responses to other stimuli, such as looming objects. Our work showcases how a combination of experiments with natural stimuli and computational modelling allows discovering novel types of stimulus selectivity and identifying their potential ethological relevance.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>retina</kwd><kwd>computational modelling</kwd><kwd>visual ecology</kwd><kwd>convolutional neural networks</kwd><kwd>2P imaging</kwd><kwd>natural stimuli</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CRC 1233 project number 276693517</award-id><principal-award-recipient><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Heisenberg Professorship BE5601/8-1</award-id><principal-award-recipient><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>EXC 2064 390727645</award-id><principal-award-recipient><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CRC 1456 project number 432680300</award-id><principal-award-recipient><name><surname>Ecker</surname><given-names>Alexander S</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>FKZ 01IS18039A</award-id><principal-award-recipient><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NEI EY031029</award-id><principal-award-recipient><name><surname>Schwartz</surname><given-names>Gregory W</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NEI F30EY031565</award-id><principal-award-recipient><name><surname>Jessen</surname><given-names>Zachary</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>grant agreement No. 101041669</award-id><principal-award-recipient><name><surname>Ecker</surname><given-names>Alexander S</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NEI EY031329</award-id><principal-award-recipient><name><surname>Schwartz</surname><given-names>Gregory W</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIGMS T32GM008152</award-id><principal-award-recipient><name><surname>Jessen</surname><given-names>Zachary</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A modelling-based analysis reveals a novel type of selectivity for chromatic contrast in a mouse retinal ganglion cell type, and experimental evidence shows that this feature makes this cell type well suited for detecting behaviourally relevant changes in visual context.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Sensory systems evolved to generate representations of an animal’s natural environment useful for survival and procreation (<xref ref-type="bibr" rid="bib54">Lettvin et al., 1959</xref>). These environments are complex and high dimensional, and different features are relevant for different species (reviewed in <xref ref-type="bibr" rid="bib8">Baden et al., 2020</xref>). As a consequence, the representations are adapted to an animal’s needs: features of the world relevant for the animal are represented with enhanced precision, whereas less important features are discarded. Sensory processing is thus best understood within the context of the environment an animal evolved in and that it interacts with (reviewed in <xref ref-type="bibr" rid="bib96">Turner et al., 2019</xref>; <xref ref-type="bibr" rid="bib84">Simoncelli and Olshausen, 2001</xref>).</p><p>The visual system is well suited for studying sensory processing, as the first features are already extracted at its experimentally well-accessible front-end, the retina (reviewed in <xref ref-type="bibr" rid="bib48">Kerschensteiner, 2022</xref>; <xref ref-type="bibr" rid="bib8">Baden et al., 2020</xref>). In the mouse, this tissue gives rise to around 40 parallel channels that detect different features (<xref ref-type="bibr" rid="bib35">Goetz et al., 2022</xref>; <xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Bae et al., 2018</xref>; <xref ref-type="bibr" rid="bib74">Rheaume et al., 2018</xref>), represented by different types of retinal ganglion cells (RGCs), whose axons send information to numerous visual centres in the brain (<xref ref-type="bibr" rid="bib61">Martersteck et al., 2017</xref>). Some of these channels encode basic features, such as luminance changes and motion, that are only combined in downstream areas to support a range of behaviours such as cricket hunting in mice (<xref ref-type="bibr" rid="bib45">Johnson et al., 2021</xref>). Other channels directly extract specific features from natural scenes necessary for specific behaviours. For instance, transient OFF α cells trigger freezing or escape behaviour in response to looming stimuli (<xref ref-type="bibr" rid="bib67">Münch et al., 2009</xref>; <xref ref-type="bibr" rid="bib103">Yilmaz and Meister, 2013</xref>; <xref ref-type="bibr" rid="bib50">Kim et al., 2020</xref>; <xref ref-type="bibr" rid="bib99">Wang et al., 2021</xref>).</p><p>For many RGC types, however, we lack an understanding of the features they encode and how these link to behaviour (<xref ref-type="bibr" rid="bib78">Schwartz and Swygart, 2020</xref>). One reason for this is that the synthetic stimuli commonly used to study retinal processing fail to drive retinal circuits ‘properly’ and, hence, cannot uncover critical response properties triggered in natural environments. This was recently illustrated at the example of spatial nonlinear processing, which was found to be more complex for natural scenes than for simpler synthetic stimuli (<xref ref-type="bibr" rid="bib46">Karamanlis and Gollisch, 2021</xref>). Such nonlinearities, which are crucial for the encoding of natural stimuli, cannot be captured by linear-nonlinear (LN) models of retinal processing, and several alternative methods have been proposed for the analysis of natural stimulus responses (reviewed in <xref ref-type="bibr" rid="bib83">Sharpee, 2013</xref>).</p><p>One approach to modelling nonlinear visual processing is to train a convolutional neural network (CNN) to predict neuronal responses. This approach has gained popularity in recent years, both in the retina (<xref ref-type="bibr" rid="bib63">McIntosh et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Maheswaranathan et al., 2023</xref>; <xref ref-type="bibr" rid="bib90">Tanaka et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Batty et al., 2017</xref>) and in higher visual areas (<xref ref-type="bibr" rid="bib102">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="bib97">Ustyuzhaninov et al., 2024</xref>). The resulting models, also referred to as ‘digital twins’, offer a number of analysis techniques that have been used to investigate, for example, the contributions of different interneurons to a cell’s response (<xref ref-type="bibr" rid="bib58">Maheswaranathan et al., 2023</xref>), or the effects of stimulus context (<xref ref-type="bibr" rid="bib32">Fu et al., 2024</xref>; <xref ref-type="bibr" rid="bib36">Goldin et al., 2022</xref>) and behavioural state (<xref ref-type="bibr" rid="bib31">Franke et al., 2022</xref>) on neural coding. In particular, feature visualisations (<xref ref-type="bibr" rid="bib69">Olah et al., 2017</xref>) can be used to quickly generate stimuli that would maximally excite the modelled neurons (<xref ref-type="bibr" rid="bib98">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Bashivan et al., 2019</xref>), which in turn can serve as interpretable short-hand descriptions of nonlinear neuronal selectivities. In visual cortex, the resulting <italic>maximally exciting inputs</italic> (MEIs) revealed more complex and diverse neuronal selectivities than were expected based on previous results obtained with synthetic stimuli and linear methods (<xref ref-type="bibr" rid="bib98">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Bashivan et al., 2019</xref>).</p><p>Here, we combined the power of CNN-based modelling with large-scale recordings from RGCs to investigate colour processing in the mouse retina under natural stimulus conditions. Colour is a salient feature in nature, and the mouse visual system dedicates intricate circuitry to the processing of chromatic information (<xref ref-type="bibr" rid="bib89">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="bib44">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="bib5">Baden et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="bib66">Mouland et al., 2021</xref>). Studies using simple synthetic stimuli have revealed nonlinear and centre-surround (i.e. spatial) interactions between colour channels, but it is not clear how these are engaged in retinal processing of natural, temporally varying environments. Since mouse photoreceptors are sensitive to green and UV light (<xref ref-type="bibr" rid="bib42">Jacobs et al., 2004</xref>), we recorded RGC responses to stimuli capturing the chromatic composition of natural mouse environments in these two chromatic channels. A model-guided search for MEIs in chromatic stimulus space predicted a novel type of chromatic tuning in tSbC RGCs, a type whose function is being debated (<xref ref-type="bibr" rid="bib59">Mani and Schwartz, 2017</xref>; <xref ref-type="bibr" rid="bib92">Tien et al., 2015</xref>; <xref ref-type="bibr" rid="bib94">Tien et al., 2022</xref>).</p><p>A detailed in silico characterisation followed up by experimental validation ex-vivo confirmed this cell type’s pronounced and unique selectivity for dynamic full-field changes from green-dominated to UV-dominated scenes, a type of visual input that matches the scene statistics of transitions across the horizon (<xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Abballe and Asari, 2022</xref>; <xref ref-type="bibr" rid="bib39">Gupta et al., 2022</xref>). We therefore suggest a role for tSbC RGCs in detecting behaviourally relevant changes in visual context, such as a transitions from ground (i.e. below the horizon) to sky (i.e. above the horizon).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Here, we investigated colour processing in the mouse retina under natural stimulus conditions. To this end, we trained a CNN model on RGC responses to a movie covering both achromatic and chromatic contrasts occurring naturally in the mouse environment, and then performed a model-guided search for stimuli that maximise the responses of RGCs.</p><sec id="s2-1"><title>Mouse RGCs display diverse responses to a natural movie stimulus</title><p>Using two-photon population Ca<sup>2+</sup> imaging, we recorded responses from 8388 cells (in 72 recording fields across 32 retinae) in the ganglion cell layer (GCL) of the isolated mouse retina (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) to a range of visual stimuli. Since complex interactions between colour channels have been mostly reported in the ventral retina and opsin-transitional zone, we focused our recordings on these regions (<xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Khani and Gollisch, 2021</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Mouse retinal ganglion cells (RGCs) display diverse responses to a natural movie stimulus.</title><p>(<bold>a</bold>) Illustration of a flat-mounted retina, with recording fields (white circles) and stimulus area centred on the red recording field indicated (cross marks optic disc; d, dorsal; v, ventral; t, temporal; n, nasal). (<bold>b</bold>) Natural movie stimulus structure (top) and example frames (bottom). The stimulus consisted of 5 s clips taken from UV-green footage recorded outside (<xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>), with 3 repeats of a 5-clip test sequence (highlighted in grey) and a 108-clip training sequence (see Methods). (<bold>c</bold>) Representative recording field (bottom; marked by red square in (<bold>a</bold>)) showing somata of ganglion cell layer (GCL) cells loaded with Ca<sup>2+</sup> indicator OGB-1. (<bold>d</bold>) Ca<sup>2+</sup> responses of exemplary RGCs (indicated by circles in (<bold>c</bold>)) to chirp (left), moving bar (centre), and natural movie (right) stimulus. (<bold>e</bold>) Same recording field as in (<bold>c</bold>) but with cells colour-coded by functional RGC group (left; see Methods and <xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>) and group responses (coloured, mean ± SD across cells; trace of example cells in (<bold>d</bold>) overlaid in black).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Additional information about the dataset, model performance, and response quality filtering pipeline.</title><p>(<bold>a</bold>) Distribution across cell types for this dataset, and for the dataset described in <xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>, which was the basis for our classifier (<xref ref-type="bibr" rid="bib73">Qiu et al., 2023</xref>). (<bold>b</bold>) Mean ± SD of model performance, evaluated as correlation between model prediction and retinal ganglion cell (RGC) response on the 25 s long test sequence, averaged across three repetitions of the test sequence, for each cell type. (<bold>c</bold>) Response quality, RGC group assignment, and model performance filtering pipeline showing the consecutive steps and the fraction of cells remaining after each step. Black bars and numbers indicate cells from all experiments (i.e. all RGCs for which we recorded chirp, moving bar [MB], and movie responses), red bars and numbers indicate the subset of cells recorded in the maximally exciting input (MEI) validation experiments (i.e. those RGCs for which we <italic>additionally</italic> recorded MEI stimuli responses). Dotted bars indicate the number of cells before the current filtering step. The filtering steps were as follows: (1) Keep only cells that pass the chirp OR MB quality criterion <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>QI</mml:mtext><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.6</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mtext>QI</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.35</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. (2) Keep only cells that the classifier assigns to a group with confidence ≥0.25. (3) Keep only cells assigned to a ganglion cell group (groups 1–32; groups 33–46 are amacrine cell groups). (4) Keep only cells with sufficiently high model performance <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> All cells passing steps 1–3 were included in the horizon detection analysis (<xref ref-type="fig" rid="fig7">Figure 7</xref>); all cells passing steps 1–4 were included in the MEI analysis (<xref ref-type="fig" rid="fig3">Figure 3</xref>); the ‘red’ cells passing steps 1–4 were included in the MEI validation analysis (<xref ref-type="fig" rid="fig4">Figure 4</xref>). All quality criteria are described in the Methods section.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig1-figsupp1-v1.tif"/></fig></fig-group><p>The stimuli included two achromatic synthetic stimuli – a contrast and frequency modulation (‘chirp’ stimulus) and a bright-on-dark bar moving in eight directions (‘moving bar’, MB) – to identify the functional cell type (see below), as well as a dichromatic natural movie (<xref ref-type="fig" rid="fig1">Figure 1b–d</xref>). The latter was composed of footage recorded outside in the field using a camera that captured the spectral bands (UV and green; <xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>) to which mouse photoreceptors are sensitive (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>360</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>510</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> nm for S- and M-cones, respectively; <xref ref-type="bibr" rid="bib42">Jacobs et al., 2004</xref>). We used 113 different movie clips, each lasting 5 s, that were displayed in pseudo-random order. Five of these constituted the test set and were repeated three times: at the beginning, in the middle, and at the end of the movie presentation, thereby allowing to assess the reliability of neuronal responses across the recording (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, top).</p><p>The responses elicited by the synthetic stimuli and the natural movie were diverse, displaying ON (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, rows 4–9), ON-OFF (row 3), and OFF (rows 1 and 2), as well as sustained and transient characteristics (e.g. rows 8 and 4, respectively). Some responses were suppressed by temporal contrast (generally, rows 10, 11; at high contrast and frequency, row 9). A total of 6984 GCL cells passed our response quality criteria (see Methods); 3527 cells could be assigned to 1 of 32 previously characterised functional RGC groups (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>) based on their responses to the chirp and MB stimuli using our recently developed classifier (<xref ref-type="fig" rid="fig1">Figure 1e</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>; <xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>). Cells assigned to any of groups 33–46 were considered displaced amacrine cells and were not analysed in this study (for detailed filtering pipeline, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>).</p></sec><sec id="s2-2"><title>CNN model captures diverse tuning of RGC groups and predicts MEIs</title><p>We trained a CNN model on the RGCs’ movie responses (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) and evaluated model performance as the correlation between predicted and trial-averaged measured test responses, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>C</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo separator="true">,</mml:mo><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mo form="postfix" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). This metric can be interpreted as an estimate of the achieved fraction of the maximally achievable correlation (see Methods). The mean correlation per RGC group ranged from 0.32 (G<sub>14</sub>) to 0.79 (G<sub>24</sub>) (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) and reached an average of 0.48 (for all <italic>N</italic>=3527 cells passing filtering steps 1–3, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). We also tested the performance of our nonlinear model against a linearised version (see Methods; equivalent to an LN model, and from here on ‘LN model’) and found that the nonlinear CNN model achieved a higher test set correlation for all RGC groups (average correlation LN model: 0.38; G<sub>14</sub>: 0.2, G<sub>24</sub>: 0.65, <xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Convolutional neural network (CNN) model captures diverse tuning of retinal ganglion cell (RGC) groups and predicts maximally exciting inputs (MEIs).</title><p>(<bold>a</bold>) Illustration of the CNN model and its output. The model takes natural movie clips as input (1), performs 3D convolutions with space-time separable filters (2) followed by a nonlinear activation function (ELU; 3) in two consecutive layers (2–4) within its core, and feeds the output of its core into a per-neuron readout. For each RGC, the readout convolves the feature maps with a learned RF modelled as a 2D Gaussian (5), and finally feeds a weighted sum of the resulting vector through a softplus nonlinearity (6) to yield the firing rate prediction for that RGC (7). Numbers indicate averaged single-trial test set correlation between predicted (red) and recorded (black) responses. (<bold>b</bold>) Test set correlation between model prediction and neural response (averaged across three repetitions) as a function of response reliability (see Methods) for <italic>N</italic>=3527 RGCs. Coloured dots correspond to example cells shown in <xref ref-type="fig" rid="fig1">Figure 1c–e</xref>. Dots in darker grey correspond to the <italic>N</italic>=1947 RGCs that passed the model test correlation and movie response quality criterion (see Methods and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). (<bold>c</bold>) Test set correlation (as in (<bold>b</bold>)) of CNN model vs. test set correlation of an LN model (for details, see Methods). Coloured dots correspond to means of RGC groups 1–32 (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>). Dark and light grey dots as in (<bold>b</bold>). (<bold>d</bold>) Illustration of model-guided search for MEIs. The trained model captures neural tuning to stimulus features (far left; heat map illustrates landscape of neural tuning to stimulus features). Starting from a randomly initialised input (second from left; a 3D tensor in space and time; only one colour channel illustrated here), the model follows the gradient along the tuning surface (far left) to iteratively update the input until it arrives at the stimulus (bottom right) that maximises the model neuron’s activation within an optimisation time window (0.66 s, grey box, top right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig2-v1.tif"/></fig><p>Next, we wanted to leverage our nonlinear CNN model to search for potentially nonlinear stimulus selectivities of mouse RGC groups. Towards this goal, we aimed to identify stimuli that optimally drive RGCs of different groups. For linear systems, the optimal stimulus is equivalent to the linear filter and can be identified with classical approaches such as reverse correlation (<xref ref-type="bibr" rid="bib20">Chichilnisky, 2001</xref>). However, since both the RGCs and the CNN model were nonlinear, a different approach was necessary. Other recent modelling studies in the visual system have leveraged CNN models to predict static MEIs for neurons in monkey visual area V4 (<xref ref-type="bibr" rid="bib10">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="bib101">Willeke et al., 2023</xref>) and mouse visual area V1 (<xref ref-type="bibr" rid="bib98">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Franke et al., 2022</xref>). We adopted this approach to predict dynamic (i.e. time-varying) MEIs for mouse RGCs. We used gradient ascent on a randomly initialised, contrast- and range-constrained input to find the stimulus that maximised the mean activation of a given model neuron within a short time window (0.66 s; see Methods; <xref ref-type="fig" rid="fig2">Figure 2d</xref>).</p><p>It is important to note that MEIs should not be confused with, or interpreted as, the linear filters derived from classical approaches such as reverse correlation (<xref ref-type="bibr" rid="bib20">Chichilnisky, 2001</xref>; <xref ref-type="bibr" rid="bib77">Schwartz et al., 2006</xref>). While both MEIs and linear filters offer simplified views of a neuron’s nonlinear response properties, they emphasise different aspects. The linear filter is optimised to provide the best possible linear approximation of the response function, identifying the stimulus direction to which the cell is most sensitive on average across the stimulus ensemble. In contrast, the MEI maximises the neuron’s response by finding the single stimulus that activates the cell most strongly. Consequently, MEIs can differ significantly from linear filters, often exhibiting greater complexity and higher frequency components (<xref ref-type="bibr" rid="bib98">Walker et al., 2019</xref>).</p></sec><sec id="s2-3"><title>MEIs reflect known functional RGC group properties</title><p>The resulting MEIs were short, dichromatic movie clips; their spatial, temporal, and chromatic properties and interactions thereof are best appreciated in lower-dimensional visualisations (<xref ref-type="fig" rid="fig3">Figure 3a–c</xref>; more example MEIs in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Spatial, temporal, and chromatic properties of maximally exciting inputs (MEIs) differ between retinal ganglion cell (RGC) groups.</title><p>(<bold>a</bold>) Spatial component of three example MEIs for green (top), UV (middle), and overlay (bottom). Solid and dashed circles indicate MEI centre and surround fit, respectively. For display, spatial components <inline-formula><mml:math id="inf5"><mml:mi>s</mml:mi></mml:math></inline-formula> in the two channels were re-scaled to a similar range and displayed on a common grey-scale map ranging from black for <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to white for <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., symmetric about 0 (grey). (<bold>b</bold>) Spatiotemporal (<italic>y</italic>–<italic>t</italic>) plot for the three example MEIs (from (<bold>a</bold>)) at a central vertical slice for green (top), UV (middle), and overlay (bottom). Grey-scale map analogous to (<bold>a</bold>). (<bold>c</bold>) Trajectories through colour space over time for the centre of the three MEIs. Trajectories start at the origin (grey level); direction of progress indicated by arrow heads. Bottom right: Bounding boxes of the respective trajectory plots. (<bold>d</bold>) Calculation of MEI centre size, defined as <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula>+<inline-formula><mml:math id="inf9"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>, with <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> the s.d. in horizontal and vertical direction, respectively, of the difference-of-Gaussians (DoG) fit to the MEI. (<bold>e</bold>) Calculation of MEI temporal frequency: Temporal components are transformed using fast Fourier transform, and MEI frequency is defined as the amplitude-weighted average frequency of the Fourier-transformed temporal component. (<bold>f</bold>) Calculation of centre contrast, which is defined as the difference in intensity at the last two peaks (indicated by <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, respectively, in (<bold>c</bold>)). For the example cell (orange markers and lines), green intensity decreases, resulting in OFF contrast, and UV intensity increases, resulting in ON contrast. (<bold>g</bold>) Distribution of green and UV MEI centre sizes across <italic>N</italic>=1613 cells (example MEIs from (<bold>a–c</bold>) indicated by arrows; symbols as shown on top of (<bold>a</bold>)). 95% of MEIs were within an angle of ±8° of the diagonal (solid and dashed lines); MEIs outside of this range are coloured by cell type. (<bold>h</bold>) As (<bold>g</bold>) but for distribution of green and UV MEI temporal frequency. 95% of MEIs were within an angle of ±11.4° of the diagonal (solid and dashed lines). (<bold>i</bold>) As (<bold>g</bold>) but for distribution of green and UV MEI centre contrast. MEI contrast is shifted away from the diagonal (dashed line) towards UV by an angle of 33.2° due to the dominance of UV-sensitive S-opsin in the ventral retina. MEIs at an angle &gt;45° occupy the upper left, colour-opponent (UV<sup>ON</sup>-green<sup>OFF</sup>) quadrant. (<bold>j, k</bold>) Fraction of MEIs per cell type that lie outside the angle about the diagonal containing 95% of MEIs for centre size and temporal frequency. Broad RGC response types indicated as in <xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>. (<bold>l</bold>) Fraction of MEIs per cell type in the upper-left, colour-opponent quadrant for contrast.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Example maximally exciting inputs (MEIs) for example cell types.</title><p>Rows in each panel as in <xref ref-type="fig" rid="fig4">Figure 4a</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Illustration of how different time windows for optimisation affect maximally exciting input (MEI) temporal properties.</title><p>(<bold>a</bold>) MEIs (bottom panels) and model neuron responses (top panels) for a short optimisation window of 2 frames (≈0.066s, indicated by grey shaded area). The top row shows the responses of a more transient retinal ganglion cell (RGC) to its own MEI (left stimulus) and to the MEI of a more sustained RGC (right stimulus). The bottom row shows the responses of the more sustained RGC to its own MEI (right stimulus) and to the MEI of the more transient RGC (right stimulus). (<bold>b</bold>) MEIs (bottom panels) and model neuron responses (top panels) for a longer optimisation window of 20 frames (≈0.66s, indicated by grey shaded area) as used throughout the paper. The top row shows the responses of a more transient RGC to its own MEI (left stimulus) and to the MEI of a more sustained RGC (right stimulus). The bottom row shows the responses of the more sustained RGC to its own MEI (right stimulus) and to the MEI of the more transient RGC (right stimulus). Same cells as in (<bold>a</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig3-figsupp2-v1.tif"/></fig></fig-group><p>To analyse the MEIs in terms of these properties, we decomposed them into their spatial and temporal components, separately for green and UV, and parameterised the spatial component as a difference-of-Gaussians (DoG) (<xref ref-type="bibr" rid="bib39">Gupta et al., 2022</xref>) (<italic>N</italic>=1613 out of 1947, see Methods). We then located MEIs along the axes in stimulus space corresponding to three properties: centre size, mean temporal frequency, and centre contrast, separately for green and UV (<xref ref-type="fig" rid="fig3">Figure 3d–f</xref>). These MEI properties reflect RGC response properties classically probed with synthetic stimuli, such as spots of different sizes (<xref ref-type="bibr" rid="bib35">Goetz et al., 2022</xref>), temporal frequency modulations (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>), and stimuli of varying chromatic contrast (<xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Khani and Gollisch, 2021</xref>). Using the MEI approach, we were able to reproduce known properties of RGC groups (<xref ref-type="fig" rid="fig3">Figure 3g–i</xref>). For example, sustained ON <inline-formula><mml:math id="inf14"><mml:mi>α</mml:mi></mml:math></inline-formula> RGCs (G<sub>24</sub>), which are known to prefer large stimuli (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Mani and Schwartz, 2017</xref>), had MEIs with large centres (G<sub>24</sub>, <italic>N</italic>=20 cells: green centre size, mean ± SD: 195 ±82 μm; UV centre size 178 ±45 μm; average across all RGC groups: green 148 ±42 μm, UV 141 ±42 μm; see <xref ref-type="fig" rid="fig3">Figure 3g</xref>).</p><p>The MEI’s temporal frequency relates to the temporal frequency preference of an RGC: MEIs of G<sub>20</sub> and G<sub>21</sub>, termed ON high frequency and ON low frequency (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>), had high and low average temporal frequency, respectively (G<sub>20</sub>, <italic>N</italic>=40 cells, green, mean ± SD: 2.71 ±0.16 Hz, UV 2.86 ±0.22 Hz; G<sub>21</sub>, <italic>N</italic>=50 cells, green, mean ± SD: 2.32 ±0.63 Hz, UV 1.98 ± 0.5 Hz; see <xref ref-type="fig" rid="fig3">Figure 3h</xref>). Some MEIs exhibit fast oscillations (<xref ref-type="fig" rid="fig3">Figure 3e</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This is not an artefact but rather a consequence of optimising a stimulus to maximise activity over a 0.66 s time window (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). To maximise the response of a transient RGC over several hundred milliseconds, it has to be stimulated repetitively, hence the oscillations in the MEI. Maximising the response over a shorter time period results in MEIs without oscillations (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>Finally, the contrast of an MEI reflects what is traditionally called a cell’s ON vs. OFF preference: MEIs of ON and OFF RGCs had positive and negative contrasts, respectively (<xref ref-type="fig" rid="fig3">Figure 3i</xref>). An ON-OFF preference can be interpreted as a tuning map with two optima – one in the OFF- and one in the ON-contrast regime. For an ON-OFF cell, there are hence two stimuli that are approximately equally effective at eliciting responses from that cell. Consequently, for the ON-OFF RGC groups, optimisation resulted in MEIs with ON or OFF contrast, depending on the relative strengths of the two optima and on the initial conditions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, G<sub>10</sub>, and see Discussion).</p><p>MEIs were also largely consistent within functional RGC groups (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Where this was not the case, the heterogeneity of MEIs could be attributed to a known heterogeneity of cells within that group. For example, MEIs of G<sub>31</sub> RGCs were diverse (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), and the cells that were originally grouped to form G<sub>31</sub> probably spanned several distinct types, as suggested by the group’s unusually high coverage factor (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>). Together, these results provided strong evidence that RGCs grouped based on responses to synthetic stimuli (chirp and MB) also form functional groups in natural movie response space.</p></sec><sec id="s2-4"><title>CNN model predicts centre colour opponency in RGC group G<sub>28</sub></title><p>Our goal was to explore chromatic tuning of RGCs and to identify novel stimulus selectivities related to chromatic contrast. Therefore, we specifically focused on regions in stimulus space where a given stimulus property differs for green and UV. For centre size and temporal frequency, we asked which RGC groups contributed to the MEIs outside of the 95th percentile around the diagonal (<xref ref-type="fig" rid="fig3">Figure 3g, h, j, and k</xref>). These 5% MEIs furthest away from the diagonal were almost exclusively contributed by ON cells; and among these, more so by slow than by fast ON cells.</p><p>MEI contrast needed to be analysed differently than size and temporal frequency for two reasons. First, due to the dominance of UV-sensitive S-opsin in the ventral retina (<xref ref-type="bibr" rid="bib89">Szél et al., 1992</xref>), stimuli in the UV channel were much more effective at eliciting RGC responses. As a result, the contrast of most MEIs is strongly shifted towards UV (<xref ref-type="fig" rid="fig3">Figure 3i</xref>). Second, contrast in green and UV can not only vary along positive valued axes (as is the case for size and temporal frequency), but can also take on opposite signs, resulting in colour-opponent stimuli. Whereas most MEIs had the same contrast polarity in both colour channels (i.e. both ON or OFF, <xref ref-type="fig" rid="fig3">Figure 3c</xref>, blue and turquoise trajectories), some MEIs had opposing contrast polarities in UV and green (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, orange trajectory, and <xref ref-type="fig" rid="fig3">Figure 3i</xref>, upper left quadrant). Thus, for contrast, we asked which RGC groups contributed to colour-opponent MEIs (i.e. MEIs in the colour-opponent, upper left or lower right quadrant in <xref ref-type="fig" rid="fig3">Figure 3i</xref>). Again, slow ON RGCs made up most of the cells with colour-opponent MEIs. Here, G<sub>28</sub> stood out: 66% (24/36) of all cells of this group had colour-opponent MEIs (UV<sup>ON</sup>-green<sup>OFF</sup>), followed by G<sub>27</sub> with 42% colour-opponent MEIs.</p><p>The colour opponency we found in G<sub>28</sub> was not centre-surround, as described before in mice (<xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>), but rather a centre opponency (‘co-extensive’ colour-opponent RF; reviewed in <xref ref-type="bibr" rid="bib79">Schwartz, 2021</xref>), as can be seen in the lower-dimensional visualisations (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>, right column; <xref ref-type="fig" rid="fig3">Figure 3c</xref>, orange trajectory).</p><p>In conclusion, our model-guided in silico exploration of chromatic stimulus space revealed a variety of preferred stimuli that captured known properties of RGC groups, and revealed a preference of G<sub>28</sub> RGCs for centre colour-opponent, UV<sup>ON</sup>-green<sup>OFF</sup> stimuli, a feature previously unknown for this RGC group.</p></sec><sec id="s2-5"><title>Experiments confirm selectivity for chromatic contrast</title><p>Next, we verified experimentally that the MEIs predicted for a given RGC group actually drive cells of that group optimally. To this end, we performed new experiments in which we added to our battery of stimuli a number of MEIs chosen according to the following criteria: We wanted the MEIs to (i) span the response space (ON, ON-OFF, OFF, transient, sustained, and contrast-suppressed) and (ii) to represent both well-described RGC types, such as <inline-formula><mml:math id="inf15"><mml:mi>α</mml:mi></mml:math></inline-formula> cells (i.e. G<sub>5,24</sub>), as well as poorly understood RGC types, such as suppressed-by-contrast cells (G<sub>28,31,32</sub>) (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). We therefore chose MEIs of RGCs from groups G<sub>1</sub> (OFF local), G<sub>5</sub> (OFF <inline-formula><mml:math id="inf16"><mml:mi>α</mml:mi></mml:math></inline-formula> sustained), G<sub>10</sub> (ON-OFF local-edge), G<sub>18</sub> (ON transient), G<sub>20</sub> (ON high frequency), G<sub>21</sub> (ON low frequency), G<sub>23</sub> (ON mini <inline-formula><mml:math id="inf17"><mml:mi>α</mml:mi></mml:math></inline-formula>), G<sub>24</sub> (sustained ON <inline-formula><mml:math id="inf18"><mml:mi>α</mml:mi></mml:math></inline-formula>), G<sub>28</sub> (ON contrast suppressed), G<sub>31</sub> (OFF suppressed 1), and G<sub>32</sub> (OFF suppressed 2). For simplicity, in the following we refer to the MEI of an RGC belonging to group <inline-formula><mml:math id="inf19"><mml:mi>g</mml:mi></mml:math></inline-formula> as group <inline-formula><mml:math id="inf20"><mml:mi>g</mml:mi></mml:math></inline-formula>’s MEI, or MEI <inline-formula><mml:math id="inf21"><mml:mi>g</mml:mi></mml:math></inline-formula>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Experiments confirm maximally exciting inputs (MEIs) predicted by model.</title><p>(<bold>a</bold>) MEIs shown during the experiment, with green and UV spatial components (top two rows), as well as green and UV temporal components (third row) and a spatiotemporal visualisation (fourth row). For display, spatial components <inline-formula><mml:math id="inf22"><mml:mi>s</mml:mi></mml:math></inline-formula> in the two channels were re-scaled to a similar range and displayed on a common grey-scale map ranging from black for <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to white for <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., symmetric about 0 (grey). Relative amplitudes of UV and green are shown in the temporal components. (<bold>b</bold>) Illustration of spatial layout of MEI experiment. White circles represent 5 × 5  grid of positions where MEIs were shown; red shading shows an example RF estimate of a recorded G<sub>32</sub> retinal ganglion cell (RGC), with black dot indicating the RF centre position (Methods). (<bold>c</bold>) Responses of example RGC from (<bold>b</bold>) to the 11 different MEI stimuli at 25 different positions. (<bold>d</bold>) Recorded [top, <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>] and predicted [bottom, <inline-formula><mml:math id="inf26"><mml:msup><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>] responses to the 11 different MEIs for example RGC <inline-formula><mml:math id="inf27"><mml:mi>n</mml:mi></mml:math></inline-formula> from (<bold>b, c</bold>). Left: Responses are averaged across the indicated dimensions <italic>x</italic>, <italic>y</italic> (different MEI locations); black bar indicates MEI stimulus duration (from 0 to 1.66 s), grey rectangle marks optimisation time window (from 1 to 1.66 s). Right: Response to different MEIs, additionally averaged across time (<italic>t</italic>; within optimisation time window). (<bold>e, f</bold>) Same as in (<bold>d</bold>), but additionally averaged across all RGCs (<inline-formula><mml:math id="inf28"><mml:mi>n</mml:mi></mml:math></inline-formula>) of G<sub>5</sub> (<italic>N</italic>=6) (<bold>e</bold>) and of G<sub>28</sub> (<italic>N</italic>=12) (<bold>f</bold>). Error bars show SD across cells. (<bold>g</bold>) Confusion matrix, each row showing the z-scored response magnitude of one RGC group (averaged across all RGCs of that group) to the MEIs in (<bold>a</bold>). Confusion matrix for recorded cells (top; ‘Data') and for model neurons (bottom; ‘Model'). Black squares highlight broad RGC response types according to <xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>: OFF cells, (G<sub>1,5</sub>) ON-OFF cells (G<sub>10</sub>), fast ON cells (G<sub>18,20</sub>), slow ON (G<sub>21,23,24</sub>) and ON contrast suppressed (G<sub>28</sub>) cells, and OFF suppressed cells (G<sub>31,32</sub>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Recorded and predicted responses of example RGC groups to the MEI stimuli.</title><p>(<bold>a</bold>) Recorded (top, <inline-formula><mml:math id="inf29"><mml:mi>r</mml:mi></mml:math></inline-formula>) and predicted (bottom, <inline-formula><mml:math id="inf30"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>) responses to the 11 different maximally exciting inputs (MEIs) for all example cell types. Left: Responses are averaged across the indicated dimensions <italic>x</italic>, <italic>y</italic>, <italic>n</italic>: different MEI locations (<italic>x</italic>, <italic>y</italic>) and retinal ganglion cells (RGCs) in a group (<italic>n</italic>); black bar indicates stimulus duration (from 0 to 1.66 s), grey rectangle marks optimisation time window (from 1 to 1.66 s). Right: Responses to different MEIs, additionally averaged across time (<italic>t</italic>) within the optimisation time window. Error bars indicate SD across cells. (<bold>b</bold>) Correlation between the measured and predicted response magnitudes to the MEI stimuli per example cell type. Cumulative histogram is across all <italic>N</italic>=788 cells; 50% of cells have a correlation between measured and predicted response magnitude of ≥0.8. (<bold>c</bold>) Mean ± SD of selectivity index (see Methods) for the example cell groups, indicating the difference in response to MEI 28 vs. the average response to all other MEIs in units of standard deviation of the response.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig4-figsupp1-v1.tif"/></fig></fig-group><p>We presented these MEIs on a regularly spaced 5 × 5 grid to achieve approximate centring of stimuli on RGC RFs in the recording field (<xref ref-type="fig" rid="fig4">Figure 4b and c</xref>). For these recordings, we fit models whose readout parameters allowed us to estimate the RGCs’ RF locations. We used these RF location estimates to calculate a spatially weighted average of the responses to the MEIs displayed at different locations, weighting the response at each location proportional to the RF strengths at those locations (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, red highlight, and <xref ref-type="fig" rid="fig4">Figure 4d</xref>, top). We then performed the same experiment in silico, confirming that the model accurately predicts responses to the MEIs (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, bottom; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). These experiments allowed us to evaluate MEI responses at the RGC group level (<xref ref-type="fig" rid="fig4">Figure 4e and f</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>We expected RGCs to show a strong response to their own group’s MEI, a weaker response to the MEIs of functionally related groups, and no response to MEIs of groups with different response profiles. Indeed, most RGC groups exhibited their strongest (G<sub>5,20,21,28,32</sub>) or second-strongest (G<sub>1,10,23</sub>) response to their own group’s MEI (<xref ref-type="fig" rid="fig4">Figure 4g</xref>, top). Conversely, RGC groups from opposing regions in response space showed no response to each others’ MEIs (e.g. G<sub>1,5</sub> [OFF cells] vs. G<sub>21-28</sub> [slow ON cells]). The model’s predictions showed a similar pattern (<xref ref-type="fig" rid="fig4">Figure 4g</xref>, bottom), thereby validating the model’s ability to generalise to the MEI stimulus regime.</p><p>Notably, G<sub>28</sub> RGCs responded very selectively to their own MEI 28, displaying only weak responses to most other MEIs (<xref ref-type="fig" rid="fig4">Figure 4f and g</xref>, selectivity index G<sub>28</sub> to MEI 28, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>SI</mml:mtext><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mn>28</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>28</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, defined as the average difference in response between MEI 28 and all other MEIs in units of standard deviation of the response, mean  ± SD: <inline-formula><mml:math id="inf32"><mml:mrow><mml:mn>2.58</mml:mn><mml:mo>±</mml:mo><mml:mn>0.76</mml:mn></mml:mrow></mml:math></inline-formula>; see Methods). This was in contrast to other RGC groups, such as G<sub>23</sub> and G<sub>24</sub>, that responded strongly to MEI 28, but also to other MEIs from the slow ON response regime (<xref ref-type="fig" rid="fig4">Figure 4g</xref>, top; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>SI</mml:mtext><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>28</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, mean ± SD: <inline-formula><mml:math id="inf34"><mml:mrow><mml:mn>1.04</mml:mn><mml:mo>±</mml:mo><mml:mn>0.69</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>SI</mml:mtext><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mn>24</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>28</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, mean ± SD: <inline-formula><mml:math id="inf36"><mml:mrow><mml:mn>1.01</mml:mn><mml:mo>±</mml:mo><mml:mn>0.46</mml:mn></mml:mrow></mml:math></inline-formula>). Hence, our validation experiments confirm the model’s prediction that RGC group G<sub>28</sub> is selective for centre colour-opponent, UV<sup>ON</sup>-green<sup>OFF</sup> stimuli.</p></sec><sec id="s2-6"><title>G<sub>28</sub> corresponds to the tSbC RGC type</title><p>Next, we sought to identify which RGC type G<sub>28</sub> corresponds to. In addition to its unique centre colour opponency, the responses of G<sub>28</sub> displayed a pronounced transient suppression to temporal contrast modulations (chirp response in <xref ref-type="fig" rid="fig1">Figure 1e</xref>). Therefore, we hypothesised that G<sub>28</sub> corresponds to the tSbC RGC type (<xref ref-type="bibr" rid="bib92">Tien et al., 2015</xref>; <xref ref-type="bibr" rid="bib93">Tien et al., 2016</xref>; <xref ref-type="bibr" rid="bib94">Tien et al., 2022</xref>), which is one of the three SbC RGC types identified so far in the mouse and is also referred to as ON delayed (OND) because of its delayed response onset (<xref ref-type="bibr" rid="bib43">Jacoby and Schwartz, 2018</xref>).</p><p>To test this hypothesis, we performed cell-attached electrophysiology recordings (<xref ref-type="fig" rid="fig5">Figure 5</xref>) targeting tSbC/OND cells (<italic>N</italic>=4), identified by their responses to spots of multiple sizes (<xref ref-type="bibr" rid="bib35">Goetz et al., 2022</xref>), and later confirmed by their distinctive morphology (<xref ref-type="bibr" rid="bib43">Jacoby and Schwartz, 2018</xref>; type 73 in <xref ref-type="bibr" rid="bib9">Bae et al., 2018</xref>; <xref ref-type="fig" rid="fig5">Figure 5c and d</xref>). We recorded spikes while presenting the MEI stimuli (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, top). Just like G<sub>28</sub> RGCs in the Ca<sup>2+</sup> imaging, tSbC/OND cells exhibited a pronounced selectivity for MEI 28, and were suppressed by most other MEIs (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, middle and bottom). Notably, the characteristic delayed response onset was visible in both the Ca<sup>2+</sup> (<xref ref-type="fig" rid="fig4">Figure 4f</xref>, top) and electrical (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) responses but was not predicted by the model (<xref ref-type="fig" rid="fig4">Figure 4f</xref>, bottom).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Electrical single-cell recordings of responses to maximally exciting input (MEI) stimuli confirm chromatic selectivity of transient suppressed-by-contrast (tSbC) retinal ganglion cells (RGCs).</title><p>(<bold>a</bold>) Spiking activity (top, raster plot; middle, estimated firing rate) of an OND RGC in response to different MEI stimuli (black bar indicates MEI stimulus duration; grey rectangle marks optimisation time window, from 1 to 1.66 s). Bottom: z-scored activity as a function of MEI stimulus, averaged across cells (solid circles w/ left y-axis, from Ca<sup>2+</sup> imaging, <italic>N</italic>=11 cells; open circles w/ right <italic>y</italic>-axis, from electrical spike recordings, <italic>N</italic>=4). Error bars show SD across cells. Colours as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>b</bold>) Like (<bold>a</bold>) but for a sustained ON <inline-formula><mml:math id="inf37"><mml:mi>α</mml:mi></mml:math></inline-formula> cell (G<sub>24</sub>; <italic>N</italic>=4 cells, both for electrical and Ca<sup>2+</sup> recordings). (<bold>c</bold>) Different ON delayed (OND/tSbC, G<sub>28</sub>) RGC (green) dye-loaded by patch pipette after cell-attached electrophysiology recording (z-projection; <italic>x–y</italic> plane). (<bold>d</bold>) Cell from (c, green) as side projection (<italic>x</italic>–<italic>z</italic>), showing dendritic stratification pattern relative to choline-acetyltransferase (ChAT) amacrine cells (tdTomato, red) within the inner plexiform layer (IPL).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig5-v1.tif"/></fig><p>As a control, we also recorded MEI responses of a different, well-characterised RGC type, sustained (s) ON <inline-formula><mml:math id="inf38"><mml:mi>α</mml:mi></mml:math></inline-formula> (G<sub>24</sub>; <xref ref-type="bibr" rid="bib53">Krieger et al., 2017</xref>; <xref ref-type="fig" rid="fig5">Figure 5b</xref>, top; <italic>N</italic>=4). Again, the electrical recordings of the cells’ MEI responses yielded virtually the same results as the Ca<sup>2+</sup> imaging (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, middle and bottom; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Crucially, sON <inline-formula><mml:math id="inf39"><mml:mi>α</mml:mi></mml:math></inline-formula> cells were not selective for MEI 28. The fact that these experiments with precise positioning of stimuli on the cells’ RFs yielded the same results as the 2P imaging experiments confirms the validity of the grid approach for stimulus presentation used in the latter.</p></sec><sec id="s2-7"><title>Chromatic contrast selectivity derives from a nonlinear transformation of stimulus space</title><p>Next, we asked whether G<sub>28</sub> (tSbC) RGC’s selectivity is a linear feature, as could be achieved by two linear filters with opposite signs for the two colour channels, or whether it is a nonlinear feature. To address this question, we tested whether an LN model (implemented using convolutions; see Methods) could recover the chromatic selectivity of G<sub>28</sub> by predicting MEIs using the LN model. We found that the LN model predicted colour-opponent MEIs for only 9 out of 36 (25%) G<sub>28</sub> RGCs (nonlinear CNN: 24 out of 36 [66%] colour-opponent MEIs; <xref ref-type="fig" rid="fig6">Figure 6a–c</xref>). This finding argues against the possibility that G<sub>28</sub>’s colour opponency can be explained on the computational level by two opposite-sign linear filters operating on the two colour channels, which could be recovered by an LN model. Instead, it suggests the presence of a nonlinear dependency between chromatic contrast (of the stimulus) and chromatic selectivity (of the cell). In other words, G<sub>28</sub> RGCs process stimuli differently depending on their chromatic contrast. This is a nonlinear property that cannot be accurately captured by an LN model that makes a single estimate of the linear filter for the whole stimulus space.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Chromatic contrast selectivity of G<sub>28</sub> retinal ganglion cells (RGCs) derives from a nonlinear transformation of stimulus space.</title><p>(<bold>a</bold>) Distribution of green and UV maximally exciting input (MEI) centre contrast for a linear-nonlinear (LN) model (red) and a nonlinear convolutional neural network (CNN) model (black). Colour-opponent cells highlighted by filled marker. (<bold>b, c</bold>) Left: MEIs for an example cell of RGC group G<sub>28</sub>, generated with the LN model (<bold>b</bold>) or the CNN model (<bold>c</bold>). The cell’s MEI centre contrast for both models is marked in (<bold>a</bold>) by asterisks. Right: Respective tuning maps of example model neuron in chromatic contrast space. Contour colours and background greys represent responses in % of maximum response; arrows indicate the direction of the response gradient across chromatic contrast space. The tuning maps were generated by evaluating the model neurons on stimuli that were generated by modulating the contrast of the green (<italic>x</italic>-axis) and UV (<italic>y</italic>-axis) component of the MEI. In these plots, the original MEI is located at (–1, 1). More details in the Methods section. (<bold>d</bold>) Difference in response predicted between LN and CNN model (in % of maximum response). (<bold>e</bold>) Contour plot as in (<bold>b, c</bold>) but of activity vs. green and UV contrast for an example transient suppressed-by-contrast (tSbC) G<sub>28</sub> RGC measured in whole-cell current-clamp mode. Labels on the contour plot indicate spike count along isoresponse curves. (<bold>f</bold>) Traces are examples of responses at the 8 extremes of –100%, 0, or 100% contrast in each colour channel. Scale bars: (<bold>b</bold>), vertical 200 µm, horizontal 0.5 s; MEI scaling in (<bold>c</bold>) as in (<bold>b</bold>). (<bold>g</bold>) Same as (<bold>e</bold>) for a second example tSbC RGC.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Both LN and CNN model predict colour-opponency for a strongly colour-opponent G_{28} RGC.</title><p>(<bold>a</bold>) Distribution of green and UV maximally exciting input (MEI) centre contrast for a linear-nonlinear (LN) model (red) and a convolutional neural network (CNN) model (black); from <xref ref-type="fig" rid="fig6">Figure 6a</xref>. (<bold>b, c</bold>) Left: MEIs for a second example cell of retinal ganglion cell (RGC) group G<sub>28</sub>, generated with the LN model (<bold>b</bold>) or the CNN model (<bold>c</bold>). The cell’s MEI centre contrast for both models is marked in (<bold>a</bold>) by cross. Right: Respective tuning maps of example neuron in chromatic contrast space. Colours represent responses in % of maximum response; arrows indicate the direction of the gradient across chromatic contrast space. (<bold>d</bold>) Difference in response between LN and CNN model (in % of maximum response). Scale bars: (<bold>b</bold>), vertical 200 µm, horizontal 0.5 s; MEI scaling in (<bold>c</bold>) same as in (<bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig6-figsupp1-v1.tif"/></fig></fig-group><p>To understand the nature of this dependency, we expanded the estimate of the model RGCs’ tuning to colour contrast around the maximum (the MEI). We did this by mapping the model neurons’ response and its gradient in 2D chromatic contrast space (<xref ref-type="fig" rid="fig6">Figure 6b and c</xref>). This analysis revealed that, indeed, G<sub>28</sub> RGCs have a nonlinear tuning for colour contrast: they are strongly UV-selective at lower contrasts, but become colour-opponent, i.e., additionally inhibited by green, for higher contrasts. For individual neurons with very strong colour opponency that extends over a large region of chromatic contrast space, also the LN model’s approximation reflects this colour opponency, which demonstrates that the LN model can in principle model colour opponency, too (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>We confirmed the model’s predictions about G<sub>28</sub>’s nonlinear tuning for colour contrast using electrical recordings as described above. The example G<sub>28</sub> (tSbC) cells shown in the figure exhibit similar nonlinear tuning in chromatic contrast space (<xref ref-type="fig" rid="fig6">Figure 6e–g</xref>). The first example cell’s firing rate (<xref ref-type="fig" rid="fig6">Figure 6f</xref>) and, consequently, the tuning curve (<xref ref-type="fig" rid="fig6">Figure 6e</xref>) peak for UV<sup>ON</sup>-green<sup>OFF</sup> stimuli (top left in panel e; upper row, second from left in f), and are lower for UV<sup>ON</sup>-green<sup>ON</sup> stimuli (top right in panel e; upper row, far right in f), reflecting the suppressive effect of green contrast on the cell’s response. The same is true for the second example cell (<xref ref-type="fig" rid="fig6">Figure 6g</xref>).</p><p>The nonlinearity in tuning to colour contrast of G<sub>28</sub> RGCs leads to a transformation of stimulus space (<xref ref-type="fig" rid="fig6">Figure 6</xref>) that amplifies the distance of colour-opponent stimuli from non-colour-opponent stimuli and is thereby expected to increase their discriminability. We therefore hypothesised that the representation of visual input formed by G<sub>28</sub> might serve to detect an ethologically relevant, colour-opponent feature from the visual scene.</p></sec><sec id="s2-8"><title>Visual context changes are characterised by changes in chromatic contrast</title><p>Chromatic contrast changes at the horizon (<xref ref-type="bibr" rid="bib49">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Gupta et al., 2022</xref>; <xref ref-type="bibr" rid="bib1">Abballe and Asari, 2022</xref>), and so does visual context: from sky to ground or vice versa. We therefore hypothesised that G<sub>28</sub> might leverage chromatic contrast as a proxy for detecting changes in visual context, such as might be caused by head or eye movements that cross the horizon. Detecting these changes in visual context could provide information that is crucial for interpreting signals in other RGC channels.</p><p>In our natural movie stimulus, the transitions between movie clips (<italic>inter-clip transitions</italic>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>) can be categorised into those with and without change in visual context: ground-to-sky and sky-to-ground transitions for vertical movements with a change in visual context, and as controls, ground-to-ground and sky-to-sky transitions for horizontal movements without change in visual context. We calculated the contrast of these transitions in the green and UV channel to map them to chromatic contrast stimulus space (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). We found that ground-to-ground and sky-to-sky transitions were distributed along the diagonal, indicating that they reflect largely achromatic contrast changes. The two transitions resembling visual input elicited by vertical movements crossing the horizon fell into the two colour-opponent quadrants of the stimulus space: sky-to-ground transitions in the lower right quadrant, and ground-to-sky transitions in the upper left quadrant (<xref ref-type="fig" rid="fig7">Figure 7a and b</xref>). The UV<sup>ON</sup>-green<sup>OFF</sup> MEIs 28, the preferred stimuli of G<sub>28</sub>, shared a location in stimulus space with ground-to-sky transitions, indicating that these two stimuli are similar in terms of chromatic contrast (<xref ref-type="fig" rid="fig3">Figure 3i</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Chromatic contrast tuning allows detection of ground-to-sky transitions.</title><p>(<bold>a</bold>) Distribution of green and UV contrasts of all movie inter-clip transitions (centre), separately for the four transition types, for each of which an example is shown: ground-to-sky (<italic>N</italic>=525, top left, red triangle), ground-to-ground (<italic>N</italic>=494, top right, green disk), sky-to-ground (<italic>N</italic>=480, bottom left, black downward triangle), and sky-to-sky (<italic>N</italic>=499, bottom right, purple square). Images show last and first frame of pre- and post-transition clip, respectively. Traces show mean full-field luminance of green and UV channels in last and first 1 s of pre- and post-transition clip. Black trace shows luminance averaged across colour channels. (<bold>b</bold>) Distributions as in (<bold>a</bold>), but shown as contours indicating isodensity lines of inter-clip transitions in chromatic contrast space. Density of inter-clip transitions was estimated separately for each type of transition from histograms within 10 × 10 bins that were equally spaced within the coloured boxes. Four levels of isodensity for each transition type shown, with density levels at 20% (outermost contour, strongest saturation), 40%, 60%, and 80% (innermost contour, weakest saturation) of the maximum density observed per transition: 28 sky-to-ground (black), 75 ground-to-ground (green), 42 sky-to-sky (purple), and 45 ground-to-sky (red) transitions per bin. Orange markers indicate locations of <italic>N</italic>=36 G<sub>28</sub> maximally exciting inputs (MEIs) in chromatic contrast space (<xref ref-type="fig" rid="fig3">Figure 3i</xref>). (<bold>c</bold>) Tuning map of G<sub>28</sub> retinal ganglion cells (RGCs) (<italic>N</italic>=78), created by averaging the tuning maps of the individual RGCs, overlaid with outermost contour lines from (<bold>b</bold>) (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). (<bold>d, e</bold>) Same as (<bold>c</bold>) for G<sub>21</sub> ((<bold>g</bold>), <italic>N</italic>=97) and G<sub>5</sub> ((<bold>h</bold>), <italic>N</italic>=33). (<bold>f</bold>) Top: Illustration of receiver operating characteristic (ROC) analysis for two RGCs, a G<sub>21</sub> (left) and a G<sub>28</sub> (right). For each RGC, responses to all inter-clip transitions were binned, separately for ground-to-sky (red) and all other transitions (grey). Middle: Sliding a threshold <inline-formula><mml:math id="inf40"><mml:mi>d</mml:mi></mml:math></inline-formula> across the response range, classifying all transitions with response <inline-formula><mml:math id="inf41"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> as ground-to-sky, and registering the false-positive rate (FPR) and true-positive rate (TPR) for each threshold yields an ROC curve. Numbers in brackets indicate (FPR, TPR) at the threshold indicated by vertical line in histograms. Bottom: Performance for each cell, quantified as area under the ROC curve (AUC), plotted as distribution across AUC values for all cells (black), G<sub>21</sub> (grey), G<sub>5</sub> (blue), and G<sub>28</sub> (orange); AUC mean ± SD indicated as dots and horizontal lines above histograms. (<bold>g</bold>) Boxplot of AUC distributions per cell type. Boxes extend from first quartile (<inline-formula><mml:math id="inf42"><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>) to third quartile (<inline-formula><mml:math id="inf43"><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>) of the data; line within a box indicates median, whiskers extend to the most extreme points still within [<inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mn>1.5</mml:mn><mml:mo>×</mml:mo><mml:mtext>IQR</mml:mtext></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1.5</mml:mn><mml:mo>×</mml:mo><mml:mtext>IQR</mml:mtext></mml:mrow></mml:math></inline-formula>], IQR = inter-quartile range. Diamonds indicate points outside this range. All plot elements (upper and lower boundaries of the box, median line, whiskers, diamonds) correspond to actual observations in the data. Numbers of RGCs for each type are indicated in the plot. (<bold>h</bold>) Illustration of stimulus with transitions with (Sky-Ground, Ground-Sky) and without (Sky-Sky, Ground-Ground) context change at different velocities (50, 150, 250, and 350 °/s) used in <italic>in silico</italic> experiments in (<bold>i, j</bold>). (<bold>i</bold>) Like (<bold>g</bold>) but for model cells and stimuli illustrated in (<bold>h</bold>) at 50/s (see (<bold>h</bold>)). (<bold>j</bold>) AUC as function of transition velocity for G<sub>28</sub> (orange) vs. example RGC groups (‘Uncertain', G<sub>31,32</sub>; Slow-ON, G<sub>21,23,24</sub>; Fast-ON, G<sub>18,20</sub>; ON-OFF, G<sub>10</sub>; OFF, G<sub>1,5</sub>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Example response traces to inter-clip transitions with and without context changes.</title><p>(<bold>a</bold>) Traces of example cells of different cell groups (G<sub>10</sub>, G<sub>18</sub>, G<sub>23</sub>, G<sub>24</sub>, G<sub>28</sub>) from a single recording field, responding to 33 (of 122) inter-clip transitions. Inter-clip transitions are colour-coded by transition type (red: ground-to-sky, purple: sky-to-sky, green: ground-to-ground, black: sky-to-ground). (<bold>b</bold>) The resulting tuning maps in chromatic contrast space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Chromatic contrast tuning in the dorsal retina allows detection of ground-to-sky transitions.</title><p>(<bold>a</bold>) Illustration of a flat-mounted retina, with recording fields in the dorsal (black circles) and ventral (white circles) retina (cross marks optic disc; d, dorsal; v, ventral; t, temporal; n, nasal). (<bold>b</bold>) Left: Distribution of green and UV contrasts of <italic>N</italic>=122 inter-clip transitions seen by a <italic>ventral</italic> group 28 (G<sub>28</sub>) RGC, coloured by transition type (red triangle, ground-to-sky; green disk, ground-to-ground; black downward triangle, sky-to-ground; purple square, sky-to-sky). Middle: Responses of example RGC in the 1 s following an inter-clip transition, averaged across transitions within the bins indicated by the grid. Right: Responses transformed into a tuning map by averaging within bins as defined by grid. Left: Responses are z-scored (<inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>c</bold>) Like (<bold>b</bold>) but for a <italic>dorsal</italic> G<sub>28</sub> RGC. (<bold>d</bold>) Tuning map of <italic>N</italic>=9 <italic>dorsal</italic> G<sub>28</sub> RGCs, created by averaging the tuning maps of the individual RGCs. (<bold>e</bold>) Same as (<bold>d</bold>) for <italic>N</italic>=13 G<sub>21</sub> RGCs. (<bold>f</bold>) Same as (<bold>d</bold>) for <italic>N</italic>=4 G<sub>5</sub> RGCs. (<bold>g</bold>) Top: Illustration of receiver operating characteristic (ROC) analysis for two dorsal RGCs, a G<sub>21</sub> (left) and a G<sub>28</sub> (right). For each RGC, responses were binned to all inter-clip transitions, separately for ground-to-sky (red) and all other transitions (grey). Middle: Sliding a threshold <inline-formula><mml:math id="inf48"><mml:mi>d</mml:mi></mml:math></inline-formula> across the response range, classifying all transitions with response <inline-formula><mml:math id="inf49"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> as ground-to-sky, and registering the false-positive rate (FPR) and true-positive rate (TPR) for each threshold yields an ROC curve (middle). Numbers in brackets indicate (FPR, TPR) at the threshold indicated by black vertical line in histogram plots. Bottom: We evaluated performance for each cell as the area under the ROC curve (AUC), and plotted the distribution across AUC values for all cells (black), for G<sub>5</sub> (blue), for G<sub>21</sub> (grey), and for G<sub>28</sub> (orange). Among the dorsal RGCs, G<sub>28</sub> RGCs achieved the highest AUC on average (mean±SD AUC, G<sub>28</sub> (<italic>N</italic>=9 cells): 0.62±0.07; all other groups (<italic>N</italic>=720): 0.49±0.09, <inline-formula><mml:math id="inf50"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.13</mml:mn></mml:mrow></mml:math></inline-formula>, bootstrapped 95% confidence interval,<inline-formula><mml:math id="inf51"><mml:mrow><mml:msub><mml:mtext>CI</mml:mtext><mml:mn>95</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.08</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.18</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> Cohen’s <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.45</mml:mn></mml:mrow></mml:math></inline-formula> two-sample permutation test G<sub>28</sub> vs. all other groups (see Methods): <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> with 100,000 permutations; next-best performing G<sub>24</sub> (<italic>N</italic>=6): 0.54±0.12, <inline-formula><mml:math id="inf54"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:math></inline-formula> bootstrapped 95% confidence interval, <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mtext>CI</mml:mtext><mml:mn>95</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.01</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.18</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> Cohen’s <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow></mml:math></inline-formula> two-sided <inline-formula><mml:math id="inf57"><mml:mi>t</mml:mi></mml:math></inline-formula>-test G<sub>28</sub> vs. G<sub>24</sub>: <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> with 100,000 permutations (not significant). AUC mean ± SD indicated as dots and horizontal lines above histograms. (<bold>h</bold>) Boxplot of AUC distributions per cell type (<italic>dorsal</italic>). The box extends from the first quartile (<inline-formula><mml:math id="inf59"><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>) to the third quartile (<inline-formula><mml:math id="inf60"><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>) of the data; the line within a box indicates the median. The whiskers extend to the most extreme points still within [<inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mn>1.5</mml:mn><mml:mo>×</mml:mo><mml:mtext>IQR</mml:mtext></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1.5</mml:mn><mml:mo>×</mml:mo><mml:mtext>IQR</mml:mtext></mml:mrow></mml:math></inline-formula>], IQR =inter-quartile range. Diamonds indicate points outside this range. All elements of the plot (upper and lower boundaries of the box, median line, whiskers, diamonds correspond to actual observations in the data. Numbers of RGCs for each type are indicated in the plot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Simulations predict tSbC cells robustly detect context changes across different speeds.</title><p>(<bold>a</bold>) Illustration transition stimulus paradigm (from <xref ref-type="fig" rid="fig7">Figure 7h</xref>). (<bold>b</bold>) Structure of stimuli for different velocities, using a ground-to-sky transition as an example. (<bold>c</bold>) Statistics of the area under the receiver operating characteristic (ROC) curve (AUC) for the sky-ground detection task in the simulation for different velocities (G<sub>28</sub> vs. the next-best retinal ganglion cell [RGC] group). Columns (from <italic>left</italic>): mean ± standard deviation of AUC values (top: G<sub>28</sub>; bottom: the respective best next RGC type); difference in mean AUC and corresponding bootstrapped 95% confidence intervals; Cohen’s <italic>d</italic> and p-value of a two-sample permutation test with 100,000 repeats. (<bold>d</bold>) Boxplots of AUC distributions per cell type for the different velocities (plots like in <xref ref-type="fig" rid="fig7">Figure 7g and j</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86860-fig7-figsupp3-v1.tif"/></fig></fig-group></sec><sec id="s2-9"><title>Chromatic contrast selectivity allows detecting visual context changes</title><p>Next, we tested if G<sub>28</sub> RGCs indeed respond strongly to visual context changes as occur in ground-to-sky transitions. To this end, we extracted the RGC responses to the inter-clip transitions, mapping their tuning to chromatic contrasts (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>), and then averaged the resulting single-cell tuning maps for each RGC group (e.g. see <xref ref-type="fig" rid="fig7">Figure 7c–e</xref>). G<sub>28</sub> is most strongly tuned to full-field transitions in the upper left quadrant containing mostly ground-to-sky inter-clip transitions (<xref ref-type="fig" rid="fig7">Figure 7c</xref>) – unlike, for example, non-colour-opponent reference RGC groups from the slow ON and OFF response regime (<xref ref-type="fig" rid="fig7">Figure 7d and e</xref>).</p><p>Could a downstream visual area detect ground-to-sky visual context changes based on input from G<sub>28</sub> RGCs? To answer this question, we performed a linear signal detection analysis for each RGC by sliding a threshold across its responses to the inter-clip transitions, classifying all transitions that elicited an above-threshold response as ground-to-sky, and evaluating false-positive and true-positive rates (FPR and TPR, respectively) for each threshold (<xref ref-type="fig" rid="fig7">Figure 7f</xref>). Plotting the resulting TPRs for all thresholds as a function of FPRs yields a receiver operating characteristic (ROC) curve (<xref ref-type="bibr" rid="bib28">Fawcett, 2006</xref>; <xref ref-type="fig" rid="fig7">Figure 7f</xref>, middle). The area under this curve (AUC) can be used as a measure of detection performance: it is equivalent to the probability that a given RGC will respond more strongly to a ground-to-sky transition than to any other type of transition. Indeed, G<sub>28</sub> RGCs achieved the highest AUC on average (<xref ref-type="fig" rid="fig7">Figure 7f</xref>, bottom, and <xref ref-type="fig" rid="fig7">Figure 7g</xref>; G<sub>28</sub>, mean ± SD AUC (<italic>N</italic>=78 cells): 0.68 ± 0.08; two-sample permutation test G<sub>28</sub> vs. all other groups with at least <italic>N</italic>=4 cells (see Methods), significant for each group, with <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0017</mml:mn></mml:mrow></mml:math></inline-formula> Bonferroni-corrected for 30 multiple comparisons).</p><p>Ground-to-sky transitions, and therefore visual context changes, can also appear in the lower visual field, which is processed by the dorsal retina, where RGCs receive weaker UV input (<xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>). We recorded additional fields in the dorsal retina (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>) and also found here that G<sub>28</sub> (tSbC) RGCs displayed the strongest tuning to ground-to-sky transitions among all dorsal RGCs (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3c-h</xref>, for statistics, see legends).</p><p>Visual context changes triggered by different behaviours, such as locomotion and head or eye movements, will differ strongly with respect to their statistics – in particular with respect to their speed. Therefore, for G<sub>28</sub> (tSbC) RGCs to play a role in detecting context changes, their detection performance should be robust across velocities. To test whether this is the case, we conducted additional in silico experiments where we ran model inference on stimuli that simulate transitions across the visual field with and without context change (<xref ref-type="fig" rid="fig7">Figure 7h</xref>) at different velocities: 50, 150, 250, and 350 visual degrees per second (° s<sup>–1</sup>; see Methods; <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3a and b</xref>) The slowest speed simulated visual input as could be elicited by locomotion, and the fastest speed approached that of saccades (<xref ref-type="bibr" rid="bib64">Meyer et al., 2020</xref>). We then performed an ROC analysis on the model cell responses. It should be noted that, because model predictions are noise-free, results from the ROC analysis based on simulated responses will overestimate detection performance. However, under the assumption of approximately equal noise levels across RGC groups, we can still draw conclusions about the relative performance of different RGC groups. This analysis confirmed that G<sub>28</sub> RGCs could distinguish ground-to-sky context changes from all other types of transitions robustly across different speeds (<xref ref-type="fig" rid="fig7">Figure 7i and j</xref>). Interestingly, the advantage of G<sub>28</sub> over other RGC groups in performing this detection task diminished with increasing speed (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3c and d</xref>; see also Discussion).</p><p>Together, these analyses demonstrate that a downstream area, reading out from a single RGC group, would achieve the best performance in detecting ground-to-sky context changes if it based its decisions on inputs from G<sub>28</sub> RGCs, robustly across different lighting conditions (transitions between movie snippets), retinal location (ventral and dorsal), and speeds.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We combined large-scale recordings of RGC responses to natural movie stimulation with CNN-based modelling to investigate colour processing in the mouse retina. By searching the stimulus space <italic>in silico</italic> to identify <italic>maximally exciting inputs</italic> (MEIs), we found a novel type of chromatic tuning in tSbC RGCs. We revealed this RGC type’s pronounced and unique selectivity for full-field changes from green-dominated to UV-dominated scenes, a stimulus that matches the chromatic statistics of ground-to-sky transitions in natural scenes. Therefore, we suggest that tSbC cells may signal context changes within their RF. Beyond our focus on tSbC cells, our study demonstrates the utility of an <italic>in silico</italic> approach for generating and testing hypotheses about the ethological relevance of sensory representations.</p><sec id="s3-1"><title>Nonlinear approaches for characterising neuronal selectivities</title><p>We leveraged image-computable models in combination with an optimisation approach to search in dynamic, chromatic stimulus space for globally optimal inputs for RGCs, the MEIs. The resulting MEI represents the peak in the nonlinear loss landscape that describes the neuron’s tuning in high-dimensional stimulus space. This approach has also been used to reveal the complexities and nonlinearities of neuronal tuning in monkey visual cortex area V4 (<xref ref-type="bibr" rid="bib10">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="bib101">Willeke et al., 2023</xref>) and mouse area V1 (<xref ref-type="bibr" rid="bib98">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Franke et al., 2022</xref>).</p><p>Finding optimal stimuli via predictive models is by no means the only way to reveal nonlinear selectivity. Several alternative approaches exist (<xref ref-type="bibr" rid="bib77">Schwartz et al., 2006</xref>; <xref ref-type="bibr" rid="bib82">Sharpee et al., 2004</xref>; <xref ref-type="bibr" rid="bib55">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Globerson et al., 2009</xref>; <xref ref-type="bibr" rid="bib58">Maheswaranathan et al., 2023</xref>) that could in principle also recover the type of tuning we report (although it is not a trivial question whether and under what conditions they would). More importantly, these approaches are not readily applicable to our data. Some approaches, such as spike-triggered covariance (<xref ref-type="bibr" rid="bib77">Schwartz et al., 2006</xref>) or spike-triggered non-negative matrix factorisation (<xref ref-type="bibr" rid="bib55">Liu et al., 2017</xref>), typically make assumptions about the distribution of input stimuli that are violated, or at least not guaranteed, for naturalistic stimuli, which consequently precludes using these methods to probe cells under their natural stimulus statistics (adaptations of these methods for naturalistic stimuli exist, e.g., see <xref ref-type="bibr" rid="bib2">Aljadeff et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Aljadeff et al., 2016</xref>). Other approaches, including maximally informative dimensions (<xref ref-type="bibr" rid="bib82">Sharpee et al., 2004</xref>) or maximum noise entropy (<xref ref-type="bibr" rid="bib34">Globerson et al., 2009</xref>), use information theory as a framework. They require estimating mutual information between stimulus and responses, which is a challenge when dealing with high-dimensional stimuli and continuous responses.</p><p>Predictive models, on the other hand, can handle high-dimensional input distributions well and can easily be adapted to different response modalities. However, one important limitation of our approach for identifying neuronal stimulus selectivities is that searching for the <italic>maximally</italic> exciting input will return a single input – even when there are several inputs that would elicit an equal response, such as ON and OFF stimuli for ON-OFF cells (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, G<sub>10</sub> MEIs). A remedy for this limitation is to search for <italic>diverse</italic> exciting inputs by generating stimuli that are both highly effective at eliciting neuronal responses and at the same time distinct from one another. <xref ref-type="bibr" rid="bib24">Ding et al., 2023</xref> used this approach to study bipartite invariance in mouse V1 (see also <xref ref-type="bibr" rid="bib16">Cadena et al., 2018</xref>).</p><p>Another limitation is that identifying the MEI does not immediately provide insight into how the different stimulus dimensions contribute to the neuron’s response, i.e., how varying the stimulus along these dimensions affects the neuron’s response. However, differentiable models readily lend themselves to explore the interactions and contributions of different stimulus dimensions in generating the neuronal response, e.g., by inspecting the gradient field along dimensions of interest as done here, or by searching for locally optimal stimulus perturbations (<xref ref-type="bibr" rid="bib36">Goldin et al., 2022</xref>). These models can also be used to understand better the properties that distinguish cell types from each other by generating <italic>most discriminative stimuli</italic>, as recently demonstrated for RGCs in mouse and marmoset (<xref ref-type="bibr" rid="bib15">Burg et al., 2023</xref>).</p><p>Together, these studies showcase the versatility of the toolkit of optimisation-based approaches at characterising nonlinear neuronal operations in high-dimensional, natural stimulus spaces. We add to this toolkit by first searching for a globally optimal stimulus, and then searching locally in its vicinity to map the cells’ loss landscape around the peak.</p></sec><sec id="s3-2"><title>Circuit mechanisms for colour opponency in tSbC RGCs</title><p>Most previous studies of colour opponency in the mouse retina have identified sparse populations of colour-opponent RGCs that have not been systematically assigned to a particular functional type (<xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="bib38">Gouras and Ekesten, 2004</xref>). The only studies that have examined the mechanisms of colour opponency in identified mouse RGC types showed a centre-surround organisation, with RF centre and surround having different chromatic preferences (<xref ref-type="bibr" rid="bib19">Chang et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="bib86">Stabio et al., 2018</xref>, but see <xref ref-type="bibr" rid="bib85">Sonoda et al., 2020</xref>). While we do not specifically analyse centre-surround opponency in this study, we see a similar trend as described previously in many RGC types, with stronger surrounds in the green channel relative to the UV channel (see <xref ref-type="fig" rid="fig4">Figure 4a</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). tSbC RGCs, in contrast, respond to spatially co-extensive colour-opponent stimuli, functionally reminiscent of colour-opponent RGCs in guinea pig (<xref ref-type="bibr" rid="bib104">Yin et al., 2009</xref>) and ground squirrels (<xref ref-type="bibr" rid="bib65">Michael, 1968</xref>).</p><p>In mice, centre-surround opponency has been attributed to the opsin gradient (<xref ref-type="bibr" rid="bib19">Chang et al., 2013</xref>) and rod contributions in the outer retina (<xref ref-type="bibr" rid="bib44">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>), whereas the circuitry for spatially co-extensive opponency remains unknown. It seems unlikely, though, that the opsin gradient plays a major role in the tSbC cell’s colour opponency, because both ventral and dorsal tSbC cells preferentially responded to full-field green-to-UV transitions. In primates, spatially co-extensive colour opponency in small bistratified RGCs is thought to arise from the selective wiring of S<sup>ON</sup> and M/L<sup>OFF</sup> bipolar cells onto the inner and outer dendritic strata, respectively (<xref ref-type="bibr" rid="bib23">Dacey and Lee, 1994</xref>, but see <xref ref-type="bibr" rid="bib29">Field et al., 2007</xref>). A similar wiring pattern seems unlikely for tSbC RGCs, since their inner dendrites do not co-stratify with the S-ON (type 9) bipolar cells, nor do their outer dendrites co-stratify with the candidate M-OFF bipolar cell (type 1) (<xref ref-type="bibr" rid="bib12">Behrens et al., 2016</xref>). The bistratified dendritic arbour distinguishes the mouse tSbC also from the colour-opponent ON RGC type in guinea pig, which is monostratified (<xref ref-type="bibr" rid="bib104">Yin et al., 2009</xref>).</p><p>The large RF centres of the tSbC cells, extending well beyond their dendritic fields, come from a non-canonical circuit, in which tonic inhibition onto the RGC via GABA<sub>B</sub> receptors is relieved via serial inhibition from different amacrine cells using GABA<sub>C</sub> receptors (<xref ref-type="bibr" rid="bib59">Mani and Schwartz, 2017</xref>). An intriguing possibility is that a colour-selective amacrine cell is part of this circuit, perhaps supporting chromatically tuned disinhibition in the absence of selective wiring from the aforementioned cone-selective bipolar cells onto the RGC.</p></sec><sec id="s3-3"><title>A new functional role for tSbC RGCs</title><p>Suppressed-by-contrast responses have been recorded along the early visual pathway in dorsal lateral geniculate nucleus (dLGN), superior colliculus (SC), and primary visual cortex (V1) (<xref ref-type="bibr" rid="bib68">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib70">Piscopo et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Ito et al., 2017</xref>), with their function still being debated (<xref ref-type="bibr" rid="bib62">Masland and Martin, 2007</xref>). In the retina, three types of SbC RGCs have so far been identified (reviewed in <xref ref-type="bibr" rid="bib43">Jacoby and Schwartz, 2018</xref>), among them the tSbC cell (<xref ref-type="bibr" rid="bib59">Mani and Schwartz, 2017</xref>; <xref ref-type="bibr" rid="bib92">Tien et al., 2015</xref>; <xref ref-type="bibr" rid="bib94">Tien et al., 2022</xref>). Despite their relatively recent discovery, tSbC RGCs have been suggested to play a role in several different visual computations. The first report of their light responses in mice connected them to the SbC RGCs previously discovered in rabbit, cat, and macaque, and suggested a role in signalling self-generated stimuli, perhaps for saccade suppression (<xref ref-type="bibr" rid="bib92">Tien et al., 2015</xref>). Aided by a new intersectional transgenic line to selectively label tSbC RGCs (<xref ref-type="bibr" rid="bib94">Tien et al., 2022</xref>), their projections were traced to areas in SC, v- and dLGN, and nucleus of the optic tract (NOT). The latter stabilises horizontal eye movements; however, as the medial terminal nucleus, which serves stabilisation of vertical eye movements, lacks tSbC innervation, it is unclear whether and how these RGCs contribute to gaze stabilisation.</p><p>A retinal study identified the circuit mechanisms responsible for some of the unique spatial and temporal response properties of tSbC cells and suggested a possible role in defocus detection to drive emmetropization in growing eyes and accommodation in adults (<xref ref-type="bibr" rid="bib59">Mani and Schwartz, 2017</xref>; <xref ref-type="bibr" rid="bib7">Baden et al., 2017</xref>). Here, we identified another potential role for these RGCs in vision based on the chromatic properties of their RFs: signalling visual context changes (see next section). These different possible functional roles are not mutually exclusive, and might even be complementary in some cases, highlighting the difficulty in assigning single features to distinct RGC types (<xref ref-type="bibr" rid="bib78">Schwartz and Swygart, 2020</xref>). In particular, the centre colour opponency that we discovered in tSbC RGCs could serve to enhance their role in defocus detection by adding a directional signal (myopic vs. hyperopic) based on the chromatic aberration of lens and cornea (<xref ref-type="bibr" rid="bib33">Gawne and Norton, 2020</xref>). Future studies may test these theories by manipulating these cells in vivo using the new transgenic tSbC mouse line (<xref ref-type="bibr" rid="bib94">Tien et al., 2022</xref>).</p></sec><sec id="s3-4"><title>Behavioural relevance of context change detection</title><p>The horizon is a prominent landmark in visual space: it bisects the visual field into two regions, ground and sky. This is particularly relevant in animals like mice, where eye motion largely accounts for head movements and keeps the visual field stable with respect to the horizon (<xref ref-type="bibr" rid="bib64">Meyer et al., 2020</xref>). Visual stimuli carry different meaning depending on where they occur relative to the horizon, and context-specific processing of visual inputs is necessary for selecting appropriate behavioural responses (reviewed in <xref ref-type="bibr" rid="bib27">Evans et al., 2019</xref>). For example, it is sensible to assume that a looming stimulus above the horizon is a predator, the appropriate response to which would be avoidance (i.e. escape or freezing). A similar stimulus below the horizon, however, is more likely to be harmless or even prey. To allow for time-critical perceptual decisions – predator or prey – and corresponding behavioural response selection – avoidance or approach – it might be useful for stimulus information (e.g. dark moving spot) and contextual information to converge early in the visual circuitry.</p><p>Notably, VGluT3-expressing amacrine cells (a ‘hub’ for distributing information about motion) represent a shared element in upstream circuitry, providing opposite-sign input to tSbC and to RGCs implicated in triggering avoidance behaviour, such as tOFF <inline-formula><mml:math id="inf64"><mml:mi>α</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib53">Krieger et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Münch et al., 2009</xref>) and W3 cells (<xref ref-type="bibr" rid="bib105">Zhang et al., 2012</xref>). In downstream circuitry, SbC inputs have been found to converge with ‘conventional’ RGC inputs onto targets in dLGN and NOT; whether tSbC axons specifically converge with tOFF <inline-formula><mml:math id="inf65"><mml:mi>α</mml:mi></mml:math></inline-formula> or W3 axons remains to be tested. Such convergence may allow ‘flagging’ the activity of these RGCs with their local context (sky/threat or ground/no threat).</p><p>Depending on the behaviour that elicits a context change – be it a head or eye movement or locomotion – the parameters of the incoming stimulus, such as illumination level and velocity, may change. To be behaviourally useful, a context-change-flagging signal needs to be reliable and robust across these different stimulus parameters. While many slow-ON RGCs achieve high detection performance at higher transition velocities, probably reacting to the increasingly flash-like stimuli, tSbC/G<sub>28</sub> RGCs were the only type with robustly high performance across different levels of illumination and all simulated speeds.</p></sec><sec id="s3-5"><title>In silico approaches to linking neural tuning and function</title><p>Modelling studies have advanced our understanding of the complexity and organisation of retinal processing in recent years. It is helpful to consider the contributions of different studies in terms of three perspectives on the retinal encoding of natural scenes: the circuit perspective (‘how?’), the normative perspective (‘why?’), and the coding perspective (‘what?’) (<xref ref-type="bibr" rid="bib60">Marr, 2010</xref>; <xref ref-type="bibr" rid="bib47">Karamanlis et al., 2022</xref>). For example, an <italic>in silico</italic> dissection of a CNN model of the retina offered explanations on how the surprisingly complex retinal computations, such as motion reversal, omitted stimulus response, and polarity reversal, emerge from simpler computations within retinal circuits (<xref ref-type="bibr" rid="bib58">Maheswaranathan et al., 2023</xref>; <xref ref-type="bibr" rid="bib90">Tanaka et al., 2019</xref>). From the normative perspective, networks trained on an efficient coding objective accurately predicted the coordination of retinal mosaics (<xref ref-type="bibr" rid="bib76">Roy et al., 2021</xref>).</p><p>Here, we proposed an approach that allows investigating the complexity of retinal processing simultaneously from the coding and the normative perspectives: A global search for most exciting mouse RGC inputs in dynamic, chromatic stimulus space provides answers to the question of <italic>what</italic> it is that retinal neurons encode. Interpreting the abstract features extracted by the retina against the backdrop of natural stimulus space points to <italic>why</italic> these features might be behaviourally relevant. And finally, classifying individual RGCs into types then allows to bring in the circuit perspective through targeted experiments aimed at dissecting <italic>how</italic> specific retinal computations are implemented.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Animals and tissue preparation</title><p>All imaging experiments were conducted at the University of Tübingen; the corresponding animal procedures were approved by the governmental review board (Regierungspräsidium Tübingen, Baden-Württemberg, Konrad-Adenauer-Str. 20, 72072 Tübingen, Germany) and performed according to the laws governing animal experimentation issued by the German Government. All electrophysiological experiments were conducted at Northwestern University; the corresponding animal procedures were performed according to standards provided by Northwestern University Center for Comparative Medicine and approved by the Institutional Animal Care and Use Committee (IACUC).</p><p>For all imaging experiments, we used 4- to 15-week-old C57Bl/6J mice (<italic>N</italic>=23; JAX 000664) of either sex (10 male, 13 female). These animals were housed under a standard 12 hr day/night rhythm at 22° and 55% humidity. On the day of the recording experiment, animals were dark-adapted for at least 1 hr, then anaesthetised with isoflurane (Baxter) and killed by cervical dislocation. All following procedures were carried out under very dim red (&gt;650 nm) light. The eyes were enucleated and hemisected in carboxygenated (95% O<sub>2</sub>, 5% CO<sub>2</sub>) artificial cerebrospinal fluid (ACSF) solution containing (in mM): 125 NaCl, 2.5 KCl, 2 CaCl<sub>2</sub>, 1 MgCl<sub>2</sub>, 1.25 NaH<sub>2</sub>PO<sub>4</sub>, 26 NaHCO<sub>3</sub>, 20 glucose, and 0.5 L-glutamine at pH 7.4. Next, the retinae were bulk-electroporated with the fluorescent Ca<sup>2+</sup> indicator Oregon-Green BAPTA-1 (OGB-1), as described earlier (<xref ref-type="bibr" rid="bib14">Briggman and Euler, 2011</xref>). In brief, the dissected retina was flat-mounted onto an Anodisc (#13, 0.2 μm pore size, GE Healthcare) with the RGCs facing up, and placed between a pair of 4 mm horizontal plate electrodes (CUY700P4E/L, Nepagene/Xceltis). A 10 µl drop of 5 mM OGB-1 (hexapotassium salt; Life Technologies) in ACSF was suspended from the upper electrode and lowered onto the retina. Next, nine pulses (≈9.2 V, 100 ms pulse width, at 1 Hz) from a pulse generator/wide-band amplifier combination (TGP110 and WA301, Thurlby handar/Farnell) were applied. Finally, the tissue was placed into the microscope’s recording chamber, where it was perfused with carboxygenated ACSF (at ≈36°C) and left to recover for ≥30 min before recordings started. To visualise vessels and damaged cells in the red fluorescence channel, the ACSF contained ≈0.1 µM Sulforhodamine-101 (SR101, Invitrogen) (<xref ref-type="bibr" rid="bib25">Euler et al., 2009</xref>). All procedures were carried out under dim red (&gt;650 nm) light.</p><p>For electrophysiology experiments, we used ChAT-Cre (JAX 006410) × Ai14 (JAX 007914) mice on a C57Bl/6J background (<italic>N</italic>=2, male, aged 27 and 30 weeks). Mice were housed with siblings in groups up to 4, fed normal mouse chow and maintained on a 12:12 hr light/dark cycle. Before the experiment, mice were dark-adapted overnight and sacrificed by cervical dislocation. Retinal tissue was isolated under infrared illumination (900 nm) with the aid of night-vision goggles and IR dissection scope attachments (BE Meyers). Retinal orientation was identified using scleral landmarks (<xref ref-type="bibr" rid="bib100">Wei et al., 2010</xref>), and preserved using relieving cuts in cardinal directions, with the largest cut at the dorsal retina. Retinas were mounted on 12 mm poly-D-lysine-coated glass affixed to a recording dish with grease, with the GCL up. Oxygenation was maintained by superfusing the dish with carboxygenated Ames medium (US Biological, A1372-25) warmed to 32°C. For cell-attached single-cell recordings, we used <ext-link ext-link-type="uri" xlink:href="https://symphony-das.github.io/">Symphony</ext-link> software (<xref ref-type="bibr" rid="bib18">Cafaro et al., 2019</xref>) with custom extensions (<xref ref-type="bibr" rid="bib80">Schwartz and Ala-Laurila, 2024</xref>).</p><p>Owing to the exploratory nature of our study, we did not use randomisation and blinding. No statistical methods were used to predetermine sample size.</p></sec><sec id="s4-2"><title>Two-photon calcium imaging</title><p>We used a MOM-type two-photon microscope (designed by W Denk, purchased from Sutter Instruments) (<xref ref-type="bibr" rid="bib25">Euler et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">Euler et al., 2019</xref>), which was equipped with a mode-locked Ti:sapphire laser (MaiTai-HP DeepSee, Newport Spectra-Physics) tuned to 927 nm, two fluorescence detection channels for OGB-1 (HQ 510/84, AHF/Chroma) and SR101 (HQ 630/60, AHF), and a water immersion objective (CF175 LWD x 16/0.8W, DIC N2, Nikon, Germany). Image acquisition was performed with custom-made software (ScanM by M Müller and TE) running under IGOR Pro 6.3 for Windows (Wavemetrics), taking time-lapsed 64 × 64 pixel image scans (≈ (100 μm)<sup>2</sup>) at 7.8125 Hz (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). For simplicity, we refer to such a time-lapsed scan of a local population of GCL cells as a ‘recording’. Despite the low frame rate, the Ca<sup>2+</sup> responses can be related to the spike rate (<xref ref-type="bibr" rid="bib95">Trapani et al., 2023</xref>; <xref ref-type="bibr" rid="bib75">Román Rosón et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Berens et al., 2018</xref>; <xref ref-type="bibr" rid="bib91">Theis et al., 2016</xref>). For documenting the position of the recording fields, the retina under the microscope was oriented such that the most ventral edge pointed always towards the experimenter. In addition, higher resolution images (512 ×512 pixel) were acquired and recording field positions relative to the optic nerve were routinely logged.</p></sec><sec id="s4-3"><title>Data preprocessing</title><p>Ca<sup>2+</sup> traces were extracted for individual ROIs as described previously (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>; <xref ref-type="bibr" rid="bib88">Szatko et al., 2020</xref>). Extracted traces <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>𝐜</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were then detrended to remove slow drifts in the recorded signal that were unrelated to changes in the neural response. First, a smoothed version of the traces, <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>𝐜</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, was calculated by applying a Savitzky-Golay filter of third polynomial order and a window length of 60 s using the SciPy implementation <monospace>scipy.signal.savgol_filter</monospace>. This smoothed version was then subtracted from the raw traces to yield the detrended traces.<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>To make traces non-negative (<inline-formula><mml:math id="inf68"><mml:msub><mml:mi>𝐜</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>), we then clipped all values smaller than the 2.5th percentile, <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>η</mml:mi><mml:mn>2.5</mml:mn></mml:msub></mml:math></inline-formula>, to that value, and then subtracted <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>η</mml:mi><mml:mn>2.5</mml:mn></mml:msub></mml:math></inline-formula> from the detrended traces:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>2.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>This procedure (i.e. clipping to, and subtracting <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>η</mml:mi><mml:mn>2.5</mml:mn></mml:msub></mml:math></inline-formula>) was more robust than simply subtracting the minimum.</p><p>Finally, traces were then divided by the standard deviation within the time window before stimulus start at <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For training the model on movie response, we then estimated firing rates <inline-formula><mml:math id="inf73"><mml:mi>𝐫</mml:mi></mml:math></inline-formula> from the detrended Ca<sup>2+</sup> traces <inline-formula><mml:math id="inf74"><mml:mi>𝐜</mml:mi></mml:math></inline-formula> using the package C2S (<ext-link ext-link-type="uri" xlink:href="https://github.com/lucastheis/c2s">https://github.com/lucastheis/c2s</ext-link>, copy archived at <xref ref-type="bibr" rid="bib56">lucastheis, 2016</xref>; <xref ref-type="bibr" rid="bib91">Theis et al., 2016</xref>).</p></sec><sec id="s4-4"><title>Inclusion criteria</title><p>We applied a sequence of quality filtering steps to recorded cells before analysis illustrated in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. As a first step, we applied a general response quality criterion, defined as a sufficiently reliable response to the MB stimulus (as quantified by a quality index <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), <italic>or</italic> a sufficiently reliable response to the chirp stimulus (as quantified by a quality index <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.35</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). The quality index is defined as in <xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>Q</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf77"><mml:mi>𝒓</mml:mi></mml:math></inline-formula> is the <italic>T</italic> by <italic>I</italic> response matrix (time samples by stimulus repetitions) and <inline-formula><mml:math id="inf78"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msub><mml:mo form="postfix" stretchy="false">⟩</mml:mo><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the mean and variance across the indicated dimension <inline-formula><mml:math id="inf80"><mml:mi>x</mml:mi></mml:math></inline-formula>, respectively.</p><p>The second and third step made sure only cells were included that were assigned to a ganglion cell group (i.e. group index between 1 and 32) with sufficient confidence. Confidence is defined as the probability assigned to the predicted class by the random forest classifier (see <xref ref-type="bibr" rid="bib73">Qiu et al., 2023</xref>), and the threshold was set at ≥0.25.</p><p>The fourth step made sure only cells with a sufficient model prediction performance, defined as an average single-trial test set correlation of <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, were included.</p><p>All cells passing steps 1–3 were included in the horizon detection analysis (<xref ref-type="fig" rid="fig7">Figure 7</xref>); all cells passing steps 1–4 were included in the MEI analysis (<xref ref-type="fig" rid="fig3">Figure 3</xref>); the ‘red’ cells passing steps 1–4 were included in the MEI validation analysis (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In the process of analysing MEIs, we fitted DoGs to their green and UV spatial component (see Methods section Concentric anisotropic 2D DoG fit). For the analysis of MEI properties (temporal frequency, centre size, chromatic contrast), we only included cells with a sufficient DoG goodness-of-fit, determined as a value of the cost function of &lt;0.11 for both green and UV on the resulting DoG fit. This threshold was determined by visual inspection of the DoG fits and led to the inclusion of 1613 out of 1947 RGCs in the MEI property analysis.</p></sec><sec id="s4-5"><title>Visual stimulation</title><p>For light stimulation (imaging experiments), we projected the image generated by a digital light processing projector (lightcrafter DPM-FE4500MKIIF, EKB Technologies Ltd) through the objective onto the tissue. The lightcrafter featured a light-guide port to couple in external, band-pass-filtered UV and green LEDs (light-emitting diodes) (green: 576 BP 10, F37-576; UV: 387 BP 11, F39-387; both AHF/Chroma) (<xref ref-type="bibr" rid="bib30">Franke et al., 2019</xref>). To optimise spectral separation of mouse M- and S-opsins, LEDs were band-pass-filtered (390/576 dual-band, F59-003, AHF/Chroma). LEDs were synchronised with the microscope’s scan retrace. Stimulator intensity (as photoisomerisation rate, 10<sup>3</sup> <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> per cone) was calibrated to range from ≈ 0.5 (black image) to ≈ 20 for M- and S-opsins, respectively. Additionally, we estimated a steady illumination component of ≈ 10<sup>4</sup> <inline-formula><mml:math id="inf83"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo lspace="0em" rspace="0em">*</mml:mo></mml:msup><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> per cone to be present during the recordings because of two-photon excitation of photopigments (<xref ref-type="bibr" rid="bib25">Euler et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">Euler et al., 2019</xref>). Before data acquisition, the retina was adapted to the light stimulation by presenting a binary noise stimulus (20 × 15 matrix, (40 μm)<sup>2</sup> pixels, balanced random sequence) at 5 Hz for 5 min to the tissue. Stimuli were presented using the software RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016985">SCR_016985</ext-link>, QDSpy (<ext-link ext-link-type="uri" xlink:href="https://github.com/eulerlab/QDSpy">https://github.com/eulerlab/QDSpy</ext-link>).</p><p>For electrophysiology experiments, stimuli were presented using a digital projector (DPM-FE4500MKII, EKB Technologies Ltd) at a frame rate of 60 Hz and a spatial resolution of 1140 × 912 pixels (1.3 μm per pixel) focused on the photoreceptor layer. Neutral density filters (Thorlabs), a triple-band pass filter (405 BP 20, 485 BP 20, 552 BP 16; 69,000×, Chroma), and a custom LED controller circuit were used to attenuate the light intensity of stimuli either to match that of the Ca<sup>2+</sup> imaging experiments (for MEI presentation) or to range from ≈0 to 200 <inline-formula><mml:math id="inf84"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo lspace="0em" rspace="0em">*</mml:mo></mml:msup><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> per rod (for cell identification). Stimuli were presented using Symphony software (<ext-link ext-link-type="uri" xlink:href="https://symphony-das.github.io/">https://symphony-das.github.io/</ext-link>) with custom extensions (<ext-link ext-link-type="uri" xlink:href="https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension">https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension</ext-link>, copy archived at <xref ref-type="bibr" rid="bib81">Schwartz-AlaLaurila-Labs, 2024</xref>).</p></sec><sec id="s4-6"><title>Identifying RGC types</title><p>To functionally identify RGC groups in the Ca<sup>2+</sup> imaging experiments, we used our default ‘fingerprinting’ stimuli, as described earlier (<xref ref-type="bibr" rid="bib6">Baden et al., 2016</xref>). These stimuli included a full-field (700 μm in diameter) chirp stimulus, and a 300 ×1,000 μm bright bar moving at 1000 μm/s in eight directions across the recording field (with the shorter edge leading; <xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p><p>The procedure and rationale for identifying cells in the electrophysiological recordings is presented in <xref ref-type="bibr" rid="bib35">Goetz et al., 2022</xref>. Cells with responses that qualitatively matched that of the OND and ON <inline-formula><mml:math id="inf85"><mml:mi>α</mml:mi></mml:math></inline-formula> types were included in the study. Following recording, cells were filled with Alexa Fluor-488 by patch pipette and imaged under a two-photon microscope. Dendrites were traced in Fiji (NIH) using the SNT plugin (<xref ref-type="bibr" rid="bib4">Arshadi et al., 2021</xref>). Dendritic arbours were computationally flattened using a custom MATLAB tool (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6578530">https://doi.org/10.5281/zenodo.6578530</ext-link>) based on the method in <xref ref-type="bibr" rid="bib87">Sümbül et al., 2014</xref>, to further confirm their identity as morphological type 73 from <xref ref-type="bibr" rid="bib9">Bae et al., 2018</xref>.</p></sec><sec id="s4-7"><title>Mouse natural movies</title><p>The natural movie stimulus consisted of clips of natural scenes recording outside in the field with a specialised, calibrated camera (<xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>). This camera featured a fish-eye lens, and two spectral channels, UV (band-pass filter F37-424, AHF, &gt; 90% transmission at 350–419 nm) and green (F47-510, &gt;90%, 470–550 nm, AHF), approximating the spectral sensitivities of mouse opsins (<xref ref-type="bibr" rid="bib42">Jacobs et al., 2004</xref>). In mice, eye movements often serve to stabilise the image on the retina during head movements (<xref ref-type="bibr" rid="bib64">Meyer et al., 2020</xref>). Therefore, the camera was also stabilised by mounting it on a gimbal. As a result, the horizon bisected the camera’s visual field.</p><p>A <italic>mouse cam movie</italic> frame contained a circular field of view of 180° corresponding to 437 pixels along the diameter. To minimise the influence of potential chromatic and spatial aberrations introduced by the lenses, we focused on image cut-outs (crops; 30 × 26, equivalent to 72 × 64 pixels in size) from upper and lower visual field, centred at [28, 56] and [-42, –31], respectively, relative to the horizon (for details, see <xref ref-type="bibr" rid="bib72">Qiu et al., 2021</xref>). Our <italic>stimulus movie</italic> consisted of 113 movie clips, each 150 frames (= 5 s) long. 108 clips were randomly reordered for each recording and split into two 54 clips-long <italic>training sequences</italic>. The remaining 5 clips formed a fixed <italic>test sequence</italic> that was presented before, in between, and after the training sequences (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). To keep intensity changes at clip transitions small, we only used clips with mean intensities between 0.04 and 0.22 (for intensities in [0, 1]). For display during the experiments, intensities were then mapped to the range covered by the stimulator, i.e., [0, 255].</p></sec><sec id="s4-8"><title>CNN model of the retina</title><p>We trained a CNN model to predict responses of RGCs to a dichromatic natural movie. The CNN model consisted of two modules, a convolutional core that was shared between all neurons, and a readout that was specific for each neuron (<xref ref-type="bibr" rid="bib52">Klindt et al., 2017</xref>).</p><p>The core module was modelled as a two-layer CNN with 16 feature channels in each layer. Both layers consisted of space-time separable 3D convolutional kernels followed by a batch normalisation layer and an ELU (exponential linear unit) nonlinearity. In the first layer, sixteen 2 × 11 × 11 × 21 (<italic>c</italic>=#input channels (green and UV) × <italic>h</italic>=height ×<italic>w</italic>=width × <italic>t</italic>=#frames) kernels were applied as valid convolution; in the second layer, sixteen 16 × 5 × 5 × 11 kernels were applied with zero padding along the spatial dimensions. We parameterised the temporal kernels as Fourier series and added one time stretching parameter per recording to account for inter-experimental variability affecting the speed of retinal processing. More precisely, every temporal kernel was represented by the first <inline-formula><mml:math id="inf86"><mml:mi>k</mml:mi></mml:math></inline-formula> sine and cosine functions, with trainable weights and phases, on an evenly spaced temporal grid, where <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> for the first layer, and <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> for the second layer. Additionally, we introduced a trainable stretch parameter for every recording to account for faster and slower response kernels. For example, the first layer temporal kernels are 21 steps long. Then, in order to stay well under the Nyquist limit, we parameterise the kernels with <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn><mml:mi>/</mml:mi><mml:mn>3</mml:mn><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> sines and cosines.</p><p>For each of those sines and cosines a weight (<inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>α</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula>) is learned to represent the shape of the temporal responses kernel (shared among cells within a recording). Per scan <inline-formula><mml:math id="inf91"><mml:mi>i</mml:mi></mml:math></inline-formula>, the time grid <inline-formula><mml:math id="inf92"><mml:mi>t</mml:mi></mml:math></inline-formula> (21 steps from 0 to 1) is stretched by a factor <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to account for different response speeds. To avoid adding additional cycles (e.g. for stretch factors <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) this is masked by an exponential envelope<disp-formula id="equ5"><label>(1)</label><mml:math id="m5"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>21</mml:mn><mml:mo>⋅</mml:mo><mml:mn>0.95</mml:mn></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Thus,<disp-formula id="equ6"><label>(2)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>is the temporal kernel parameterisation, that allows the model to learn a shared temporal filter that is made faster or slower for each specific scan (<xref ref-type="bibr" rid="bib106">Zhao et al., 2020</xref>).</p><p>In the readout, we modelled each cell’s spatial receptive field (RF) as a 2D isotropic Gaussian, parameterised as <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi class="mathcal">𝒩</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo separator="true">;</mml:mo><mml:mi>σ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We then modelled the neural response as an affine function of the core feature maps weighted by the spatial RF, followed by a softplus nonlinearity.</p><p>For the linearised version of the model, the architecture was exactly the same except for the fact that there was no ELU nonlinearity after both convolutional layers. The resulting CNN was therefore equivalent to an LN model.</p></sec><sec id="s4-9"><title>Model training and evaluation</title><p>We trained our network by minimising the Poisson loss<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf96"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of neurons, <inline-formula><mml:math id="inf97"><mml:msup><mml:mi>𝒓</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the measured and <inline-formula><mml:math id="inf98"><mml:msup><mml:mover><mml:mi>𝒓</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>𝒏</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> the predicted firing rate of neuron <inline-formula><mml:math id="inf99"><mml:mi>n</mml:mi></mml:math></inline-formula> for an input of duration <italic>t</italic>=50 frames. We followed the training schedule of <xref ref-type="bibr" rid="bib57">Lurz et al., 2021</xref>. Specifically, we used early stopping (<xref ref-type="bibr" rid="bib71">Prechelt, 1998</xref>) on the correlation between predicted and measured neuronal responses on the validation set, which consisted of 15 out of the 108 movie clips. If the correlation failed to increase during any five consecutive passes through the entire training set (epochs), we stopped the training and restored the model to the best performing model over the course of training. We went through four cycles of early stopping, restoring the model to the best performing, and continuing training, each time reducing the initial learning rate of 0.01 by a learning rate decay factor of 0.3. Network parameters were iteratively optimised via stochastic gradient descent (SGD) using the Adam optimiser (<xref ref-type="bibr" rid="bib51">Kingma and Ba, 2015</xref>) with a batch size of 32 and a chunk size (number of frames for each element in the batch) of 50. For all analyses and MEI generation, we used an ensemble of models as described in <xref ref-type="bibr" rid="bib31">Franke et al., 2022</xref>. Briefly, we trained five instances of the same model initialised with different random seeds. Inputs to the ensemble model were passed to each member and the final ensemble model prediction was obtained by averaging the outputs of the five members. For ease of notation, we thus redefine <inline-formula><mml:math id="inf100"><mml:msup><mml:mover><mml:mi>𝒓</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>𝒏</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> to be the <italic>ensemble</italic> model prediction.</p><p>After training, we evaluated model performance for each modelled neuron <inline-formula><mml:math id="inf101"><mml:mi>n</mml:mi></mml:math></inline-formula> as the correlation to the mean, i.e., the correlation between predicted response <inline-formula><mml:math id="inf102"><mml:msup><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and measured response <inline-formula><mml:math id="inf103"><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> to the held-out test sequence, the latter averaged across three repetitions <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>2</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>3</mml:mn><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Unlike the single-trial correlation <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> which is always limited to values <inline-formula><mml:math id="inf107"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> by inherent neuronal noise, a perfect model can in theory achieve a value of 1 for the correlation to the mean, in the limit of infinitely many repetitions when the sample average <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a perfect estimate of the true underlying response <inline-formula><mml:math id="inf109"><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. The observed correlation to the mean can thus be interpreted as an estimate of the fraction of the maximally achievable correlation achieved by our model. For deciding which cells to exclude from analysis, we used average single-trial correlation (<inline-formula><mml:math id="inf110"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:mi>C</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo separator="true">,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mo form="postfix" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) since this measure reflects both model performance as well as reliability of the neuronal response to the movie stimulus for neuron <inline-formula><mml:math id="inf111"><mml:mi>n</mml:mi></mml:math></inline-formula> (see also Methods section on Inclusion criteria).</p></sec><sec id="s4-10"><title>Synthesising MEIs</title><p>We synthesised MEIs for RGCs as described previously (<xref ref-type="bibr" rid="bib98">Walker et al., 2019</xref>). Formally, for each model neuron <inline-formula><mml:math id="inf112"><mml:mi>n</mml:mi></mml:math></inline-formula> we wanted to find<disp-formula id="equ8"><label>(3)</label><mml:math id="m8"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:munder><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>30</mml:mn><mml:mo>:</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>i.e., the input <inline-formula><mml:math id="inf113"><mml:msup><mml:mi>𝒙</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">*</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> where the model neuron’s response <inline-formula><mml:math id="inf114"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:mover><mml:mi>𝒓</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝒙</mml:mi><mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mrow><mml:mn>30</mml:mn><mml:mo lspace="0.2222em" rspace="0.2222em">:</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo form="postfix" stretchy="false">⟩</mml:mo><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, averaged across frames 30–50, attains a maximum, subject to norm and range constraints (see below). To this end, we randomly initialised an input <inline-formula><mml:math id="inf115"><mml:mrow><mml:msubsup><mml:mi>𝒙</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi class="mathcal">ℛ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of duration <italic>t</italic>=50 frames with Gaussian white noise, and then iteratively updated <inline-formula><mml:math id="inf116"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> according to the gradient of the model neuron’s response:<disp-formula id="equ9"><label>(4)</label><mml:math id="m9"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mi>δ</mml:mi><mml:mrow><mml:mi>δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>30</mml:mn><mml:mo>:</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> was the learning rate. The optimisation was performed using SGD, and was subject to a norm and a range constraint. The norm constraint was applied jointly across both channels and ensured that the L2 norm of each MEI did not exceed a fixed budget <inline-formula><mml:math id="inf118"><mml:mi>b</mml:mi></mml:math></inline-formula> of 30. The norm-constrained MEI <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> was calculated at each iteration as<disp-formula id="equ10"><label>(5)</label><mml:math id="m10"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>The range constraint was defined and applied for each colour channel separately and ensured that the range of the MEI values stayed within the range covered by the training movie. This was achieved by clipping values of the MEI exceeding the range covered by the training movie to the minimum or maximum value. Optimisation was run for at least 100 iterations, and then stopped when the number of iterations reached 1000, or when it had converged (whichever occurred first). Convergence was defined as 10 consecutive iterations with a change in model neuron activation of less than 0.001; model neuron activations ranged from ≈1 to ≈10. We denote the resulting MEI for neuron <inline-formula><mml:math id="inf120"><mml:mi>n</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf121"><mml:msup><mml:mi>𝒙</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">*</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>.</p></sec><sec id="s4-11"><title>Analysing MEIs</title><p>We analysed MEIs to quantify their spatial, temporal, and chromatic properties.</p><sec id="s4-11-1"><title>Spatial and temporal components of MEIs</title><p>For each colour channel <inline-formula><mml:math id="inf122"><mml:mi>c</mml:mi></mml:math></inline-formula>, we decomposed the spatiotemporal MEIs into a spatial component and a temporal component by singular value decomposition:<disp-formula id="equ11"><mml:math id="m11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;"><mml:mrow><mml:mi>U</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>S</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mtext>svd</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mi>𝒙</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">*</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf123"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">*</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi class="mathcal">ℛ</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mo>×</mml:mo><mml:mn>288</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo></mml:mrow></mml:math></inline-formula> [green, UV] is the MEI of neuron <inline-formula><mml:math id="inf125"><mml:mi>n</mml:mi></mml:math></inline-formula> in a given colour channel with its spatial dimension (18 ×16=288) flattened out. As a result, any spatiotemporal dependencies are removed and we only analyse spatial and temporal properties separately. The following procedures were carried out in the same manner for the green and the UV component of the MEI, and we drop the colour channel index <inline-formula><mml:math id="inf126"><mml:mi>c</mml:mi></mml:math></inline-formula> for ease of notation. The temporal component is then defined as the first left singular vector, <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and the spatial component is defined as the first right singular vector, <inline-formula><mml:math id="inf128"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">:</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula>, reshaped to the original dimensions 18 × 16 .</p></sec><sec id="s4-11-2"><title>Concentric anisotropic 2D DoG fit</title><p>We modelled the spatial component as concentric anisotropic DoG using the nonlinear least-squares solver <monospace>scipy.optimize.least_squares</monospace> with soft-L1 loss function (<xref ref-type="bibr" rid="bib39">Gupta et al., 2022</xref>). The DoGs were parameterised by a location <inline-formula><mml:math id="inf129"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> shared between centre and surround, amplitudes <inline-formula><mml:math id="inf130"><mml:msup><mml:mtext>A</mml:mtext><mml:mi>c</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf131"><mml:msup><mml:mtext>A</mml:mtext><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula>, variances <inline-formula><mml:math id="inf132"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo separator="true">,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf133"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo separator="true">,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and rotation angles <inline-formula><mml:math id="inf134"><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf135"><mml:msup><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula> separately for centre and surround:<disp-formula id="equ12"><mml:math id="m12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;"><mml:mrow><mml:mtext>DoG</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>with<disp-formula id="equ13"><mml:math id="m13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>y</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mtext>A</mml:mtext><mml:mi>c</mml:mi></mml:msup><mml:mtext>exp</mml:mtext><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>g</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;"><mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>and<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mtext>cos</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mtext>sin</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>sin</mml:mtext><mml:mn>2</mml:mn><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>sin</mml:mtext><mml:mn>2</mml:mn><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mtext>sin</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mtext>cos</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>and likewise for <inline-formula><mml:math id="inf136"><mml:msup><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math></inline-formula>. We initialised <inline-formula><mml:math id="inf137"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in the following way: Since we set the model readout’s location parameters to (0, 0) for all model neurons when generating their MEIs, we also expected the MEIs to be centred at (0, 0), as well. Hence, we determined the location of the minimum and the maximum value of the MEI; whichever was closer to the centre (0,0) provided the initial values for the parameters <inline-formula><mml:math id="inf138"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Starting from there, we then first fit a single Gaussian to the MEI, and took the resulting parameters as initial parameters for the DoG fit. This was a constrained optimisation problem, with lower and upper bounds on all parameters; in particular, such that the location parameter would not exceed the canvas of the MEI, and such that the variance would be strictly positive.</p></sec><sec id="s4-11-3"><title>MEI properties</title><sec id="s4-11-3-1"><title>Centre size</title><p>We defined the diameter of the centre of the MEI in the horizontal and the vertical orientation, respectively, as <inline-formula><mml:math id="inf139"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf140"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The centre size was calculated as <inline-formula><mml:math id="inf141"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We then estimated a contour outlining the MEI centre as the line that is defined by all points at which the 2D centre Gaussian <inline-formula><mml:math id="inf142"><mml:msup><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:math></inline-formula> attains the value <inline-formula><mml:math id="inf143"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>y</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf144"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>y</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>). The centre mask <inline-formula><mml:math id="inf145"><mml:mi>m</mml:mi></mml:math></inline-formula> was then defined as a binary matrix with all pixels within the convex hull of this contour being 1 and all other pixels set to 0. This mask is used for calculating centre chromatic contrast (see below).</p></sec><sec id="s4-11-3-2"><title>Temporal frequency</title><p>To estimate temporal frequency of the MEIs, we estimated the power spectrum of the temporal components using a fast Fourier transform after attenuating high frequency noise by filtering with a fifth-order low-pass Butterworth filter with cutoff frequency 10 Hz. We then estimated the mean frequency of the temporal component by calculating an average of the frequency components, each weighted with its relative power.</p></sec><sec id="s4-11-3-3"><title>Contrast</title><p>The contrast of the MEIs in the two channels, <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo></mml:mrow></mml:math></inline-formula> [green, UV], was defined as the difference between the mean value within the centre mask <inline-formula><mml:math id="inf148"><mml:mi>m</mml:mi></mml:math></inline-formula> at the two last peaks of the temporal component of the MEI in the UV channel at time points <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>:<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⊙</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⊙</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where ⊙ denotes the element-wise multiplication of the MEI and the binary mask (see <xref ref-type="fig" rid="fig3">Figure 3f</xref>). The peaks were found with the function <monospace>scipy.signal.find_peaks</monospace>, and the peaks found for the UV channel were used to calculate contrast both in the green and the UV channel.</p></sec></sec></sec><sec id="s4-12"><title>Validating MEIs experimentally</title><sec id="s4-12-1"><title>Generating MEI stimuli</title><p>To test experimentally whether the model correctly predicts which stimuli would maximally excite RGCs of different RGC groups, we performed a new set of experiments (numbers indicated in red in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), where we complemented our stimulus set with MEI stimuli. For the MEI stimuli, we selected 11 RGCs, chosen to span the responses space and to represent both well-described and poorly understood RGC groups, for which we generated MEIs at different positions on a 5 × 5 grid (spanning 110 μm in vertical and horizontal direction). We decomposed the MEIs as described above, and reconstructed MEIs as rank 1 tensors by taking the outer product of the spatial and temporal components:<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>The MEI stimuli, lasting 50 frames (1.66 s) were padded with 10 frames (0.34 s) of inter-stimulus grey, and were randomly interleaved. With 11 stimuli, presented at 25 positions and lasting 2 s each, the total stimulus duration was 11 × 25 × 2 s = 550 s. Since the model operated on a z-scored (0 mean, 1 SD) version of the movie, MEIs as predicted by the model lived in the same space and had to be transformed back to the stimulator range ([0, 255]) before being used as stimuli in an experiment by scaling with the movie’s SD and adding the movie’s mean. The MEIs’ green channel was then displayed with the green LED, and the UV channel was displayed with the UV LED. For experiments at Northwestern University, an additional transform was necessary to achieve the same levels of photoreceptor activation (photoisomerisation rates) for M- and S-cones with different LEDs. To ensure proper chromatic scaling between the different experimental apparatuses with different spectral profiles, we described the relative activation of M- and S-cones by the green and UV LEDs in the stimulation setup used in the two-photon imaging experiments (setup <bold>A</bold>) by a matrix<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0.19</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>and the relative activation of M- and S-cones by the stimulation setup used in the patch-clamp experiments (setup <bold>B</bold>) by a matrix<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.035</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where diagonal entries describe the activation of M-cones by the green LED, and of S-cones by the UV LED, and entries in the off-diagonal describe the cross-activation (i.e. M-cones by UV-LED and S-cones by green LED). The activation of M-cones and S-cones <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> by a stimulus <inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>𝒙</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi class="mathcal">ℛ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> displayed on a given stimulation setup was approximated as <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>𝐞</mml:mi><mml:mo>=</mml:mo><mml:mi>𝐀</mml:mi><mml:mi>𝒙</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib21">Christenson et al., 2022</xref>). Hence, a stimulus <inline-formula><mml:math id="inf154"><mml:msup><mml:mi>𝒙</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math></inline-formula> displayed on setup <bold>B</bold>, defined as <inline-formula><mml:math id="inf155"><mml:mrow><mml:msup><mml:mi>𝒙</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>𝐁</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>𝐀</mml:mi><mml:mi>𝒙</mml:mi></mml:mrow></mml:math></inline-formula>, will achieve the same photoreceptor activation as stimulus <inline-formula><mml:math id="inf156"><mml:mi>𝒙</mml:mi></mml:math></inline-formula> displayed on setup <bold>A</bold>. Since the solution exceeded the valid range of the stimulator ([0, 255]), we added an offset and multiplied with a scalar factor to ensure all stimuli were within the valid range.</p></sec><sec id="s4-12-2"><title>Analysing RGC responses to MEI stimuli</title><p>We wanted to evaluate the responses of RGCs to the MEI stimuli in a spatially resolved fashion, i.e., weighting responses to MEIs displayed at different locations proportional to the strength of the RGCs RF at that location. In order to be able to meaningfully compare MEI responses between RGCs and across groups, for each RGC, we first centred and scaled the responses to zero mean and a standard deviation of 1. Then, for each RGC <inline-formula><mml:math id="inf157"><mml:mi>n</mml:mi></mml:math></inline-formula>, we computed a spatial average of its responses, weighting its responses at each spatial location <inline-formula><mml:math id="inf158"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>y</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> proportional to the Gaussian density <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mi class="mathcal">𝒩</mml:mi><mml:mrow><mml:msub><mml:mi>𝝁</mml:mi><mml:mi>𝒏</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>y</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where the parameters of the Gaussian <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mi>𝝁</mml:mi><mml:mi>𝒏</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were the model’s estimated readout parameters for neuron <inline-formula><mml:math id="inf161"><mml:mi>n</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4b, c, and d</xref>, left):<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="mathcal" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn><mml:mo>×</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the 60 frames (2 s) long response of neuron <inline-formula><mml:math id="inf163"><mml:mi>n</mml:mi></mml:math></inline-formula> to an MEI at position <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, resampled from the recording frame rate of 7.81 Hz to 30 Hz. We then averaged <inline-formula><mml:math id="inf165"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>𝒓</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>i</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mo form="postfix" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> across time in the optimisation time window, i.e., frames 30–50, to get a scalar response <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each MEI stimulus (<xref ref-type="fig" rid="fig4">Figure 4d</xref>).</p></sec><sec id="s4-12-3"><title>Selectivity index</title><p>To quantify the selectivity of the response <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of an RGC <inline-formula><mml:math id="inf168"><mml:mi>n</mml:mi></mml:math></inline-formula> to an MEI <inline-formula><mml:math id="inf169"><mml:msubsup><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi><mml:mo lspace="0em" rspace="0em" style="font-weight:bold;">*</mml:mo></mml:msubsup></mml:math></inline-formula>, we defined a selectivity index as follows. First, we standardised the responses <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> across all MEIs by subtracting the mean and dividing by the standard deviation. The selectivity index of RGC group G<sub>g</sub> to MEI <inline-formula><mml:math id="inf171"><mml:msubsup><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi><mml:mo lspace="0em" rspace="0em" style="font-weight:bold;">*</mml:mo></mml:msubsup></mml:math></inline-formula> was then defined as<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>SI</mml:mtext><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf172"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the Kronecker delta. In words, the SI is the difference (in units of SD response) between the response to the MEI of interest (<inline-formula><mml:math id="inf173"><mml:msubsup><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi><mml:mo lspace="0em" rspace="0em" style="font-weight:bold;">*</mml:mo></mml:msubsup></mml:math></inline-formula>) and the mean response to all other (10) MEIs, <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, averaged across all cells <inline-formula><mml:math id="inf175"><mml:mi>n</mml:mi></mml:math></inline-formula> belonging to the group of interest G<sub>g</sub>.</p></sec></sec><sec id="s4-13"><title>Characterising nonlinear processing of chromatic contrast</title><p>We wanted to analyse the tuning of G<sub>28</sub>/tSbC RGCs to chromatic contrast and to this end, we mapped the model response and its gradient across a range of chromatic contrasts (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Specifically, the MEIs have <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>18</mml:mn><mml:mo>×</mml:mo><mml:mn>16</mml:mn><mml:mo>×</mml:mo><mml:mn>50</mml:mn><mml:mo>=</mml:mo><mml:mn>28,800</mml:mn></mml:mrow></mml:math></inline-formula> pixels and dimensions, 14,400 for each colour channel. Now let <inline-formula><mml:math id="inf177"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">*</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi class="mathcal">ℛ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>x</mml:mi><mml:mn>28800</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be the cell’s MEI estimated using the CNN model, with the first <italic>d</italic>=14,400 dimensions defining the green pixels and the remaining dimensions defining the UV pixels. Then for each cell, we first consider a two-dimensional subspace spanned by two basis vectors <inline-formula><mml:math id="inf178"><mml:mrow><mml:msub><mml:mi>𝐞</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>𝐞</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> where<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>d</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In words, the first basis vector <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>𝐞</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> consists of the green component of the MEI in the green channel, multiplied by <inline-formula><mml:math id="inf180"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and of 0 s in the UV channel, and the second basis vector consists of the UV component in the UV channel and of 0 s in the green channel for <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>𝐞</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. <inline-formula><mml:math id="inf182"><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are two scaling factors chosen to equalise contrast (as measured by L2 norm) across colour channels while preserving the contrast of the stimulus as a whole. In the subspace spanned by these basis vectors, the point <inline-formula><mml:math id="inf184"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> represents a contrast-scaled version of the original MEI. We then sampled 11 points along each dimension, equally spaced between <inline-formula><mml:math id="inf185"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, which resulted in stimuli that are identical in terms of their spatial and temporal properties and only differ in their contrast. We then evaluated the model neuron response at these points in the subspace (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). We also evaluated the gradient of the model neuron response at these points and plotted the direction of the gradient projected into the subspace spanned by <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6b and c</xref>).</p></sec><sec id="s4-14"><title>Detection performance analysis</title><p>To test the performance of individual RGCs of different groups in detecting the target class of inter-clip transitions (ground-to-sky) from all other classes of inter-clip transitions, we performed an ROC analysis (<xref ref-type="bibr" rid="bib28">Fawcett, 2006</xref>). For each RGC, we calculated its response to an inter-clip transition occurring at time <inline-formula><mml:math id="inf187"><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> as the baseline-subtracted average response within 1 s following the transition, i.e., <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <italic>T</italic>=30 frames at 30 Hz. For all <italic>N</italic>=40 equally spaced thresholds within the response range of an RGC, we then calculated the TPR and FPR of a hypothetical classifier classifying all transitions eliciting an above-threshold response as a positive, and all other transitions as negative. Plotting the TPR as a function of FPR yields an ROC curve, the area under which (AUC) is equivalent to the probability that the RGC will respond more strongly to a randomly chosen inter-clip transition of the target class than to a randomly chosen inter-clip transition of a different class. The AUC thus is a measure of performance for RGCs in this detection task.</p></sec><sec id="s4-15"><title>Detection task in simulation</title><p>We simulated the four types of transitions (sky-sky, sky-ground, ground-ground, ground-sky) in natural scenes at different velocities, which could be triggered by different behaviours such as locomotion or eye movements. With the simulated context-changing stimuli, we predicted model neuron responses in silico and then determined if G<sub>28</sub> could perform the detection task robustly well across speeds.</p><p>For generating the stimuli, 500 frames were randomly extracted from the same mouse natural movies used for the 2P imaging experiments. For each frame, we simulated visual transitions by moving a 72×64 pixel-large window along a fixed trajectory (<xref ref-type="fig" rid="fig7">Figure 7h</xref>, bottom) at four different angular velocities: 50, 150, 250, and 350/s, corresponding to 4, 12, 20, and 28 pixels per frame, respectively (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3a and b</xref>). Each edge of the trajectory is 220 pixels long, covering 90.6 of visual angle. Each selected scene frame was sampled eight times (i.e. twice per velocity). To avoid potential biases due to asymmetries in the mouse natural movie, we sampled each frame for each velocity both in clockwise and in counterclockwise direction. The stimuli were then down-sampled to 18×16 pixels and shown to the model at a frame rate of 30 Hz. Because the trajectories contained different numbers of moving frames for the four velocities, we ‘padded’ the stimuli at the beginning and the end of each transition stimulus by duplicating the start and end frames, resulting in a total of 60 frames each (see illustration in <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>).</p></sec><sec id="s4-16"><title>Statistical analysis</title><sec id="s4-16-1"><title>Permutation test</title><p>We wanted to test how likely the difference in AUC observed for different RGC groups are to occur under the null hypothesis that the underlying distributions they are sampled from are equal. To this end, we performed a permutation test. We generated a null distribution for our test statistic, the absolute difference in AUC values <inline-formula><mml:math id="inf189"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula>, by shuffling the RGC group labels of the two groups of interest (e.g. G<sub>28</sub> and G<sub>24</sub>) and calculating the test statistic with shuffled labels 100,000 times. We only included RGC groups with at least <italic>N</italic>=4 cells in this analysis. We then obtained a p-value for <inline-formula><mml:math id="inf190"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula> observed with true labels as the proportion of entries in the null distribution larger than <inline-formula><mml:math id="inf191"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-16-2"><title>Bootstrapped confidence intervals</title><p>We bootstrapped confidence intervals for <inline-formula><mml:math id="inf192"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). For <inline-formula><mml:math id="inf193"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula>, we generated a bootstrapped distribution by sampling 100 times with replacement from the AUC values of the two groups that were being compared and calculated <inline-formula><mml:math id="inf194"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula>. We then estimated the 95% confidence interval for <inline-formula><mml:math id="inf195"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula> as the interval defined by the  2.5th and  97.5th percentile of the bootstrapped distribution of <inline-formula><mml:math id="inf196"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula>.</p><p>For <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we generated a bootstrapped distribution by sampling 100 times with replacement from the MEI responses of RGC group <inline-formula><mml:math id="inf198"><mml:mi>g</mml:mi></mml:math></inline-formula> and then calculating <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mtext>RDM</mml:mtext><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for each sample. We then estimated the 95% confidence interval for <inline-formula><mml:math id="inf201"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as the interval defined by the 2.5th and 97.5th percentile of the bootstrapped distribution of <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-16-3"><title>Estimating effect size</title><p>The effect size of difference in AUC observed for different RGC groups <inline-formula><mml:math id="inf203"><mml:mi>l</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf204"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf205"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mtext>AUC</mml:mtext></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>), was estimated as Cohen’s <inline-formula><mml:math id="inf206"><mml:mi>d</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib22">Cohen, 1988</xref>; <xref ref-type="bibr" rid="bib37">Goulet-Pelletier and Cousineau, 2018</xref>):<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf207"><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf208"><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> the sample mean and standard deviation, respectively, of the AUC observed for the <inline-formula><mml:math id="inf209"><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> RGCs of group k.</p></sec><sec id="s4-16-4"><title>Estimating linear correlation</title><p>Wherever the linear correlation between two paired samples <inline-formula><mml:math id="inf210"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mi>y</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf212"><mml:mi>N</mml:mi></mml:math></inline-formula> was calculated for evaluating model performance, <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, we used Pearson’s correlation coefficient:<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:msqrt><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Validation, Investigation</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Methodology</p></fn><fn fn-type="con" id="con4"><p>Validation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Software</p></fn><fn fn-type="con" id="con6"><p>Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Validation, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con8"><p>Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con12"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con13"><p>Conceptualization, Supervision, Funding acquisition, Visualization, Writing - original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All imaging experiments were conducted at the University of Tübingen; the corresponding animal procedures were approved by the governmental review board (Regierungspräsidium Tübingen, Baden-Württemberg, Konrad-Adenauer-Str. 20, 72072 Tübingen, Germany) and performed according to the laws governing animal experimentation issued by the German Government. All electrophysiological experiments were conducted at Northwestern University; the corresponding animal procedures were performed according to standards provided by Northwestern University Center for Comparative Medicine and approved by the Institutional Animal Care and Use Committee (IACUC).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86860-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and the movie stimulus are available at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/eulerlab/rgc-natstim">https://gin.g-node.org/eulerlab/rgc-natstim</ext-link>. Code for training models and reproducing analyses and figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/eulerlab/rgc-natstim-model">https://github.com/eulerlab/rgc-natstim-model</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib40">Hoefling et al., 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Höfling</surname><given-names>H</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Behrens</surname><given-names>C</given-names></name><name><surname>Yeng</surname><given-names>D</given-names></name><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Klindt</surname><given-names>DA</given-names></name><name><surname>Jessen</surname><given-names>Z</given-names></name><name><surname>Schwartz</surname><given-names>GM</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>rgc-natstim-model</data-title><source>g-node</source><pub-id pub-id-type="accession" xlink:href="https://gin.g-node.org/eulerlab/rgc-natstim">rgc-natstim-model</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Jonathan Oesterle and Dominic Gonschorek for feedback on the manuscript, Jan Lause for statistical consulting, and Merle Harrer for general assistance. We also thank all members of the Sinz lab for regular discussions on the project. This work was supported by the German Research Foundation (DFG; CRC 1233 'Robust Vision: Inference Principles and Neural Mechanisms', project number 276693517 to PB, MB, TE, KF; Heisenberg Professorship, BE5601/8-1 to PB; Excellence Cluster EXC 2064, project number 390727645 to PB; CRC 1456 'Mathematics of Experiment' project number 432680300 to ASE), the Federal Ministry of Education and Research (FKZ 01IS18039A to PB), National Institutes of Health (NIH; NEI EY031029, NEI EY031329 to GWS; NEI F30EY031565, NIGMS T32GM008152 to ZJ), and the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (ASE, grant agreement No. 101041669). We acknowledge support from the Open Access Publication Fund of the University of Tübingen.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abballe</surname><given-names>L</given-names></name><name><surname>Asari</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Natural image statistics for mouse vision</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0262763</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0262763</pub-id><pub-id pub-id-type="pmid">35051230</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aljadeff</surname><given-names>J</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spike triggered covariance in strongly correlated gaussian stimuli</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003206</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003206</pub-id><pub-id pub-id-type="pmid">24039563</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aljadeff</surname><given-names>J</given-names></name><name><surname>Lansdell</surname><given-names>BJ</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Analysis of neuronal spike trains, deconstructed</article-title><source>Neuron</source><volume>91</volume><fpage>221</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.039</pub-id><pub-id pub-id-type="pmid">27477016</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arshadi</surname><given-names>C</given-names></name><name><surname>Günther</surname><given-names>U</given-names></name><name><surname>Eddison</surname><given-names>M</given-names></name><name><surname>Harrington</surname><given-names>KIS</given-names></name><name><surname>Ferreira</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>SNT: a unifying toolbox for quantification of neuronal anatomy</article-title><source>Nature Methods</source><volume>18</volume><fpage>374</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01105-7</pub-id><pub-id pub-id-type="pmid">33795878</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Wei</surname><given-names>T</given-names></name><name><surname>Zaichuk</surname><given-names>M</given-names></name><name><surname>Wissinger</surname><given-names>B</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A tale of two retinal domains: near-optimal sampling of achromatic contrasts in natural scenes through asymmetric photoreceptor distribution</article-title><source>Neuron</source><volume>80</volume><fpage>1206</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.030</pub-id><pub-id pub-id-type="pmid">24314730</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The functional diversity of retinal ganglion cells in the mouse</article-title><source>Nature</source><volume>529</volume><fpage>345</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1038/nature16468</pub-id><pub-id pub-id-type="pmid">26735013</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Schaeffel</surname><given-names>F</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual neuroscience: a retinal ganglion cell to report image focus?</article-title><source>Current Biology</source><volume>27</volume><fpage>R139</fpage><lpage>R141</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.12.022</pub-id><pub-id pub-id-type="pmid">28222289</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Understanding the retinal basis of vision across species</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>5</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1038/s41583-019-0242-1</pub-id><pub-id pub-id-type="pmid">31780820</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Mu</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>JS</given-names></name><name><surname>Turner</surname><given-names>NL</given-names></name><name><surname>Tartavull</surname><given-names>I</given-names></name><name><surname>Kemnitz</surname><given-names>N</given-names></name><name><surname>Jordan</surname><given-names>CS</given-names></name><name><surname>Norton</surname><given-names>AD</given-names></name><name><surname>Silversmith</surname><given-names>WM</given-names></name><name><surname>Prentki</surname><given-names>R</given-names></name><name><surname>Sorek</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>DL</given-names></name><name><surname>Bland</surname><given-names>D</given-names></name><name><surname>Sterling</surname><given-names>ALR</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Briggman</surname><given-names>KL</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><collab>Eyewirers</collab></person-group><year iso-8601-date="2018">2018</year><article-title>Digital museum of retinal ganglion cells with dense anatomy and physiology</article-title><source>Cell</source><volume>173</volume><fpage>1293</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.04.040</pub-id><pub-id pub-id-type="pmid">29775596</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><volume>364</volume><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Batty</surname><given-names>E</given-names></name><name><surname>Merel</surname><given-names>J</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Heitman</surname><given-names>A</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>A</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multilayer recurrent network models of primate retinal ganglion cell responses</article-title><conf-name>Proceedings of the 5th International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>C</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Haverkamp</surname><given-names>S</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Connectivity map of bipolar cells and photoreceptors in the mouse retina</article-title><source>eLife</source><volume>5</volume><elocation-id>e20041</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.20041</pub-id><pub-id pub-id-type="pmid">27885985</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Deneux</surname><given-names>T</given-names></name><name><surname>Chenkov</surname><given-names>N</given-names></name><name><surname>McColgan</surname><given-names>T</given-names></name><name><surname>Speiser</surname><given-names>A</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Mineault</surname><given-names>P</given-names></name><name><surname>Rupprecht</surname><given-names>P</given-names></name><name><surname>Gerhard</surname><given-names>S</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Bolte</surname><given-names>B</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Ringach</surname><given-names>D</given-names></name><name><surname>Stone</surname><given-names>J</given-names></name><name><surname>Rogerson</surname><given-names>LE</given-names></name><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Theis</surname><given-names>L</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Community-based benchmarking improves spike rate inference from two-photon calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006157</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006157</pub-id><pub-id pub-id-type="pmid">29782491</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briggman</surname><given-names>KL</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Bulk electroporation and population calcium imaging in the adult mammalian retina</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2601</fpage><lpage>2609</lpage><pub-id pub-id-type="doi">10.1152/jn.00722.2010</pub-id><pub-id pub-id-type="pmid">21346205</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Burg</surname><given-names>MF</given-names></name><name><surname>Zenkel</surname><given-names>T</given-names></name><name><surname>Vystrčilová</surname><given-names>M</given-names></name><name><surname>Oesterle</surname><given-names>J</given-names></name><name><surname>Höfling</surname><given-names>L</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Lause</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Most Discriminative Stimuli for Functional Cell Type Identification</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2401.05342">http://arxiv.org/abs/2401.05342</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Weis</surname><given-names>MA</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Diverse feature visualizations reveal invariances in early layers of deep neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Weis</surname><given-names>MA</given-names></name></person-group><source>Computer Vision - ECCV 2018</source><publisher-name>Springer Link</publisher-name><fpage>225</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-01258-8_14</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Cafaro</surname><given-names>M</given-names></name><name><surname>Midgley</surname><given-names>F</given-names></name><name><surname>Neward</surname><given-names>T</given-names></name><name><surname>Wark</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Symphony-DAS</data-title><version designator="v2.6.3.0">v2.6.3.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://symphony-das.github.io">https://symphony-das.github.io</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Breuninger</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Chromatic coding from cone-type unselective circuits in the mouse retina</article-title><source>Neuron</source><volume>77</volume><fpage>559</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.012</pub-id><pub-id pub-id-type="pmid">23395380</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id><pub-id pub-id-type="pmid">11405422</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christenson</surname><given-names>MP</given-names></name><name><surname>Mousavi</surname><given-names>SN</given-names></name><name><surname>Oriol</surname><given-names>E</given-names></name><name><surname>Heath</surname><given-names>SL</given-names></name><name><surname>Behnia</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Exploiting colour space geometry for visual stimulus design across animals</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>377</volume><elocation-id>20210280</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2021.0280</pub-id><pub-id pub-id-type="pmid">36058250</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Statistical Power Analysis for the Behavioral Sciences</source><publisher-name>Routledge</publisher-name><pub-id pub-id-type="doi">10.4324/9780203771587</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname><given-names>DM</given-names></name><name><surname>Lee</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The “blue-on” opponent pathway in primate retina originates from a distinct bistratified ganglion cell type</article-title><source>Nature</source><volume>367</volume><fpage>731</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1038/367731a0</pub-id><pub-id pub-id-type="pmid">8107868</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Tran</surname><given-names>DT</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Wang</surname><given-names>E</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Bipartite Invariance in Mouse Primary Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.15.532836</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Hausselt</surname><given-names>SE</given-names></name><name><surname>Margolis</surname><given-names>DJ</given-names></name><name><surname>Breuninger</surname><given-names>T</given-names></name><name><surname>Castell</surname><given-names>X</given-names></name><name><surname>Detwiler</surname><given-names>PB</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Eyecup scope--optical recordings of light stimulus-evoked fluorescence signals in the retina</article-title><source>Pflugers Archiv</source><volume>457</volume><fpage>1393</fpage><lpage>1414</lpage><pub-id pub-id-type="doi">10.1007/s00424-008-0603-5</pub-id><pub-id pub-id-type="pmid">19023590</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Studying a light sensor with light: multiphoton imaging in the retina</chapter-title><person-group person-group-type="editor"><name><surname>Hartveit</surname><given-names>Espen</given-names></name></person-group><source>In Multiphoton Microscopy</source><publisher-name>Humana Press</publisher-name><fpage>225</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-9702-2_10</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>DA</given-names></name><name><surname>Stempel</surname><given-names>AV</given-names></name><name><surname>Vale</surname><given-names>R</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cognitive control of escape behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>334</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.012</pub-id><pub-id pub-id-type="pmid">30852123</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fawcett</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An introduction to ROC analysis</article-title><source>Pattern Recognition Letters</source><volume>27</volume><fpage>861</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2005.10.010</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatial properties and functional organization of small bistratified ganglion cells in primate retina</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>13261</fpage><lpage>13272</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3437-07.2007</pub-id><pub-id pub-id-type="pmid">18045920</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Maia Chagas</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Zimmermann</surname><given-names>MJ</given-names></name><name><surname>Bartel</surname><given-names>P</given-names></name><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An arbitrary-spectrum spatial visual stimulator for vision research</article-title><source>eLife</source><volume>8</volume><elocation-id>e48779</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48779</pub-id><pub-id pub-id-type="pmid">31545172</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Galdamez</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>N</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>State-dependent pupil dilation rapidly shifts visual feature selectivity</article-title><source>Nature</source><volume>610</volume><fpage>128</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05270-3</pub-id><pub-id pub-id-type="pmid">36171291</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>Shrinivasan</surname><given-names>S</given-names></name><name><surname>Baroni</surname><given-names>L</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Pierzchlewicz</surname><given-names>P</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Froebe</surname><given-names>R</given-names></name><name><surname>Ntanavara</surname><given-names>L</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Wang</surname><given-names>E</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Tran</surname><given-names>DT</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Antolik</surname><given-names>J</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Pattern Completion and Disruption Characterize Contextual Modulation in the Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.13.532473</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gawne</surname><given-names>TJ</given-names></name><name><surname>Norton</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An opponent dual-detector spectral drive model of emmetropization</article-title><source>Vision Research</source><volume>173</volume><fpage>7</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2020.03.011</pub-id><pub-id pub-id-type="pmid">32445984</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Globerson</surname><given-names>A</given-names></name><name><surname>Stark</surname><given-names>E</given-names></name><name><surname>Vaadia</surname><given-names>E</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The minimum information principle and its application to neural code analysis</article-title><source>PNAS</source><volume>106</volume><fpage>3490</fpage><lpage>3495</lpage><pub-id pub-id-type="doi">10.1073/pnas.0806782106</pub-id><pub-id pub-id-type="pmid">19218435</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goetz</surname><given-names>J</given-names></name><name><surname>Jessen</surname><given-names>ZF</given-names></name><name><surname>Jacobi</surname><given-names>A</given-names></name><name><surname>Mani</surname><given-names>A</given-names></name><name><surname>Cooler</surname><given-names>S</given-names></name><name><surname>Greer</surname><given-names>D</given-names></name><name><surname>Kadri</surname><given-names>S</given-names></name><name><surname>Segal</surname><given-names>J</given-names></name><name><surname>Shekhar</surname><given-names>K</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Schwartz</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unified classification of mouse retinal ganglion cells using function, morphology, and gene expression</article-title><source>Cell Reports</source><volume>40</volume><elocation-id>111040</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.111040</pub-id><pub-id pub-id-type="pmid">35830791</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldin</surname><given-names>MA</given-names></name><name><surname>Lefebvre</surname><given-names>B</given-names></name><name><surname>Virgili</surname><given-names>S</given-names></name><name><surname>Pham Van Cang</surname><given-names>MK</given-names></name><name><surname>Ecker</surname><given-names>A</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Ferrari</surname><given-names>U</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Context-dependent selectivity to natural images in the retina</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>5556</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-33242-8</pub-id><pub-id pub-id-type="pmid">36138007</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goulet-Pelletier</surname><given-names>JC</given-names></name><name><surname>Cousineau</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A review of effect sizes and their confidence intervals, part I: the cohen’s d family</article-title><source>The Quantitative Methods for Psychology</source><volume>14</volume><fpage>242</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.20982/tqmp.14.4.p242</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouras</surname><given-names>P</given-names></name><name><surname>Ekesten</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Why do mice have ultra-violet vision?</article-title><source>Experimental Eye Research</source><volume>79</volume><fpage>887</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.exer.2004.06.031</pub-id><pub-id pub-id-type="pmid">15642326</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>D</given-names></name><name><surname>Młynarski</surname><given-names>W</given-names></name><name><surname>Sumser</surname><given-names>A</given-names></name><name><surname>Symonova</surname><given-names>O</given-names></name><name><surname>Svatoň</surname><given-names>J</given-names></name><name><surname>Joesch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Panoramic visual statistics shape retina-wide organization of receptive fields</article-title><source>Neuroscience</source><volume>01</volume><fpage>1</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1101/2022.01.11.475815</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hoefling</surname><given-names>L</given-names></name><name><surname>Deng</surname><given-names>Y</given-names></name><name><surname>D’Agostino</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>rgc-natstim-model</data-title><version designator="swh:1:rev:fab17f3c34167effb97f2fbbf534ea5f5cfcb8a8">swh:1:rev:fab17f3c34167effb97f2fbbf534ea5f5cfcb8a8</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9b10b35ed7b1ff364637c849296fd407e66d8dc9;origin=https://github.com/eulerlab/rgc-natstim-model;visit=swh:1:snp:c7e52be290f24dba2e99afe2ddc8634c365a4bf9;anchor=swh:1:rev:fab17f3c34167effb97f2fbbf534ea5f5cfcb8a8">https://archive.softwareheritage.org/swh:1:dir:9b10b35ed7b1ff364637c849296fd407e66d8dc9;origin=https://github.com/eulerlab/rgc-natstim-model;visit=swh:1:snp:c7e52be290f24dba2e99afe2ddc8634c365a4bf9;anchor=swh:1:rev:fab17f3c34167effb97f2fbbf534ea5f5cfcb8a8</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>S</given-names></name><name><surname>Feldheim</surname><given-names>DA</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Segregation of visual response properties in the mouse superior colliculus and their modulation during locomotion</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8428</fpage><lpage>8443</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3689-16.2017</pub-id><pub-id pub-id-type="pmid">28760858</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>GH</given-names></name><name><surname>Williams</surname><given-names>GA</given-names></name><name><surname>Fenwick</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Influence of cone pigment coexpression on spectral sensitivity and color vision in the mouse</article-title><source>Vision Research</source><volume>44</volume><fpage>1615</fpage><lpage>1622</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.01.016</pub-id><pub-id pub-id-type="pmid">15135998</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>J</given-names></name><name><surname>Schwartz</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Typology and circuitry of suppressed-by-contrast retinal ganglion cells</article-title><source>Frontiers in Cellular Neuroscience</source><volume>12</volume><elocation-id>269</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2018.00269</pub-id><pub-id pub-id-type="pmid">30210298</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joesch</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A neuronal circuit for colour vision based on rod-cone opponency</article-title><source>Nature</source><volume>532</volume><fpage>236</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1038/nature17158</pub-id><pub-id pub-id-type="pmid">27049951</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>KP</given-names></name><name><surname>Fitzpatrick</surname><given-names>MJ</given-names></name><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>McCracken</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>PR</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cell-type-specific binocular vision guides predation in mice</article-title><source>Neuron</source><volume>109</volume><fpage>1527</fpage><lpage>1539</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.010</pub-id><pub-id pub-id-type="pmid">33784498</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamanlis</surname><given-names>D</given-names></name><name><surname>Gollisch</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Nonlinear spatial integration underlies the diversity of retinal ganglion cell responses to natural images</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>3479</fpage><lpage>3498</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3075-20.2021</pub-id><pub-id pub-id-type="pmid">33664129</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamanlis</surname><given-names>D</given-names></name><name><surname>Schreyer</surname><given-names>HM</given-names></name><name><surname>Gollisch</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Retinal encoding of natural scenes</article-title><source>Annual Review of Vision Science</source><volume>8</volume><fpage>171</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-100820-114239</pub-id><pub-id pub-id-type="pmid">35676096</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Feature detection by retinal ganglion cells</article-title><source>Annual Review of Vision Science</source><volume>8</volume><fpage>135</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-100419-112009</pub-id><pub-id pub-id-type="pmid">35385673</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khani</surname><given-names>MH</given-names></name><name><surname>Gollisch</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linear and nonlinear chromatic integration in the mouse retina</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1900</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22042-1</pub-id><pub-id pub-id-type="pmid">33772000</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Shen</surname><given-names>N</given-names></name><name><surname>Hsiang</surname><given-names>JC</given-names></name><name><surname>Johnson</surname><given-names>KP</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dendritic and parallel processing of visual threats in the retina control defensive responses</article-title><source>Science Advances</source><volume>6</volume><elocation-id>eabc9920</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abc9920</pub-id><pub-id pub-id-type="pmid">33208370</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: a method for stochastic optimization</article-title><source>Proceedings of the 3rd International Conference on Learning Representations</source><volume>01</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1145/1830483.1830503</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Klindt</surname><given-names>DA</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural System Identification for Large Populations Separating ‘What’ and ‘where</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1711.02653">http://arxiv.org/abs/1711.02653</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krieger</surname><given-names>B</given-names></name><name><surname>Qiao</surname><given-names>M</given-names></name><name><surname>Rousso</surname><given-names>DL</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Four alpha ganglion cell types in mouse retina: Function, structure, and molecular signatures</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0180091</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0180091</pub-id><pub-id pub-id-type="pmid">28753612</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lettvin</surname><given-names>JY</given-names></name><name><surname>Maturana</surname><given-names>HR</given-names></name><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>What the frog’s eye tells the frog’s brain</article-title><source>Proceedings of the IRE</source><volume>47</volume><fpage>1940</fpage><lpage>1951</lpage><pub-id pub-id-type="doi">10.1109/JRPROC.1959.287207</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>JK</given-names></name><name><surname>Schreyer</surname><given-names>HM</given-names></name><name><surname>Onken</surname><given-names>A</given-names></name><name><surname>Rozenblit</surname><given-names>F</given-names></name><name><surname>Khani</surname><given-names>MH</given-names></name><name><surname>Krishnamoorthy</surname><given-names>V</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Gollisch</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inference of neuronal functional circuitry with spike-triggered non-negative matrix factorization</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>149</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00156-9</pub-id><pub-id pub-id-type="pmid">28747662</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="software"><person-group person-group-type="author"><collab>lucastheis</collab></person-group><year iso-8601-date="2016">2016</year><data-title>C2s</data-title><version designator="swh:1:rev:9f12398a33a17e557b4a689f1dce902123c6f2eb">swh:1:rev:9f12398a33a17e557b4a689f1dce902123c6f2eb</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2434f9554d743b4715f82ce6a26b3a9e07368212;origin=https://github.com/lucastheis/c2s;visit=swh:1:snp:507ec9a7abe956e3292ecfc341605b14b0eb47ff;anchor=swh:1:rev:9f12398a33a17e557b4a689f1dce902123c6f2eb">https://archive.softwareheritage.org/swh:1:dir:2434f9554d743b4715f82ce6a26b3a9e07368212;origin=https://github.com/lucastheis/c2s;visit=swh:1:snp:507ec9a7abe956e3292ecfc341605b14b0eb47ff;anchor=swh:1:rev:9f12398a33a17e557b4a689f1dce902123c6f2eb</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lurz</surname><given-names>KK</given-names></name><name><surname>Bashiri</surname><given-names>M</given-names></name><name><surname>Willeke</surname><given-names>K</given-names></name><name><surname>Jagadish</surname><given-names>AK</given-names></name><name><surname>Wang</surname><given-names>E</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Generalization in data-driven models of primary visual cortex</article-title><source>Neuroscience</source><volume>01</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1101/2020.10.05.326256</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>McIntosh</surname><given-names>LT</given-names></name><name><surname>Tanaka</surname><given-names>H</given-names></name><name><surname>Grant</surname><given-names>S</given-names></name><name><surname>Kastner</surname><given-names>DB</given-names></name><name><surname>Melander</surname><given-names>JB</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Brezovec</surname><given-names>LE</given-names></name><name><surname>Wang</surname><given-names>JH</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Interpreting the retinal neural code for natural scenes: from computations to neurons</article-title><source>Neuron</source><volume>111</volume><fpage>2742</fpage><lpage>2755</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.06.007</pub-id><pub-id pub-id-type="pmid">37451264</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mani</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Circuit mechanisms of a retinal ganglion cell with stimulus-dependent response latency and activation beyond its dendrites</article-title><source>Current Biology</source><volume>27</volume><fpage>471</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.12.033</pub-id><pub-id pub-id-type="pmid">28132812</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Vision: A Computational Investigation Into the Human Representation and Processing of Visual Information</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martersteck</surname><given-names>EM</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Evarts</surname><given-names>M</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Duan</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Ouellette</surname><given-names>B</given-names></name><name><surname>Royall</surname><given-names>JJ</given-names></name><name><surname>Stoecklin</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Diverse central projection patterns of retinal ganglion cells</article-title><source>Cell Reports</source><volume>18</volume><fpage>2058</fpage><lpage>2072</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.01.075</pub-id><pub-id pub-id-type="pmid">28228269</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masland</surname><given-names>RH</given-names></name><name><surname>Martin</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The unsolved mystery of vision</article-title><source>Current Biology</source><volume>17</volume><fpage>R577</fpage><lpage>R582</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.05.040</pub-id><pub-id pub-id-type="pmid">17686423</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>LT</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning models of the retinal response to natural scenes</article-title><conf-name>Neural Information Processing Systems</conf-name><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1172/JCI44752.288</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>AF</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two distinct types of eye-head coupling in freely moving mice</article-title><source>Current Biology</source><volume>30</volume><fpage>2116</fpage><lpage>2130</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.042</pub-id><pub-id pub-id-type="pmid">32413309</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michael</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields of single optic nerve fibers in a mammal with an all-cone retina. I: contrast-sensitive units</article-title><source>Journal of Neurophysiology</source><volume>31</volume><fpage>249</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1152/jn.1968.31.2.249</pub-id><pub-id pub-id-type="pmid">4879855</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mouland</surname><given-names>JW</given-names></name><name><surname>Pienaar</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>C</given-names></name><name><surname>Watson</surname><given-names>AJ</given-names></name><name><surname>Lucas</surname><given-names>RJ</given-names></name><name><surname>Brown</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extensive cone-dependent spectral opponency within a discrete zone of the lateral geniculate nucleus supporting mouse color vision</article-title><source>Current Biology</source><volume>31</volume><fpage>3391</fpage><lpage>3400</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.05.024</pub-id><pub-id pub-id-type="pmid">34111401</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Münch</surname><given-names>TA</given-names></name><name><surname>da Silveira</surname><given-names>RA</given-names></name><name><surname>Siegert</surname><given-names>S</given-names></name><name><surname>Viney</surname><given-names>TJ</given-names></name><name><surname>Awatramani</surname><given-names>GB</given-names></name><name><surname>Roska</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Approach sensitivity in the retina processed by a multifunctional neural circuit</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1308</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1038/nn.2389</pub-id><pub-id pub-id-type="pmid">19734895</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Mordvintsev</surname><given-names>A</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Feature visualization</article-title><source>Distill</source><volume>2</volume><elocation-id>e0007</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00007</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piscopo</surname><given-names>DM</given-names></name><name><surname>El-Danaf</surname><given-names>RN</given-names></name><name><surname>Huberman</surname><given-names>AD</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Diverse visual features encoded in mouse lateral geniculate nucleus</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>4642</fpage><lpage>4656</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5187-12.2013</pub-id><pub-id pub-id-type="pmid">23486939</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Prechelt</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>Early stopping - but when?</chapter-title><person-group person-group-type="editor"><name><surname>Prechelt</surname><given-names>L</given-names></name></person-group><source>Neural Networks:Tricks of the Trade. Lecture Notes in Computer Science</source><publisher-name>Springer</publisher-name><fpage>55</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1007/3-540-49430-8_3</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Klindt</surname><given-names>D</given-names></name><name><surname>Kautzky</surname><given-names>M</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Schaeffel</surname><given-names>F</given-names></name><name><surname>Rifai</surname><given-names>K</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Natural environment statistics in the upper and lower visual field are reflected in mouse retinal specializations</article-title><source>Current Biology</source><volume>31</volume><fpage>3233</fpage><lpage>3247</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.05.017</pub-id><pub-id pub-id-type="pmid">34107304</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Klindt</surname><given-names>DA</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Gonschorek</surname><given-names>D</given-names></name><name><surname>Hoefling</surname><given-names>L</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Efficient coding of natural scenes improves neural system identification</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011037</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011037</pub-id><pub-id pub-id-type="pmid">37093861</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rheaume</surname><given-names>BA</given-names></name><name><surname>Jereen</surname><given-names>A</given-names></name><name><surname>Bolisetty</surname><given-names>M</given-names></name><name><surname>Sajid</surname><given-names>MS</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Renna</surname><given-names>K</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name><name><surname>Robson</surname><given-names>P</given-names></name><name><surname>Trakhtenberg</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Single cell transcriptome profiling of retinal ganglion cells identifies cellular subtypes</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2759</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05134-3</pub-id><pub-id pub-id-type="pmid">30018341</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Bauer</surname><given-names>Y</given-names></name><name><surname>Kotkat</surname><given-names>AH</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mouse dLGN receives functional input from a diverse population of retinal ganglion cells with limited convergence</article-title><source>Neuron</source><volume>102</volume><fpage>462</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.040</pub-id><pub-id pub-id-type="pmid">30799020</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Jun</surname><given-names>NY</given-names></name><name><surname>Davis</surname><given-names>EL</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inter-mosaic coordination of retinal receptive fields</article-title><source>Nature</source><volume>592</volume><fpage>409</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03317-5</pub-id><pub-id pub-id-type="pmid">33692544</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-triggered neural characterization</article-title><source>Journal of Vision</source><volume>6</volume><fpage>484</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1167/6.4.13</pub-id><pub-id pub-id-type="pmid">16889482</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>GW</given-names></name><name><surname>Swygart</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Circuits for feature selectivity in the inner retina</chapter-title><person-group person-group-type="editor"><name><surname>Schwartz</surname><given-names>GW</given-names></name></person-group><source>The Senses: A Comprehensive Reference</source><publisher-name>Elsevier</publisher-name><fpage>275</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-809324-5.24186-2</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2021">2021</year><chapter-title>Chapter 16 - color processing</chapter-title><person-group person-group-type="editor"><name><surname>Schwartz</surname><given-names>GW</given-names></name></person-group><source>In Retinal Computation</source><publisher-name>Academic Press</publisher-name><fpage>288</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-819896-4.00017-2</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>G</given-names></name><name><surname>Ala-Laurila</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>sa-labs-extension</data-title><version designator="ccde6eb">ccde6eb</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension">https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension</ext-link></element-citation></ref><ref id="bib81"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Schwartz-AlaLaurila-Labs</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Sa-labs-extension</data-title><version designator="swh:1:rev:6ffbbc63bd3b1f558db2dd8bf604ae12983491e9">swh:1:rev:6ffbbc63bd3b1f558db2dd8bf604ae12983491e9</version><source>Software Heriatge</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:aaf6ba71844a9da2a3433a5d9594e04b61f973c1;origin=https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension;visit=swh:1:snp:5f2147554b856b650bf9b3e7b7c7048ad4bf6624;anchor=swh:1:rev:6ffbbc63bd3b1f558db2dd8bf604ae12983491e9">https://archive.softwareheritage.org/swh:1:dir:aaf6ba71844a9da2a3433a5d9594e04b61f973c1;origin=https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension;visit=swh:1:snp:5f2147554b856b650bf9b3e7b7c7048ad4bf6624;anchor=swh:1:rev:6ffbbc63bd3b1f558db2dd8bf604ae12983491e9</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname><given-names>T</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title><source>Neural Computation</source><volume>16</volume><fpage>223</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1162/089976604322742010</pub-id><pub-id pub-id-type="pmid">15006095</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname><given-names>TO</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Computational identification of receptive fields</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>103</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170253</pub-id><pub-id pub-id-type="pmid">23841838</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonoda</surname><given-names>T</given-names></name><name><surname>Okabe</surname><given-names>Y</given-names></name><name><surname>Schmidt</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Overlapping morphological and functional properties between M4 and M5 intrinsically photosensitive retinal ganglion cells</article-title><source>The Journal of Comparative Neurology</source><volume>528</volume><fpage>1028</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1002/cne.24806</pub-id><pub-id pub-id-type="pmid">31691279</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stabio</surname><given-names>ME</given-names></name><name><surname>Sabbah</surname><given-names>S</given-names></name><name><surname>Quattrochi</surname><given-names>LE</given-names></name><name><surname>Ilardi</surname><given-names>MC</given-names></name><name><surname>Fogerson</surname><given-names>PM</given-names></name><name><surname>Leyrer</surname><given-names>ML</given-names></name><name><surname>Kim</surname><given-names>MT</given-names></name><name><surname>Kim</surname><given-names>I</given-names></name><name><surname>Schiel</surname><given-names>M</given-names></name><name><surname>Renna</surname><given-names>JM</given-names></name><name><surname>Briggman</surname><given-names>KL</given-names></name><name><surname>Berson</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The M5 cell: a color-opponent intrinsically photosensitive retinal ganglion cell</article-title><source>Neuron</source><volume>97</volume><fpage>150</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.030</pub-id><pub-id pub-id-type="pmid">29249284</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sümbül</surname><given-names>U</given-names></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>McCulloch</surname><given-names>K</given-names></name><name><surname>Becker</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>B</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Masland</surname><given-names>RH</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A genetic and computational approach to structurally classify neuronal types</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>3512</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms4512</pub-id><pub-id pub-id-type="pmid">24662602</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Korympidou</surname><given-names>MM</given-names></name><name><surname>Ran</surname><given-names>Y</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Dalkara</surname><given-names>D</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural circuits in the mouse retina support color vision in the upper visual field</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3481</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17113-8</pub-id><pub-id pub-id-type="pmid">32661226</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szél</surname><given-names>A</given-names></name><name><surname>Röhlich</surname><given-names>P</given-names></name><name><surname>Caffé</surname><given-names>AR</given-names></name><name><surname>Juliusson</surname><given-names>B</given-names></name><name><surname>Aguirre</surname><given-names>G</given-names></name><name><surname>Van Veen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Unique topographic separation of two spectral classes of cones in the mouse retina</article-title><source>The Journal of Comparative Neurology</source><volume>325</volume><fpage>327</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1002/cne.903250302</pub-id><pub-id pub-id-type="pmid">1447405</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>H</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>McIntosh</surname><given-names>LT</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction</article-title><conf-name>Proceedings of the 33rd Conference on Neural Information Processing Systems</conf-name><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.5555/3454287</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theis</surname><given-names>L</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Benchmarking spike rate inference in population calcium imaging</article-title><source>Neuron</source><volume>90</volume><fpage>471</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.014</pub-id><pub-id pub-id-type="pmid">27151639</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tien</surname><given-names>NW</given-names></name><name><surname>Pearson</surname><given-names>JT</given-names></name><name><surname>Heller</surname><given-names>CR</given-names></name><name><surname>Demas</surname><given-names>J</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Genetically identified suppressed-by-contrast retinal ganglion cells reliably signal self-generated visual stimuli</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10815</fpage><lpage>10820</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1521-15.2015</pub-id><pub-id pub-id-type="pmid">26224863</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tien</surname><given-names>NW</given-names></name><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Target-specific glycinergic transmission from VGluT3-expressing amacrine cells shapes suppressive contrast responses in the retina</article-title><source>Cell Reports</source><volume>15</volume><fpage>1369</fpage><lpage>1375</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.04.025</pub-id><pub-id pub-id-type="pmid">27160915</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tien</surname><given-names>NW</given-names></name><name><surname>Vitale</surname><given-names>C</given-names></name><name><surname>Badea</surname><given-names>TC</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Layer-specific developmentally precise axon targeting of transient suppressed-by-contrast retinal ganglion cells</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>7213</fpage><lpage>7221</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2332-21.2022</pub-id><pub-id pub-id-type="pmid">36002262</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trapani</surname><given-names>F</given-names></name><name><surname>Spampinato</surname><given-names>GLB</given-names></name><name><surname>Yger</surname><given-names>P</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Differences in nonlinearities determine retinal cell types</article-title><source>Journal of Neurophysiology</source><volume>130</volume><fpage>706</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1152/jn.00243.2022</pub-id><pub-id pub-id-type="pmid">37584082</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Sanchez Giraldo</surname><given-names>LG</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stimulus- and goal-oriented frameworks for understanding natural vision</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0284-0</pub-id><pub-id pub-id-type="pmid">30531846</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ustyuzhaninov</surname><given-names>I</given-names></name><name><surname>Burg</surname><given-names>MF</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Digital Twin Reveals Combinatorial Code of Non-Linear Computations in the Mouse Primary Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.02.10.479884</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>2060</fpage><lpage>2065</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0517-x</pub-id><pub-id pub-id-type="pmid">31686023</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>E</given-names></name><name><surname>De</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>OFF-transient alpha RGCs mediate looming triggered innate defensive response</article-title><source>Current Biology</source><volume>31</volume><fpage>2263</fpage><lpage>2273</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.03.025</pub-id><pub-id pub-id-type="pmid">33798432</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Elstrott</surname><given-names>J</given-names></name><name><surname>Feller</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two-photon targeted recording of GFP-expressing neurons for light responses and live-cell imaging in the mouse retina</article-title><source>Nature Protocols</source><volume>5</volume><fpage>1347</fpage><lpage>1352</lpage><pub-id pub-id-type="doi">10.1038/nprot.2010.106</pub-id><pub-id pub-id-type="pmid">20595962</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Restivo</surname><given-names>K</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Nix</surname><given-names>AF</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Shinn</surname><given-names>T</given-names></name><name><surname>Nealley</surname><given-names>C</given-names></name><name><surname>Rodriguez</surname><given-names>G</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Deep Learning-Driven Characterization of Single Cell Tuning in Primate Visual Area V4 Unveils Topological Organization</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.05.12.540591</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yilmaz</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid innate defensive responses of mice to looming visual stimuli</article-title><source>Current Biology</source><volume>23</volume><fpage>2011</fpage><lpage>2015</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.08.015</pub-id><pub-id pub-id-type="pmid">24120636</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>L</given-names></name><name><surname>Smith</surname><given-names>RG</given-names></name><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Physiology and morphology of color-opponent ganglion cells in a retina expressing a dual gradient of S and M opsins</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>2706</fpage><lpage>2724</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5471-08.2009</pub-id><pub-id pub-id-type="pmid">19261865</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>IJ</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title><source>PNAS</source><volume>109</volume><fpage>E2391</fpage><lpage>E2398</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211547109</pub-id><pub-id pub-id-type="pmid">22891316</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Klindt</surname><given-names>DA</given-names></name><name><surname>Maia Chagas</surname><given-names>A</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Rogerson</surname><given-names>L</given-names></name><name><surname>Protti</surname><given-names>DA</given-names></name><name><surname>Behrens</surname><given-names>C</given-names></name><name><surname>Dalkara</surname><given-names>D</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The temporal structure of the inner retina at a single glance</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>4399</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-60214-z</pub-id><pub-id pub-id-type="pmid">32157103</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86860.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.11.30.518492" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.11.30.518492"/></front-stub><body><p>This study presents a fundamental and very technically strong dataset of mouse ganglion cells responding to natural stimuli that include more natural chromatic properties. Fits of convolutional neural networks to experimental measurements highlighted a novel form of color opponency in suppressed-by-contrast ganglion cells. More generally, the work provides a compelling example of how modern experimental and computational tools can be used to generate and test hypotheses about sensory function under natural conditions.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86860.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Baccus</surname><given-names>Stephen A</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Wei</surname><given-names>Wei</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>The University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.11.30.518492">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.11.30.518492v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A chromatic feature detector in the retina signals visual context changes&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Lois Smith as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>All reviewers were impressed by the technical aspects of the work. Three key issues emerged in review and were emphasized in discussions among the reviewers. First, the MEI consists of a single linear visual feature to represent a neuron's spatial and temporal stimulus selectivity. Such a linear feature cannot capture many key nonlinear aspects of retinal signaling (e.g. direction selectivity among many others). This limitation needs to be clearer in the paper. It is, for example, important that a reader does not come away thinking that the paper provides a general approach to identifying stimulus selectivity. Second, the MEIs in some ways resemble more standard measures of receptive field properties and in others differ strongly. A more complete comparison is needed of MEIs vs. standard measures, as well as a discussion of why they might differ (which could be related to the first main point above). Third, the paper emphasizes the possible role of these cells in horizon detection, but the analyses that support this role are based on ground-to-sky transitions, and the horizon itself was excluded from the stimulus set. Hence it does not seem possible to evaluate the role of horizon detection. It is important that discussions of ethological function are more grounded in the analyses provided in the paper.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Line 170-171: Suggest modifying the sentence &quot;This established that …&quot; to be more objective. It's not clear that a correlation of 0.5 supports that sentence.</p><p>Line 173: Isn't the linearized CNN equivalent to a straight linear filter model? If so I would state that – it's a simpler description. If not, I would clarify how it is different.</p><p>Line 357-359: What is the indication that the warping is in the color opponent region?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. Line 856. Why were clips restricted to a certain mean intensity range, and how did this influence the analysis? The concern is that allowing a more natural range of means would then violate the sky-ground transition specificity, as cells might fire in a broader range of conditions.</p><p>2. Figure S2. Add UV / Green labels to the two grayscale maps as in Figure 4a.</p><p>3. Figure 2A. Number of channels should be added to the figure.</p><p>4. Figure 2A. 'Receptive fields' label is only indicated in the last layer but is defined by the whole network. This is confusing, perhaps for the last layer something should be labelled with something more specifically relating to spatial localization?</p><p>5. Figure 2A. Is the Kronecker (or 'tensor') product symbol really intended? If so, please state so in the legend and clarify in methods how this relates to convolution implementation. If it is just generally meant to be convolution, a different symbol should be used and stated. In other words, it helps understanding to be formally correct with symbols, with more formalized mathematics or physics usage preferred to less regulated machine learning usage.</p><p>6. Figure 2B. How can so many cells be above the maximum set by the cell's response reliability?</p><p>7. Figure 2c. A 'linear model' is stated, but from the methods, it seems like an LN model (the final threshold is still present). If it really is a linear model, this is an inappropriate straw man (a threshold should be included) but if it is an LN model, that should be stated correctly.</p><p>8. Line 240 -241. Due to &quot;the fixed contrast budget across channels&quot;, the contrast of most MEIs is shifted toward the UV. Please explain what is meant by the fixed contrast budget and its effects.</p><p>9. Figure 4G. The block structure of Figure 4G is hard to interpret, because it is not clear what the expectation is. For many of the cells, the MEI is not the most effective input, not even for the model. Attention is focused on the one cell that has the most distinctive response to its MEI, but is this just a random occurrence? Some guidance clarifying the interpretation of this confusion matrix would be helpful, or whether it is just interesting but we are not supposed to have any real interpretation of it.</p><p>10. Line 255. If only 2/3 of the cells have color opponent MEIs, are they really a cell class? Does this call into question the classification approach or the MEI approach?</p><p>11. Related to the difficulties of the representation analysis, it is not clear what 'sacrifice' occurs in favor of chromatic discrimination. If cell 28 did not exist, some chromatic discrimination would be lost, but other cell types would still be there to represent other features.</p><p>12. Figure 7d. the retinal cross-section image is confusing. Due to retinal warping, the ChAT bands droop below the IPL boundaries indicated, and therefore the arborization level seen in the maximal projection is misleading. It would be better to use the ChAT bands to correct for warping at each spatial location so that the dendritic arborization could be interpreted.</p><p>13. Line 609. This section relates to a previous claim, summarized as 'the dumber the animal the smarter the retina'. This is regarded as a myth, and experimental evidence does not support this (see Gollisch and Meister, 2010 discussion, point 2). It would be better for the field not to revive this idea.</p><p>14. Line 624. There is a misstatement about how this paper contributes to 'how' a computation is implemented, as there is no mechanistic information here about the circuitry.</p><p>15. Line 871. Please make clear that the two channels are chromatic, and a different word should be used than 'channel' because the layer has 16 channels by the usual meaning.</p><p>16. Line 874. More detail should be given about the Fourier parameterization.</p><p>17. The limitations of the slow frame rate should be more clearly acknowledged.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;A chromatic feature detector in the retina signals visual context changes&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Lois Smith (Senior Editor), a Reviewing Editor and the three original reviewers.</p><p>All the reviewers agreed that the manuscript has improved in revision and appreciated the new data and analyses that were added. There are a few remaining issues that need to be addressed, as outlined in the reviews below. Most important is clarifying the limitations of the MEI analysis earlier in the paper. These are detailed in the individual reviews below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This paper has improved with the new analyses and revisions added in response to reviews. These changes have clarified several issues, provided stronger evidence for others, and brought the results and text closer in alignment. A few issues have either emerged or been highlighted in the revision:</p><p>MEI and temporal tuning. The text at times could be read as saying that the MEIs reflect a better way to measure tuning properties. A concern about this terminology is the dependence of the MEI on the approach used to optimize responses (e.g. on the time window used in that approach). For example, the temporal frequencies that contribute to the time course of the MEI are quite low – likely reflecting the optimization procedure. One suggestion is to clarify in the section starting on line 204 that the MEIs reflect a combination of a cell's own properties and the optimization procedure (as you have already for the oscillations in MEIs for some cells).</p><p>Figure 6b, c: What do the contrasts beyond -1 or 1 mean?</p><p>Figure 6e, f: This cell does not look opponent, with both UV and green producing On responses. Is this the only cell this experiment was performed on? If not, were other cells more in alignment with the predictions from the model?</p><p>Figure 7i: The colors are hard to see and match to those in the rest of the figure. In addition, the gray bars and labels should be defined in the caption.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The paper has improved but still does not acknowledge its limitations sufficiently.</p><p>1. With respect to the difference between the MEI and the linear receptive field, there was no misunderstanding in the previous review, and the current manuscript still does not acknowledge the limitations of the MEI analysis. This point could have been made more explicit in the previous review, and so the following is an attempt to do so.</p><p>By analogy, in a topographic map of the world, the MEI is the location of Mt. Everest. The linear receptive field is a plane fit to the direction of the steepest ascent fit to the world map, additionally including an average slope. A Linear-Nonlinear model allows variation in height along that single direction. But a full understanding between the relationship between change in spatial position and height requires the full two-dimensional map.</p><p>In the retina, and in a CNN model of the retina, parallel rectified pathways create sensitivity to many more different directions in stimulus space, commonly referred to as features. Each separate interneuron with a threshold potentially creates a different feature or dimension, with firing rate and sensitivity varying in each point of the high-dimensional space spanned by the set of features.</p><p>Linear-Nonlinear models and MEIs only encode sensitivity to a single feature. However, multiple distinct features are required to produce a wide set of important phenomenon, including nonlinear subunits that cause sensitivity to fine textures, direction selectivity for both light and dark objects (On-Off), object motion sensitivity, latency coding, the omitted stimulus response, responses to motion reversal, and pattern adaptation (reviewed in Gollisch and Meister, 2010). All of these properties rely on multiple distinct features and their interactions. Any analysis based solely on an MEI necessarily abandons consideration of how sensitivity along these different features combines to produce a computation. The new Figure 6c, which examines two stimulus directions near the MEI is an improvement, but it is only two directions in one region of stimulus space.</p><p>There is substantial concern that readers will misinterpret the 'Most Exciting' input to mean the 'Most Important'. It would therefore serve the field for a clear statement to be made that analysis of MEIs alone (1) will not capture interactions of multiple nonlinear neural pathways, (2) therefore will not capture nonlinear interactions between multiple stimulus features, and (3) will consequently not explain the set of phenomena that rely on these neural pathways and stimulus features.</p><p>In response to the authors rebuttal, to clarify the nonlinear analysis that one might do beyond the analysis of MEIs to identify multiple dimensions, these would include Spike-Triggered Covariance (Fairhall et al., 2006), Maximally Informative Dimensions (Sharpee et al., 2003), Maximum Noise Entropy (Globerson et al., 2009), Nonnegative Matrix Factorization (Liu et al., 2017), proximal algorithms (Maheswaranathan et al., 2018), and model reduction approaches to reduce to the CNN model to fewer dimensions (Maheswaranathan et al., 2023) among others.</p><p>2. It should be made explicit for each analysis whether it is based on measured responses or stimuli presented to the model. In Figure 7, a proper signal detection analysis cannot be performed without accounting for noise on single trials. It is unclear whether analyzed responses are directly from data, or from stimuli presented to the model. For extrapolation to different speeds, these are clearly from new stimuli presented to the model. It should be stated that without accounting for measured noise, ROC analyses will overestimate detection of ground-sky transitions.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86860.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>All reviewers were impressed by the technical aspects of the work. Three key issues emerged in review and were emphasized in discussions among the reviewers.</p></disp-quote><p>We thank editors and reviewers for their constructive input.</p><disp-quote content-type="editor-comment"><p>First, the MEI consists of a single linear visual feature to represent a neuron's spatial and temporal stimulus selectivity. Such a linear feature cannot capture many key nonlinear aspects of retinal signaling (e.g. direction selectivity among many others). This limitation needs to be clearer in the paper. It is, for example, important that a reader does not come away thinking that the paper provides a general approach to identifying stimulus selectivity.</p><p>Second, the MEIs in some ways resemble more standard measures of receptive field properties and in others differ strongly. A more complete comparison is needed of MEIs vs. standard measures, as well as a discussion of why they might differ (which could be related to the first main point above).</p></disp-quote><p>We think that there is a misunderstanding about the differences between the MEI approach and the more standard measures of receptive field (RF) properties the reviewers refer to. We apologize if this was not clear enough in the original manuscript.</p><p>In the following, we will first explain the differences between MEIs and linear RF estimates in more detail. We then show a new analysis (also included in the revised manuscript) that shows more clearly that for G28/tSbC cells (the chromatic feature detectors described in our paper) the MEI is not the same as the linear filter. In fact, the linear filter does not recover the chromatic selectivity reliably, and hence a nonlinear approach such as the MEIs is necessary to characterize G28/tSbC RGCs adequately.</p><p>The fundamental difference between more standard measures of receptive field (RF) properties and MEIs is the following: Classical approaches for estimating the RF typically aim at capturing a cell’s linear filter. The estimated linear filter is an approximation of how the cell processes incoming stimuli; RF estimates can therefore be used as linear filters, for example in Linear-Nonlinear (LN) models that predict cell responses to stimuli. The MEI approach, conversely, is an approach for estimating the optimal stimulus for a cell: an optimisation procedure is leveraged to maximize an objective function, which, in the simplest case, can be the model neuron response prediction. However, the objective function can be any function of the predicted response. Importantly, we would like to point out that MEIs do not simply approximate nonlinear stimulus selectivity by a linear filter, but instead are the image that leads to the peak in the nonlinear activation landscape, which is why they are well-suited for characterizing nonlinear systems, such as the retina. For a truly linear system, the two coincide, but not for the retina, because, as the reviewers rightly point out, the retina is nonlinear. Crucially, the MEI approach can also recover stimulus selectivity if a cell is selective for a combination of stimulus features, since the combination of features can be described as a point in high-dimensional stimulus space.</p><p>To demonstrate the necessity of a non-linear approach (i.e., MEIs) for recovering the chromatic opponency of RGC type G28, we tested whether we could recover this feature using an LN model (i.e., a CNN model with no nonlinearities between convolutional layers, but with a final thresholding nonlinearity in place). Indeed, with the LN model, only 9 out of 36 G28 cells were identified as color-opponent (nonlinear model: 24 out of 36; see new Figure 6a). This result demonstrates that the preference for chromatic contrast of G28 cells does not result from a color opponent linear filter (i.e. negative weights for green, and positive weights for UV), which could also be captured by an LN-type model. Rather, there must be a nonlinear dependence between chromatic contrast (of the stimulus) and chromatic selectivity (of the cell).</p><p>To understand the nature of this dependence, we further expanded the analysis by estimating the model neurons’ tuning to color contrast around the maximum (the MEI). To this end, we mapped the model neurons’ response in 2D chromatic contrast space (new Figure 6b-d), revealing that, indeed, G28/tSbC RGCs have a nonlinear tuning for color contrast: their selectivity for color contrast changes depending on the location in the space of color contrast. We verified these model predictions by electrically recording from morphologically identified G28/tSbC cells (see new Figure 6e,f, also above).</p><p>In summary, in the revised manuscript, we (a) added a more detailed explanation of the difference between more conventional linear approaches for estimating stimulus selectivity on the one hand, and the MEI approach as a more general approach to identify nonlinear stimulus selectivity on the other hand. We also offered intuitions on how to interpret the MEIs, especially for those features that differ from linear RF measures (e.g. the temporal oscillations). We also (b) discussed the limitations of the MEI approach (see also replies to individual reviewer comments below). Further, we (c) added an analysis (and new electrophysiological data) of the chromatic contrast selectivity in tSbC RGC, demonstrating that this is a nonlinear feature, which could only be recovered using a nonlinear model.</p><disp-quote content-type="editor-comment"><p>Third, the paper emphasizes the possible role of these cells in horizon detection, but the analyses that support this role are based on ground-to-sky transitions, and the horizon itself was excluded from the stimulus set. Hence it does not seem possible to evaluate the role of horizon detection. It is important that discussions of ethological function are more grounded in the analyses provided in the paper.</p></disp-quote><p>We acknowledge that we were not clear enough about the proposed function for this cell type. What we suggest is that G28/tSbC cells contribute to detecting context changes which are marked by a change in color contrast, such as transitions across the horizon and specifically, from ground to sky. We do not think that the cells are detecting the horizon itself. We improved the revised manuscript in this respect.</p><p>To back-up our hypothesis that G28/tSbC cells play a role in detecting context changes, we performed an additional simulation analysis. We sampled 1,000 stimuli from the original mouse movie (Qiu et al. Curr Biol 2021) by moving a window across a scene (Figure 7h), mimicking the kind of stimulus elicited by eye, head, and/or whole body movement. As before, each stimulus mimicked one of four transition types: ground-&gt;ground, sky-&gt;sky, sky-&gt;ground, and ground-&gt;sky. The former two transitions correspond to changes in visual input without context change, whereas the latter two reflect a context change. Importantly, the ground-&gt;sky and sky-&gt;ground transitions in these simulations contain the horizon.</p><p>We then simulated responses for all 32 RGC groups and performed an ROC analysis (analogous to the one already described in the manuscript) to determine how well the different RGC groups could distinguish ground-&gt;sky transitions from the other transitions.</p><p>To test robustness of detection performance across different speeds of transitions, we performed this analysis at different angular velocities (50, 150, 250, and 350°/s; see Figure 6-supplement 3a,b). Interestingly, while most slow ON RGC types improve their performance with increasing speeds, G28 performs well robustly across all four speeds.</p><p>In the revised manuscript, we added this new analysis (cf. Figure 7h-j, Figure 6-supplement 3) and discuss the implications of these results in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Line 170-171: Suggest modifying the sentence &quot;This established that …&quot; to be more objective. It's not clear that a correlation of 0.5 supports that sentence.</p></disp-quote><p>We removed the sentence.</p><disp-quote content-type="editor-comment"><p>Line 173: Isn't the linearized CNN equivalent to a straight linear filter model? If so I would state that – it's a simpler description. If not, I would clarify how it is different.</p></disp-quote><p>The linearised CNN is equivalent to an LN model. We now state this in the manuscript and refer to this model as the “LN model”.</p><disp-quote content-type="editor-comment"><p>Line 357-359: What is the indication that the warping is in the color opponent region?</p></disp-quote><p>We removed the referenced section from the manuscript (see also above). However, it is visible from Figure 6b-d, and Figure 5-supplement 1 that the warping is in the color opponent region.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. Line 856. Why were clips restricted to a certain mean intensity range, and how did this influence the analysis? The concern is that allowing a more natural range of means would then violate the sky-ground transition specificity, as cells might fire in a broader range of conditions.</p></disp-quote><p>In a pilot study with unrestricted mean intensity range, we found that strong differences in mean intensity between clips dominated RGC responses, so we restricted the mean intensity range. For our additional analysis (Figure 7h-j), however, we did not restrict the range of intensities, and still found G28/tSbC cells to be selective for ground-sky transitions.</p><disp-quote content-type="editor-comment"><p>2. Figure S2. Add UV / Green labels to the two grayscale maps as in Figure 4a.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>3. Figure 2A. Number of channels should be added to the figure.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>4. Figure 2A. 'Receptive fields' label is only indicated in the last layer but is defined by the whole network. This is confusing, perhaps for the last layer something should be labelled with something more specifically relating to spatial localization?</p></disp-quote><p>We changed the label to “spatial RF”.</p><disp-quote content-type="editor-comment"><p>5. Figure 2A. Is the Kronecker (or 'tensor') product symbol really intended? If so, please state so in the legend and clarify in methods how this relates to convolution implementation. If it is just generally meant to be convolution, a different symbol should be used and stated. In other words, it helps understanding to be formally correct with symbols, with more formalized mathematics or physics usage preferred to less regulated machine learning usage.</p></disp-quote><p>Thanks for pointing that out; we replaced the symbol by that for a convolution.</p><disp-quote content-type="editor-comment"><p>6. Figure 2B. How can so many cells be above the maximum set by the cell's response reliability?</p></disp-quote><p>This is because the test set correlation and the response reliability are calculated in different ways (see Methods) such that their absolute values are not directly comparable. We expect a correlation between the two metrics, which the panel is meant to illustrate, but not a direct correspondence in terms of absolute values.</p><disp-quote content-type="editor-comment"><p>7. Figure 2c. A 'linear model' is stated, but from the methods, it seems like an LN model (the final threshold is still present). If it really is a linear model, this is an inappropriate straw man (a threshold should be included) but if it is an LN model, that should be stated correctly.</p></disp-quote><p>We apologize for the confusion, as stated above, it is indeed a CNN model equivalent to an LN model. We now clearly state that in the paper and changed the label to “LN model”.</p><disp-quote content-type="editor-comment"><p>8. Line 240 -241. Due to &quot;the fixed contrast budget across channels&quot;, the contrast of most MEIs is shifted toward the UV. Please explain what is meant by the fixed contrast budget and its effects.</p></disp-quote><p>This statement was slightly misleading; the underlying reason for the UV-shift in MEI contrast is the dominance of UV-sensitive S-opsin in the ventral retina, making stimuli in the UV channel much more effective at eliciting RGC responses. The sharing of the fixed contrast budget across channels (see also Franke et al. 2022, Nature) was the <italic>methodological</italic> feature that led to this biological fact being reflected in the MEIs. In other words, the MEIs generated like this are the optimal stimuli, <italic>conditioned on having a shared fixed contrast budget</italic>. The optimal stimuli conditioned on having separate contrast budgets for the two color channels are expected to look different, i.e. with more contrast in the green channel.</p><p>We updated the Results section accordingly.</p><disp-quote content-type="editor-comment"><p>9. Figure 4G. The block structure of Figure 4G is hard to interpret, because it is not clear what the expectation is. For many of the cells, the MEI is not the most effective input, not even for the model. Attention is focused on the one cell that has the most distinctive response to its MEI, but is this just a random occurrence? Some guidance clarifying the interpretation of this confusion matrix would be helpful, or whether it is just interesting but we are not supposed to have any real interpretation of it.</p></disp-quote><p>We have a paragraph in the manuscript (original as well as revised) dedicated to this panel. In brief, we wanted to illustrate that RGCs show a strong response to their own group’s MEI, a weaker response to the MEIs of functionally related groups, and no response to MEIs of groups with different response profiles (Figure 4g top). Conversely, RGC groups from opposing regions in response space showed no response to each others’ MEIs. The model’s predictions showed a similar pattern (Figure 4g, bottom), thereby validating the model’s ability to generalize to the MEI stimulus regime.</p><disp-quote content-type="editor-comment"><p>10. Line 255. If only 2/3 of the cells have color opponent MEIs, are they really a cell class? Does this call into question the classification approach or the MEI approach?</p></disp-quote><p>We think that this is due to the classifier we employed, which did not use chromatic selectivity for classification (but chirp and moving bar responses, as well as soma size and direction/orientation selectivity). It was trained on the original dataset (Baden et al.. Nature 2016), where we decided to merge two clusters (38 and 39) to G28 (see Figure 2a there). This was done because the clusters only differed slightly in their direction and orientation selectivity index and we wanted to define functional RGC groups conservatively. Therefore, it is possible that G28 may collect other cells with similar chirp and moving bar responses but no color opponency. Similarly, the fact that G27 – with very similar chirp and moving bar responses – contains some color-opponent cells may also be explained by the original classifier.</p><disp-quote content-type="editor-comment"><p>11. Related to the difficulties of the representation analysis, it is not clear what 'sacrifice' occurs in favor of chromatic discrimination. If cell 28 did not exist, some chromatic discrimination would be lost, but other cell types would still be there to represent other features.</p></disp-quote><p>Our argument here is in terms of resources spent on a cell that monitors a particular region of stimulus space, while not monitoring the rest of stimulus space. In other words, we agree with the reviewer: If G28/tSbC cells did not exist, chromatic discrimination would be lost – but its existence suggests that the resources spent on it are relevant. However, as discussed above, we removed the referenced section from the manuscript.</p><disp-quote content-type="editor-comment"><p>12. Figure 7d. the retinal cross-section image is confusing. Due to retinal warping, the ChAT bands droop below the IPL boundaries indicated, and therefore the arborization level seen in the maximal projection is misleading. It would be better to use the ChAT bands to correct for warping at each spatial location so that the dendritic arborization could be interpreted.</p></disp-quote><p>We replaced the image by one of a different tSbC cell, where the tissue displayed less warping.</p><disp-quote content-type="editor-comment"><p>13. Line 609. This section relates to a previous claim, summarized as 'the dumber the animal the smarter the retina'. This is regarded as a myth, and experimental evidence does not support this (see Gollisch and Meister, 2010 discussion, point 2). It would be better for the field not to revive this idea.</p></disp-quote><p>There may not be experimental data supporting the idea that a larger “cortical” computational capacity results in “simpler” retinal receptive fields, but we think that recent modeling results (Lindsay et al. ICLR 2019) are compelling enough to consider this idea. Nonetheless, we amended this part of the discussion.</p><disp-quote content-type="editor-comment"><p>14. Line 624. There is a misstatement about how this paper contributes to 'how' a computation is implemented, as there is no mechanistic information here about the circuitry.</p></disp-quote><p>Changed.</p><disp-quote content-type="editor-comment"><p>15. Line 871. Please make clear that the two channels are chromatic, and a different word should be used than 'channel' because the layer has 16 channels by the usual meaning.</p></disp-quote><p>Thanks for pointing this out – we changed it to “input channels (green and UV)”, since the two color channels are the input channels in our case.</p><disp-quote content-type="editor-comment"><p>16. Line 874. More detail should be given about the Fourier parameterization.</p></disp-quote><p>Thanks for pointing this out – we changed it to “input channels (green and UV)”, since the two color channels are the input channels in our case.</p><disp-quote content-type="editor-comment"><p>17. The limitations of the slow frame rate should be more clearly acknowledged.</p></disp-quote><p>We are not sure to which specific limitations the reviewer refers. In any case, despite the low frame rate, the Ca<sup>2+</sup> responses can be related to the spike rate (e.g., Román Rosón et al. Neuron 2019; Trapani et al. J Neurophysiol 2023, see their Figure Suppl. 4). We added a sentence to the Methods section.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>All the reviewers agreed that the manuscript has improved in revision and appreciated the new data and analyses that were added. There are a few remaining issues that need to be addressed, as outlined in the reviews below. Most important is clarifying the limitations of the MEI analysis earlier in the paper. These are detailed in the individual reviews below.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>This paper has improved with the new analyses and revisions added in response to reviews. These changes have clarified several issues, provided stronger evidence for others, and brought the results and text closer in alignment. A few issues have either emerged or been highlighted in the revision:</p></disp-quote><p>We thank the reviewer for appreciating our revisions</p><disp-quote content-type="editor-comment"><p>MEI and temporal tuning. The text at times could be read as saying that the MEIs reflect a better way to measure tuning properties. A concern about this terminology is the dependence of the MEI on the approach used to optimize responses (e.g. on the time window used in that approach). For example, the temporal frequencies that contribute to the time course of the MEI are quite low – likely reflecting the optimization procedure. One suggestion is to clarify in the section starting on line 204 that the MEIs reflect a combination of a cell's own properties and the optimization procedure (as you have already for the oscillations in MEIs for some cells).</p></disp-quote><p>Thank you for pointing out that we came across as implying that MEIs are somehow “better” than other approaches. That is not what we wanted to imply. It is one method of many that can reveal nonlinear tuning properties and we used it to reveal a novel such tuning property. In the revised version of the manuscript, we have carefully rephrased all places where this impression might have been created.</p><disp-quote content-type="editor-comment"><p>Figure 6b, c: What do the contrasts beyond -1 or 1 mean?</p></disp-quote><p>For this analysis, we created visual stimuli in the two-dimensional subspace spanned by the MEI’s green and UV channel, respectively. In other words, we used the MEI’s green and UV channel as basis vectors and took a linear combination with different weights. These weights represent green and UV contrast, respectively (with respect to the MEI’s contrast). We then computed the predicted response of the model to these stimuli, which is shown in the figure. Note that we flipped the sign of the green channel’s basis vector, such that a positive weight corresponds to an ON-type stimulus and a negative weight to an OFF-type stimulus. That means the MEI is at (–1,1) in this plot.</p><p>In the previous visualisation, the axes were labelled to indicate the absolute contrast of the resulting stimuli, which could be outside the range (-1, 1), depending on the original MEIs contrast. We now realise that contrasts outside the -1…1 range may be confusing. We have therefore changed the axis labels to indicate the multiplicative scalar, and hence range from -1 to 1. We clarify this in the legend and explain in more detail in the methods section. In addition, we now use the same colour schemes for panels (b, c) and (e).</p><p>Note that, in the previous version of the manuscript, we chose a slightly different subspace (spanned by the UV component of the MEI both in the green and UV channel). For ease of interpretability, we now changed to the subspace described above. As a consequence, the panels in Figure 6 look slightly different.</p><disp-quote content-type="editor-comment"><p>Figure 6e, f: This cell does not look opponent, with both UV and green producing On responses. Is this the only cell this experiment was performed on? If not, were other cells more in alignment with the predictions from the model?</p></disp-quote><p>We thank the reviewer for this question and the opportunity to clarify. The colour opponency of this cell derives from a suppressive effect of green contrast, which can be best understood when comparing responses to a UV-ON, green-OFF stimulus (top left in panel (e); upper row, second from left in (f)) with the responses to a UV-ON, green-ON stimulus (top right in Figure 6 panel (e); upper row, far right in Figure 6. panel (f)).</p><p>In the manuscript, we previously wrote:</p><p>“This analysis revealed that, indeed, G28 RGCs have a nonlinear tuning for colour contrast: they are strongly UV-selective at lower contrasts, but become colour-opponent, i.e. additionally inhibited by green, for higher contrasts.”</p><p>We now add further explanation about the nature of the colour opponency:</p><p>“We confirmed the model’s predictions about G28’s nonlinear tuning for colour contrast using electrical recordings as described above. The example G28 (tSbC) cells shown in the figure exhibit similar nonlinear tuning in chromatic contrast space. The first example cell's firing rate and, consequently, the tuning curve peak for UV ON-green OFF stimuli (top left in panel e; upper row, second from left in f), and are lower for UV ON-green ON stimuli (top right in panel e; upper row, far right in f), reflecting the suppressive effect of green contrast on the cell's response.”</p><disp-quote content-type="editor-comment"><p>Figure 7i: The colors are hard to see and match to those in the rest of the figure. In addition, the gray bars and labels should be defined in the caption.</p></disp-quote><p>We changed that panel in Figure 7: instead of having all AUC curves in one plot, we now show the curves separately for the example RGC groups (vs. the curve of G28). We hope that this improves the readability of the panel.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The paper has improved but still does not acknowledge its limitations sufficiently.</p></disp-quote><p>We thank the reviewer for appreciating our revisions.</p><disp-quote content-type="editor-comment"><p>1. With respect to the difference between the MEI and the linear receptive field, there was no misunderstanding in the previous review, and the current manuscript still does not acknowledge the limitations of the MEI analysis. This point could have been made more explicit in the previous review, and so the following is an attempt to do so.</p><p>By analogy, in a topographic map of the world, the MEI is the location of Mt. Everest. The linear receptive field is a plane fit to the direction of the steepest ascent fit to the world map, additionally including an average slope. A Linear-Nonlinear model allows variation in height along that single direction. But a full understanding between the relationship between change in spatial position and height requires the full two-dimensional map.</p><p>In the retina, and in a CNN model of the retina, parallel rectified pathways create sensitivity to many more different directions in stimulus space, commonly referred to as features. Each separate interneuron with a threshold potentially creates a different feature or dimension, with firing rate and sensitivity varying in each point of the high-dimensional space spanned by the set of features.</p><p>Linear-Nonlinear models and MEIs only encode sensitivity to a single feature. However, multiple distinct features are required to produce a wide set of important phenomenon, including nonlinear subunits that cause sensitivity to fine textures, direction selectivity for both light and dark objects (On-Off), object motion sensitivity, latency coding, the omitted stimulus response, responses to motion reversal, and pattern adaptation (reviewed in Gollisch and Meister, 2010). All of these properties rely on multiple distinct features and their interactions. Any analysis based solely on an MEI necessarily abandons consideration of how sensitivity along these different features combines to produce a computation. The new Figure 6c, which examines two stimulus directions near the MEI is an improvement, but it is only two directions in one region of stimulus space.</p><p>There is substantial concern that readers will misinterpret the 'Most Exciting' input to mean the 'Most Important'. It would therefore serve the field for a clear statement to be made that analysis of MEIs alone (1) will not capture interactions of multiple nonlinear neural pathways, (2) therefore will not capture nonlinear interactions between multiple stimulus features, and (3) will consequently not explain the set of phenomena that rely on these neural pathways and stimulus features.</p><p>In response to the authors rebuttal, to clarify the nonlinear analysis that one might do beyond the analysis of MEIs to identify multiple dimensions, these would include Spike-Triggered Covariance (Fairhall et al., 2006), Maximally Informative Dimensions (Sharpee et al., 2003), Maximum Noise Entropy (Globerson et al., 2009), Nonnegative Matrix Factorization (Liu et al., 2017), proximal algorithms (Maheswaranathan et al., 2018), and model reduction approaches to reduce to the CNN model to fewer dimensions (Maheswaranathan et al., 2023) among others.</p></disp-quote><p>We thank the reviewer for clarifying the nature of their concerns. We did not mean to imply either that MEIs are the only approach, somehow the best or provide a complete picture of neuronal tuning. It is one method among many that can reveal nonlinear tuning properties and we used it to reveal a novel such tuning property. In the revised version of the manuscript, we have carefully rephrased all places where this impression might have been created. Specifically:</p><p>We rewrote parts of the Introduction and the Discussion to more clearly position the MEI approach as one of many potential analysis approaches, point out its limitations and discuss other approaches as well.</p><p>We added further guidance on how to interpret MEIs (as compared to Linear Receptive Fields) in the Results section.</p><disp-quote content-type="editor-comment"><p>2. It should be made explicit for each analysis whether it is based on measured responses or stimuli presented to the model. In Figure 7, a proper signal detection analysis cannot be performed without accounting for noise on single trials. It is unclear whether analyzed responses are directly from data, or from stimuli presented to the model. For extrapolation to different speeds, these are clearly from new stimuli presented to the model. It should be stated that without accounting for measured noise, ROC analyses will overestimate detection of ground-sky transitions.</p></disp-quote><p>Thank you for catching that this was not clear everywhere. We added labels to Figure 7 making explicit, which results are based on measured responses and which ones are based on model predictions. This is also clearly stated in the text.</p><p>We now additionally state that without accounting for measured noise, ROC analyses will overestimate detection of ground-sky transitions. Still, assuming roughly the same noise level across RGC groups, we can consider the ROC results based on model predictions as an upper bound on the expected detection performance for each RGC group. Therefore, we can interpret relative performance across RGC groups.</p></body></sub-article></article>