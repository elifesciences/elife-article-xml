<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74402</article-id><article-id pub-id-type="doi">10.7554/eLife.74402</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Humans perseverate on punishment avoidance goals in multigoal reinforcement learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-255628"><name><surname>Sharp</surname><given-names>Paul B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4949-1501</contrib-id><email>paul.sharp@mail.huji.ac.il</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-172342"><name><surname>Russek</surname><given-names>Evan M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-238908"><name><surname>Huys</surname><given-names>Quentin JM</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19714"><name><surname>Dolan</surname><given-names>Raymond J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9356-761X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-237556"><name><surname>Eldar</surname><given-names>Eran</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03qxff017</institution-id><institution>The Hebrew University of Jerusalem</institution></institution-wrap><addr-line><named-content content-type="city">Jerusalem</named-content></addr-line><country>Israel</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02704qw51</institution-id><institution>Wellcome Centre for Human Neuroimaging, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Division of Psychiatry, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>24</day><month>02</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e74402</elocation-id><history><date date-type="received" iso-8601-date="2021-10-03"><day>03</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-02-21"><day>21</day><month>02</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Sharp et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Sharp et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-74402-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-74402-figures-v2.pdf"/><abstract><p>Managing multiple goals is essential to adaptation, yet we are only beginning to understand computations by which we navigate the resource demands entailed in so doing. Here, we sought to elucidate how humans balance reward seeking and punishment avoidance goals, and relate this to variation in its expression within anxious individuals. To do so, we developed a novel multigoal pursuit task that includes trial-specific instructed goals to either pursue reward (without risk of punishment) or avoid punishment (without the opportunity for reward). We constructed a computational model of multigoal pursuit to quantify the degree to which participants could disengage from the pursuit goals when instructed to, as well as devote less model-based resources toward goals that were less abundant. In general, participants (<italic>n</italic> = 192) were less flexible in avoiding punishment than in pursuing reward. Thus, when instructed to pursue reward, participants often persisted in avoiding features that had previously been associated with punishment, even though at decision time these features were unambiguously benign. In a similar vein, participants showed no significant downregulation of avoidance when punishment avoidance goals were less abundant in the task. Importantly, we show preliminary evidence that individuals with chronic worry may have difficulty disengaging from punishment avoidance when instructed to seek reward. Taken together, the findings demonstrate that people avoid punishment less flexibly than they pursue reward. Future studies should test in larger samples whether a difficulty to disengage from punishment avoidance contributes to chronic worry.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>anxiety</kwd><kwd>multitask</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010629</institution-id><institution>Fulbright Association</institution></institution-wrap></funding-source><award-id>PS00318453</award-id><principal-award-recipient><name><surname>Sharp</surname><given-names>Paul B</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>1094/20</award-id><principal-award-recipient><name><surname>Eldar</surname><given-names>Eran</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>098362/Z/12/Z</award-id><principal-award-recipient><name><surname>Dolan</surname><given-names>Raymond J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01MH124092</award-id><principal-award-recipient><name><surname>Eldar</surname><given-names>Eran</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01MH125564</award-id><principal-award-recipient><name><surname>Eldar</surname><given-names>Eran</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Israel Binational Science Foundation</institution></institution-wrap></funding-source><award-id>2019801</award-id><principal-award-recipient><name><surname>Eldar</surname><given-names>Eran</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Humans perseverate on previously instructed goals in a novel multigoal reinforcement learning task, and do this to a greater extent for punishment avoidance goals.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Adaptive behavior demands we flexibly shift between pursuit of multiple goals, but disengaging from one goal in order to pursue another is often challenging. Switching between different goals is computationally demanding as it requires us to disengage processing relevant to prior goals and recruit knowledge necessary to determine the best action to pursue new goals. Consider a teenager about to play for a championship of her basketball league, a coveted prize she is poised to attain. As the game begins, she suddenly remembers that earlier that day she again forgot to show up for a school exam, and consequently might end up getting expelled from school. Although current tasks demand she reallocate attention towards the basketball game, she persists in worry about a potential looming disaster awaiting when the game ends.</p><p>One possibility is that managing multiple goals is influenced by the valence of goal outcomes (i.e., goal valence) (<xref ref-type="bibr" rid="bib32">Guitart-Masip et al., 2012</xref>). Thus, people might devote more resources to pursuing goals involving potential punishment than to goals involving potential reward because of a tendency for losses to loom larger in magnitude than objectively equivalent gains (<xref ref-type="bibr" rid="bib54">Novemsky and Kahneman, 2018</xref>). At the same time, people may adapt to their present environment such that a tendency to prioritize punishment avoidance might be attenuated if reward seeking goals are more frequently encountered than punishment avoidance goals. Thus, our first aim was to determine whether computational strategies for multigoal pursuit differ as a function of goal valence. Specifically, we investigated the degree to which individuals engage, and subsequently, disengage reward seeking and punishment avoidance goals under instruction, and how goal engagement and disengagement are impacted by the frequency with which the goals are encountered.</p><p>A striking example of a maladaptive preference for punishment avoidance manifests in individuals with pathological anxiety (<xref ref-type="bibr" rid="bib7">Bar-Haim et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Berenbaum, 2010</xref>; <xref ref-type="bibr" rid="bib27">Gagne and Dayan, 2021</xref>; <xref ref-type="bibr" rid="bib61">Sharp and Eldar, 2019</xref>; <xref ref-type="bibr" rid="bib69">Warren et al., 2021</xref>). Such individuals tend to learn more quickly from punishment than reward (<xref ref-type="bibr" rid="bib5">Aylward et al., 2019</xref>), and this can lead to avoidance of even moderately risky situations (<xref ref-type="bibr" rid="bib14">Charpentier et al., 2017</xref>). Furthermore, evidence suggests that anxiety is associated with failing to terminate planning in relation to potential threats (<xref ref-type="bibr" rid="bib9">Berenbaum et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Hunter et al., 2022</xref>). However, anxiety-associated failures to effectively disengage punishment avoidance goals have not been examined in a task that tests people’s ability to engage or disengage from punishment avoidance goals at will. Such a test is required to disambiguate between underlying computational mechanisms explaining how these failures occur (<xref ref-type="bibr" rid="bib11">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib40">Korn and Bach, 2019</xref>).</p><p>On the one hand, it is possible that in naturalistic settings anxious individuals allocate more resources toward punishment avoidance because they believe the environment demands it, and thus, if given explicit safety signals they would effectively disengage punishment avoidance, perhaps even more so than less anxious individuals (<xref ref-type="bibr" rid="bib71">Wise and Dolan, 2020</xref>). On the other hand, anxious individuals might fail to disengage punishment avoidance even in the presence of explicit safety signals, evincing a more fundamental failure in exercising executive control. Importantly, both hypotheses are consistent with anxious individuals opting for avoidance behavior in approach–avoidance conflict tasks (<xref ref-type="bibr" rid="bib46">Loh et al., 2017</xref>), but diverge in settings where punishment avoidance and reward seeking goals are unambiguously separated in time and space. Thus, our second aim was to explore potential computations involved in disengagement of punishment avoidance goals in anxiety.</p><p>We developed a novel multigoal pursuit task that required participants to learn by trial and error the probabilities that different actions lead to different task features. Learning was incentivized by occasionally coupling certain features with monetary punishment and other features with monetary reward (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Critically, on each trial, participants were instructed either to avoid the punishment feature or to seek the reward feature, and these goals switched frequently, requiring participants to continuously adjust their behavioral policy. Unbeknownst to participants, we manipulated how frequently certain goals were encountered in each task block, allowing us to determine whether more costly decision-making resources are devoted to pursuing more frequent, and thus more reward-impacting, goals in a resource rational (RR) manner (<xref ref-type="bibr" rid="bib45">Lieder and Griffiths, 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Multigoal pursuit task.</title><p>(<bold>A</bold>) Key task components. Participants were instructed to learn the likelihood of observing two features (gold and black circles) after taking each of two actions (pressing ‘g’ or ‘j’ on the keyboard), and integrate this knowledge with instructed trial-specific goals denoting the present reward or punishment value of each feature. There were two possible goals: in one participants were instructed to seek the reward feature (reward feature = +1 point, punishment feature = 0) and in the other to avoid the punishment feature (reward feature = 0, punishment feature = −1 point). Thus, if the goal was to seek reward, participants should have selected the action most likely to lead to the reward feature (gold circle), irrespective of whether the action lead to the punishment feature (as the value of the latter is 0). Critically, whether each of the two features was present was determined independently, that is, for each action there were four different possible outcome configurations (both features present/reward feature only/punishment feature only/both features absent). To pursue goals, participants had to learn via experience four probabilities (left panel, all <italic>p</italic>(feature|action)) comprising the likelihood of observing each feature following each action (i.e., they were never instructed about these probabilities). Continued learning was required because the true probabilities of observing features for different actions drifted across trials according to semi-independent random walks (bottom left). Although participants were instructed with a more neutral narrative (see Methods), here we refer to the gold circle as the reward feature and the black circle as the punishment feature. However, the gold circle was rewarding <italic>only</italic> for reward seeking goal trials (and of no value during punishment avoidance goal trials), whereas the black circle was punishing <italic>only</italic> during punishment avoidance goal trials (and of no value during reward seeking goal trials). In the actual task implementation, the color for the reward and punishment features, and the random walks each feature took, were counterbalanced across participants. (<bold>B</bold>) Phases of a single trial. First, participants were shown both fractals and the current goal, and asked to select an action (‘Decision’). After they took an action (here, clicking ‘j’, denoted by the red outline), participants were shown feedback, which comprised the feature outcomes, the present reward value of each feature, and the total points gained (possible total points were: (1) ‘You lost 1’, (2) 0, or (3) ‘You won 1’). Finally, participants were shown the feature outcomes they would have seen had they chosen the other action (‘Counterfactual’), which could be any of four possible feature combinations. (<bold>C</bold>) Goal abundance manipulation. A totality of 160 trials were divided into two equal length blocks, constituting reward- and punishment-rich contexts. In a reward-rich context, reward seeking trials outnumbered punishment avoidance trials, and the converse was true in a punishment-rich context. Note, both the sequence and order of blocks were counterbalanced across goal types to ensure neither factor could account for prioritizing a specific goal.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig1-v2.tif"/></fig><p>We report evidence that participants relied to varying degrees on three strategies. Whereas a model-based (MB) strategy was employed to learn the probabilities by which actions led to features for the purpose of flexibly pursuing instructed goals, there was also evidence for a model-free strategy that disregarded instructed goals and relied on points won or lost to reinforce actions (<xref ref-type="bibr" rid="bib41">Lai and Gershman, 2021</xref>). Most interestingly, we find evidence for use of a novel strategy we term, ‘goal perseveration’ (GP), whereby participants learn feature probabilities akin to an MB strategy but utilize this knowledge in a less costly and less flexible way, so as to always avoid punishment (even when instructed to seek reward) and to always seek reward (even when instructed to avoid punishment). Strikingly, this GP strategy was used to a greater extent for punishment avoidance, suggesting that disengaging punishment avoidance is harder, perhaps for evolutionarily sensible reasons (<xref ref-type="bibr" rid="bib72">Woody and Szechtman, 2011</xref>). By contrast, the more flexible MB strategy was leveraged to a greater degree during reward seeking. Furthermore, participants flexibly increased MB control toward reward seeking goals when they were more abundant.</p><p>Finally, in a series of exploratory analyses, we sought to determine whether and how anxious individuals express a preference for punishment avoidance goals. In so doing, we found preliminary evidence that the degree of reliance on a GP strategy to avoid punishment was positively associated with dispositional worry, which appears to be unique to those expressing worry and not to individuals with obsessive–compulsive (OC) or somatic anxiety symptoms.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Task description</title><p>We recruited a large online sample of participants (<italic>N</italic> = 192, excluding 56 who did not meet performance criteria; excluded participants did not differ significantly on any psychopathology measure from the retained sample; see Methods) to play an online version of a novel multigoal pursuit task (<xref ref-type="fig" rid="fig1">Figure 1</xref>). At each trial, participants could take two possible actions, defined by fractal images, to seek or avoid certain features. The trial’s goal was defined by the effective reinforcement value of the features, which varied from trial to trial as instructed to participants explicitly at the beginning of each trial. Thus, in reward seeking trials, encountering a ‘reward’ feature (gold circle) gifted the participant one point whereas the ‘punishment’ feature (black circle) was inconsequential (value = 0). By contrast, in punishment avoidance trials, the punishment feature took away one point whereas the reward feature had no value. Note that the reward value of either feature was continuously presented throughout the choice deliberation time (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), ensuring that there should be no reason for participants to forget the present trial’s goal. To determine whether participants adapted their decision-making toward more frequently encountered goals, we designed our task such that one goal was more abundant in each half of the task.</p><p>After participants made a decision, they were shown the choice outcome feature, followed by a counterfactual feature outcome for the choice not made (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The probabilities linking features to actions varied over time, and participants could estimate these continuously drifting probabilities from experience by observing which features actions led to. Presenting both actual and counterfactual outcomes removed the need for participants to explore less-visited actions to gain information, thus ruling out information seeking as a normative explanation for deviations from optimal choice. Of note, this task design differs from influential two-factor learning paradigms (<xref ref-type="bibr" rid="bib52">Mowrer, 1951</xref>) extensively used to study anxiety, in that in our task both action-feature and feature-value associations changed throughout the experiment, mandating continued learning and flexible decision-making.</p></sec><sec id="s2-2"><title>Three computational strategies</title><sec id="s2-2-1"><title>Model based</title><p>We sought to identify computations individuals employed to learn and enact decisions in our task. A suitable computational strategy for this task is to continuously learn which task features follow each action irrespective of the instructed goal, and when deciding which action to take, rely specifically on knowledge about features relevant to the presently instructed goal. This strategy is an instance of a broader family of ‘model-based’ strategies that flexibly use knowledge about which actions lead to which states (<xref ref-type="bibr" rid="bib23">Dolan and Dayan, 2013</xref>). By simulating an artificial MB agent, we show that a unique signature of MB control in our task manifests, when current and previous goals differ (henceforth referred to as ‘goal-switch’ trials), in the way the current goal determines whether observed features in the last trial impact subsequent action. For example, an MB agent will avoid an action leading to a punishment feature in the last trial only when the current instructed goal is to avoid punishment (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, top row). Such behavior cannot be produced by the other strategies discussed subsequently unless the current and previous goals are the same.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral signatures of computational strategies in simulated and real data.</title><p>(<bold>A</bold>) Last outcome effects in simulated data. Each row comprises data generated by simulating one of the candidate computational strategies used to enact decisions in the present task (see Methods for parameters used). Each plot depicts the proportion of times the simulated agent takes a different action than that taken on the last trial (‘switch probability’), as a function of features experienced on the last trial <italic>for the chosen action</italic> (gold/black circles; a gray bar indicates the feature was absent), the previous goal (left vs. right plots), and the current goal (light vs. dark bars). (<bold>B</bold>) Last outcome effects in empirical data. Real participants’ switch probabilities as a function of last trial’s feature outcomes, and current and previous goals. For illustration, we overlay repeated measures <italic>t</italic>-tests of specific MB (difference between blue and black bars) and GP (green bars) predictions, broken down by goal valence. A more thorough analysis of strategies used by participants is shown in panel C. *p &lt; 0.05, **p &lt; 0.01, *****p <italic>&lt;</italic> 10<sup>−5</sup>, *******p <italic>&lt;</italic> 10<sup>−7</sup> . (<bold>C</bold>) Empirical evidence for each strategy. Posterior distributions derived from fitting a Bayesian linear mixed-effects logistic regression evince main effects for MB (blue), GP (green), and MF (red) strategies. Evidence reflects MB and MF were leveraged for punishment avoidance and reward seeking goals whereas GP was leveraged for punishment avoidance goals, with only trending evidence it was used for reward seeking. (<bold>D</bold>) Effect of goal valence on strategy utilization. We estimated goal valence effects by examining the posterior distribution of differences between the parameters in panel C and found evidence indicating model-based utilization was greater for reward seeking, whereas goal-perseveration utilization was greater for punishment avoidance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Resource rational simulations.</title><p>We simulated the average increase in total points per unit increase in utilizing a model-based strategy for reward pursuit and punishment avoidance in either of the two task blocks. In the top row, we compare this unit increase in total points in the reward-rich task block and show that the slope (average total points per unit increase in utilization) is greater for reward (left) than for punishment (right). The opposite is true in the punishment-rich task block (bottom row). There, the slope is greater for punishment (right) than for reward (left). Note, in each subplot, the color of the line denotes the utilization strength for the goal not plotted on the <italic>x</italic>-axis. Each of these lines was generated by averaging over total points won over 2000 plays of the task for agents with a learning rate of 0.2 for chosen and unchosen actions that are purely model based.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Priors for group-level means and variances in our mixed-effects modeling for empirical data.</title><p>For each group-level effect, a wide prior for both the mean (top two rows) and variance (bottom two rows) hyperparameters were derived automatically as a function of partial correlations between each independent and dependent variable.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Postpredictive check.</title><p>We conducted a postpredictive check by creating synthetic data from the best-fitting parameters in our winning model (using medians for skewed distributions and means for nonskewed distributions). We then fit the same Bayesian mixed-effects logistic regression we report in the main text to these synthetic data. Doing so resulted in all but one of the effects we found in the empirical being recapitulated in the synthetic data. As was true in the empirical data, MB RWD (mode = −0.48, confidence interval [CI] = [−0.61,−0.35]), MB PUN (mode = 0.25, CI = [0.13,0.38]), GP PUN (mode = 0.16, CI = [0.06,0.26]), MF RWD (mode = −0.17, CI = [−0.25,−0.09]), and MF PUN (mode = 0.10, CI = [0.02,0.18]) effects were detected, whereas GP RWD (mode = −0.09, CI = [−0.20,0.02, pd = 0.94]) was not. Moreover, as was true in the empirical data, MB RWD was greater than MB PUN (mode = 0.21, CI = [0.04,0.40]) and MF RWD and PUN were not significantly different from each other (mode = 0.07, CI = [−0.03,0.18]). Unlike the empirical data, GP PUN only trended in the direction of being greater than GP RWD (mode = −0.06, CI = [−0.20,0.08], pd = 0.81).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig2-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-2-2"><title>Model-free</title><p>An MB strategy can be highly effective in our task, but it demands constant adaptation of one’s actions to frequently changing goals. Thus, we expect participants might resort to less costly, approximate strategies (i.e., heuristics). One common heuristic simplifies MB strategies by learning which actions have higher expected value purely based on experienced rewards and punishments. This so-called ‘model-free’ (MF) reinforcement learning strategy is ubiquitously deployed in single-goal tasks (<xref ref-type="bibr" rid="bib18">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Sutton and Barto, 2018</xref>). In the present multitask setting, this would entail forgoing adaptation to the current goal and instead simply learning the overall expected values of the two available actions. Since the previous goal is what determines the value of the last observed features, a unique signature of an MF strategy is how a previous goal determines the impact of last observed features on subsequent action, regardless of whether the goal has switched. For example, if an action led to a punishment on the last trial, then that action will tend to be avoided irrespective of the current goal (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom row).</p></sec><sec id="s2-2-3"><title>Goal perseveration</title><p>A MF strategy is relatively simple to implement but not particularly effective since it does not utilize the information provided by feature outcomes that currently have no reward or punishment value (i.e., a feature that is irrelevant given the trial’s goal or that is a counterfactual outcome of the unchosen action). An alternative strategy, that we term ‘goal perseveration’, might strike a better balance between simplicity and effectiveness. This strategy inherits the exact same knowledge of feature probabilities acquired by MB learning, but simplifies action selection by persistently avoiding punishment and seeking reward, simultaneously, regardless of instructed goal. This, in principle, eliminates effortful goal switching while utilizing all available information about the changing action-feature mapping. Thus, rather than constituting a separate decision system in its own right, GP is best thought of as a behavior produced by a strategic cost-saving MB agent. In goal-switch trials, a GP strategy would manifest in the observed features having the same impact on subsequent actions regardless of the current or previous trial’s instructed goal. For example, a GP agent will avoid an action that led to a punishment feature at the last trial even if both previous and current goals were to seek reward (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, middle row).</p></sec><sec id="s2-2-4"><title>The benefits and costs of each strategy</title><p>MB strategies typically harvest more reward than heuristic strategies but are computationally costly, hence individuals will tend to use them to a greater extent when properly incentivized (<xref ref-type="bibr" rid="bib38">Konovalov and Krajbich, 2020</xref>; <xref ref-type="bibr" rid="bib39">Kool et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Patzelt et al., 2019</xref>). To determine whether our task properly incentivized the use of an MB strategy, we simulated agents playing the task many times and computed the average amount of reward earned and punishment avoided with each computational strategy. This showed that an MB strategy in our task led to significantly more reward than the other strategies (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; e.g., around 40% more than a GP agent), and only 15% worse than an idealized model that has access to the true feature probabilities for each action. The advantage of the MB strategy was due in large part to the task involving frequent goal switching (41.8% of trials). Finally, the least costly MF strategy also earns the least reward in the present task (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Task performance of distinct strategies.</title><p>(<bold>A</bold>) Average total points gained by computational strategies. Punishment, reward, and total points (i.e., reward minus punishment) were averaged over 2000 simulations for each strategy. Strategies included model based (MB), model free (MF), and three versions of a goal perseveration (GP reward seeking with MB punishment avoidance [GP-R], GP punishment avoidance with MB reward seeking [GP-P], and GP for both reward and punishment goals [GP]). Details of parameters and models for each agent simulated are detailed in Methods. Each agent played the task 2000 times. Measures are range normalized such that 0 corresponds to performance of an agent that purely guesses and one corresponds to performance of the best-performing agent. (<bold>B</bold>) Punishment avoided by computational strategies. Here, the plot tallies successful attempts by agents to avoid punishment. The results illustrate that a hybrid agent that employs the goal-perseveration punishment avoidance strategy, and utilizes model-based control for reward seeking, avoids punishment as successfully as a fully model-based agent. (<bold>C</bold>) Reward earned by computational strategies. Here, the plot tallies successful attempts by agents to seek reward. This highlights that a hybrid agent that employs the goal-perseveration punishment avoidance strategy gains less reward than a model-based agent.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig3-v2.tif"/></fig></sec></sec><sec id="s2-3"><title>Empirical evidence of each computational strategy</title><sec id="s2-3-1"><title>Evidence of MB learning</title><p>To estimate whether participants leveraged each of the three strategies, we fit a Bayesian linear mixed-effects logistic regression to participant choices on goal-switch trials, wherein unique signatures of each strategy are detectable. Besides accounting for each strategy’s signature, the regression controlled for the main effect of goal. The MB regression parameter predicted whether a participant switched to a new action on the current trial as a function of the interaction between the features observed last trial for chosen and unchosen actions and the instructed goal on the current trial (see Methods). Thus, we found a strong main effect of MB behavior (MB main effect mode = 0.59, confidence interval [CI] = [0.43,0.76], pd = 1; <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>Examination of the data prior to regression analysis suggested a difference in utilization of MB control for reward seeking relative to punishment avoidance (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). To determine whether an MB effect was present for both punishment avoidance and reward seeking goals, we enhanced the regression with separate MB parameters for the two goals. Posterior estimates showed that individuals engaged MB control for both reward seeking (mode = −0.52, CI = [−0.69,−0.38], pd = 1) and punishment avoidance goals was highly trending (mode = 0.12, CI = [−0.01,0.28], pd = 0.96). Moreover, we found a larger MB effect for reward seeking than for punishment avoidance (mode = 0.41, CI = [0.20,0.62], pd = 1).</p></sec><sec id="s2-3-2"><title>Evidence of MF learning</title><p>We next determined whether participants also used an MF strategy, as captured by a regression parameter reflecting an interaction between the features observed at the last trial for chosen actions and the instructed goal on the last trial (this interaction represents the reward or punishment incurred last trial). Posterior estimates showed a MF strategy was employed by participants (MF main effect mode = 0.24, CI = 0.14,0.36, pd = 1), both in response to reward (mode = −0.14, CI = [−0.23,−0.04]; <xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom row) and punishment (mode = 0.22, CI = [0.12,0.31]). We found no evidence that the valence of the feedback impacted MF behavior to a greater degree (mode = −0.08, CI = [−0.21,0.06], pd = 0.87).</p></sec><sec id="s2-3-3"><title>Evidence of a GP strategy</title><p>Finally, we determined whether participants used a GP strategy, as captured by a regression parameter reflecting effects of reward and punishment features observed last trial irrespective of goal. We observed a strong GP effect (GP main effect mode = 0.26, CI = [0.14,0.40], pd = 1). Breaking the GP effect down by valence showed that GP was utilized for punishment avoidance (mode = 0.33, CI = [0.20,0.45], pd = 1), significantly more so than for reward seeking (mode = −0.11, CI = [−0.23,0.02], pd = 0.95; difference between goals: mode = −0.20, CI = [−0.37,−0.04], pd = 1).</p></sec></sec><sec id="s2-4"><title>Quantifying the contribution of each strategy to decision-making</title><p>The presence of unique signatures of MB, MF, and GP decision strategies in the empirical data presents strong evidence for the use of these strategies, but the signature measures are limited to examining goal-switch trials and, within those trials, examining the impact of features observed on the very last trial. To comprehensively quantify the extent to which participants utilized each strategy for reward seeking and punishment avoidance, we next developed a series of computational models that aim to explain all participant choices given the features observed on all preceding trials.</p><p>We first sought to determine whether each strategy explained unique variance in participants’ choices. To do so, we implemented a stepwise model comparison (see Methods for full details of the models) that began with a null model comprising only action perseveration (AP). Specifically, an AP strategy reflects the tendency of participants to stay with the action taken at the last trial, which has been found in various prior studies on single-goal reinforcement learning (<xref ref-type="bibr" rid="bib18">Daw et al., 2011</xref>; Bayesian information criterion [BIC] = 38,273.53).</p><p>We subsequently investigated whether an MB strategy explained unique variance in participant’s choices above the null. To do so, we compared the null model to a similar model where we added an MB strategy. We found that the MB model explained significantly more variance than the null model (Δ iBIC = −3900.94), a finding that coheres with our expectation that participants would utilize an MB strategy to make choices.</p><p>Before considering additional strategies, we asked whether individuals adjusted how they utilized an MB strategy for reward seeking and punishment avoidance in a ‘resource rational’ (RR) fashion, based on how abundant each goal was in the task block (MB<sub>RR</sub>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Allowing the model to adjust in this way improved an index of model fit significantly (Δ iBIC = −175.95; <xref ref-type="fig" rid="fig4">Figure 4A</xref>) providing evidence that individuals reallocated MB resources toward goals that were more abundant.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Results from computational modeling.</title><p>*p &lt; 0.05, ***p &lt; 0.001. (<bold>A</bold>) Stepwise model comparison. The plot compares all models to the winning model ‘MB<sub>RR</sub>+ GP + MF’. (<bold>B</bold>) Model-based utilization is greater for reward seeking than for punishment avoidance. Here and in panel C, distributions are compared in terms of their medians due to a heavy positive skew. (<bold>C</bold>) Goal-perseveration utilization is greater for punishment avoidance than for reward seeking. Panels B and C show the distributions of utilization weights that best fitted each individual participant’s choices.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Plots of symptom distributions and noting cutoff for clinical symptoms and percent of clinical symptoms.</title><p>Worry and Obsessive Compulsive (OC) measures comprised 36% and 41.2% of respondents endorsing symptoms that are great enough to be considered mild presentations these forms of psychopathology. By contrast, somatic anxiety only comprised 4.7% of respondents endorsing clinically relevant symptoms. Note, we conducted tests to see whether subjects removed from the study due to not misunderstanding the task were significantly different in terms of these measures of psychopathology and none approached significance (lowest p value = 0.33).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Parameter recovery.</title><p>Each entry in the correlation matrix represents the Pearson correlation between the ground-truth parameter values (sampled from the group-level distributions that were fitted to our empirical data) used to simulate 192 synthetic agents and the parameter values fitted to the synthetic data using our hierarchical model-fitting procedure. On-diagonal entries denote parameter recovery estimates, and off-diagonal entries quantify undesirable tradeoff between model parameters. The following are the group priors we generated the data comprising the best-fit group hyperparameters: MF ~ gamma(1.11, 0.55), LR value ~ beta(0.82, 0.50), MB Reward ~ gamma(0.60, 6.73), MB Punish ~ gamma(0.45, 4.23), LR features ~ beta(0.32, 1.33), Ch Punish ~ normal(−0.15, 1.02), Ch Reward ~ normal(0.50, 1.11), LR cf ~ beta(0.19, 1.24), GP Reward ~ gamma(0.43, 1.69), GP Punish ~ gamma(0.60, 1.43), AP ~ normal(0.11, 0.54). MF = model-free system, MB = model-based system, GP = goal-perseveration system, Ch = change in utilization as a function of task block, LR = learning rate, CF = counterfactual, AP = action perseveration. Model recoverability. We generated synthetic data using the priors posted below for 192 subjects using either the winning model or the second-best model (‘MB<sub>RR</sub>+ MF + GP1’). We then fit these data using the model-fitting procedure described in the main text. The model that generated the data was recovered 10/10 times. For second-best model, all priors were the same except that a single GP utilization weight was used with the following prior: GP Reward ~ gamma(0.53, 1.55).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Alternative model descriptions and full model comparison.</title><p>As explained in text, our null model started with just action perseveration (AP; tendency to stay with last action taken), which was included as a process in all subsequent models. Next, we established key components of the model-based (MB) system that were not explained in the main text. First, we found an MB system that only learns from chosen feedback, and disregards counterfactual feedback (‘MB no CF’) fit the data far worse than a model including counterfactual feedback (‘MB 1 LR’). Subsequently, we showed that an MB agent that learns from counterfactual feedback at a different learning rate (‘MB’) outperforms an agent that enforces the learning rate be equal across factual and counterfactual feedback (‘MB 1 LR’).After arriving at the winning model in the stepwise fashion described in Results in the main text, we tested a variet of other models by remodeling certain processes within each system. We first instantiated the resource-rational learning as described in the main text either just within the the goal-perseveration system (‘MB + MF + GP<sub>RR</sub> ’) or in both the GP and MB systems (‘MB<sub>RR</sub> + MF + GP<sub>RR</sub>). We then sought to determine if the resource rational shift in MB resources was a function of the most recently experienced goals (which could vary within task block; ‘MB + RFHist + MF + GP’). To do so, the agent computed a running prediction of the upcoming goal, which was updated by a goal prediction error, defined by the disparity between the goal outcome (1 if reward pursuit, −1 if punishment avoidance) and the predicted goal quantity ( <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), and a goal learning rate, <italic>δ</italic>:</p><p>This goal prediction was then used to modulate model-based utilization weights on a trial-by-trial basis. Unique reward pursuit and punishment avoidance weights defined the direction and magnitude that the goal prediction quantity modulated model-based utilization weights:</p><p>If goals were modulated according to goal abundance (as predicted), <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> should take on a negative value, as this would result in the model-based utilization weight decreasing for reward pursuit and increasing for punishment avoidance when an agent’s recent goal history involves more punishment avoidance trials. By the same logic, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was predicted to take on a positive value. Next, we sought to determine if differences in goal valence were better explained by differences in learning (via two separate learning rates) relative to our original implementation where goal valence differences were encoded in utilization weights (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Doing so, we had to exclude the resource rational component because the fitted learning rates greatly exceeded 1 and extended far into the negative range (i.e., &lt;0), which rendered the parameter uninterpretable. As such we compared two models: (1) in which there were separate learning rates for reward and punishment features, a single utilization weight for MB and a single utilization weight for the GP (‘MBLR + MF + GPLR’; Bayesian information criterion [BIC] = 33,098.176) versus (2) a single learning rate for reward and punishment features and, and separate utilization weights for reward pursuit and punishment avoidance in both the MB and GP systems, as was true of the winning model (‘MB + MF + GP’; BIC = 32,810.86). This system also included action perseveration and model-free control in line with the winning model. Doing so demonstrated that, as was true in the winning model, goal pursuit differences encoded in utilization weights as opposed to learning rates explained the data better. Note, it was not possible to fit separate learning rates and utilization weights across each goal within the MB and GP systems as they would be unrecoverable due to trading off. Subsequently, we tested a model in which action values for each goal were learned in a model-free way. That is, instead of two action values in the original model-free system, there were four action values, two for each goal (‘GP<sub>RR</sub> + G-MF’). Updates to action values for a given goal occurred only when that goal was instructed on the current trial. Similarly, decision-making was only influenced by the two action values relevant to the instructed goal. Like the winning model, action values for each goal had their own utilization weights (<inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) that shifted across task block in the same way as the winning model with change parameters. The model additionally encoded action perseveration.We next sought to model the MF system in such a way that encoded a GP-like signature; namely, that the experience of punishment features always was a punishment (−1 points), and the experience of a reward feature was always rewarding (+1 points), irrespective of the instructed goal (‘MBRR + MF3’). Thus, each trial, there were three action values in the MF system: (1) as originally implemented, (2) as predicting whether one’s action leads to reward features, and (3) whether one’s action leads to punishing features. These three action values were intergrade with action values from the MB system and AP to determine choices. Next, we sought to verify that the order in which we included controllers did not affect modeling results. To do so, we excluded from the winning model just the MF system (‘MB<sub>RR</sub> + GP’), just the GP system (‘MB<sub>RR</sub> + MF’), and just the MB system (‘GP<sub>RR</sub> + MF). All were inferior to the winning model. Finally, we tested an alternative model where GP behavior may derive from an MB strategy that occasionally forgets the reward function (Forgetful-MB + MthF + AP), allowing this forgetting to occur at different rates during reward and punishment goals. This model-based agent includes two additional parameters, <italic>f</italic><sub><italic>R</italic></sub> and <italic>f</italic><sub><italic>p</italic></sub>, which govern the probability of forgetting the presented reward function on reward pursuit trials and punishment avoidance trials, respectively. Thus, on each trial, the model replaces the instructed goal with the opposite goal (e.g., if the actual goal was [−1, 0], the participant used [0,1]) with some fixed probability (either <italic>f</italic><sub><italic>R</italic></sub> or <italic>f</italic><sub><italic>p</italic></sub>, depending on the trial type). We again found that this model fit worse than the winning model, confirming that a model where the model-based controller forgets the current reward with different rates on reward and punishment trials does not account for our results supporting GP as well.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Correlations between fitted parameters.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig4-figsupp4-v2.tif"/></fig></fig-group><p>We next tested whether a MF strategy explains unique variance in choice data beyond the MB<sub>RR</sub> model. To do so, we compared the MB<sub>RR</sub> model to a similar model that combined both MB<sub>RR</sub> and MF strategies. In controlling choice, the two strategies were combined via a weighted sum of the values they each assigned to a given action. Thus, a strategy’s weight quantifies the degree to which it was utilized. The MB<sub>RR</sub>+ MF models’ fit was superior to the MB<sub>RR</sub> model (Δ iBIC = −896.72; <xref ref-type="fig" rid="fig4">Figure 4A</xref>), providing evidence that individuals used both MF and MB<sub>RR</sub> strategies to inform their decisions.</p><p>Finally, we tested whether a GP strategy might explain additional variance beyond the MB<sub>RR</sub> and MF strategies. Enhancing the MB<sub>RR</sub> + MF with the addition of a GP strategy significantly improved model fit (Δ iBIC = −243.73; <xref ref-type="fig" rid="fig4">Figure 4A</xref>), indicating participants also used a GP strategy. Henceforth, we refer to this final MB<sub>RR</sub> + MF + GP model as the ‘winning model’ (see Methods for full model formalism). To further validate this winning model, we compared it to several alternative models that were found inferior in fitting the data, including models where we removed GP, MF, and MB<sub>RR</sub> processes to ensure the order in which we added each strategy did not impact the final result, and an alternative MF account wherein the goal was encoded as part of state representation (see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for model specifications and model comparison results). Ultimately, we showed that generating data from the winning model, using best-fitting participant parameters, could account for all but one of the mechanism-agnostic results reported in <xref ref-type="fig" rid="fig2">Figure 2C and D</xref> (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p></sec><sec id="s2-5"><title>Punishment avoidance is less flexible than reward seeking</title><p>To investigate algorithmic differences between reward seeking and punishment avoidance, we used the winning model to extract the parameter values that best fit each participant. We focused our analysis on parameters quantifying the degree to which individuals utilized a particular strategy to pursue reward or avoid punishment goals. We validated that MB, GP, and MF inverse temperature parameters were recoverable from simulated experimental data, and that the degree of recoverability (i.e., the correlations of true and recovered parameter values, which were between 0.76 and 0.91; <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) was in line with extant reinforcement learning modeling studies (<xref ref-type="bibr" rid="bib55">Palminteri et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Haines et al., 2020</xref>). Similarly, low correlations between estimated parameters (all weaker than 0.16) demonstrate our experimental design and model-fitting procedure successfully dissociated between model parameters (<xref ref-type="bibr" rid="bib70">Wilson and Collins, 2019</xref>).</p><p>In doing so, we found that individuals relied significantly more on an MB strategy for reward seeking relative to punishment avoidance (two-tailed p &lt; 0.001, nonparametric test [see Methods]; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). By contrast, individuals relied more heavily on GP for punishment avoidance relative to reward seeking (two-tailed p = 0.026, nonparametric test; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). These results suggest participants did not adaptively ‘turn off’ the goal to avoid punishment to the same extent as they did so for the goal to pursue reward.</p><p>Finally, we examined whether individuals prioritized punishment avoidance and reward seeking goals based on their relative abundance. To do so, we extracted computational parameters controlling a shift in MB utilization across task blocks for both goal types. Each of these utilization change parameters was compared to a null value of 0 using a nonparametric permutation test to derive valid p values (see Methods). This analysis revealed that individuals were sensitive to reward goal abundance (mean = 0.50, p = &lt;0.001) but not to punishment goal abundance (mean = −0.13, p = 0.13). This result comports with previous results which highlighted a difficulty disengaging from punishment avoidance. Moreover, this result points to why our winning model, that allowed MB utilization weights to change across task block, explained participant data better than a model that kept MB utilization weights constant.</p></sec><sec id="s2-6"><title>Preliminary evidence chronic worry is associated with greater perseverance of punishment avoidance</title><p>In a set of exploratory analyses, we sought to investigate how anxiety might be related to a prioritization of punishment avoidance goals. To do so, we assayed participants using self-report measures of chronic worry (<xref ref-type="bibr" rid="bib48">Meyer et al., 1990</xref>) and somatic anxiety (<xref ref-type="bibr" rid="bib13">Casillas and Clark, 2000</xref>) and OC symptoms (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). For each regression model, we computed p values using a nonparametric permutation test wherein we shuffled the task data with respect to the psychopathology scores, repeating the analysis on each of 10,000 shuffled datasets to derive an empirical null distribution of the relevant <italic>t</italic>-statistics.</p><p>We first report the bivariate relations between each form of psychopathology and inverse temperature parameters reflecting tendencies to utilize MB and GP punishment avoidance. Given that individuals with OCD and anxiety symptoms may overprioritize threat detection, it is conceivable that there is a relationship between all three forms of psychopathology and MB punishment avoidance. However, we found no significant or trending relationships between any form of psychopathology and MB control for punishment avoidance (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left column). An alternative possibility is that individuals with anxiety suffer from a dysregulation in goal pursuit, reflecting a failure to disengage punishment avoidance when instructed to do so. On this basis, we explored whether worry and somatic anxiety are positively associated with GP for punishment avoidance. In so doing we found initial evidence of a potential relationship between the tendency to worry and punishment avoidance perseveration (<italic>B</italic> = 2.15, <italic>t</italic> = 1.4, p = 0.16; <xref ref-type="fig" rid="fig5">Figure 5A</xref>, right column).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Exploratory relationships between threat-related psychopathology and goal-directed control for punishment avoidance.</title><p>Each row reflects a different regression model, where the score for each psychopathology measure in the left column is the dependent variable, and inverse temperature parameters reflecting model-based (‘MB Punish’) and goal-perseveration (‘GP Punish’) punishment avoidance are the regressors. Each effect is presented in the following format: <italic>β</italic> (standard error), p value. (<bold>A</bold>) Bivariate relationships without control covariates. (<bold>B</bold>) Regression coefficients when controlling for co-occurring levels of psychopathology as well as for general valence-independent levels of utilization of MB (inverse temperature and learning rate) and non-MB (AP, MF, and GP inverse temperatures) strategies. In all tables, p values are uncorrected for multiple comparisons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig5-v2.tif"/></fig><p>To provide a more specific test of our key hypotheses, we removed variance of noninterest in order to sensitize our analyses to the unique relationships between forms of psychopathology and types of punishment avoidance. Firstly, generalized, as opposed specific obsessive, worry is thought to be particularly associated with difficulty in disengaging from worry (<xref ref-type="bibr" rid="bib8">Berenbaum, 2010</xref>), since it lasts significantly longer in both clinical (<xref ref-type="bibr" rid="bib17">Dar and Iqbal, 2015</xref>) and community samples (<xref ref-type="bibr" rid="bib43">Langlois et al., 2000</xref>). Thus, we dissociated generalized from obsessive worry using the same approach taken in previous studies (<xref ref-type="bibr" rid="bib24">Doron et al., 2012</xref>; <xref ref-type="bibr" rid="bib64">Stein et al., 2010</xref>), namely, by including a measure of OCD symptoms as a control covariate. Controlling for OCD symptoms has the additional benefit of accounting for known relations between OCD and poor learning of task structure, reduced MB control, and perseverative tendencies (<xref ref-type="bibr" rid="bib28">Gillan et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Seow et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Sharp et al., 2021</xref>). Secondly, another potentially confounding relationship exists between worry and somatic anxiety (<xref ref-type="bibr" rid="bib60">Sharp et al., 2015</xref>), likely reflecting a general anxiety factor. Thus, we isolated worry by controlling for somatic anxiety, as commonly done in studies seeking to quantify distinct relationships of worry and somatic anxiety with cognitive performance (<xref ref-type="bibr" rid="bib69">Warren et al., 2021</xref>) or associated neural mechanisms (<xref ref-type="bibr" rid="bib63">Silton et al., 2011</xref>). Finally, we controlled for covariance between computational strategies that might reflect general task competencies. This included the utilization of MB (including learning rates and inverse temperatures) since observed anticorrelations in the empirical data (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>) between GP and MB may derive from causal factors such as attention or IQ, as well as a general tendency to mitigate cognitive effort by using less costly strategies (AP, MF, and GP inverse temperatures; <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>).</p><p>This analysis showed a stronger relationship between worry and punishment perseveration (<italic>β</italic> = 3.14 (1.38), <italic>t</italic> = 2.27, p = 0.04, <xref ref-type="fig" rid="fig5">Figure 5B</xref>). No other significant relationship was observed between punishment perseveration or MB punishment avoidance and psychopathology (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Of note, we additionally found no association between the parameter governing how MB punishment was modulated by task block and levels of worry, both when including worry alone (<italic>β</italic> = 2.5 (1.91), <italic>t</italic> = 1.31, p = 0.19) and when controlling for the same covariates as detailed above (<italic>β</italic> = 1.46 (1.65), <italic>t</italic> = 0.88, p = 0.38). Ultimately, we validated the full model using a fivefold cross-validation procedure which showed that regressing worry onto the aforementioned covariates (using a ridge regression implementation) explains significantly more variance in left out test-set data (<italic>R</italic><sup>2</sup> = 0.24) relative to the models of the bivariate relationships between worry and GP Punishment (<italic>R</italic><sup>2</sup> = 0.01) and MB Punishment (<italic>R</italic><sup>2</sup> = 0.00).</p><p>It is important to note that all aforementioned p values testing our key hypotheses (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) are corrected for multiple comparisons using a correction procedure designed for exploratory research (<xref ref-type="bibr" rid="bib58">Rubin, 2017</xref>), which only controls for number of statistical tests within each hypothesis. Using a more conservative Bonferroni error correction for all four regression models, as typically employed in hypothesis-driven confirmatory work (<xref ref-type="bibr" rid="bib26">Frane, 2020</xref>), resulted in a p value for the key effect of worry and punishment perseveration that no longer passed a conventional significance thresholds (p = 0.08). Thus, future work with a more targeted, hypothesis-driven approach needs to be conducted to ensure our tentative inferences regarding worry are valid and robust.</p><p>To illustrate the consequences of GP punishment avoidance on pursuit of reward in anxious participants, we simulated a GP + MB agent that adaptively engages reward-relevant information when instructed to, but perseverates in avoiding punishment during reward seeking. We show that such a strategy is as good as an MB agent in avoiding punishment, but comes with the cost of suboptimal reward seeking (<xref ref-type="fig" rid="fig3">Figure 3B and C</xref>). This trade-off mirrors the negative consequence of real-world threat avoidance in trait anxious individuals (<xref ref-type="bibr" rid="bib1">Aderka et al., 2012</xref>). Moreover, this gives a potential normative explanation of punishment perseveration in anxious individuals; namely, if anxious individuals prioritize avoiding threat, they can do so just as well using punishment perseveration as using an MB strategy while expending fewer resources.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using a novel multigoal pursuit task, we investigated computational strategies humans leverage to navigate environments necessitating punishment avoidance and reward seeking. Our findings indicate the use of a strategy that avoids goal switching wherein individuals learn a model of the task but use it in a goal-insensitive manner, failing to deactivate goals when they are irrelevant. This less flexible, but computationally less costly, strategy was leveraged more in order to avoid punishment as opposed to a pursuit of reward. Beyond trial-to-trial perseverance, inflexibility in punishment avoidance manifested in a lack of blockwise adjustment to the abundance of punishment goals. By contrast, we found that a flexible MB strategy was used more for reward seeking, and was flexibly modulated in an RR way in response to an abundance of reward seeking goals changing between task blocks. Finally, we demonstrate preliminary evidence that a greater GP reliance for punishment avoidance in those individuals with greater chronic worry.</p><p>The strategic deployment of GP primarily toward punishment avoidance indicates such behavior is not merely a reflection of a noisy or forgetful MB system. Our finding that humans use less flexible computational strategies to avoid punishment, than to seek reward, aligns with the idea of distinct neural mechanisms supporting avoidance and approach behavior (<xref ref-type="bibr" rid="bib47">McNaughton and Gray, 2000</xref>; <xref ref-type="bibr" rid="bib42">Lang et al., 1998</xref>). Moreover, comparative ethology and evolutionary psychology (<xref ref-type="bibr" rid="bib57">Pinker, 1997</xref>) suggest there are good reasons why punishment avoidance might be less flexible than reward seeking. <xref ref-type="bibr" rid="bib72">Woody and Szechtman, 2011</xref> opined that ‘to reduce the potentially deadly occurrence of false negative errors (failure to prepare for upcoming danger), it is adaptive for the system to tolerate a high rate of false positive errors (false alarms).’ Indeed, we demonstrate that in the presence of multiple shifting goals, perseverance in punishment avoidance results in false positives during reward seeking (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), but avoids ‘missing’ punishment avoidance opportunities because of lapses in goal switching (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Future work could further test these ideas, as well as potential alternative explanations (<xref ref-type="bibr" rid="bib21">Dayan and Huys, 2009</xref>).</p><p>GP may thus in fact constitute an RR strategy (<xref ref-type="bibr" rid="bib45">Lieder and Griffiths, 2019</xref>) for approximating MB control. To illustrate this, consider that MB learning is computationally demanding in our task specifically because goals switch between trials. When the goals switch, an MB agent must retrieve and use predictions concerning a different feature. Additionally, the agent needs to continuously update its predictions concerning features even when they are not presently relevant for planning. GP avoids these computationally costly operations by pursuing goals persistently, thus avoiding switching and ensuring that features are equally relevant for planning and learning. In this way, GP saves substantial computational resources compared to MB yet is able to perform relatively well on the task, achieving better performance than MF. Additionally, if a participant selectively cares about avoiding losses (for instance, due to loss aversion), GP can perform as well as MB. Thus, we propose the GP heuristic reflects a strategic choice, which can achieve good performance while avoiding the substantial resource requirements associated with MB control. In this sense it fulfils a similar role as other proposed approximations to MB evaluation including MF RL (<xref ref-type="bibr" rid="bib65">Sutton and Barto, 2018</xref>), the successor representation (<xref ref-type="bibr" rid="bib20">Dayan, 1993</xref>; <xref ref-type="bibr" rid="bib51">Momennejad et al., 2017</xref>), mixing MB and MF evaluation (<xref ref-type="bibr" rid="bib37">Keramati et al., 2016</xref>), habitual goal selection (<xref ref-type="bibr" rid="bib15">Cushman and Morris, 2015</xref>), and other identified heuristics in action evaluation (<xref ref-type="bibr" rid="bib19">Daw and Dayan, 2014</xref>).</p><p>Our exploratory findings that an inflexibility in punishment avoidance was more pronounced in individuals with chronic worry is suggestive of a computational deficit that may serve to unify several known effects relating trait worry to failure to terminate threat processing. For example, in paradigms that explicitly instruct participants to ignore threat-irrelevant information, such as the dot-probe (<xref ref-type="bibr" rid="bib4">Asmundson and Stein, 1994</xref>) and modified emotional Stroop (<xref ref-type="bibr" rid="bib16">Dalgleish, 1995</xref>; <xref ref-type="bibr" rid="bib68">van den Hout et al., 1995</xref>) tasks, individuals with trait worry have difficulty inhibiting processing of threat (<xref ref-type="bibr" rid="bib7">Bar-Haim et al., 2007</xref>). Moreover, increased GP punishment avoidance may be involved in the overactivity of threat-related circuitry in anxious individuals during tasks where threat is not present (<xref ref-type="bibr" rid="bib31">Grupe and Nitschke, 2013</xref>; <xref ref-type="bibr" rid="bib53">Nitschke et al., 2009</xref>). However, we note that there was a significant positive skew in the somatic arousal measure, which although likely due to random sampling error (given that other symptom measures were highly representative of population distributions), may nonetheless limit our ability to generalize findings from the present sample to the population.</p><p>Our findings go beyond previous findings that report, in single-goal reinforcement learning tasks, that anxiety is associated with altered MF but intact MB control (<xref ref-type="bibr" rid="bib29">Gillan et al., 2019</xref>). Our findings suggest a conflict between punishment avoidance and reward seeking may be necessary to uncover how knowledge of task structure is used in anxiety. Indeed, prior approach–avoidance conflict paradigms have found that trait anxiety is positively associated with neural correlates of punishment avoidance (rejected gambles that could result in loss) (<xref ref-type="bibr" rid="bib46">Loh et al., 2017</xref>) and avoidant behavior (<xref ref-type="bibr" rid="bib6">Bach, 2015</xref>).</p><p>A limitation of our task is that differences in strategy utilization for reward seeking and punishment avoidance (see Methods) could in part reflect differences in sensitivity to reward versus punishment. However, reward and punishment sensitivity cannot account for the effects we observe across strategies, since on the one hand, punishment avoidance was greater for GP, whereas reward seeking was greater within an MB framework. Greater punishment sensitivity compared to reward sensitivity would predict the same direction of valence effect for both behavioral strategies. Moreover, knowledge of reward features had a greater net impact on choice across both goal-oriented strategies (sum of weights across both MB and GP strategies is greater for reward seeking). That said, we recognize that differences in outcome sensitivity, which are algorithmically equivalent to differences in the magnitude of external incentives, may cause a shift from one strategy to another (<xref ref-type="bibr" rid="bib39">Kool et al., 2017</xref>). Thus, an open question relates to how reward and punishment sensitivity might impact flexibility in goal pursuit.</p><p>Future work can further address how humans manage multigoal learning in the context of larger decision trees with multiple stages of decisions. In such environments, it is thought people employ a successor feature learning strategy, whereby the long-run multistep choice features are stored and updated following feedback (<xref ref-type="bibr" rid="bib67">Tomov et al., 2021</xref>). Such multistep tasks can be enhanced with shifting reward seeking and punishment avoidance goals to determine how altered strategies we identify with pathological worry might relate to trade-offs between MB and successor feature strategies for prediction. Another possibility is that punishment-related features capture involuntary attention in our task because they are tagged by a Pavlovian system, and this interacts with an MB system that learns task structure. Indeed, prior work (<xref ref-type="bibr" rid="bib22">Dayan and Berridge, 2014</xref>) has discussed possibilities of MB Pavlovian hybrid strategies.</p><p>In relation to why GP punishment avoidance may specifically be associated with chronic worry, we suggest that failures to disengage punishment avoidance may serve to explain so-called ‘termination’ failures in chronic worry (<xref ref-type="bibr" rid="bib8">Berenbaum, 2010</xref>). The causal role of GP in failures to terminate worry episodes could avail of the fact that such failures are dissociable from a tendency to suffer ‘initiation’ failures, which involve worrying too easily in response to many stimuli (<xref ref-type="bibr" rid="bib9">Berenbaum et al., 2018</xref>). Although the perseveration of worry may appear relevant to obsessions in OC symptoms, punishment avoidance in OC has been empirically demonstrated to be specific to idiographic domains of potential threat (e.g., sanitation <xref ref-type="bibr" rid="bib3">Amir et al., 2009</xref>), an issue the present study did not investigate. Additionally, we did not find that GP was associated with somatic anxiety possibly due perhaps to random sampling error as we had an unusually low percentage meeting a threshold for mild symptomatology (4.7%; typical convenience samples are typically in the range of 12–18 <xref ref-type="bibr" rid="bib60">Sharp et al., 2015</xref>; <xref ref-type="bibr" rid="bib66">Telch et al., 1989</xref>). More importantly, somatic anxiety is thought to involve lower-order cognitive processes than those likely involved in multigoal pursuit (<xref ref-type="bibr" rid="bib60">Sharp et al., 2015</xref>). Given that present results are preliminary in nature, future studies will need to test a prediction that chronic worry is associated with punishment perseveration in a larger sample. This should also include testing whether this association holds in a clinical population, as variation in symptoms in a clinical population may relate to punishment perseveration differently (<xref ref-type="bibr" rid="bib36">Imperiale et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Groen et al., 2019</xref>). Additionally, doing so may be enhanced by including parameters relating worry to punishment perseveration within the model-fitting procedure itself, and so better account for uncertainty in the estimation of such covariance (<xref ref-type="bibr" rid="bib33">Haines et al., 2020</xref>).</p><p>In conclusion, we show humans are less flexible in avoiding punishment relative to pursuing reward, relying on a GP strategy that persists in punishment avoidance even when it is irrelevant to do so, and failing to deprioritize punishment avoidance goals when they are less abundant. Moreover, we show that GP is augmented in individuals with chronic worry, hinting at a candidate computational explanation for a persistent overprioritization of threat in anxiety.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Sample and piloting</title><p>Prior to disseminating the task, we conducted a pilot study varying the number of features and actions participants could choose. We first found that less than half of individuals we recruited performed above chance levels when individuals had to learn a task with three actions and nine feature probabilities. We thus reduced the complexity of the task and found that including only two actions and four features allowed most participants to leverage an MB strategy.</p><p>Two hundred and forty-eight participants were recruited through Prolific services online Prolific recruiting service (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.co/">https://www.prolific.co/</ext-link>) using the final task design from English-speaking countries to ensure participants understood task instruction. After expected data-scrubbing, our sample size had &gt;99% power to detect valence differences in reinforcement learning parameters, conservatively assuming a medium effect size relative to large effects found previously (to account for differences between multigoal and single-goal settings; e.g., <xref ref-type="bibr" rid="bib55">Palminteri et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Lefebvre et al., 2017</xref>). Moreover, our sample had &gt;80% power to detect small-medium effect sizes relating computational parameters and individual differences in anxiety ( <xref ref-type="bibr" rid="bib62">Sharp et al., 2021</xref>). Participants gave written informed consent before taking part in the study, which was approved by the university’s ethics review board (project ID number 16639/001). The final sample was 37% male, with a mean age of 33.92 years (standard deviation [SD] = 11.97). Rates of mild but clinically relevant levels of OCD (45%) and worry (40%) comported with prior studies (<xref ref-type="bibr" rid="bib62">Sharp et al., 2021</xref>), indicating good representation of individual variation in at least some psychopathological symptoms.</p></sec><sec id="s4-2"><title>Data preprocessing</title><p>Eleven participants completed less than 90% of the trials and were removed. We next sought to define participants that did not understand task instructions. To do so, we computed the proportion of times participants switched from the last action taken as a function of the feature outcomes of that action and the current goal. We used these proportions to define four learning signals, two for each goal. Note, the term ‘average’ henceforth is shorthand for ‘average switching behavior across all trials’. Facing reward goals, participants should (1) switch less than average if they observed a reward feature last trial and (2) switch more if they did not observe a reward feature last trial. Facing punishment goals, participants should (1) switch more than average if they observed a punishment feature last trial and (2) switch less than average if they did not observe a punishment feature last trial. We removed six additional participants because their switch behavior was the exact opposite as they should be for <italic>each</italic> of these four learning signals. When facing a reward goal, they switched more than average having observed a reward feature last trial and switched less than average having not observed a reward feature last trial. Moreover, when facing a punishment goal, they switched less having observed a punishment feature last trial and switched more than average having not observed a punishment feature last trial. We additionally removed participants that: (1) treating a punishment feature as a reward feature (i.e., show the opposite of the two learning signals for punishment; 13 participants) and (2) treating reward feature as a punishment feature (show the opposite of the two learning signals for reward; 26 participants). Excluded subjects performed significantly worse in terms of choice accuracy. To derive accuracy, we computed the percentage of choices subjects made in line with an ideal observer that experienced the same outcome history (in terms of features) as each participant. On average, excluded subjects chose correctly 49.6% of time, whereas included subjects chose correctly 63.5% of time (difference between groups: <italic>t</italic>(190) = 9.66, p &lt; 0.00001).</p><p>Indeed, these errors in following task structure are fundamental failures that result in average performance that is as poor or worse than an agent that purely guesses which action to take at each trial (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Doing so resulted in our final sample of <italic>n</italic> = 192, with a low percentage of removed participants (22%) compared to similar online computational studies (<xref ref-type="bibr" rid="bib67">Tomov et al., 2021</xref>). Importantly, removed participants were no different in terms of mean scores on key measures of psychopathology (greatest mean difference found in OCD; Welch’s <italic>t</italic> = −0.96, p = 0.33).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Excluded participants’ strategies perform similar to or worse than purely guessing.</title><p>To motivate our exclusion criteria, we simulated task performance by agents that falsify these criteria and calculated their average winnings over 5000 simulations each. The guessing agent chooses according to a coin flip. The models instantiating strategies used by excluded subjects comprise those that treat reward features as punishment features (‘Mistake Reward for Punishment’), treat punishment features as if they were reward features (‘Mistake Punishment for Reward’) or incorrectly reverse the treatment of feature types (‘Complete Reversal of Features’). Each model performed as poorly, or significantly worse, than a model that purely guesses, demonstrating a fundamental failure in these strategies for the present task. By contrast, a GP-only agent (‘GP’) that ignores goal-switching instructions does significantly better than a guessing model, and only a little worse than a pure model-based agent (‘MB as instructed’).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-fig6-v2.tif"/></fig><p>Additionally, including such subjects would reduce our sensitivity to estimating differences in the utilization of GP and MB for goals of differing valence, as such subjects treated the task as if there was only a single goal, or that the goals were opposite of their instructed nature. Moreover, given their model of the task, such subjects could approach the task optimally using a MF strategy, and thus were not incentivized to use goal-directed strategies at all.</p><p>To determine whether our relatively strict subject exclusion policy might have affected the results, we conducted a sensitivity analysis on a larger sample (<italic>n</italic> = 248; 98% retention) including subjects that mistreated the instructed value of certain features. To account for these subjects’ behavior, we used normal priors to allow negative inverse temperature parameters. Fitting these revised models to our data, we again demonstrate that our winning model was the best-fitting model compared to all other models. Second, we show that the GP valence effect held and even came out stronger in this larger sample. Thus, the mean difference in GP utilization for punishment and reward goals was 0.24 in our original sample and 0.50 in the larger sample (p &lt; 0.0001). Finally, we show the MB valence effect also held in this larger sample (original sample mean difference between MB reward and MB punishment = 2.10; larger sample mean difference = 1.27, both p values &lt;0.0001).</p></sec><sec id="s4-3"><title>Symptom questionnaires</title><p>Before participants completed the online task, they filled out three questionnaires covering transdiagnostic types of psychopathology. Chronic worry was measured via the 16-item Penn State Worry Questionnaire (<xref ref-type="bibr" rid="bib48">Meyer et al., 1990</xref>). Anxious arousal was measured with the 10-question mini version of the Mood and Anxiety Symptom Questionnaire – Anxious Arousal subscale (<xref ref-type="bibr" rid="bib13">Casillas and Clark, 2000</xref>). Obsessive compulsiveness was measured with the Obsessive Compulsive Inventory – Revised (<xref ref-type="bibr" rid="bib25">Foa et al., 2002</xref>).</p></sec><sec id="s4-4"><title>Multigoal pursuit task</title><p>To examine how people learn and utilize knowledge about the outcomes of their actions in order to seek reward and avoid punishment, we designed a novel multigoal pursuit task. The task consisted of 160 trials. On each trial, participants had 4 s to make a choice between two choice options (represented as fractal images; (<xref ref-type="fig" rid="fig1">Figure 1B</xref>)). Choosing a fractal could then lead to two independent outcome features, a gold feature and a black feature. Any given choice could thus lead to both features, neither feature, or one of the two features. The chances that a choice led to a certain feature varied according to slowly changing probabilities (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). These probabilities were partially independent of one another (i.e., the rank correlation between any pairs of correlation did not exceed 0.66; full span: [0.02,−0.17,0.24,0.28,−0.66,0.43]). The same sequence of feature probabilities (probability of encountering a given feature conditioned on a given choice) was used for all participants. This sequence was generated by starting each sequence at a random value between 0.25 and 0.75, and adding independent noise (normally distributed mean = 0.0, SD = 0.04) at each trial to each sequence, yet bounding the probabilities to be between 0.2 and 0.8. To incentivize choice based on feature probabilities, we ensured that in resultant sequences the probability of reaching a given feature differed between the two choices by at least 0.27 for each feature, on average across the 160 trials.</p><p>To manipulate participant’s goals, throughout the task, one of the two outcome features (which we refer to as the reward feature) either provided 1 or 0 point, and the other outcome feature (which we refer to as the punishment feature) would either provide 0 points or take away one point. The number of points that a given outcome feature provided on a given trial was determined by trial-specific instructed goals (on the screen on which choice options were presented, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). A punishment avoidance goal meant the punishment feature took on the value of −1 and the reward feature took on the value of 0, whereas the reward seeking goal meant the punishment feature took on a value of 0 and the reward feature took on the value of +1. This information was presented in a color (gold or silver) matching the corresponding outcome feature’s color. Importantly, the color the feature took on and the probability trajectory for either feature were counterbalanced across participants. Participants were instructed in advance that one feature in the task might tend to provide points and the other feature might tend to take away points, but they were not told which features these would be.</p><p>To manipulate goal abundance, the frequency of which feature was set to the nonzero outcome varied in the first half versus the second half of the experiment (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In one half of the experiment (punishment context), the punishment feature took away a point (and the reward feature did not provide a point) on 65% of trials, and in the other half (reward context) the reward feature provided a point on 65% of trials. Which context occurred in the first versus second half of the experiment was counterbalanced across participants.</p><p>After the participant observed which outcome features of a choice they received (2 s), they observed the number of points they received (1 s), including the individual reward value of each feature received, in white, as well as a sentence summarizing their total earnings from that trial (e.g., ‘You lost 1’). Following this, in order to eliminate the need for exploratory choices, the participant was shown the features they would have received, had they chosen the alternative choice option (2 s). There was then a 1-s intertrial interval.</p></sec><sec id="s4-5"><title>Clinical analyses</title><p>Although the computational parameters were nonindependently estimated by our hierarchical model-fitting procedure, it is vital to note this does not compromise the validity of the least-squares solution to the regressions we ran. Indeed, Friedman, <xref ref-type="bibr" rid="bib34">Hastie et al., 2009</xref> show that, ‘Even if the independent variables were not drawn randomly, the criterion is still valid if the dependent variables are conditionally independent given the [independent variable] inputs’ (p. 44). However, we note that it is in practice difficult to determine whether such conditional independence is met. In each regression, we excluded the learning rate for counterfactual feedback, as well as learning rate for value in the MF system, due to high collinearity with other parameters (see Methods). We verified low multicollinearity among included parameters (variance inflation factor &lt;5 for independent variables <xref ref-type="bibr" rid="bib2">Akinwande et al., 2015</xref>). We report all bivariate correlations between fitted parameters in <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>.</p></sec><sec id="s4-6"><title>Algorithms defining MB, MF, and GP strategies</title><p>We first describe how each learning system in the winning model derived action values. Then, we describe how action values were integrated into a single decision process. Together, these comprise the best-fitting model that we report on in Results.</p><sec id="s4-6-1"><title>MB system</title><p>An MB agent learns each of the four semi-independent transition probabilities of reward and punishment features given either of the two actions one can choose. Each trial, this agent updates their prior estimate of observing a feature given an action (either ‘press <italic>g</italic>’ or ‘press <italic>j</italic>’) incrementally via a feature prediction error and a learning rate, <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Here, an agent pressed ‘<italic>g</italic>’, observed a punishment feature, and updated the probability of observing a punishment feature conditional on choosing to press ‘<italic>g</italic>’:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Here, the ‘<italic>t</italic>’ subscript refers to the trial, and ‘1’ in the parentheses means that the participant observed a punishment feature. If the feature was not present, the absence would be coded as a ‘0’. This same coding (one for feature observation, 0 if absent) was also used to encode the presence or absence of a reward feature.</p><p>The model learns similarly from counterfactual feedback, albeit at a different rate. Thus, at each trial, MB agents update feature probabilities for the action they did not choose via the same equation as above but with learning rate <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . If the agent pressed ‘<italic>g</italic>’ the counterfactual update would be:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Each of the four probabilities an MB agent learns is stored in a matrix, where rows are defined by actions and columns by feature type (i.e., reward or punishment). These stored feature probabilities are multiplied by ‘utilization weights’ (<inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) that reflect the degree to which an agent utilizes an MB strategy to pursue reward or avoid punishment. No additional parameter controls utilization of an MB strategy (e.g., there is no additional overall <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p><p>Each trial, the agent computes the expected value of each outcome by multiplying stored feature probabilities given each action with the values of the features that are defined by the trial-specific goal. Here, the agent is facing an avoid punishment trial, for which the presence of a punishment feature results in taking away one point (i.e., a value of −1; below we abbreviate press as ‘p’, reward as ‘rew’, and punishment as ‘pun’):<disp-formula id="equ3"><label>(3a)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Via this computation an MB agent disregards the irrelevant goal (here reward seeking). If the agent were facing a reward goal, <xref ref-type="disp-formula" rid="equ3 equ4">Equation 3a and 3b</xref> would be:<disp-formula id="equ4"><label>(3b)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-6-2"><title>Resource reallocation</title><p>Within the MB system, utilization weights changed across block according to a change parameter. Below is an example of how this reallocation occurred for <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> :<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note that negative <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> values were allowed, and thus the model did not assume a priori that, for instance, individuals would have increased MB control for reward in the rewarding block (it could be the opposite). Thus, if the data are nevertheless consistent with a positive <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , this is an indication that, although participants were not explicitly told which block they were in, they tended to prioritize the more abundant goal in each block (see ‘Alternative models’ for an attempt to model how participants inferred goal frequency).</p></sec><sec id="s4-6-3"><title>MF system</title><p>A MF agent learns the value of either action directly based on the reward and punishment received. In our task, outcomes took on values of {−1,0,1}. Action values were updated incrementally via a value prediction error and learning rate for value, <inline-formula><mml:math id="inf16"><mml:mi mathvariant="normal">η</mml:mi></mml:math></inline-formula>. Below is an example updating the action value for press = <italic>j</italic> (which we omit from the right side of the equation for concision):<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">η</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p></sec><sec id="s4-6-4"><title>Goal perseveration</title><p>A GP agent uses the same matrix of estimated feature probabilities as the MB system, but multiplies them by a static vector, <inline-formula><mml:math id="inf17"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></inline-formula> , which means the system always engages both goals regardless of instructions. This is the only way in which the GP agent differs from the MB agent. Having its own utilization weights (<inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) allows the system to vary across individuals in the degree to which the ‘avoid punishment’ and ‘seek reward’ goals are each pursued when they are irrelevant:<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note, we also fit a model where the GP strategy learns its own matrix of estimated feature probabilities separate from that learned by the MB strategy (i.e., with a different learning rate), but this did not fit participants’ choices as well (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>.).</p></sec><sec id="s4-6-5"><title>Action perseveration</title><p>Actions taken on the last trial were represented by a one-hot vector (e.g., <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>), which we store in a variable, <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which was multiplied by its own utilization parameter, <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> .</p></sec><sec id="s4-6-6"><title>Stochastic decision process</title><p>All action values derived by each system were integrated via their utilization weights. Below we show the integrated action value for press = <underline><italic>j</italic></underline> (which we omit from the right side of the equation for concision):<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Note there are no utilization weights in the above equation for MB and GP <italic>Q</italic> values because they were already integrated into these <italic>Q</italic> values in <xref ref-type="disp-formula" rid="equ3 equ4 equ7">Equations 3a, 3b and 6</xref>. The integrated action value was then inputted into a softmax function to generate the policy, which can be described by the probability of pressing ‘<italic>j</italic>’ (since the probability of pressing ‘<italic>g</italic>’ is one minus that):<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi> </mml:mi><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p></sec></sec><sec id="s4-7"><title>Alternative models tested</title><p>We tested several alternative models that did not explain the data as well as the winning model described above (full details in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). First, we tested models that included only one of the strategies described above (i.e., only MB, only GP, and only MF). We then tested models in a stepwise fashion detailed in <xref ref-type="fig" rid="fig2">Figure 2</xref>, which demonstrated that adding each strategy contributed to explaining unique variance in the data.</p><p>We additionally tested a model where differences in reward seeking and punishment avoidance were captured by the learning process as opposed to the utilization of the learned knowledge. To do so, we endowed the model with different MB and GP learning rates for punishment and reward features (<inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in <xref ref-type="disp-formula" rid="equ7">Equation 6</xref>, and a single utilization weight (<inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>With regard to the MB strategy, we additionally tested a model where learning from counterfactual outcomes was implemented with the same learning rate as learning from the outcomes of one’s actions.</p><p>With regard to resource reallocation, we additionally tested models where it occurred in just the GP utilization weights, or in both GP and MB utilization (in the same fashion described in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>). After finding that data were best explained by the model where resource reallocation only occurred in the MB system, we tested if resource reallocation changed from trial to trial as function of recently experienced goals. That is, we examined whether individuals recruit more resources toward the goal one has most recently experienced, which could differ within a given task block.</p><p>With regard to the MF strategy, we tested a model where goals were encoded as part of its state representation (G-MF). Specifically, action values were learned separately for trials with punishment avoidance goals (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and reward seeking goals (<inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). In this version of an MF strategy, experienced outcomes only influence decision-making under the goal in which they were experienced. The main way it differs from an MB strategy is that learning relevant to a particular goal occurs <italic>only when that goal is active</italic>. Thus, <italic>Q</italic> values cannot track feature probabilities changing during nonmatched goal trials (e.g., how reward feature probabilities might shift during punishment avoidance trials). This may be one reason why it was inferior to the best-fitting model. Similar to the best-fitting model, this model included separate utilization weights (<inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and a single learning rate. GP and perseveration strategies were included as in the best-fitting model, and resource reallocation was applied to the G-MF strategy in the same way as described in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>.</p></sec><sec id="s4-8"><title>Model fitting</title><p>Models were fit with a hierarchical variant of expectation–maximization algorithm known as iterative importance sampling (<xref ref-type="bibr" rid="bib10">Bishop, 2006</xref>), which has been shown to provide high parameter and model recoverability (<xref ref-type="bibr" rid="bib50">Michely et al., 2020b</xref>; <xref ref-type="bibr" rid="bib49">Michely et al., 2020a</xref>). The priors for this model-fitting procedure largely do not affect the results, because the procedure iteratively updates priors via likelihood-weighted resampling in order to converge on the distributions of parameters that maximize the integrated BIC, an approximation of model evidence. Note, all parameters had weakly informed priors (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Specifically, the fitting procedure works by (1) drawing 100,000 samples from all group-level parameter distributions for each participant, (2) deriving participant-level likelihoods for each sampled parameter, (3) resampling parameters after weighting each sample by its likelihood, and (4) fitting new hyperparameters to the overall distribution of resampled parameter values. This process continues iteratively until the integrated BIC of the new parameter settings does not exceed that of the last iteration’s parameter settings.</p></sec><sec id="s4-9"><title>Model and parameter recoverability</title><p>To verify that the experiment was appropriately designed to dissociate between the tested models and their parameter values, we simulated experimental data from the best-fitting and reduced models and successfully recovered key inverse temperature parameters (all above 0.58 correlation, average correlation = 0.79). The model that generated the data was recovered 10/10 times compared to the next best-fitting model (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p></sec><sec id="s4-10"><title>Simulating mechanism-agnostic stay-switch behavior</title><p>In order to examine model predictions (<xref ref-type="fig" rid="fig2">Figure 2</xref>), we used each given model to simulate experimental data from 400 participants, each time generating a new dataset by setting model-relevant beta parameters to 5, learning rate parameters to 0.2, and other parameters to 0. We then computed the proportion of trials in which the model chose a different action compared to the previous trial. This ‘switch probability’ was computed for each combination of the previous and current trials’ goals, and the features observed on the previous trial. We ensured there were no significant differences in the direction and significance of key effects across task versions by separately fitting our Bayesian logistic regression noted above to the subset of subjects that performed each task version. Doing so showed that all effects held and to a remarkably similar degree in both task versions (see full results in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec><sec id="s4-11"><title>Simulating the optimality of each computational strategy</title><p>We simulated artificial agents playing the exact task 2000 times and plotted the mean reward earned. Each artificial agent was also endowed with a learning rate for feature probabilities, which sampled from a grid of values over the 0–1 range at 0.02 increments. For each type of agent, we set the utilization weights of the relevant strategy to five and other utilization weights (for nonused strategies) to 0.</p></sec><sec id="s4-12"><title>Testing differences between reward seeking and punishment avoidance parameters</title><p>As a consequence of the iterative nature of the model-fitting procedure, parameters for individual participants are not independently estimated, precluding the use of Bayesian or frequentist parametric tests. We thus used nonparametric tests to compute unbiased p values. Due to a heavy positive skew in the distributions of utilization weight parameters at the group level (<xref ref-type="fig" rid="fig4">Figure 4B and C</xref>), we compared between them in terms of their median levels. We note that the skew in inverse temperature parameters is to be expected given their Gamma prior distributions are inherently skewed (<xref ref-type="bibr" rid="bib28">Gillan et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Sharp et al., 2021</xref>). Thus, we generated null distributions of median differences in utilization weights for both MB and GP strategies. To do so, we ran our hierarchical model-fitting procedure 300 times on 300 simulated datasets that assumed reward and punishment utilization weights were sampled from the same distribution (null hypothesis). The utilization weights that best fitted the simulated data were used to generate the null distribution of median differences. We then generated p values for median differences in the empirical data by seeing how extreme the empirical value was with respect to the generated null distribution. Each simulated dataset comprised the same number of participants as in the original sample (<italic>n</italic> = 192) and sampled both parameters with replacement from a joint distribution representing the null hypothesis that the two parameters are equal. The null distribution was derived through running our model-fitting procedure on the empirical data for one iteration to derive true posteriors at the participant level, and combining the participant-level median parameter estimates for both parameters of interest (e.g., utilization parameters for MB reward and MB punishment) to form a joint distribution. All other parameters were drawn from the group-level distributions derived by fitting the winning model to participants’ data.</p></sec><sec id="s4-13"><title>Testing significance of resource reallocation parameters</title><p>We tested the difference of resource reallocation parameters (<inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>) from zero using a permutation test, wherein we generated a null distribution by shuffling the labels denoting task block within each participant and recording the mean for each change parameter. We then generated p-values for the empirical means of change parameters by computing the proportion of the null distribution exceeding the empirical value.</p></sec><sec id="s4-14"><title>Behavioral signatures Bayesian logistic regression</title><p>The regression sought to explain whether participants stayed (coded as 1) or switched (coded as 0) on trial ‘<italic>t</italic>’, which we refer to as ‘<inline-formula><mml:math id="inf31"><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>’ in <xref ref-type="disp-formula" rid="equ14">Equation 9</xref>, for the subset of trials where the current goal differed from the goal encountered on the previous trial. Since current and previous goals are perfectly anticorrelated in such trials, the main effect of goal was simply encoded as:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>GP effects were modeled by variables that encoded whether features were observed for both chosen and unchosen actions last trial with the following encoding scheme (here for reward):<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>MB effects were modeled by the interaction of the GP terms and the current goal as follows (here again for the MB reward effect):<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>X</mml:mi><mml:mtext> </mml:mtext><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Last, we modeled MF effects as the interaction between reward and punishment features observed for <italic>chosen</italic> actions and the last goal faced (here, for MF reward effects):<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>X</mml:mi><mml:mtext> </mml:mtext><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The dissociation between this MF signature and the MB signature described above relies on the insensitivity of the MF strategy to counterfactual outcomes, which possess no present value.</p><p>We included all independent variables in a Bayesian mixed-effects logistic regression as follows:<disp-formula id="equ14"><label>(9)</label><mml:math id="m14"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mover><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Posterior probability distributions of each effect were estimated using a sampling procedure in BAyesian Model-Building Interface (Bambi) in Python (<xref ref-type="bibr" rid="bib12">Capretto et al., 2020</xref>), which is a high-level interface using the PyMC3 Bayesian modeling software. The default sampler in Bambi an adaptive dynamic Hamiltonian Monte Carlo algorithm, which is an instance of a Markov chain Monte Carlo sampler. In all models, all estimated effects had good indicators of reliable sampling from the posterior, including r-hat below 1.1 and effective sample size above 1000 for all parameters. Note, <xref ref-type="disp-formula" rid="equ14">Equation 9</xref> is written at the participant level. Each effect was drawn from a normal group distribution the mean and variance of which were drawn from prior distributions, estimated by Bambi’s default algorithm, which is informed by implied partial correlations between the dependent and independent variables, and has been demonstrated to produce weakly informative but reasonable priors (<xref ref-type="bibr" rid="bib12">Capretto et al., 2020</xref>). For hypothesis testing, we compared the 95% most credible parameter values (i.e., the 95% highest density intervals) to a null value of 0.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Funding acquisition, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Participants gave written informed consent before taking part in the study, which was approved by the university's ethics review board (project ID number 16639/001).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Results of fitting two separate hierarchical Bayesian logistic regression models to empirical data from each version of the task.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-74402-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-74402-transrepform1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data are available in the main text or the supplementary materials. All code and analyses can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/psharp1289/multigoal_RL">https://github.com/psharp1289/multigoal_RL</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:cc43efe45753c1336673c28aa98678bd599fb174;origin=https://github.com/psharp1289/multigoal_RL;visit=swh:1:snp:a2b8d1831e0a74a31a1da2a3215c20dbfb16f7de;anchor=swh:1:rev:1cf24428da17e8bcb2fab6d0ff9a7a59ee1586f7">swh:1:rev:1cf24428da17e8bcb2fab6d0ff9a7a59ee1586f7</ext-link>.</p></sec><ack id="ack"><title>Acknowledgements</title><p>PBS is supported by a Fulbright postdoctoral fellowship. EE is supported by NIH grants R01MH124092 and R01MH125564, ISF grant 1094/20 and US Israel BSF grant 2019801. RJD holds a Wellcome Trust Investigator award (098362/Z/12/Z). The Max Planck UCL Centre for Computational Psychiatry and Ageing Research is a joint initiative supported by the Max Planck Society and University College London.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aderka</surname><given-names>IM</given-names></name><name><surname>Hofmann</surname><given-names>SG</given-names></name><name><surname>Nickerson</surname><given-names>A</given-names></name><name><surname>Hermesh</surname><given-names>H</given-names></name><name><surname>Gilboa-Schechtman</surname><given-names>E</given-names></name><name><surname>Marom</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functional impairment in social anxiety disorder</article-title><source>Journal of Anxiety Disorders</source><volume>26</volume><fpage>393</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.janxdis.2012.01.003</pub-id><pub-id pub-id-type="pmid">22306132</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akinwande</surname><given-names>MO</given-names></name><name><surname>Dikko</surname><given-names>HG</given-names></name><name><surname>Samson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variance Inflation Factor: As a Condition for the Inclusion of Suppressor Variable(s) in Regression Analysis</article-title><source>Open Journal of Statistics</source><volume>05</volume><fpage>754</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.4236/ojs.2015.57075</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amir</surname><given-names>N</given-names></name><name><surname>Najmi</surname><given-names>S</given-names></name><name><surname>Morrison</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attenuation of attention bias in obsessive-compulsive disorder</article-title><source>Behaviour Research and Therapy</source><volume>47</volume><fpage>153</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/j.brat.2008.10.020</pub-id><pub-id pub-id-type="pmid">19046576</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asmundson</surname><given-names>GJG</given-names></name><name><surname>Stein</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Selective processing of social threat in patients with generalized social phobia: Evaluation using a dot-probe paradigm</article-title><source>Journal of Anxiety Disorders</source><volume>8</volume><fpage>107</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/0887-6185(94)90009-4</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aylward</surname><given-names>J</given-names></name><name><surname>Valton</surname><given-names>V</given-names></name><name><surname>Ahn</surname><given-names>WY</given-names></name><name><surname>Bond</surname><given-names>RL</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Altered learning under uncertainty in unmedicated mood and anxiety disorders</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>1116</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0628-0</pub-id><pub-id pub-id-type="pmid">31209369</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anxiety-Like Behavioural Inhibition Is Normative under Environmental Threat-Reward Correlations</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004646</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004646</pub-id><pub-id pub-id-type="pmid">26650585</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Haim</surname><given-names>Y</given-names></name><name><surname>Lamy</surname><given-names>D</given-names></name><name><surname>Pergamin</surname><given-names>L</given-names></name><name><surname>Bakermans-Kranenburg</surname><given-names>MJ</given-names></name><name><surname>van IJzendoorn</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study</article-title><source>Psychological Bulletin</source><volume>133</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.133.1.1</pub-id><pub-id pub-id-type="pmid">17201568</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An initiation-termination two-phase model of worrying</article-title><source>Clinical Psychology Review</source><volume>30</volume><fpage>962</fpage><lpage>975</lpage><pub-id pub-id-type="doi">10.1016/j.cpr.2010.06.011</pub-id><pub-id pub-id-type="pmid">20675023</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berenbaum</surname><given-names>H</given-names></name><name><surname>Chow</surname><given-names>PI</given-names></name><name><surname>Flores</surname><given-names>LE</given-names></name><name><surname>Schoenleber</surname><given-names>M</given-names></name><name><surname>Thompson</surname><given-names>RJ</given-names></name><name><surname>Most</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A test of the initiation–termination model of worry</article-title><source>Journal of Experimental Psychopathology</source><volume>9</volume><elocation-id>204380871877896</elocation-id><pub-id pub-id-type="doi">10.1177/2043808718778965</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Pattern recognition</article-title><source>Machine Learning</source><volume>128</volume><fpage>1</fpage><lpage>738</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Jocham</surname><given-names>G</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/nn.3961</pub-id><pub-id pub-id-type="pmid">25730669</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Capretto</surname><given-names>T</given-names></name><name><surname>Piho</surname><given-names>C</given-names></name><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Westfall</surname><given-names>J</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Martin</surname><given-names>OA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bambi: A Simple Interface for Fitting Bayesian Linear Models in Python</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2012.10754">https://arxiv.org/abs/2012.10754</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Casillas</surname><given-names>A</given-names></name><name><surname>Clark</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The Mini mood and anxiety symptom questionnaire (Mini-MASQ)</article-title><conf-name>72nd annual Meeting of the Midwestern Psychological Association</conf-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charpentier</surname><given-names>CJ</given-names></name><name><surname>Aylward</surname><given-names>J</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Enhanced Risk Aversion, But Not Loss Aversion, in Unmedicated Pathological Anxiety</article-title><source>Biological Psychiatry</source><volume>81</volume><fpage>1014</fpage><lpage>1022</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2016.12.010</pub-id><pub-id pub-id-type="pmid">28126210</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cushman</surname><given-names>F</given-names></name><name><surname>Morris</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Habitual control of goal selection in humans</article-title><source>PNAS</source><volume>112</volume><fpage>13817</fpage><lpage>13822</lpage><pub-id pub-id-type="doi">10.1073/pnas.1506367112</pub-id><pub-id pub-id-type="pmid">26460050</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalgleish</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Performance on the emotional stroop task in groups of anxious, expert, and control subjects: A comparison of computer and card presentation formats</article-title><source>Cognition &amp; Emotion</source><volume>9</volume><fpage>341</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1080/02699939508408971</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dar</surname><given-names>KA</given-names></name><name><surname>Iqbal</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Worry and Rumination in Generalized Anxiety Disorder and Obsessive Compulsive Disorder</article-title><source>The Journal of Psychology</source><volume>149</volume><fpage>866</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1080/00223980.2014.986430</pub-id><pub-id pub-id-type="pmid">25495066</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-Based Influences on Humans’ Choices and Striatal Prediction Errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id><pub-id pub-id-type="pmid">21435563</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The algorithmic anatomy of model-based evaluation</article-title><source>PNAS</source><volume>369</volume><elocation-id>1655</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0478</pub-id><pub-id pub-id-type="pmid">25267820</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Improving Generalization for Temporal Difference Learning: The Successor Representation</article-title><source>Neural Computation</source><volume>5</volume><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Serotonin in affective control</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>95</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135607</pub-id><pub-id pub-id-type="pmid">19400722</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Berridge</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Model-based and model-free Pavlovian reward learning: revaluation, revision, and revelation</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><volume>14</volume><fpage>473</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.3758/s13415-014-0277-8</pub-id><pub-id pub-id-type="pmid">24647659</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Goals and habits in the brain</article-title><source>Neuron</source><volume>80</volume><fpage>312</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.007</pub-id><pub-id pub-id-type="pmid">24139036</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doron</surname><given-names>G</given-names></name><name><surname>Derby</surname><given-names>DS</given-names></name><name><surname>Szepsenwol</surname><given-names>O</given-names></name><name><surname>Talmor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Tainted love: Exploring relationship-centered obsessive compulsive symptoms in two non-clinical cohorts</article-title><source>Journal of Obsessive-Compulsive and Related Disorders</source><volume>1</volume><fpage>16</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.jocrd.2011.11.002</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foa</surname><given-names>EB</given-names></name><name><surname>Huppert</surname><given-names>JD</given-names></name><name><surname>Leiberg</surname><given-names>S</given-names></name><name><surname>Langner</surname><given-names>R</given-names></name><name><surname>Kichic</surname><given-names>R</given-names></name><name><surname>Hajcak</surname><given-names>G</given-names></name><name><surname>Salkovskis</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The Obsessive-Compulsive Inventory: development and validation of a short version</article-title><source>Psychological Assessment</source><volume>14</volume><fpage>485</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1037/1040-3590.14.4.485</pub-id><pub-id pub-id-type="pmid">12501574</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frane</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Misguided Opposition to Multiplicity Adjustment Remains a Problem</article-title><source>Journal of Modern Applied Statistical Methods</source><volume>18</volume><fpage>2</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.22237/jmasm/1556669400</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gagne</surname><given-names>C</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Peril, Prudence and Planning as Risk, Avoidance and Worry</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/tcn7e</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillan</surname><given-names>CM</given-names></name><name><surname>Kosinski</surname><given-names>M</given-names></name><name><surname>Whelan</surname><given-names>R</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Characterizing a psychiatric symptom dimension related to deficits in goal-directed control</article-title><source>eLife</source><volume>5</volume><elocation-id>e11305</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11305</pub-id><pub-id pub-id-type="pmid">26928075</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillan</surname><given-names>C</given-names></name><name><surname>Vaghi</surname><given-names>M</given-names></name><name><surname>Hezemans</surname><given-names>F</given-names></name><name><surname>van Ghesel</surname><given-names>GS</given-names></name><name><surname>Dafflon</surname><given-names>J</given-names></name><name><surname>Brühl</surname><given-names>A</given-names></name><name><surname>Savulich</surname><given-names>G</given-names></name><name><surname>Robbins</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Experimentally-Induced and Real-World Acute Anxiety Have No Effect on Goal-Directed Behaviour</article-title><source>Animal Behavior and Cognition</source><volume>1</volume><elocation-id>606145</elocation-id><pub-id pub-id-type="doi">10.1101/606145</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>RN</given-names></name><name><surname>Wichers</surname><given-names>M</given-names></name><name><surname>Wigman</surname><given-names>JTW</given-names></name><name><surname>Hartman</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Specificity of psychopathology across levels of severity: a transdiagnostic network analysis</article-title><source>Scientific Reports</source><volume>9</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-54801-y</pub-id><pub-id pub-id-type="pmid">31797974</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grupe</surname><given-names>DW</given-names></name><name><surname>Nitschke</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Uncertainty and anticipation in anxiety: an integrated neurobiological and psychological perspective</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>488</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1038/nrn3524</pub-id><pub-id pub-id-type="pmid">23783199</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Fuentemilla</surname><given-names>L</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Go and no-go learning in reward and punishment: interactions between affect and effect</article-title><source>NeuroImage</source><volume>62</volume><fpage>154</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.024</pub-id><pub-id pub-id-type="pmid">22548809</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Haines</surname><given-names>N</given-names></name><name><surname>Kvam</surname><given-names>PD</given-names></name><name><surname>Irving</surname><given-names>LH</given-names></name><name><surname>Smith</surname><given-names>C</given-names></name><name><surname>Beauchaine</surname><given-names>TP</given-names></name><name><surname>Pitt</surname><given-names>MA</given-names></name><name><surname>Ahn</surname><given-names>W-Y</given-names></name><name><surname>Turner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Theoretically Informed Generative Models Can Advance the Psychological and Brain Sciences: Lessons from the Reliability Paradox</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/xr7y3</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The Elements of Statistical Learning</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-84858-7</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>LE</given-names></name><name><surname>Meer</surname><given-names>EA</given-names></name><name><surname>Gillan</surname><given-names>CM</given-names></name><name><surname>Hsu</surname><given-names>M</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Increased and biased deliberation in social anxiety</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01180-y</pub-id><pub-id pub-id-type="pmid">34400815</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imperiale</surname><given-names>MN</given-names></name><name><surname>Lieb</surname><given-names>R</given-names></name><name><surname>Calkins</surname><given-names>ME</given-names></name><name><surname>Meinlschmidt</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multimorbidity networks of mental disorder symptom domains across psychopathology severity levels in community youth</article-title><source>Journal of Psychiatric Research</source><volume>141</volume><fpage>267</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/j.jpsychires.2021.07.010</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keramati</surname><given-names>M</given-names></name><name><surname>Smittenaar</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adaptive integration of habits into depth-limited planning defines a habitual-goal-directed spectrum</article-title><source>PNAS</source><volume>113</volume><fpage>12868</fpage><lpage>12873</lpage><pub-id pub-id-type="doi">10.1073/pnas.1609094113</pub-id><pub-id pub-id-type="pmid">27791110</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konovalov</surname><given-names>A</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mouse tracking reveals structure knowledge in the absence of model-based choice</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-15696-w</pub-id><pub-id pub-id-type="pmid">32312966</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Cushman</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems</article-title><source>Psychological Science</source><volume>28</volume><fpage>1321</fpage><lpage>1333</lpage><pub-id pub-id-type="doi">10.1177/0956797617708288</pub-id><pub-id pub-id-type="pmid">28731839</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korn</surname><given-names>CW</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Minimizing threat via heuristic and optimal policies recruits hippocampus and medial prefrontal cortex</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>733</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0603-9</pub-id><pub-id pub-id-type="pmid">31110338</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>L</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Policy compression: An information bottleneck in action selection</article-title><source>The Psychology of Learning and Motivation</source><volume>195</volume><elocation-id>004</elocation-id><pub-id pub-id-type="doi">10.1016/bs.plm.2021.02.004</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>PJ</given-names></name><name><surname>Bradley</surname><given-names>MM</given-names></name><name><surname>Cuthbert</surname><given-names>BN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Emotion, motivation, and anxiety: brain mechanisms and psychophysiology</article-title><source>Biological Psychiatry</source><volume>44</volume><fpage>1248</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/s0006-3223(98)00275-3</pub-id><pub-id pub-id-type="pmid">9861468</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langlois</surname><given-names>F</given-names></name><name><surname>Freeston</surname><given-names>MH</given-names></name><name><surname>Ladouceur</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Differences and similarities between obsessive intrusive thoughts and worry in a non-clinical population: study 1</article-title><source>Behaviour Research and Therapy</source><volume>38</volume><fpage>157</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/s0005-7967(99)00027-3</pub-id><pub-id pub-id-type="pmid">10661001</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lefebvre</surname><given-names>G</given-names></name><name><surname>Lebreton</surname><given-names>M</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Bourgeois-Gironde</surname><given-names>S</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioural and neural characterization of optimistic reinforcement learning</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0067</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</article-title><source>The Behavioral and Brain Sciences</source><volume>43</volume><elocation-id>e1</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X1900061X</pub-id><pub-id pub-id-type="pmid">30714890</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loh</surname><given-names>E</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Berron</surname><given-names>D</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name><name><surname>Guitart-Masip</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parsing the Role of the Hippocampus in Approach-Avoidance Conflict</article-title><source>Cerebral Cortex (New York, N.Y)</source><volume>27</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw378</pub-id><pub-id pub-id-type="pmid">27993819</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>N</given-names></name><name><surname>Gray</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Anxiolytic action on the behavioural inhibition system implies multiple types of arousal contribute to anxiety</article-title><source>Journal of Affective Disorders</source><volume>61</volume><fpage>161</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/s0165-0327(00)00344-x</pub-id><pub-id pub-id-type="pmid">11163419</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>ML</given-names></name><name><surname>Metzger</surname><given-names>RL</given-names></name><name><surname>Borkovec</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Development and validation of the Penn State Worry Questionnaire</article-title><source>Behaviour Research and Therapy</source><volume>28</volume><fpage>487</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1016/0005-7967(90)90135-6</pub-id><pub-id pub-id-type="pmid">2076086</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michely</surname><given-names>J</given-names></name><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Erdman</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>IM</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>SSRIs Modulate Asymmetric Learning from Reward and Punishment</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>108266</elocation-id><pub-id pub-id-type="doi">10.1101/2020.05.21.108266</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michely</surname><given-names>J</given-names></name><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Martin</surname><given-names>IM</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>A mechanistic account of serotonin’s impact on mood</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-16090-2</pub-id><pub-id pub-id-type="pmid">32393738</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The successor representation in human reinforcement learning</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id><pub-id pub-id-type="pmid">31024137</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mowrer</surname><given-names>OH</given-names></name></person-group><year iso-8601-date="1951">1951</year><article-title>Two-factor learning theory: summary and comment</article-title><source>Psychological Review</source><volume>58</volume><fpage>350</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1037/h0058956</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitschke</surname><given-names>JB</given-names></name><name><surname>Sarinopoulos</surname><given-names>I</given-names></name><name><surname>Oathes</surname><given-names>DJ</given-names></name><name><surname>Johnstone</surname><given-names>T</given-names></name><name><surname>Whalen</surname><given-names>PJ</given-names></name><name><surname>Davidson</surname><given-names>RJ</given-names></name><name><surname>Kalin</surname><given-names>NH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Anticipatory activation in the amygdala and anterior cingulate in generalized anxiety disorder and prediction of treatment response</article-title><source>The American Journal of Psychiatry</source><volume>166</volume><fpage>302</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2008.07101682</pub-id><pub-id pub-id-type="pmid">19122007</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novemsky</surname><given-names>N</given-names></name><name><surname>Kahneman</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Boundaries of Loss Aversion</article-title><source>Journal of Marketing Research</source><volume>42</volume><fpage>119</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1509/jmkr.42.2.119.62292</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Lefebvre</surname><given-names>G</given-names></name><name><surname>Kilford</surname><given-names>EJ</given-names></name><name><surname>Blakemore</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Confirmation bias in human reinforcement learning: Evidence from counterfactual feedback processing</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005684</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005684</pub-id><pub-id pub-id-type="pmid">28800597</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patzelt</surname><given-names>EH</given-names></name><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Millner</surname><given-names>AJ</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Incentives Boost Model-Based Control Across a Range of Severity on Several Psychiatric Constructs</article-title><source>Biological Psychiatry</source><volume>85</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2018.06.018</pub-id><pub-id pub-id-type="pmid">30077331</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pinker</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>How the Mind Works</source><publisher-loc>New York</publisher-loc><publisher-name>Norton</publisher-name></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Do p Values Lose Their Meaning in Exploratory Analyses? It Depends How You Define the Familywise Error Rate</article-title><source>Review of General Psychology</source><volume>21</volume><fpage>269</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1037/gpr0000123</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seow</surname><given-names>TXF</given-names></name><name><surname>Benoit</surname><given-names>E</given-names></name><name><surname>Dempsey</surname><given-names>C</given-names></name><name><surname>Jennings</surname><given-names>M</given-names></name><name><surname>Maxwell</surname><given-names>A</given-names></name><name><surname>O’Connell</surname><given-names>R</given-names></name><name><surname>Gillan</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Model-Based Planning Deficits in Compulsivity Are Linked to Faulty Neural Representations of Task Structure</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>6539</fpage><lpage>6550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0031-21.2021</pub-id><pub-id pub-id-type="pmid">34131033</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname><given-names>PB</given-names></name><name><surname>Miller</surname><given-names>GA</given-names></name><name><surname>Heller</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transdiagnostic dimensions of anxiety: Neural mechanisms, executive functions, and new directions</article-title><source>International Journal of Psychophysiology</source><volume>98</volume><fpage>365</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2015.07.001</pub-id><pub-id pub-id-type="pmid">26156938</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname><given-names>PB</given-names></name><name><surname>Eldar</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational Models of Anxiety: Nascent Efforts and Future Directions</article-title><source>Current Directions in Psychological Science</source><volume>28</volume><fpage>170</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1177/0963721418818441</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname><given-names>PB</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Eldar</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Disrupted state transition learning as a computational marker of compulsivity</article-title><source>Psychological Medicine</source><volume>1</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1017/S0033291721003846</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silton</surname><given-names>RL</given-names></name><name><surname>Heller</surname><given-names>W</given-names></name><name><surname>Engels</surname><given-names>AS</given-names></name><name><surname>Towers</surname><given-names>DN</given-names></name><name><surname>Spielberg</surname><given-names>JM</given-names></name><name><surname>Edgar</surname><given-names>JC</given-names></name><name><surname>Sass</surname><given-names>SM</given-names></name><name><surname>Stewart</surname><given-names>JL</given-names></name><name><surname>Sutton</surname><given-names>BP</given-names></name><name><surname>Banich</surname><given-names>MT</given-names></name><name><surname>Miller</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Depression and anxious apprehension distinguish frontocingulate cortical activity during top-down attentional control</article-title><source>Journal of Abnormal Psychology</source><volume>120</volume><fpage>272</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1037/a0023204</pub-id><pub-id pub-id-type="pmid">21553941</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>DJ</given-names></name><name><surname>Fineberg</surname><given-names>NA</given-names></name><name><surname>Bienvenu</surname><given-names>OJ</given-names></name><name><surname>Denys</surname><given-names>D</given-names></name><name><surname>Lochner</surname><given-names>C</given-names></name><name><surname>Nestadt</surname><given-names>G</given-names></name><name><surname>Leckman</surname><given-names>JF</given-names></name><name><surname>Rauch</surname><given-names>SL</given-names></name><name><surname>Phillips</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Should OCD be classified as an anxiety disorder in DSM-V?</article-title><source>Depression and Anxiety</source><volume>27</volume><fpage>495</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1002/da.20699</pub-id><pub-id pub-id-type="pmid">20533366</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Telch</surname><given-names>MJ</given-names></name><name><surname>Lucas</surname><given-names>JA</given-names></name><name><surname>Nelson</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Nonclinical panic in college students: an investigation of prevalence and symptomatology</article-title><source>Journal of Abnormal Psychology</source><volume>98</volume><fpage>300</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1037//0021-843x.98.3.300</pub-id><pub-id pub-id-type="pmid">2788668</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomov</surname><given-names>MS</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-task reinforcement learning in humans</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>764</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-01035-y</pub-id><pub-id pub-id-type="pmid">33510391</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Hout</surname><given-names>M</given-names></name><name><surname>Tenney</surname><given-names>N</given-names></name><name><surname>Huygens</surname><given-names>K</given-names></name><name><surname>Merckelbach</surname><given-names>H</given-names></name><name><surname>Kindt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Responding to subliminal threat cues is related to trait anxiety and emotional vulnerability: A successful replication of Macleod and Hagan (1992)</article-title><source>Behaviour Research and Therapy</source><volume>33</volume><fpage>451</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/0005-7967(94)00062-o</pub-id><pub-id pub-id-type="pmid">7755532</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>SL</given-names></name><name><surname>Heller</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Structure of Executive Dysfunction in Depression and Anxiety</article-title><source>Journal of Affective Disorders</source><volume>279</volume><fpage>208</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.jad.2020.09.132</pub-id><pub-id pub-id-type="pmid">33059224</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wise</surname><given-names>T</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Associations between aversive learning processes and transdiagnostic psychiatric symptoms in a general population sample</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-17977-w</pub-id><pub-id pub-id-type="pmid">32826918</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woody</surname><given-names>EZ</given-names></name><name><surname>Szechtman</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Adaptation to potential threat: the evolution, neurobiology, and psychopathology of the security motivation system</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>35</volume><fpage>1019</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2010.08.003</pub-id><pub-id pub-id-type="pmid">20727910</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74402.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Humans perseverate on punishment avoidance goals in multigoal reinforcement learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Claire M Gillan as Reviewing Editor and Reviewer #3, and the evaluation has been overseen by Christian Büchel as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>The reviewers were overall favourably disposed to this manuscript – it was felt that the paper extends previous work on how humans use a mix of learning strategies to make decisions in cognitively demanding environments to show that humans flexibly adjust goal-directed learning based on statistics of the environment but inflexibly engage in punishment avoidance regardless of current goals. The task design and model are novel and interesting. However, all 3 reviewers had substantial concerns (i) aspects of the behavioural analysis (modelling), (ii) the interpretations of behavioural effects, and perhaps most substantially, (iii) the extension to psychopathology. I have outlined these essential revisions below by way of summary. Individual reviewers made further suggestions that can be considered optional, though we encourage that they also be addressed as they will strengthen the paper.</p><p>Essential revisions:</p><p>1) All three reviewers felt that the clinical analyses, albeit interesting, were not approached with the same rigour and clarity as the analyses of behaviour preceding them (which were excellent). A revision must address this comprehensively, including (i) how non-independence of parameter estimates influence clinical analyses, (ii) the inclusion of clinical covariates without first reporting bivariate effects, (iii) correction for multiple comparisons should be more extensive, (iv) visualisation of all clinical effects not just select ones, (v) more complete reporting with respect to all clinical measures gathered (including analysis of somatic anxiety). Related to this, the authors may wish to consider in the discussion that there are potential difference between non-clinical and clinical cohorts in terms of correlation of clinical measures, though the data are still a bit mixed:</p><p>Imperiale, M. N., Lieb, R., Calkins, M. E., and Meinlschmidt, G. (2021). Multimorbidity networks of mental disorder symptom domains across psychopathology severity levels in community youth. Journal of psychiatric research, 141, 267-275.</p><p>Groen, R. N., Wichers, M., Wigman, J. T., and Hartman, C. A. (2019). Specificity of psychopathology across levels of severity: a transdiagnostic network analysis. Scientific reports, 9(1), 1-10.</p><p>2) With respect to the modelling of behaviour, (i) more information about parameter recovery is requested, and (ii) concerns were raised about parameter estimation more generally given the skew evident in the inverse temperatures, coupled with the multicollinearity. More information should be provided here, including proportion of recovered parameters that can capture the main parameter-based results of the paper.</p><p>3) The interpretation of the key behavioural effects: could the authors defend the interpretation that GP is not an actual strategy, but rather a noisy or distracted attempt at MB learning. It was felt that it is rather a big claim to say that the GP approach is a reasonable third strategy on top of MF/MB learning and if this interpretation is to be maintained, the authors need to back this up by showing it is something more than just an imprecise MB approach. This ties in with additional concerns about the exclusion of participants that did not obey certain rules of the task. It was not clear how their exclusion was justified given a valid interpretation of some of these key effects may be that (i) people find it hard to keep instructions in mind in complex tasks, (ii) people may be utilising strategies that you have not defined and are not well understood but are nonetheless real – e.g. belief that reward probabilities / 'luck' switches from trial to trial. Other concerns regarding exclusion were that they appear to be asymmetric with respect to reward/punishment conditions, suggesting these data are meaningful.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. Psychopathology analyses.</p><p>a. If the authors wish to make a connection to psychopathology, reporting the relationship between worry alone – rather than controlling for other overlapping symptom measures and model parameters – would be more appropriate. The recommendations for testing multiple, partially overlapping psychopathology measures in this paper may be helpful: DOI: 10.1177/21677026211017834</p><p>b. An alternate approach would be to focus this paper on the main findings about learning strategies and to save relationships to psychopathology for a future paper with a more appropriate sample.</p><p>2. Parameter-based analyses.</p><p>a. Providing more information on parameter recovery is needed. In particular, showing the proportion of recovered parameters that can capture the main parameter-based results of the paper (Figure 2C/D) would show that these findings reflect true underlying parameter differences rather than artifacts of model estimation.</p><p>b. If the authors retain the psychopathology analyses, they should be conducted in a way that does not assume independence of parameter estimates.</p><p>c. Alternatively, the analyses using relative model fits and trialwise regressions provide most of the information needed for the conclusions of the paper. The parameter-based analyses could be omitted with the focus instead on these other kinds of analyses.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors used the term &quot;predicted&quot; quite a bit to describe associations. I don't think this is justified (they haven't really done any predictive analyses).</p><p>If I understand correctly, the same 4 random walks were used for all participants (randomised between the 4 associations). Of the two shown, one looks much more stable than the other. It would be useful to see all 4 walks to see how comparable they are (if I am correct that the same 4 are used for all participants). If the walks are very different, should their allocation to the associations be controlled for in the analysis?</p><p>It would be useful to report the relationship between worry and the block effect (i.e. you suggest high worry is associated with higher GP/lower MB for losses-do worried people adapt to changes in the base rates of the outcomes?).</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Well done on an interesting read and a contribution that will be informative for a lot of researchers. I have some suggestions to improve the paper.</p><p>All analyses with the 3 clinical factors should be presented in full; including supplementary figures if possible. Simple associations should be carried out before adding covariates to assist the reader in interpreting these findings and in generating hypotheses based on them. OCD is said to be not related to parameters at p=.08, while worry is at p=0.04 (uncorrected i guess more like p=0.02 for the latter), these are not likely to be different from one-another. And they may depend on the inclusion of these variables in the same model. Reader needs more transparency around these effects and any claims of specificity need more support. The data presented actually suggests the opposite.</p><p>Relatedly, the result in relation to worry, the effect is marginal at p=.04. While 2 multiple comparisons are controlled for, this is a fairly liberal decision given several tests were conducted and reported (i.e. GP MB and MF for punishment/reward = 6 at least; plus the 3 clinical scales = 18 etc). I'd encourage the authors to report all of the associations in a table, correct for multiple comparisons. This will serve the same purpose of suggesting the most interesting avenue for future research but also give the reader a fuller view on specificity of this to worry. This exploratory framing for the clinical effects does not detract from the main contribution of the paper or the potential for this to be especially interesting for 'worry' – it would just make them clearer and let the reader decide that for themselves a bit more.</p><p>There needs to be a bit more done with respect to relating the clinical variables to the model parameters. I would have thought this would be best placed within the hierarchical model itself. Alternatively, I wonder if these is a point-estimate that could be generated that is more flexible and less dependent on the overall group effects and other parameter values.</p><p>The authors describe issues with collinearity of the parameter values. Can a correlation matrix in the supplement be included that reports these (I think currently you can sort of see it based on simulated vs real data, but this is not the same as correlating real vs real across params).</p><p>I strongly encourage all subjects are retained (though i feel less strongly about excluding those not completing enough trials, 90% even seems a bit harsh/wasteful of data). If not, then a clear justification for why the strategy or approach of these subjects is not an accurate reflection of potentially the decision making preferences of 22% of the population. More standard indicators of inattentive responding focus on RTs, overly rigid responding that renders modelling choice impossible. Not clear why these were not used here as they seem better justified indicators of inattentive subjects. At the risk of belabouring the point(!), defining these subjects as 'not understanding instructions' could be applied to many of the key findings of this paper (i.e. avoidance perseveration suggests they don't pay attention to the current goals etc). So I think this practice is not ideal.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74402.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All three reviewers felt that the clinical analyses, albeit interesting, were not approached with the same rigour and clarity as the analyses of behaviour preceding them (which were excellent). A revision must address this comprehensively, including (i) how non-independence of parameter estimates influence clinical analyses, (ii) the inclusion of clinical covariates without first reporting bivariate effects, (iii) correction for multiple comparisons should be more extensive, (iv) visualisation of all clinical effects not just select ones, (v) more complete reporting with respect to all clinical measures gathered (including analysis of somatic anxiety). Related to this, the authors may wish to consider in the discussion that there are potential difference between non-clinical and clinical cohorts in terms of correlation of clinical measures, though the data are still a bit mixed:</p><p>Imperiale, M. N., Lieb, R., Calkins, M. E., and Meinlschmidt, G. (2021). Multimorbidity networks of mental disorder symptom domains across psychopathology severity levels in community youth. Journal of psychiatric research, 141, 267-275.</p><p>Groen, R. N., Wichers, M., Wigman, J. T., and Hartman, C. A. (2019). Specificity of psychopathology across levels of severity: a transdiagnostic network analysis. Scientific reports, 9(1), 1-10.</p></disp-quote><p>We thank the reviewers and editor for thoughtful comments, including suggestions for improving the clarity and comprehensiveness of our clinical analyses. Our approach to addressing this set of concerns is twofold: Firstly, we followed Reviewer 3’s suggestion (see Reviewer comment 3.7) to re-frame the clinical analyses as exploratory, requiring further testing before definitive conclusions can be drawn. Secondly, in accordance with the reviewers’ suggestions, we substantially expanded the breadth and clarity of the reported analyses. We next detail these two sets of modifications.</p><p>A. Re-framing clinical analyses and exploratory and preliminary</p><p>To clarify that present clinical analyses are exploratory, and further investigation is required to test the validity of the findings, we implemented the following changes to the text:</p><p>I. A revision of the Abstract to make clear that inferences about worry and punishment perseveration are preliminary:</p><p><bold>“</bold>Importantly, we show preliminary evidence that individuals with chronic worry may have difficulty disengaging from punishment avoidance when instructed to seek reward. Taken together, the findings demonstrate that people avoid punishment less flexibly than they pursue reward. Future studies should test in larger samples whether a difficulty to disengage from punishment avoidance contributes to chronic worry.”</p><p>II. A revision of the Introduction, in which we now state that our analyses on psychopathology should be regarded as exploratory, and require further testing in larger samples with more targeted hypotheses (p.4):</p><p>“Finally, in a series of exploratory analyses, we determined whether and how anxious individuals express a preference for punishment avoidance goals. In so doing, we found preliminary evidence that the degree of reliance on a goal-perseveration strategy to avoid punishment was positively associated with dispositional worry, which appears unique to those expressing worry and not to individuals with obsessive-compulsive or somatic anxiety symptoms.”</p><p>III. A revision of the Discussion to emphasize the tentative nature of conclusions we can draw regarding our analyses on worry, and also to consider that relationships between symptoms and cognitive indices may differ in a clinical population (p.18):</p><p>“Given that present results are preliminary in nature, future studies will need to test a prediction that chronic worry is associated with punishment perseveration in a larger sample. This should also include testing whether this association holds in a clinical population, as variation in symptoms in a clinical population may relate to punishment perseveration differently (Imperiale et al., 2021; Groen et al., 2019).”</p><p>B. More comprehensive reporting of clinical analyses</p><p>To further address the reviewers’ concerns, we expanded the reporting of the clinical analyses as follows:</p><p>I. We now report the adjusted p-values using a family-wise error correction approach for exploratory research (Rubin, 2017), and we explicitly note that correction typically employed in confirmatory research would render the results insignificant. The cited exploratory approach defines the family of tests by the number of tests within a given exploratory hypothesis. In the present set of analyses, we consider two exploratory hypotheses: (1) that anxiety is associated with greater GP punishment and (2) that anxiety is associated with greater MB punishment. We multiplied α level by 2 for both hypotheses because we explored whether these processes were associated with either somatic anxiety or chronic worry, controlling for co-occurring psychopathology and traits of non-interest (IQ, attention and effort). Our approach, and its associated caveat, are now noted in Results (p.15):</p><p>“It is important to note that all aforementioned p-values testing our key hypotheses (Table 2B) are corrected for multiple comparisons using a correction procedure designed for exploratory research (Rubin, 2017), which only controls for number of statistical tests within each hypothesis. Using a more conservative Bonferroni error correction for all 4 regression models, as typically employed in hypothesis-driven confirmatory work (Frane, 2020), resulted in a p-value for the key effect of worry and punishment perseveration that no longer passed a conventional significance thresholds (p=0.08). Thus, future work with a more targeted, hypothesis-driven approach needs to be conducted to ensure our tentative inferences regarding worry are valid and robust.”</p><p>II. To make transparent the associations between key variables in the model and forms of psychopathology, we now present in the main text and in new Figure 5 all relevant bivariate relationships, including with somatic anxiety. We have thus added the following to Results (p.14):</p><p>“We first report the bivariate relations between each form of psychopathology and inverse temperature parameters reflecting tendencies to utilize MB and GP punishment avoidance. Given that individuals with OCD and anxiety symptoms may over-prioritize threat detection, it is conceivable that there is a relationship between all three forms of psychopathology and model-based punishment avoidance. However, we found no significant or trending relationships between any form of psychopathology and model-based control for punishment avoidance (Figure 5A, left column). An alternative possibility is that individuals with anxiety suffer from a dysregulation in goal pursuit, reflecting a failure to disengage punishment avoidance when instructed to do so. On this basis, we explored whether worry and somatic anxiety are positively associated with goal-perseveration for punishment avoidance. In so doing we found initial evidence of a trending relationship between the tendency to worry and punishment avoidance perseveration (B=2.15, t = 1.4, p=0.16; Figure 5A, right column).”</p><p>III. The reviewers correctly point out that individual parameters were non-independently estimated. However, these only serve as predictors in the reported regression analyses, and thus, these analyses make no assumption that they were independently sampled (Hastie et al., 2009). We now clarify this issue in Methods (p.21):</p><p>“Although the computational parameters were non-independently estimated by our hierarchical model-fitting procedure, it is vital to note this does not compromise the validity of the least-squares solution to the regressions we ran. Indeed, Friedman, Hastie, and Tibshirani (2009) show that, ‘Even if the independent variables were not drawn randomly, the criterion is still valid if the dependent variables are conditionally independent given the [independent variable] inputs’ (p.44).”</p><p>Nevertheless, to mitigate doubt regarding the results of our regression analyses, we conducted nonparametric permutation tests, wherein we shuffled the task data with respect to the psychopathology scores. We include these nonparametric analyses in Results (p.14):</p><p>“For each regression model, we computed p-values using a nonparametric permutation test wherein we shuffled the task data with respect to the psychopathology scores, repeating the analysis on each of 10,000 shuffled datasets to derive an empirical null distribution of the relevant t-statistics.”</p><p>IV. To motivate use of covariates in the regression analyses, we now expound in Results on why we included each covariate. Additionally, we now validate the inclusion of covariates by showing that a regression with the covariates predicts left-out data better than regression without covariates (p.14-15):</p><p>“To provide a more specific test of our key hypotheses, we removed variance of non-interest in order to sensitize our analyses to unique relationships between forms of psychopathology and types of punishment avoidance. Firstly, generalized, as opposed specific obsessive, worry is thought to be particularly associated with difficulty in disengaging from worry (Berenbaum, 2010), since it lasts significantly longer in both clinical (Dar and Iqbal, 2015) and community samples (Langlois, Freeston, and Ladouceur, 2000). Thus, we dissociated generalized from obsessive worry using the same approach taken in previous studies (Doron et al., 2013; Stein et al., 2010), namely, by including a measure of OCD symptoms as a control covariate. Controlling for OCD symptoms has the additional benefit of accounting for known relations between OCD and poor learning of task structure, reduced model-based control, and perseverative tendencies (Gillan et al., 2016; Seow et al., 2021; Sharp et al., 2021). Secondly, another potentially confounding relationship exists between worry and somatic anxiety (e.g., Sharp, Miller and Heller, 2015), likely reflecting a general anxiety factor. Thus, we isolated worry by controlling for somatic anxiety, as commonly done in studies seeking to quantify distinct relationships of worry and somatic anxiety with cognitive performance (e.g., Warren, Miller and Heller, 2021) or associated neural mechanisms (e.g., Silton et al., 2011). Finally, we controlled for covariance between computational strategies that might reflect general task competencies. This included the utilization of MB (including learning rates and inverse temperatures) since observed anticorrelations in the empirical data (Figure S7) between GP and MB may derive from causal factors such as attention or IQ, as well as a general tendency to mitigate cognitive effort by using less costly strategies (AP, MF and GP inverse temperatures; Figure S7). This analysis showed a stronger relationship between worry and punishment perseveration (ß=3.14 (1.38), t = 2.27, p=0.04, Figure 5C). No other significant relationship was observed between punishment perseveration or model-based punishment avoidance and psychopathology (Figure 5C). Ultimately, we validated the full model using a 5-fold cross-validation procedure which showed that regressing worry onto the aforementioned covariates (using a ridge regression implementation) explains significantly more variance in left out test-set data (R<sup>2</sup>=0.24) relative to the models of the bivariate relationships between worry and GP Punishment (R<sup>2</sup>=0.01) and MB Punishment (R<sup>2</sup>=0.00).”</p><disp-quote content-type="editor-comment"><p>2) With respect to the modelling of behaviour, (i) more information about parameter recovery is requested, and (ii) concerns were raised about parameter estimation more generally given the skew evident in the inverse temperatures, coupled with the multicollinearity. More information should be provided here, including proportion of recovered parameters that can capture the main parameter-based results of the paper.</p></disp-quote><p>We thank the reviewers for raising these concerns regarding parameter multicollinearity, skew, and recoverability. To address these, we now show that multicollinearity involving the key inverse temperature parameters is low, and clarify that skew is expected given the inherently skewed γ priors that we (and many others) use to model these parameters. Additionally, we demonstrate that these parameters are highly recoverable, in line with levels reported in extant computational modelling literature. The changes to the manuscript include:</p><p>I. Multicollinearity. We show that multicollinearity involving key parameters of interest is relatively low, within acceptable levels with respect to prior studies. To report levels of multicollinearity more comprehensively, we now include a full heatmap of correlations between fitted parameters in Figure 4 —figure supplement 4. The only large correlation was between the learning rate for factual and counterfactual feature outcomes (r=0.68). Modest correlations were also observed between model-based inverse temperatures (r=0.37) and between model-based change parameters (r=0.34).</p><p>Moreover, we clarify the levels of multicollinearity between fitted parameters in Methods (p.21):</p><p>“We report all bivariate correlations between fitted parameters in Figure 4 – Supplementary figure 4.”</p><p>II. Skew. We clarify that skew is expected for key inverse temperature parameters, given that they were modelled with γ prior distributions that are inherently skewed. Indeed, extant studies show similar levels of skew in parameter distributions modelled using γ priors (Gillan et al., 2016; Sharp, Dolan and Eldar, 2021). We now clarify this issue in Methods (p.25):</p><p>“We note that the skew in inverse temperature parameters is to be expected given their Γ prior distributions are inherently skewed (Gillan et al., 2016; Sharp, Dolan and Eldar, 2021).”</p><p>III. Recoverability. As the Reviewers point out, an inability to dissociate between certain parameters is a common problem in reinforcement learning modelling (e.g., Palminteri et al., 2017). Thus, we now provide recoverability levels for all model parameters and how they trade-off against each other, in new Figure S4. Recoverability of the four parameters of interest, MB and GP for reward and punishment, spans the correlation range of 0.76 to 0.91, as consistent with levels of parameter recovery in extant studies (Haines, Vasilleva and Ahn, 2018; Palminteri et al., 2017). Additionally, all between-parameter (i.e., off-diagonal) correlations involving the key model parameters were low (i.e., weaker than 0.16), showing that the experimental design and model-fitting procedure were capable of successfully dissociating between these parameters (Wilson and Collins, 2019). This is now reported in Results (p.12):</p><p>“We validated that MB, GP and MF inverse temperature parameters were recoverable from simulated experimental data, and that the degree of recoverability (i.e., the correlations of true and recovered parameter values, which were between 0.76 and 0.91; Figure S4) was in line with extant reinforcement learning modelling studies (Haines, Vasilleva and Ahn, 2018; Palminteri et al., 2017). Similarly, low correlations between estimated parameters (all weaker than 0.16) demonstrate our experimental design and model-fitting procedure successfully dissociated between model parameters (Wilson and Collins, 2019).”</p><disp-quote content-type="editor-comment"><p>3) The interpretation of the key behavioural effects: could the authors defend the interpretation that GP is not an actual strategy, but rather a noisy or distracted attempt at MB learning. It was felt that it is rather a big claim to say that the GP approach is a reasonable third strategy on top of MF/MB learning and if this interpretation is to be maintained, the authors need to back this up by showing it is something more than just an imprecise MB approach.</p></disp-quote><p>We thank the reviewers for an opportunity to both clarify our interpretation of how GP relates to MB and MF, as well as expand our argument that GP is a strategic heuristic, and not simply a noisy or distracted attempt at MB learning. Below, we begin by clarifying that, since it shares its data structures with MB, GP is best thought of as a heuristic alteration to MB learning. We then argue that this heuristic is indeed a resource rational strategy, not simply a goal-forgetting MB agent.</p><p>Firstly, a purely forgetful agent would not demonstrate the observed valence effect that is rational for agents wishing to avoid potentially fatal punishment. Secondly, our experiment was specifically designed to prevent forgetting in that the instructed goal was presented on the screen for the entire duration that participants deliberated their decisions. Thirdly, we show through formalizing a model of MB forgetting that a key prediction of that model – namely, that GP and MB utilization should be highly positively correlated – does not hold in our empirical data. Finally, we expound on the various ways that GP computations save costly resources while producing high-performance (in terms of reward earned) policies.</p><p>I. The GP system is a strategically modified model-based strategy.</p><p>We first clarify that GP corresponds to a MB agent that strategically avoids goal-switching, and therefore is not as distinct from MB as might have been suggested in the previous version of our manuscript. To emphasize this in the text, we have added the following to Results (p.6):</p><p>“An alternative strategy, that we term “goal-perseveration” (GP), might strike a better balance between simplicity and effectiveness. This strategy inherits the exact same knowledge of feature probabilities acquired by model-based learning, but simplifies action selection by persistently avoiding punishment and seeking reward, simultaneously, regardless of instructed goal. This, in principle, eliminates effortful goal-switching while utilizing all available information about the changing action-feature mapping. Thus, rather than constituting a separate decision system in its own right, GP is best thought of as a behavior produced by a strategic cost-saving MB agent.”</p><p>II. Forgetting the reward function would not predict a valence effect in GP.</p><p>A non-strategic forgetful MB agent would be just as likely to pursue the wrong goal regardless of which goal is presented on screen. This would be inconsistent with the significant valence effect that we observed, where participants tended to pursue the uninstructed goal predominantly during reward trials. The observed valence effect is consistent, instead, with a rational strategy proposed in prior work (Woody and Szechtman, 2011). Specifically, a punishment avoidance system should be far more attuned to false negatives (failing to detect a true threat) than its reward system counterpart because such missed attempts to avoid punishment could be fatal. We now highlight this point further in the Discussion (p.16):</p><p>“The strategic deployment of GP primarily towards punishment avoidance indicates such behavior is not merely a reflection of a noisy or forgetful MB system. Indeed, our finding that humans use less flexible computational strategies to avoid punishment, than to seek reward, aligns with the idea of distinct neural mechanisms supporting avoidance and approach behavior (McNaughton and Gray, 2000; Lang, Bradley and Cuthbert, 1998). Moreover, comparative ethology and evolutionary psychology (Pinker, 1997) suggest there are good reasons why punishment avoidance might be less flexible than reward seeking. Woody and Szechtman (2011) opined that “to reduce the potentially deadly occurrence of false negative errors (failure to prepare for upcoming danger), it is adaptive for the system to tolerate a high rate of false positive errors (false alarms).” Indeed, we demonstrate that in the presence of multiple shifting goals, perseverance in punishment avoidance results in false positives during reward seeking (Figure 3B), but avoids ‘missing’ punishment avoidance opportunities because of lapses in goal switching (Figure 3C). Future work could further test these ideas, as well as potential alternative explanations (Dayan and Huys, 2009).”</p><p>III. A model-based strategy that forgets the reward provides a worse account of participant choices than a combination of MB and GP strategies.</p><p>Reviewer 2 (comment 2.1) additionally suggested that a forgetful MB system could potentially explain the valanced effect of GP if forgetting of the goal set occurred more often on either reward or punishment trials. Although we argue that a dependence of forgetting on goal valence suggests that such forgetting would itself be strategic, in the interest of determining whether such an account could in principle explain our results, we now fit a new model (Forgetful + MB + MF + AP) that addresses this potential mechanism. This model included all components of the winning model, except for the GP system, and extended the model-based system to include two additional parameters, <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> which determine the probability of forgetting the presented reward vector (which defines the goal) on reward trials and punishment trials respectively. Specifically, according to the model, on each trial according to a fixed probability (either <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> depending on whether the trial had a reward or punishment goal) the participant replaced the instructed goal with the opposite goal (e.g., if the actual goal was punishment avoidance, the participant used the reward pursuit goal).</p><p>We found that this model fit the data worse than a model which included separate MB and GP controllers with no goal forgetting (MB + MF + GP + AP), thereby confirming that a model where the model-based controller forgets the current goal with different rates on reward and punishment trials does not account for our results supporting GP as well. We have added this modelling result in Figure 4 —figure supplement 3 and amended the figure to include the new BIC for this model.</p><p>In order to investigate what aspects of the data were better captured by the winning model, we used the forgetful model (Forgetful + MB + MF + AP) to simulate a new dataset, using the best-fit parameters for each participant. We predicted that because in this model both MB and (apparent) GP-like effects on choice emerge from a single MB strategy (that at times forgets the rewards) then such effects should be correlated in the simulated dataset. We confirmed this prediction by fitting the GP model (MB + MF + GP + AP) to this simulated data, and showing that MB and GP parameters, within a valence, were indeed correlated. In contrast, when fit to the actual data, no correlation between MB and GP effects are detected, suggesting that these do not emerge from a single MB system, but instead reflect distinct task strategies. We present <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> the empirical GP correlations from our data on the left, and the simulated GP correlations generated by forgetful MB agents:</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>A comparison of GP-MB correlations for empirical data (left) and for data simulated using forgetful-MB agents (right).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-sa2-fig1-v2.tif"/></fig><p>We have added this text to our model comparison results detailed in Figure 4 —figure supplement 4:</p><p>“Finally, we tested an alternative model where GP behavior may derive from a MB strategy that occasionally forgets the reward function (Forgetful-MB+MthF+AP), allowing this forgetting to occur at different rates during reward and punishment goals. This model-based agent includes two additional parameters, <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which govern the probability of forgetting the presented reward function on reward pursuit trials and punishment avoidance trials respectively. Thus, on each trial, the model replaces the instructed goal with the opposite goal (e.g. if the actual goal was [-1, 0], the participant used [0,1]) with some fixed probability (either <inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, depending on the trial type). We again found that this model fit worse than the winning model, confirming that a model where the model-based controller forgets the current reward with different rates on reward and punishment trials does not account for our results supporting GP as well.”</p><p>IV. Availability of reward information on screen makes forgetting of reward information implausible. In addition to the aforementioned empirical evidence for why a forgetful MB agent is unlikely to explain our results, we also clarify here that we designed the task so as to reduce this specific kind of forgetting. We did so by continuously presenting the instructed goal on the screen while participants made their choice (presented in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>). The instructed goal only disappeared once a choice was made.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>A depiction of what participants saw on the screen for the entire decision period in our multigoal reinforcement learning task.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-sa2-fig2-v2.tif"/></fig><p>Thus, any forgetting could be easily remedied by glancing at the screen. We now highlight in Results (p. 5) this additional reason to discount the possibility the participants forgot the current reward instructions:</p><p>“Note that the reward value of either feature was continuously presented throughout the choice deliberation time (Figure 1b), ensuring that there should be no reason for participants to forget the present trial’s goal.”</p><p>V. GP is a resource rational task strategy.</p><p>A necessary criterion for considering GP a heuristic strategy, as opposed to simply reflecting noise or error, is that it needs to fulfil a function that would cause it to be selected. This stands in contrast to errors, which do not fulfil such a function, and an agent would be better off avoiding. We suggest that GP fulfils a function of bypassing computational costs that make model-based RL computationally demanding, while still achieving good performance in the task. Our perspective here is inspired by the resource rationality framework, which argue that many heuristic deviations from seemingly rational strategies (in this task model-based reinforcement learning), rather than reflecting errors, instead reflect strategic balancing of task performance with computational costs of implementation (Gigerenzer and Goldstein, 2011; Lieder and Griffiths, 2020).</p><p>To see how GP may constitute a resource rational strategy, consider that the key feature of this task which makes model-based RL difficult to utilize is that the goals switch between trials. When the goals switch, the model-based agent is required to change which feature predictions it uses to derive action values, which takes time. At the same time, feature predictions need to be constantly updated even on trials for which a feature was not relevant, creating a burdensome dissociation between information used for decision and information used for learning. Like model-free RL, a GP agent entirely avoids these goal-switching costs, while obtaining substantially greater rewards than a model-free agent would. It does so by constantly using the same model-based feature predictions. Additionally, as noted above with regards to the observed valence effect, a GP agent that prioritizes avoiding losses can achieve this as well as model-based learning, while avoiding the relevant switching costs.</p><p>We have now elaborated in the Discussion (p. 14-15) how GP avoids the costs of MB control, and how it can constitute a resource rational heuristic that approximates model-based evaluation:</p><p>“GP may thus in fact constitute a resource-rational strategy (Lieder and Griffiths, 2020) for approximating MB control. To illustrate this, consider that model-based learning is computationally demanding in our task specifically because goals switch between trials. When the goals switch, a model-based agent must retrieve and use predictions concerning a different feature. Additionally, the agent needs to continuously update its predictions concerning features even when they are not presently relevant for planning. GP avoids these computationally costly operations by pursuing goals persistently, thus avoiding switching and ensuring that features are equally relevant for planning and learning. In this way, GP saves substantial computational resources compared to MB yet is able to perform relatively well on the task, achieving better performance than MF. Additionally, if a participant selectively cares about avoiding losses (for instance, due to loss aversion), GP can perform as well as MB. Thus, we propose the GP heuristic reflects a strategic choice, which can achieve good performance while avoiding the substantial resource requirements associated with model-based control. In this sense it fulfils a similar role as other proposed approximations to model-based evaluation including model-free RL (Sutton and Barto, 2018), the successor representation (Dayan, 1993; Momennejad et al., 2017), mixing model-based and model-free evaluation (Keramati, Smittenaar, Dolan, and Dayan, 2016), habitual goal selection (Cushman and Morris, 2015) and other identified heuristics in action evaluation (Daw and Dayan, 2014).”</p><disp-quote content-type="editor-comment"><p>This ties in with additional concerns about the exclusion of participants that did not obey certain rules of the task. It was not clear how their exclusion was justified given a valid interpretation of some of these key effects may be that (i) people find it hard to keep instructions in mind in complex tasks, (ii) people may be utilising strategies that you have not defined and are not well understood but are nonetheless real – e.g. belief that reward probabilities / 'luck' switches from trial to trial. Other concerns regarding exclusion were that they appear to be asymmetric with respect to reward/punishment conditions, suggesting these data are meaningful.</p></disp-quote><p>We thank the reviewers for pointing out the need to explain the rationale underlying our criteria for excluding participants. We first note that it is the norm in studies of learning and decision making to exclude participants whose performance is indistinguishable from pure guessing (e.g., Bornstein and Daw, 2013; Otto et al., 2013). Equivalently, in the present study our approach was to exclude only participants whose strategy was tantamount to performing at chance-level or below. We now show this by simulating each of the excluded strategies and measuring its performance. Furthermore, we measure the excluded participants’ actual accuracy and show that it was indeed indistinguishable from chance. In addition, we clarify that these participants’ fundamentally different model of the task makes it impossible to estimate the effects of interest for these participants. Finally, we conduct a sensitivity analysis in which we include these excluded participants and demonstrate that the key effects in our paper all hold.</p><p>I. Chance-level performance or worse among excluded participants and their strategies</p><p>We first verified through simulation that the strategies that we used as exclusion criteria yield equal or worse reward than purely guessing. Thus, we simulated the performance in our task of agents that either (i) treated reward features as punishment features, (ii) treated punishment features as reward features, or (iii) both (i.e., reversed the meaning of feature types), and compared them to agents that (iv) purely guess, (v) use both feature types in a model-based way, and finally (vi) use both feature types as intended but using a purely goal-perseveration strategy. We thus show that the strategies of participants we excluded (i, ii, and iii) do as poorly or worse than purely guessing. This motivation for the exclusion criteria is now reported in Methods (p.19):</p><p>“These errors in following task structure are fundamental failures that result in average performance that is as poor or worse than an agent that purely guesses which action to take at each trial (Figure 6).”</p><p>Secondly, we examined participants’ actual accuracy, by comparing their choices to those of an ideal observer model would make given the participants’ observations. In the excluded sample, participants on average chose correctly 49.65% of the time, whereas in the included group participants chose correctly 63.5% of the time (difference between groups: t=9.66,p&lt;0.00001). We now add the following text to the Methods (p.19):</p><p>“Excluded subjects performed significantly worse in terms of choice accuracy. To derive accuracy, we computed the percentage of choices subjects made in line with an ideal observer that experienced the same outcome history (in terms of features) as each participant. On average, excluded subjects chose correctly 49.6% of time, whereas included subjects chose correctly 63.5% of time (difference between groups: t(190)=9.66, p&lt;0.00001).“</p><p>II. Including subjects that misunderstand feature types would add noise to the hypothesis tests</p><p>Including participants we previously excluded would inject considerable noise into our estimation of how reward vs. punishment feature types were differentially utilized. For example, estimates of how individuals avoided punishment features would include participants that treated these features as if they were reward. Therefore, statistical tests of MB and GP goal valence differences would be contaminated by such subjects. In fact, testing whether punishment and reward goals were pursued to different degrees with a given strategy (which is the hypothesis of the study) is a non-sensical test for subjects that treated the task as if there was only one goal or that confused reward and punishment. Furthermore, given their distorted model of the task (where the task was reduced to one goal), these subjects had no incentive to use a more complex strategy than simple model-free learning, and thus their inclusion corrupts our ability to infer how subjects recruit different goal-directed systems for decision making. We now expand on these reasons in Methods (p.19):</p><p>“Additionally, including such subjects would reduce our sensitivity to estimating differences in the utilization of GP and MB for goals of differing valence, as such subjects treated the task as if there was only a single goal, or that the goals were opposite to their instructed nature. Moreover, given their model of the task, such subjects could approach the task optimally using a MF strategy, and thus would not be incentivized to use goal-directed strategies at all.”</p><p>III. Sensitivity Analysis</p><p>To mitigate any remaining concern about subject exclusion, we conducted a sensitivity analysis that aligns with Reviewer 3’s suggestion to determine if our key effects hold in the larger sample that includes subjects that incorrectly treat certain feature types as if they were another type. Modelling the large sample (n=242; 98% retention) required us to allow negative inverse temperature parameters, since these are required to account for subjects who for instance treated a reward feature as if it were a punishment feature and the converse. We thus replaced the γ priors for the inverse temperature parameters with normal distributions. The analyses for this larger sample resulted in all key results in the main paper holding, and in some cases, strengthening. We now note in the Methods (p.20):</p><p>“To determine whether our relatively strict subject exclusion policy might have affected the results, we conducted a sensitivity analysis on a larger sample (n=248; 98% retention) including subjects that mistreated the instructed value of certain features. To account for these subjects’ behavior, we used normal priors to allow negative inverse temperature parameters. Fitting these revised models to our data, we again demonstrate that our winning model was the best-fitting model compared to all other models. Second, we show that the GP valence effect held and even came out stronger in this larger sample. Thus, the mean difference in GP utilization for punishment and reward goals was 0.24 in our original sample and 0.50 in the larger sample (p&lt; 0.0001). Finally, we show the MB valence effect also held in this larger sample (original sample mean difference between MB reward and MB punishment = 2.10 ; larger sample mean difference = 1.27, both p-values &lt; 0.0001).”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. Psychopathology analyses.</p><p>a. If the authors wish to make a connection to psychopathology, reporting the relationship between worry alone – rather than controlling for other overlapping symptom measures and model parameters – would be more appropriate. The recommendations for testing multiple, partially overlapping psychopathology measures in this paper may be helpful: DOI: 10.1177/21677026211017834</p></disp-quote><p>We agree with the reviewer that presenting bivariate relationships between psychopathology and task strategies is important and now present in full bivariate relations between all dimensions of psychopathology and the computational parameters of interest, which we detail in response to Editor comment 1, Points II and IV. However, as noted above, we have principled reasons for controlling for co-occurring forms of psychopathology, and thus have chosen to present both analyses.</p><disp-quote content-type="editor-comment"><p>b. An alternate approach would be to focus this paper on the main findings about learning strategies and to save relationships to psychopathology for a future paper with a more appropriate sample.</p></disp-quote><p>We thank the reviewer for the suggestion, which is indicative of the tentative nature of psychopathology findings which we agree are preliminary in nature. However, we believe a more transparent alternative is to include these analyses and re-frame them as exploratory, per R3’s suggestion. Doing so will enable future studies to target worry and goal-perseveration a priori. We thus amended our framing of the psychopathology analyses as exploratory in the Abstract, Introduction and Discussion. Each of the changes to the main text is detailed in response to Editor Comment 1, Re-framing clinical analyses and exploratory and preliminary.</p><disp-quote content-type="editor-comment"><p>2. Parameter-based analyses.</p><p>a. Providing more information on parameter recovery is needed. In particular, showing the proportion of recovered parameters that can capture the main parameter-based results of the paper (Figure 2C/D) would show that these findings reflect true underlying parameter differences rather than artifacts of model estimation.</p><p>b. If the authors retain the psychopathology analyses, they should be conducted in a way that does not assume independence of parameter estimates.</p><p>c. Alternatively, the analyses using relative model fits and trialwise regressions provide most of the information needed for the conclusions of the paper. The parameter-based analyses could be omitted with the focus instead on these other kinds of analyses.</p></disp-quote><p>Although we agree with the reviewer that there is substantial overlap in the model-agnostic regression and parameter-based analyses, we opt for retaining both sets of analyses because only the computational modelling explains subjects’ choices beyond what can be explained solely based on the previous trial’s observations. Thus, we now emphasize the added value of analyzing the fitted parameters from the computational model in Results (p.11):</p><p>“The presence of unique signatures of MB, MF, and GP decision strategies in the empirical data presents strong evidence for the use of these strategies, but the signature measures are limited to examining goal-switch trials and, within those trials, examining the impact of features observed on the very last trial. To comprehensively quantify the extent to which participants utilized each strategy for reward seeking and punishment avoidance, we next developed a series of computational models that aim to explain all participant choices given the features observed on all preceding trials.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The authors used the term &quot;predicted&quot; quite a bit to describe associations. I don't think this is justified (they haven't really done any predictive analyses).</p></disp-quote><p>We apologize for the excessive use of causal language to describe regression results. We have now changed this terminology to “positively/negatively associated with” throughout the article.</p><disp-quote content-type="editor-comment"><p>If I understand correctly, the same 4 random walks were used for all participants (randomised between the 4 associations). Of the two shown, one looks much more stable than the other. It would be useful to see all 4 walks to see how comparable they are (if I am correct that the same 4 are used for all participants). If the walks are very different, should their allocation to the associations be controlled for in the analysis?</p></disp-quote><p>To clarify, the task included two types of random walks: the first was more volatile (the best bandit switched once per block), while the second had more irreducible uncertainty (significantly closer across both random walks to 0.5 probability), both of which make learning more difficult. Importantly, random walks were counterbalanced across subjects: in version 1 of the task, the reward feature took the first type of random walk (i.e., more volatility) and the punishment feature took the second type of random walk (i.e., more irreducible uncertainty). In task version 2, the feature:random walk mapping was flipped.</p><fig id="sa2fig3" position="float"><label>Author response image 3.</label><caption><title>Random walks from task version 1.</title><p>Here, the reward feature took a more volatile walk, whereas the punishment feature had greater irreducible uncertainty. In task version 2 (given to the other half of participants) the feature:random walk mapping was flipped.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74402-sa2-fig3-v2.tif"/></fig><p>To test whether there were significant differences in GP and MB valence effects as a function of which feature type was paired with a given random walk, we fitted hierarchical logistic regressions quantifying the reported model-agnostic signatures (Figure 2C in text) twice, so as to independently analyze data from each task version (i.e., using each feature:random walk mapping). This allowed us to compare whether GP and MB valence effects change as a function of the feature:random walk mapping. We demonstrate across both task versions that the signature of MB Reward &gt; MB Punishment and the signature of GP Punishment &gt; GP Reward. Moreover, there is striking consistency in estimates across task versions and substantial overlap in the estimated HDIs for each effect (Table S1; ‘HDI’ refers to the 94% highest density interval of the posterior, bounded by the 3% and 97% quantiles). We now report this new validatory analysis in Methods (p.25):</p><p>“We ensured there were no significant differences in the direction and significance of key effects across task versions by separately fitting our Bayesian logistic regression noted above to the subset of subjects that performed each task version. Doing so showed that all effects held and to a remarkably similar degree in both task versions (see full results in Supplemental Table 1).”</p><disp-quote content-type="editor-comment"><p>It would be useful to report the relationship between worry and the block effect (i.e. you suggest high worry is associated with higher GP/lower MB for losses-do worried people adapt to changes in the base rates of the outcomes?).</p></disp-quote><p>We thank the reviewer for this suggestion. We have now tested for a possibility of an interaction between worry and a block effect, and the results did not support this interaction. This analysis is now reported in Results (p.15):</p><p>“Of note, we additionally found no association between the parameter governing how MB punishment was modulated by task block and levels of worry, both when including worry alone (ß=2.5 (1.91), t=1.31, p=0.19) and when controlling for the same covariates as detailed above (ß=1.46 (1.65), t=0.88, p=0.38).”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Well done on an interesting read and a contribution that will be informative for a lot of researchers. I have some suggestions to improve the paper.</p><p>All analyses with the 3 clinical factors should be presented in full; including supplementary figures if possible. Simple associations should be carried out before adding covariates to assist the reader in interpreting these findings and in generating hypotheses based on them. OCD is said to be not related to parameters at p=.08, while worry is at p=0.04 (uncorrected i guess more like p=0.02 for the latter), these are not likely to be different from one-another. And they may depend on the inclusion of these variables in the same model. Reader needs more transparency around these effects and any claims of specificity need more support. The data presented actually suggests the opposite.</p></disp-quote><p>We thank the reviewer for helpful suggestions to improve the clarity and transparency of our clinical analyses. We now present in full all our analyses, as detailed in response to Editor comment 1. Of note here, the trending <italic>negative</italic> relationship between OCD and punishment perseveration was in the opposite direction of the relationship between punishment perseveration and worry. This is now clearly highlighted in an updated Figure 5<bold>.</bold></p><disp-quote content-type="editor-comment"><p>Relatedly, the result in relation to worry, the effect is marginal at p=.04. While 2 multiple comparisons are controlled for, this is a fairly liberal decision given several tests were conducted and reported (i.e. GP MB and MF for punishment/reward = 6 at least; plus the 3 clinical scales = 18 etc). I'd encourage the authors to report all of the associations in a table, correct for multiple comparisons. This will serve the same purpose of suggesting the most interesting avenue for future research but also give the reader a fuller view on specificity of this to worry. This exploratory framing for the clinical effects does not detract from the main contribution of the paper or the potential for this to be especially interesting for 'worry' – it would just make them clearer and let the reader decide that for themselves a bit more.</p></disp-quote><p>We have changed the language about our hypotheses at the article’s outset, present all results in full, and present corrected and uncorrected p-values to be transparent about our correction for multiple comparisons. We temper our claims about the relation between worry and punishment perseveration in the Abstract, Introduction and Discussion, as detailed above in response to Editor comment 1, Re-framing clinical analyses and exploratory and preliminary.</p><disp-quote content-type="editor-comment"><p>There needs to be a bit more done with respect to relating the clinical variables to the model parameters. I would have thought this would be best placed within the hierarchical model itself. Alternatively, I wonder if these is a point-estimate that could be generated that is more flexible and less dependent on the overall group effects and other parameter values.</p><p>The authors describe issues with collinearity of the parameter values. Can a correlation matrix in the supplement be included that reports these (I think currently you can sort of see it based on simulated vs real data, but this is not the same as correlating real vs real across params).</p></disp-quote><p>We now report this heatmap in Figure 4 —figure supplement 4, detailed in response to Editor comment 2.</p><disp-quote content-type="editor-comment"><p>I strongly encourage all subjects are retained (though i feel less strongly about excluding those not completing enough trials, 90% even seems a bit harsh/wasteful of data). If not, then a clear justification for why the strategy or approach of these subjects is not an accurate reflection of potentially the decision making preferences of 22% of the population. More standard indicators of inattentive responding focus on RTs, overly rigid responding that renders modelling choice impossible. Not clear why these were not used here as they seem better justified indicators of inattentive subjects. At the risk of belabouring the point(!), defining these subjects as 'not understanding instructions' could be applied to many of the key findings of this paper (i.e. avoidance perseveration suggests they don't pay attention to the current goals etc). So I think this practice is not ideal.</p></disp-quote><p>We agree with the reviewer that more comprehensive justification for, and scrutinization of, our exclusion criteria is warranted. We first demonstrate, by simulating agents that would be excluded in our study due to misunderstanding feature types, that the excluded strategies' performance in terms of points won is equal to, or worse than, that of agents that are purely guessing which action to take at each trial. By constrast, simulating a goal-perseveration strategy shows that it is far more adaptive in terms of reward earned and reduction in computation costs. We additionally show that excluded subjects as a group indeed perform at chance level on average. Finally, we show in a sensitivity analysis that if we include such subjects, all the major effects hold, and in some cases, become even stronger. We address this concern thoroughly in response to Editor comment 4.</p><p>References</p><p>1. Bornstein, A. M., and Daw, N. D. (2013). Cortical and hippocampal correlates of deliberation during model-based decisions for rewards in humans. PLoS Computational Biology, 9(12), e1003387.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>2. Cushman, F., and Morris, A. (2015). Habitual control of goal selection in humans. Proceedings of the National Academy of Sciences, 112(45), 13817-13822.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>3. Dar, K. A., and Iqbal, N. (2015). Worry and rumination in generalized anxiety disorder and obsessive compulsive disorder. The Journal of psychology, 149(8), 866-880.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>4. Daw, N. D., and Dayan, P. (2014). The algorithmic anatomy of model-based evaluation. Philosophical Transactions of the Royal Society B: Biological Sciences, 369(1655), 20130478.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>5. Dayan, P. (1993). Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4), 613-624.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>6. Doron, G., Derby, D. S., Szepsenwol, O., and Talmor, D. (2012). Tainted love: Exploring relationship-centered obsessive compulsive symptoms in two non-clinical cohorts. Journal of Obsessive-Compulsive and Related Disorders, 1(1), 16-24.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>7. Frane, A. V. (2020). Misguided opposition to multiplicity adjustment remains a problem. Journal of Modern Applied Statistical Methods, 18(2), 28.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>8. Friedman, J., Hastie, T., and Tibshirani, R. (2009). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.</p><p>9. Gigerenzer, G., and Goldstein, D. G. (2011). The recognition heuristic: A decade of research. Judgment and Decision Making, 6(1), 100-121.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>10. Gillan, C. M., Kosinski, M., Whelan, R., Phelps, E. A., and Daw, N. D. (2016). Characterizing a psychiatric symptom dimension related to deficits in goal-directed control. eLife, 5, e11305.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>11. Groen, R. N., Wichers, M., Wigman, J. T., and Hartman, C. A. (2019). Specificity of psychopathology across levels of severity: a transdiagnostic network analysis. Scientific reports, 9(1), 1-10.</p><p>12. Haines, N., Vassileva, J., and Ahn, WY (2018). The outcome ‐ representation learning model: A novel reinforcement learning model of the iowa gambling task. Cognitive Science , 42 (8), 2534-2561.</p><p>13. Haines, N., Kvam, P. D., Irving, L. H., Smith, C., Beauchaine, T. P., Pitt, M. A., and Turner, B. (2020). Theoretically Informed Generative Models Can Advance the Psychological and Brain Sciences: Lessons from the Reliability Paradox. PsyArxiv.</p><p>14. Imperiale, M. N., Lieb, R., Calkins, M. E., and Meinlschmidt, G. (2021). Multimorbidity networks of mental disorder symptom domains across psychopathology severity levels in community youth. Journal of psychiatric research, 141, 267-275.</p><p>15. Keramati, M., Smittenaar, P., Dolan, R. J., and Dayan, P. (2016). Adaptive integration of habits into depth-limited planning defines a habitual-goal–directed spectrum. Proceedings of the National Academy of Sciences, 113(45), 12868-12873.</p><p>16. Langlois, F., Freeston, M. H., and Ladouceur, R. (2000). Differences and similarities between obsessive intrusive thoughts and worry in a non-clinical population: Study 1. Behaviour Research and Therapy, 38(2), 157-173.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>17. Leeuwenberg, A. M., van Smeden, M., Langendijk, J. A., van der Schaaf, A., Mauer, M. E., Moons, K. G., and Schuit, E. (2021). Comparing methods addressing multi-collinearity when developing prediction models. arXiv preprint arXiv:2101.01603.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>18. Lieder, F., and Griffiths, TL (2020). Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Behavioral and Brain Sciences , 43.</p><p>19. Momennejad, I., Russek, EM, Cheong, JH, Botvinick, MM, Daw, ND, and Gershman, SJ (2017). The successor representation in human reinforcement learning. Nature Human Behavior , 1 (9), 680-692.</p><p>20. Otto, AR, Raio, CM, Chiang, A., Phelps, EA, and Daw, ND (2013). Working-memory capacity protects model-based learning from stress. Proceedings of the National Academy of Sciences , 110 (52), 20941-20946.</p><p>21. Palminteri, S., Lefebvre, G., Kilford, E. J., and Blakemore, S. J. (2017). Confirmation bias in human reinforcement learning: Evidence from counterfactual feedback processing. PLoS Computational Biology, 13(8), e1005684.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>22. Rubin, M. (2017). Do p values lose their meaning in exploratory analyzes? It depends on how you define the familywise error rate. Review of General Psychology , 21 (3), 269-275.</p><p>23. Seow, T. X., Benoit, E., Dempsey, C., Jennings, M., Maxwell, A., O'Connell, R., and Gillan, C. M. (2021). Model-based planning deficits in compulsivity are linked to faulty neural representations of task structure. Journal of Neuroscience, 41(30), 6539-6550.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>24. Sharp, PB, Dolan, RJ, and Eldar, E. (2021). Disrupted state transition learning as a computational marker of compulsivity. Psychological Medicine , 1-11.</p><p>25. Silton, R. L., Heller, W., Engels, A. S., Towers, D. N., Spielberg, J. M., Edgar, J. C., … and Miller, G. A. (2011). Depression and anxious apprehension distinguish frontocingulate cortical activity during top-down attentional control. Journal of Abnormal Psychology, 120(2), 272.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>26. Stein, D. J., Fineberg, N. A., Bienvenu, O. J., Denys, D., Lochner, C., Nestadt, G., … and Phillips, K. A. (2010). Should OCD be classified as an anxiety disorder in DSM‐V?. Depression and anxiety, 27(6), 495-506.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>27. Sutton, R. S., and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>28. Warren, S. L., Heller, W., and Miller, G. A. (2021). The structure of executive dysfunction in depression and anxiety. Journal of Affective Disorders, 279, 208-216.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>29. Wilson, R. C., and Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, e49547.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p><p>30. Woody, E. Z., and Szechtman, H. (2011). Adaptation to potential threat: the evolution, neurobiology, and psychopathology of the security motivation system. Neuroscience and Biobehavioral Reviews, 35(4), 1019-1033.&lt;milestone-start /&gt;‏&lt;milestone-end /&gt;</p></body></sub-article></article>