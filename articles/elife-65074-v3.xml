<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">65074</article-id><article-id pub-id-type="doi">10.7554/eLife.65074</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Likelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-216258"><name><surname>Fengler</surname><given-names>Alexander</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0104-3905</contrib-id><email>alexander_fengler@brown.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-216259"><name><surname>Govindarajan</surname><given-names>Lakshmi N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0936-2919</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-216260"><name><surname>Chen</surname><given-names>Tony</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-24218"><name><surname>Frank</surname><given-names>Michael J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8451-0523</contrib-id><email>Michael_Frank@brown.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Department of Cognitive, Linguistic and Psychological Sciences, Brown University</institution><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Carney Institute for Brain Science, Brown University</institution><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Psychology and Neuroscience Department, Boston College</institution><addr-line><named-content content-type="city">Chestnut Hill</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role>Reviewing Editor</role><aff><institution>École normale supérieure, PSL University, INSERM</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>04</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e65074</elocation-id><history><date date-type="received" iso-8601-date="2020-11-21"><day>21</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-04-01"><day>01</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Fengler et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Fengler et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-65074-v3.pdf"/><abstract><p>In cognitive neuroscience, computational modeling can formally adjudicate between theories and affords quantitative fits to behavioral/brain data. Pragmatically, however, the space of plausible generative models considered is dramatically limited by the set of models with known likelihood functions. For many models, the lack of a closed-form likelihood typically impedes Bayesian inference methods. As a result, standard models are evaluated for convenience, even when other models might be superior. Likelihood-free methods exist but are limited by their computational cost or their restriction to particular inference scenarios. Here, we propose neural networks that learn approximate likelihoods for arbitrary generative models, allowing fast posterior sampling with only a one-off cost for model simulations that is amortized for future inference. We show that these methods can accurately recover posterior parameter distributions for a variety of neurocognitive process models. We provide code allowing users to deploy these methods for arbitrary hierarchical model instantiations without further training.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Cognitive neuroscience studies the links between the physical brain and cognition. Computational models that attempt to describe the relationships between the brain and specific behaviours quantitatively is becoming increasingly popular in this field. This approach may help determine the causes of certain behaviours and make predictions about what behaviours will be triggered by specific changes in the brain.</p><p>Many of the computational models used in cognitive neuroscience are built based on experimental data. A good model can predict the results of new experiments given a specific set of conditions with few parameters. Candidate models are often called ‘generative’: models that can simulate data. However, typically, cognitive neuroscience studies require going the other way around: they need to infer models and their parameters from experimental data. Ideally, it should also be possible to properly assess the remaining uncertainty over the parameters after having access to the experimental data. To facilitate this, the Bayesian approach to statistical analysis has become popular in cognitive neuroscience.</p><p>Common software tools for Bayesian estimation require a ‘likelihood function’, which measures how well a model fits experimental data for given values of the unknown parameters. A major obstacle is that for all but the most common models, obtaining precise likelihood functions is computationally costly. In practice, this requirement limits researchers to evaluating and comparing a small subset of neurocognitive models for which a likelihood function is known. As a result, it is convenience, rather than theoretical interest, that guides this process.</p><p>In order to provide one solution for this problem, Fengler et al. developed a method that allows users to perform Bayesian inference on a larger number of models without high simulation costs. This method uses likelihood approximation networks (LANs), a computational tool that can estimate likelihood functions for a broad class of models of decision making, allowing researchers to estimate parameters and to measure how well models fit the data. Additionally, Fengler et al. provide both the code needed to build networks using their approach and a tutorial for users.</p><p>The new method, along with the user-friendly tool, may help to discover more realistic brain dynamics underlying cognition and behaviour as well as alterations in brain function.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>approximate bayesian computation</kwd><kwd>neural networks</kwd><kwd>sequential sampling models</kwd><kwd>cognitive neuroscience</kwd><kwd>computational models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>P50 MH119467-01</award-id><principal-award-recipient><name><surname>Frank</surname><given-names>Michael J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01 MH084840-08A1</award-id><principal-award-recipient><name><surname>Frank</surname><given-names>Michael J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel method and software provides researchers with the capability to rapidly, flexibly, and robustly perform Bayesian parameter estimation of theoretically meaningful models in cognitive neuroscience that were heretofore intractable.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Computational modeling has gained traction in cognitive neuroscience in part because it can guide principled interpretations of functional demands of cognitive systems while maintaining a level of tractability in the production of quantitative fits of brain-behavior relationships. For example, models of reinforcement learning (RL) are frequently used to estimate the neural correlates of the exploration/exploitation tradeoff, of asymmetric learning from positive versus negative outcomes, or of model-based vs. model-free control (<xref ref-type="bibr" rid="bib75">Schönberg et al., 2007</xref>; <xref ref-type="bibr" rid="bib57">Niv et al., 2012</xref>; <xref ref-type="bibr" rid="bib24">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib94">Zajkowski et al., 2017</xref>; <xref ref-type="bibr" rid="bib5">Badre et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Daw et al., 2011b</xref>). Similarly, models of dynamic decision-making processes are commonly used to disentangle the strength of the evidence for a given choice from the amount of that evidence needed to commit to any choice, and how such parameters are impacted by reward, attention, and neural variability across species (<xref ref-type="bibr" rid="bib66">Rangel et al., 2008</xref>; <xref ref-type="bibr" rid="bib23">Forstmann et al., 2010</xref>; <xref ref-type="bibr" rid="bib44">Krajbich and Rangel, 2011</xref>; <xref ref-type="bibr" rid="bib25">Frank et al., 2015</xref>; <xref ref-type="bibr" rid="bib93">Yartsev et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Doi et al., 2020</xref>). Parameter estimates might also be used as a theoretically driven method to reduce the dimensionality of brain/behavioral data that can be used for prediction of, for example, clinical status in computational psychiatry (<xref ref-type="bibr" rid="bib40">Huys et al., 2016</xref>).</p><p>Interpreting such parameter estimates requires robust methods that can estimate their generative values, ideally including their uncertainty. For this purpose, Bayesian statistical methods have gained traction. The basic conceptual idea in Bayesian statistics is to treat parameters θ and data <inline-formula><mml:math id="inf1"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> as stemming from a joint probability model <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Statistical inference proceeds by using Bayes’ rule,<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>to get at <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the posterior distribution over parameters' given data. <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is known as the prior distribution over the parameters θ, and <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is known as the evidence or just probability of the data (a quantity of importance for model comparison). The term <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the probability (density) of the dataset <inline-formula><mml:math id="inf7"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> given parameters θ, is known as the likelihood (in accordance with usual notation, we will also write <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the following). It is common in cognitive science to represent likelihoods as <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf10"><mml:mi>s</mml:mi></mml:math></inline-formula> specifies a particular stimulus. We suppress <inline-formula><mml:math id="inf11"><mml:mi>s</mml:mi></mml:math></inline-formula> in our notation, but note that our approach easily generalizes when explicitly conditioning on trial-wise stimuli. Bayesian parameter estimation is a natural way to characterize uncertainty over parameter values. In turn, it provides a way to identify and probe parameter tradeoffs. While we often do not have access to <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> directly, we can draw samples from it instead, for example, via Markov Chain Monte Carlo (MCMC) (<xref ref-type="bibr" rid="bib74">Robert and Casella, 2013</xref>; <xref ref-type="bibr" rid="bib18">Diaconis, 2009</xref>; <xref ref-type="bibr" rid="bib73">Robert and Casella, 2011</xref>).</p><p>Bayesian estimation of the full posterior distributions over model parameters contrasts with maximum likelihood estimation (MLE) methods that are often used to extract single best parameter values, without considering their uncertainty or whether other parameter estimates might give similar fits. Bayesian methods naturally extend to settings that assume an implicit hierarchy in the generative model in which parameter estimates at the individual level are informed by the distribution across a group, or even to assess within an individual how trial-by-trial variation in (for example) neural activity can impact parameter estimates (commonly known simply as hierarchical inference). Several toolboxes exist for estimating the parameters of popular models like the drift diffusion model (DDM) of decision-making and are widely used by the community for this purpose (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Heathcote et al., 2019</xref>; <xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Ahn et al., 2017</xref>). Various studies have used these methods to characterize how variability in neural activity, and manipulations thereof, alters learning and decision parameters that can quantitatively explain variability in choice and response time distributions (<xref ref-type="bibr" rid="bib12">Cavanagh et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Frank et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Herz et al., 2016</xref>; <xref ref-type="bibr" rid="bib63">Pedersen and Frank, 2020</xref>).</p><p>Traditionally, however, posterior sampling or MLE for such models required analytical likelihood functions: a closed-form mathematical expression providing the likelihood of observing specific data (reaction times and choices) for a given model and parameter setting. This requirement limits the application of any likelihood-based method to a relatively small subset of cognitive models chosen for so-defined convenience instead of theoretical interest. Consequently, model comparison and estimation exercises are constrained as many important but likelihood-free models were effectively <italic>untestable</italic> or required weeks to process a single model formulation. Testing any slight adjustment to the generative model (e.g., different hierarchical grouping or splitting conditions) requires a repeated time investment of the same order. For illustration, we focus on the class of sequential sampling models (SSMs) applied to decision-making scenarios, with the most common variants of the DDM. The approach is, however, applicable to any arbitrary domain.</p><p>In the standard DDM, a two-alternative choice decision is modeled as a noisy accumulation of evidence toward one of two decision boundaries (<xref ref-type="bibr" rid="bib70">Ratcliff and McKoon, 2008</xref>). This model is widely used as it can flexibly capture variations in choice, error rates, and response time distributions across a range of cognitive domains and its parameters have both psychological and neural implications. While the likelihood function is available for the standard DDM and some variants including inter-trial variability of its drift parameter, even seemingly small changes to the model form, such as dynamically varying decision bounds (<xref ref-type="bibr" rid="bib14">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Hawkins et al., 2015</xref>) or multiple-choice alternatives (<xref ref-type="bibr" rid="bib44">Krajbich and Rangel, 2011</xref>), are prohibitive for likelihood-based estimation, and instead require expensive Monte Carlo (MC) simulations, often without providing estimates of uncertainty across parameters.</p><p>In the last decade and a half, approximate Bayesian computation (ABC) algorithms have grown to prominence (<xref ref-type="bibr" rid="bib78">Sisson et al., 2018</xref>). These algorithms enable one to sample from posterior distributions over model parameters, where models are defined only as simulators, which can be used to construct empirical likelihood distributions (<xref ref-type="bibr" rid="bib78">Sisson et al., 2018</xref>). ABC approaches have enjoyed successful application across life and physical sciences (e.g., <xref ref-type="bibr" rid="bib4">Akeret et al., 2015</xref>), and notably, in cognitive science (<xref ref-type="bibr" rid="bib82">Turner and Van Zandt, 2018</xref>), enabling researchers to estimate theoretically interesting models that were heretofore intractable. However, while there have been many advances without sacrificing information loss in the posterior distributions (<xref ref-type="bibr" rid="bib81">Turner and Sederberg, 2014</xref>; <xref ref-type="bibr" rid="bib38">Holmes, 2015</xref>), such ABC methods typically require many simulations to generate synthetic or empirical likelihood distributions, and hence can be computationally expensive (in some cases prohibitive – it can take weeks to estimate parameters for a single model). This issue is further exacerbated when embedded within a sequential MCMC sampling scheme, which is needed for unbiased estimates of posterior distributions. For example, one typically needs to simulate between 10,000 and 100,000 times (the exact number varies depending on the model) for each proposed combination of parameters (i.e., for each sample along a Markov chain [MC], which may itself contain tens of thousands of samples), after which they are discarded. This situation is illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>, where the red arrows point at the computations involved in the approach suggested by <xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>High level overview of the proposed methods.</title><p>(<bold>A</bold>) The space of theoretically interesting models in the cognitive neurosciences (red) is much larger than the space of mechanistic models with analytical likelihood functions (green). Traditional approximate Bayesian computation (ABC) methods require models that have low-dimensional sufficient statistics (blue). (<bold>B</bold>) illustrates how likelihood approximation networks can be used in lieu of online simulations for efficient posterior sampling. The left panel shows the predominant 'probability density approximation' (PDA) method used for ABC in the cognitive sciences (<xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>). For each step along a Markov chain, 10K–100K simulations are required to obtain a single likelihood estimate. The right panel shows how we can avoid the simulation steps during inference using amortized likelihood networks that have been pretrained using empirical likelihood functions (operationally in this paper: kernel density estimates and discretized histograms).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig1-v3.tif"/></fig><p>To address this type of issue, the statistics and machine learning communities have increasingly focused on strategies for the amortization of simulation-based computations (<xref ref-type="bibr" rid="bib33">Gutmann et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib60">Papamakarios et al., 2019a</xref>; <xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Radev et al., 2020b</xref>; <xref ref-type="bibr" rid="bib64">Radev et al., 2020a</xref>; <xref ref-type="bibr" rid="bib29">Gonçalves et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Järvenpää et al., 2021</xref>). The aim is generally to use model simulations upfront and learn a reusable approximation of the function of interest (targets can be the likelihood, the evidence, or the posterior directly).</p><p>In this paper, we develop a general ABC method (and toolbox) that allows users to perform inference on a significant number of neurocognitive models without repeatedly incurring substantial simulation costs. To motivate our particular approach and situate it in the context of other methods, we outline the following key desiderata:</p><list list-type="order"><list-item><p>The method needs to be easily and rapidly deployable to end users for Bayesian inference on various models hitherto deemed computationally intractable. This desideratum naturally leads us to an amortization approach, where end users can benefit from costs incurred upfront.</p></list-item><list-item><p>Our method should be sufficiently flexible to support arbitrary inference scenarios, including hierarchical inference, estimation of latent covariate (e.g., neural) processes on the model parameters, arbitrary specification of parameters across experimental conditions, and without limitations on dataset sizes. This desideratum leads us to amortize the likelihood functions, which (unlike other amortization strategies) can be immediately applied to arbitrary inference scenarios without further cost.</p></list-item><list-item><p>We desire approximations that do not a priori sacrifice covariance structure in the parameter posteriors, a limitation often induced for tractability in variational approaches to approximate inference (<xref ref-type="bibr" rid="bib8">Blei et al., 2017</xref>).</p></list-item><list-item><p>End users should have access to a convenient interface that integrates the new methods seamlessly into preexisting workflows. The aim is to allow users to get access to a growing database of amortized models through this toolbox and enable increasingly complex models to be fit to experimental data routinely, with minimal adjustments to the user’s working code. For this purpose, we will provide an extension to the widely used HDDM toolbox (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Pedersen and Frank, 2020</xref>) for parameter estimation of DDM and RL models.</p></list-item><list-item><p>Our approach should exploit modern computing architectures, specifically parallel computation. This leads us to focus on the encapsulation of likelihoods into neural network (NN) architectures, which will allow batch processing for posterior inference.</p></list-item></list><p>Guided by these desiderata, we developed two amortization strategies based on NNs and empirical likelihood functions. Rather than simulate during inference, we instead train NNs as parametric function approximators to learn the likelihood function from an initial set of a priori simulations across a wide range of parameters. By learning (log) likelihood functions directly, we avoid posterior distortions that result from inappropriately chosen (user-defined) summary statistics and corresponding distance measures as applied in traditional ABC methods (<xref ref-type="bibr" rid="bib78">Sisson et al., 2018</xref>). Once trained, likelihood evaluation only requires a forward pass through the NN (as if it were an analytical likelihood) instead of necessitating simulations. Moreover, any algorithm can be used to facilitate posterior sampling, MLE or maximum a posteriori estimation (MAP).</p><p>For generality, and because they each have their advantages, we use two classes of architectures, multilayered perceptrons (MLPs) and convolutional neural networks (CNNs), and two different posterior sampling methods (MCMC and importance sampling). We show proofs of concepts using posterior sampling and parameter recovery studies for a range of cognitive process models of interest. The trained NNs provide the community with a (continually expandable) bank of encapsulated likelihood functions that facilitate consideration of a larger (previously computationally inaccessible) set of cognitive models, with orders of magnitude speedup relative to simulation-based methods. This speedup is possible because costly simulations only have to be run once per model upfront and henceforth be avoided during inference: previously executed computations are amortized and then shared with the community.</p><p>Moreover, we develop standardized amortization pipelines that allow the user to apply this method to arbitrary models, requiring them to provide only a functioning simulator of their model of choice.</p><p>In the 'Approximate Bayesian Computation' section, we situate our approach in the greater context of ABC, with a brief review of online and amortization algorithms. The subsequent sections describe our amortization pipelines, including two distinct strategies, as well as their suggested use cases. The 'Test beds' section provides an overview of the (neuro)cognitive process models that comprise our benchmarks. The 'Results' section shows proof-of-concept parameter recovery studies for the two proposed algorithms, demonstrating that the method accurately recovers both the posterior mean and variance (uncertainty) of generative model parameters, and that it does so at a runtime speed of orders of magnitude faster than traditional ABC approaches without further training. We further demonstrate an application to hierarchical inference, in which our trained networks can be imported into widely used toolboxes for arbitrary inference scenarios. In the'Discussion' and the last section, we further situate our work in the context of other ABC amortization strategies and discuss the limitations and future work.</p><sec id="s1-1"><title>Approximate Bayesian computation</title><p>ABC methods apply when one has access to a parametric stochastic simulator (also referred to as generative model), but, unlike the usual setting for statistical inference, no access to an explicit mathematical formula for the likelihood of observations given the simulator’s parameter setting.</p><p>While likelihood functions for a stochastic stimulator can in principle be mathematically derived, this can be exceedingly challenging even for some historically famous models such as the Ornstein–Uhlenbeck (OU) process (<xref ref-type="bibr" rid="bib45">Lipton and Kaushansky, 2018</xref>) and may be intractable in many others. Consequently, the statistical community has increasingly developed ABC tools that enable posterior inference of such ‘likelihood-free’ stochastic models while completely bypassing any likelihood derivations (<xref ref-type="bibr" rid="bib15">Cranmer et al., 2020</xref>).</p><p>Given a parametric stochastic simulator model <inline-formula><mml:math id="inf13"><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:math></inline-formula> and dataset <inline-formula><mml:math id="inf14"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula>, instead of exact inference based on <inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, these methods attempt to draw samples from an approximate posterior <inline-formula><mml:math id="inf16"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Consider the following general equation:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>𝐱</mml:mi></mml:msub></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mpadded width="+5pt"><mml:msub><mml:mi>s</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>s</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub></mml:math></inline-formula> refers to sufficient statistics (roughly, summary statistics of a dataset that retain sufficient information about the parameters of the generative process). <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> refers to a kernel-based distance measure/cost function, which evaluates a probability density function for a given distance between the observed and expected summary statistics <inline-formula><mml:math id="inf19"><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="bold">𝐱</mml:mi></mml:msub></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow></mml:math></inline-formula>. The parameter <inline-formula><mml:math id="inf20"><mml:mi>h</mml:mi></mml:math></inline-formula> (commonly known as bandwidth parameter) modulates the cost gradient. Higher values of <inline-formula><mml:math id="inf21"><mml:mi>h</mml:mi></mml:math></inline-formula> lead to more graceful decreases in cost (and therefore a worse approximation of the true posterior).</p><p>By generating simulations, one can use such summary statistics to obtain approximate likelihood functions, denoted as <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where approximation error can be mitigated by generating large numbers of simulations. The caveat is that the amount of simulation runs needed to achieve a desired degree of accuracy in the posterior can render such techniques computationally infeasible.</p><p>With a focus on amortization, our goal is to leverage some of the insights and developments in the ABC community to develop NN architectures that can learn approximate likelihoods deployable for any inference scenario (and indeed any inference method, including MCMC, variational inference, or even MLE) without necessitating repeated training. We next describe our particular approach and return to a more detailed comparison to existing methods in the 'Discussion' section.</p></sec><sec id="s1-2"><title>Learning the likelihood with simple NN architectures</title><p>In this section, we outline our approach to amortization of computational costs of large numbers of simulations required by traditional ABC methods. Amortization approaches incur a one-off (potentially quite large) simulation cost to enable cheap, repeated inference for any dataset. Recent research has led to substantial developments in this area (<xref ref-type="bibr" rid="bib15">Cranmer et al., 2020</xref>). The most straightforward approach is to simply simulate large amounts of data and compile a database of how model parameters are related to observed summary statistics of the data (<xref ref-type="bibr" rid="bib50">Mestdagh et al., 2019</xref>). This database can then be used during parameter estimation in empirical datasets using a combination of nearest-neighbor search and local interpolation methods. However, this approach suffers from the curse of dimensionality with respect to storage demands (a problem that is magnified with increasing model parameters). Moreover, its reliance on summary statistics (<xref ref-type="bibr" rid="bib78">Sisson et al., 2018</xref>) does not naturally facilitate flexible reuse across inference scenarios (e.g., hierarchical models, multiple conditions while fixing some parameters across conditions, etc.).</p><p>To fulfill all desiderata outlined in the introduction, we focus on directly encapsulating the likelihood function over empirical observations of a simulation model so that likelihood evaluation is substantially cheaper than constructing (empirical) likelihoods via model simulation online during inference. Such empirical observations could be, for example, trial-wise choices and reaction times (RTs). This strategy then allows for flexible reuse of such approximate likelihood functions <inline-formula><mml:math id="inf23"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in a large variety of inference scenarios applicable to common experimental design paradigms. Specifically, we encapsulate <inline-formula><mml:math id="inf24"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as a feed-forward NN, which allows for parallel evaluation by design. We refer to these networks as likelihood approximation networks (LANs).</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> spells out the setting (A) and usage (B) of such a method. The LAN architectures used in this paper are simple, small in size, and are made available for download for local usage. While this approach does not allow for instantaneous posterior inference, it does considerably reduce computation time (by up to three orders of magnitude; see ‘Run time’ section) when compared to approaches that demand simulations at inference. Notably, this approach also substantially speeds up inference even for models that are not entirely likelihood free but nevertheless require costly numerical methods to obtain likelihoods. Examples are the full-DDM with inter-trial variability in parameters (for which likelihoods can be obtained via numerical integration, as is commonly done in software packages such as HDDM but with substantial cost), but also other numerical methods for generalized DDMs (<xref ref-type="bibr" rid="bib76">Shinn et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch, 2016</xref>). At the same time, we maintain the high degree of flexibility with regards to deployment across arbitrary inference scenarios. As such, LANs can be treated as a highly flexible plug-in to existing inference algorithms and remain conceptually simple and lightweight.</p><p>Before elaborating our LAN approach, we briefly situate it in the context of some related work.</p><p>One branch of literature that interfaces ABC with deep learning attempts to amortize posterior inference directly in end-to-end NNs (<xref ref-type="bibr" rid="bib65">Radev et al., 2020b</xref>; <xref ref-type="bibr" rid="bib64">Radev et al., 2020a</xref>; <xref ref-type="bibr" rid="bib62">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib60">Papamakarios et al., 2019a</xref>; <xref ref-type="bibr" rid="bib29">Gonçalves et al., 2020</xref>). Here, NN architectures are trained with a large number of simulated datasets to produce posterior distributions over parameters, and once trained, such networks can be applied to directly estimate parameters from new datasets without the need for further simulations. However, the goal to directly estimate posterior parameters from data requires the user to first train a NN for the very specific inference scenario in which it is applied empirically. Such approaches are not easily deployable if a user wants to test multiple inference scenarios (e.g., parameters may vary as a function of task condition or brain activity, or in hierarchical frameworks), and consequently, they do not achieve our second desideratum needed for a user-friendly toolbox, making inference cheap and simple.</p><p>We return to discuss the relative merits and limitations of these and further alternative approaches in the 'Discussion' section.</p><p>Formally, we use model simulations to learn a function <inline-formula><mml:math id="inf25"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf26"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the output of a NN with parameter vector <inline-formula><mml:math id="inf27"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> (weights, biases). The function <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is used as an approximation <inline-formula><mml:math id="inf29"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the likelihood function <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Once learned, we can use such <inline-formula><mml:math id="inf31"><mml:mi>f</mml:mi></mml:math></inline-formula> as a plug-in to Bayes' rule:<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mover><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To perform posterior inference, we can now use a broad range of existing MC or MCMC algorithms. We note that, fulfilling our second desideratum, an extension to hierarchical inference is as simple as plugging in our NN into a probabilistic model of the form<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where α, β are generic group-level global parameters, and γ serves as a generic fixed hyperparameter vector.</p><p>We provide proofs of concepts for two types of LANs. While we use MLPs and convolutional neural networks (CNNs), of conceptual importance is the distinction between the two problem representations they tackle, rather than the network architectures per se.</p><p>The first problem representation, which we call the pointwise approach, considers the functions <inline-formula><mml:math id="inf32"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where θ is the parameter vector of a given stochastic simulator model, and <inline-formula><mml:math id="inf33"><mml:mi>x</mml:mi></mml:math></inline-formula> is a single datapoint (trial outcome). The pointwise approach is a mapping from the input dimension <inline-formula><mml:math id="inf34"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf35"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> refers to the cardinality, to the one-dimensional output. The output is simply the log-likelihood of the single datapoint <inline-formula><mml:math id="inf36"><mml:mi>x</mml:mi></mml:math></inline-formula> given the parameter vector θ. As explained in the next section, for this mapping we chose simple MLPs.</p><p>The second problem representation, which we will refer to as the histogram approach, instead aims to learn a function <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which maps a parameter vector θ to the likelihood over the full (discretized) dataspace (i.e., the likelihood of the entire RT distributions at once). We represent the output space as an outcome histogram with dimensions <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> (where in our applications <inline-formula><mml:math id="inf39"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of bins for a discretization of reaction times, and <inline-formula><mml:math id="inf40"><mml:mi>m</mml:mi></mml:math></inline-formula> refers to the number of distinct choices). Thus, our mapping has input dimension <inline-formula><mml:math id="inf41"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> and output dimension <inline-formula><mml:math id="inf42"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula>. Representing the problem this way, we chose CNNs as the network architecture.</p><p>In each of the above cases, we pursued the architectures that seemed to follow naturally, without any commitment to their optimality. The pointwise approach operates on low-dimensional inputs and outputs. With network evaluation speed being of primary importance to us, we chose a relatively shallow MLP to learn this mapping, given that it was expressive enough. However, when learning a mapping from a parameter vector θ to the likelihood over the full dataspace, as in the histogram approach, we map a low-dimensional data manifold to a much higher dimensional one. Using an MLP for this purpose would imply that the number of NN parameters needed to learn this function would be orders of magnitude larger than using a CNN. Not only would this mean that forward passes through the network would take longer, but also an increased propensity to overfit on an identical data budget.</p><p>The next two sections will give some detail regarding the networks chosen for the pointwise and histogram LANs. <xref ref-type="fig" rid="fig1">Figure 1</xref> illustrates the general idea of our approach while <xref ref-type="fig" rid="fig2">Figure 2</xref> gives a conceptual overview of the exact training and inference procedure proposed. These details are thoroughly discussed in the last section of the paper.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>High-level overview of our approaches.</title><p>For a given model <inline-formula><mml:math id="inf43"><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:math></inline-formula>, we sample model parameters θ from a region of interest (left 1) and run 100k simulations (left 2). We use those simulations to construct a kernel density estimate-based empirical likelihood, and a discretized (histogram-like) empirical likelihood. The combination of parameters and the respective likelihoods is then used to train the likelihood networks (right 1). Once trained, we can use the multilayered perceptron and convolutional neural network for posterior inference given an empirical/experimental dataset (right 2).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig2-v3.tif"/></fig></sec><sec id="s1-3"><title>Pointwise approach: learn likelihoods of individual observations with MLPs</title><p>As a first approach, we use simple MLPs to learn the likelihood function of a given stochastic simulator. The network learns the mapping <inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where θ represents the parameters of our stochastic simulator, <inline-formula><mml:math id="inf45"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> are the simulator outcomes (in our specific examples below, <inline-formula><mml:math id="inf46"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula>, refers to the tuple <inline-formula><mml:math id="inf47"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of reaction times and choices), and <inline-formula><mml:math id="inf48"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the log-likelihood of the respective outcome. The trained network then serves as the approximate likelihood function <inline-formula><mml:math id="inf49"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We emphasize that the network is trained as a function approximator to provide us with a computationally cheap likelihood estimate, not to provide a surrogate simulator. Biological plausibility of the network is therefore not a concern when it comes to architecture selection. Fundamentally this approach attempts to learn the log-likelihood via a nonlinear regression, for which we chose an MLP. Log-likelihood labels for training were derived from empirical likelihoods functions (details are given in the 'Materials and methods' section), which in turn were constructed as kernel density estimates (KDEs). The construction of KDEs roughly followed <xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>. We chose the Huber loss function (details are given in the 'Materials and methods' section) because it is more robust to outliers and thus less susceptible to distortions that can arise in the tails of distributions.</p></sec><sec id="s1-4"><title>Histogram approach: learn likelihoods of entire dataset distributions with CNNs</title><p>Our second approach is based on a CNN architecture. Whereas the MLP learned to output a single scalar likelihood output for each datapoint (‘trial’, given a choice, reaction time, and parameter vector), the goal of the CNN was to evaluate, for the given model parameters, the likelihood of an arbitrary number of datapoints via one forward pass through the network. To do so, the output of the CNN was trained to produce a probability distribution over a discretized version of the dataspace, given a stochastic model and parameter vector. The network learns the mapping <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>→</mml:mo><mml:mi>log</mml:mi><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>As a side benefit, in line with the methods proposed by <xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib61">Papamakarios et al., 2019b</xref>, the CNN can in fact act as a surrogate simulator; for the purpose of this paper, we do not exploit this possibility however. Since here we attempt to learn distributions, labels were simple binned empirical likelihoods, and as a loss function we chose the Kullback Leibler (KL) divergence between the network's output distribution and the label distribution (details are given in the 'Materials and methods' section).</p></sec><sec id="s1-5"><title>Training specifics</title><p>Most of the specifics regarding training procedures are discussed in detail in the 'Materials and methods' section; however, we mention some aspects here to aid readability.</p><p>We used 1.5 M and 3 M parameter vectors (based on 100k simulations each) to train the MLP and CNN approach, respectively. We chose these numbers consistently across all models and in fact trained on less examples for the MLP only due to RAM limitations we faced on our machines (which in principle can be circumvented). These numbers are purposely high (but in fact quite achievable with access to a computing cluster, simulations for each model was on the order of hours only) since we were interested in a workable proof of concept. We did not investigate the systematic minimization of training data; however, some crude experiments indicate that a decrease by an order of magnitude did not seriously affect performance.</p><p>We emphasize that this is in line with the expressed philosophy of our approach. The point of amortization is to throw a lot of resources at the problem once so that downstream inference is made accessible even on basic setups (a usual laptop). In case simulators are prohibitively expensive even for reasonably sized computer clusters, minimizing training data may gain more relevance. In such scenarios, training the networks will be very cheap compared to simulation time, which implies that retraining with progressively more simulations until one observes asymptotic test performance is a viable strategy.</p></sec><sec id="s1-6"><title>Test beds</title><p>We chose variations of SSMs common in the cognitive neurosciences as our test bed (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The range of models we consider permits great flexibility in allowable data distributions (choices and response times). We believe that initial applications are most promising for such SSMs because (1) analytical likelihoods are available for the most common variants (and thus provide an upper-bound benchmark for parameter recovery) and (2) there exist many other interesting variants for which no analytic solution exists.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Pictorial representation of the stochastic simulators that form our test bed.</title><p>Our point of departure is the standard simple drift diffusion model (DDM) due to its analytical tractability and its prevalence as the most common sequential sampling model (SSM) in cognitive neuroscience. By systematically varying different facets of the DDM, we test our likelihood approximation networks (LANs) across a range of SSMs for parameter recovery, goodness of fit (posterior predictive checks), and inference runtime. We divide the resulting models into four classes as indicated by the legend. We consider the simple DDM in the <italic>analytical likelihood</italic> (solid line) category, although, strictly speaking, the likelihood involves an infinite sum and thus demands an approximation algorithm introduced by Navarro and Fuss, but this algorithm is sufficiently fast to evaluate so that it is not a computational bottleneck. The full-DDM needs <italic>numerical quadrature</italic> (dashed line) to integrate over variability parameters, which inflates the evaluation time by 1–2 orders of magnitude compared to the simple DDM. Similarly, likelihood approximations have been derived for a range of models using the Fokker–Planck equations (dotted-dashed line), which again incurs nonsignificant evaluation cost. Finally, for some models no approximations exist and we need to resort to computationally expensive <italic>simulations</italic> for likelihood estimates (dotted line). Amortizing computations with LANs can substantially speed up inference for all but the <italic>analytical likelihood</italic> category (but see runtime for how it can even provide speedup in that case for large datasets).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig3-v3.tif"/></fig><p>We note that there is an intermediate case in which numerical methods can be applied to obtain likelihoods for a broader class of models (e.g., <xref ref-type="bibr" rid="bib76">Shinn et al., 2020</xref>). These methods are nevertheless computationally expensive and do not necessarily afford rapid posterior inference. Therefore, amortization via LANs is attractive even for these models. <xref ref-type="fig" rid="fig3">Figure 3</xref> further outlines this distinction.</p><p>We emphasize that our methods are quite general, and any model that generates discrete choices and response times from which simulations can be drawn within a reasonable amount of time can be suitable to the amortization techniques discussed in this paper (given that the model has parameter vectors of dimension roughly <italic>lt</italic><sub>15</sub>). In fact, LANs are not restricted to models of reaction time and choice to begin with, even though we focus on these as test beds.</p><p>As a general principle, all models tested below are based on stochastic differential equations (SDEs) of the following form:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐗</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐁</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>𝐗</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>where we are concerned with the probabilistic behavior of the particle (or vector of particles) <inline-formula><mml:math id="inf51"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>. The behavior of this particle is driven by <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, an underlying drift function, <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, an underlying noise transformation function, <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>B</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, an incremental noise process, and <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula>, a starting point.</p><p>Of interest to us are specifically the properties of the first-passage-time-distributions (FPTD) for such processes, which are needed to compute the likelihood of a given response time/choice pair <inline-formula><mml:math id="inf56"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. In these models, the exit region of the particle (i.e., the specific boundary it reaches) determines the choice, and the time point of that exit determines the response time. The joint distribution of choices and reaction times is referred to as a FPTD.</p><p>Given some exit region <inline-formula><mml:math id="inf57"><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi></mml:math></inline-formula>, such FPTDs are formally defined as<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo movablelimits="false">inf</mml:mo><mml:mi>τ</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In words, a first passage time, for a given exit region, is defined as the first time point of entry into the exit region, and the FPTD is the probability, respectively, of such an exit happening at any specified time <inline-formula><mml:math id="inf58"><mml:mi>t</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib67">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib21">Feller, 1968</xref>). Partitioning the exit region into subsets <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (e.g., representing <inline-formula><mml:math id="inf60"><mml:mi>n</mml:mi></mml:math></inline-formula> choices), we can now define the set of defective distributions<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:math></inline-formula> describes the collection of parameters driving the process. For every <inline-formula><mml:math id="inf62"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>,<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mi>𝐗</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:mtext>exits into</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="7.5pt" stretchy="false">(</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mi>i</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>gets chosen</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf63"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi class="ltx_font_mathcaligraphic">𝒾</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi class="ltx_font_mathcaligraphic">𝓃</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> jointly define the FPTD such that<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>These functions <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math></inline-formula>, jointly serve as the likelihood function s.t.<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For illustration, we focus the general model formulation above to the standard DDM. Details regarding the other models in our test bed are relegated to the 'Materials and methods' section.</p><p>To obtain the DDM from the general equation above, we set <inline-formula><mml:math id="inf65"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> (a fixed drift across time), <inline-formula><mml:math id="inf66"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (a fixed noise variance across time), and <inline-formula><mml:math id="inf67"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐁</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The DDM applies to the two alternative decision cases, where decision corresponds to the particle crossings of an upper or lower fixed boundary. Hence, <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>ℝ</mml:mi><mml:mo>≥</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>ℝ</mml:mi><mml:mo>≤</mml:mo><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf70"><mml:mi>a</mml:mi></mml:math></inline-formula> is a parameter of the model. The DDM also includes a normalized starting point <inline-formula><mml:math id="inf71"><mml:mi>w</mml:mi></mml:math></inline-formula> (capturing potential response biases or priors), and finally a nondecision time τ (capturing the time for perceptual encoding and motor output). Hence, the parameter vector for the DDM is then <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The SDE is defined as<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐗</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>v</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐖</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>𝐗</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The DDM serves principally as a basic proof of concept for us, in that it is a model for which we can compute the exact likelihoods analytically (<xref ref-type="bibr" rid="bib21">Feller, 1968</xref>; <xref ref-type="bibr" rid="bib53">Navarro and Fuss, 2009</xref>).</p><p>The other models chosen for our test bed systematically relax some of the fixed assumptions of the basic DDM, as illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><p>We note that many new models can be constructed from the components tested here. As an example of this modularity, we introduce inhibition/excitation to the race model, which gives us the leaky competing accumulator (LCA) (<xref ref-type="bibr" rid="bib83">Usher and McClelland, 2001</xref>). We could then further extend this model by introducing parameterized bounds. We could introduce reinforcement learning parameters to a DDM (<xref ref-type="bibr" rid="bib63">Pedersen and Frank, 2020</xref>) or in combination with any of the other decision models. Again we emphasize that while these diffusion-based models provide a large test bed for our proposed methods, applications are in no way restricted to this class of models.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Networks learn likelihood function manifolds</title><p>Across epochs of training, both training and validation loss decrease rapidly and remain low (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) suggesting that overfitting is not an issue, which is sensible in this context. The low validation loss further shows that the network can interpolate likelihoods to specific parameter values it has not been exposed to (with the caveat that it has to be exposed to the same range; no claims are made about extrapolation).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Likelihoods and manifolds: DDM.</title><p>(<bold>A</bold>) shows the training and validation loss for the multilayered perceptron (MLP) for the drift diffusion model across epochs. Training was driven by the Huber loss. The MLP learned the mapping <inline-formula><mml:math id="inf73"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>↦</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, that is, the log-likelihood of a single-choice/RT datapoint given the parameters. Training error declines rapidly, and validation loss trailed training loss without further detriment (no overfitting). Please see <xref ref-type="fig" rid="fig2">Figure 2</xref> and the 'Materials and methods' section for more details about training procedures. (<bold>B</bold>) illustrates the marginal likelihood manifolds for choices and RTs by varying one parameter in the trained region. Reaction times are mirrored for choice options −1, and 1, respectively, to aid visualization. (<bold>C</bold>) shows MLP likelihoods in green for four random parameter vectors, overlaid on top of a sample of 100 kernel density estimate (KDE)-based empirical likelihoods derived from 100k samples each. The MLP mirrors the KDE likelihoods despite not having been explicitly trained on these parameters. Moreover, the MLP likelihood sits firmly at the mean of sample of 100 KDEs. Negative and positive reaction times are to be interpreted as for (<bold>B</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig4-v3.tif"/></fig><p>Indeed, a simple interrogation of the learned likelihood manifolds shows that they smoothly vary in an interpretable fashion with respect to changes in generative model parameters (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Moreover, <xref ref-type="fig" rid="fig4">Figure 4C</xref> shows that the MLP likelihoods mirror those obtained by KDEs using 100,000 simulations, even though the model parameter vectors were drawn randomly and thus not trained per se. We also note that the MLP likelihoods appropriately filter out simulation noise (random fluctuations in the KDE empirical likelihoods across separate simulation runs of <inline-formula><mml:math id="inf74"><mml:mrow><mml:mn>100</mml:mn><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> samples each). This observation can also be gleaned from <xref ref-type="fig" rid="fig4">Figure 4C</xref>, which shows the learned likelihood to sit right at the center of sampled KDEs (note that for each subplot 100 such KDEs were used). As illustrated in the Appendix, these observations hold across all tested models. One perspective on this is to consider the MLP likelihoods as equivalent to KDE likelihoods derived from a much larger number of underlying samples and interpolated. The results for the CNN (not shown to avoid redundancy) mirror the MLP results. Finally, while <xref ref-type="fig" rid="fig4">Figure 4</xref> depicts the learned likelihood for the simple DDM for illustration purposes, the same conclusions apply to the learned manifolds for all of the tested models (as shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figures 1</xref>–<xref ref-type="fig" rid="app1fig6">6</xref>). Indeed inspection of those manifolds is insightful for facilitating interpretation of the dynamics of the underlying models, how they differ from each other, and the corresponding RT distributions that can be captured.</p></sec><sec id="s2-2"><title>Parameter recovery</title><sec id="s2-2-1"><title>Benchmark: analytical likelihood available</title><p>While the above inspection of the learned manifolds is promising, a true test of the method is to determine whether one can perform proper inference of generative model parameters using the MLP and CNN. Such parameter recovery exercises are typically performed to determine whether a given model is identifiable for a given experimental setting (e.g., number of trials, conditions, etc.). Indeed, when parameters are collinear, recovery can be imperfect even if the estimation method itself is flawless (<xref ref-type="bibr" rid="bib90">Wilson and Collins, 2019</xref>; <xref ref-type="bibr" rid="bib56">Nilsson et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Daw, 2011a</xref>). A Bayesian estimation method, however, should properly assign uncertainty to parameter estimates in these circumstances, and hence it is also important to evaluate the posterior variances over model parameters.</p><p>Thus as a benchmark, we first consider the basic DDM for which an arbitrarily close approximation to the analytical likelihood is available (<xref ref-type="bibr" rid="bib53">Navarro and Fuss, 2009</xref>). This benchmark allows us to compare parameter recovery given (1) the analytical likelihood, (2) an approximation to the likelihood specified by training an MLP on the analytical likelihood (thus evaluating the potential loss of information incurred by the MLP itself), (3) an approximation to the likelihood specified by training an MLP on KDE-based empirical likelihoods (thus evaluating any further loss incurred by the KDE reconstruction of likelihoods), and (4) an approximate likelihood resulting from training the CNN architecture, on empirical histograms. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the results for the DDM.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Simple drift diffusion model parameter recovery results for (<bold>A</bold>) analytical likelihood (ground truth), (<bold>B</bold>) multilayered perceptron (MLP) trained on analytical likelihood, (<bold>C</bold>) MLP trained on kernel density estimate (KDE)-based likelihoods (100K simulations per KDE), and (<bold>D</bold>) convolutional neural network trained on binned likelihoods.</title><p>The results represent posterior means, based on inference over datasets of size <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula>‘trials’. Dot shading is based on parameter-wise normalized posterior variance, with lighter shades indicating larger posterior uncertainty of the parameter estimate.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig5-v3.tif"/></fig><p>For the simple DDM and analytical likelihood, parameters are nearly perfectly recovered given <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula> datapoints (‘trials’) (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Notably, these results are mirrored when recovery is performed using the MLP trained on the analytical likelihood (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). This finding corroborates, as visually suggested by the learned likelihood manifolds, the conclusion that globally the likelihood function was well behaved. Moreover, only slight reductions in recoverability were incurred when the MLP was trained on the KDE likelihood estimates (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), likely due to the known small biases in KDE itself (<xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>). Similar performance is achieved using the CNN instead of MLP (<xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p><p>As noted above, an advantage of Bayesian estimation is that we obtain an estimate of the posterior uncertainty in estimated parameters. Thus, a more stringent requirement is to additionally recover the correct posterior variance for a given dataset <inline-formula><mml:math id="inf77"><mml:mi class="ltx_font_mathcaligraphic">𝒟</mml:mi></mml:math></inline-formula> and model <inline-formula><mml:math id="inf78"><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:math></inline-formula>. One can already see visually in <xref ref-type="fig" rid="fig5">Figure 5C, D</xref> that posterior uncertainty is larger when the mean is further from the ground truth (lighter shades of gray indicate higher posterior variance). However, to be more rigorous one can assess whether the posterior variance is precisely what it should be.</p><p>The availability of an analytical likelihood for the DDM, together with our use of sampling methods (as opposed to variational methods that can severely bias posterior variance), allows us to obtain the ‘ground truth’ uncertainty in parameter estimates. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows that the sampling from a MLP trained on analytical likelihoods, an MLP trained on KDE-based likelihoods, and a CNN all yield excellent recovery of the variance. For an additional run that involved datasets of size <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4096</mml:mn></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula>, we observed a consistent decrease in posterior variance across all methods (not shown) as expected.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Inference using likelihood approximation networks (LANs) recovers posterior uncertainty.</title><p>Here, we leverage the analytic solution for the drift diffusion model to plot the ‘ground truth’ posterior variance on the x-axis, against the posterior variance from the LANs on the y-axis. (Left) Multilayered perceptrons (MLPs) trained on the analytical likelihood. (Middle) MLPs trained on kernel density estimate-based empirical likelihoods. (Right) Convolutional neural networks trained on binned empirical likelihoods. Datasets were equivalent across methods for each model (left to right) and involved <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula> samples.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig6-v3.tif"/></fig></sec><sec id="s2-2-2"><title>No analytical likelihood available</title><p>As a proof of concept for the more general ABC setting, we show parameter recovery results for two nonstandard models, the linear collapse (LC) and Weibull models, as described in the 'Test bed' section. The results are summarized in <xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig8">8</xref> and described in more detail in the following two paragraphs.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Linear collapse model parameter recovery and posterior predictives.</title><p>(Left) Parameter recovery results for the multilayered perceptron (top) and convolutional neural network (bottom). (Right) Posterior predictive plots for two representative datasets. Model samples of all parameters (black) match those from the true generative model (red), but one can see that for the lower dataset, the bound trajectory is somewhat more uncertain (more dispersion of the bound). In both cases, the posterior predictive (black histograms) is shown as predicted choice proportions and RT distributions for upper and lower boundary responses, overlaid on top of the ground truth data (red; hardly visible since overlapping/matching).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig7-v3.tif"/></fig><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Weibull model parameter recovery and posterior predictives.</title><p>(Left) Parameter recovery results for the multilayered perceptron (top) and convolutional neural network (bottom). (Right) Posterior predictive plots for two representative datasets in which parameters were poorly estimated (denoted in blue on the left). In these examples, model samples (black) recapitulate the generative parameters (red) for the nonboundary parameters, and the recovered bound trajectory is poorly estimated relative to the ground truth, despite excellent posterior predictives in both cases (RT distributions for upper and lower boundary, same scheme as <xref ref-type="fig" rid="fig7">Figure 7</xref>). Nevertheless, one can see that the net decision boundary is adequately recovered within the range of the RT data that are observed. Across all datasets, the net boundary <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mfrac><mml:mi>t</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mi>α</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is well recovered within the range of the data observed, and somewhat less so outside of the data, despite poor recovery of individual Weibull parameters α and β.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig8-v3.tif"/></fig></sec></sec><sec id="s2-3"><title>Parameter recovery</title><p><xref ref-type="fig" rid="fig7">Figure 7</xref> shows that both the MLP and CNN methods consistently yield very good to excellent parameter recovery performance for the LC model, with parameter-wise regression coefficients globally above <inline-formula><mml:math id="inf83"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. As shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>, parameter recovery for the Weibull model is less successful, however, particularly for the Weibull collapsing bound parameters. The drift parameter <inline-formula><mml:math id="inf84"><mml:mi>v</mml:mi></mml:math></inline-formula>, the starting point bias <inline-formula><mml:math id="inf85"><mml:mi>w</mml:mi></mml:math></inline-formula>, and the nondecision time are estimated well; however, the boundary parameters <inline-formula><mml:math id="inf86"><mml:mi>a</mml:mi></mml:math></inline-formula>, α, and β are less well recovered by the posterior mean. Judging by the parameter recovery plot, the MLP seems to perform slightly less well on the boundary parameters when compared to the CNN.</p><p>To interrogate the source of the poor recovery of α and β parameters, we considered the possibility that the model itself may have issues with identifiability, rather than poor fit. <xref ref-type="fig" rid="fig8">Figure 8</xref> shows that indeed, for two representative datasets in which these parameters are poorly recovered, the model nearly perfectly reproduces the ground truth data in the posterior predictive RT distributions. Moreover, we find that whereas the individual Weibull parameters are poorly recovered, the net boundary <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is very well recovered, particularly when evaluated within the range of the observed dataset. This result is reminiscent of the literature on sloppy models (<xref ref-type="bibr" rid="bib32">Gutenkunst et al., 2007</xref>), where sloppiness implies that various parameter configurations can have the same impact on the data. Moreover, two further conclusions can be drawn from this analysis. First, when fitting the Weibull model, researchers should interpret the bound trajectory as a latent parameter rather than the individual α and β parameters per se. Second, the Weibull model may be considered as viable only if the estimated bound trajectory varies sufficiently within the range of the empirical RT distributions. If the bound is instead flat or linearly declining in that range, the simple DDM or LC models may be preferred, and their simpler form would imply that they would be selected by any reasonable model comparison metric. Lastly, given our results the Weibull model could likely benefit from reparameterization if the desire is to recover individual parameters rather than the bound trajectory <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Given the common use of this model in collapsing bound studies (<xref ref-type="bibr" rid="bib34">Hawkins et al., 2015</xref>) and that the bound trajectories are nevertheless interpretable, we leave this issue for future work.</p><p>The Appendix shows parameter recovery studies on a number of other stochastic simulators with non-analytical likelihoods, described in the 'Test bed' section. The appendices show tables of parameter-wise recovery <inline-formula><mml:math id="inf89"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for all models tested. In general, recovery ranges from good to excellent. Given the Weibull results above, we attribute the less good recovery for some of these models to identifiability issues and specific dataset properties rather than to the method per se. We note that our parameter recovery studies here are in general constrained to the simplest inference setting equivalent to a single-subject, single-condition experimental design. Moreover, we use uninformative priors for all parameters of all models. Thus, these results provide a lower bound on parameter recoverability, provided of course that the datasets were generated from the valid parameter ranges on which the networks were trained; see Section 0.10 for how recovery can benefit from more complex experimental designs with additional task conditions, which more faithfully represents the typical inference scenario deployed by cognitive neuroscientists. Lastly, some general remarks about the parameter recovery performance. A few factors can negatively impact how well one can recover parameters. First, if the model generally suffers from identifiability issues, the resulting tradeoffs in the parameters can lead the MCMC chain to get stuck on the boundary for one or more parameters. This issue is endemic to all models and unrelated to likelihood-free methods or LANs, and should at best be attacked at the level of reparameterization (or a different experimental design that can disentangle model parameters). Second, if the generative parameters of a dataset are too close to (or beyond) the bounds of the trained parameter space, we may also end with a chain that gets stuck on the boundary of the parameter space. We confronted this problem by training on parameter spaces that yield response time distributions that are broader than typically observed experimentally for models of this class, while also excluding obviously defective parameter setups. Defective parameter setups were defined in the context of our applications as parameter vectors that generate data that never allow one or the other choice to occur (as in <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, data that concentrates more than half of the reaction times within a single 1 ms bin and data that generated mean reaction times beyond 10 s). These guidelines were chosen as a mix of basic rationale and domain knowledge regarding usual applications of DDMs to experimental data. As such, the definition of defective data may depend on the model under consideration.</p></sec><sec id="s2-4"><title>Runtime</title><p>A major motivation for this work is the amortization of network training time during inference, affording researchers the ability to test a variety of theoretically interesting models for linking brain and behavior without large computational cost. To quantify this advantage, we provide some results on the posterior sampling runtimes using (1) the MLP with slice sampling (<xref ref-type="bibr" rid="bib55">Neal, 2003</xref>) and (2) CNN with iterated importance sampling.</p><p>The MLP timings are based on slice sampling (<xref ref-type="bibr" rid="bib55">Neal, 2003</xref>), with a minimum of <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula> samples. The sampler was stopped at some <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≥</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula>, for which the Geweke statistic (<xref ref-type="bibr" rid="bib28">Geweke, 1992</xref>) indicated convergence (the statistic was computed once every 100 samples for <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≥</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula>). Using an alternative sampler, based on differential evolution Markov chain Monte Carlo (DEMCMC) and stopped when the Gelman–Rubin <inline-formula><mml:math id="inf94"><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&lt;</mml:mo><mml:mn>1.1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib27">Gelman and Rubin, 1992</xref>), yielded very similar timing results and was omitted in our figures.</p><p>For the reported importance sampling runs, we used 200K importance samples per iteration, starting with γ values of 64, which was first reduced to 1 where in iteration <inline-formula><mml:math id="inf95"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>64</mml:mn><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula>, before a stopping criterion based on relative improvement of the confusion metric was used.</p><p><xref ref-type="fig" rid="fig9">Figure 9A</xref> shows that all models can be estimated in the order of hundreds of seconds (minutes), comprising a speed improvement of at least two orders of magnitude compared to traditional ABC methods using KDE during inference (i.e., the PDA method motivating this work; <xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>). Indeed, this estimate is a lower bound on the speed improvement: we extrapolate only the observed difference between network evaluation and online simulations, ignoring the additional cost of constructing and evaluating the KDE-based likelihood. We decided to use this benchmark because it provides a fairer comparison to more recent PDA approaches in which the KDE evaluations can be sped up considerably (<xref ref-type="bibr" rid="bib38">Holmes, 2015</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Computation times.</title><p>(<bold>A</bold>) Comparison of sampler timings for the multilayered perceptron (MLP) and convolutional neural network (CNN) methods, for datasets of size 1024 and 4096 (respectively MLP-1024, MLP-4096, CNN-1024, CNN-4096). For comparison, we include a lower bound estimate of the sample timings using traditional PDA approach during online inference (using 100k online simulations for each parameter vector). 100K simulations were used because we found this to be required for sufficiently smooth likelihood evaluations and is the number of simulations used to train our networks; fewer samples can of course be used at the cost of worse estimation, and only marginal speedup since the resulting noise in likelihood evaluations tends to prevent chain mixing; see <xref ref-type="bibr" rid="bib38">Holmes, 2015</xref>. We arrive at 100k seconds via simple arithmetic. It took our slice samplers on average approximately 200k likelihood evaluations to arrive at 2000 samples from the posterior. Taking 500 ms * 200,000 gives the reported number. Note that this is a generous but rough estimate since the cost of data simulation varies across simulators (usually quite a bit higher than the drift diffusion model [DDM] simulator). Note further that these timings scale linearly with the number of participants and task conditions for the online method, but not for likelihood approximation networks, where they can be in principle be parallelized. (<bold>B</bold>) compares the timings for obtaining a single likelihood evaluation for a given dataset. MLP and CNN refer to Tensorflow implementations of the corresponding networks. Navarro Fuss refers to a cython (<xref ref-type="bibr" rid="bib6">Behnel et al., 2010</xref>) (cpu) implementation of the algorithm suggested (<xref ref-type="bibr" rid="bib53">Navarro and Fuss, 2009</xref>) for fast evaluation of the analytical likelihood of the DDM. 100k-sim refers to the time it took a highly optimized cython (cpu) version of a DDM sampler to generate 100k simulations (averaged across 100 parameter vectors).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig9-v3.tif"/></fig><p>Notably, due to its potential for parallelization (especially on GPUs), our NN methods can even induce performance speedups relative to analytical likelihood evaluations. Indeed, <xref ref-type="fig" rid="fig9">Figure 9B</xref> shows that as the dataset grows runtime is significantly faster than even a highly optimized cython implementation of the Navarro Fuss algorithm (<xref ref-type="bibr" rid="bib53">Navarro and Fuss, 2009</xref>) for evaluation of the analytic DDM likelihood. This is also noteworthy in light of the full-DDM (as described in the 'Test bed' section), for which it is currently common to compute the likelihood term via quadrature methods, in turn based on repeated evaluations of the Navarro Fuss algorithm. This can easily inflate the evaluation time by 1–2 orders of magnitude. In contrast, evaluation times for the MLP and CNN are only marginally slower (as a function of the slightly larger network size in response to higher dimensional inputs). We confirm (omitted as separate figure) from experiments with the HDDM Python toolbox that our methods end up approximately <inline-formula><mml:math id="inf97"><mml:mrow><mml:mn>10</mml:mn><mml:mo>-</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> times faster for the full-DDM than the current implementation based on numerical integration, maintaining comparable parameter recovery performance. We strongly suspect there to be additional remaining potential for performance optimization.</p></sec><sec id="s2-5"><title>Hierarchical inference</title><p>One of the principal benefits of LANs is that they can be directly extended – without further training – to arbitrary hierarchical inference scenarios, including those in which (1) individual participant parameters are drawn from group distributions, (2) some parameters are pooled and others separated across task conditions, and (3) neural measures are estimated as regressors on model parameters (<xref ref-type="fig" rid="fig10">Figure 10</xref>). Hierarchical inference is critical for improving parameter estimation particularly for realistic cognitive neuroscience datasets in which thousands of trials are not available for each participant and/or where one estimates impacts of noisy physiological signals onto model parameters (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Boehm et al., 2018</xref>; <xref ref-type="bibr" rid="bib85">Vandekerckhove et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Ratcliff and Childers, 2015</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Illustration of the common inference scenarios applied in the cognitive neurosciences and enabled by our amortization methods.</title><p>The figure uses standard plate notation for probabilistic graphical models. White single circles represent random variables, white double circles represent variables computed deterministically from their inputs, and gray circles represent observations. For illustration, we split the parameter vector of our simulator model (which we call θ in the rest of the paper) into two parts θ and λ since some, but not all, parameters may sometimes vary across conditions and/or come from global distribution. (Upper left) Basic hierarchical model across M participants, with N observations (trials) per participant. Parameters for individuals are assumed to be drawn from group distributions. (Upper right) Hierarchical models that further estimate the impact of trial-wise neural regressors onto model parameters. (Lower left) Nonhierarchical, standard model estimating one set of parameters across all trials. (Lower right) Common inference scenario in which a subset of parameters (θ) are estimated to vary across conditions M, while others (λ) are global. Likelihood approximation networks can be immediately repurposed for all of these scenarios (and more) without further training.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig10-v3.tif"/></fig><p>To provide a proof of concept, we developed an extension to the HDDM Python toolbox (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>), widely used for hierarchical inference of the DDM applied to such settings. Lifting the restriction of previous versions of HDDM to only DDM variants with analytical likelihoods, we imported the MLP likelihoods for all two-choice models considered in this paper. Note that GPU-based computation is supported out of the box, which can easily be exploited with minimal overhead using free versions of Google’s Colab notebooks. We generally observed GPUs to improve speed approximately fivefold over CPU-based setups for the inference scenarios we tested. Preliminary access to this interface and corresponding instructions can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/hddmnn_tutorial">https://github.com/lnccbrown/lans/tree/master/hddmnn_tutorial</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:985b12b70c56af4d31f0674d3317a4aeaac1f419;origin=https://github.com/lnccbrown/lans/;visit=swh:1:snp:5eaab2fe7281d7891f747744994023109d07c00c;anchor=swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6">swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6</ext-link>); <xref ref-type="bibr" rid="bib22">Fengler, 2021</xref>.</p><p><xref ref-type="fig" rid="fig11">Figure 11</xref> shows example results from hierarchical inference using the LC model, applied to synthetic datasets comprising 5 and 20 subjects (a superset of participants). Recovery of individual parameters was adequate even for five participants, and we also observe the expected improvement of recovery of the group-level parameters μ and σ for 20 participants.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Hierarchical inference results using the multilayered perceptron likelihood imported into the HDDM package.</title><p>(<bold>A</bold>) Posterior inference for the linear collapse model on a synthetic dataset with 5 participants and 500 trials each. Posterior distributions are shown with caterpillar plots (thick lines correspond to <inline-formula><mml:math id="inf98"><mml:mrow><mml:mn>5</mml:mn><mml:mo>-</mml:mo><mml:mn>95</mml:mn></mml:mrow></mml:math></inline-formula> percentiles, thin lines correspond to <inline-formula><mml:math id="inf99"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>99</mml:mn></mml:mrow></mml:math></inline-formula> percentiles) grouped by parameters (ordered from above <inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Ground truth simulated values are denoted in red. (<bold>B</bold>) Hierarchical inference for synthetic data comprising 20 participants and 500 trials each. μ and σ indicate the group-level mean and variance parameters. Estimates of group-level posteriors improve with more participants as expected with hierarchical methods. Individual-level parameters are highly accurate for each participant in both scenarios.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig11-v3.tif"/></fig><p><xref ref-type="fig" rid="fig12">Figure 12</xref> shows an example that illustrates how parameter recovery is affected when a dataset contains multiple experimental conditions (e.g., different difficulty levels). It is common in such scenarios to allow task conditions to affect a single (or subset)-model parameter (in the cases shown: <inline-formula><mml:math id="inf101"><mml:mi>v</mml:mi></mml:math></inline-formula>), while other model parameters are pooled across conditions. As expected, for both the full-DDM (A) and the Levy model (B), the estimation of global parameters is improved when increasing the number of conditions from 1 to 5 to 10 (left to right, where the former are subsets of the latter datasets). These experiments confirm that one can more confidently estimate parameters that are otherwise difficult to estimate such as the noise α in the Levy model and <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> the standard deviation of the drift in the full-DDM.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Effect of multiple experimental conditions on inference.</title><p>The panel shows an example of posterior inference for 1, (left), 5 (middle), and 10 (right) conditions. (<bold>A</bold>) and (<bold>B</bold>) refer to the full drift diffusion model (DDM) and Levy model, respectively. The drift parameter <inline-formula><mml:math id="inf103"><mml:mi>v</mml:mi></mml:math></inline-formula> is estimated to vary across conditions, while the other parameters are treated as global across conditions. Inference tends to improve for all global parameters when adding experimental conditions. Importantly, this is particularly evident for parameters that are otherwise notoriously difficult to estimate such as <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> (trial-by-trial variance in drift in the full-DDM) and α (the noise distribution in the Levy model). Red stripes show the ground truth values of the given parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-fig12-v3.tif"/></fig><p>Both of these experiments provide evidence that our MLPs provide approximate likelihoods which behave in accordance with what is expected from proper analytical methods, while also demonstrating their robustness to other samplers (i.e., we used HDDM slice samplers without further modification for all models).</p><p>We expect that proper setting of prior distributions (uniform in our examples) and further refinements to the slice sampler settings (to help mode discovery) can improve these results even further. We include only the MLP method in this section since it is most immediately amenable to the kind of trial-by-trial-level analysis that HDDM is designed for. We plan to investigate the feasibility of including the CNN method into HDDM in future projects.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our results demonstrate the promise and potential of amortized LANs for Bayesian parameter estimation of neurocognitive process models. Learned manifolds and parameter recovery experiments showed successful inference using a range of network architectures and posterior sampling algorithms, demonstrating the robustness of the approach.</p><p>Although these methods are extendable to any model of similar complexity, we focused here on a class of SSMs, primarily because the most popular of them – the DDM – has an analytic solution, and is often applied to neural and cognitive data. Even slight departures from the standard DDM framework (e.g., dynamic bounds or changes in the noise distribution) are often not considered for full Bayesian inference due to the computational complexity associated with traditional ABC methods. We provide access to the learned likelihood functions (in the form of network weights) and code to enable users to fit a variety of such models with orders of magnitude speedup (minutes instead of days). In particular, we provided an extension to the commonly used HDDM toolbox (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>) that allows users to apply these models to their own datasets immediately. We also provide access to code that would allow users to train their own likelihood networks and perform recovery experiments, which can then be made available to the community.</p><p>We offered two separate approaches with their own relative advantages and weaknesses. The MLP is suited for evaluating likelihoods of individual observations (choices, response times) given model parameters, and as such can be easily extended to hierarchical inference settings and trial-by-trial regression of neural activity onto model parameters. We showed that importing the MLP likelihood functions into the HDDM toolbox affords fast inference over a variety of models without tractable likelihood functions. Moreover, these experiments demonstrated that use of the NN likelihoods even confers a performance speedup over the analytical likelihood function – particularly for the full-DDM, which otherwise required numerical methods on top of the analytical likelihood function for the simple DDM.</p><p>Conversely, the CNN approach is well suited for estimating likelihoods across parameters for entire datasets in parallel, as implemented with importance sampling. More generally and implying potential further improvements, any sequential MC method may be applied instead. These methods offer a more robust path to sampling from multimodal posteriors compared to MCMC, at the cost of the curse of dimensionality, rendering them potentially less useful for highly parameterized problems, such as those that require hierarchical inference. Moreover, representing the problem directly as one of learning probability distributions and enforcing the appropriate constraints by design endows the CNN approach with a certain conceptual advantage. Finally, we note that in principle (with further improvements) trial-level inference is possible with the CNN approach, and vice versa, importance sampling can be applied to the MLP approach.</p><p>In this work, we employed sampling methods (MCMC and importance sampling) for posterior inference because in the limit they are well known to allow for accurate estimation of posterior distributions on model parameters, including not only mean estimates but their variances and covariances. Accurate estimation of posterior variances is critical for any hypothesis testing scenario because it allows one to be confident about the degree of uncertainty in parameter estimates. Indeed, we showed that for the simple DDM we found that posterior inference using our networks yielded nearly perfect estimation of the variances of model parameters (which are available due to the analytic solution). Of course, our networks can also be deployed for other estimation methods even more rapidly: they can be immediately used for MLE via gradient descent or within other approximate inference methods, such as variational inference (see <xref ref-type="bibr" rid="bib2">Acerbi, 2020</xref> for a related approach).</p><p>Other approaches exist for estimating generalized diffusion models. A recent example, not discussed thus far, is the pyDDM Python toolbox (<xref ref-type="bibr" rid="bib76">Shinn et al., 2020</xref>), which allows MLE of generalized drift diffusion models. The underlying solver is based on the Fokker–Planck equations, which allow access to approximate likelihoods (where the degree of approximation is traded off with computation time/discretization granularity) for a flexible class of diffusion-based models, notably allowing arbitrary evidence trajectories, starting point, and nondecision time distributions. However, to incorporate trial-by-trial effects would severely inflate computation time (on the order of the number of trials) since the solver would have to operate on a trial-by-trial level. Moreover, any model that is not driven by Gaussian diffusion, such as the Levy model we considered here or the linear ballistic accumulator, is out of scope with this method. In contrast, LANs can be trained to estimate any such model, limited only by the identifiability of the generative model itself. Finally, pyDDM does not afford full Bayesian estimation and thus quantification of parameter uncertainty and covariance.</p><p>We moreover note that our LAN approach can be useful even if the underlying simulation model admits other likelihood approximations, regardless of trial-by-trial effect considerations, since a forward pass through a LAN may be speedier. Indeed, we observed substantial speedups in HDDM for using our LAN method to the full-DDM, for which numerical methods were previously needed to integrate over inter-trial variability.</p><p>We emphasize that our test bed application to SSMs does not delimit the scope of application of LANs. Neither are reasonable architectures restricted to MLPs and CNNs (see <xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib61">Papamakarios et al., 2019b</xref> for related approaches that use completely different architectures). Models with high-dimensional (roughly <italic>gt</italic><sub>15</sub>) parameter spaces may present a challenge for our global amortization approach due to the curse of dimensionality. Further, models with discrete parameters of high cardinality may equally present a given network with training difficulties. In such cases, other methods may be preferred over likelihood amortization generally (e.g., <xref ref-type="bibr" rid="bib2">Acerbi, 2020</xref>); given that this is an open and active area of research, we can expect surprising developments that may in fact turn the tide again in the near future.</p><p>Despite some constraints, this still leaves a vast array of models in reach for LANs, of which our test bed can be considered only a small beginning.</p><p>By focusing on LANs, our approach affords the flexibility of networks serving as plug-ins for hierarchical or arbitrarily complex model extensions. In particular, the networks can be immediately transferred, without further training, to arbitrary inference scenarios in which researchers may be interested in evaluating links between neural measures and model parameters, and to compare various assumptions about whether parameters are pooled and split across experimental manipulations. This flexibility in turn sets our methods apart from other amortization and NN-based ABC approaches offered in the statistics, machine learning, and computational neuroscience literature (<xref ref-type="bibr" rid="bib62">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib60">Papamakarios et al., 2019a</xref>; <xref ref-type="bibr" rid="bib29">Gonçalves et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Radev et al., 2020b</xref>), while staying conceptually extremely simple. Instead of focusing on extremely fast inference for very specialized inference scenarios, our approach focuses on achieving speedy inference while not implicitly compromising modeling flexibility through amortization step.</p><p>Closest to our approach is the work of <xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>, and <xref ref-type="bibr" rid="bib61">Papamakarios et al., 2019b</xref>, both of which attempt to target the likelihood with a neural density estimator. While flexible, both approaches imply the usage of summary statistics, instead of a focus on trial-wise likelihood functions. Our work can be considered a simple alternative with explicit focus on trial-wise likelihoods.</p><p>Besides deep learning-based approaches, another major machine learning-inspired branch of the ABC literature concerns log-likelihood and posterior approximations via Gaussian process surrogates (GPSs) (<xref ref-type="bibr" rid="bib49">Meeds and Welling, 2014</xref>; <xref ref-type="bibr" rid="bib41">Järvenpää et al., 2018</xref>; <xref ref-type="bibr" rid="bib2">Acerbi, 2020</xref>). A major benefit of GPSs lies in the ability for clever training data selection via active learning since such GPSs allow uncertainty quantification out of the box, which in turn can be utilized for the purpose of targeting high-uncertainty regions in parameter space. GPS-based computations scale with the number of training examples, however, which make them much more suitable for minimizing the computational cost for a given inference scenario than facilitating global amortization as we suggest in this paper (for which one usually need larger sets of training data than can traditionally be handled efficiently by GPS). Again when our approach is applicable, it will offer vastly greater flexibility once a LAN is trained.</p><sec id="s3-1"><title>Limitations and future work</title><p>There are several limitations of the methods presented in this paper, which we hope to address in future work. While allowing for great flexibility, the MLP approach suffers from the drawback that we do not enforce (or exploit) the constraint that <inline-formula><mml:math id="inf105"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a valid probability distribution, and hence the networks have to learn this constraint implicitly and approximately. Enforcing this constraint has the potential to improve estimation of tail probabilities (a known issue for KDE approaches to ABC more generally; <xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>).</p><p>The CNN encapsulation exploits the fact that <inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒳</mml:mi></mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, however, makes estimation of trial-by-trial effects more resource hungry. We plan to investigate the potential of the CNN for trial-by-trial estimation in future research.</p><p>A potential solution that combines the strengths of both the CNN and MLP methods is to utilize mixture density networks to encapsulate the likelihood functions. We are currently exploring this avenue. Mixture density networks have been successfully applied in the context of ABC (<xref ref-type="bibr" rid="bib62">Papamakarios and Murray, 2016</xref>); however, training can be unstable without extra care (<xref ref-type="bibr" rid="bib31">Guillaumes, 2017</xref>). Similarly, invertible flows (<xref ref-type="bibr" rid="bib72">Rezende and Mohamed, 2015</xref>) and/or mixture density networks <xref ref-type="bibr" rid="bib7">Bishop, 1994</xref> may be used to learn likelihood functions (<xref ref-type="bibr" rid="bib61">Papamakarios et al., 2019b</xref>; <xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>); however, the philosophy remains focused on distributions of summary statistics for single datasets. While impressive improvements have materialized at the intersection of ABC and deep learning methods (<xref ref-type="bibr" rid="bib60">Papamakarios et al., 2019a</xref>; <xref ref-type="bibr" rid="bib30">Greenberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Gonçalves et al., 2020</xref>) (showing some success with posterior amortization for models up to 30 parameters, but restricted to a local region of high posterior density in the resulting parameter space), generally less attention has been paid to amortization methods that are not only of case-specific efficiency but sufficiently modular to serve a large variety of inference scenarios (e.g., <xref ref-type="fig" rid="fig10">Figure 10</xref>). This is an important gap that we believe the popularization of the powerful ABC framework in the domain of experimental science hinges upon. A second and short-term avenue for future work is the incorporation of our presented methods into the HDDM Python toolbox (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>) to extend its capabilities to a larger variety of SSMs. Initial work in this direction has been completed, the alpha version of the extension being available in the form of a tutorial at <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/hddmnn_tutorial">https://github.com/lnccbrown/lans/tree/master/hddmnn_tutorial</ext-link>.</p><p>Our current training pipeline can be further optimized on two fronts. First, no attempt was made to minimize the size of the network needed to reliably approximate likelihood functions so as to further improve computational speed. Second, little attempt was made to optimize the amount of training provided to networks. For the models explored here, we found it sufficient to simply train the networks for a very large number of simulated datapoints such that interpolation across the manifold was possible. However, as model complexity increases, it would be useful to obtain a measure of the networks’ uncertainty over likelihood estimates for any given parameter vector. Such uncertainty estimates would be beneficial for multiple reasons. One such benefit would be to provide a handle on the reliability of sampling, given the parameter region. Moreover, such uncertainty estimates could be used to guide the online generation of training data to train the networks in regions with high uncertainty. At the intersection of ABC and NNs, active learning has been explored via uncertainty estimates based on network ensembles (<xref ref-type="bibr" rid="bib47">Lueckmann et al., 2019</xref>). We plan to additionally explore the use of Bayesian NNs, which provide uncertainty over their weights, for this purpose (<xref ref-type="bibr" rid="bib54">Neal, 1995</xref>).</p><p>One more general shortcoming of our methods is the reliance on empirical likelihoods for training, which in turn are based on a fixed number of samples across parameter vectors, just as the PDA method proposed by <xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>. Recently, this approach has been criticized fundamentally on grounds of producing bias in the generated KDE-based likelihood estimates (<xref ref-type="bibr" rid="bib84">van Opheusden et al., 2020</xref>). A reduction of the approximate likelihood problem to one of inverse binomial sampling was proposed (<xref ref-type="bibr" rid="bib84">van Opheusden et al., 2020</xref>), which will generate unbiased likelihood estimates. To address this concern, we will investigate adaptive strategies for the selection of the simulations count <inline-formula><mml:math id="inf107"><mml:mi>n</mml:mi></mml:math></inline-formula>. We however highlight two points here that aim to put the promise of unbiased likelihoods in perspective. First, our networks add interpolation to the actual estimation of a likelihood. Likelihoods close in parameter space therefore share information that translates into an effectively higher simulation count than the 100k chosen to construct each empirical likelihood used for training. Quantifying this benefit precisely we leave for future research; however, we suspect, as suggested by <xref ref-type="fig" rid="fig4">Figure 4</xref>, that it may be substantial. Second, while we generally acknowledge that bias in the tails remains somewhat of an issue in our approach, resolution is at best partial even in the proposed methods of <xref ref-type="bibr" rid="bib84">van Opheusden et al., 2020</xref>. For the estimation of parameters for which a given datapoint is extremely unlikely (i.e., the data is generally unlikely under the model), the authors suggest to threshold the simulation count so that their algorithm is guaranteed to stop. This effectively amounts to explicitly allowing for bias again. As another alternative, the authors suggest to introduce a lapse rate in the generative model, which the LAN approach can accommodate as well. However, the introduction of a lapse rate does not deal with tail events directly either, but rather assumes that tail events are unrelated to the process of interest. This in turn will render a lower but fixed number of simulations <inline-formula><mml:math id="inf108"><mml:mi>N</mml:mi></mml:math></inline-formula> feasible for training LANs as well. This is notwithstanding the desirable minimization of simulation times even for high likelihood events, especially when trial-wise simulations are in fact necessary (which tends to be in cases where amortization with LANs is a priori not a good computational strategy to begin with). Hence, although the inverse binomial sampling approach is elegant conceptually, excessive computation remains an issue when we need accurate estimates of the probability of actual tail events. Generally, however, we maintain it is desirable and important for future work to make use of the otherwise great potential of adaptive sampling to minimize total computational cost.</p><p>Furthermore, we relegate to future research proper exploitation of the fact that LANs are by design differentiable in the parameters. We are currently working on an integration of LANs with Tensorflow probability (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>), utilizing autograd to switch our MCMC method to the gradient-based NUTS sampler (<xref ref-type="bibr" rid="bib37">Hoffman and Gelman, 2014</xref>). The main benefits of this sampler are robust mixing behavior, tolerance for high levels of correlations in the parameter space, while at the same time maintaining the ability to sample from high-dimensional posteriors. High level of correlations in posteriors is traditionally an Achilles' heel of the otherwise robust coordinate-wise slice samplers. DEMCMC and iterated Importance samplers are somewhat more robust in this regards; however, both may not scale efficiently to high-dimensional problems. Robustness concerns aside, initial numerical results additionally show some promising further speedups.</p><p>Another important branch for future work lies in the utilization of LANs for model comparison. Initial results are promising in that we obtained satisfactory model recovery using the deviance information criterion (DIC) used for model selection in the standard HDDM package. However, this issue demands much more attention to evaluate other model selection metrics and extensive further numerical experiments, which we relegate to future work.</p><p>Lastly, in contrast to the importance sampler driving the posterior inference for the CNN, we believe that some of the performance deficiencies of the MLP are the result of our MCs not having converged to the target distribution. A common problem seems to be that the sampler hits the bounds of the constrained parameter space and does not recover from that. As shown in <xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig8">8</xref>, even ostensibly bad parameter recoveries follow a conceptual coherence and lead to good posterior predictive performance. We therefore may be underreporting the performance of the MLP and plan to test the method on an even more comprehensive suite of MCMC samplers, moreover including thus far neglected potential for reparameterization.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Test beds</title><sec id="s4-1-1"><title>General information</title><p>All models were simulated using the Euler–Maruyama method, which for some fixed discretization step size <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> evolves the process as<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐁</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where the definition of <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐁</mml:mi></mml:mrow></mml:math></inline-formula> depends on the noise process. For simple Brownian motion, this translates into Gaussian displacements, specifically <inline-formula><mml:math id="inf111"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐁</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which is commonly denoted as <inline-formula><mml:math id="inf112"><mml:mrow><mml:mpadded lspace="1.7pt" width="+1.7pt"><mml:mi>d</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi></mml:mrow></mml:math></inline-formula>. More generally, the noise need not be Gaussian, and indeed we later apply our methods to the Levy flight model for which the noise process is an alpha stable distribution, denoted as <inline-formula><mml:math id="inf113"><mml:msub><mml:mi mathvariant="bold">𝐋</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> s.t. <inline-formula><mml:math id="inf114"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐋</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The models chosen for our test bed systematically vary different aspects of complexity, as illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The DDM provides a benchmark and a sanity check since we can compute its likelihood analytically. The full-DDM provides us with a model for which analytical computations are still based on the analytical likelihood of the DDM; however, evaluation is slowed by the necessity for numerical integration. This forms a first test for the speed of evaluation of our methods. For the Ornstein–Uhlenbeck, Levy, Race, and DDM with parameterized boundary models, we cannot base our calculations on an analytical likelihood, but we can nevertheless perform parameter recovery and compare to other methods that utilize empirical likelihoods. The Ornstein–Uhlenbeck model adds state-dependent behavior to the diffusion while the Levy model adds variation in the noise process and the Race models expand the output dimensions according to the number of choices.</p></sec><sec id="s4-1-2"><title>Full-DDM</title><p>The full-DDM maintains the same specification for the driving SDE, but also allows for trial-to-trial variability in three parameters (<xref ref-type="bibr" rid="bib70">Ratcliff and McKoon, 2008</xref>). We allow the drift rate <inline-formula><mml:math id="inf115"><mml:mi>v</mml:mi></mml:math></inline-formula> to vary trial by trial; according to a normal distribution, <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the nondecision time τ to vary according to a uniform distribution <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>τ</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the starting point <inline-formula><mml:math id="inf118"><mml:mi>w</mml:mi></mml:math></inline-formula> to vary according to a uniform distribution as well <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The parameter vector for the full-DDM is then <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To calculate the FPTD for this model, we can use the analytical likelihood expression from the DDM. However, we need to use numerical integration to take into account the random parameters (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>). This inflates execution time by a factor equivalent to the number of executions needed to compute the numerical integral.</p></sec><sec id="s4-1-3"><title>Ornstein–Uhlenbeck model</title><p>The Ornstein–Uhlenbeck model introduces a state dependency on the drift rate <inline-formula><mml:math id="inf121"><mml:mi>v</mml:mi></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf122"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf123"><mml:mi>g</mml:mi></mml:math></inline-formula> is an inhibition/excitation parameter. If <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, it acts as a leak (the particle is mean reverting). If <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the particle accelerates away from the 0 state, as in an attractor model. At <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we recover the simple DDM process. This leaves us with a parameter vector <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The corresponding SDE is defined as<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐗</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>𝐗</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐖</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>𝐗</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This model does not have an analytical likelihood function that can be employed for cheap inference (<xref ref-type="bibr" rid="bib52">Mullowney and Iyengar, 2006</xref>). We discuss alternatives, other than our proposed methods, to simple analytical likelihoods later. For our purposes, approximate inference is necessary for this model. The Ornstein–Uhlenbeck model is usually defined only for <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>; our parameter space makes it strictly speaking a relaxation.</p></sec><sec id="s4-1-4"><title>Levy flights</title><p>The Levy flight (<xref ref-type="bibr" rid="bib89">Wieschen et al., 2020</xref>; <xref ref-type="bibr" rid="bib71">Reynolds and Rhodes, 2009</xref>) model dispenses with the Gaussian noise assumption in that the incremental noise process instead follows an alpha-stable distribution <inline-formula><mml:math id="inf129"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>. Specifically, we consider distributions <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that are centered at 0, symmetric, and have unitary scale parameter. These distributions have a first moment for <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but infinite variance for <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. An important special case is <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf134"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The parameter vector for this process is <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We fix <inline-formula><mml:math id="inf136"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf137"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The SDE is defined as<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐗</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>v</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐋</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>𝐗</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The Levy flight is a flexible model used across disciplines for some of its theoretical optimality properties (<xref ref-type="bibr" rid="bib91">Wosniack et al., 2017</xref>) despite not possessing closed-form FPTDs. We add it here as it is different from the other models under consideration; in principle, it could also capture decision-making scenarios in which there are sudden jumps in the accumulation of evidence (e.g., due to internal changes in attention). Its behavior is shaped by altering the properties of the incremental noise process directly.</p></sec><sec id="s4-1-5"><title>Parameterized collapsing decision bounds</title><p>We will consider variations of the DDM in which the decision boundary is not fixed but is time-varying (represented by a boundary parameter <inline-formula><mml:math id="inf138"><mml:mi>a</mml:mi></mml:math></inline-formula> with a parameterized boundary function <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). In such cases, we augment the parameter vector θ with the set <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> and drop <inline-formula><mml:math id="inf141"><mml:mi>a</mml:mi></mml:math></inline-formula>. Such variations are optimal in a variety of settings (e.g., when there are response deadlines, <xref ref-type="bibr" rid="bib26">Frazier and Angela, 2008</xref>, or distributions of trial types with different difficulties, <xref ref-type="bibr" rid="bib48">Malhotra et al., 2018</xref>; <xref ref-type="bibr" rid="bib59">Palestro et al., 2018</xref>), and also better reflect the underlying dynamics of decision bounds within biologically inspired neural models (<xref ref-type="bibr" rid="bib58">O'Reilly and Frank, 2006</xref>; <xref ref-type="bibr" rid="bib69">Ratcliff and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib88">Wiecki and Frank, 2013</xref>). The boundary functions considered in the following are the Weibull bound (Weibull),<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mfrac><mml:mi>t</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mi>α</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and the linear collapse bound (LC),<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>*</mml:mo><mml:mfrac><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s4-2"><title>Race models: N &gt; 2</title><p>The Race model departs from previous model formulations in that it has a particle for each of <inline-formula><mml:math id="inf142"><mml:mi>N</mml:mi></mml:math></inline-formula> choice options instead of a single particle representing the evidence for one option over another. The function <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> now represents the probability of particle <inline-formula><mml:math id="inf144"><mml:mi>i</mml:mi></mml:math></inline-formula> to be the first of all particle to cross the bound <inline-formula><mml:math id="inf145"><mml:mi>a</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf146"><mml:mi>t</mml:mi></mml:math></inline-formula>. We consider race models for which the drift and starting point can vary for each particle separately. Treating the boundary as a constant <inline-formula><mml:math id="inf147"><mml:mi>a</mml:mi></mml:math></inline-formula> leaves us with a parameter vector <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The SDE is defined for each particle separately (or in vector form) as<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>𝐗</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐖</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:msubsup><mml:mi>𝐗</mml:mi><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>𝐗</mml:mi><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These models represent the most straightforward extension to a multichoice scenario.</p></sec><sec id="s4-3"><title>Multilayered perceptron</title><sec id="s4-3-1"><title>Network specifics</title><p>We apply the same simple architecture consistently across all example contexts in this paper. Our networks have three hidden layers, <inline-formula><mml:math id="inf149"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, of sizes <inline-formula><mml:math id="inf150"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, each using <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> activation functions. The output layer consists of a single node with linear activation function.</p></sec><sec id="s4-3-2"><title>Training process</title><sec id="s4-3-2-1"><title>Training hyperparameters</title><p>The network is trained via stochastic back-propagation using the Adam (<xref ref-type="bibr" rid="bib43">Kingma and Ba, 2014</xref>) optimization algorithm. As a loss function, we utilize the Huber loss (<xref ref-type="bibr" rid="bib39">Huber, 1992</xref>) defined as<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-3-2-2"><title>Training data</title><p>We used the following approach to generate training data across all examples shown below.</p><p>First, we generate 100K simulations from the stochastic simulator (or model <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) for each of 1.5 M parameter configurations. Since for the examples we consider the stochasticity underlying the models are in the form of a SDE, all simulations were conducted using the simple Euler–Maruyama method with timesteps <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> of 0.001 s. The maximum time we allowed the algorithms to run was 20 s, much more than necessary for a normal application of the simulator models under consideration.</p><p>Based on these simulations, we then generate empirical likelihood functions using KDEs (<xref ref-type="bibr" rid="bib80">Turner et al., 2015</xref>). KDEs use atomic datapoints <inline-formula><mml:math id="inf154"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and reformulate them into a continuous probability distribution <inline-formula><mml:math id="inf155"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:mfrac><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where we choose <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as a standard Gaussian kernel <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>exp</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf158"><mml:mi>h</mml:mi></mml:math></inline-formula>, the so-called bandwidth parameter, is set by utilizing Silverman’s rule of thumb (<xref ref-type="bibr" rid="bib77">Silverman, 1986</xref>). Where the data made Silverman’s rule inapplicable, we set a lower bound on <italic>h</italic> as 10<sup>−3</sup>. Additionally, we follow <xref ref-type="bibr" rid="bib13">Charpentier and Flachaire, 2015</xref> in transforming our KDE to accommodate positive random variables with skewed distributions (in adherence to the properties of data resulting from the response time models forming our examples).</p><p>To ensure that the networks accurately learn likelihoods across a range of plausible data, for each parameter set we trained the networks by sampling 1000 datapoints from a mixture distribution with three components (mixture probabilities respectively <inline-formula><mml:math id="inf159"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>). The first component draws samples directly from the KDE distributions. The second component is uniform on <inline-formula><mml:math id="inf160"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, and the third component samples uniformly on <inline-formula><mml:math id="inf161"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. The aim of this mixture is to allow the network to see, for each parameter setting of the stochastic simulator, training examples of three kinds: (1) ‘Where it matters’, that is, where the bulk of the probability mass is given in the generative model. (2) Regions of low probability to inform the likelihood estimate in those regions (i.e., to prevent distortion of likelihood estimates for datapoints that are unlikely to be generated under the model). (3) Examples on the negative real line to ensure that it is learned to consistently drive likelihood predictions to 0 for datapoints close to 0.</p><p>The supervision signal for training has two components. For positive datapoints (reaction times in our examples), we evaluate the log-likelihood according to our KDE. Likelihoods of negative datapoints were set to an arbitrary low value of 10<sup>−29</sup> (a log-likelihood of −66.79). 10<sup>−29</sup> also served as the lower bounds on likelihood evaluations. While this constrains our accuracy on the very tails of distributions, extremely low evaluations unduly affect the training procedure. Since the generation of training data can easily be parallelized across machines, we simply front-loaded the data generation accordingly. We refer back to <xref ref-type="fig" rid="fig2">Figure 2</xref> for a conceptual overview.</p><p>This procedure yields 1.5<italic>B</italic> labeled training examples on which we train the network. We applied early stopping upon a lack of loss improvement for more than five epochs of training. All models were implemented using Tensorflow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>).</p><p>We note here that this amount of training examples is likely an overshoot by potentially one or more orders of magnitude. We did not systematically test for the minimum amount of training examples needed to train the networks. Minimal experiments we ran showed that roughly one-tenth of the training examples lead to very much equivalent training results. Systematic minimization of the training data is left for future numerical experiments since we do not deem it essential for purposes of a proof of concept.</p></sec></sec><sec id="s4-3-3"><title>Sampling algorithms</title><p>Once trained, we can now run standard MCMC schemes, where, instead of an analytical likelihood, we evaluate <inline-formula><mml:math id="inf162"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as a forward pass through the MLP. <xref ref-type="fig" rid="fig1">Figure 1B</xref> schematically illustrates this approach (following the green arrows) and contrasts with currently applied methods (red arrows). We report multiple so-conducted parameter recovery experiments in the 'Results' section and validate the approach first with models with known analytical likelihood functions.</p><p>Regarding sampling, we utilized two MCMC algorithms, which showed generally very similar results. In contrast to the importance sampling algorithm used for the CNN (described below), MCMC methods are known for having trouble with multimodal posteriors. Running our experiments across algorithms was a safeguard against incorporating sampler-specific deficiencies into our analysis. We however acknowledge that even more extensive experiments may be necessary for comprehensive guarantees. First, having an ultimate implementation of our method into the HDDM Python toolbox (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>) in view, we use slice sampling (as used by the toolbox), specifically the step-out procedure following <xref ref-type="bibr" rid="bib55">Neal, 2003</xref>. Second, we used a custom implementation of the DEMCMC algorithm (<xref ref-type="bibr" rid="bib10">Braak, 2006</xref>), known for being robust in higher dimensional parameter spaces. Our DEMCMC implementation adds reflecting boundaries to counteract problematic behavior when the sampler attempts to move beyond the parameter space, which is truncated by the (broad) range of parameters in which the MLP was trained. The number of chains we use is consistently determined as <inline-formula><mml:math id="inf163"><mml:mrow><mml:mn>5</mml:mn><mml:mo>*</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, five times the number of parameters of a given stochastic model. Samplers were initialized by using slight perturbations of five maximum likelihood estimates and computed via differential evolution optimization (<xref ref-type="bibr" rid="bib79">Storn and Price, 1997</xref>; <xref ref-type="bibr" rid="bib86">Virtanen et al., 2020</xref>). Since results were very similar across samplers, we restrict ourselves mostly to reporting results derived from the slice sampler, given that this sampler forms the back-end of the HDDM user interface we envision. Implementations of the MLP method, the samplers we used, as well as the training pipeline can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/al-mlp">https://github.com/lnccbrown/lans/tree/master/al-mlp</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:985b12b70c56af4d31f0674d3317a4aeaac1f419;origin=https://github.com/lnccbrown/lans/;visit=swh:1:snp:5eaab2fe7281d7891f747744994023109d07c00c;anchor=swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6">swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6</ext-link>); <xref ref-type="bibr" rid="bib22">Fengler, 2021</xref>.</p></sec><sec id="s4-3-4"><title>Additional notes</title><p>Note that we restricted parameter recovery for the MLP to datasets that distributed at least 5% of choices to the less frequently chosen option. This modest filtering accommodates the fact that such datasets were also excluded form the training data for the MLP model since they (1) present difficulties for the KDE estimator and (2) lead to generally less stable parameter estimates (i.e., it is not advisable to use diffusion models when choices are deterministic).</p></sec></sec><sec id="s4-4"><title>Convolutional neural network</title><sec id="s4-4-1"><title>Network specifics</title><p>The CNN takes as an input a parameter vector θ, giving as output a discrete probability distribution over the relevant dataspace. In the context of our examples below, the output space is of dimensions <inline-formula><mml:math id="inf164"><mml:mrow><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℛ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℛ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf165"><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the number of relevant choice alternatives, and <inline-formula><mml:math id="inf166"><mml:msub><mml:mi>N</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> is the number of bins for the reaction time for each choice (<inline-formula><mml:math id="inf167"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> for all examples below). The network architecture consists of a sequence of three fully connected upsampling layers, <inline-formula><mml:math id="inf168"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, of respectively <inline-formula><mml:math id="inf169"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>64</mml:mn><mml:mo>,</mml:mo><mml:mn>256</mml:mn><mml:mo>,</mml:mo><mml:mn>1024</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> nodes. These are followed by a sequence of three convolutional layers <inline-formula><mml:math id="inf170"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>3</mml:mn><mml:mi>C</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf171"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> kernels, and a final fully connected layer with softmax activation. The network size was not minimized through architecture search, which, along with other potential further speed improvements, we leave for future research.</p></sec><sec id="s4-4-2"><title>Training process</title><p>For the CNN, we use 100K simulations from the stochastic simulator for each of 3 M parameter vectors and bin the simulation outcomes as normalized counts into <inline-formula><mml:math id="inf172"><mml:mrow><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℛ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℛ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula> slots respectively (looking ahead to our examples, <inline-formula><mml:math id="inf173"><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> concerns the number of choice outcomes, and <inline-formula><mml:math id="inf174"><mml:msub><mml:mi>N</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> the number of bins into which the reaction time outcomes are split for a given simulator). The resultant relative frequency histograms (empirical likelihood functions) <inline-formula><mml:math id="inf175"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒳</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, then serve as the target labels during training, with the corresponding parameters θ serving as feature vectors. For a given parameter vector θ, the CNN gives out a histogram <inline-formula><mml:math id="inf176"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf177"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> are the network parameters. The network is then trained by minimizing the KL divergence between observed and generated histograms<disp-formula id="equ19"><mml:math id="m19"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒟</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo largeop="true" mathsize="120%" movablelimits="false" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="120%">i</mml:mi><mml:mo mathsize="120%" stretchy="false">=</mml:mo><mml:mn mathsize="120%">0</mml:mn></mml:mrow><mml:mi mathsize="120%">c</mml:mi></mml:munderover><mml:munderover><mml:mo largeop="true" mathsize="120%" movablelimits="false" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="120%">j</mml:mi><mml:mo mathsize="120%" stretchy="false">=</mml:mo><mml:mn mathsize="120%">0</mml:mn></mml:mrow><mml:mi mathsize="120%">d</mml:mi></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mover accent="true"><mml:mi mathsize="120%" mathvariant="normal">ℓ</mml:mi><mml:mo mathsize="120%" stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mi mathsize="120%">θ</mml:mi><mml:mo maxsize="120%" minsize="120%">|</mml:mo><mml:msub><mml:mi mathsize="120%">x</mml:mi><mml:mrow><mml:mi mathsize="120%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">j</mml:mi></mml:mrow></mml:msub><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow><mml:mi mathsize="120%">log</mml:mi><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi mathsize="120%" mathvariant="normal">ℓ</mml:mi><mml:mo mathsize="120%" stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mi mathsize="120%">θ</mml:mi><mml:mo lspace="2.5pt" mathsize="120%" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathsize="120%">x</mml:mi><mml:mrow><mml:mi mathsize="120%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="120%" mathvariant="normal">ℓ</mml:mi><mml:mrow><mml:mi mathsize="120%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">c</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">l</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mi mathsize="120%">θ</mml:mi><mml:mo lspace="2.5pt" mathsize="120%" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathsize="120%">x</mml:mi><mml:mrow><mml:mi mathsize="120%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Training <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒟</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is not the only option. We note that it would have been a valid choice to train on <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒟</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true"><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib51">Minka, 2013</xref>) or the symmetrized Kullback–Leibler divergence instead. Training results, however, were good enough for our present purposes to leave a precise performance comparison across those loss functions for future research, leaving room for further improvements.</p><p>As for the MLP, we use the Adam optimizer (<xref ref-type="bibr" rid="bib43">Kingma and Ba, 2014</xref>) and implemented the network in Tensorflow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>).</p></sec><sec id="s4-4-3"><title>Sampling algorithm</title><p>One benefit of using the CNN lies in the enhanced potential for parallel processing across large number of parameter configurations and datapoints. To fully exploit this capability, instead of running a (sequential) MCMC algorithm for our parameter recovery studies, we use iterated importance sampling, which can be done in parallel. Specifically, we use adaptive importance sampling based on mixtures of t-distributions, following a slightly adjusted version of the suggestions in <xref ref-type="bibr" rid="bib11">Cappé et al., 2008</xref>; <xref ref-type="bibr" rid="bib92">Wraith et al., 2009</xref>.</p><p>While importance sampling is well established, for clarity and the setting in which we apply it, we explain some of the details here. Importance sampling algorithms are driven by the basic equality<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>which holds for any pair of probability distributions such that <inline-formula><mml:math id="inf180"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is our posterior distribution, and <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the proposal distribution. We now sample <inline-formula><mml:math id="inf184"><mml:mi>N</mml:mi></mml:math></inline-formula> tuples θ according to <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and assign each <inline-formula><mml:math id="inf186"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> an importance weight <inline-formula><mml:math id="inf187"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi mathvariant="bold">𝐢</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi mathvariant="bold">𝐢</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p><p>To get samples from the posterior distribution, we sample with replacement the θ from the set <inline-formula><mml:math id="inf188"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, with probabilities assigned as the normalized weights <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We note that importance sampling is exact for <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. However for finite <inline-formula><mml:math id="inf191"><mml:mi>N</mml:mi></mml:math></inline-formula>, the performance is strongly dependent on the quality of the proposal distribution <inline-formula><mml:math id="inf192"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A bad match of <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> leads to high variance in the importance weights, which drives down performance of the algorithm, as commonly measured by the effective sample size (<xref ref-type="bibr" rid="bib46">Liu, 2008</xref>)<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Iterated importance sampling uses consecutive importance sampling rounds to improve the proposal distribution <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A final importance sampling round is used to get the importance sample we use as our posterior sample. Specifically, we start with a mixture of t-distributions <inline-formula><mml:math id="inf196"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf197"><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi></mml:math></inline-formula> is the number of mixture components. Each component of <inline-formula><mml:math id="inf198"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is centered at the MAP according to a optimization run (again we used differential evolution). The component-covariance matrix is estimated by a numerical approximation of the Hessian at the respective MAP. Each round <inline-formula><mml:math id="inf199"><mml:mi>i</mml:mi></mml:math></inline-formula>, based on the importance sample <inline-formula><mml:math id="inf200"><mml:msub><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, we update the proposal distribution (to a new mixture of t-distributions) using the update equations derived in <xref ref-type="bibr" rid="bib11">Cappé et al., 2008</xref>.</p><p>As suggested by <xref ref-type="bibr" rid="bib11">Cappé et al., 2008</xref>, convergence is assessed using the normalized perplexity statistic (the exponentiated Shannon entropy of the importance weights). For run <inline-formula><mml:math id="inf201"><mml:mi>i</mml:mi></mml:math></inline-formula>, this is computed as <inline-formula><mml:math id="inf202"><mml:msup><mml:mi>exp</mml:mi><mml:mfrac><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi/><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id="inf203"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To help convergence, we depart from the basic setup suggested in <xref ref-type="bibr" rid="bib11">Cappé et al., 2008</xref> in the following way. We apply an annealing factor <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext> </mml:mtext><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, so that for iteration <inline-formula><mml:math id="inf205"><mml:mi>k</mml:mi></mml:math></inline-formula> of the importance sampler we are operating on the target <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfrac></mml:msup></mml:mrow></mml:math></inline-formula>. Smoothing the target during the first iterations helps with successfully adjusting the proposal distribution <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="fig2">Figure 2</xref> visualizes the CNN approach. Again, we emphasize that more numerical experiments using a larger variety of sampling algorithms are desirable, but are out of the scope for this paper. Implementations of the CNN method, the samplers we used, as well as the training pipeline can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/al-cnn">https://github.com/lnccbrown/lans/tree/master/al-cnn</ext-link>.</p></sec></sec><sec id="s4-5"><title>Strengths and weaknesses</title><p>In this section, we clarify a few strengths and weaknesses of the two presented methods and their respective use cases. First, representing the likelihood function datapoint-wise as an MLP output, or globally via the CNN output histogram, affects the potential for parallelization. As exploited by the choice of sampler, the CNN is very amenable to parallelization across parameters since inputs are parameter tuples only. Since the output is represented as a global likelihood histogram, the dataset likelihood is computed as the summation of the elementwise multiplied of bin-log-likelihoods, with a correspondingly binned dataset (counts over bins). This has the highly desirable property of making evaluation cost (time) independent of dataset size. While the MLP in principle allows parallel processing of inputs, the datapoint-wise representation of input values (<inline-formula><mml:math id="inf208"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>) makes the potential for cross-parameter parallelization dependent on dataset sizes. While a single evaluation of the CNN is more costly, cross-parameter batch processing can make it preferable to the MLP. Second, the CNN has an advantage during training, where the representation of the output as a softmax layer, and corresponding training via minimization of the KL divergence, provides a more robust training signal to ensure probability distributions compared to the purely local one in which the MLP learns a scalar likelihood output as a simple regression problem. Third, and conversely, the MLP formulation is more natural for trial-wise parameter estimates since the histogram representations may be redundant in case datapoints are in fact evaluated one by one (given datapoint-wise parameters induced by trial-by-trial effects on parameter vectors). Give equivalent success in learning likelihoods, we see potential for speedup when using the pointwise approach in this case. In principle, both approaches however allow one to estimate the impact of trial-wise regressors on model parameters during inference, without further training. It is, for example, common in the cognitive neuroscience literature to allow the cross-trial time-course of EEG, fMRI, or spike signals to be modeled as a trial-by-trial regressor on model parameters of, for example, DDMs (<xref ref-type="bibr" rid="bib87">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Frank et al., 2015</xref>; <xref ref-type="bibr" rid="bib12">Cavanagh et al., 2011</xref>; <xref ref-type="bibr" rid="bib36">Herz et al., 2016</xref>; <xref ref-type="bibr" rid="bib63">Pedersen and Frank, 2020</xref>). Another relevant example is the incorporation of latent learning dynamics. If a subject’s choice behavior is driven by reinforcement learning across stimuli, we can translate this into trial-by-trial effects on the parameter vectors of a generative process model (<xref ref-type="bibr" rid="bib63">Pedersen and Frank, 2020</xref>). These applications are implicitly enabled at no extra cost with the MLP method, while the trial-by-trial split multiplies the necessary computations for the CNN by the number <inline-formula><mml:math id="inf209"><mml:mi>N</mml:mi></mml:math></inline-formula> of datapoints when compared to scenarios that only need dataset-wise parameters. We stress again, however, that in general both the CNN and the MLP can directly be used for hierarchical inference scenarios. The preceding discussion pertains to further potential for optimization and relative strengths, not categorical potential for application to a given scenario. With respect to the latter, both methods are essentially equal.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was funded by NIMH grants P50 MH 119467-01 and R01 MH084840-08A1. We thank Michael Shvartsman, Matthew Nassar, and Thomas Serre for helpful comments and discussion regarding the earlier versions of this manuscript. Furthermore, we thank Mads Lund Pederson for help with integrating our methods into the HDDM Python toolbox. Lastly, we would like to thank the two reviewers of the manuscript for helpful suggestions, which improved the readability of the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Senior editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Validation, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-65074-transrepform-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All code is provided freely and is available at the following links: <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/hddmnn_tutorial">https://github.com/lnccbrown/lans/tree/master/hddmnn_tutorial</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/al-mlp">https://github.com/lnccbrown/lans/tree/master/al-mlp</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/lnccbrown/lans/tree/master/al-cnn">https://github.com/lnccbrown/lans/tree/master/al-cnn</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6">https://archive.softwareheritage.org/swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name><name><surname>Ghemawat</surname> <given-names>S</given-names></name><name><surname>Irving</surname> <given-names>G</given-names></name><name><surname>Isard</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: a system for large-scale machine learning</article-title><conf-name>12th USENIX Symposium on Operating SystemsDesign and Implementation (OSDI 16)</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Variational bayesian monte carlo with noisy likelihoods</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahn</surname> <given-names>WY</given-names></name><name><surname>Haines</surname> <given-names>N</given-names></name><name><surname>Zhang</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Revealing neurocomputational mechanisms of reinforcement learning and Decision-Making with the hBayesDM package</article-title><source>Computational Psychiatry</source><volume>1</volume><fpage>24</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1162/CPSY_a_00002</pub-id><pub-id pub-id-type="pmid">29601060</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akeret</surname> <given-names>J</given-names></name><name><surname>Refregier</surname> <given-names>A</given-names></name><name><surname>Amara</surname> <given-names>A</given-names></name><name><surname>Seehars</surname> <given-names>S</given-names></name><name><surname>Hasner</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Approximate bayesian computation for forward modeling in cosmology</article-title><source>Journal of Cosmology and Astroparticle Physics</source><volume>2015</volume><elocation-id>043</elocation-id><pub-id pub-id-type="doi">10.1088/1475-7516/2015/08/043</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>Doll</surname> <given-names>BB</given-names></name><name><surname>Long</surname> <given-names>NM</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rostrolateral prefrontal cortex and individual differences in uncertainty-driven exploration</article-title><source>Neuron</source><volume>73</volume><fpage>595</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.025</pub-id><pub-id pub-id-type="pmid">22325209</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behnel</surname> <given-names>S</given-names></name><name><surname>Bradshaw</surname> <given-names>R</given-names></name><name><surname>Citro</surname> <given-names>C</given-names></name><name><surname>Dalcin</surname> <given-names>L</given-names></name><name><surname>Seljebotn</surname> <given-names>DS</given-names></name><name><surname>Smith</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cython: the best of both worlds</article-title><source>Computing in Science &amp; Engineering</source><volume>13</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Mixture Density Networks</source><publisher-name>Technical report</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname> <given-names>DM</given-names></name><name><surname>Kucukelbir</surname> <given-names>A</given-names></name><name><surname>McAuliffe</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variational inference: a review for statisticians</article-title><source>Journal of the American Statistical Association</source><volume>112</volume><fpage>859</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1080/01621459.2017.1285773</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehm</surname> <given-names>U</given-names></name><name><surname>Annis</surname> <given-names>J</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Kellen</surname> <given-names>D</given-names></name><name><surname>Krypotos</surname> <given-names>A-M</given-names></name><name><surname>Lerche</surname> <given-names>V</given-names></name><name><surname>Logan</surname> <given-names>GD</given-names></name><name><surname>Palmeri</surname> <given-names>TJ</given-names></name><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Servant</surname> <given-names>M</given-names></name><name><surname>Singmann</surname> <given-names>H</given-names></name><name><surname>Starns</surname> <given-names>JJ</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Estimating across-trial variability parameters of the diffusion decision model: expert advice and recommendations</article-title><source>Journal of Mathematical Psychology</source><volume>87</volume><fpage>46</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2018.09.004</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braak</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A markov chain monte carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces</article-title><source>Statistics and Computing</source><volume>16</volume><fpage>239</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1007/s11222-006-8769-1</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cappé</surname> <given-names>O</given-names></name><name><surname>Douc</surname> <given-names>R</given-names></name><name><surname>Guillin</surname> <given-names>A</given-names></name><name><surname>Marin</surname> <given-names>J-M</given-names></name><name><surname>Robert</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Adaptive importance sampling in general mixture classes</article-title><source>Statistics and Computing</source><volume>18</volume><fpage>447</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1007/s11222-008-9059-x</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>JF</given-names></name><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Cohen</surname> <given-names>MX</given-names></name><name><surname>Figueroa</surname> <given-names>CM</given-names></name><name><surname>Samanta</surname> <given-names>J</given-names></name><name><surname>Sherman</surname> <given-names>SJ</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Subthalamic nucleus stimulation reverses mediofrontal influence over decision threshold</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1462</fpage><lpage>1467</lpage><pub-id pub-id-type="doi">10.1038/nn.2925</pub-id><pub-id pub-id-type="pmid">21946325</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charpentier</surname> <given-names>A</given-names></name><name><surname>Flachaire</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Log-transform kernel density estimation of income distribution</article-title><source>L’actualité Économique</source><volume>91</volume><fpage>141</fpage><lpage>159</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname> <given-names>P</given-names></name><name><surname>Puskas</surname> <given-names>GA</given-names></name><name><surname>El-Murr</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decisions in changing conditions: the urgency-gating model</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>11560</fpage><lpage>11571</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1844-09.2009</pub-id><pub-id pub-id-type="pmid">19759303</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cranmer</surname> <given-names>K</given-names></name><name><surname>Brehmer</surname> <given-names>J</given-names></name><name><surname>Louppe</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The frontier of simulation-based inference</article-title><source>PNAS</source><volume>117</volume><fpage>30055</fpage><lpage>30062</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id><pub-id pub-id-type="pmid">32471948</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Trial-by-trial data analysis using computational models</article-title><conf-name>Decision Making, Affect, and Learning: Attention and Performance XXIII</conf-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Model-based influences on humans' choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id><pub-id pub-id-type="pmid">21435563</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diaconis</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The markov chain monte carlo revolution</article-title><source>Bulletin of the American Mathematical Society</source><volume>46</volume><fpage>179</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1090/S0273-0979-08-01238-X</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doi</surname> <given-names>T</given-names></name><name><surname>Fan</surname> <given-names>Y</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Ding</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The caudate nucleus contributes causally to decisions that balance reward and uncertain visual information</article-title><source>eLife</source><volume>9</volume><elocation-id>e56694</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56694</pub-id><pub-id pub-id-type="pmid">32568068</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast and accurate monte carlo sampling of first-passage times from Wiener Diffusion models</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>20490</elocation-id><pub-id pub-id-type="doi">10.1038/srep20490</pub-id><pub-id pub-id-type="pmid">26864391</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Feller</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1968">1968</year><source>An Introduction to Probability Theory and Its Applications</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fengler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>projectABC</data-title><source>Software Heritage</source><version designator="swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6">swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6">https://archive.softwareheritage.org/swh:1:rev:e3369b9df138c75d0e490be0c48c53ded3e3a1d6</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Anwander</surname> <given-names>A</given-names></name><name><surname>Schäfer</surname> <given-names>A</given-names></name><name><surname>Neumann</surname> <given-names>J</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortico-striatal connections predict control over speed and accuracy in perceptual decision making</article-title><source>PNAS</source><volume>107</volume><fpage>15916</fpage><lpage>15920</lpage><pub-id pub-id-type="doi">10.1073/pnas.1004932107</pub-id><pub-id pub-id-type="pmid">20733082</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Samanta</surname> <given-names>J</given-names></name><name><surname>Moustafa</surname> <given-names>AA</given-names></name><name><surname>Sherman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hold your horses: impulsivity, deep brain stimulation, and medication in parkinsonism</article-title><source>Science</source><volume>318</volume><fpage>1309</fpage><lpage>1312</lpage><pub-id pub-id-type="doi">10.1126/science.1146157</pub-id><pub-id pub-id-type="pmid">17962524</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Gagne</surname> <given-names>C</given-names></name><name><surname>Nyhus</surname> <given-names>E</given-names></name><name><surname>Masters</surname> <given-names>S</given-names></name><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Cavanagh</surname> <given-names>JF</given-names></name><name><surname>Badre</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>fMRI and EEG predictors of dynamic decision parameters during human reinforcement learning</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>485</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2036-14.2015</pub-id><pub-id pub-id-type="pmid">25589744</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Frazier</surname> <given-names>PI</given-names></name><name><surname>Angela</surname> <given-names>JY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sequential hypothesis testing under stochastic deadlines</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>465</fpage><lpage>472</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical Science</source><volume>7</volume><fpage>457</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geweke</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Evaluating the accuracy of sampling-based approaches to the calculations of posterior moments</article-title><source>Bayesian Statistics</source><volume>4</volume><fpage>641</fpage><lpage>649</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonçalves</surname> <given-names>PJ</given-names></name><name><surname>Lueckmann</surname> <given-names>JM</given-names></name><name><surname>Deistler</surname> <given-names>M</given-names></name><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Öcal</surname> <given-names>K</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Chintaluri</surname> <given-names>C</given-names></name><name><surname>Podlaski</surname> <given-names>WF</given-names></name><name><surname>Haddad</surname> <given-names>SA</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Greenberg</surname> <given-names>DS</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title><source>eLife</source><volume>9</volume><elocation-id>e56261</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56261</pub-id><pub-id pub-id-type="pmid">32940606</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Greenberg</surname> <given-names>D</given-names></name><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Macke</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title><italic>Automatic posterior transformation for Likelihood-Free inference</italic></article-title><conf-name>International Conference on Machine Learning, PMLR</conf-name><fpage>2404</fpage><lpage>2414</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Guillaumes</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mixture density networks for distribution and uncertainty estimation</article-title><publisher-name>Universitat Politècnica de Catalunya. Facultat d’Informàtica de Barcelona, PhD thesis</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutenkunst</surname> <given-names>RN</given-names></name><name><surname>Waterfall</surname> <given-names>JJ</given-names></name><name><surname>Fergal</surname> <given-names>PC</given-names></name><name><surname>Brown</surname> <given-names>KS</given-names></name><name><surname>Myers</surname> <given-names>CR</given-names></name><name><surname>Sethna</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sloppy models and parameter indeterminancy in systems biology</article-title><source>PLOS Computational Biology</source><volume>3</volume><elocation-id>30189</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0030189</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutmann</surname> <given-names>MU</given-names></name><name><surname>Dutta</surname> <given-names>R</given-names></name><name><surname>Kaski</surname> <given-names>S</given-names></name><name><surname>Corander</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Likelihood-free inference via classification</article-title><source>Statistics and Computing</source><volume>28</volume><fpage>411</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s11222-017-9738-6</pub-id><pub-id pub-id-type="pmid">31997856</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>2476</fpage><lpage>2484</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2410-14.2015</pub-id><pub-id pub-id-type="pmid">25673842</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Lin</surname> <given-names>YS</given-names></name><name><surname>Reynolds</surname> <given-names>A</given-names></name><name><surname>Strickland</surname> <given-names>L</given-names></name><name><surname>Gretton</surname> <given-names>M</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dynamic models of choice</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>961</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1067-y</pub-id><pub-id pub-id-type="pmid">29959755</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herz</surname> <given-names>DM</given-names></name><name><surname>Zavala</surname> <given-names>BA</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural correlates of decision thresholds in the human subthalamic nucleus</article-title><source>Current Biology</source><volume>26</volume><fpage>916</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.01.051</pub-id><pub-id pub-id-type="pmid">26996501</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>MD</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo</article-title><source>Journal of Machine Learning Research : JMLR</source><volume>15</volume><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A practical guide to the probability density approximation (PDA) with improved implementation and error characterization</article-title><source>Journal of Mathematical Psychology</source><volume>68-69</volume><fpage>13</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2015.08.006</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huber</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><chapter-title>Robust estimation of a location parameter</chapter-title><person-group person-group-type="editor"><name><surname>Kotz</surname> <given-names>S</given-names></name><name><surname>Johnson</surname> <given-names>N. L</given-names></name></person-group><source>Breakthroughs in Statistics</source><publisher-name>Springer</publisher-name><fpage>492</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1007/978-1-4612-4380-9_35</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Maia</surname> <given-names>TV</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational psychiatry as a bridge from neuroscience to clinical applications</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>404</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1038/nn.4238</pub-id><pub-id pub-id-type="pmid">26906507</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Järvenpää</surname> <given-names>M</given-names></name><name><surname>Gutmann</surname> <given-names>MU</given-names></name><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Marttinen</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gaussian process modelling in approximate bayesian computation to estimate horizontal gene transfer in Bacteria</article-title><source>The Annals of Applied Statistics</source><volume>12</volume><fpage>2228</fpage><lpage>2251</lpage><pub-id pub-id-type="doi">10.1214/18-AOAS1150</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Järvenpää</surname> <given-names>M</given-names></name><name><surname>Gutmann</surname> <given-names>MU</given-names></name><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Marttinen</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Parallel gaussian process surrogate bayesian inference with noisy likelihood evaluations</article-title><source>Bayesian Analysis</source><volume>16</volume><fpage>147</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1214/20-BA1200</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname> <given-names>I</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</article-title><source>PNAS</source><volume>108</volume><fpage>13852</fpage><lpage>13857</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101328108</pub-id><pub-id pub-id-type="pmid">21808009</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lipton</surname> <given-names>A</given-names></name><name><surname>Kaushansky</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the first hitting time density of an ornstein-uhlenbeck process</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.02390">https://arxiv.org/abs/1810.02390</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Monte Carlo Strategies in Scientific Computing</source><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-76371-2</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Karaletsos</surname> <given-names>T</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Likelihood-free inference with emulator networks</article-title><conf-name>Symposium on Advances in Approximate Bayesian Inference, PMLR</conf-name><fpage>32</fpage><lpage>53</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname> <given-names>G</given-names></name><name><surname>Leslie</surname> <given-names>DS</given-names></name><name><surname>Ludwig</surname> <given-names>CJH</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Time-varying decision boundaries: insights from optimality analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>971</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1340-6</pub-id><pub-id pub-id-type="pmid">28730465</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Meeds</surname> <given-names>E</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Gps-abc: gaussian process surrogate approximate bayesian computation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1401.2838">https://arxiv.org/abs/1401.2838</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mestdagh</surname> <given-names>M</given-names></name><name><surname>Verdonck</surname> <given-names>S</given-names></name><name><surname>Meers</surname> <given-names>K</given-names></name><name><surname>Loossens</surname> <given-names>T</given-names></name><name><surname>Tuerlinckx</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prepaid parameter estimation without likelihoods</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007181</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007181</pub-id><pub-id pub-id-type="pmid">31498789</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Minka</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Expectation propagation for approximate bayesian inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.2294">https://arxiv.org/abs/1301.2294</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Mullowney</surname> <given-names>P</given-names></name><name><surname>Iyengar</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Maximum Likelihood Estimation and Computation for the Ornstein-Uhlenbeck Process</source><publisher-name>stat.pitt</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarro</surname> <given-names>DJ</given-names></name><name><surname>Fuss</surname> <given-names>IG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fast and accurate calculations for first-passage times in Wiener Diffusion models</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>222</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2009.02.003</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Neal</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayesian learning for neural networks</article-title><publisher-name>University of Toronto, PhD thesis</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Slice sampling</article-title><source>The Annals of Statistics</source><volume>31</volume><fpage>705</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1214/aos/1056562461</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nilsson</surname> <given-names>H</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hierarchical bayesian parameter estimation for cumulative prospect theory</article-title><source>Journal of Mathematical Psychology</source><volume>55</volume><fpage>84</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2010.08.006</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Edlund</surname> <given-names>JA</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural prediction errors reveal a risk-sensitive reinforcement-learning process in the human brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>551</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5498-10.2012</pub-id><pub-id pub-id-type="pmid">22238090</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname> <given-names>RC</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</article-title><source>Neural Computation</source><volume>18</volume><fpage>283</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1162/089976606775093909</pub-id><pub-id pub-id-type="pmid">16378516</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palestro</surname> <given-names>JJ</given-names></name><name><surname>Weichart</surname> <given-names>E</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Turner</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Some task demands induce collapsing bounds: evidence from a behavioral analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>1225</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1479-9</pub-id><pub-id pub-id-type="pmid">29845433</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Nalisnick</surname> <given-names>E</given-names></name><name><surname>Rezende</surname> <given-names>DJ</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name><name><surname>Lakshminarayanan</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Normalizing flows for probabilistic modeling and inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.02762">https://arxiv.org/abs/1912.02762</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Sterratt</surname> <given-names>D</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Sequential neural likelihood: fast likelihood-free inference with autoregressive flows PMLR</article-title><conf-name>The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>837</fpage><lpage>848</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast ε-free inference of simulation models with bayesian conditional density estimation</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1028</fpage><lpage>1036</lpage></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedersen</surname> <given-names>ML</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simultaneous hierarchical bayesian parameter estimation for reinforcement learning and drift diffusion models: a tutorial and links to neural data</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>458</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1007/s42113-020-00084-w</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Radev</surname> <given-names>ST</given-names></name><name><surname>Mertens</surname> <given-names>UK</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Ardizzone</surname> <given-names>L</given-names></name><name><surname>Kothe</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>BayesFlow: learning complex stochastic models with invertible neural networks</article-title><conf-name>IEEE Transactions on Neural Networks and Learning Systems</conf-name><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.3042395</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radev</surname> <given-names>ST</given-names></name><name><surname>Mertens</surname> <given-names>UK</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Köthe</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Towards end-to-end likelihood-free inference with convolutional neural networks</article-title><source>British Journal of Mathematical and Statistical Psychology</source><volume>73</volume><fpage>23</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1111/bmsp.12159</pub-id><pub-id pub-id-type="pmid">30793299</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rangel</surname> <given-names>A</given-names></name><name><surname>Camerer</surname> <given-names>C</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A framework for studying the neurobiology of value-based decision making</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>545</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1038/nrn2357</pub-id><pub-id pub-id-type="pmid">18545266</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Childers</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Individual differences and fitting methods for the two-choice diffusion model of decision making</article-title><source>Decision</source><volume>2</volume><fpage>237</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1037/dec0000030</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reinforcement-based decision making in corticostriatal circuits: mutual constraints by neurocomputational and diffusion models</article-title><source>Neural Computation</source><volume>24</volume><fpage>1186</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00270</pub-id><pub-id pub-id-type="pmid">22295983</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname> <given-names>AM</given-names></name><name><surname>Rhodes</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The lévy flight paradigm: random search patterns and mechanisms</article-title><source>Ecology</source><volume>90</volume><fpage>877</fpage><lpage>887</lpage><pub-id pub-id-type="doi">10.1890/08-0153.1</pub-id><pub-id pub-id-type="pmid">19449680</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rezende</surname> <given-names>D</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variational inference with normalizing flows</article-title><conf-name>International Conference on Machine Learning, PMLR</conf-name><fpage>1530</fpage><lpage>1538</lpage></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robert</surname> <given-names>C</given-names></name><name><surname>Casella</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A short history of markov chain monte carlo: subjective recollections from incomplete data</article-title><source>Statistical Science</source><volume>26</volume><fpage>102</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1214/10-STS351</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Robert</surname> <given-names>C</given-names></name><name><surname>Casella</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Monte Carlo Statistical Methods</source><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4757-4145-2</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönberg</surname> <given-names>T</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Joel</surname> <given-names>D</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reinforcement learning signals in the human striatum distinguish learners from nonlearners during reward-based decision making</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>12860</fpage><lpage>12867</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2496-07.2007</pub-id><pub-id pub-id-type="pmid">18032658</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn</surname> <given-names>M</given-names></name><name><surname>Lam</surname> <given-names>NH</given-names></name><name><surname>Murray</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A flexible framework for simulating and fitting generalized drift-diffusion models</article-title><source>eLife</source><volume>9</volume><elocation-id>e56938</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56938</pub-id><pub-id pub-id-type="pmid">32749218</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Silverman</surname> <given-names>BW</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>Density Estimation for Statistics and Data Analysis</source><volume>26</volume><publisher-name>CRC press</publisher-name><pub-id pub-id-type="doi">10.1201/9781315140919</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sisson</surname> <given-names>SA</given-names></name><name><surname>Fan</surname> <given-names>Y</given-names></name><name><surname>Beaumont</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Handbook of Approximate Bayesian Computation</source><publisher-name>CRC Press</publisher-name><pub-id pub-id-type="doi">10.1201/9781315117195</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storn</surname> <given-names>R</given-names></name><name><surname>Price</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces</article-title><source>Journal of Global Optimization</source><volume>11</volume><fpage>341</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1023/A:1008202821328</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>van Maanen</surname> <given-names>L</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Informing cognitive abstractions through neuroimaging: the neural drift diffusion model</article-title><source>Psychological Review</source><volume>122</volume><fpage>312</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1037/a0038894</pub-id><pub-id pub-id-type="pmid">25844875</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A generalized, likelihood-free method for posterior estimation</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>227</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0530-0</pub-id><pub-id pub-id-type="pmid">24258272</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Van Zandt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Approximating bayesian inference through model simulation</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>826</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.003</pub-id><pub-id pub-id-type="pmid">30093313</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title><source>Psychological Review</source><volume>108</volume><fpage>550</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.108.3.550</pub-id><pub-id pub-id-type="pmid">11488378</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Opheusden</surname> <given-names>B</given-names></name><name><surname>Acerbi</surname> <given-names>L</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unbiased and efficient log-likelihood estimation with inverse binomial sampling</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008483</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008483</pub-id><pub-id pub-id-type="pmid">33362195</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name><name><surname>Tuerlinckx</surname> <given-names>F</given-names></name><name><surname>Lee</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hierarchical diffusion models for two-choice response times</article-title><source>Psychological Methods</source><volume>16</volume><fpage>44</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1037/a0021765</pub-id><pub-id pub-id-type="pmid">21299302</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname> <given-names>P</given-names></name><name><surname>Gommers</surname> <given-names>R</given-names></name><name><surname>Oliphant</surname> <given-names>TE</given-names></name><name><surname>Haberland</surname> <given-names>M</given-names></name><name><surname>Reddy</surname> <given-names>T</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Burovski</surname> <given-names>E</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name><name><surname>Weckesser</surname> <given-names>W</given-names></name><name><surname>Bright</surname> <given-names>J</given-names></name><name><surname>van der Walt</surname> <given-names>SJ</given-names></name><name><surname>Brett</surname> <given-names>M</given-names></name><name><surname>Wilson</surname> <given-names>J</given-names></name><name><surname>Millman</surname> <given-names>KJ</given-names></name><name><surname>Mayorov</surname> <given-names>N</given-names></name><name><surname>Nelson</surname> <given-names>ARJ</given-names></name><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Kern</surname> <given-names>R</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Carey</surname> <given-names>CJ</given-names></name><name><surname>Polat</surname> <given-names>İ</given-names></name><name><surname>Feng</surname> <given-names>Y</given-names></name><name><surname>Moore</surname> <given-names>EW</given-names></name><name><surname>VanderPlas</surname> <given-names>J</given-names></name><name><surname>Laxalde</surname> <given-names>D</given-names></name><name><surname>Perktold</surname> <given-names>J</given-names></name><name><surname>Cimrman</surname> <given-names>R</given-names></name><name><surname>Henriksen</surname> <given-names>I</given-names></name><name><surname>Quintero</surname> <given-names>EA</given-names></name><name><surname>Harris</surname> <given-names>CR</given-names></name><name><surname>Archibald</surname> <given-names>AM</given-names></name><name><surname>Ribeiro</surname> <given-names>AH</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>van Mulbregt</surname> <given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Sofer</surname> <given-names>I</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>HDDM: hierarchical bayesian estimation of the Drift-Diffusion model in Python</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><pub-id pub-id-type="pmid">23935581</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A computational model of inhibitory control in frontal cortex and basal ganglia</article-title><source>Psychological Review</source><volume>120</volume><fpage>329</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1037/a0031542</pub-id><pub-id pub-id-type="pmid">23586447</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wieschen</surname> <given-names>EM</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Radev</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Jumping to conclusion? A lévy flight model of decision making</article-title><source>The Quantitative Methods for Psychology</source><volume>16</volume><fpage>120</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.20982/tqmp.16.2.p120</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Collins</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id><pub-id pub-id-type="pmid">31769410</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wosniack</surname> <given-names>ME</given-names></name><name><surname>Santos</surname> <given-names>MC</given-names></name><name><surname>Raposo</surname> <given-names>EP</given-names></name><name><surname>Viswanathan</surname> <given-names>GM</given-names></name><name><surname>da Luz</surname> <given-names>MGE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The evolutionary origins of lévy walk foraging</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005774</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005774</pub-id><pub-id pub-id-type="pmid">28972973</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wraith</surname> <given-names>D</given-names></name><name><surname>Kilbinger</surname> <given-names>M</given-names></name><name><surname>Benabed</surname> <given-names>K</given-names></name><name><surname>Cappé</surname> <given-names>O</given-names></name><name><surname>Cardoso</surname> <given-names>J-F</given-names></name><name><surname>Fort</surname> <given-names>G</given-names></name><name><surname>Prunet</surname> <given-names>S</given-names></name><name><surname>Robert</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Estimation of cosmological parameters using adaptive importance sampling</article-title><source>Physical Review D</source><volume>80</volume><elocation-id>023507</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevD.80.023507</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yartsev</surname> <given-names>MM</given-names></name><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Yoon</surname> <given-names>AM</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Causal contribution and dynamical encoding in the striatum during evidence accumulation</article-title><source>eLife</source><volume>7</volume><elocation-id>e34929</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34929</pub-id><pub-id pub-id-type="pmid">30141773</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zajkowski</surname> <given-names>WK</given-names></name><name><surname>Kossut</surname> <given-names>M</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A causal role for right frontopolar cortex in directed, but not random, exploration</article-title><source>eLife</source><volume>6</volume><elocation-id>e27430</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.27430</pub-id><pub-id pub-id-type="pmid">28914605</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Parameter recovery</title><p>Here, we provide additional figures concerning parameter recovery studies. <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> summarizes the parameter-wise <inline-formula><mml:math id="inf210"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> between ground truth and the posterior mean estimates for each tested model and for each the CNN and MLP (where applicable) methods in turn. For the MLP, results are based on a reference run that used training data constructed from KDE empirical likelihoods utilizing 100k simulations each, and a slice sampler stopped with help of the Geweke diagnostic. Results in the paper are based on slice samplers as well as slice samplers, which explains why not all <inline-formula><mml:math id="inf211"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> values match exactly the ones found in other figures. Our findings were, however, generally robust across samplers.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Parameter recovery for a variety of test bed models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>DDM</bold></th><th/><th>N</th><th>v</th><th>a</th><th>w</th><th>ndt</th><th/><th/><th/><th/><th/><th/><th/><th/></tr></thead><tbody><tr><td><inline-formula><mml:math id="inf212"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>1.0</td><td>1.0</td><td>0.99</td><td>1</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>1.0</td><td>1.0</td><td>0.99</td><td>1</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>1</td><td>0.94</td><td>0.98</td><td>1</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>1</td><td>1</td><td>0.99</td><td>1</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><bold>DDM-SDV</bold></td><td/><td/><td><bold>v</bold></td><td><bold>a</bold></td><td><bold>w</bold></td><td><bold>ndt</bold></td><td><bold>sdv</bold></td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf213"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>0.95</td><td>0.94</td><td>0.96</td><td>1</td><td>0.57</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.94</td><td>0.95</td><td>0.97</td><td>1</td><td>0.58</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>0.98</td><td>0.97</td><td>0.98</td><td>1</td><td>0.79</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.98</td><td>0.99</td><td>1</td><td>0.87</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><bold>LC</bold></td><td/><td/><td><bold>v</bold></td><td><bold>a</bold></td><td><bold>w</bold></td><td><bold>ndt</bold></td><td><bold>θ</bold></td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf214"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>0.99</td><td>0.93</td><td>0.97</td><td>1</td><td>0.98</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.94</td><td>0.98</td><td>1</td><td>0.97</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>0.96</td><td>0.94</td><td>0.97</td><td>1</td><td>0.97</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.97</td><td>0.94</td><td>0.98</td><td>1</td><td>0.97</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><bold>OU</bold></td><td/><td/><td><bold>v</bold></td><td><bold>a</bold></td><td><bold>w</bold></td><td><bold>ndt</bold></td><td><bold>g</bold></td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf215"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>0.98</td><td>0.89</td><td>0.98</td><td>0.99</td><td>0.12</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.79</td><td>0.95</td><td>0.99</td><td>0.03</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>0.99</td><td>0.94</td><td>0.97</td><td>1</td><td>0.41</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.95</td><td>0.98</td><td>1</td><td>0.45</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><bold>Levy</bold></td><td/><td/><td><bold>v</bold></td><td><bold>a</bold></td><td><bold>w</bold></td><td><bold>ndt</bold></td><td><bold>α</bold></td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf216"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>0.96</td><td>0.94</td><td>0.84</td><td>1</td><td>0.33</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.97</td><td>0.91</td><td>0.61</td><td>1</td><td>0.2</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>0.99</td><td>0.97</td><td>0.9</td><td>1</td><td>0.71</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.98</td><td>0.95</td><td>1</td><td>0.8</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td><bold>Weibull</bold></td><td/><td/><td><bold>v</bold></td><td><bold>a</bold></td><td><bold>w</bold></td><td><bold>ndt</bold></td><td><bold>α</bold></td><td><bold>β</bold></td><td/><td/><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf217"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>0.99</td><td>0.82</td><td>0.96</td><td>1</td><td>0.2</td><td>0.43</td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.8</td><td>0.98</td><td>0.99</td><td>0.26</td><td>0.41</td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>0.98</td><td>0.91</td><td>0.96</td><td>1</td><td>0.4</td><td>0.69</td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.98</td><td>0.91</td><td>0.97</td><td>1</td><td>0.37</td><td>0.63</td><td/><td/><td/><td/><td/><td/></tr><tr><td><bold>Full-DDM</bold></td><td/><td/><td><bold>v</bold></td><td><bold>a</bold></td><td><bold>w</bold></td><td><bold>ndt</bold></td><td><bold>dw</bold></td><td><bold>sdv</bold></td><td><bold>dndt</bold></td><td/><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf218"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>MLP</td><td>1024</td><td>0.95</td><td>0.94</td><td>0.88</td><td>1</td><td>0</td><td>0.28</td><td>0.47</td><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.93</td><td>0.94</td><td>0.88</td><td>1</td><td>0</td><td>0.25</td><td>0.38</td><td/><td/><td/><td/><td/></tr><tr><td/><td>CNN</td><td>1024</td><td>0.98</td><td>0.98</td><td>0.93</td><td>1</td><td>0</td><td>0.62</td><td>0.79</td><td/><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.99</td><td>0.99</td><td>0.97</td><td>1</td><td>0</td><td>0.8</td><td>0.91</td><td/><td/><td/><td/><td/></tr><tr><td><bold>Race 3</bold></td><td/><td/><td><bold>v0</bold></td><td><bold>v1</bold></td><td><bold>v2</bold></td><td><bold>a</bold></td><td><bold>w0</bold></td><td><bold>w1</bold></td><td><bold>w2</bold></td><td><bold>ndt</bold></td><td/><td/><td/><td/></tr><tr><td><inline-formula><mml:math id="inf219"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>CNN</td><td>1024</td><td>0.88</td><td>0.86</td><td>0.89</td><td>0.19</td><td>0.49</td><td>0.51</td><td>0.5</td><td>0.99</td><td/><td/><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.93</td><td>0.91</td><td>0.93</td><td>0.18</td><td>0.49</td><td>0.47</td><td>0.47</td><td>1</td><td/><td/><td/><td/></tr><tr><td><bold>Race 4</bold></td><td/><td/><td><bold>v0</bold></td><td><bold>v1</bold></td><td><bold>v2</bold></td><td><bold>v3</bold></td><td><bold>a</bold></td><td><bold>w0</bold></td><td><bold>w1</bold></td><td><bold>w2</bold></td><td><bold>w3</bold></td><td><bold>ndt</bold></td><td/><td/></tr><tr><td><inline-formula><mml:math id="inf220"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>CNN</td><td>1024</td><td>0.73</td><td>0.68</td><td>0.71</td><td>0.73</td><td>0.11</td><td>0.49</td><td>0.5</td><td>0.48</td><td>0.49</td><td>0.99</td><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.79</td><td>0.76</td><td>0.77</td><td>0.81</td><td>0.18</td><td>0.5</td><td>0.5</td><td>0.51</td><td>0.55</td><td>0.99</td><td/><td/></tr><tr><td><bold>LCA 3</bold></td><td/><td/><td><bold>v0</bold></td><td><bold>v1</bold></td><td><bold>v2</bold></td><td><bold>a</bold></td><td><bold>w0</bold></td><td><bold>w1</bold></td><td><bold>w2</bold></td><td><bold>g</bold></td><td><bold>b</bold></td><td><bold>ndt</bold></td><td/><td/></tr><tr><td><inline-formula><mml:math id="inf221"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>CNN</td><td>1024</td><td>0.58</td><td>0.56</td><td>0.58</td><td>0.47</td><td>0.7</td><td>0.72</td><td>0.68</td><td>0.27</td><td>0.57</td><td>1</td><td/><td/></tr><tr><td/><td/><td>4096</td><td>0.51</td><td>0.5</td><td>0.52</td><td>0.44</td><td>0.67</td><td>0.67</td><td>0.66</td><td>0.23</td><td>0.52</td><td>1</td><td/><td/></tr><tr><td><bold>LCA 4</bold></td><td/><td/><td><bold>v0</bold></td><td><bold>v1</bold></td><td><bold>v2</bold></td><td><bold>v3</bold></td><td><bold>a</bold></td><td><bold>w0</bold></td><td><bold>w1</bold></td><td><bold>w2</bold></td><td><bold>w3</bold></td><td><bold>g</bold></td><td><bold>b</bold></td><td><bold>ndt</bold></td></tr><tr><td><inline-formula><mml:math id="inf222"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></td><td>CNN</td><td>1024</td><td>0.5</td><td>0.46</td><td>0.54</td><td>0.51</td><td>0.51</td><td>0.71</td><td>0.69</td><td>0.69</td><td>0.67</td><td>0.18</td><td>0.7</td><td>0.99</td></tr><tr><td/><td/><td>4096</td><td>0.42</td><td>0.42</td><td>0.46</td><td>0.42</td><td>0.52</td><td>0.67</td><td>0.63</td><td>0.68</td><td>0.65</td><td>0.15</td><td>0.64</td><td>1</td></tr><tr><td colspan="15">MLP: multilayered perceptron; CNN: convolutional neural network; DDM: drift diffusion model; LC: linear collapse; LCA: leaky competing accumulator.<break/></td></tr></tbody></table></table-wrap></sec><sec id="s9" sec-type="appendix"><title>Manifolds/likelihoods</title><p>We show some examples of the likelihood manifolds for the various models that we tested.</p><sec id="s9-1"><title>DDM-SDV</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Likelihoods and manifolds: DDM-SDV.</title><p>(<bold>A</bold>) shows the training and validation loss for Huber as well as mean squared error (MSE) for the drift diffusion model (DDM)-SDV model. Training was driven by the Huber loss. (<bold>B</bold>) illustrates the likelihood manifolds by varying one parameter in the trained region. (<bold>C</bold>) shows multilayered perceptron likelihoods in green on top of a sample of 50 kernel density estimate-based empirical likelihoods derived from 20k samples each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-app1-fig1-v3.tif"/></fig></sec></sec><sec id="s10" sec-type="appendix"><title>Linear collapse</title><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Likelihoods and manifolds: linear collapse.</title><p>(<bold>A</bold>) shows the training and validation loss for Huber as well as MSE for the linear collapse model. Training was driven by the Huber loss. (<bold>B</bold>) illustrates the likelihood manifolds by varying one parameter in the trained region. (<bold>C</bold>) shows multilayered perceptron likelihoods in green on top of a sample of 50 kernel density estimate-based empirical likelihoods derived from 20k samples each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-app1-fig2-v3.tif"/></fig></sec><sec id="s11" sec-type="appendix"><title>Weibull</title><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Likelihoods and manifolds: Weibull.</title><p>(<bold>A</bold>) shows the training and validation loss for Huber as well as MSE for the Weibull model. Training was driven by the Huber loss. (<bold>B</bold>) illustrates the likelihood manifolds by varying one parameter in the trained region. (<bold>C</bold>) shows multilayered perceptron likelihoods in green on top of a sample of 50 kernel density estimate-based empirical likelihoods derived from 100k samples each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-app1-fig3-v3.tif"/></fig></sec><sec id="s12" sec-type="appendix"><title>Levy</title><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Likelihoods and Manifolds: Levy.</title><p>(<bold>A</bold>) shows the training and validation loss for Huber as well as MSE for the Levy model. Training was driven by the Huber loss. (<bold>B</bold>) illustrates the likelihood manifolds by varying one parameter in the trained region. (<bold>C</bold>) shows multilayered perceptron likelihoods in green on top of a sample of 50 kernel density estimate-based empirical likelihoods derived from 100k samples each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-app1-fig4-v3.tif"/></fig></sec><sec id="s13" sec-type="appendix"><title>Ornstein</title><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Likelihoods and manifolds: Ornstein.</title><p>(<bold>A</bold>) shows the training and validation loss for Huber as well as MSE for the Ornstein model. Training was driven by the Huber loss. (<bold>B</bold>) illustrates the likelihood manifolds by varying one parameter in the trained region. (<bold>C</bold>) shows multilayered perceptron likelihoods in green on top of a sample of 50 kernel density estimate-based empirical likelihoods derived from 100k samples each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-app1-fig5-v3.tif"/></fig></sec><sec id="s14" sec-type="appendix"><title>Full-DDM</title><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Likelihoods and manifolds: full-DDM.</title><p>(<bold>A</bold>) shows the training and validation loss for Huber as well as MSE for the full drift diffusion model. Training was driven by the Huber loss. (<bold>B</bold>) illustrates the likelihood manifolds by varying one parameter in the trained region. (<bold>C</bold>) shows multilayered perceptron likelihoods in green on top of a sample of 50 kernel density estimate-based empirical likelihoods derived from 100k samples each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65074-app1-fig6-v3.tif"/></fig></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65074.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role>Reviewing Editor</role><aff><institution>École normale supérieure, PSL University, INSERM</institution><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Acerbi</surname><given-names>Luigi</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib><contrib contrib-type="reviewer"><name><surname>Daunizeau</surname><given-names>Jean</given-names> </name><role>Reviewer</role><aff><institution>INSERM</institution><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Your work addresses a very timely and important issue in the field: the restricted availability of candidate computational models that can be tested as explanations of brain or behavioral data. Your approach, which uses artificial neural networks for speeding up inference, helps achieving significantly faster solutions for fitting computational models to data, with potential impact for a broad research community. The ability to decouple the slow training of neural networks to learn the likelihood function of a given model from the fast use of previously trained neural networks to fit data is particularly appealing. We congratulate you for this work which should stimulate the exploration of previously untested computational models of brain activity and behavior.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Likelihood Approximation Networks (LANs) for Fast Inference of Simulation Models in Cognitive Neuroscience&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by Valentin Wyart as the Reviewing Editor and Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Luigi Acerbi (Reviewer #1); Jean Daunizeau (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>This manuscript describes a novel method based on artificial neural networks to approximate the trial likelihood function of a model whose likelihood is difficult or impossible to evaluate analytically. The authors demonstrate their method on a class of generalized drift-diffusion models, performing Bayesian inference and various forms of hierarchical inference, with good empirical results. The authors discuss the appeal of their method for broadening the family of models that can be employed in computational cognitive neuroscience.</p><p>Both reviewers have found your manuscript to be addressing a very timely and important issue in the field, namely: the restricted availability of candidate computational models that can be tested as explanations of observed data. The described work fits very well with recent work in the area which uses (deep) neural networks for speeding up inference. Both reviewers have found that the methods proposed in the manuscript can indeed help achieve faster solutions when fitting computational models to data, with potential impact for the scientific community. Both reviewers felt that the manuscript deserves publication in <italic>eLife</italic> given that the following points outlined below are addressed to improve the clarity of the presentation and also the limits of the proposed method.</p><p>Essential revisions:</p><p>1. Clarifications:</p><p>For most of the manuscript, the authors make a distinction of &quot;analytical likelihood&quot; vs. &quot;no likelihood&quot; (where one needs to do simulation-based inference). However, there is a middle-ground of models for which the likelihood is not analytical but can be computed numerically. These models constitute a superset of the analytical models. The authors touch upon this point only in the Discussion (when talking about Shinn et al. 2020 and generalized DDMs), but this point and the reference should come up much earlier as it is quite important. There are still good reasons for wanting to do amortized inference as opposed to using numerical likelihoods (the latter can be very expensive, and they don't exist for all DDMs), but the readers should be made aware of these alternative options – even beyond the DDM example.</p><p>Mathematical notations are problematic. First, most notations are not described when they first appear, and the reader often has to guess what the variables are. For example, the KDE notation is first presented when introducing the ABC approach (section 2), but one has to read the Additional Methods (section 10) to find out what they really are. Second, mathematical notations are not fully consistent throughout the paper. For example, the parameters are called either P or theta, the data is sometimes called D and sometimes X, whereas X also sometimes denotes the DDM's accumulated evidence. The variable W sometimes means the ANN's weights, sometimes the DDM's Wiener noise, sometimes the DDM's initial condition. The authors should make sure that notations are self-contained and consistent throughout the manuscript.</p><p>Another concern regarding mathematical notations is that the typical notation for modelling likelihoods in cognitive science is p(x|s,theta), where x is the observation/response, s a vector describing the &quot;stimulus&quot; in the current trial, and theta the model parameters. In this paper, the authors merge s into theta, which is fine, but it is important to specify it (e.g., this affects the dimensionality of theta).</p><p>Important aspects of the training of the networks should be explained earlier in the manuscript. First, the networks are trained on hundreds of billions of simulations: O(10<sup>6</sup>) parameter settings times 10<sup>5</sup> samples per parameter setting. This ballpark should be mentioned early (e.g., in Figure 1 and 2 or related text). More generally, it is important to stress that amortized inference is not free at all – it incurs a very large cost upfront. So, for example, it is likely not worth it in the scenario in which a researcher is iterating through many different models (as opposed to a single model applied many times). Second, the initial introduction of the neural network models (lines 255-269) is confusing, due to some unfortunate choice of wording in the explanation (and despite the fact that what the authors are doing is quite simple):</p><p>1) In the pointwise representation, the network is learning a mapping from x and theta to p(x|theta).</p><p>2) In the histogram representation, the network is learning a mapping from theta to p(x|theta), where p(x|theta) is represented as a histogram over the space of (discretized) observations x.</p><p>What is confusing is that the choice of architecture used in the paper (MLP for pointwise, CNN for histogram) seems to be orthogonal to what the network is representing. The histogram representation could use a MLP architecture. In fact, the authors should explain why they state that the histogram representation &quot;naturally leads to CNNs as the network architecture&quot; (e.g., there seems to be no translational invariance in the problem). Why is MLP a priori better for approximating pointwise likelihoods, and CNN better for histogram likelihoods? What, in these architectures, makes them suited for the tasks? For example, what would authors expect, if they used CNNs for pointwise likelihoods and MLPs for histogram likelihoods? Did authors try different architectures and ended concluding that this was the best choice, or is it an educated guess?</p><p>Also, the expression &quot;discretized global likelihood of an entire dataset&quot; confusing, because to me that would be learning p(D|theta). Instead, the authors mean learning the full trial likelihood function p(x|theta), which in their case helps compute the likelihood of the entire datasets (because the trials are assumed to be i.i.d.). Last, the authors use &quot;parameterization&quot; when they mean &quot;parameter vector&quot;.</p><p>Overall, it would be very helpful if the authors were to rewrite this part more clearly. Figure 2 could be made larger, highlighting the input and output of the networks.</p><p>2. Limitations of the method:</p><p>Related to an earlier point, it is important to stress explicitly that amortized inference is not free at all – it incurs a very large cost upfront (here, training ANNs on hundreds of billions of simulations: O(10<sup>6</sup>) parameter settings times 10<sup>5</sup> samples per parameter setting). So, for example, it is likely not worth it in the scenario in which a researcher is iterating through many different models (as opposed to a single model applied many times).</p><p>The method described only applies to models whose likelihood is conditionally independent given the model and trial parameters (theta); that is whose likelihood factorizes as p(D|theta) = \prod<sub>i</sub> p(x<sub>i</sub>|theta). Models with a Markovian dependence on previous observations can still be addressed (by putting a limited number of previous observations into theta). However, the likelihood above does not apply to models with latent dynamics. The models that satisfy the requirements of the method are still plenty of models in cognitive science, but still, it is very important to specify the restrictions of the method. The authors can look at the section &quot;Definitions and notation&quot; of van Opheusden et al. (2020) for a more in-depth discussion.</p><p>The authors should state clearly the scalability of the proposed approach based on the dimensionality of theta (which includes the &quot;stimulus&quot; s, see an earlier point). The bottleneck here seems to be the coverage of the &quot;prior&quot; space. It is already impressive that the method works alright in 6-7 dimensions; still, it can be expected to worsen quickly due to the curse of dimensionality (not because of the network – but because of the inability of the prior sampling to cover the space). For example (which would be good to mention), Goncalves et al. (2020) have shown that neural networks can reconstruct posteriors in up to ~30 dimensions (with some tricks). However, to do so they cannot use pure, &quot;a priori&quot; amortized inference, but they need to iterate the sampling procedure to zoom into the region of interest.</p><p>Relatedly, the authors might want to comment on what happens when s or theta is not a continuous parameter (for example, see again van Opheusden et al., 2020; in one of the models, s takes an integer value of a &quot;board configuration&quot; which is not necessarily part of a nice metric space; e.g. board state 15 is not necessarily close to 16). Clearly the network can still work in principle but effectively we would expect discontinuities (due to learning in a discrete space) which might cause issues. In other words, unless the authors have some evidence that their method also works well for discrete parameters, they might want to mention this limitation of the method.</p><p>Relatedly also, the authors should explain more clearly how they chose the range of parameters which they sample from during ANN training. In a sense, and although the goal here is to approximate the likelihood, it seems to me that this step effectively places some form of implicit prior distribution on parameters (at least in terms of admissible parameter values). In turn, this induces some estimation artefacts, which can be eyeballed on some panels of Figure 8 (cf. α and β bound parameters of the Weibull model). Wouldn't the estimation error be much higher, had the authors simulated data under broader parameter ranges (further beyond the sampling range that was used for ANN training)? Does a restricted sampling range not eventually induce some form of potential non-identifiability issue? These points should be discussed more explicitly in the manuscript.</p><p>The authors have shown that the method they propose yields efficient (Bayesian) statistical inference on model parameters. However, one may also want to use the method for comparing models. Can authors demonstrate the model comparison capabilities of their method? If the authors believe this is out of scope, they should at least discuss this possible application of their method. ABC methods have sometimes been criticized for the fact that they induce model section biases. However, these biases may be related to the selection of data sufficient statistics (which may be unfair to some models). How do these points translate to the current approach?</p><p>The authors have demonstrated the appeal of their method on DDM variants, which typically predict response times and choices. Practically speaking, this means storing the trained ANNs for these two types of dependent variables. But what if one uses these models to predict another set of dependent behavioral variables (e.g. including choice confidence)? It seems that one would have to re-perform all the simulations. Or is there a way to leverage what has already been done? The authors may want to discuss this point somewhere in the manuscript.</p><p>3. Figures are not described with a sufficient amount of detail, and the read has at times to guess what captions do not state. For example, what do negative RT mean in Figure 4 (I guess: RT conditional on &quot;the other&quot; choice outcome)? What are the middle panels of Figure 7 showing (I guess: posterior intervals on ANGLE bounds over decision time, as well as empirical and fitted choice-dependant RT histograms)? What are the X-1024 and X-4096 variants of their ANN-based approach? What are the nodes/variables in the DAGs of Figure 10? These ambiguities should be solved in the revised manuscript.</p><p>4. The relationship of this work to a few related lines of work may deserve to be discussed. In particular, the approach proposed here appears to share similarities to (variational) autoencoders, which is now an established ANN-based method for solving very similar problems. Also, it may be worth discussing the pros and cons of the authors' approach, when compared to other existing approaches to likelihood-free inference, which would approximate the likelihood in some other way (using, e.g., Gaussian processes). Anecdotally, for the specific case of 'vanilla' DDM, there are recent advances to the problem of deriving conditional and marginal moments of RT distributions (Srivastava et al., 2016). These are much faster to evaluate than Fuss and Navarro (2009), because they provide analytical expressions that do not require any iterative numerical computations. Embedded within an ad-hoc likelihood (derived from an observation equation of the form: RT = E[RT|theta] + i.i.d. Gaussian noise), these can provide very fast/efficient parameter estimates. It would be worth discussing these kinds of approaches somewhere in the Discussion section.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65074.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. Clarifications:</p><p>For most of the manuscript, the authors make a distinction of &quot;analytical likelihood&quot; vs. &quot;no likelihood&quot; (where one needs to do simulation-based inference). However, there is a middle-ground of models for which the likelihood is not analytical but can be computed numerically. These models constitute a superset of the analytical models. The authors touch upon this point only in the Discussion (when talking about Shinn et al. 2020 and generalized DDMs), but this point and the reference should come up much earlier as it is quite important. There are still good reasons for wanting to do amortized inference as opposed to using numerical likelihoods (the latter can be very expensive, and they don't exist for all DDMs), but the readers should be made aware of these alternative options – even beyond the DDM example.</p></disp-quote><p>We agree that this is an important distinction that the paper should have emphasized more, beyond alluding to this in Figure 3 (whereby the full DDM was depicted in a different color because it requires numerical integration and is therefore costly to evaluate). We now make it explicit in Section 3 and mention it again in the main text in Section 4 when describing the test-bed models. Indeed, powerful numerical approximation methods exist for some likelihoods, but these do not afford straightforward or rapid Bayesian inference. An example of this is already present in the full DDM, given the numerical integration, which typically slows down estimation compared to standard DDM by at least an order of magnitude. Informal tests against the full DDM likelihood in the HDDM python package (Wickie et al. 2013) show that we gain an approximately 40 fold increase in speed for this model when using LANs instead. Notably, the generalized DDMs discussed in Shinn et al. 2020 did not include a method for posterior estimation of parameters (which would likely be quite costly to compute). Moreover, these generalized DDMs are limited to cases with Gaussian noise. Any departures from that, such as Levy flights, are not accommodated. This issue is not a concern for LANs.</p><disp-quote content-type="editor-comment"><p>Mathematical notations are problematic. First, most notations are not described when they first appear, and the reader often has to guess what the variables are. For example, the KDE notation is first presented when introducing the ABC approach (section 2), but one has to read the Additional Methods (section 10) to find out what they really are. Second, mathematical notations are not fully consistent throughout the paper. For example, the parameters are called either P or theta, the data is sometimes called D and sometimes X, whereas X also sometimes denotes the DDM's accumulated evidence. The variable W sometimes means the ANN's weights, sometimes the DDM's Wiener noise, sometimes the DDM's initial condition. The authors should make sure that notations are self-contained and consistent throughout the manuscript.</p><p>Another concern regarding mathematical notations is that the typical notation for modelling likelihoods in cognitive science is p(x|s,theta), where x is the observation/response, s a vector describing the &quot;stimulus&quot; in the current trial, and theta the model parameters. In this paper, the authors merge s into theta, which is fine, but it is important to specify it (e.g., this affects the dimensionality of theta).</p></disp-quote><p>We thank the reviewers for pointing this out. We have now improved notational consistencies. We now refer to network parameters as Φ, and datasets and data points are consistently 𝐱 and 𝑥. Model parameters and parameter dimensions are now always 𝜃 and |<sub>Θ|</sub>. Wiener noise is mentioned once as a capital bold 𝑑<sub>𝐖</sub>, which is distinct from (lower case) 𝑤 as the starting point parameter. We also made explicit that we suppress the stimulus 𝑠 in our discussions when first defining the likelihood.</p><disp-quote content-type="editor-comment"><p>Important aspects of the training of the networks should be explained earlier in the manuscript. First, the networks are trained on hundreds of billions of simulations: O(10<sup>6</sup>) parameter settings times 10<sup>5</sup> samples per parameter setting. This ballpark should be mentioned early (e.g., in Figure 1 and 2 or related text). More generally, it is important to stress that amortized inference is not free at all – it incurs a very large cost upfront. So, for example, it is likely not worth it in the scenario in which a researcher is iterating through many different models (as opposed to a single model applied many times).</p></disp-quote><p>We have now added additional details to our initial discussion of the ANNs and training procedures to help readability. As we now reiterate in the text, our approach’s primary benefit lies in reusability and flexibility, which we believe will afford practical advantages for end users to evaluate models of interest across arbitrary inference scenarios. Note that we did not attempt a structured minimization of training data. Indeed, informal experiments indicate performance would be quite similar with an order of magnitude less training. The reason we are not so focused on the precise training time is that the cost of training LANs is only incurred once for any theoretically meaningful model. At this point, the trained network can be added to a bank of LANs, which can be made available to the community and reused without further training. In turn, the community will be able to test their data against a broader variety of models (and to easily add their own to the bank) and which can be evaluated for arbitrary inference scenarios (hierarchical estimation, allow some parameters to vary across conditions or to relate to neural measure X, etc.). While it is true that, in principle, we would prefer to further minimize amortization cost (and we may attempt to do so in future), we view in the long run that this component is comparatively negligible for any model that would be reused.</p><disp-quote content-type="editor-comment"><p>Second, the initial introduction of the neural network models (lines 255-269) is confusing, due to some unfortunate choice of wording in the explanation (and despite the fact that what the authors are doing is quite simple):</p><p>1) In the pointwise representation, the network is learning a mapping from x and theta to p(x|theta).</p><p>2) In the histogram representation, the network is learning a mapping from theta to p(x|theta), where p(x|theta) is represented as a histogram over the space of (discretized) observations x.</p><p>What is confusing is that the choice of architecture used in the paper (MLP for pointwise, CNN for histogram) seems to be orthogonal to what the network is representing. The histogram representation could use a MLP architecture. In fact, the authors should explain why they state that the histogram representation &quot;naturally leads to CNNs as the network architecture&quot; (e.g., there seems to be no translational invariance in the problem). Why is MLP a priori better for approximating pointwise likelihoods, and CNN better for histogram likelihoods? What, in these architectures, makes them suited for the tasks? For example, what would authors expect, if they used CNNs for pointwise likelihoods and MLPs for histogram likelihoods? Did authors try different architectures and ended concluding that this was the best choice, or is it an educated guess?</p></disp-quote><p>We agree that the proposed network architectures are not hard constraints, and we did not intend to imply otherwise. Indeed, we had discussed this very issue amongst ourselves. We have now clarified in the manuscript that the problem representation is what matters. We pursued the architectures in each case that seemed to follow naturally but without any commitment to their optimality. When learning a mapping from simulation model parameters to a histogram representation of the likelihood, we map a low dimensional data manifold to a much higher dimensional one. Using an MLP for this purpose would imply that the number of neural network parameters needed to learn this function would be orders of magnitude larger than using a CNN. Not only would this mean that “forward passes“ through the network would take longer, but also an increased propensity to overfit on an identical data budget. Given that our networks performed adequately well in the training objective and parameter recovery, we felt that we did not need to pursue a rigorous architecture search for this paper’s main points to be developed. We will optimize these for speed in future work.</p><disp-quote content-type="editor-comment"><p>Also, the expression &quot;discretized global likelihood of an entire dataset&quot; confusing, because to me that would be learning p(D|theta). Instead, the authors mean learning the full trial likelihood function p(x|theta), which in their case helps compute the likelihood of the entire datasets (because the trials are assumed to be i.i.d.).</p></disp-quote><p>We recognized this as potentially confusing and rewrote sections of the related paragraphs for clarity.</p><disp-quote content-type="editor-comment"><p>Last, the authors use &quot;parameterization&quot; when they mean &quot;parameter vector&quot;.</p><p>Overall, it would be very helpful if the authors were to rewrite this part more clearly.</p></disp-quote><p>Thank you, we have now fixed in response to the same point elsewhere in the reviewer comments.</p><disp-quote content-type="editor-comment"><p>Figure 2 could be made larger, highlighting the input and output of the networks.</p></disp-quote><p>Thanks for this suggestion. We have now edited Figure 2 to showcase the neural networks more prominently.</p><disp-quote content-type="editor-comment"><p>2. Limitations of the method:</p><p>Related to an earlier point, it is important to stress explicitly that amortized inference is not free at all – it incurs a very large cost upfront (here, training ANNs on hundreds of billions of simulations: O(10<sup>6</sup>) parameter settings times 10<sup>5</sup> samples per parameter setting). So, for example, it is likely not worth it in the scenario in which a researcher is iterating through many different models (as opposed to a single model applied many times).</p></disp-quote><p>Please see a detailed response to this comment above. We now added these aspects at the end of section 3, and included some discussion that puts the quoted numbers into perspective. We agree that this will likely aid readability and help with a more nuanced perspective on global amortization.</p><disp-quote content-type="editor-comment"><p>The method described only applies to models whose likelihood is conditionally independent given the model and trial parameters (theta); that is whose likelihood factorizes as p(D|theta) = \prod<sub>i</sub> p(x<sub>i</sub>|theta). Models with a Markovian dependence on previous observations can still be addressed (by putting a limited number of previous observations into theta). However, the likelihood above does not apply to models with latent dynamics. The models that satisfy the requirements of the method are still plenty of models in cognitive science, but still, it is very important to specify the restrictions of the method. The authors can look at the section &quot;Definitions and notation&quot; of van Opheusden et al. (2020) for a more in-depth discussion.</p></disp-quote><p>We thank the reviewers for raising this point. The problems with amortization when one of the “parameters“ may be a discrete space of high cardinality are indeed real. However, in general our approach can in fact incorporate latent processes on the parameters, which can induce sequential dependence. An example is the RL-DDM model in Pedersen et al. 2020, which puts a latent reinforcement learning process on some of the DDM parameters, which we can easily accommodate with LANs, and which we note in the discussion.</p><disp-quote content-type="editor-comment"><p>The authors should state clearly the scalability of the proposed approach based on the dimensionality of theta (which includes the &quot;stimulus&quot; s, see an earlier point). The bottleneck here seems to be the coverage of the &quot;prior&quot; space. It is already impressive that the method works alright in 6-7 dimensions; still, it can be expected to worsen quickly due to the curse of dimensionality (not because of the network – but because of the inability of the prior sampling to cover the space). For example (which would be good to mention), Goncalves et al. (2020) have shown that neural networks can reconstruct posteriors in up to ~30 dimensions (with some tricks). However, to do so they cannot use pure, &quot;a priori&quot; amortized inference, but they need to iterate the sampling procedure to zoom into the region of interest.</p></disp-quote><p>Indeed, we would generally not advise to apply amortization with LANs naively for models of &gt; <sub>15</sub> parameters (the CNN was tested on models in that ballpark with the LCA4 in the paper) – or at least we haven’t tested that scenario yet. It is correct that in general the curse of dimensionality can hardly be avoided here, if global amortization is the goal. We do cite Goncalves et al. (2020) twice, and we now also explicitly mention the ballpark figure of 30 parameters with their method. However, a major part of our argument is that modularity of the amortized piece of the pipeline is important. While the amortization of a 30 dimensional posterior is impressive, it remains inflexible to any consideration of experimental design and in spirit focuses on ’single model’, ’single dataset’ fits. Even hierarchical estimation of the basic DDM model with 10 subjects, will elevate us to &gt; 40 parameters and render posterior amortization futile. While we emphasize the virtues of our approach, which lies in the flexibility and reuse, we certainly agree that it is not always the right choice to amortize likelihoods and that in some situations one may rightfully prefer the methods of e.g. Goncalves et al. (2020) (and others). Scaling of our approach with respect to a single model of possibly large parameter spaces, should follow the limitations of the approach of Goncalves et al. (2020), when training is restricted to one round (without ’zooming in’).</p><disp-quote content-type="editor-comment"><p>Relatedly, the authors might want to comment on what happens when s or theta is not a continuous parameter (for example, see again van Opheusden et al., 2020; in one of the models, s takes an integer value of a &quot;board configuration&quot; which is not necessarily part of a nice metric space; e.g. board state 15 is not necessarily close to 16). Clearly the network can still work in principle but effectively we would expect discontinuities (due to learning in a discrete space) which might cause issues. In other words, unless the authors have some evidence that their method also works well for discrete parameters, they might want to mention this limitation of the method.</p></disp-quote><p>Thank you, we have added this limitation briefly. In principle discrete parameters can work, however in general it is true that it may not be advisable for parameters which have a lot of discrete states, for which conditioning on the positions on a chess-board is a good example. The resulting manifold will likely not be smooth enough. Also, the amount of training data needed to amortize such scenario fully, may simply render global amortization an unreasonable computational strategy to begin with.</p><disp-quote content-type="editor-comment"><p>Relatedly also, the authors should explain more clearly how they chose the range of parameters which they sample from during ANN training. In a sense, and although the goal here is to approximate the likelihood, it seems to me that this step effectively places some form of implicit prior distribution on parameters (at least in terms of admissible parameter values). In turn, this induces some estimation artefacts, which can be eyeballed on some panels of Figure 8 (cf. α and β bound parameters of the Weibull model). Wouldn't the estimation error be much higher, had the authors simulated data under broader parameter ranges (further beyond the sampling range that was used for ANN training)? Does a restricted sampling range not eventually induce some form of potential non-identifiability issue? These points should be discussed more explicitly in the manuscript.</p></disp-quote><p>Generally performance will not be optimal if the generative parameters are too close (or outside as one may consider for experimental data) to the boundary of the training region, or if a dataset is defective in a number of ways (e.g., too few choices in one or the other category) which reduces parameter identifiability even for analytic likelihood methods. We now added some clarifying remarks at the end of the section on parameter recovery. The parameter spaces were chosen so as to strongly envelope reasonable expectations for experimental data. In actuality, our LANs were trained to cover a far broader range of RTs compared to the general scope in which DDM-like models are evaluated (usually decisions that take a few seconds; our LANs cover up to 30 seconds, which could always be further expanded but is unlikely to be meaningful). We explain this again in a response to a similar criticism below. We discuss this as well in response to the comment about Line 438-489.</p><disp-quote content-type="editor-comment"><p>The authors have shown that the method they propose yields efficient (Bayesian) statistical inference on model parameters. However, one may also want to use the method for comparing models. Can authors demonstrate the model comparison capabilities of their method? If the authors believe this is out of scope, they should at least discuss this possible application of their method. ABC methods have sometimes been criticized for the fact that they induce model section biases. However, these biases may be related to the selection of data sufficient statistics (which may be unfair to some models). How do these points translate to the current approach?</p></disp-quote><p>We now discuss this in section 8. It is indeed an important topic and a feasible application of the method. Initial results in the context of an extension to the HDDM package, show that DIC appears to work adequately to the same degree it does for analytic likelihood. However, to properly develop the model comparison aspect, and to do it justice would demand a substantial addition to the paper, which we indeed intended to leave for future work.</p><disp-quote content-type="editor-comment"><p>The authors have demonstrated the appeal of their method on DDM variants, which typically predict response times and choices. Practically speaking, this means storing the trained ANNs for these two types of dependent variables. But what if one uses these models to predict another set of dependent behavioral variables (e.g. including choice confidence)? It seems that one would have to re-perform all the simulations. Or is there a way to leverage what has already been done? The authors may want to discuss this point somewhere in the manuscript.</p></disp-quote><p>Thanks for this point. While there are some models that consider confidence as arising directly from the decision variable in DDMs (e.g., van den Berg et al., 2016), here we conceptualize this question more generally as asking about an approach to a joint model, where confidence is modeled conditioned on reaction times and choices, with some probabilistic model 𝑝<sub>𝑀𝑐𝑜𝑛𝑓</sub> (𝑐𝑜𝑛𝑓𝑖𝑑𝑒𝑛𝑐𝑒|𝑟𝑡, 𝑐, 𝜃<sub>𝑐𝑜𝑛𝑓</sub> ).</p><p>As a whole we would be left with,</p><p>𝑝𝑀<sub>𝑐𝑜𝑛𝑓</sub> (𝑐𝑜𝑛𝑓𝑖𝑑𝑒𝑛𝑐𝑒|𝑟𝑡, 𝑐, 𝜃<sub>𝑐𝑜𝑛𝑓</sub> )𝑝<sub>𝐷𝐷𝑀</sub>(𝑟𝑡, 𝑐|𝜃<sub>𝐷𝐷𝑀</sub>)</p><p>In essence, the question becomes how to “integrate out“ reaction times and choices from this model of choice confidence. This can be achieved either by using numerical integration using pre-computed LANs, or by attempting to train the joint model directly as a LAN (in principle training data from the DDM-LAN could be reused here or the DDM-LAN itself, a variety of options would exist). This is, in fact, an important second-order application, which we hope to address in future work.</p><disp-quote content-type="editor-comment"><p>3. Figures are not described with a sufficient amount of detail, and the read has at times to guess what captions do not state. For example, what do negative RT mean in Figure 4 (I guess: RT conditional on &quot;the other&quot; choice outcome)? What are the middle panels of Figure 7 showing (I guess: posterior intervals on ANGLE bounds over decision time, as well as empirical and fitted choice-dependant RT histograms)? What are the X-1024 and X-4096 variants of their ANN-based approach? What are the nodes/variables in the DAGs of Figure 10? These ambiguities should be solved in the revised manuscript.</p></disp-quote><p>Thank you for pointing out the lack of clarity in the captions. We hope to have resolved the resulting ambiguities. Figure 10 now explains the plate notation used and what 𝜃 and 𝜆 stand for right away. We added explanations regarding the legends in Figure 9. We added information to Figure 7 so that we explicitly describe the left, middle and right panels. We clarified what negative RTs mean in Figure 4 (indeed they are mirrored to display RTs for the alternative choice).</p><disp-quote content-type="editor-comment"><p>4. The relationship of this work to a few related lines of work may deserve to be discussed. In particular, the approach proposed here appears to share similarities to (variational) autoencoders, which is now an established ANN-based method for solving very similar problems. Also, it may be worth discussing the pros and cons of the authors' approach, when compared to other existing approaches to likelihood-free inference, which would approximate the likelihood in some other way (using, e.g., Gaussian processes). Anecdotally, for the specific case of 'vanilla' DDM, there are recent advances to the problem of deriving conditional and marginal moments of RT distributions (Srivastava et al., 2016). These are much faster to evaluate than Fuss and Navarro (2009), because they provide analytical expressions that do not require any iterative numerical computations. Embedded within an ad-hoc likelihood (derived from an observation equation of the form: RT = E[RT|theta] + i.i.d. Gaussian noise), these can provide very fast/efficient parameter estimates. It would be worth discussing these kinds of approaches somewhere in the Discussion section.</p></disp-quote><p>1. Variational Autoencoder: While a link can be drawn to autoencoders (see Fengler et al.(2020) Cognitive Science Society conference paper), we consider it not essential and potentially misleading (this reflects an update in our own thinking), in the following sense. Our networks are not attempting to learn latent variables, but rather the ’forward part’ of the problem. We ultimately use the ’forward part’ to infer the latent parameter vectors 𝜃 as part of the Bayesian inference framework, but this has little to do with the amortization strategy. We are not attempting a mapping from data back to data (via a bottleneck), in both strategies and moreover the learned mappings are not stochastic. Hence we decided to leave out any connection to the variational autoencoder framework.</p><p>2. Gaussian Processes: We rewrote parts of sections 3 and expanded the discussion on related works to include Gaussian Process approaches and other likelihood based variants, to make sure the contextualization is more thorough early on, instead of relying solely on elaborations in the discussion.</p><p>3. ’Vanilla’ DDM alternatives: The approach of Srivastava et al. (2016) aims to capture moments of RT distributions. While one could build adhoc likelihood functions of the form suggested by the reviewer, this seems to defeat the purpose of attempting to be faithful to the actual likelihood (there seems to be no guarantee that basing the likelihood on 𝑅𝑇 = 𝐸[𝑅𝑇|𝑡ℎ𝑒𝑡𝑎]+𝑒𝑝𝑠), which may lead to highly inaccurate posteriors when compared to our method (of course this is testable). Notwithstanding the potential general usefulness of the work of Srivastava et al. (2016), this approach may be of questionable value when one is interested in fast and “accurate“ posterior inference, and moreover does not generalize to model variations such as the LCA, Levy Flights, etc.</p></body></sub-article></article>