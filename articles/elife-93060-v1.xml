<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93060</article-id><article-id pub-id-type="doi">10.7554/eLife.93060</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93060.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Aligned and oblique dynamics in recurrent neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Schuessler</surname><given-names>Friedrich</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6716-7492</contrib-id><email>f.schuessler@tu-berlin.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7682-5178</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7473-1223</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author"><name><surname>Barak</surname><given-names>Omri</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7894-6344</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Faculty of Electrical Engineering and Computer Science, Technical University of Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Science of Intelligence, Research Cluster of Excellence</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03g001n57</institution-id><institution>Champalimaud Foundation</institution></institution-wrap><addr-line><named-content content-type="city">Lisbon</named-content></addr-line><country>Portugal</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>Laboratoire de Neurosciences Cognitives et Computationnelles, INSERM U960, Ecole Normale Superieure-PSL Research University</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03qryx823</institution-id><institution>Rappaport Faculty of Medicine and Network Biology Research Laboratories, Technion - Israel Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP93060</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-27"><day>27</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-23"><day>23</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2307.07654"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-13"><day>13</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93060.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-11"><day>11</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93060.2"/></event></pub-history><permissions><copyright-statement>© 2024, Schuessler et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Schuessler et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93060-v1.pdf"/><abstract><p>The relation between neural activity and behaviorally relevant variables is at the heart of neuroscience research. When strong, this relation is termed a neural representation. There is increasing evidence, however, for partial dissociations between activity in an area and relevant external variables. While many explanations have been proposed, a theoretical framework for the relationship between external and internal variables is lacking. Here, we utilize recurrent neural networks (RNNs) to explore the question of when and how neural dynamics and the network’s output are related from a geometrical point of view. We find that training RNNs can lead to two dynamical regimes: dynamics can either be aligned with the directions that generate output variables, or oblique to them. We show that the choice of readout weight magnitude before training can serve as a control knob between the regimes, similar to recent findings in feedforward networks. These regimes are functionally distinct. Oblique networks are more heterogeneous and suppress noise in their output directions. They are furthermore more robust to perturbations along the output directions. Crucially, the oblique regime is specific to recurrent (but not feedforward) networks, arising from dynamical stability considerations. Finally, we show that tendencies toward the aligned or the oblique regime can be dissociated in neural recordings. Altogether, our results open a new perspective for interpreting neural activity by relating network dynamics and their output.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>recurrent neural networks</kwd><kwd>learning</kwd><kwd>neural representations</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>EXC 2002/1 &quot;Science of Intelligence&quot; - project no. 390523135</award-id><principal-award-recipient><name><surname>Schuessler</surname><given-names>Friedrich</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>1442/2</award-id><principal-award-recipient><name><surname>Barak</surname><given-names>Omri</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0017/2021</award-id><principal-award-recipient><name><surname>Barak</surname><given-names>Omri</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-17-EURE-0017</award-id><principal-award-recipient><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An analysis of the relation between neural activity and behavioral output uncovers two dynamical regimes, shows how to model them, and demonstrates how to find them in experimental data.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The relation between neural activity and behavioral variables is often expressed in terms of neural representations. Sensory input and motor output have been related to the tuning curves of single neurons (<xref ref-type="bibr" rid="bib26">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib46">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib23">Hafting et al., 2005</xref>) and, since the advent of large-scale recordings, to population activity (<xref ref-type="bibr" rid="bib7">Buonomano and Maass, 2009</xref>; <xref ref-type="bibr" rid="bib64">Saxena and Cunningham, 2019</xref>; <xref ref-type="bibr" rid="bib81">Vyas et al., 2020</xref>). Both input and output can be decoded from population activity (<xref ref-type="bibr" rid="bib11">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Mante et al., 2013</xref>), even in real-time, closed-loop settings (<xref ref-type="bibr" rid="bib63">Sadtler et al., 2014</xref>; <xref ref-type="bibr" rid="bib83">Willett et al., 2021</xref>). However, neural activity is often not fully explained by observable behavioral variables. Some components of the unexplained neural activity have been interpreted as random trial-to-trial fluctuations (<xref ref-type="bibr" rid="bib16">Galgali et al., 2023</xref>), potentially linked to unobserved behavior (<xref ref-type="bibr" rid="bib73">Stringer et al., 2019b</xref>; <xref ref-type="bibr" rid="bib44">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Wang et al., 2023</xref>). Activity may further be due to other ongoing computations not immediately related to behavior, such as preparatory motor activity in a null space of the motor readout (<xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Hennequin et al., 2014</xref>). Finally, neural activity may partially be due to other constraints, e.g., related to the underlying connectivity (<xref ref-type="bibr" rid="bib2">Atallah and Scanziani, 2009</xref>; <xref ref-type="bibr" rid="bib47">Okun and Lampl, 2008</xref>), the process of learning (<xref ref-type="bibr" rid="bib63">Sadtler et al., 2014</xref>), or stability, i.e., the robustness of the neural dynamics to perturbations (<xref ref-type="bibr" rid="bib62">Russo et al., 2020</xref>).</p><p>Here, we aim for a theoretical understanding of neural representations: Which factors might determine how strongly activity and behavioral output variables are related? To this end, we use trained recurrent neural networks (RNNs). In this setting, output variables are determined by the task at hand, and neural activity can be described by its projection onto the principal components (PCs). We show that these networks can operate between two extremes: an ‘aligned’ regime in which the output weights and the largest PCs are strongly correlated, and a second, ‘oblique’ regime, where the output weights and the largest PCs are poorly correlated.</p><p>What determines the regime in which a network operates? We show that quite general considerations lead to a link between the magnitude of output weights and the regime of the network. As a consequence, we can use output magnitude as a control knob for trained RNNs. Indeed, when we trained RNN models on different neuroscience tasks, large output weights led to oblique dynamics, and small output weights to aligned dynamics. Recent results in feedforward networks identified two regimes – rich and lazy – that can also arise from choices of output weights (<xref ref-type="bibr" rid="bib9">Chizat et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Jacot et al., 2018</xref>). In an extensive Methods section, we further analyze in detail how the oblique and aligned regimes arise during learning. There we show that the dynamical nature of RNNs, in particular demanding stable dynamics, leads to the replacement of unstable, lazy, solutions by oblique ones.</p><p>We then considered the functional consequences of the two regimes. Building on the concept of feedback loops driving the network dynamics (<xref ref-type="bibr" rid="bib75">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib57">Rivkind and Barak, 2017</xref>), we show that, in the aligned regime, the largest PCs and the output are qualitatively similar. In the oblique regime, in contrast, the two may be qualitatively different. This functional decoupling in oblique networks leads to a large freedom for neural dynamics. Different networks with oblique dynamics thus tend to employ different dynamics for the same tasks. Aligned dynamics, in contrast, are much more stereotypical. Furthermore, as a result of how neural dynamics and output are coupled, oblique and aligned networks react differently to perturbations of the neural activity along the output direction. In particular, oblique (but not aligned) networks develop an additional negative feedback loop that suppresses output noise. We finally show that neural recordings from different experiments can have different degrees of alignment, which indicates that our theoretical results may be useful in identifying different regimes for different experiments, tasks, or brain regions.</p><p>Altogether, our work opens a new perspective relating network dynamics and their output, yielding important insights for modeling brain dynamics as well as experimentally accessible questions about learning and dynamics in the brain.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Aligned and oblique population dynamics</title><p>We consider an animal performing a task while both behavior and neural activity are recorded. For example, the task might be to produce a periodic motion, described by the output <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of <xref ref-type="fig" rid="fig1">Figure 1A</xref>. For simplicity, we assume that the behavioral output can be decoded linearly from the neural activity (<xref ref-type="bibr" rid="bib41">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Sadtler et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Willett et al., 2021</xref>). We can thus write<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with readout weights <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The activity of neuron <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mo>…</mml:mo><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and we refer to the vector <inline-formula><mml:math id="inf5"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> as the state of the network.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic of aligned and oblique dynamics in recurrent neural networks.</title><p>(<bold>A</bold>) Output generated by both networks. (<bold>B</bold>) Neural activity of aligned (top) and oblique (bottom) dynamics, visualized in the space spanned by three neurons. Here, the activity (green) is three-dimensional, but most of the variance is concentrated along the two largest principal components (PCs) (blue). For aligned dynamics, the output weights (red) are small and lie in the subspace spanned by the largest PCs; they are hence correlated to the activity. For oblique dynamics, the output weights are large and lie outside of the subspace spanned by the largest PCs; they are hence poorly correlated to the activity. (<bold>C</bold>) Projection of activity onto the two largest PCs. For oblique dynamics, the output weights are orthogonal to the leading PCs. (<bold>D</bold>) Evolution of PC projections over time. For aligned dynamics, the projection on the PCs resembles the output <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and reconstructing the output from the largest two components is possible. For the oblique dynamics, such reconstruction is not possible, because the projections oscillate much more slowly than the output.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig1-v1.tif"/></fig><p>Neural activity has to generate the output in some subspace of the state space, where each axis represents the activity of one neuron. In the simplest case (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, top), the output is produced along the largest PCs of activity, as shown by the fact that projecting the neural activity <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> onto the largest PCs returns the target oscillation (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, top). We call such dynamics ‘aligned’ because of the alignment between the subspace spanned by the largest PCs and the output vector (red).</p><p>There is, however, another possibility. Neural activity may have many other components not directly related to the output and these other components may even dominate the overall activity. In this case (<xref ref-type="fig" rid="fig1">Figure 1B and D</xref>, bottom), the two largest PCs are not enough to read out the output, and smaller PCs are needed. We call such dynamics ‘oblique’ because the subspace spanned by the largest PCs and the output vector are poorly aligned.</p><p>We consider these two possibilities as distinct dynamical regimes, noting that intermediate situations are also possible. The actual regime of neural dynamics has important consequences for how one interprets neural recordings. For aligned dynamics, analyzing the dynamics within the largest PCs may lead to insights about the computations generating the output (<xref ref-type="bibr" rid="bib81">Vyas et al., 2020</xref>). For oblique dynamics, such an analysis is hampered by the dissociation between the large PCs and the components generating the output (<xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>).</p></sec><sec id="s2-2"><title>Magnitude of output weights controls regime in trained RNNs</title><p>What determines which regime a neural network operates in? Given that behavior is the same, but representations differ, we study this question using trained RNNs. This framework constrains what networks do, but not how they do it (<xref ref-type="bibr" rid="bib77">Sussillo, 2014</xref>; <xref ref-type="bibr" rid="bib3">Barak, 2017</xref>). The specific property of representation we are interested in is the alignment, or correlation, between output weights and states:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the vector norms <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> quantify the magnitude of each vector.</p><p>For aligned dynamics, the correlation is large, corresponding to the alignment between the leading PCs of the neural activity and the output weights (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, top). In contrast, for oblique dynamics, this correlation is small (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, bottom). Note that the concept of correlation can be generalized to accommodate multiple time points and multidimensional output (see section Generalized correlation).</p><p>Studying the same task means that the output <inline-formula><mml:math id="inf10"><mml:mi>z</mml:mi></mml:math></inline-formula> is the same, so it is instructive to express it in terms of the correlation <inline-formula><mml:math id="inf11"><mml:mi>ρ</mml:mi></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Recent work on feedforward networks showed that <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> can have a large effect on the resulting representations (<xref ref-type="bibr" rid="bib27">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Chizat et al., 2019</xref>). <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> shows that <inline-formula><mml:math id="inf13"><mml:mi>ρ</mml:mi></mml:math></inline-formula> is indeed linked to <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula>, but <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> can also vary. In a detailed analysis in the Methods (sections Analysis of solutions under noiseless conditions to Oblique solutions arise for noisy, nonlinear systems), we show that for recurrent networks, stability considerations preclude <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> from being small. This implies that if we choose a readout norm <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> and then train the RNN on a given task, the correlation must compensate.</p><p>If we choose small output weights, we expect aligned dynamics, because a large correlation is necessary to generate sufficiently large output (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, top). If instead we choose large output weights, we expect oblique dynamics, because only a small correlation keeps the output magnitude from growing too large.</p><p>We tested whether output weights can serve as a control knob to select dynamical regimes using an RNN model trained on an abstract version of the cycling task introduced in <xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>. The networks were trained to generate a 2D signal that rotated in the plane spanned by two outputs <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). An input pulse at the beginning of each trial indicated the desired direction of rotation. We set up two models with either small or large output weights and trained the recurrent weights of each with gradient descent (section Details on RNN models and training).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Aligned and oblique dynamics for a cycling task (<xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>).</title><p>(<bold>A</bold>) A network with two outputs was trained to generate either clockwise or anticlockwise rotations, depending on the context (top). Our model recurrent neural network (RNN) (bottom) received a context input pulse, generated dynamics <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> via recurrent weights <inline-formula><mml:math id="inf21"><mml:mi>W</mml:mi></mml:math></inline-formula>, and yielded the output as linear projections of the states. We trained the recurrent weights <inline-formula><mml:math id="inf22"><mml:mi>W</mml:mi></mml:math></inline-formula> with gradient descent. (<bold>B, C</bold>) Resulting internal dynamics for two networks with small (top) and large (bottom) output weights, corresponding to aligned and oblique dynamics, respectively. (<bold>B</bold>) Dynamics projected on the first two principal components (PCs) and the remaining direction <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> of the first output vector (for <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>). The output weights are amplified to be visible. Arrowheads indicate the direction of the dynamics. Note that for the large output weights, the dynamics in the first two PCs co-rotated, despite the counter-rotating output. (<bold>C</bold>) Output reconstructed from the largest PCs, with dimension <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (full lines) or 8 (dotted). Two dimensions already yield a fit with <inline-formula><mml:math id="inf26"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula> for aligned dynamics (top), but almost no output for oblique (bottom, <inline-formula><mml:math id="inf27"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula>, no arrows shown). For the latter, a good fit with <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>% is only reached with <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig2-v1.tif"/></fig><p>After both models learned the task, we projected the network activity into a three-dimensional (3D) space spanned by the two largest PCs of the dynamics <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. A third direction, <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, spanned the remaining part of the first output vector <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. The resulting plots, <xref ref-type="fig" rid="fig2">Figure 2B</xref>, corroborate our hypothesis: Small output weights led to aligned dynamics with a large correlation between the largest PCs and the output weights. In contrast, the large output weights of the second network were almost orthogonal, or oblique, to the two leading PCs. Further qualitative differences between the two solutions in terms of the direction of trajectories will be discussed below.</p><p>Another way to quantify these regimes is by the ability to reconstruct the output from the large PCs of neural activity, as quantified by the coefficient of determination <inline-formula><mml:math id="inf33"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. For the aligned network, the projection on the two largest PCs (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, solid) already led to a good reconstruction. For the oblique networks, the two largest PCs were not sufficient. We needed the first eight dimensions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, dashed) to obtain a good reconstruction (<inline-formula><mml:math id="inf34"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>). In contrast to the differences in these fits, the neural dynamics themselves were much more similar between the networks. Specifically, 90% of the variance was explained by four and five dimensions for the aligned and oblique networks, respectively.</p><p>Can we use the output weights to induce aligned or oblique dynamics in more general settings? We trained RNN models with small or large initial output weights on five different neuroscience tasks (section Task details). All weights (input, recurrent, and output) were trained using the Adam algorithm (section Details on RNN models and training). After training, we measured the three quantities of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>: magnitudes of neural activity and output weights, and the correlation between the two. The results in <xref ref-type="fig" rid="fig3">Figure 3A</xref> show that across tasks, initialization with large output weights led to oblique dynamics (small correlation), and with small output weights to aligned dynamics (large correlation). While training could, in principle, change the initially small output weights to large ones (and vice versa), we noticed that this does not happen. Small output weights did increase with training, but the large gap in norms remained. This shows that setting the output weights at initialization can serve to determine their scale after learning under realistic settings. While explaining this observation is beyond the scope of this work, we note that (1) changing the internal weights suffices to solve the task, and (2) the extent to which the output weights change during learning depends on the algorithm and specific parameterization (<xref ref-type="bibr" rid="bib27">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Geiger et al., 2020</xref>; <xref ref-type="bibr" rid="bib86">Yang and Hu, 2020</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The magnitude of output weights determines regimes across multiple neuroscience tasks (<xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib76">Sussillo and Barak, 2013</xref>).</title><p>(<bold>A</bold>) Correlation and norms of output weights and neural activity. For each task, we initialized networks with small or large output weights (dark vs light orange). The initial norms <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> are indicated by the dashed lines. Learning only weakly changes the norm of the output weights. Note that all <italic>y</italic>-axes are logarithmically scaled. (<bold>B</bold>) Variance of <inline-formula><mml:math id="inf36"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> explained and <inline-formula><mml:math id="inf37"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> of reconstructed output for projections of <inline-formula><mml:math id="inf38"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> on increasing number of principal components (PCs). Results from one example network trained on the cycling task are shown for each condition. (<bold>C</bold>) Number of PCs necessary to reach 90% of the variance of <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or of the <inline-formula><mml:math id="inf40"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> of the output reconstruction (top/bottom; dotted lines in <bold>B</bold>). In (<bold>A, C</bold>) violin plots show the distribution over five sample networks, with vertical bars indicating the mean and the extreme values (where visible).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig3-v1.tif"/></fig><p>In <xref ref-type="fig" rid="fig3">Figure 3B and C</xref>, we adopted the perspective of <xref ref-type="fig" rid="fig2">Figure 2C</xref> and quantified how well we can reconstruct the output from a projection of <inline-formula><mml:math id="inf41"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> onto its largest <inline-formula><mml:math id="inf42"><mml:mi>D</mml:mi></mml:math></inline-formula> PCs (section Regression). As expected, both the variance of <inline-formula><mml:math id="inf43"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> explained and the quality of output reconstruction increased, for an increasing number of PCs <inline-formula><mml:math id="inf44"><mml:mi>D</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). How both quantities increased, however, differs between the two regimes. While the variance explained increased similarly in both cases, the quality of the reconstruction increased much more slowly for the model with large output weights. We quantified this phenomenon by comparing the dimensions at which either the variance of <inline-formula><mml:math id="inf45"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> explained or <inline-formula><mml:math id="inf46"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> reaches 90%, denoted by <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, respectively.</p><p>In <xref ref-type="fig" rid="fig3">Figure 3C</xref>, we compare <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> across multiple networks and tasks. Generally, larger output weights led to larger numbers for both. However, for large output weights, the number of PCs necessary to obtain a good reconstruction increased much more drastically than the dimension of the data. Thus, the output was less well represented by the large PCs of the dynamics for networks with large output weights, in agreement with our notion of oblique dynamics.</p><p>Importantly, reaching the aligned and oblique regimes relies on ensuring robust and stable dynamics, which we achieve by adding noise to the dynamics during training. This yields a similar magnitude of neural activity <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> across networks and tasks (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We show in Methods, section Analysis of solutions under noiseless conditions, that learning in simple, noise-free conditions with large output weights can lead to solutions not captured by either aligned or oblique dynamics; those solutions, however, are unstable. Furthermore, we observed that some of the qualitative differences between aligned and oblique dynamics are less pronounced if we initialized networks with small recurrent weights and initially decaying dynamics (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>).</p></sec><sec id="s2-3"><title>Neural dynamics decouple from the output for the oblique regime</title><p>What are the functional consequences of the two regimes? A hint might be seen in an intriguing qualitative difference between the aligned and oblique solutions for the cycling task in <xref ref-type="fig" rid="fig2">Figure 2</xref>. For the aligned network, the two trajectories for the two different contexts (green and purple) are counter-rotating (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, top). This agrees with the output, which also counter-rotates as demanded by the task (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). In contrast, the neural activity of the oblique network <italic>co-rotates</italic> in the leading two PCs (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, bottom). This is despite the counter-rotating output, since this network also solves the task (not shown). The co-rotation also indicates why reconstructing the output from the leading two PCs is not possible (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Naturally, the dynamics also contain counter-rotating trajectories for producing the correct output, but these are only present in low-variance PCs. (Note also that for aligned networks, one can also observe co-rotation in low-variance PCs, see <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>.) Taken together, aligned and oblique dynamics differ in the coupling between leading neural dynamics and output. For aligned dynamics, the two are strongly coupled. For oblique dynamics, the two decouple qualitatively.</p><p>Such a decoupling for oblique, but not aligned, dynamics leads to a prediction regarding the universality of solutions (<xref ref-type="bibr" rid="bib39">Maheswaranathan et al., 2019</xref>; <xref ref-type="bibr" rid="bib80">Turner et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Pagan et al., 2022</xref>). For aligned dynamics, the coupling implies that the internal dynamics are strongly constrained by the task. We thus expect different learners to converge to similar solutions, even if their initial connectivity is random and unstructured. In <xref ref-type="fig" rid="fig4">Figure 4A</xref>, we show the dynamics of three randomly initialized aligned networks trained on the cycling task, projected onto the three leading PCs. Apart from global rotations, the dynamics in the three networks are very similar.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Variability between learners for the two regimes.</title><p>(<bold>A, B</bold>) Examples of networks trained on the cycling task with small (aligned) or large (oblique) output weights. The top left and central networks, respectively, are the same as those plotted in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>C</bold>) Dissimilarity between solutions across different tasks. Aligned dynamics (red) were less dissimilar to each other than oblique ones (yellow). The violin plots show the distribution over all possible different pairs for five samples (mean and extrema as bars).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig4-v1.tif"/></fig><p>For oblique dynamics, the task-defined output exerts weaker constraints on the internal dynamics. Any variability experienced during learning can potentially build up, and eventually create qualitatively different solutions. Three examples of oblique networks solving the cycling tasks indeed show visibly different dynamics (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Further analysis shows that the models also differ in the frequency components in the leading dynamics (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>).</p><p>The degree of variability between learners depends on the task. The observable differences in the PC projections were most striking for the cycling task. For the flip-flop task, for example, solutions were generally noisier in the oblique regime than in the aligned but did not have observable qualitative differences in either regime (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>). We quantified the difference between models for the different neuroscience tasks considered before. To compare different neural dynamics, we used a dissimilarity measure invariant under rotation (section Dissimilarity measure) (<xref ref-type="bibr" rid="bib84">Williams et al., 2021</xref>). The results are shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. Two observations stand out: First, across tasks, the dissimilarity was higher for networks in the oblique regime than for those in the aligned. Second, both overall dissimilarity and the discrepancy between regimes differed strongly between tasks. The largest dissimilarity (for oblique dynamics) and the largest discrepancy between regimes was found for the cycling. The smallest discrepancy between regimes was found for the flip-flop task. Such a difference between tasks is consistent with the differences in the range of possible solutions for different tasks, as reported in <xref ref-type="bibr" rid="bib80">Turner et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Maheswaranathan et al., 2019</xref>.</p><p>What are the underlying mechanisms for the qualitative decoupling in oblique, but not aligned networks? For aligned dynamics, we saw that the small output weights demand large activity to generate the output. In other words, the activity along the largest PCs must be coupled to the output. For oblique dynamics, this constraint is not present, which opens the possibility for small components outside the largest PCs to generate the output. If this is the case, we have a decoupling, such as the observed co-rotation in the cycling task, and the possible variability between solutions. We discuss this point in more detail in the Methods, section Mechanisms behind decoupling of neural dynamics and output.</p><p>In the following two sections, we will explore how the decoupling between neural dynamics and output for oblique, but not aligned, dynamics influences the response to perturbations and the effects of noise during learning.</p></sec><sec id="s2-4"><title>Differences in response to perturbations</title><p>Understanding how networks respond to external perturbations and internal noise requires some insight into how dynamics are generated. Dynamics of trained networks are mostly generated internally, through recurrent interactions. In robust networks, these internally generated dynamics are a prominent part of the largest PCs (among input-driven components; section Analysis of solutions under noiseless conditions and Oblique solutions arise for noisy, nonlinear systems). Internally generated dynamics are sustained by positive feedback loops, through which neurons excite each other. Those loops are low-dimensional, with activity along a few directions of the dynamics being amplified and fed back along the same directions. This results in dynamics being driven by effective feedback loops along the largest PCs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As shown above, the largest PCs can either be aligned or not aligned, with the output weights. This leads to predictions about how aligned and oblique networks differentiate in their responses to perturbations along different directions.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Perturbations differentially affect dynamics in the aligned and oblique regimes.</title><p>(<bold>A</bold>) Cartoon illustrating the relationship between perturbations along output weights or principal components (PCs) and the feedback loops driving autonomous dynamics. (<bold>B</bold>) Output after perturbation for aligned (top) and oblique (bottom) networks trained on the cycling task. The unperturbed network (light red line) yields a sine wave along the first output direction <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. At <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, a perturbation with amplitude <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>34</mml:mn></mml:mrow></mml:math></inline-formula> is applied along the output weights (dashed red) or the first PC (dashed-dotted blue). The perturbations only differ in the directions applied. While the immediate response for the oblique network to a perturbation along the output weights is much larger, <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula>, the long-term dynamics yield the same output as the unperturbed network. See also <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref> for more details. (<bold>C</bold>) Loss for perturbations of different amplitudes for the two networks in (<bold>B</bold>). Lines and shades are means and standard deviations over different perturbation times <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>5</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>15</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and random directions spanned by the output weights (red) or the two largest PCs (blue). The loss is the mean squared error between output and target for <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The gray dot indicates an example in (<bold>B</bold>). (<bold>D</bold>) Relative susceptibility of networks to perturbation directions for different tasks and dynamical regimes. We measured the area under the curve (AUC) of loss over perturbation amplitude for perturbations along the output weights of the two largest PCs. The relative susceptibility is the ratio between the two AUCs. The example in (<bold>C</bold>) is indicated by gray triangles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig5-v1.tif"/></fig><p>Our intuition about feedback loops suggests that networks respond strongly to a perturbation that is aligned with the directions contributing to the feedback loop, but weakly to a perturbation that is orthogonal to them. In particular, if a perturbation is applied along the output weights, aligned and oblique dynamics should dissociate, with a strong disruption of dynamics for aligned, but not for oblique dynamics (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><p>To test this, we compare the response to perturbations along the output direction and the largest PCs. We apply perturbations to the neural activity at a single point in time: <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> evolves undisturbed until time <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>. At that point, it is shifted to <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>. After the perturbation, we let the network evolve freely and compare this evolution to that of an unperturbed copy. Such a perturbation mimics a very short optogenetic perturbation applied to a selected neural population (<xref ref-type="bibr" rid="bib48">O’Shea et al., 2022</xref>; <xref ref-type="bibr" rid="bib14">Finkelstein et al., 2021</xref>). In <xref ref-type="fig" rid="fig5">Figure 5B</xref>, we show the output after such perturbations for an aligned (top) and an oblique network (bottom) trained on the cycling task. The time point and amplitude are the same for both directions and networks. For each network and type of perturbation, there is an immediate deflection and a long-term response. For both networks, perturbing along the PCs (blue) leads to a long-term phase shift. Only in the aligned network, however, perturbation along the output direction (red) leads to a visible long-term response. In the oblique network, the amplitude of the immediate response is larger, but the long-term response is <italic>smaller</italic>. Our results for the oblique network, but not for the aligned, agree with simulations of networks generating EMG data from the cycling experiments (<xref ref-type="bibr" rid="bib65">Saxena et al., 2022</xref>).</p><p>To quantify the relative long-term susceptibility of networks to perturbations along output weights or PCs, we sampled from different times <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> and different directions in the 2D subspaces spanned either by the two output vectors or by the two largest PCs. For each perturbation, we measured the loss of the perturbed networks on the original task (excluding the immediate deflection after the perturbation by starting to compute the loss at <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows that the aligned network is almost equally susceptible to perturbations along the PCs and the output weights. In contrast, the oblique network is much more susceptible to perturbations along the PCs.</p><p>We repeated this analysis for oblique and aligned networks trained on the five different tasks. We computed the area under the curve (AUC) for both loss profiles in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. We then defined the ‘relative susceptibility’ as the ratio <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mrow><mml:mtext/><mml:mi>AUC</mml:mi></mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:msub><mml:mi>/</mml:mi><mml:msub><mml:mrow><mml:mtext/><mml:mi>AUC</mml:mi></mml:mrow><mml:mrow><mml:mtext/><mml:mi>PC</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig5">Figure 5D</xref>. For aligned networks (red), the relative susceptibility was close to 1 indicating similarly strong responses to both types of perturbations. For oblique networks (yellow), it was much smaller than 1, indicating that long-term responses to perturbations along the output direction were weaker than those to perturbations along the PCs.</p></sec><sec id="s2-5"><title>Noise suppression for oblique dynamics</title><p>In the oblique regime, the output weights are large. To produce the correct output (and not a too large one), the large PCs of the dynamics are almost orthogonal to the output weights. The large output weights, however, pose a robustness problem: Small noise in the direction of the output weights is also amplified at the level of the readout. We show that learning leads to a slow process of sculpting noise statistics to avoid this effect (Figure 11). Specifically, a negative feedback loop is generated that suppresses fluctuations along the output direction (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, Figure 10; <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>). Because the positive feedback loop that gives rise to the large PCs is mostly orthogonal to the output direction, it remains unaffected by this additional negative feedback loop. A detailed analysis of how learning is affected by noise shows that, for large output weights, the network first learns a solution that is not robust to noise. This solution is then transformed to increasingly stable and oblique dynamics over longer time scales (section Learning with noise for linear RNNs and Oblique solutions arise for noisy, nonlinear systems).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Noise suppression along the output direction in the oblique regime.</title><p>(<bold>A</bold>) A cartoon of the feedback loop structure for aligned (top) and oblique (bottom) dynamics. The latter develops a negative feedback loop which suppresses fluctuations along the output direction. (<bold>B</bold>) Comparing the distribution of variance of mean-subtracted activity along different directions for networks trained on the cycling task (see <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>): principal components (PCs) of trial-averaged activity (blue), readout (red), and random (gray) directions. For the PCs and output weights, we sampled 100 normalized combinations of either the first two PCs or the two output vectors. For the random directions, we drew 1000 random vectors in the full, <inline-formula><mml:math id="inf64"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional space. (<bold>C</bold>) Noise compression across tasks as measured by the ratio between variance along output and random directions. The dashed line indicates neither compression nor expansion. Black markers indicate the values for the two examples in (<bold>B, C</bold>). Note the log-scales in (<bold>B, C</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig6-v1.tif"/></fig><p>To illustrate the effect of the negative feedback loop, we consider the fluctuations around trial averages. We take a collection of states <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and then subtract the task-conditioned averages <inline-formula><mml:math id="inf66"><mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to compute <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We then project <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> onto three different direction categories: the largest PCs of the averaged data <inline-formula><mml:math id="inf69"><mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, the output directions, or randomly drawn directions.</p><p>How strongly the activity fluctuates along each direction is quantified by the variance of the projections (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). For both aligned and oblique dynamics, the variance is much larger along the PCs than along random directions. This is not necessarily expected, because the PCA was performed on the <italic>averaged</italic> activity, without the fluctuations. Instead, it is a dynamical effect: the same positive feedback that generates the autonomous dynamics also amplifies the noise (section Oblique solutions arise for noisy, nonlinear systems).</p><p>The two network regimes, however, dissociate when considering the variance along the output direction. For aligned dynamics, there is no negative feedback loop, and <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is correlated with the PCs. The variance along the output direction is hence similar to that along the PCs, and larger than along random directions. For oblique dynamics, the negative feedback loop suppresses the fluctuations along the output direction, so that they become weaker than along random directions.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, we quantify this dissociation across different tasks. We measured the ratio between variance along output and random directions. Aligned networks have a ratio much larger than one, indicating that the fluctuations along the output direction are increased due to the autonomous dynamics along the PCs. In contrast, oblique networks have a ratio smaller than 1 for all tasks, which indicates noise compression along the output.</p></sec><sec id="s2-6"><title>Different degrees of alignment in experimental settings</title><p>For the cycling task, we observed that dynamics were qualitatively different for the two regimes, with trajectories either counter- or co-rotating (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Interestingly, the experimental results of <xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>, matched the oblique, but not the aligned dynamics. The authors observed co-rotating dynamics in the leading PCs of motor cortex activity despite counter-rotating activity of simultaneously recorded muscle activity. Here, we test whether our theory can help to more clearly and quantitatively distinguish between the two regimes in experimental settings.</p><p>In typical experimental settings, we do not have direct access to output weights. We can, however, approximate these by fitting neural data to simultaneously recorded behavioral output, such as hand velocity in a motor control experiment (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, top). Following the model above, where the output is a weighted average of the states, we reconstruct the output from the neural activity with linear regression. To quantify the dynamical regime, we then compute the correlation <inline-formula><mml:math id="inf71"><mml:mi>ρ</mml:mi></mml:math></inline-formula> between the weights from fitting and the neural data. Additionally, we can also quantify the alignment by the ‘relative fitting dimension’ <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the number of PCs necessary to recover the output and <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> number to the number of PCs necessary to represent 90% of the variance of the neural data. We computed both the correlation and the relative fitting dimension for different publicly available data sets (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). For details, see section Experimental data.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Quantifying aligned and oblique dynamics in experimental data (<xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Pei et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Hennig et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Degenhart et al., 2020</xref>).</title><p>(<bold>A</bold>) Diagram of the two types of experimental data considered. Here, we always took the velocity as the output (hand, finger, or cursor). In motor control experiments (top), we first needed to obtain the output weights <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> via linear regression. We then computed the correlation <inline-formula><mml:math id="inf76"><mml:mi>ρ</mml:mi></mml:math></inline-formula> and the reconstruction dimension <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, i.e., the number of principal components (PCs) of <inline-formula><mml:math id="inf78"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> necessary to obtain a coefficient of determination <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>%. In brain-computer interface (BCI) experiments (bottom), the output (cursor velocity) is generated from neural activity <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> via output weights <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> defined by the experimenter. This allowed us to directly compute correlations and fitting dimensions. (<bold>B</bold>) Correlation <inline-formula><mml:math id="inf82"><mml:mi>ρ</mml:mi></mml:math></inline-formula> (top) and relative fitting dimension <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (bottom) for several publicly available data sets. The cycling task data (purple) were trial-conditioned averages, the BCI experiments (red) and Neural Latents Benchmark (NLB) tasks (yellow) single-trial data. Results for the full data sets are shown as dots. Violin plots indicate results for 20 random subsets of 25% of the data points in each data set (bars indicate mean). See small text below <italic>x</italic>-axis for the number of time points and neurons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig7-v1.tif"/></fig><p>We started with data sets from two monkeys performing the cycling task (<xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>). The data contained motor cortex activity, hand movement, and EMG from the arms, all averaged over multiple trials of the same condition. In <xref ref-type="fig" rid="fig7">Figure 7B</xref>, we show results for reconstructing the hand velocity. The correlation was small, <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.04</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.07</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. To obtain a good reconstruction, we needed a substantial fraction of the dimension of the neural data: The relative fitting dimension was <inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.7</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.8</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Our results agree with previous studies, showing that the best decoding directions are only weakly correlated with the leading PCs of motor cortex activity (<xref ref-type="bibr" rid="bib66">Schroeder et al., 2022</xref>).</p><p>We also analyzed data made available through the Neural Latents Benchmark (NLB) (<xref ref-type="bibr" rid="bib52">Pei et al., 2021</xref>). In two different tasks, monkeys needed to perform movements along a screen. In a random target task, the monkeys had to point at a randomly generated target position on a screen, with a successive target point generated once the previous one was reached (<xref ref-type="bibr" rid="bib40">Makin et al., 2018</xref>). In a maze task, the monkeys were trained to follow a trajectory through a maze with their hand (<xref ref-type="bibr" rid="bib10">Churchland et al., 2010</xref>). In both cases, we reconstructed the finger or hand velocity from neural activity on single trials. The correlation was higher than in the cycling task, <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.13</mml:mn></mml:mrow></mml:math></inline-formula>. The relative fitting dimension was lower than in the trial-averaged cycling data, albeit still on the same order: <inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>fit</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.4</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.5</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>Finally, we considered brain-computer interface (BCI) experiments (<xref ref-type="bibr" rid="bib63">Sadtler et al., 2014</xref>). In these experiments, monkeys were trained to control a cursor on a screen via activity read out from their motor cortex (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, bottom). The output weights generating the cursor velocity were set by the experimenter (so we don’t need to fit). Importantly, the output weights were typically chosen to be spanned by the largest PCs (the ‘neural manifold’), suggesting aligned dynamics. For three example data sets (<xref ref-type="bibr" rid="bib22">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Hennig et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Degenhart et al., 2020</xref>), we obtained higher correlation values, <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.17</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.23</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The relative fitting dimension was much smaller than for the non-BCI data sets, especially for the two largest data sets, where <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.03</mml:mn><mml:mo>,</mml:mo><mml:mn>0.06</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The higher correlation and much smaller relative fitting dimension suggest that, indeed, the neural dynamics arising in BCI experiments are more aligned, and those in non-BCI settings are more oblique. These trends also hold when decoding other behavioral outputs for the cycling task and the NLB tasks (position, acceleration, or EMG), even if the ability to decode and the numerical values for correlation and fitting dimension may fluctuate considerably (<xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>). Thus, while we do not observe strongly different regimes as in the simulations, we do see an ordering between different data sets according to the alignment between outputs and neural dynamics. It would be interesting to test the differences between BCI and non-BCI data on larger data sets, and different experiments with different dimensions of neural data (<xref ref-type="bibr" rid="bib20">Gao et al., 2017</xref>; <xref ref-type="bibr" rid="bib72">Stringer et al., 2019a</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We analyzed the relationship between neural dynamics and behavior, asking to which extent a network’s output is represented in its dynamics. We identified two different limiting regimes: aligned dynamics, in which the dominant activity in a network is related to its output, and oblique dynamics, where the output is only a small modulation on top of the dominating dynamics. We demonstrated that these two regimes have different functional implications. We also examined how they arise through learning, and how they relate to experimental findings.</p><p>Linking neural activity to external variables is one of the core challenges of neuroscience (<xref ref-type="bibr" rid="bib26">Hubel and Wiesel, 1962</xref>). In most cases, however, such links are far from perfect. The activity of single neurons can be related in a nonlinear, mixed manner, to task variables (<xref ref-type="bibr" rid="bib56">Rigotti et al., 2013</xref>). Even when considering populations of neurons, a large fraction of neural activity is not easily accounted for by external variables (<xref ref-type="bibr" rid="bib1">Arieli et al., 1996</xref>). Various explanations have been proposed for this disconnect. In the visual cortex, activity has been shown to be related to ‘irrelevant’ external variables, such as body movements (<xref ref-type="bibr" rid="bib73">Stringer et al., 2019b</xref>). Follow-up work showed that, in primates, some of these effects can be explained by the induced changes on retinal images (<xref ref-type="bibr" rid="bib79">Talluri et al., 2022</xref>), but this study still explained only half of the neural variability. An alternative explanation hinges on the redundancy of the neural code, which allows ‘null spaces’ in which activity can visit without affecting behavior (<xref ref-type="bibr" rid="bib58">Rokni et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Kao et al., 2021</xref>). Through the oblique regime, our study offers a simple explanation for this phenomenon: in the presence of large output weights, resistance to noise or perturbations requires large, potentially task-unrelated neural dynamics. Conversely, generating task-related output in the presence of large, task-unrelated dynamics requires large readout weights.</p><p>We showed theoretically and in simulations that, when training RNNs, the magnitude of output weights is a central parameter that controls which regime is reached. This finding is vital for the use of RNNs as hypothesis generators (<xref ref-type="bibr" rid="bib77">Sussillo, 2014</xref>; <xref ref-type="bibr" rid="bib3">Barak, 2017</xref>; <xref ref-type="bibr" rid="bib81">Vyas et al., 2020</xref>), where it is often implicitly assumed that training results in universal solutions (<xref ref-type="bibr" rid="bib39">Maheswaranathan et al., 2019</xref>) (even though biases in the distribution of solutions have been discussed; <xref ref-type="bibr" rid="bib78">Sussillo et al., 2015</xref>). Here, we show that a specific control knob allows one to move between qualitatively different solutions of the same task, thereby expanding the control over the hypothesis space (<xref ref-type="bibr" rid="bib80">Turner et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Pagan et al., 2022</xref>). Note in particular that the default initialization in standard learning frameworks has large output weights, which results in oblique dynamics (or unstable solutions if training without noise, see Methods, section Analysis of solutions under noiseless conditions).</p><p>The role of the magnitude of output weights is also discussed in machine learning settings, where different learning regimes have been found (<xref ref-type="bibr" rid="bib27">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Chizat et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Mei et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Jacot et al., 2022</xref>). In particular, ‘lazy’ solutions were observed for large output weights in feedforward networks. We show in Methods, section Analysis of solutions under noiseless conditions, that these are unstable for recurrent networks and are replaced in a second phase of learning by oblique solutions. This second, slower, phase is reminiscent of implicit regularization in overparameterized networks (<xref ref-type="bibr" rid="bib55">Ratzon et al., 2024</xref>; <xref ref-type="bibr" rid="bib5">Blanc et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Li et al., 2021</xref>; <xref ref-type="bibr" rid="bib87">Yang et al., 2023</xref>). On a broader scale, which learning regime is relevant when modeling biological learning is an open question that has only just begun to be explored (<xref ref-type="bibr" rid="bib15">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib37">Liu et al., 2023</xref>).</p><p>The particular control knob we studied has an analog in the biological circuit – the synaptic weights. We can thus use experimental data to study whether the brain might rely on oblique or aligned dynamics. Existing experimental work has partially addressed this question. In particular, the work by <xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>, has been a major inspiration for our study. Our results share some of the key findings from that paper – the importance of stability leading to ‘untangled’ dynamics (<xref ref-type="bibr" rid="bib74">Susman et al., 2021</xref>) and a dissociation between hidden dynamics and output. In addition, we suggest a specific mechanism to reach oblique dynamics – training networks with large output weights. Furthermore, we characterize the aligned and oblique regimes along experimentally accessible axes.</p><p>We see three avenues for exploring our results experimentally. First, simultaneous measurements of neural dynamics and muscle activity could be used to quantify noise along the output direction. This would allow checking whether noise is compressed in this direction, and in particular, whether such compression occurs on a slow time scale after initial task acquisition. We suggest how to test this in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. Second, we show how the dynamical regimes dissociate under perturbations along specific directions. Experiments along these lines have recently become possible (<xref ref-type="bibr" rid="bib60">Russell et al., 2022</xref>; <xref ref-type="bibr" rid="bib14">Finkelstein et al., 2021</xref>; <xref ref-type="bibr" rid="bib8">Chettih and Harvey, 2019</xref>). Future work is left to combine our model with biological constraints that induce additional effects during perturbations, e.g., through non-normal synaptic connectivity (<xref ref-type="bibr" rid="bib48">O’Shea et al., 2022</xref>; <xref ref-type="bibr" rid="bib32">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="bib6">Bondanelli and Ostojic, 2020</xref>; <xref ref-type="bibr" rid="bib38">Logiaco et al., 2021</xref>). Third, our work connects to the setting of BCI, where the experimenter chooses the output weights at the beginning of learning (<xref ref-type="bibr" rid="bib63">Sadtler et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Willett et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Rajeswaran et al., 2024</xref>). Typically, the output weights are set to lie ‘within the manifold’ of the leading PCs so that we expect aligned dynamics (<xref ref-type="bibr" rid="bib63">Sadtler et al., 2014</xref>). In experiments where the output weights were rotated out of the manifold (without changing the norm), learning took longer and led to a rotation of the manifold, i.e., at least a partial alignment (<xref ref-type="bibr" rid="bib45">Oby et al., 2019</xref>). Our theory suggests directly comparing the degree of alignment between dynamics obtained from within- and out-of-manifold initializations. Furthermore, it would be interesting to systematically change the norm of the output weights (in particular for out-of-manifold initializations) to see whether larger output weights lead to more oblique solutions. If this is the case, we suggest testing whether such more oblique solutions meet our predictions, e.g., higher variability between individuals and noise suppression.</p><p>Overall, our results provide an explanation for the plethora of relationships between neural activity and external variables. It will be interesting to see whether future studies will find hallmarks of either regime for different experiments, tasks, or brain regions.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Details on RNN models and training</title><p>We consider rate-based RNNs with <inline-formula><mml:math id="inf90"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons. The states <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are governed by<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf92"><mml:mi>W</mml:mi></mml:math></inline-formula> is a recurrent weight matrix and <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>tanh</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> a nonlinearity applied element-wise. The network receives a low-dimensional input <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>𝐬</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula> via input weights <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. It is also driven by white, isotropic noise with zero mean and covariance <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The initial states <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are drawn from a centered normal distribution with variance <inline-formula><mml:math id="inf98"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>init</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> at each trial. This serves as additional noise. The output is a low-dimensional, linear projection of the states: <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>𝐳</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf100"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mo>…</mml:mo><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>The initial output weights are drawn from centered normal distributions with variance <inline-formula><mml:math id="inf101"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. ‘Small’ output weights refer to <inline-formula><mml:math id="inf102"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, and ‘large’ ones to <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We have <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> at initialization. Note that large initial output weights are the current default in standard learning environments (<xref ref-type="bibr" rid="bib50">Paszke et al., 2017</xref>; <xref ref-type="bibr" rid="bib86">Yang and Hu, 2020</xref>). The recurrent weights were initialized from centered normal distributions with variance <inline-formula><mml:math id="inf105"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. We chose <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula> so that dynamics were chaotic before learning (<xref ref-type="bibr" rid="bib71">Sompolinsky et al., 1988</xref>).</p><p>To simulate the noisy RNN dynamics numerically, we used the Euler-Maruyama method (<xref ref-type="bibr" rid="bib34">Kloeden and Platen, 1992</xref>) with a time step of <inline-formula><mml:math id="inf107"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. We used the Adam algorithm (<xref ref-type="bibr" rid="bib33">Kingma and Ba, 2014</xref>) implemented in PyTorch (<xref ref-type="bibr" rid="bib50">Paszke et al., 2017</xref>). Apart from the learning rate, we kept the parameters for Adam at the default (some filtering, no weight decay). We selected learning rates and the number of training steps such that learning was relatively smooth and converged sufficiently within the given number of trials. Learning rates were set to <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. Details for all simulation parameters can be found in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Task, simulation, and network parameters for <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig6">6</xref>.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">Symbol</th><th align="left" valign="bottom">Cycling</th><th align="left" valign="bottom">Flip-flop</th><th align="left" valign="bottom">Mante</th><th align="left" valign="bottom">Romo</th><th align="left" valign="bottom">Complex sine</th></tr></thead><tbody><tr><td align="left" valign="bottom"># inputs</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom"># outputs</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Trial duration</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf111"><mml:mi>T</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">72</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">48</td><td align="left" valign="bottom">29</td><td align="left" valign="bottom">50</td></tr><tr><td align="left" valign="bottom">Fixation duration</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf112"><mml:msub><mml:mi>t</mml:mi><mml:mi>fix</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">3</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1.3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Stimulus duration</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf115"><mml:msub><mml:mi>t</mml:mi><mml:mi>stim</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">50</td></tr><tr><td align="left" valign="bottom">Stimulus delay</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf116"><mml:msub><mml:mi>t</mml:mi><mml:mi>sd</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">–</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">–</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>12</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Decision delay</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf119"><mml:msub><mml:mi>t</mml:mi><mml:mi>delay</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Decision duration</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf120"><mml:msub><mml:mi>t</mml:mi><mml:mi>dec</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">71</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf121"><mml:msub><mml:mi>t</mml:mi><mml:mi>sd</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">50</td></tr><tr><td align="left" valign="bottom">Simulation time step</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf122"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="5">– 0.2 –</td></tr><tr><td align="left" valign="bottom">Target time step</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf123"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>target</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">0.2</td></tr><tr><td align="left" valign="bottom">Activation noise</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf124"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.05</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td></tr><tr><td align="left" valign="bottom">Initial state noise</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf125"><mml:msub><mml:mi>σ</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="5">– 1.0 –</td></tr><tr><td align="left" valign="bottom">Network size</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf126"><mml:mi>N</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom" colspan="5">– 512 –</td></tr><tr><td align="left" valign="bottom"># training epochs</td><td align="left" valign="bottom"/><td align="left" valign="bottom">1000</td><td align="left" valign="bottom">4000</td><td align="left" valign="bottom">4000</td><td align="left" valign="bottom">6000</td><td align="left" valign="bottom">6000</td></tr><tr><td align="left" valign="bottom">Learning rate aligned</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf127"><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">0.005</td><td align="left" valign="bottom">0.002</td><td align="left" valign="bottom">0.005</td><td align="left" valign="bottom">0.005</td></tr><tr><td align="left" valign="bottom">Learning rate oblique</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf128"><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">0.005</td></tr><tr><td align="left" valign="bottom">Batch size</td><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="5">– 32 –</td></tr></tbody></table></table-wrap><p>For the comparisons over different tasks (<xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig6">6</xref>), we trained five networks for each task. All weights (<inline-formula><mml:math id="inf129"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:mi>W</mml:mi><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) were adapted. For the example networks trained on the cycling task (<xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig5">5</xref>, and <xref ref-type="fig" rid="fig6">6</xref>), we used networks with <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> neurons and only changed the recurrent weights <inline-formula><mml:math id="inf131"><mml:mi>W</mml:mi></mml:math></inline-formula>. We also trained for longer (5000 training steps) and with a higher learning rate (<inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-2"><title>Task details</title><p>The tasks the networks were trained on are taken from the neuroscience literature: a cycling task (<xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>), a 3-bit flip-flop task, and a ‘complex sine’ task (with input-dependent frequencies) (<xref ref-type="bibr" rid="bib76">Sussillo and Barak, 2013</xref>), a context-dependent decision-making task (‘Mante’) (<xref ref-type="bibr" rid="bib41">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>), and a working memory task comparing the amplitudes of two pulse stimuli (‘Romo’) (<xref ref-type="bibr" rid="bib59">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>). All tasks have similar structure (<xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>): A trial of length <inline-formula><mml:math id="inf133"><mml:mi>T</mml:mi></mml:math></inline-formula> starts with a fixation period (length <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext/><mml:mi>fix</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). This is followed by an input for <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext/><mml:mi>stim</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. For the cycling and flip-flop task, the inputs are pulses of amplitude 1; else see below. After a delay <inline-formula><mml:math id="inf136"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext/><mml:mi>delay</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the output of the network is required to reach an input-dependent value during a time period <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext/><mml:mi>dec</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. During this decision period, we set target points <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> every <inline-formula><mml:math id="inf139"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext/><mml:mi>target</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> time steps. The loss was defined as the mean squared error between network output and target at these time points. Below, we provide further details for each task.</p><sec id="s4-2-1"><title>Cycling task</title><p>The network receives an initial pulse, whose direction (<inline-formula><mml:math id="inf140"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf141"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>) determines the sense of direction of the target. The target is given by a rotation in 2D, <inline-formula><mml:math id="inf142"><mml:mrow><mml:mover><mml:mi>𝐳</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mrow><mml:mspace width="0.1667em"/><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, with frequency <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for the two directions (clockwise or anticlockwise).</p></sec><sec id="s4-2-2"><title>Flip-flop task</title><p>The network repeatedly receives input pulses along one of three directions, followed by decision periods. In each decision period, the output coordinate corresponding to the last input should reach ±1 depending on the sign of the input. All other coordinates should remain at ±1 as defined by the last time they were triggered. To make sure that this is well defined, we trigger all inputs at time steps <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> with random signs.</p></sec><sec id="s4-2-3"><title>Mante task</title><p>Input channels for this task are split into two groups of size <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>: half of the channels for the signal and the other half for the context, indicating which of the signal channels is relevant. All signal channels <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> deliver a constant mean <inline-formula><mml:math id="inf149"><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> plus additional white noise: <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The mean is drawn uniformly from <inline-formula><mml:math id="inf151"><mml:mrow><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo separator="true">,</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo separator="true">,</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, and the noise amplitude is <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>. For simulations, we draw a standard normal variable <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> at time step <inline-formula><mml:math id="inf154"><mml:mi>k</mml:mi></mml:math></inline-formula>, and set <inline-formula><mml:math id="inf155"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:msqrt><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. Only a single contextual input is active at each trial, <inline-formula><mml:math id="inf156"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf157"><mml:mi>j</mml:mi></mml:math></inline-formula> chosen uniformly from the number of context <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The target during the decision period is the sign of the relevant input, <inline-formula><mml:math id="inf159"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>sign</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2-4"><title>Romo task</title><p>For the Romo task, the input consists of two input pulses separated by random delays <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext/><mml:mi>sd</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The amplitude of the inputs is drawn independently from <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1.5</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with the condition of being at least 0.2 apart (else both are redrawn). During the decision period, the network needs to yield <inline-formula><mml:math id="inf162"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, depending on which of the two pulses was larger.</p></sec><sec id="s4-2-5"><title>Complex sine</title><p>The target is <inline-formula><mml:math id="inf163"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with frequency <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mspace width="0.1667em"/><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mspace width="0.1667em"/><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and boundaries <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mspace width="0.1667em"/><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, and where <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The input is a constant input of amplitude <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="s4-3"><title>Generalized correlation</title><p>For <xref ref-type="fig" rid="fig3">Figure 3A</xref>, we used a generalized correlation measure which allows for multiple output dimensions, multiple time points, and noisy data. Consider neural activity of <inline-formula><mml:math id="inf168"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons at <inline-formula><mml:math id="inf169"><mml:mi>P</mml:mi></mml:math></inline-formula> time points stacked into the matrix <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mo>…</mml:mo><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We assume the states to be centered in time, <inline-formula><mml:math id="inf171"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. The corresponding <inline-formula><mml:math id="inf173"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional output is summarized in the <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> matrix<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with weights <inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We define the generalized correlation as<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>X</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The norm is the Frobenius norm, <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>X</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. In particular, we have<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Z</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>X</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The case of 1D output and a single time step discussed in the main text, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, is recovered up to the sign, which we discard. Note that in that case, the vectors <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> should be centered along coordinates to receive a valid correlation. Our numerical results did not change qualitatively when centering across coordinates only or both coordinates and time.</p><p>For trajectories with multiple conditions, we stack these instances in a matrix <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> the number of conditions, and <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> the number of time points per trajectory. For noisy trajectories, we first average over multiple instances per condition and time point to obtain a similar matrix <inline-formula><mml:math id="inf182"><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula>.</p></sec><sec id="s4-4"><title>Regression</title><p>In <xref ref-type="fig" rid="fig3">Figure 3B and C</xref> we computed the number of PCs necessary to either represent the dynamics or fit the output. We simulated the trained networks again on their corresponding tasks. We did not apply noise during these simulations, since keeping the same noise as during training would reduce the quality of the output for large output weights; trial averaging yielded similar results to the ones obtained without noise (not shown).</p><p>The simulations yielded neural states <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and outputs <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:msup><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf185"><mml:mi>P</mml:mi></mml:math></inline-formula> is the number of data points (batch size times number of time points <inline-formula><mml:math id="inf186"><mml:mi>T</mml:mi></mml:math></inline-formula>). We applied PCA to the states <inline-formula><mml:math id="inf187"><mml:mi>X</mml:mi></mml:math></inline-formula>. The cumulative explained variance ratio obtained from PCA is plotted in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. We then projected <inline-formula><mml:math id="inf188"><mml:mi>X</mml:mi></mml:math></inline-formula> onto the first <inline-formula><mml:math id="inf189"><mml:mi>k</mml:mi></mml:math></inline-formula> PCs and fitted these projections to the output with ridge regression (cross-validated, using scikit-learn’s RidgeCV <xref ref-type="bibr" rid="bib51">Pedregosa et al., 2011</xref>).</p></sec><sec id="s4-5"><title>Dissimilarity measure</title><p>For measuring the dissimilarity between learners in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we apply a measure following <xref ref-type="bibr" rid="bib84">Williams et al., 2021</xref>. We define the distance between two sets with <inline-formula><mml:math id="inf190"><mml:mi>P</mml:mi></mml:math></inline-formula> data points <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>X</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the hat corresponds to centering along the rows, and <inline-formula><mml:math id="inf192"><mml:mi>Q</mml:mi></mml:math></inline-formula> is an orthogonal matrix. The solution to this so-called orthogonal Procrustes’ problem is found via the singular value decomposition <inline-formula><mml:math id="inf193"><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msup><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The optimal transformation is <inline-formula><mml:math id="inf194"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mo lspace="0em" rspace="0em">*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and the numerator in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> is then <inline-formula><mml:math id="inf195"><mml:mrow><mml:mrow><mml:mtext/><mml:mi>Tr</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msup><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Q</mml:mi><mml:mo lspace="0em" rspace="0em">*</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>Tr</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>Note that this is more restricted than canonical correlation analysis (CCA), which is also commonly used (<xref ref-type="bibr" rid="bib18">Gallego et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Gallego et al., 2020</xref>). In particular, CCA involves whitening the matrices <inline-formula><mml:math id="inf196"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf197"><mml:mi>Y</mml:mi></mml:math></inline-formula> before applying a rotation (<xref ref-type="bibr" rid="bib84">Williams et al., 2021</xref>). This sets all singular values to 1. For originally low-D data, this mostly means amplifying the noise, unless the data was previously projected onto a small number of PCs. In the latter case, the procedure still removes the information about how much each PC contributes.</p></sec><sec id="s4-6"><title>Experimental data</title><p>We detail the analyses of neural data in section Different degrees of alignment in experimental settings. We made use of publicly available data sets: data from the cycling task of <xref ref-type="bibr" rid="bib61">Russo et al., 2018</xref>, two data sets available through the NLB (<xref ref-type="bibr" rid="bib52">Pei et al., 2021</xref>), and data from monkeys trained on a center-out reaching task with a BCI (<xref ref-type="bibr" rid="bib22">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Hennig et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Degenhart et al., 2020</xref>). For all data sets, we first obtain firing rates <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf199"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of measured neurons, and <inline-formula><mml:math id="inf200"><mml:mi>T</mml:mi></mml:math></inline-formula> the number of data points, (see <xref ref-type="fig" rid="fig7">Figure 7B</xref> for these numbers). We also collect the simultaneously measured behavior in the matrix <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig7">Figure 7</xref>, we only analyzed cursor or hand velocity for behavior, so that the output dimension is <inline-formula><mml:math id="inf202"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. See <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref> for similar results for hand position and acceleration or the largest two PCs of the EMG data recorded for the cycling task.</p><p>For the cycling task, the firing rates were binned in 1 ms bins and convolved with a 25 ms Gaussian filter. The mean firing rate was 22 and 18 Hz for the two monkeys, respectively. For the NLB data, spikes came in 1 ms bins. We binned data to 45 ms bins and applied a Gaussian filter with 45 ms width. This increased the quality of the fit, as firing rates were much lower (mean of 5 Hz for both) than in the cycling data set. For the BCI experiments, firing rates came as spike counts in 45 ms bins. The mean firing rate was (45, 45, 55) Hz for the data of <xref ref-type="bibr" rid="bib22">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Hennig et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Degenhart et al., 2020</xref>, respectively. In agreement with the original BCI experiments, we did not apply a filter to the neural data.</p><p>For fitting, we centered both firing rates <inline-formula><mml:math id="inf203"><mml:mi>X</mml:mi></mml:math></inline-formula> and output <inline-formula><mml:math id="inf204"><mml:mi>Z</mml:mi></mml:math></inline-formula> across time (but not coordinates). We also added a delay of 100 ms between firing rates and output for the cycling and NLB data sets, which increased the quality of the fits. We then fitted the output <inline-formula><mml:math id="inf205"><mml:mi>Z</mml:mi></mml:math></inline-formula> to the firing rates <inline-formula><mml:math id="inf206"><mml:mi>X</mml:mi></mml:math></inline-formula> with ridge regression, with regularization obtained from cross-validation. We treated the coefficients as output weights <inline-formula><mml:math id="inf207"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The trial average data of the cycling tasks was very well fitted for both monkeys, <inline-formula><mml:math id="inf208"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.97</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.98</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. For the NLB tasks with single-trial data, the fits were not as good, <inline-formula><mml:math id="inf209"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0.73</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0.69</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. For two of the BCI data sets (<xref ref-type="bibr" rid="bib22">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Hennig et al., 2018</xref>), the output weights were also given, and we checked that the fit recovers these. For the third BCI data set (<xref ref-type="bibr" rid="bib12">Degenhart et al., 2020</xref>), we did not have access to the output weights, and only access to the cursor velocity after Kalman filtering. Here, fitting yielded <inline-formula><mml:math id="inf210"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>For the fitting dimension <inline-formula><mml:math id="inf211"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="fig7">Figure 7B</xref>, bottom, we used an adapted definition of <inline-formula><mml:math id="inf212"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>: Because <inline-formula><mml:math id="inf213"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>% is not reached for all data sets, we asked for the number of PCs necessary to obtain 90% of the <inline-formula><mml:math id="inf214"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> value obtained for the full data set.</p><p>We also considered whether the correlation <inline-formula><mml:math id="inf215"><mml:mi>ρ</mml:mi></mml:math></inline-formula> scales with the number of neurons <inline-formula><mml:math id="inf216"><mml:mi>N</mml:mi></mml:math></inline-formula>. In our model, oblique and aligned dynamics can be defined in terms of such a scaling: aligned dynamics have highly correlated output weights and low-dimensional dynamics, so that <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, i.e., independent of the network size. For oblique dynamics, large output weights with norm <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> lead to vanishing correlation, <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. This is indeed similar to the relation between two random vectors, for which the correlation is precisely <inline-formula><mml:math id="inf220"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> (in the limit of large <inline-formula><mml:math id="inf221"><mml:mi>N</mml:mi></mml:math></inline-formula>). In <xref ref-type="fig" rid="fig8">Figure 8</xref>, we show the scaling of <inline-formula><mml:math id="inf222"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf223"><mml:mi>ρ</mml:mi></mml:math></inline-formula> with the number of subsampled neurons. For the cycling task and NLB data, the correlation scaled slightly weaker than <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. For the BCI data, the scaling was closer to <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> which is in between the aligned and oblique regimes of the model. These insights, however, are limited due to the trial averaging for the cycling task and the limited number of time points for the NLB tasks (not enough to reach <inline-formula><mml:math id="inf226"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). Applying these measures to larger data sets could yield more definitive insights.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Correlation scaling with number of neurons.</title><p>Scaling of the correlation <inline-formula><mml:math id="inf227"><mml:mi>ρ</mml:mi></mml:math></inline-formula> with the number of neurons <inline-formula><mml:math id="inf228"><mml:mi>N</mml:mi></mml:math></inline-formula> in experimental data. We fitted the output weights to subsets of <inline-formula><mml:math id="inf229"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons and computed the quality of fit (top) and the correlation between the resulting output weight and firing rates (bottom). To compare with random vectors, the correlation is scaled by <inline-formula><mml:math id="inf230"><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math></inline-formula>. Dashed lines are <inline-formula><mml:math id="inf231"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, for <inline-formula><mml:math id="inf232"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>4</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> for comparison. The aligned regime corresponds to <inline-formula><mml:math id="inf233"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, and the oblique one to <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig8-v1.tif"/></fig></sec><sec id="s4-7"><title>Analysis of solutions under noiseless conditions</title><p>In the sections below, we explore in detail under which conditions aligned and oblique solutions arise, and which other solutions arise if these conditions are not met.</p><p>We first consider small output weights and show that these lead to aligned solutions. Then, for large output weights, we show that without noise, two different, unstable solutions arise. Finally, we consider how adding noise affects learning dynamics. For a linear model, we can solve the dynamics of learning analytically and show how a negative feedback loop arises, that suppresses noise along the output direction. However, the linear model does not yield an oblique solution, so we also consider a nonlinear model for which we show in detail why oblique solutions arise.</p><p>We start by analyzing a simplified version of the network dynamics (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>): autonomous dynamics without noise,<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with fixed initial condition <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We assume a 1D output <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and a target <inline-formula><mml:math id="inf237"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> defined on a finite set of time points <inline-formula><mml:math id="inf238"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>We illustrate the theory with an example of a simple sine wave task (<xref ref-type="fig" rid="fig9">Figure 9</xref>). We demand the network to autonomously produce a sine wave with fixed frequency <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>. At the beginning of the task, the network receives an input pulse that sets the starting point of the trajectory. We set the noise <inline-formula><mml:math id="inf240"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>init</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> on the initial state <inline-formula><mml:math id="inf241"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to zero. We define the target as 20 target points in the interval <inline-formula><mml:math id="inf242"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>21</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (two cycles; purple dots in <xref ref-type="fig" rid="fig9">Figure 9</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Different solutions for networks trained on a sine wave task.</title><p>All networks have <inline-formula><mml:math id="inf243"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> neurons. Four regimes: (<bold>A</bold>) aligned for small output weights, (<bold>B</bold>) marginal for large output weights, small recurrent weights, (<bold>C</bold>) lazy for both large output and recurrent weights, (<bold>D</bold>) oblique for large output weights and noise added during training. Left: Output (dark), target (purple dots), and four states (light) of the network after training. Black bars indicate the scales for output and states (length = 1; same for all regimes). The output beyond the target interval <inline-formula><mml:math id="inf244"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:mn>21</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> can be considered as extrapolation. The network in the oblique regime, (<bold>D</bold>), receives white noise during training, and the evaluation is shown with the same noise. Without noise, this network still produces a sine wave (not shown). Right: Projection of states on the first two principal components (PCs) and the orthogonal component <inline-formula><mml:math id="inf245"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> of the output vector. All axes have the same scale, which allows for comparison between the dynamics. Vectors show the (amplified) output weights, dotted lines the projection on the PCs (not visible for lazy and oblique). The insets for the marginal solution (B, left and right) show the dynamics magnified by <inline-formula><mml:math id="inf246"><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig9-v1.tif"/></fig><sec id="s4-7-1"><title>Small weights lead to aligned solutions</title><p>For small output weights, <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, gradient-based learning in such a noise-less system has been analyzed by <xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>. Learning changes the dynamics qualitatively through low-rank weight changes <inline-formula><mml:math id="inf248"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>. These weight changes are spanned by existing directions such as the output weights. The resulting dynamics <inline-formula><mml:math id="inf249"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are thus aligned to the output weights. This means that the correlation between the two is large, independent of the network size, <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The target of the task is also independent of <inline-formula><mml:math id="inf251"><mml:mi>N</mml:mi></mml:math></inline-formula>, so that after learning we have <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Given the small output weights, we can thus infer the size of the states:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mphantom></mml:mpadded></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mphantom></mml:mpadded></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mphantom></mml:mpadded></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mphantom></mml:mpadded></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mspace width="mediummathspace"/><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mspace width="mediummathspace"/><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>so that <inline-formula><mml:math id="inf253"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, or equivalently single neuron activations <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This scaling corresponds to the aligned regime.</p><p>For the sine wave task, training with small output weights converges to an intuitive solution (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Neural activity evolves on a limit cycle, and the output is a sine wave that extrapolates beyond the training interval. Plotting activity and output weights along the largest two PCs and the remaining direction <inline-formula><mml:math id="inf255"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> confirms substantial correlation, <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, as expected. The solution was robust to adding noise after training (not shown). Changes in the initial dynamics or the presence of noise during training did not lead to qualitatively different solutions (<xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>). A further look at the eigenvalue spectrum of the trained recurrent weights revealed a pair of complex conjugate outliers corresponding to the limit cycle (<xref ref-type="bibr" rid="bib42">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>), and a bulk of remaining eigenvalues concentrated on a disk with radius <inline-formula><mml:math id="inf257"><mml:mi>g</mml:mi></mml:math></inline-formula>, <xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>.</p></sec><sec id="s4-7-2"><title>Large weights, no noise: linearization of dynamics</title><p>We now consider learning with large output weights, <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, for noise-less dynamics, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>. We start with the assumption that the activity changes for each neuron are small, <inline-formula><mml:math id="inf259"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, or <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf261"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the activity before learning. To perform a task, learning needs to induce output changes <inline-formula><mml:math id="inf263"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to reach the target <inline-formula><mml:math id="inf264"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Note that a possible order-one initial output <inline-formula><mml:math id="inf265"><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> must also be compensated. Together with the output weight scale, we arrive at<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mphantom></mml:mpadded></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi></mml:msub><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mphantom></mml:mpadded></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mspace width="mediummathspace"/><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mspace width="mediummathspace"/><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf266"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>corr</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This shows that our assumption of small state changes <inline-formula><mml:math id="inf267"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula> is consistent – it allows for a solution – and that such small changes need to be strongly correlated to the output weights. Note that we make the distinction between the changes <inline-formula><mml:math id="inf268"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula> and the final activity <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>, because the latter may be dominated by <inline-formula><mml:math id="inf270"><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. In the main text, we only consider the correlation between <inline-formula><mml:math id="inf271"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf272"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, because (as we show below) solutions with small <inline-formula><mml:math id="inf273"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula> are not robust, and the final <inline-formula><mml:math id="inf274"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> will be dominated by <inline-formula><mml:math id="inf275"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>For now, however, we ignore robustness and continue with the assumption of small <inline-formula><mml:math id="inf276"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>. Given this assumption, we linearize the dynamics around the initial trajectory <inline-formula><mml:math id="inf277"><mml:mrow><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with diagonal matrix <inline-formula><mml:math id="inf278"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝐱</mml:mi><mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the weights changes <inline-formula><mml:math id="inf279"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> that induce <inline-formula><mml:math id="inf280"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>. Note that we haven’t yet constrained the weight changes <inline-formula><mml:math id="inf281"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> so we cannot discard the terms of the kind <inline-formula><mml:math id="inf282"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>. The next steps depend on the initial trajectories <inline-formula><mml:math id="inf283"><mml:mrow><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-7-3"><title>Initially decaying dynamics lead to a marginal regime</title><p>We first consider networks with decaying dynamics before learning. This is obtained by drawing the initial recurrent weights independently from <inline-formula><mml:math id="inf284"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf285"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib71">Sompolinsky et al., 1988</xref>). With such dynamics, <inline-formula><mml:math id="inf286"><mml:mrow><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> vanishes exponentially in time. In <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, we disregard the term <inline-formula><mml:math id="inf287"><mml:mi>𝐚</mml:mi></mml:math></inline-formula> and have <inline-formula><mml:math id="inf288"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, so that<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To have self-sustained dynamics, the matrix <inline-formula><mml:math id="inf289"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> must have a leading eigenvalue <inline-formula><mml:math id="inf290"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> with real part above the stability line: <inline-formula><mml:math id="inf291"><mml:mrow><mml:mi>ℜ</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>The distance <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> must be small, else the states would become large. To understand how <inline-formula><mml:math id="inf293"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> needs to scale with <inline-formula><mml:math id="inf294"><mml:mi>N</mml:mi></mml:math></inline-formula>, we turn to a simple model studied before (<xref ref-type="bibr" rid="bib42">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>): an autonomously generated fixed point and rank-one connectivity <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mi>𝐮</mml:mi><mml:msup><mml:mi>𝐮</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The vector <inline-formula><mml:math id="inf296"><mml:mi>𝐮</mml:mi></mml:math></inline-formula> has entries <inline-formula><mml:math id="inf297"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. A fixed point of the dynamics (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>) fulfills <inline-formula><mml:math id="inf298"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mi>𝐮</mml:mi><mml:msup><mml:mi>𝐮</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mspace width="-0.1667em" style="margin-left:-0.1667em;"/><mml:mi>ϕ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝐱</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Projecting on <inline-formula><mml:math id="inf299"><mml:mi>𝐮</mml:mi></mml:math></inline-formula> and applying partial integration in the limit <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>, we obtain<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf301"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="false">∫</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mi>u</mml:mi><mml:mspace width="0.1667em"/><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>u</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with standard normal measure <inline-formula><mml:math id="inf302"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf303"><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> is the scale of the states, <inline-formula><mml:math id="inf304"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf305"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The fixed point is situated along the vector <inline-formula><mml:math id="inf306"><mml:mi>𝐮</mml:mi></mml:math></inline-formula>. To have the smallest possible fixed point generate some output, we set the output weights to <inline-formula><mml:math id="inf307"><mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:mi>𝐮</mml:mi></mml:mrow></mml:math></inline-formula>. Then, we have correlation <inline-formula><mml:math id="inf308"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf309"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>𝐱</mml:mi><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mspace width="0.1667em"/><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mspace width="0.1667em"/><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In other words, a small fixed point with <inline-formula><mml:math id="inf310"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>. We expand <inline-formula><mml:math id="inf311"><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> around zero. For even <inline-formula><mml:math id="inf312"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf313"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:mrow></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>‴</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>‴</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Sigmoidal functions have <inline-formula><mml:math id="inf314"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo><mml:mo class="tml-prime">′</mml:mo><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, e.g., <inline-formula><mml:math id="inf315"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo><mml:mo class="tml-prime">′</mml:mo><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:mrow></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>tanh</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Hence <inline-formula><mml:math id="inf317"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf318"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>. Remarkably, the perturbation leading to states with <inline-formula><mml:math id="inf319"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> only needs to have a distance <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> away from the stability line.</p><p>The insights from this simplified setting extend to the example of the sine wave task (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). The model with large output weights, <inline-formula><mml:math id="inf321"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula>, and no noise yields a limit cycle. The output extrapolates in time, but the states are very small, scaling as <inline-formula><mml:math id="inf322"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (analysis over different <inline-formula><mml:math id="inf323"><mml:mi>N</mml:mi></mml:math></inline-formula> not shown). Such a solution is only marginally stable – adding a white noise with <inline-formula><mml:math id="inf324"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> after training destroyed the rotation (not shown). The eigenvalues were again split into two outliers and a bulk (<xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>). However, the two outliers now had a real part <inline-formula><mml:math id="inf325"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>, i.e., they were very close to the stability line of the fixed point at zero. To better illustrate the marginal solution, we also set the initial state <inline-formula><mml:math id="inf326"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to small values, <inline-formula><mml:math id="inf327"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. For <inline-formula><mml:math id="inf328"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, there would be an initial decay much larger than the limit cycle.</p></sec><sec id="s4-7-4"><title>Initially chaotic dynamics lead to a lazy regime</title><p>In contrast to the situation before, initially chaotic dynamics (for <inline-formula><mml:math id="inf329"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) imply order-one initial states, <inline-formula><mml:math id="inf330"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all trial times <inline-formula><mml:math id="inf331"><mml:mi>t</mml:mi></mml:math></inline-formula>. The driving term <inline-formula><mml:math id="inf332"><mml:mi>𝐚</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> can thus not be ignored and we expect it to be on the same scale as <inline-formula><mml:math id="inf333"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mn>1</mml:mn><mml:mo>∼</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>∼</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The smallest possible weight changes <inline-formula><mml:math id="inf334"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> will be those for which <inline-formula><mml:math id="inf335"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> yields a maximal response, but other vectors do not yield a strong response. This is captured by the operator norm, <inline-formula><mml:math id="inf336"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mo lspace="0.2222em" rspace="0.2222em">:</mml:mo><mml:mi>𝐱</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup><mml:mtext> with </mml:mtext><mml:mi>‖</mml:mi><mml:mi>𝐱</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. We can then write <inline-formula><mml:math id="inf337"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>‖</mml:mi><mml:mo>∼</mml:mo><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula>, and hence <inline-formula><mml:math id="inf338"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:math></inline-formula>. The operator norm also bounds the eigenvalues <inline-formula><mml:math id="inf339"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> and hence the effect of the matrix on the dynamics of the system. For large <inline-formula><mml:math id="inf340"><mml:mi>N</mml:mi></mml:math></inline-formula>, this implies that the changes <inline-formula><mml:math id="inf341"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> are too small to change the dynamics qualitatively, and the latter remain chaotic. Note that because the network dynamics are chaotic, the term <inline-formula><mml:math id="inf342"><mml:mi>𝐛</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> diverges, so that our discussion is only valid for short times. Numerically, we find small weight changes and chaotic solutions even for large target times <inline-formula><mml:math id="inf343"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (not shown).</p><p>For the sine wave task, the network with initially chaotic dynamics indeed converges to such a solution (<xref ref-type="fig" rid="fig9">Figure 9C</xref>). The output does not extrapolate beyond the training interval <inline-formula><mml:math id="inf344"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>21</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, and dynamics remain qualitatively similar to those before training. During the training interval, the dynamics also remain close to the initial trajectories (dashed line). Testing the response to small perturbations in <inline-formula><mml:math id="inf345"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> indicated that the dynamics remain chaotic (not shown). No limit cycle was formed, and the spectrum of eigenvalues did not show outliers (<xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>).</p><p>We called this regime ‘lazy’, following similar settings in feedforward networks (<xref ref-type="bibr" rid="bib27">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Chizat et al., 2019</xref>). Note that there, the output <inline-formula><mml:math id="inf346"><mml:mrow><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is linearized around the weights at initialization (as opposed to the dynamics, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>). This can be done in our case as well:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Demanding <inline-formula><mml:math id="inf347"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> yields a linear equation for each time point <inline-formula><mml:math id="inf348"><mml:mi>t</mml:mi></mml:math></inline-formula>. As we have <inline-formula><mml:math id="inf349"><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> parameters, this system is typically underconstrained. Gradient descent for this linear system leads to the minimal norm solution, which can also be found directly using the Moore-Penrose pseudo-inverse. Numerically, we found that the weights <inline-formula><mml:math id="inf350"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>lin</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> obtained by this linearization are very close to those found by gradient descent (GD) on the nonlinear system, <inline-formula><mml:math id="inf351"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>GD</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, with Frobenius norm <inline-formula><mml:math id="inf352"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>GD</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>lin</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>∼</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> (section Linear approximation for lazy learning).</p></sec><sec id="s4-7-5"><title>Marginal and lazy solutions disappear with noise during training</title><p>The deduction above hinges on the assumption that <inline-formula><mml:math id="inf353"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula> is small. This assumption does not hold if dynamics are noisy, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>. For marginal dynamics, the noise would push solutions to different attractors or different positions along the limit cycle. For lazy dynamics, the chaotic dynamics would amplify any perturbations along the trajectory (if chaos persists under noise; <xref ref-type="bibr" rid="bib67">Schuecker et al., 2018</xref>).</p><p>We will explore how learning is affected by noise in the sections below. Here, we only show that adding noise for our example task abolishes the marginal or lazy solutions and leads to oblique ones (<xref ref-type="fig" rid="fig9">Figure 9D</xref>). We added white noise with amplitude <inline-formula><mml:math id="inf354"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> to the dynamics during learning. After training, the output was a noisy sine wave. States were order one, and the 3D projection showed dynamics along a limit cycle that was almost orthogonal to the output vector. The noise in the 2D subspace of the first two PCs was small, <inline-formula><mml:math id="inf355"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and thus did not disrupt the dynamics (e.g. very little phase shift). The eigenvalue spectrum had two outliers whose real part was increased in comparison to those in the aligned regime (<xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>). Note that for the chosen values <inline-formula><mml:math id="inf356"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf357"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula>, the network actually was not chaotic at initialization (<xref ref-type="bibr" rid="bib67">Schuecker et al., 2018</xref>). However, the choice of <inline-formula><mml:math id="inf358"><mml:mi>g</mml:mi></mml:math></inline-formula> does not influence the solution in the oblique regime qualitatively, so both marginal and lazy solutions cease to exist g.</p></sec></sec><sec id="s4-8"><title>Learning with noise for linear RNNs</title><p>In the next two sections, we aim to understand how adding noise affects dynamics and training. We start with a simple setting of a linear RNN which allows us to track the learning dynamics analytically. Despite its simplicity, this setting already captures a range of observations: different time scales for learning the bias and variance part, and the rise of a negative feedback loop for noise suppression. Oblique dynamics, however, do not arise, showing that these need autonomously generated, nonlinear dynamics, covered in section Oblique solutions arise for noisy, nonlinear systems.</p><p>We consider a linear network driven by a constant input and additional white noise. The dynamics read<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with noise <inline-formula><mml:math id="inf359"><mml:mi>𝝃</mml:mi></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>. We focus on a simplified task, which is to produce a constant nonzero output <inline-formula><mml:math id="inf360"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> once the average dynamics converged, i.e., for large trial times. The output is <inline-formula><mml:math id="inf361"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>𝐱</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>, where the bar denotes average over the noise, and the delta fluctuations around the average. The average is given by <inline-formula><mml:math id="inf362"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We train the network by changing only the recurrent weights <inline-formula><mml:math id="inf363"><mml:mi>W</mml:mi></mml:math></inline-formula> via gradient descent. For small output weights, the fluctuations are too small to affect training: <inline-formula><mml:math id="inf364"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Apart from a small correction, learning dynamics are then the same as for small output weights and no noise, a setting analyzed in <xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>. Here, we only consider the case of large output weights.</p><p>The loss separates into two parts, <inline-formula><mml:math id="inf365"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf366"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf367"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:menclose notation="top" class="tml-overline"><mml:mrow><mml:mi>δ</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:menclose><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mi>z</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Learning aims to minimize the sum. We first consider learning based on each part alone and then join both to describe the full learning dynamics.</p><p>Learning based on the bias part alone converges to a lazy solution (see section Details linear model: Bias only: lazy learning). For no initial weights, <inline-formula><mml:math id="inf368"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we have to leading order<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>⊥</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mi>η</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, <inline-formula><mml:math id="inf369"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> is rank one with norm <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>. Furthermore, for a learning rate <inline-formula><mml:math id="inf371"><mml:mi>η</mml:mi></mml:math></inline-formula>, it converges in <inline-formula><mml:math id="inf372"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>η</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> time steps. We will see that this is very fast compared to learning the variance part.</p><sec id="s4-8-1"><title>Learning to reduce noise alone slowly produces a negative feedback loop</title><p>Next, we consider learning based on the variance part <inline-formula><mml:math id="inf373"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> alone, i.e., to reduce fluctuations in the output while ignoring the mean. The network dynamics are linear, so that <inline-formula><mml:math id="inf374"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></inline-formula> is an Ornstein-Uhlenbeck process. Its stationary variance <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:math></inline-formula> is the solution to the Lyapunov equation<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf376"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>. The variance part of the loss is then<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>One can state the gradient of this loss in terms of a second Lyapunov equation (<xref ref-type="bibr" rid="bib85">Yan et al., 2016</xref>):<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi mathvariant="normal">Σ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where Ω is the solution to<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Generally, solving both Lyapunov equations analytically is not possible, and even results for random matrices are still sparse (e.g. for symmetric Wigner matrices <inline-formula><mml:math id="inf377"><mml:mi>W</mml:mi></mml:math></inline-formula> <xref ref-type="bibr" rid="bib53">Preciado and Rahimian, 2016</xref>). To gain intuition, we thus restrict ourselves to the case of no initial connectivity, which leads to the connectivity spanned by input and output weights only. We start with the simplest case of a rank-one matrix only spanned by the output weights, <inline-formula><mml:math id="inf378"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, and extend to rank two in the following section. The Lyapunov equations then become 1D, and we obtain<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The gradient <inline-formula><mml:math id="inf379"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is therefore in the same subspace as <inline-formula><mml:math id="inf380"><mml:mi>W</mml:mi></mml:math></inline-formula>, and we can evaluate the 1D dynamics<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf381"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the number of update steps. We assume <inline-formula><mml:math id="inf382"><mml:mi>τ</mml:mi></mml:math></inline-formula> to be continuous, i.e., we assume a sufficiently small learning rate and approximate the discrete dynamics of gradient descent with gradient flow. With initial condition <inline-formula><mml:math id="inf383"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the solution is<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mi>η</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which is negative for <inline-formula><mml:math id="inf384"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The loss then decays as<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mi>η</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>namely at order <inline-formula><mml:math id="inf385"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in learning time. We thus obtained that, during the variance phase of learning, connectivity develops a negative feedback look aligned with the output weights, which serves to suppress output noise. For very long learning times, <inline-formula><mml:math id="inf386"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mi>/</mml:mi><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula>, learning can in principle reduce the output fluctuations to <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:math></inline-formula>. Note, however, that this implies a huge negative feedback loop, <inline-formula><mml:math id="inf388"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which potentially leads to instability in a system with delays or discretized dynamics (<xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>).</p></sec><sec id="s4-8-2"><title>Optimizing mean and fluctuations occurs on different time scales</title><p>We now consider learning both the mean and the variance part. For zero initial recurrent weights, <inline-formula><mml:math id="inf389"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the input and output vectors make up the only relevant directions in space. We thus express the recurrent weights as a rank-two matrix, <inline-formula><mml:math id="inf390"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>M</mml:mi><mml:msup><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, with orthonormal basis <inline-formula><mml:math id="inf391"><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. The hats indicate normalized vectors. For large networks and large output weights, the first vector is already normalized, <inline-formula><mml:math id="inf392"><mml:mrow><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The second vector is the input weights after Gram-Schmidt. Assuming that <inline-formula><mml:math id="inf393"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf394"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are drawn independently, we have a small, random correlation <inline-formula><mml:math id="inf395"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and can write <inline-formula><mml:math id="inf396"><mml:mrow><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mspace width="0.1667em"/><mml:mi>.</mml:mi></mml:mrow></mml:math></inline-formula></p><p>We computed the learning dynamics in terms of the coefficient matrix <inline-formula><mml:math id="inf397"><mml:mi>M</mml:mi></mml:math></inline-formula>, using the same tools introduced above, and the insight that learning the mean is much faster than learning to reduce the variance. The details are relegated to section Details linear model: Bias and variance combined, here, we summarize the results. We obtained<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Before discussing the temporal evolution of the two components, we analyze the structure of the matrix. The eigenvalue <inline-formula><mml:math id="inf398"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.1667em"/><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is a negative feedback loop along the eigenvector <inline-formula><mml:math id="inf399"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf400"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.1667em"/><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> is a small feedforward component <inline-formula><mml:math id="inf401"><mml:mi>b</mml:mi></mml:math></inline-formula> which maps the input to the output. Along the input direction <inline-formula><mml:math id="inf402"><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, learning does not change the dynamics: The second eigenvalue, corresponding to this direction, is zero.</p><p>The dynamics unfold on two time scales. First, there is a very fast learning of the bias via the feedforward coefficient<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mi>η</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>During this phase, the eigenvalue <inline-formula><mml:math id="inf403"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.1667em"/><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> remains at zero, so that overall weight changes remain small, <inline-formula><mml:math id="inf404"><mml:mrow><mml:mi>‖</mml:mi><mml:mi>W</mml:mi><mml:mi>‖</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. The average fixed point also does not change much, <inline-formula><mml:math id="inf405"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mspace width="0.1667em"/><mml:mo separator="true">,</mml:mo></mml:mrow></mml:math></inline-formula> in comparison to the fixed point before learning, <inline-formula><mml:math id="inf406"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mspace width="0.1667em"/><mml:mi>.</mml:mi></mml:mrow></mml:math></inline-formula> The loss evolves as<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn><mml:mi>N</mml:mi><mml:mi>η</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The variance part of the loss does not change during this phase.</p><p>In a second, slower learning phase, the eigenvalue <inline-formula><mml:math id="inf407"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula> evolves like above, <xref ref-type="disp-formula" rid="equ28">Equation 28</xref>, the case where only the variance part is learned. The second coefficient compensates for the resulting change in the output. This compensation happens at a much faster time scale (<inline-formula><mml:math id="inf408"><mml:msup><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:math></inline-formula> times faster than <inline-formula><mml:math id="inf409"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula>), so we consider its steady state:<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Because of this compensation, the bias part of the loss always remains at zero, and the full loss <inline-formula><mml:math id="inf410"><mml:mrow><mml:mi>L</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> evolves as before, <xref ref-type="disp-formula" rid="equ29">Equation 29</xref>. Meanwhile, the average fixed point does not change anymore; we have <inline-formula><mml:math id="inf411"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p><p>We compare our theoretical predictions against numerical simulations in <xref ref-type="fig" rid="fig10">Figure 10</xref>. For the task, we let the linear network dynamics converge from <inline-formula><mml:math id="inf412"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>𝟎</mml:mn></mml:mrow></mml:math></inline-formula> until <inline-formula><mml:math id="inf413"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>, and demand the output <inline-formula><mml:math id="inf414"><mml:mrow><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to be at the target <inline-formula><mml:math id="inf415"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> during the interval <inline-formula><mml:math id="inf416"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>15</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>20</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Because the first learning phase converges <inline-formula><mml:math id="inf417"><mml:mi>N</mml:mi></mml:math></inline-formula> times faster than the second one (with <inline-formula><mml:math id="inf418"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>), using a single learning rate <inline-formula><mml:math id="inf419"><mml:mi>η</mml:mi></mml:math></inline-formula> is problematic. One can either observe the first phase only (for small <inline-formula><mml:math id="inf420"><mml:mi>η</mml:mi></mml:math></inline-formula>) or risk unstable learning during the first phase (for large <inline-formula><mml:math id="inf421"><mml:mi>η</mml:mi></mml:math></inline-formula>). We thus split learning into two parts with adapted learning rates. For the initial phase, we set a learning rate to <inline-formula><mml:math id="inf422"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf423"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Figure 10</xref>, left column). For the second phase, we set <inline-formula><mml:math id="inf424"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Figure 10</xref>, right column). Theory and simulation agree well for both phases. Small deviations can be observed for the second phase and long learning times: nonzero coefficients <inline-formula><mml:math id="inf425"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.1667em"/><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf426"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.1667em"/><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and a corresponding increase in the norm <inline-formula><mml:math id="inf427"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula>. Testing with larger network sizes showed that these errors decreased as <inline-formula><mml:math id="inf428"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which is consistent with our theory above (not shown).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Noise-induced learning for a linear network with input-driven fixed point.</title><p>Learning separates into fast learning of the bias part of the loss (left), and slow learning reducing the variance part (right). Learning rates are <inline-formula><mml:math id="inf429"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf430"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, with <inline-formula><mml:math id="inf431"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula> and network size <inline-formula><mml:math id="inf432"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>. Learning epochs in the first phase are counted from –1000, so that the second phase starts at 0. In the right column, the initial learning phase with learning time steps multiplied by <inline-formula><mml:math id="inf433"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is shown for comparison. In all plots, simulations (full lines) are compared with theory (dashed lines). (<bold>A</bold>) Loss <inline-formula><mml:math id="inf434"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The two components are obtained by averaging over a batch with 32 examples at each learning step. The full loss is not plotted in the slow phase, because it is indistinguishable from <inline-formula><mml:math id="inf435"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. (<bold>B</bold>) Coefficients of the 2-by-2 coupling matrix <inline-formula><mml:math id="inf436"><mml:mi>M</mml:mi></mml:math></inline-formula>. <inline-formula><mml:math id="inf437"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>11</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is the feedback loop along the output weights, <inline-formula><mml:math id="inf438"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>12</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> a feedforward coupling from input to output. The theory predicts <inline-formula><mml:math id="inf439"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>21</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mn>22</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Norm of state changes during training. The theory predicts that it remains constant during the second phase and small compared to <inline-formula><mml:math id="inf440"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. Other parameters: target <inline-formula><mml:math id="inf441"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf442"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, overlap between input and output vectors <inline-formula><mml:math id="inf443"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>0.5</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig10-v1.tif"/></fig><p>Our results show that learning in this linear system is a hybrid between oblique and aligned during the second phase. We have a large term that compresses the output noise, but a very small, ‘lazy’ correction in the feedforward component that corrects the output, and the states are only marginally changed. Although this system is a somewhat degenerate limiting case, we can still derive important insights. The two most striking features – the different time scales of the two learning processes, and the slow emergence of negative feedback, <inline-formula><mml:math id="inf444"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, along the output – are found robustly also for nonlinear networks and other tasks.</p></sec></sec><sec id="s4-9"><title>Oblique solutions arise for noisy, nonlinear systems</title><p>We now examine the origin of oblique solutions. The linear system did not yield oblique solutions, so we turn to a nonlinear model. We consider a 1D flip-flop task, where the network has to yield a constant, nonzero output <inline-formula><mml:math id="inf445"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> depending on the sign of the last input pulse. We further simplify the analysis by only considering the steady state of a network, not how the input pulse mediates the transition. At the output, we thus only consider the average <inline-formula><mml:math id="inf446"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> and the fluctuations <inline-formula><mml:math id="inf447"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>. As for the linear network, the loss splits into a bias part <inline-formula><mml:math id="inf448"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and variance part <inline-formula><mml:math id="inf449"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:menclose notation="top" class="tml-overline"><mml:mrow><mml:mi>δ</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:menclose></mml:mrow></mml:math></inline-formula>. As before, we assume that learning only acts on a low-dimensional parameter matrix <inline-formula><mml:math id="inf450"><mml:mi>M</mml:mi></mml:math></inline-formula>.</p><p>Because following the learning dynamics in nonlinear networks is difficult, we take a different approach. We develop a mean field theory to show how noise affects the dynamics of a nonlinear network with autonomous fixed points. This allows us to compute the loss components <inline-formula><mml:math id="inf451"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf452"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in terms of <inline-formula><mml:math id="inf453"><mml:mi>M</mml:mi></mml:math></inline-formula>. We then show that the minimum of the loss function corresponds to oblique solutions. This leads to a clear interpretation of the mechanisms pushing for oblique solutions. Finally, we show that the theory quantitatively predicts the outcome of learning with gradient descent.</p><sec id="s4-9-1"><title>Rank-two connectivity model with fixed point</title><p>We first introduce the connectivity model and compute the latent dynamics using mean field theory. We constrain the recurrent connectivity to a rank-two model of the form <inline-formula><mml:math id="inf454"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>U</mml:mi><mml:mi>M</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf455"><mml:mi>M</mml:mi></mml:math></inline-formula> is a 2×2 coefficient matrix to be learned. The randomly drawn projection matrix <inline-formula><mml:math id="inf456"><mml:mrow><mml:mi>U</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> has entries <inline-formula><mml:math id="inf457"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> drawn independently from a standard normal distribution. This implies orthogonality to leading order, <inline-formula><mml:math id="inf458"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We will discard the correction term, as it does not change our results apart from a constant bias. We further assume that the input and output vectors are spanned by <inline-formula><mml:math id="inf459"><mml:mi>U</mml:mi></mml:math></inline-formula>, although not necessarily identified with the components of <inline-formula><mml:math id="inf460"><mml:mi>U</mml:mi></mml:math></inline-formula> as in the previous section. Note also that for clarity we do not normalize <inline-formula><mml:math id="inf461"><mml:mi>U</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf462"><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> before. The assumption of Gaussian connectivity greatly simplifies the math, while its restrictions are irrelevant to the task considered here (<xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>; <xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Dubreuil et al., 2021</xref>).</p><p>To understand the dynamics <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> analytically, we make use of the low-rank connectivity. Following previous work (<xref ref-type="bibr" rid="bib57">Rivkind and Barak, 2017</xref>; <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>), we split the dynamics into two parts: a parallel part <inline-formula><mml:math id="inf463"><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">∥</mml:mo></mml:msub></mml:math></inline-formula> in the subspace spanned by <inline-formula><mml:math id="inf464"><mml:mi>U</mml:mi></mml:math></inline-formula>, and an orthogonal part <inline-formula><mml:math id="inf465"><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msub></mml:math></inline-formula>. This yields<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>U</mml:mi><mml:mi>M</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>U</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>U</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Notice that the parallel part is partially driven by the orthogonal one, but not vice versa. The parallel part can be expressed in terms of the latent variable<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The scaling here ensures that <inline-formula><mml:math id="inf466"><mml:mi>𝜿</mml:mi></mml:math></inline-formula> is order one if <inline-formula><mml:math id="inf467"><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">∥</mml:mo></mml:msub></mml:math></inline-formula> has order-one states. Because the readout is assumed to be spanned by <inline-formula><mml:math id="inf468"><mml:mi>U</mml:mi></mml:math></inline-formula>, the output is fully determined by the parallel part. We can write<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with projected output weights <inline-formula><mml:math id="inf469"><mml:mrow><mml:msub><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Note that we assume large output weights, <inline-formula><mml:math id="inf470"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf471"><mml:msub><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is also normalized.</p><p>We split the latent state into its average over the noise <inline-formula><mml:math id="inf472"><mml:mi>𝝃</mml:mi></mml:math></inline-formula> and fluctuations, <inline-formula><mml:math id="inf473"><mml:mrow><mml:mi>𝜿</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula>. Similarly, the output splits into <inline-formula><mml:math id="inf474"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>z</mml:mi><mml:mspace width="0.1667em"/><mml:mi>.</mml:mi></mml:mrow></mml:math></inline-formula> The loss then has two components, <inline-formula><mml:math id="inf475"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, with<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We want to understand situations with small loss. For the bias term, the average <inline-formula><mml:math id="inf476"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> must be either small or oblique to the readout weights. For the variance term, the covariance of the fluctuations, <inline-formula><mml:math id="inf477"><mml:mrow><mml:mrow><mml:mtext/><mml:mi>cov</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:menclose notation="top" class="tml-overline"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi><mml:mi>δ</mml:mi><mml:msup><mml:mi>𝜿</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:menclose></mml:mrow></mml:math></inline-formula>, must be compressed along the output direction: Even though the covariance is <inline-formula><mml:math id="inf478"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, it still projects to the output at <inline-formula><mml:math id="inf479"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, as reflected by the factor <inline-formula><mml:math id="inf480"><mml:mi>N</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ39">Equation 39</xref>. <xref ref-type="fig" rid="fig11">Figure 11</xref> illustrates the situation in a cartoon from this high-level perspective.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Cartoon illustrating the split into bias and variance components of the loss, and noise suppression along the output direction.</title><p>The two-dimensional subspace spanned by <inline-formula><mml:math id="inf481"><mml:mi>U</mml:mi></mml:math></inline-formula> illustrates the main directions under consideration: the principal components (PCs) of the average trajectories (here only a fixed point <inline-formula><mml:math id="inf482"><mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>), and the direction of output weights <inline-formula><mml:math id="inf483"><mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:mi>U</mml:mi><mml:msub><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Left: During learning, a fast process keeps the average output close to the target so that <inline-formula><mml:math id="inf484"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Center: The variance component, <inline-formula><mml:math id="inf485"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, is determined by the projection of the fluctuations <inline-formula><mml:math id="inf486"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula> onto the output vector. Note that the noise in the low-D subspace is very small, <inline-formula><mml:math id="inf487"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, but the output is still affected due to the large output weights. Right: During training, the noise becomes non-isotropic. Along the average direction <inline-formula><mml:math id="inf488"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula>, the fluctuations are increased as a byproduct of the positive feedback <inline-formula><mml:math id="inf489"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>. Meanwhile, a slow learning process suppresses the output variance via a negative feedback <inline-formula><mml:math id="inf490"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig11-v1.tif"/></fig><p>To understand the underlying mechanisms in detail, we next explore how the relevant variables <inline-formula><mml:math id="inf491"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula> are determined by the coupling matrix <inline-formula><mml:math id="inf493"><mml:mi>M</mml:mi></mml:math></inline-formula>. We do so by applying mean field theory, following previous works (<xref ref-type="bibr" rid="bib42">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>; <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>; <xref ref-type="bibr" rid="bib67">Schuecker et al., 2018</xref>). Detailed derivations can be found in the section Details nonlinear autonomous system with noise. Here, we present the high-level results. The average dynamics converge to a fixed point determined by the equation for the latent variable,<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> term is a constant offset for any given network that we ignore without loss of generality. The average slope is<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mi>u</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with variance<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mrow><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>(We have <inline-formula><mml:math id="inf495"><mml:mrow><mml:menclose notation="top" class="tml-overline"><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:menclose><mml:mo>=</mml:mo><mml:msqrt><mml:menclose notation="top" class="tml-overline"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:menclose></mml:msqrt></mml:mrow></mml:math></inline-formula> because the fluctuations are small.) The orthogonal variance is simply the variance of the noise, <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/></mml:mrow></mml:mstyle></mml:math></inline-formula>. The fixed point (<xref ref-type="disp-formula" rid="equ40">Equation 40</xref>) implies that for a nonzero fixed point, the matrix <inline-formula><mml:math id="inf497"><mml:mi>M</mml:mi></mml:math></inline-formula> must have an eigenvalue<disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This is very similar to the noiseless situation discussed briefly above, <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>. However, here the additional variance <inline-formula><mml:math id="inf498"><mml:menclose notation="top" class="tml-overline"><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⟂</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:menclose></mml:math></inline-formula> decreases the average slope due to saturation of the nonlinearity. This in turn increases the minimal eigenvalue for a nonzero fixed point, which can be found by setting <inline-formula><mml:math id="inf499"><mml:mrow><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>𝟎</mml:mn></mml:mrow></mml:math></inline-formula>, and hence <inline-formula><mml:math id="inf500"><mml:mrow><mml:menclose notation="top" class="tml-overline"><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:menclose><mml:mo>=</mml:mo><mml:menclose notation="top" class="tml-overline"><mml:msub><mml:mi>σ</mml:mi><mml:mo>⟂</mml:mo></mml:msub></mml:menclose></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:msub><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In other words, the noise decreases the effective gain <inline-formula><mml:math id="inf501"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, and thus the connectivity eigenvalue <inline-formula><mml:math id="inf502"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> needs to compensate. From the point of view of the spectrum, we are thus pushed away from the margin <inline-formula><mml:math id="inf503"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>. These considerations, however, do not exclude the possibility that dynamics converge to an average fixed point that is small and correlated, which is at odds with oblique dynamics. To understand why learning leads to oblique dynamics, we need to move beyond the average <inline-formula><mml:math id="inf504"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> and take into account the fluctuations <inline-formula><mml:math id="inf505"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-9-2"><title>Fluctuations of the latent variable</title><p>The fluctuations <inline-formula><mml:math id="inf506"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula> around a fixed point <inline-formula><mml:math id="inf507"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> are driven by the noise, both directly and indirectly via the dynamics. The direct contribution is a white noise term of order <inline-formula><mml:math id="inf508"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> because <inline-formula><mml:math id="inf509"><mml:mi>𝝃</mml:mi></mml:math></inline-formula> is isotropic and independent of <inline-formula><mml:math id="inf510"><mml:mi>U</mml:mi></mml:math></inline-formula>. A detailed analysis (section Details nonlinear autonomous system with noise) shows that the indirect contribution is given by a colored noise term which originates from the finite size fluctuations in the variance of the orthogonal part. This second term is also <inline-formula><mml:math id="inf511"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which implies that the fluctuations are small, <inline-formula><mml:math id="inf512"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We can thus linearize their dynamics around the mean <inline-formula><mml:math id="inf513"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula>, which yields<disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the order-one term <inline-formula><mml:math id="inf514"><mml:mi>𝜻</mml:mi></mml:math></inline-formula> contains both the white and the colored noise term. The Jacobian <inline-formula><mml:math id="inf515"><mml:mi>A</mml:mi></mml:math></inline-formula> depends on <inline-formula><mml:math id="inf516"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf517"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula>:<disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>‴</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The averages are again evaluated at the joint variance <inline-formula><mml:math id="inf518"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. Apart from the increased variance, the stability analysis yields the same results as in the noise-free case (<xref ref-type="bibr" rid="bib68">Schuessler et al., 2020a</xref>): The Jacobian has the eigenvalues <inline-formula><mml:math id="inf519"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo><mml:mo class="tml-prime">′</mml:mo><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:mrow></mml:msup><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf520"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The average over the third derivative <inline-formula><mml:math id="inf521"><mml:mrow><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo><mml:mo class="tml-prime">′</mml:mo><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:mrow></mml:msup><mml:mo form="postfix" stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is negative, so that <inline-formula><mml:math id="inf522"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. We assume that the second eigenvalue is smaller than the first, <inline-formula><mml:math id="inf523"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf524"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The fixed point under consideration is hence stable.</p><p>Next, we compute the covariance of the fluctuations at steady state, see section Details nonlinear autonomous system with noise. The outcome is<disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf525"><mml:mrow><mml:msub><mml:mi>𝐯</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>/</mml:mi><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> is the normalized eigenvector of <inline-formula><mml:math id="inf526"><mml:mi>M</mml:mi></mml:math></inline-formula> corresponding to eigenvalue <inline-formula><mml:math id="inf527"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>. The 2×2 matrix <inline-formula><mml:math id="inf528"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> is the covariance introduced by the white noise part alone and obeys the Lyapunov equation<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The second term in <xref ref-type="disp-formula" rid="equ47">Equation 47</xref> stems from the colored noise component of <inline-formula><mml:math id="inf529"><mml:mi>𝜻</mml:mi></mml:math></inline-formula>.</p><p>The loss (<xref ref-type="disp-formula" rid="equ39">Equation 39</xref>) is obtained by projecting the covariance on the output weights. Importantly, the factor <inline-formula><mml:math id="inf530"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> in the covariance is compensated by the factor <inline-formula><mml:math id="inf531"><mml:mi>N</mml:mi></mml:math></inline-formula> in the loss. Hence, even if the covariance shrinks with increasing network size, the output is still affected at order one. We next explore the implications of minimizing this loss.</p></sec><sec id="s4-9-3"><title>Minimizing the loss by balancing saturation and negative feedback loop</title><p>To gain an understanding of how the output fluctuations responsible for <inline-formula><mml:math id="inf532"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be reduced, we first consider the case of a symmetric coefficient matrix <inline-formula><mml:math id="inf533"><mml:mi>M</mml:mi></mml:math></inline-formula>. Simulations of networks trained with gradient descent below show that this approximation is reasonable. For symmetric <inline-formula><mml:math id="inf534"><mml:mi>M</mml:mi></mml:math></inline-formula>, the orthogonal eigenvectors <inline-formula><mml:math id="inf535"><mml:msub><mml:mi>𝐯</mml:mi><mml:mo lspace="0em" rspace="0em">±</mml:mo></mml:msub></mml:math></inline-formula> with eigenvalues <inline-formula><mml:math id="inf536"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">±</mml:mo></mml:msub></mml:math></inline-formula> of the recurrent weights <inline-formula><mml:math id="inf537"><mml:mi>M</mml:mi></mml:math></inline-formula> are also eigenvectors of <inline-formula><mml:math id="inf538"><mml:mi>A</mml:mi></mml:math></inline-formula>, in that case corresponding to the eigenvalues <inline-formula><mml:math id="inf539"><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">±</mml:mo></mml:msub></mml:math></inline-formula>. This allows to diagonalize the Lyapunov <xref ref-type="disp-formula" rid="equ48">Equation 48</xref> and yields the solution<disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the loss (<xref ref-type="disp-formula" rid="equ39">Equation 39</xref>), we further need the relation between the eigenvectors <inline-formula><mml:math id="inf540"><mml:msub><mml:mi>𝐯</mml:mi><mml:mo lspace="0em" rspace="0em">±</mml:mo></mml:msub></mml:math></inline-formula> and the output weights. Because the fixed point <inline-formula><mml:math id="inf541"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> is parallel to the eigenvector, we have <inline-formula><mml:math id="inf542"><mml:mrow><mml:msubsup><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>𝐯</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:math></inline-formula>. For the other eigenvector, orthogonality yields <inline-formula><mml:math id="inf543"><mml:mrow><mml:msubsup><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>𝐯</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. Inserting this into <xref ref-type="disp-formula" rid="equ39 equ47">Equations 39 and 47</xref> yields an expression in terms of the Jacobian eigenvalues <inline-formula><mml:math id="inf544"><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">±</mml:mo></mml:msub></mml:math></inline-formula>, the correlation <inline-formula><mml:math id="inf545"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, and the norm of the fixed point <inline-formula><mml:math id="inf546"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For more explicit insight, we choose the nonlinearity <inline-formula><mml:math id="inf547"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>erf</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mi>x</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf548"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Similar to <inline-formula><mml:math id="inf549"><mml:mrow><mml:mtext/><mml:mi>tanh</mml:mi></mml:mrow></mml:math></inline-formula>, this function is bounded between ±1 and has slope <inline-formula><mml:math id="inf550"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> at the origin. For this function, we can explicitly compute the relevant Gaussian integrals. The minimal eigenvalue of <inline-formula><mml:math id="inf551"><mml:mi>M</mml:mi></mml:math></inline-formula> to produce a fixed point, <xref ref-type="disp-formula" rid="equ44">Equation 44</xref>, is then given by <inline-formula><mml:math id="inf552"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. For <inline-formula><mml:math id="inf553"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the resulting fixed point has norm<disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>as shown in <xref ref-type="fig" rid="fig12">Figure 12A</xref>. The larger eigenvalue of the Jacobian is <inline-formula><mml:math id="inf554"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>/</mml:mi><mml:msubsup><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. We next obtain the correlation between fixed point and output weights by assuming that the bias part of the loss (<xref ref-type="disp-formula" rid="equ38">Equation 38</xref>) is kept at zero. This is reasonable because it requires only a small adaptation to the weights that leaves the variance part mostly untouched. The resulting correlation is<disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mfrac><mml:msup><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Mechanisms behind oblique solutions predicted by mean field theory.</title><p>(<bold>A–D</bold>) Mean field theory predictions as a function of positive feedback strength <inline-formula><mml:math id="inf555"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>. The dotted lines indicate <inline-formula><mml:math id="inf556"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, the minimal eigenvalue necessary to generate fixed points. (<bold>A</bold>) Norm of fixed point <inline-formula><mml:math id="inf557"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Correlation <inline-formula><mml:math id="inf558"><mml:mi>ρ</mml:mi></mml:math></inline-formula> so that <inline-formula><mml:math id="inf559"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C, D</bold>) Loss due to fluctuations for different <inline-formula><mml:math id="inf560"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula> or networks sizes <inline-formula><mml:math id="inf561"><mml:mi>N</mml:mi></mml:math></inline-formula>. Dots indicate minima. (<bold>E–G</bold>) Latent states <inline-formula><mml:math id="inf562"><mml:mi>𝜿</mml:mi></mml:math></inline-formula> of simulated networks for randomly drawn projections <inline-formula><mml:math id="inf563"><mml:mi>U</mml:mi></mml:math></inline-formula>. The symmetric matrix <inline-formula><mml:math id="inf564"><mml:mi>M</mml:mi></mml:math></inline-formula> is fixed by setting <inline-formula><mml:math id="inf565"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> as noted, <inline-formula><mml:math id="inf566"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, and demanding <inline-formula><mml:math id="inf567"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (for the mean field prediction). Dots are samples from the simulation interval <inline-formula><mml:math id="inf568"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>20</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>100</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>H–J</bold>) Histogram for the corresponding output <inline-formula><mml:math id="inf569"><mml:mi>z</mml:mi></mml:math></inline-formula>. Mean is indicated by full lines, the dashed lines indicate the target <inline-formula><mml:math id="inf570"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula>. Other parameters: <inline-formula><mml:math id="inf571"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf572"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf573"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig12-v1.tif"/></fig><p>so that increasing <inline-formula><mml:math id="inf574"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> also decreases the correlation (<xref ref-type="fig" rid="fig12">Figure 12B</xref>). Finally, we obtain an expression for the variance part of the loss only in terms of the eigenvalues of <inline-formula><mml:math id="inf575"><mml:mi>M</mml:mi></mml:math></inline-formula>:<disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We show <inline-formula><mml:math id="inf576"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id="inf577"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> for different negative feedback loop sizes <inline-formula><mml:math id="inf578"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig12">Figure 12C</xref>) and different network sizes <inline-formula><mml:math id="inf579"><mml:mi>N</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig12">Figure 12D</xref>). The first term diverges at the phase transition where the fixed point appears, <inline-formula><mml:math id="inf580"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo stretchy="false">↘</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Learning will thus push the weights away from the phase transition toward larger <inline-formula><mml:math id="inf581"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>. With such increasing <inline-formula><mml:math id="inf582"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>, the fixed point norm increases, and the fixed point rotates away from the output, decreasing the correlation. This in term emphasizes the last term, scaled by <inline-formula><mml:math id="inf583"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. The last term is reduced with increasingly negative <inline-formula><mml:math id="inf584"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula>, corresponding to the negative feedback loop that suppresses noise.</p><p>Learning can in principle strengthen this feedback further and further, <inline-formula><mml:math id="inf585"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>→</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> (apart from possible stability issues; <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>). However, as for the linear network, section Learning with noise for linear RNNs, this process takes time. We thus assume <inline-formula><mml:math id="inf586"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula> to be fixed and search for a minimum across <inline-formula><mml:math id="inf587"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>. For <inline-formula><mml:math id="inf588"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the last term in <xref ref-type="disp-formula" rid="equ53">Equation 53</xref> <italic>increases</italic> with increasing <inline-formula><mml:math id="inf589"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>: the effective feedback loop in the full, nonlinear system is weakened by saturation. The loss <inline-formula><mml:math id="inf590"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> thus has a minimum at some moderate <inline-formula><mml:math id="inf591"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>.</p><p>To illustrate the mechanisms described above, we simulated networks at different <inline-formula><mml:math id="inf592"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> and with <inline-formula><mml:math id="inf593"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>. For each <inline-formula><mml:math id="inf594"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>, we compute <inline-formula><mml:math id="inf595"><mml:mi>ρ</mml:mi></mml:math></inline-formula> according to <xref ref-type="disp-formula" rid="equ52">Equation 52</xref>. Setting <inline-formula><mml:math id="inf596"><mml:mrow><mml:msub><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, we then set the resulting symmetric <inline-formula><mml:math id="inf597"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. For each network sample, we then draw independent random projections <inline-formula><mml:math id="inf598"><mml:mrow><mml:mi>U</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We started simulations at either one of the two nonzero fixed points. For <inline-formula><mml:math id="inf599"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> just above <inline-formula><mml:math id="inf600"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, the noise pushes activity from one basin of attraction to the next (<xref ref-type="fig" rid="fig12">Figure 12C</xref>). The resulting output becomes centered around zero and independent of the initial condition for long simulation times (<xref ref-type="fig" rid="fig12">Figure 12F</xref>). For the optimal <inline-formula><mml:math id="inf601"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>, the trajectories remain close to either one fixed point (<xref ref-type="fig" rid="fig12">Figure 12F</xref>). The output forms two overlapping distributions, each closely matching the target on average (<xref ref-type="fig" rid="fig12">Figure 12I</xref>). For larger <inline-formula><mml:math id="inf602"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>, the fixed points become increasingly larger (<xref ref-type="fig" rid="fig12">Figure 12G</xref>). While this decreases the probability of leaving the basin of attraction even further, the variance along the output weights becomes larger (slightly wider histograms in <xref ref-type="fig" rid="fig12">Figure 12J</xref>). Note that the mean starts to deviate from the prediction. This is not covered by our theory and is potentially due to the linearization of the fluctuations.</p><p>All-in-all, this section revealed a potential path to oblique solutions, initiated by the large fluctuations close to a phase transition, and the interplay between the negative feedback loop and saturation. In the following section, we show that learning via gradient descent actually follows this path and that the parameters predicted by the minimum of <inline-formula><mml:math id="inf603"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> quantitatively predict solutions from learning.</p></sec><sec id="s4-9-4"><title>Oblique solutions from learning are predicted by the mean field theory</title><p>We trained neural networks on the fixed points task described above by applying gradient descent to the 2×2 matrix <inline-formula><mml:math id="inf604"><mml:mi>M</mml:mi></mml:math></inline-formula>, initialized at <inline-formula><mml:math id="inf605"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The gradients <inline-formula><mml:math id="inf606"><mml:msub><mml:mi>G</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf607"><mml:mi>M</mml:mi></mml:math></inline-formula> are equivalent to those with respect to <inline-formula><mml:math id="inf608"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>U</mml:mi><mml:mi>M</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> restricted to the subspace spanned by <inline-formula><mml:math id="inf609"><mml:mi>U</mml:mi></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="inf610"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>U</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>G</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>U</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Such a restriction is exact in the case of linear RNNs without random initial connectivity, and yields qualitative insights even for nonlinear networks with random initial connectivity (<xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>).</p><p>The output weights were set to <inline-formula><mml:math id="inf611"><mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf612"><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is the first of the two projection vectors <inline-formula><mml:math id="inf613"><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. We thus have large output weights with norm <inline-formula><mml:math id="inf614"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Trajectories were initialized at <inline-formula><mml:math id="inf615"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>𝟎</mml:mn></mml:mrow></mml:math></inline-formula>. At the beginning of a trial with target output <inline-formula><mml:math id="inf616"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the network receives one pulse <inline-formula><mml:math id="inf617"><mml:mrow><mml:mi>s</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">±</mml:mo><mml:mi>δ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> along the input weights <inline-formula><mml:math id="inf618"><mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The input direction is hence the second available direction for the rank-two connectivity. This is a sensible choice as networks without the restriction to rank-two weights would span the recurrent weights from existing directions, and a rank-two connectivity would hence also be spanned by <inline-formula><mml:math id="inf619"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf620"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>).</p><p>The loss over learning time for one network is shown in <xref ref-type="fig" rid="fig13">Figure 13A</xref>. Learning consisted of two phases: a first phase in which the network did not possess a fixed point and hence did not match the target on average, <inline-formula><mml:math id="inf621"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. At some point, <inline-formula><mml:math id="inf622"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> rapidly decreases, and <inline-formula><mml:math id="inf623"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> dominates the overall loss. During the second phase, <inline-formula><mml:math id="inf624"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> slowly decreases, with <inline-formula><mml:math id="inf625"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> hovering around zero.</p><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Mean field theory predicts learning with gradient descent.</title><p>(<bold>A–C</bold>) Learning dynamics with gradient descent for example network with <inline-formula><mml:math id="inf626"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula> neurons and with noise variance <inline-formula><mml:math id="inf627"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>A</bold>) Loss with separate bias and variance components. (<bold>B</bold>) Matrix coefficients <inline-formula><mml:math id="inf628"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The dotted lines almost identical to <inline-formula><mml:math id="inf629"><mml:msub><mml:mi>M</mml:mi><mml:mn>22</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf630"><mml:msub><mml:mi>M</mml:mi><mml:mn>11</mml:mn></mml:msub></mml:math></inline-formula> indicate the eigenvalues <inline-formula><mml:math id="inf631"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf632"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula>, respectively. The dashed line indicates <inline-formula><mml:math id="inf633"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>. (<bold>C</bold>) Fixed point norm and correlation. (<bold>D–F</bold>) Final loss, fixed point norm, and correlation for networks of different sizes <inline-formula><mml:math id="inf634"><mml:mi>N</mml:mi></mml:math></inline-formula>. Shown are mean (dots and lines) and standard deviation (shades) for five sample networks, and the prediction by the mean field theory. Gray lines indicate scaling as <inline-formula><mml:math id="inf635"><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf636"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>4</mml:mn><mml:mo separator="true">,</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. Note the log-log axes for (<bold>E, F</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig13-v1.tif"/></fig><p>The coefficients of the matrix <inline-formula><mml:math id="inf637"><mml:mi>M</mml:mi></mml:math></inline-formula> indicate the underlying learning dynamics (<xref ref-type="fig" rid="fig13">Figure 13B</xref>). The coefficient along the output weights, <inline-formula><mml:math id="inf638"><mml:msub><mml:mi>M</mml:mi><mml:mn>11</mml:mn></mml:msub></mml:math></inline-formula>, is almost identical with <inline-formula><mml:math id="inf639"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula>. It continually grows in the negative direction, unaffected by the different phases. Its time course is very similar to the time course observed for the linear system (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). In contrast, the coefficient along the input weights, <inline-formula><mml:math id="inf640"><mml:msub><mml:mi>M</mml:mi><mml:mn>22</mml:mn></mml:msub></mml:math></inline-formula>, mirrors the two phases. It grows increasingly fast in the first phase and saturates during the second phase. Its value is very close to the larger eigenvalue, <inline-formula><mml:math id="inf641"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula>. The transition between the two phases of learning happens at the phase transition of the dynamical system when the fixed point emerges for <inline-formula><mml:math id="inf642"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo><mml:mo separator="true">,</mml:mo><mml:mrow><mml:mtext/><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The off-diagonal entries show that <inline-formula><mml:math id="inf643"><mml:mi>M</mml:mi></mml:math></inline-formula> is asymmetric during the first phase and becomes symmetric later on. The coefficient <inline-formula><mml:math id="inf644"><mml:msub><mml:mi>M</mml:mi><mml:mn>12</mml:mn></mml:msub></mml:math></inline-formula> corresponds to the feedforward mode mapping the state decaying from <inline-formula><mml:math id="inf645"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> after the pulse to the output weights <inline-formula><mml:math id="inf646"><mml:mrow><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mi>𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>Tracing the fixed point norm <inline-formula><mml:math id="inf647"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula> and the correlation <inline-formula><mml:math id="inf648"><mml:mi>ρ</mml:mi></mml:math></inline-formula> over learning time shows what we expected (<xref ref-type="fig" rid="fig13">Figure 13C</xref>): The norm grows rapidly at the phase transition, which is accompanied by a decrease in the correlation. The example yields a fixed point with norm <inline-formula><mml:math id="inf649"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula> and correlation <inline-formula><mml:math id="inf650"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> for a network with <inline-formula><mml:math id="inf651"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula>. The mere numbers already suggest that we call this an oblique solution, but the theory description above is based on how these numbers scale with network size <inline-formula><mml:math id="inf652"><mml:mi>N</mml:mi></mml:math></inline-formula>. We trained networks with different <inline-formula><mml:math id="inf653"><mml:mi>N</mml:mi></mml:math></inline-formula> but otherwise the same conditions. They reached the same loss (<xref ref-type="fig" rid="fig13">Figure 13A</xref>, <inline-formula><mml:math id="inf654"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> not shown). The fixed point norm decreases weakly with <inline-formula><mml:math id="inf655"><mml:mi>N</mml:mi></mml:math></inline-formula>, with <inline-formula><mml:math id="inf656"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, for some <inline-formula><mml:math id="inf657"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig13">Figure 13E</xref>). The correlation decreases faster, yet not quite with <inline-formula><mml:math id="inf658"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig13">Figure 13F</xref>).</p><p>We compared the outcome of learning to our mean field theory. Given that <inline-formula><mml:math id="inf659"><mml:mi>M</mml:mi></mml:math></inline-formula> is approximately symmetric at the end of learning, we directly applied our results from the previous sections, again assuming <inline-formula><mml:math id="inf660"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. We fixated <inline-formula><mml:math id="inf661"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub></mml:math></inline-formula> to match the value at the end of training and computed the <inline-formula><mml:math id="inf662"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> that minimized <inline-formula><mml:math id="inf663"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ53">Equation 53</xref>. The results for the norm and correlation match those values obtained with gradient descent very closely.</p><p>Our high-level description of oblique and aligned dynamics did not involve scaling with network size (section Aligned and oblique population dynamics). However, the underlying assumption was that the activity of single neurons <inline-formula><mml:math id="inf664"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is not vanishing for large networks, i.e., <inline-formula><mml:math id="inf665"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This would imply <inline-formula><mml:math id="inf666"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf667"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This is indeed what we observed for the more complex tasks when training networks of different sizes (not shown). We note that our results for the simple fixed point task deviate weakly from this (<xref ref-type="fig" rid="fig13">Figure 13E and F</xref>). This hints at other factors pushing solutions to <inline-formula><mml:math id="inf668"><mml:mrow><mml:mi>‖</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. One such factor may be that the loss function <inline-formula><mml:math id="inf669"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>var</mml:mi></mml:mrow></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is very flat for <inline-formula><mml:math id="inf670"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> larger than the optimum (<xref ref-type="fig" rid="fig12">Figure 12C and D</xref>). If learning pushes trajectories beyond the optimum at some point, e.g., due to large updates or if the optimum shifts over learning , then the learning signal to reduce <inline-formula><mml:math id="inf671"><mml:msub><mml:mi>λ</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math></inline-formula> afterward may be too small to yield visible effects in finite learning time.</p><p>In summary, the last two sections capture the main mechanisms that drive solutions to the oblique regime in nonlinear, noise-driven networks. Although marginally small solutions seem possible for large output weights, such solutions are close to a phase transition, so the resulting system is very susceptible to noise. To accommodate robust solutions, the trajectories (here the fixed points) must increase in magnitude, while rotating away from the output weights. This further allows the implementation of a negative feedback loop that suppresses noise along the output direction. Its efficacy can be reduced by too much saturation, which in turn keeps solutions from growing ever larger. The resulting sweet spot is a network with oblique dynamics.</p></sec></sec><sec id="s4-10"><title>Mechanisms behind decoupling of neural dynamics and output</title><p>Here, we discuss in more detail the underlying mechanisms for the qualitative decoupling in oblique networks. We make a high-level argument that splits into two parts: first the possibility of decoupling in oblique, but not aligned, networks, and second a putative mechanism driving the decoupling.</p><p>For the first part, we observe that the output in oblique networks can be obtained from the leading components of the dynamics (along the PCs), but importantly also from the non-leading ones. To see this, we unpack the output in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in a slightly different way than before, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. Namely, we split the activity vector <inline-formula><mml:math id="inf672"><mml:mi>𝐱</mml:mi></mml:math></inline-formula> into its component along the leading PCs, <inline-formula><mml:math id="inf673"><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and the remaining, trailing component, <inline-formula><mml:math id="inf674"><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>trail</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. By definition of the leading PCs as the directions of largest variance, the leading component is expected to be large, and the trailing one small. Inserting this decomposition <inline-formula><mml:math id="inf675"><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>trail</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> into <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> leads to<disp-formula id="equ54"><label>(54)</label><mml:math id="m54"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with separately defined correlations <inline-formula><mml:math id="inf676"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>corr</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf677"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext/><mml:mi>trail</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext/><mml:mi>corr</mml:mi></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:mspace width="0.1667em"/><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>trail</mml:mi></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>For aligned networks, we recover the results from before: <inline-formula><mml:math id="inf678"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is small so that the output can only be generated by the leading part with large correlation <inline-formula><mml:math id="inf679"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The trailing part is unconstrained but is also not contributing to either the output or the leading dynamics, and hence not of interest.</p><p>For oblique networks, <inline-formula><mml:math id="inf680"><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is large so that the output can be generated by either of the two terms in <xref ref-type="disp-formula" rid="equ54">Equation 54</xref>. The correlation <inline-formula><mml:math id="inf681"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> has to be small because else the output would be too large. The other correlation, <inline-formula><mml:math id="inf682"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext/><mml:mi>trail</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, can be large, because non-dominant component <inline-formula><mml:math id="inf683"><mml:msub><mml:mi>𝐱</mml:mi><mml:mrow><mml:mtext/><mml:mi>trail</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is small. Both terms are potentially of the same magnitude, which means both can potentially contribute to the output. If the dominant part alone generates the output, then neural dynamics and output are coupled and the solution is similar to an aligned one (<xref ref-type="fig" rid="fig14">Figure 14</xref>, right). If, however, the non-dominant part alone generates the output, and the correlation <inline-formula><mml:math id="inf684"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext/><mml:mi>lead</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is so small that the dominant part does not contribute to the output, then the dominant part is not constrained by the task (<xref ref-type="fig" rid="fig14">Figure 14</xref>, center right). In that case, the dominant dynamics and the output can decouple qualitatively, and we may see the large variability between learners observed above.</p><fig id="fig14" position="float"><label>Figure 14.</label><caption><title>Path to oblique solutions for networks with large output weights.</title><p>Left: All networks produce the same output (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Center: Unstable solutions that arise early in learning. For lazy solutions, initial chaotic activity is slightly adapted, without changing the dynamics qualitatively. For marginal solutions, vanishingly small initial activity is replaced with very small dynamics sufficient to generate the output. Right: With more learning time and noise added during the learning process, stable, oblique solutions arise. The neural dynamics along the largest principal components (PCs) can be either decoupled from the output (center right) or coupled (right). For decoupled dynamics, the components along the largest PCs (blue subspace) differ qualitatively from those generating the output (same as <xref ref-type="fig" rid="fig1">Figure 1B</xref>, bottom). The dynamics along the largest PCs inherit task-unrelated components from the initial dynamics or randomness during learning. Another possibility are oblique, but coupled dynamics (right). Such solutions don't inherit task-unrelated components of the dynamics at initialization. They are qualitatively similar to aligned solutions, and the output is generated by a small projection of the output weights onto the largest PCs (dashed orange arrow).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-fig14-v1.tif"/></fig><p>The existence of decoupled solutions for oblique dynamics leads to the second question: Why and when do such solutions arise? Understanding this requires more detailed insights into the learning process. Roughly speaking, learning in the oblique regime has two opposite goals: first, to generate the desired output as fast as possible, and hence to induce changes to activity that are as small as possible (section Analysis of solutions under noiseless conditions); and second, to generate solutions that are robust and stable, and hence to induce changes in activity that are large enough to not be disrupted by noise (section Oblique solutions arise for noisy, nonlinear systems). During the process of learning, small, unstable solutions appear first (<xref ref-type="fig" rid="fig14">Figure 14</xref>, center). These may be highly variable, depending strongly on random initialization or other randomness experienced during learning. Such solutions then slowly solidify into stable solutions, that may inherit the variability of the early solutions (<xref ref-type="fig" rid="fig14">Figure 14</xref>, right).</p><p>The process of how learning transforms small, unstable solutions to larger, robust ones is analyzed in section Learning with noise for linear RNNs and Oblique solutions arise for noisy, nonlinear systems. The details of how this process introduces variability between learners, however, are not discussed there and left for future work.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93060-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The code to reproduce the results can be accessed via <ext-link ext-link-type="uri" xlink:href="https://github.com/frschu/aligned_oblique_in_rnns/">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib70">Schuessler, 2024</xref>). No new data has been generated.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Two-target cycling task, primate motor cortex</data-title><source>Mendeley Data</source><pub-id pub-id-type="doi">10.17632/tfcwp8bp5j.1</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Sterkin</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dynamics of ongoing activity: explanation of the large variability in evoked cortical responses</article-title><source>Science</source><volume>273</volume><fpage>1868</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1126/science.273.5283.1868</pub-id><pub-id pub-id-type="pmid">8791593</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Instantaneous modulation of gamma oscillation frequency by balancing excitation with inhibition</article-title><source>Neuron</source><volume>62</volume><fpage>566</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.04.027</pub-id><pub-id pub-id-type="pmid">19477157</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recurrent neural networks as versatile tools of neuroscience research</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.06.003</pub-id><pub-id pub-id-type="pmid">28668365</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shaping dynamics with multiple populations in low-rank recurrent networks</article-title><source>Neural Computation</source><volume>33</volume><fpage>1572</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01381</pub-id><pub-id pub-id-type="pmid">34496384</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Blanc</surname><given-names>G</given-names></name><name><surname>Gupta</surname><given-names>N</given-names></name><name><surname>Valiant</surname><given-names>G</given-names></name><name><surname>Valiant</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process</article-title><conf-name>Conference on Learning Theory</conf-name><fpage>483</fpage><lpage>513</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bondanelli</surname><given-names>G</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Coding with transient trajectories in recurrent neural networks</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007655</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007655</pub-id><pub-id pub-id-type="pmid">32053594</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname><given-names>DV</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>113</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nrn2558</pub-id><pub-id pub-id-type="pmid">19145235</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-neuron perturbations reveal feature-specific competition in V1</article-title><source>Nature</source><volume>567</volume><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0997-6</pub-id><pub-id pub-id-type="pmid">30842660</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chizat</surname><given-names>L</given-names></name><name><surname>Oyallon</surname><given-names>E</given-names></name><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>On lazy training in differentiable programming</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>2937</fpage><lpage>2947</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical preparatory activity: representation of movement or first cog in a dynamical machine?</article-title><source>Neuron</source><volume>68</volume><fpage>387</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.015</pub-id><pub-id pub-id-type="pmid">21040842</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degenhart</surname><given-names>AD</given-names></name><name><surname>Bishop</surname><given-names>WE</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stabilization of a brain-computer interface via the alignment of low-dimensional spaces of neural activity</article-title><source>Nature Biomedical Engineering</source><volume>4</volume><fpage>672</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1038/s41551-020-0542-9</pub-id><pub-id pub-id-type="pmid">32313100</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Role of Population Structure in Computations through Neural Dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.03.185942</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Economo</surname><given-names>MN</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attractor dynamics gate cortical information flow during decision-making</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>843</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00840-6</pub-id><pub-id pub-id-type="pmid">33875892</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>T</given-names></name><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Orthogonal representations for robust context-dependent task performance in brains and neural networks</article-title><source>Neuron</source><volume>110</volume><fpage>1258</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.01.005</pub-id><pub-id pub-id-type="pmid">35085492</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galgali</surname><given-names>AR</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Residual dynamics resolves recurrent contributions to neural computation</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>326</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01230-2</pub-id><pub-id pub-id-type="pmid">36635498</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Naufel</surname><given-names>SN</given-names></name><name><surname>Ethier</surname><given-names>C</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical population activity within a preserved neural manifold underlies multiple motor behaviors</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4233</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06560-z</pub-id><pub-id pub-id-type="pmid">30315158</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Long-term stability of cortical population dynamics underlying consistent behavior</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>260</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0555-4</pub-id><pub-id pub-id-type="pmid">31907438</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A theory of multineuronal dimensionality, dynamics and measurement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/214262</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiger</surname><given-names>M</given-names></name><name><surname>Spigler</surname><given-names>S</given-names></name><name><surname>Jacot</surname><given-names>A</given-names></name><name><surname>Wyart</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Disentangling feature and lazy training in deep neural networks</article-title><source>Journal of Statistical Mechanics</source><volume>2020</volume><elocation-id>113301</elocation-id><pub-id pub-id-type="doi">10.1088/1742-5468/abc4de</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning by neural reassociation</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>607</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0095-3</pub-id><pub-id pub-id-type="pmid">29531364</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><volume>82</volume><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id><pub-id pub-id-type="pmid">24945778</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Lund</surname><given-names>PJ</given-names></name><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Constraints on neural redundancy</article-title><source>eLife</source><volume>7</volume><elocation-id>e36774</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36774</pub-id><pub-id pub-id-type="pmid">30109848</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jacot</surname><given-names>A</given-names></name><name><surname>Gabriel</surname><given-names>F</given-names></name><name><surname>Hongler</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural tangent kernel: convergence and generalization in neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8571</fpage><lpage>8580</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jacot</surname><given-names>A</given-names></name><name><surname>Ged</surname><given-names>F</given-names></name><name><surname>Şimşek</surname><given-names>B</given-names></name><name><surname>Hongler</surname><given-names>C</given-names></name><name><surname>Gabriel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2106.15933">https://arxiv.org/abs/2106.15933</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kadmon</surname><given-names>J</given-names></name><name><surname>Timcheck</surname><given-names>J</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predictive Coding in Balanced Neural Networks with Noise, Chaos and Delays</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>16677</fpage><lpage>16688</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Sadabadi</surname><given-names>MS</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Optimal anticipatory control as A theory of motor preparation: A thalamo-cortical circuit model</article-title><source>Neuron</source><volume>109</volume><fpage>1567</fpage><lpage>1581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.009</pub-id><pub-id pub-id-type="pmid">33789082</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical activity in the null space: permitting preparation without movement</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3643</pub-id><pub-id pub-id-type="pmid">24487233</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>CM</given-names></name><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Chow</surname><given-names>CC</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Darshan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Distributing task-related neural activity across a cortical network through task-independent connections</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>2851</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-38529-y</pub-id><pub-id pub-id-type="pmid">37202424</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1412.6980">https://arxiv.org/pdf/1412.6980</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kloeden</surname><given-names>PE</given-names></name><name><surname>Platen</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Numerical Solution of Stochastic Differential Equations</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-662-12616-5</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Arora</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What Happens after SGD Reaches Zero Loss?--A Mathematical Framework</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.06914">https://arxiv.org/abs/2110.06914</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><name><surname>Belkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the linearity of large non-linear models: when and why the tangent kernel is constant</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>YH</given-names></name><name><surname>Baratin</surname><given-names>A</given-names></name><name><surname>Cornford</surname><given-names>J</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Lajoie</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>How Connectivity Structure Shapes Rich and Lazy Learning in Neural Circuits</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2310.08513">https://arxiv.org/abs/2310.08513</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logiaco</surname><given-names>L</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Escola</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Thalamic control of cortical dynamics in a model of flexible motor sequencing</article-title><source>Cell Reports</source><volume>35</volume><elocation-id>109090</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109090</pub-id><pub-id pub-id-type="pmid">34077721</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Universality and individuality in neural dynamics across large populations of recurrent networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>15629</fpage><lpage>15641</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname><given-names>JG</given-names></name><name><surname>O’Doherty</surname><given-names>JE</given-names></name><name><surname>Cardoso</surname><given-names>MMB</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Superior arm-movement decoding from cortex with a new, unsupervised-learning algorithm</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>026010</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa9e95</pub-id><pub-id pub-id-type="pmid">29192609</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Linking connectivity, dynamics, and computations in low-rank recurrent neural networks</article-title><source>Neuron</source><volume>99</volume><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.003</pub-id><pub-id pub-id-type="pmid">30057201</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>S</given-names></name><name><surname>Montanari</surname><given-names>A</given-names></name><name><surname>Nguyen</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A mean field view of the landscape of two-layer neural networks</article-title><source>PNAS</source><volume>115</volume><fpage>E7665</fpage><lpage>E7671</lpage><pub-id pub-id-type="doi">10.1073/pnas.1806579115</pub-id><pub-id pub-id-type="pmid">30054315</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Degenhart</surname><given-names>AD</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>New neural activity patterns emerge with long-term learning</article-title><source>PNAS</source><volume>116</volume><fpage>15210</fpage><lpage>15215</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820296116</pub-id><pub-id pub-id-type="pmid">31182595</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Lampl</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>535</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1038/nn.2105</pub-id><pub-id pub-id-type="pmid">18376400</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Duncker</surname><given-names>L</given-names></name><name><surname>Goo</surname><given-names>W</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Vyas</surname><given-names>S</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Diester</surname><given-names>I</given-names></name><name><surname>Ramakrishnan</surname><given-names>C</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Direct Neural Perturbations Reveal a Dynamical Mechanism for Robust Computation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.12.16.520768</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Tang</surname><given-names>VD</given-names></name><name><surname>Aoi</surname><given-names>MC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A New Theoretical Framework Jointly Explains Behavioral and Neural Variability across Subjects Performing Flexible Decision-Making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.11.28.518207</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Automatic differentiation in pytorch</data-title><version designator="0.1">0.1</version><source>PyTorch</source><ext-link ext-link-type="uri" xlink:href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: Machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Zoltowski</surname><given-names>DM</given-names></name><name><surname>Wu</surname><given-names>A</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>O’Doherty</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural latents benchmark ’21: evaluating latent variable models of neural population activity</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Preciado</surname><given-names>VM</given-names></name><name><surname>Rahimian</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Controllability Gramian spectra of random networks</article-title><conf-name>2016 American Control Conference</conf-name><fpage>3874</fpage><lpage>3879</lpage><pub-id pub-id-type="doi">10.1109/ACC.2016.7525517</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rajeswaran</surname><given-names>P</given-names></name><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Lajoie</surname><given-names>G</given-names></name><name><surname>Orsborn</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Assistive Sensory-Motor Perturbations Influence Learned Neural Representations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.03.20.585972</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratzon</surname><given-names>A</given-names></name><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Representational drift as a result of implicit regularization</article-title><source>eLife</source><volume>12</volume><elocation-id>RP90069</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.90069</pub-id><pub-id pub-id-type="pmid">38695551</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivkind</surname><given-names>A</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Local dynamics in trained recurrent neural networks</article-title><source>Physical Review Letters</source><volume>118</volume><elocation-id>258101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.118.258101</pub-id><pub-id pub-id-type="pmid">28696758</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokni</surname><given-names>U</given-names></name><name><surname>Richardson</surname><given-names>AG</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Motor learning with unstable neural representations</article-title><source>Neuron</source><volume>54</volume><fpage>653</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.04.030</pub-id><pub-id pub-id-type="pmid">17521576</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title><source>Nature</source><volume>399</volume><fpage>470</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1038/20939</pub-id><pub-id pub-id-type="pmid">10365959</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>LE</given-names></name><name><surname>Dalgleish</surname><given-names>HWP</given-names></name><name><surname>Nutbrown</surname><given-names>R</given-names></name><name><surname>Gauld</surname><given-names>OM</given-names></name><name><surname>Herrmann</surname><given-names>D</given-names></name><name><surname>Fişek</surname><given-names>M</given-names></name><name><surname>Packer</surname><given-names>AM</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>All-optical interrogation of neural circuits in behaving mice</article-title><source>Nature Protocols</source><volume>17</volume><fpage>1579</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1038/s41596-022-00691-w</pub-id><pub-id pub-id-type="pmid">35478249</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Bittner</surname><given-names>SR</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>London</surname><given-names>BM</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Miri</surname><given-names>A</given-names></name><name><surname>Marshall</surname><given-names>NJ</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor cortex embeds muscle-like commands in an untangled population response</article-title><source>Neuron</source><volume>97</volume><fpage>953</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.004</pub-id><pub-id pub-id-type="pmid">29398358</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Khajeh</surname><given-names>R</given-names></name><name><surname>Bittner</surname><given-names>SR</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural trajectories in the supplementary motor area and motor cortex exhibit distinct geometries, compatible with different classes of computation</article-title><source>Neuron</source><volume>107</volume><fpage>745</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.05.020</pub-id><pub-id pub-id-type="pmid">32516573</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural constraints on learning</article-title><source>Nature</source><volume>512</volume><fpage>423</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1038/nature13665</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>S</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Towards the neural population doctrine</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>103</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.02.002</pub-id><pub-id pub-id-type="pmid">30877963</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>S</given-names></name><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Cunningham</surname><given-names>J</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Motor cortex activity across movement speeds is predicted by network-level strategies for generating muscle activity</article-title><source>eLife</source><volume>11</volume><elocation-id>e67620</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67620</pub-id><pub-id pub-id-type="pmid">35621264</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>KE</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cortical control of virtual self-motion using task-specific subspaces</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>220</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2687-20.2021</pub-id><pub-id pub-id-type="pmid">34716229</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuecker</surname><given-names>J</given-names></name><name><surname>Goedeke</surname><given-names>S</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optimal sequence memory in driven random networks</article-title><source>Physical Review X</source><volume>8</volume><elocation-id>041029</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.8.041029</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuessler</surname><given-names>F</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Dynamics of random recurrent networks with correlated low-rank structure</article-title><source>Physical Review Research</source><volume>2</volume><elocation-id>013111</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevResearch.2.013111</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schuessler</surname><given-names>F</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The interplay between randomness and structure during learning in rnns</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>13352</fpage><lpage>13362</lpage></element-citation></ref><ref id="bib70"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schuessler</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Aligned_oblique_in_rnns</data-title><version designator="swh:1:rev:9d4c813feda9f772aade3026c1f0893ddb81dd8d">swh:1:rev:9d4c813feda9f772aade3026c1f0893ddb81dd8d</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a8b33f3aae17aef6e6738a7a3ab9699859668708;origin=https://github.com/frschu/aligned_oblique_in_rnns;visit=swh:1:snp:bcdf7fb28913a4a5b40bce8ed92993d0ce285405;anchor=swh:1:rev:9d4c813feda9f772aade3026c1f0893ddb81dd8d">https://archive.softwareheritage.org/swh:1:dir:a8b33f3aae17aef6e6738a7a3ab9699859668708;origin=https://github.com/frschu/aligned_oblique_in_rnns;visit=swh:1:snp:bcdf7fb28913a4a5b40bce8ed92993d0ce285405;anchor=swh:1:rev:9d4c813feda9f772aade3026c1f0893ddb81dd8d</ext-link></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Crisanti</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Chaos in random neural networks</article-title><source>Physical Review Letters</source><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id><pub-id pub-id-type="pmid">10039285</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>High-dimensional geometry of population responses in visual cortex</article-title><source>Nature</source><volume>571</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1346-5</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Susman</surname><given-names>L</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Brenner</surname><given-names>N</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Quality of internal representation shapes learning performance in feedback neural networks</article-title><source>Physical Review Research</source><volume>3</volume><elocation-id>013176</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevResearch.3.013176</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural circuits as computational dynamical systems</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.008</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds A naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Talluri</surname><given-names>BC</given-names></name><name><surname>Kang</surname><given-names>I</given-names></name><name><surname>Lazere</surname><given-names>A</given-names></name><name><surname>Quinn</surname><given-names>KR</given-names></name><name><surname>Kaliss</surname><given-names>N</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name><name><surname>Nienborg</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Activity in Primate Visual Cortex Is Minimally Driven by Spontaneous Movements</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.09.08.507006</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>E</given-names></name><name><surname>Dabholkar</surname><given-names>K</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Charting and navigating the space of solutions for recurrent neural networks</article-title><conf-name>Advances in Neural Information Processing Systems 34</conf-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname><given-names>S</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computation through neural population dynamics</article-title><source>Annual Review of Neuroscience</source><volume>43</volume><fpage>249</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-092619-094115</pub-id><pub-id pub-id-type="pmid">32640928</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>ZA</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Not Everything, Not Everywhere, Not All at Once: A Study of Brain-Wide Encoding of Movement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.06.08.544257</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Avansino</surname><given-names>DT</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>High-performance brain-to-text communication via handwriting</article-title><source>Nature</source><volume>593</volume><fpage>249</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03506-2</pub-id><pub-id pub-id-type="pmid">33981047</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>A</given-names></name><name><surname>Kunz</surname><given-names>E</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Linderman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Generalized shape metrics on neural representations</article-title><conf-name>Advances in Neural Information Processing Systems 34</conf-name></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>K</given-names></name><name><surname>Cheng</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adjoint methods of sensitivity analysis for Lyapunov equation</article-title><source>Structural and Multidisciplinary Optimization</source><volume>53</volume><fpage>225</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1007/s00158-015-1323-z</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Hu</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Feature Learning in Infinite-Width Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.14522">https://arxiv.org/abs/2011.14522</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>N</given-names></name><name><surname>Tang</surname><given-names>C</given-names></name><name><surname>Tu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Stochastic gradient descent introduces an effective landscape-dependent regularization favoring flat solutions</article-title><source>Physical Review Letters</source><volume>130</volume><elocation-id>237101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.130.237101</pub-id><pub-id pub-id-type="pmid">37354404</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Supplementary information</title><sec sec-type="appendix" id="s8-1"><title>A.1. Linear approximation for lazy learning</title><p>We numerically investigated the linearization, <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>, of the output trajectory <inline-formula><mml:math id="inf685"><mml:mrow><mml:mi>z</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in weight changes <inline-formula><mml:math id="inf686"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>. For this, we used the sine wave task and the same configuration as for the lazy network in <xref ref-type="fig" rid="fig9">Figure 9C</xref>. In <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>, we plot the output of the full network after inserting the linear solution <inline-formula><mml:math id="inf687"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>lin</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The fit to the training points got more accurate with increasing network size. However, it did not reach zero error for the networks shown. In general, the error increased with increasing trial time. This indicates that the linear approximation was not valid. This is in line with the nature of the dynamical system: Trajectories become more nonlinear in <inline-formula><mml:math id="inf688"><mml:mi>W</mml:mi></mml:math></inline-formula> with increasing time. The loss on the training set (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>) summarizes the observations seen before: The linear solution became more accurate with increasing network size.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Solution to linearized network dynamics in the lazy regime.</title><p>(<bold>A</bold>) Network output for weight changes <inline-formula><mml:math id="inf689"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>lin</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> obtained from linearized dynamics for different network sizes. Each plot shows 10 different networks (one example in bold). Target points in purple. (<bold>B</bold>) Output for networks trained with gradient descent (GD) from the same initial conditions as those above. (<bold>C</bold>) Loss on the training set for the linear (lin) and GD solutions. (<bold>D</bold>) Frobenius norms of weight changes <inline-formula><mml:math id="inf690"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> of the linear and GD solutions, as well as of the difference between the two. <inline-formula><mml:math id="inf691"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>lin</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext/><mml:mi>GD</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (dashed green). Gray and black dashed lines for comparison of scales.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig1-v1.tif"/></fig><p>How close was the linear solution to the one found by GD? To test this, we optimized the network with GD starting the same initial condition <inline-formula><mml:math id="inf692"><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. The output of this network is shown in orange in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>. As seen before (<xref ref-type="fig" rid="fig9">Figure 9C</xref>) training points were met, but the dynamics after the last training points remained chaotic. We then quantified the norms of the two solutions and the difference between both. As shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D</xref>, both solutions had similar norm, which decays as <inline-formula><mml:math id="inf693"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. The difference decayed faster, as <inline-formula><mml:math id="inf694"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. This indicates that the linearized system can give a good insight into the solution found by training with GD, despite the inaccuracies of the linear approximation. In particular, the main features (scaling, unstable solutions, no extrapolation) are the same for the linear and the gradient descent solutions.</p></sec><sec sec-type="appendix" id="s8-2"><title>A.2 Details of linear model: bias only: lazy learning</title><p>We consider learning for the linear, input-driven model, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>.</p><p>We start with the bias part, which is equivalent to the noise-free model (<xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>). Disregarding initial transients, the average output is <inline-formula><mml:math id="inf695"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mspace width="-0.1667em" style="margin-left:-0.1667em;"/><mml:mi>B</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf696"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The gradient of the loss <inline-formula><mml:math id="inf697"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is then<disp-formula id="equ55"><label>(55)</label><mml:math id="m55"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We make the ansatz of small weight changes and hence linearize the output in weights (like in feedforward networks, <xref ref-type="bibr" rid="bib36">Liu et al., 2020</xref>):<disp-formula id="equ56"><label>(56)</label><mml:math id="m56"><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf698"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> the gradient of the output at initialization,<disp-formula id="equ57"><label>(57)</label><mml:math id="m57"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">|</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf699"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The dot-product signifies sum over all entries, <inline-formula><mml:math id="inf700"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⋅</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for two matrices <inline-formula><mml:math id="inf701"><mml:mrow><mml:mi>A</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>. Because the output is linear, the weight changes are spanned by the gradient, <inline-formula><mml:math id="inf702"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>/</mml:mi><mml:mi>‖</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>‖</mml:mi></mml:mrow></mml:math></inline-formula>. Insertion into gradient descent dynamics yields<disp-formula id="equ58"><label>(58)</label><mml:math id="m58"><mml:mrow><mml:mrow><mml:mover><mml:mi>b</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>η</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ59"><label>(59)</label><mml:math id="m59"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We used that <inline-formula><mml:math id="inf703"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>𝐯</mml:mi><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>‖</mml:mi><mml:msubsup><mml:mi>B</mml:mi><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>𝐯</mml:mi><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mi>‖</mml:mi><mml:mi>𝐯</mml:mi><mml:msup><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> for any vector <inline-formula><mml:math id="inf704"><mml:mi>𝐯</mml:mi></mml:math></inline-formula> independent of <inline-formula><mml:math id="inf705"><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (which is the case for the input and output vectors) (<xref ref-type="bibr" rid="bib69">Schuessler et al., 2020b</xref>). For initial condition <inline-formula><mml:math id="inf706"><mml:mrow><mml:mi>b</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the solution to <xref ref-type="disp-formula" rid="equ58">Equation 58</xref> is<disp-formula id="equ60"><label>(60)</label><mml:math id="m60"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>m</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>η</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The corresponding output changes are <inline-formula><mml:math id="inf707"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>. Because <inline-formula><mml:math id="inf708"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>∼</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, the output converges to the target in <inline-formula><mml:math id="inf709"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>η</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> learning steps. The resulting weights are<disp-formula id="equ61"><label>(61)</label><mml:math id="m61"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the hats indicate normalized vectors. Consistent with linearization, the norm (both Frobenius and operator) of the changes is small: <inline-formula><mml:math id="inf710"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mi>‖</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p><p>To connect with the following paragraph, we consider the case <inline-formula><mml:math id="inf711"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf712"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>. This is for reference, and <xref ref-type="fig" rid="fig10">Figure 10</xref>; see details below. We express the results in the orthonormalized bases introduced below:<disp-formula id="equ62"><label>(62)</label><mml:math id="m62"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>⊥</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ63"><label>(63)</label><mml:math id="m63"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mi>η</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The corresponding activity changes are <inline-formula><mml:math id="inf713"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.1667em"/><mml:mo separator="true">,</mml:mo></mml:mrow></mml:math></inline-formula> with norm <inline-formula><mml:math id="inf714"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mspace width="0.1667em"/><mml:mi>.</mml:mi></mml:mrow></mml:math></inline-formula> The output changes are <inline-formula><mml:math id="inf715"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mspace width="0.1667em"/><mml:mo separator="true">,</mml:mo></mml:mrow></mml:math></inline-formula> so that the loss is <inline-formula><mml:math id="inf716"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext/><mml:mi>bias</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:mn>4</mml:mn><mml:mi>N</mml:mi><mml:mi>η</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.1667em"/><mml:mi>.</mml:mi></mml:mrow></mml:math></inline-formula></p></sec><sec sec-type="appendix" id="s8-3"><title>A.3 Details of linear model: bias and variance combined</title><p>We now combine results from the previous section, concerning the bias, with results from the Methods, concerning the variance, to derive the full learning dynamics of the linear model. For the case of no initial connectivity, <inline-formula><mml:math id="inf717"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the gradients for both the bias and the variance component, <xref ref-type="disp-formula" rid="equ23 equ55">Equations 23 and 55</xref>, are spanned by the output and input vector. By orthonormalizing, we define <inline-formula><mml:math id="inf718"><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf719"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The weights are then constrained to <inline-formula><mml:math id="inf720"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>M</mml:mi><mml:msup><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. For ease of writing, we define the vectors <inline-formula><mml:math id="inf721"><mml:mrow><mml:msub><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>0</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf722"><mml:mrow><mml:msub><mml:mi>𝐯</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo separator="true">,</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:msup><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf723"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mspace width="-0.1667em" style="margin-left:-0.1667em;"/><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the random, <inline-formula><mml:math id="inf724"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> correlation between input and output vector. As before, the norms of input and output vectors are <inline-formula><mml:math id="inf725"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf726"><mml:mrow><mml:mi>‖</mml:mi><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mi>‖</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>The projection of the deterministic gradient is then<disp-formula id="equ64"><label>(64)</label><mml:math id="m64"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with deterministic output<disp-formula id="equ65"><label>(65)</label><mml:math id="m65"><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and projection <inline-formula><mml:math id="inf727"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For the stochastic part, we need to solve the two Lyapunov <xref ref-type="disp-formula" rid="equ21 equ24">Equations 21 and 24</xref>. They reduce to low-D versions after projection. For the first one, we have<disp-formula id="equ66"><label>(66)</label><mml:math id="m66"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and the projection solves the 2D Lyapunov equation<disp-formula id="equ67"><label>(67)</label><mml:math id="m67"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf728"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similarly, the low-D version of the second Lyapunov <xref ref-type="disp-formula" rid="equ24">Equation 24</xref> is obtained by defining <inline-formula><mml:math id="inf729"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mspace width="0.1667em"/><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:msub><mml:msup><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false" class="tml-capshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, with<disp-formula id="equ68"><label>(68)</label><mml:math id="m68"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The full learning dynamics for gradient flow are then given by<disp-formula id="equ69"><label>(69)</label><mml:math id="m69"><mml:mrow><mml:mrow><mml:mover><mml:mi>M</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf730"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and<disp-formula id="equ70"><label>(70)</label><mml:math id="m70"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For a small output scale, the deterministic part is order one, while the stochastic one is order <inline-formula><mml:math id="inf731"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> (through <inline-formula><mml:math id="inf732"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:math></inline-formula>). Assuming <inline-formula><mml:math id="inf733"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, learning thus reaches an arbitrary small order-one loss after <inline-formula><mml:math id="inf734"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> updates, with noise adding a <inline-formula><mml:math id="inf735"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> stochastic loss, and noise-driven learning equally adding order <inline-formula><mml:math id="inf736"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> deviations to the entries of <inline-formula><mml:math id="inf737"><mml:mi>M</mml:mi></mml:math></inline-formula>. Hence, learning remains unaffected by the noise unless it is continued for <inline-formula><mml:math id="inf738"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>η</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> steps.</p><p>For a large output scale, the deterministic part is fast but only leads to <inline-formula><mml:math id="inf739"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> entries. In such a situation, the stochastic part can be expanded in orders of <inline-formula><mml:math id="inf740"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. To leading order, it yields dynamics along the first entry of <inline-formula><mml:math id="inf741"><mml:mi>M</mml:mi></mml:math></inline-formula>.</p><p>In detail: For a large readout, <inline-formula><mml:math id="inf742"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we expand <inline-formula><mml:math id="inf743"><mml:mi>M</mml:mi></mml:math></inline-formula> to first order in <inline-formula><mml:math id="inf744"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ71"><label>(71)</label><mml:math id="m71"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The leading order of the output is<disp-formula id="equ72"><label>(72)</label><mml:math id="m72"><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ϵ</mml:mi></mml:mfrac><mml:mfrac><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which implies that <inline-formula><mml:math id="inf745"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> throughout learning. Under this assumption, we have<disp-formula id="equ73"><label>(73)</label><mml:math id="m73"><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Furthermore, the deterministic gradient is<disp-formula id="equ74"><label>(74)</label><mml:math id="m74"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>ϵ</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This component of learning converges on the order of <inline-formula><mml:math id="inf746"><mml:mrow><mml:mi>η</mml:mi><mml:mi>/</mml:mi><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> learning steps due to the prefactor <inline-formula><mml:math id="inf747"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> and the fact that <inline-formula><mml:math id="inf748"><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> enters <inline-formula><mml:math id="inf749"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> at order one. The higher order terms therefore only lead to <inline-formula><mml:math id="inf750"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> corrections.</p><p>The stochastic part of the gradient simplifies by assuming <inline-formula><mml:math id="inf751"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The latter two are justified self-consistently because neither the deterministic nor the stochastic part contribute to <inline-formula><mml:math id="inf752"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf753"><mml:mi>d</mml:mi></mml:math></inline-formula> at order one if we start with <inline-formula><mml:math id="inf754"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. With this assumption, we have<disp-formula id="equ75"><label>(75)</label><mml:math id="m75"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Consistent with our assumption, we only have order-one growth for the entry <inline-formula><mml:math id="inf755"><mml:mi>a</mml:mi></mml:math></inline-formula>. The resulting dynamics are<disp-formula id="equ76"><label>(76)</label><mml:math id="m76"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mi>η</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf756"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the number of learning steps. Further, because the deterministic part is so much faster, we can assume that the second entry, <inline-formula><mml:math id="inf757"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, is always constrained by the target. Namely, from <xref ref-type="disp-formula" rid="equ73">Equation 73</xref>, we have<disp-formula id="equ77"><label>(77)</label><mml:math id="m77"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Lastly, the first-order corrections for the other entries all vanish due to vanishing initial conditions:<disp-formula id="equ78"><label>(78)</label><mml:math id="m78"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The loss due to the stochastic part is then<disp-formula id="equ79"><label>(79)</label><mml:math id="m79"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mi>η</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>while the deterministic part is kept at <inline-formula><mml:math id="inf758"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>During learning, the average states change only very little. To see this, we express the connectivity as <inline-formula><mml:math id="inf759"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>𝐰</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mrow><mml:mtext/><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>𝐰</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> with<disp-formula id="equ80"><label>(80)</label><mml:math id="m80"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and orthonormalized input weights<disp-formula id="equ81"><label>(81)</label><mml:math id="m81"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The output is then<disp-formula id="equ82"><label>(82)</label><mml:math id="m82"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The initial states are <inline-formula><mml:math id="inf760"><mml:mrow><mml:msub><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>𝐰</mml:mi><mml:mrow><mml:mtext/><mml:mi>in</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, so the changes are<disp-formula id="equ83"><label>(83)</label><mml:math id="m83"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the scalar part is also the norm. Thus, <inline-formula><mml:math id="inf761"><mml:mrow><mml:mi>‖</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mover><mml:mi>𝐱</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>‖</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, which is small, because the entries are <inline-formula><mml:math id="inf762"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Paradoxically, the norm does not change with learning time. Essentially, once the lazy learning takes place at the beginning of learning, the norm does not change anymore (to leading order).</p></sec><sec sec-type="appendix" id="s8-4"><title>A.4 Details of nonlinear autonomous system with noise</title><p>This section contains detailed derivations for the average and fluctuations of the latent projection <inline-formula><mml:math id="inf763"><mml:mi>𝜿</mml:mi></mml:math></inline-formula>. The theoretical treatment of the network dynamics is very similar to that for a network with only a negative feedback loop for ‘balance’ by <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>. What is added here is a colored noise term that enters via the variance <inline-formula><mml:math id="inf764"><mml:msubsup><mml:mi>σ</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the orthogonal dynamics. This is dependent on a nonzero fixed point (not present in <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>).</p><p>The dynamics of the latent variable are<disp-formula id="equ84"><label>(84)</label><mml:math id="m84"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>That is, the low-D variable is driven autonomously by itself, but externally by the high-D orthogonal part <inline-formula><mml:math id="inf765"><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msub></mml:math></inline-formula>. To resolve this into low-D dynamics only, we apply two averages: the trial-conditioned average, which is over the noise <inline-formula><mml:math id="inf766"><mml:mi>𝝃</mml:mi></mml:math></inline-formula>, and denoted by the bar, <inline-formula><mml:math id="inf767"><mml:mrow><mml:menclose notation="top" class="tml-overline"><mml:mo lspace="0em" rspace="0em">⋅</mml:mo></mml:menclose><mml:mo>=</mml:mo><mml:msub><mml:mi>𝔼</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mo form="prefix" stretchy="false">⋅</mml:mo><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>; and the average over the statistics of <inline-formula><mml:math id="inf768"><mml:mi>U</mml:mi></mml:math></inline-formula>, which arises naturally from the projection on <inline-formula><mml:math id="inf769"><mml:mi>U</mml:mi></mml:math></inline-formula> (a population average). We start with the latter, writing<disp-formula id="equ85"><label>(85)</label><mml:math id="m85"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>One can show that the error is small, <inline-formula><mml:math id="inf770"><mml:mrow><mml:msub><mml:mi>𝔼</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf771"><mml:mrow><mml:msub><mml:mi>𝔼</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. For the average, we apply partial integration as in the noise-free case, <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>, and arrive at<disp-formula id="equ86"><label>(86)</label><mml:math id="m86"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the average on the right-hand side is over the standard normal variable <inline-formula><mml:math id="inf772"><mml:mi>u</mml:mi></mml:math></inline-formula>. In contrast to the noise-free case, the variance is now comprised of two terms,<disp-formula id="equ87"><label>(87)</label><mml:math id="m87"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf773"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the population average of <inline-formula><mml:math id="inf774"><mml:mrow><mml:msub><mml:mi>𝐱</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We included the time index here to emphasize that both variables still contain fluctuations around the trial-conditioned average.</p><p>We now split the term (<xref ref-type="disp-formula" rid="equ85">Equation 85</xref>) into average and fluctuations around this. We self-consistently assume that all fluctuations, denoted with a <inline-formula><mml:math id="inf775"><mml:mi>δ</mml:mi></mml:math></inline-formula>, are small, which allows us to linearize around the respective averages. We have<disp-formula id="equ88"><label>(88)</label><mml:math id="m88"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mover><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We only keep terms up to order <inline-formula><mml:math id="inf776"><mml:mrow><mml:mi>O</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The fluctuations of the error are <inline-formula><mml:math id="inf777"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝝐</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> and hence discarded. By linearization, we further have <inline-formula><mml:math id="inf778"><mml:mrow><mml:menclose notation="top" class="tml-overline"><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>u</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">⟩</mml:mo></mml:mrow></mml:menclose><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:menclose notation="top" class="tml-overline"><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:menclose><mml:mi>u</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, with<disp-formula id="equ89"><label>(89)</label><mml:math id="m89"><mml:mrow><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The orthogonal part is an Ornstein-Uhlenbeck process with steady-state covariance<disp-formula id="equ90"><label>(90)</label><mml:math id="m90"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the variance is inherited from the white noise, <inline-formula><mml:math id="inf779"><mml:mrow><mml:menclose notation="top" class="tml-overline"><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⟂</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:menclose><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p><p>We take the average over the latent dynamics <xref ref-type="disp-formula" rid="equ84">Equation 84</xref>. Apart from an initial transient, which we discard, this leaves us with a fixed point<disp-formula id="equ91"><label>(91)</label><mml:math id="m91"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mover><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We now discuss the fluctuations in <xref ref-type="disp-formula" rid="equ88">Equation 88</xref>. There are two sources of fluctuations, <inline-formula><mml:math id="inf780"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf781"><mml:mrow><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, so that we have<disp-formula id="equ92"><label>(92)</label><mml:math id="m92"><mml:mrow><mml:mi>δ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>All derivatives are evaluated at the averages (we discard the bar in the following lines to keep notation at bay). We further compute<disp-formula id="equ93"><label>(93)</label><mml:math id="m93"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>u</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>‴</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ94"><label>(94)</label><mml:math id="m94"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ95"><label>(95)</label><mml:math id="m95"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Inserting these terms into the dynamics (<xref ref-type="disp-formula" rid="equ84">Equation 84</xref>), we arrive at<disp-formula id="equ96"><label>(96)</label><mml:math id="m96"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mi>M</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">κ</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The white noise term has variance<disp-formula id="equ97"><label>(97)</label><mml:math id="m97"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mover><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>U</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the fluctuations in the population average of the variance, we use <xref ref-type="disp-formula" rid="equ90">Equation 90</xref> to compute the covariance function between different time points,<disp-formula id="equ98"><label>(98)</label><mml:math id="m98"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:msubsup><mml:mi>σ</mml:mi><mml:mo>⊥</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo accent="false">¯</mml:mo></mml:mover><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo></mml:msub><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:msqrt><mml:msup><mml:mi>u</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For the third line, the average is taken over the independent standard normal variables <inline-formula><mml:math id="inf782"><mml:mrow><mml:mi>u</mml:mi><mml:mo separator="true">,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Putting all the pieces together, we have<disp-formula id="equ99"><label>(99)</label><mml:math id="m99"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with Jacobian<disp-formula id="equ100"><label>(100)</label><mml:math id="m100"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and noise term <inline-formula><mml:math id="inf783"><mml:mrow><mml:mi>𝜻</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with zero average and covariance<disp-formula id="equ101"><label>(101)</label><mml:math id="m101"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ102"><label>(102)</label><mml:math id="m102"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>‴</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We inserted the eigenvalue equation, <inline-formula><mml:math id="inf784"><mml:mrow><mml:mi>M</mml:mi><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. We note that the colored noise term did not appear in previous studies such as <xref ref-type="bibr" rid="bib29">Kadmon et al., 2020</xref>, because there was no positive feedback and corresponding fixed point <inline-formula><mml:math id="inf785"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula>.</p><p>We now compute the variance of the fluctuations <inline-formula><mml:math id="inf786"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>𝜿</mml:mi></mml:mrow></mml:math></inline-formula>. The computation is simplified by the fact the <inline-formula><mml:math id="inf787"><mml:mover><mml:mi>𝜿</mml:mi><mml:mo stretchy="false" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula> is also eigenvector of <inline-formula><mml:math id="inf788"><mml:mi>A</mml:mi></mml:math></inline-formula>, with eigenvalue<disp-formula id="equ103"><label>(103)</label><mml:math id="m103"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>‴</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The eigenvector is also the direction in which the colored noise acts. Because of this, we can write<disp-formula id="equ104"><label>(104)</label><mml:math id="m104"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The first summand stems from the white noise term,<disp-formula id="equ105"><label>(105)</label><mml:math id="m105"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The integral <inline-formula><mml:math id="inf789"><mml:mrow><mml:mi>I</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be computed to yield<disp-formula id="equ106"><label>(106)</label><mml:math id="m106"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the steady-state variance, we take <inline-formula><mml:math id="inf790"><mml:mrow><mml:mi>t</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>. The first term yields <inline-formula><mml:math id="inf791"><mml:mrow><mml:munder><mml:mi>lim</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mspace width="0.1667em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which can be expressed as the solution of the Lyapunov equation<disp-formula id="equ107"><label>(107)</label><mml:math id="m107"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The integral converges to<disp-formula id="equ108"><label>(108)</label><mml:math id="m108"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munder><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Joining all the bits and pieces, we have the steady-state covariance<disp-formula id="equ109"><label>(109)</label><mml:math id="m109"><mml:mrow><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with normalized eigenvector<disp-formula id="equ110"><label>(110)</label><mml:math id="m110"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the loss, we only need to consider the variance along the output vector. Namely, we have the output fluctuations<disp-formula id="equ111"><label>(111)</label><mml:math id="m111"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with projected output weights<disp-formula id="equ112"><label>(112)</label><mml:math id="m112"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus,<disp-formula id="equ113"><label>(113)</label><mml:math id="m113"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">κ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Summary of all measures for initially decaying or chaotic networks.</title><p>(<bold>A</bold>) Correlation, (<bold>B</bold>) norm of output weights, (<bold>C</bold>) norm of states, (<bold>D</bold>) dissimilarity between learners, (<bold>E</bold>) dimension of activity <inline-formula><mml:math id="inf792"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, (<bold>F</bold>) fit dimension <inline-formula><mml:math id="inf793"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, (<bold>G</bold>) susceptibility to perturbations, (<bold>H</bold>) noise suppression.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Projection of neural dynamics onto first four principal components (PCs) for the cycling task, <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The <italic>x</italic>-axis for all plots is PC 1 of the respective dynamics (left: aligned, right: oblique). The <italic>y</italic>-axes are PCs 2–4. Axis labels indicate the relative variance explained by each PC. Arrows indicate direction. Note that there is co-rotation for the aligned network for PCs 1 and 3, as well as the counter-rotation for the oblique network for PCs 1 and 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig3-v1.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Power spectral densities for the six networks shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>The dashed orange line indicates the output frequency. Note the high power for non-target frequencies in the first principal components (PCs) in some of the large output solutions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Example of task variability for the flip-flop task.</title><p>The titles in each row indicate the spectral radius <inline-formula><mml:math id="inf794"><mml:mi>g</mml:mi></mml:math></inline-formula> of the initial recurrent connectivity (<inline-formula><mml:math id="inf795"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for initially chaotic, else decaying, activity), and the norm of initial output weights.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig5-v1.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Neural activity in response to the perturbations applied in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title><p>Activity is plotted in the space spanned by the leading two principal components (PCs) and the output weights <inline-formula><mml:math id="inf796"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. We first show the unperturbed trajectories in each network (<bold>A, B</bold>), then the perturbed ones for perturbations along the first output direction (<bold>C, D</bold>) and along the first PC (<bold>E, F</bold>). The unperturbed trajectories are also plotted for comparison. Yellow dots indicate the point where the perturbation is applied. All perturbations but the one along the output for aligned lead to trajectories on the same attractor, but potentially with a phase shift. Note that in general, perturbations can also lead to the activity converging on a different attractor. Here, we see a specific example of this happening for the cycling task in the aligned regime.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig6-v1.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Noise compression over training time for the cycling task.</title><p>Example networks trained on the cycling tasks with <inline-formula><mml:math id="inf797"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext/><mml:mi>noise</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf798"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>. The network at the end of training is analyzed in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. (<bold>A</bold>) Full loss (gray) and decomposition in bias (golden) and variance (purple) parts over learning time. The bias part decays rapidly (the <italic>y</italic>-axis is clipped, initial loss <inline-formula><mml:math id="inf799"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.4</mml:mn></mml:mrow></mml:math></inline-formula>), whereas the variance part needs many more training steps to decrease. Dotted lines indicate the two examples in (<bold>B–D</bold>). (<bold>B</bold>) Output fluctuations <inline-formula><mml:math id="inf800"><mml:mrow><mml:mi>δ</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> around the trial-conditioned average <inline-formula><mml:math id="inf801"><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">‾</mml:mo></mml:mover></mml:math></inline-formula>. Mean is over 16 samples for each of the two trial conditions (clockwise and anticlockwise rotation). Because both output dimensions <inline-formula><mml:math id="inf802"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>2</mml:mn><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are equivalent in scale, we collected both for the histogram. (<bold>C, D</bold>) Example output trajectories early (<bold>C</bold>) and late (<bold>D</bold>) in learning. Shown are the mean (dark) and five samples (light).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Fitting neural activity to different output modalities (hand position, velocity, acceleration, EMG).</title><p>Output modality is indicated by the x-ticks, the corresponding data sets by the color and the labels below. (<bold>A, B</bold>) Correlation and relative fitting dimension. Similar to <xref ref-type="fig" rid="fig7">Figure 7B</xref>, where these are shown for velocity alone. (<bold>C–E</bold>) Additional details. (<bold>C</bold>) Coefficient of determination <inline-formula><mml:math id="inf803"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> of the linear regression. (<bold>D</bold>) Number of principal components (PCs) necessary to reach 90% of the variance of the neural activity <inline-formula><mml:math id="inf804"><mml:mi>X</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) Number of PCs necessary to reach 90% of the <inline-formula><mml:math id="inf805"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> value of the full neural activity. For each output modality, the delay between activity and output is optimized. Position decodes earlier (300–200 ms) than velocity or acceleration (100–50 ms); no delay for EMG. The data <inline-formula><mml:math id="inf806"><mml:mi>X</mml:mi></mml:math></inline-formula> is the same with each data set apart from a potential shift by the respective delay, so that dimension <inline-formula><mml:math id="inf807"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo separator="true">,</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> in (<bold>D</bold>) is almost the same. Note that we also computed trial averages for the Neural Latents Benchmark (NLB) maze task to test for the effect of trial averaging. These, however, have a small sample number (189 data points, but 162 neurons), which leads to considerable discrepancy between the dots (full data) and the subsamples with only 25% of the data points, for example in the correlation (<bold>A</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig8-v1.tif"/></fig><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Learning curves and eigenvalue spectra for sine wave task.</title><p>(<bold>A</bold>) Loss over training steps for four networks. The light red line is the bias term of the loss for the oblique network. (<bold>B</bold>) Norm of weight changes over learning time. (<bold>C</bold>) Eigenvalue spectra of connectivity matrix <inline-formula><mml:math id="inf808"><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula> after training. The dashed line indicates the stability line for the fixed point at the origin.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig9-v1.tif"/></fig><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>The aligned regime is robust to the choice of other hyperparameters.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig10-v1.tif"/></fig><fig id="app1fig11" position="float"><label>Appendix 1—figure 11.</label><caption><title>The oblique regime is robust to the choice of other hyperparameters.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig11-v1.tif"/></fig><fig id="app1fig12" position="float"><label>Appendix 1—figure 12.</label><caption><title>Training history leads to order-one fixed point norm.</title><p>We trained recurrent neural networks (RNNs) on the example fixed point task. Similar to <xref ref-type="fig" rid="fig13">Figure 13</xref>, but with smaller noise <inline-formula><mml:math id="inf809"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and with learning rate increased by 2 and number of epochs by 2.5. (<bold>A–C</bold>) Learning dynamics with gradient descent for one network with <inline-formula><mml:math id="inf810"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons. The first 400 epochs are dominated by, <inline-formula><mml:math id="inf811"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf812"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> becomes <italic>positive</italic>. The negative feedback loop <inline-formula><mml:math id="inf813"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, only forms later in learning. The matrix <inline-formula><mml:math id="inf814"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> does not become symmetric during learning. (<bold>D, E</bold>) Fixed point norm and correlation for different <inline-formula><mml:math id="inf815"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> evaluated when <inline-formula><mml:math id="inf816"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (left) and at the end of learning (right). The time points are indicated by a square and triangle in (<bold>C</bold>), respectively. At <inline-formula><mml:math id="inf817"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, simulation and theory agree for the scaling: <inline-formula><mml:math id="inf818"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf819"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. At the end of training, the theory predicts a decreasing fixed point norm, but the simulated networks inherit the order-one norm from the training history.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93060-app1-fig12-v1.tif"/></fig></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93060.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Salk Institute for Biological Studies</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This work provides an <bold>important</bold> and novel framework for interpreting the interactions between recurrent dynamics across stages of neural processing. The authors report that two different kinds of dynamics exist in recurrent networks differing in the extent to which they align with the output weights. The authors also present <bold>convincing</bold> evidence that both types of dynamics exist in the brain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93060.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work, authors utilize recurrent neural networks (RNNs) to explore the question of when and how neural dynamics and the network's output are related from a geometrical point of view. The authors found that RNNs operate between two extremes: an 'aligned' regime in which the weights and the largest PCs are strongly correlated and an 'oblique' regime where the output weights and the largest PCs are poorly correlated. Large output weights led to oblique dynamics, and small output weights to aligned dynamics. This feature impacts whether networks are robust to perturbation along output directions. Results were linked to experimental data by showing that these different regimes can be identified in neural recordings from several experiments.</p><p>Strengths:</p><p>Diverse set of relevant tasks</p><p>Similarity measure well chosen</p><p>Explored various hyperparameter settings</p><p>Weaknesses:</p><p>One of the major connections to found BCI data with neural variance aligned to the outputs. Maybe I was confused about something, but doesn't this have to be the case based on the design of the experiment? The outputs of the BCI are chosen to align with the largest principal components of the data.</p><p>Proposed experiments maybe have already been done (New neural activity patterns emerge with long-term learning, Oby et al. 2019). My understanding of these results is that activity moved to be aligned as the manifold changed, but more analyses could be done to more fully understand the relationship between those experiments and this work.</p><p>Analysis of networks was thorough, but connections to neural data were weak. I am thoroughly convinced of the reported effect of large or small output weights in networks. I also think this framing could aid in future studies of interactions between brain regions.</p><p>This is an interesting framing to consider the relationship between upstream activity and downstream outputs. As more labs record from several brain regions simultaneously, this work will provide an important theoretical framework for thinking about the relative geometries of neural representations between brain regions.</p><p>It will be interesting to compare the relationship between geometries of representations and neural dynamics across connected different brain areas that are closer to the periphery vs. more central.</p><p>Exciting to think about the versatility of the oblique regime for shared representations and network dynamics across different computations.</p><p>Versatility of oblique regime could lead to differences between subjects in neural data.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93060.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper tackles the problem of understanding when the dynamics of neural population activity do and do not align with some target output, such as an arm movement. The authors develop a theoretical framework based on RNNs showing that an alignment of neural dynamics to an output can be simply controlled by the magnitude of the read-out weight vector while the RNN is being trained: small magnitude vectors result in aligned dynamics, where low-dimensional neural activity recapitulates the target; large magnitude vectors result in &quot;oblique&quot; dynamics, where encoding is spread across many dimensions. The paper further explores how the aligned and oblique regimes differ, in particular that the oblique regime allows degenerate solutions for the same target output.</p><p>Strengths:</p><p>- A really interesting new idea that different dynamics of neural circuits can arise simply from the initial magnitude of the output weight vector: once written out (Eq 3) it becomes obvious, which I take as the mark of a genuinely insightful idea</p><p>- The offered framework potentially unifies a collection of separate experimental results and ideas, largely from studies of motor cortex in primate: the idea that much of the ongoing dynamics do not encode movement parameters; the existence of the &quot;null space&quot; of preparatory activity; and that ongoing dynamics of motor cortex can rotate in the same direction even when the arm movement is rotating in opposite directions.</p><p>- The main text is well written, with a wide-ranging set of key results synthesised and illustrated well and concisely</p><p>- Shows the occurrence of the aligned and oblique regimes generalises across a range of simulated behavioural tasks</p><p>- A deep analytical investigation of when the regimes occur and how they evolve over training</p><p>- Shows where the oblique regime may be advantageous: allows multiple solutions to the same problem; and differs in sensitivity to perturbation and noise</p><p>- An insightful corollary result that noise in training is needed to obtain the oblique regime</p><p>- Tests whether the aligned and oblique regimes can be seen in neural recordings from primate cortex in a range of motor control tasks</p><p>- The revised text offers greater clarity and precision about when the aligned and oblique regimes occur and in the interpretation of the analyses of neural data</p><p>Weaknesses:</p><p>- The depth of analytical treatment in the Methods is impressive; however, the paper and the Methods analyses are largely independent, with the numerous results in the latter not being mentioned in the Results or Discussion. It in effect operates as two papers.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93060.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schuessler</surname><given-names>Friedrich</given-names></name><role specific-use="author">Author</role><aff><institution>Technical University of Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><role specific-use="author">Author</role><aff><institution>Champalimaud Foundation</institution><addr-line><named-content content-type="city">Lisbon</named-content></addr-line><country>Portugal</country></aff></contrib><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Barak</surname><given-names>Omri</given-names></name><role specific-use="author">Author</role><aff><institution>Technion- Israel Institute of Technology</institution><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>In this work, the authors utilize recurrent neural networks (RNNs) to explore the question of when and how neural dynamics and the network's output are related from a geometrical point of view. The authors found that RNNs operate between two extremes: an 'aligned' regime in which the weights and the largest PCs are strongly correlated and an 'oblique' regime where the output weights and the largest PCs are poorly correlated. Large output weights led to oblique dynamics, and small output weights to aligned dynamics. This feature impacts whether networks are robust to perturbation along output directions. Results were linked to experimental data by showing that these different regimes can be identified in neural recordings from several experiments.</p><p>Strengths:</p><p>A diverse set of relevant tasks.</p><p>A well-chosen similarity measure.</p><p>Exploration of various hyperparameter settings.</p><p>Weaknesses:</p><p>One of the major connections found BCI data with neural variance aligned to the outputs.</p><p>Maybe I was confused about something, but doesn't this have to be the case based on the design of the experiment? The outputs of the BCI are chosen to align with the largest principal components of the data.</p></disp-quote><p>The reviewer is correct. We indeed expected the BCI experiments to yield aligned dynamics. Our goal was to use this as a comparison for other, non-BCI recordings in which the correlation is smaller, i.e. dynamics closer to the oblique regime. We adjusted our wording accordingly and added a small discussion at the end of the experimental results, Section 2.6.</p><disp-quote content-type="editor-comment"><p>Proposed experiments may have already been done (new neural activity patterns emerge with long-term learning, Oby et al. 2019). My understanding of these results is that activity moved to be aligned as the manifold changed, but more analyses could be done to more fully understand the relationship between those experiments and this work.</p></disp-quote><p>The on- vs. off-manifold experiments are indeed very close to our work. On-manifold initializations, as stated above, are expected to yield aligned solutions. Off-manifold initializations allow, in principle, for both aligned and oblique solutions and are thus closer to our RNN simulations. If, during learning, the top PCs (dominant activity) rotate such that they align with the pre-defined output weights, then the system has reached an aligned solution. If the top PCs hardly change, and yet the behavior is still good, this is an oblique solution. There is some indication of an intermediate result (Figure 4C in Oby et al.), but the existing analysis there did not fully characterize these properties. Furthermore, our work suggests that systematically manipulating the norm of readout weights in off-manifold experiments can yield new insights. We thus view these as relevant results but suggest both further analysis and experiments. We rewrote the corresponding section in the discussion to include these points.</p><disp-quote content-type="editor-comment"><p>Analysis of networks was thorough, but connections to neural data were weak. I am thoroughly convinced of the reported effect of large or small output weights in networks. I also think this framing could aid in future studies of interactions between brain regions.</p><p>This is an interesting framing to consider the relationship between upstream activity and downstream outputs. As more labs record from several brain regions simultaneously, this work will provide an important theoretical framework for thinking about the relative geometries of neural representations between brain regions.</p><p>It will be interesting to compare the relationship between geometries of representations and neural dynamics across connected different brain areas that are closer to the periphery vs. more central.</p><p>It is exciting to think about the versatility of the oblique regime for shared representations and network dynamics across different computations.</p><p>The versatility of the oblique regime could lead to differences between subjects in neural data.</p></disp-quote><p>Thank you for the suggestions. Indeed, this is precisely why relative measures of the regime are valuable, even in the absence of absolute thresholds for regimes. We included your suggestions in the discussion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This paper tackles the problem of understanding when the dynamics of neural population activity do and do not align with some target output, such as an arm movement. The authors develop a theoretical framework based on RNNs showing that an alignment of neural dynamics to output can be simply controlled by the magnitude of the read-out weight vector while the RNN is being trained. Small magnitude vectors result in aligned dynamics, where low-dimensional neural activity recapitulates the target; large magnitude vectors result in &quot;oblique&quot; dynamics, where encoding is spread across many dimensions. The paper further explores how the aligned and oblique regimes differ, in particular, that the oblique regime allows degenerate solutions for the same target output.</p><p>Strengths:</p><p>- A really interesting new idea that different dynamics of neural circuits can arise simply from the initial magnitude of the output weight vector: once written out (Eq 3) it becomes obvious, which I take as the mark of a genuinely insightful idea.</p><p>- The offered framework potentially unifies a collection of separate experimental results and ideas, largely from studies of the motor cortex in primates: the idea that much of the ongoing dynamics do not encode movement parameters; the existence of the &quot;null space&quot; of preparatory activity; and that ongoing dynamics of the motor cortex can rotate in the same direction even when the arm movement is rotating in opposite directions.</p><p>- The main text is well written, with a wide-ranging set of key results synthesised and illustrated well and concisely.</p><p>- The study shows that the occurrence of the aligned and oblique regimes generalises across a range of simulated behavioural tasks.</p><p>- A deep analytical investigation of when the regimes occur and how they evolve over training.</p><p>- The study shows where the oblique regime may be advantageous: allows multiple solutions to the same problem; and differs in sensitivity to perturbation and noise.</p><p>- An insightful corollary result that noise in training is needed to obtain the oblique regime.</p><p>- Tests whether the aligned and oblique regimes can be seen in neural recordings from primate cortex in a range of motor control tasks.</p><p>Weaknesses:</p><p>- The magnitude of the output weights is initially discussed as being fixed, and as far as I can tell all analytical results (sections 4.6-4.9) also assume this. But in all trained models that make up the bulk of the results (Figures 3-6) all three weight vectors/matrices (input, recurrent, and output) are trained by gradient descent. It would be good to see an explanation or results offered in the main text as to why the training always ends up in the same mapping (small-&gt;aligned; large-&gt;oblique) when it could, for example, optimise the output weights instead, which is the usual target (e.g. Sussillo &amp; Abbott 2009 Neuron).</p></disp-quote><p>We understand the reviewer’s surprise. We chose a typical setting (training all weights of an RNN with Adam) to show that we don’t have to fine-tune the setting (e.g. by fixing the output weights) to see the two regimes. However, other scenarios in which the output weights do change are possible, depending on the algorithm and details in the way the network is parameterized. Understanding why some settings lead to our scenario (no change in scale) and others don’t is not a simple question. A short explanation here, nonetheless:</p><p>- Small changes to the internal weights are sufficient to solve the tasks.</p><p>- Different versions of gradient descent and different ways of parametrizing the network lead to different results in which parts of the weights get trained. This goes in particular for how weight scales are introduced, e.g. [Jacot et al. 2018 Neurips], [Geiger et al. 2020 Journal of Statistical Mechanics], or [Yang, Hu 2020, arXiv, Feature learning in infinite-width networks]. One insight from these works is that plain gradient descent (GD) with small output weights leads to learning only at the output (and often divergence or unsuccessful learning). For this reason, plain GD (or stochastic GD) is not suitable for small output weights (the aligned regime). Other variants of GD, such as Adam or RMSprop, don’t have this problem because they shift the emphasis of learning to the hidden layers (here the recurrent weights). This is due to the normalization of the gradients.</p><p>- FORCE learning [Sussillo &amp; Abbott 2009] is somewhat special in that the output weights are simultaneously also used as feedback weights. That is, not only the output weights but also an additional low-rank feedback loop through these output weights is trained. As a side note: By construction, such a learning algorithm thus links the output directly to the internal dynamics, so that one would only expect aligned solutions – and the output weights remain correspondingly small in these algorithms [Mastrogiuseppe, Ostojic, 2019, Neural Comp].</p><p>- In our setting, the output is not fed back to the network, so training the output alone would usually not suffice. Indeed, optimizing just the output weights is similar to what happens in the lazy training regime. These solutions, however, are not robust to noise, and we show that adding noise during the training does away with these solutions.</p><p>To address this issue in the manuscript, we added the following sentence to section 2.2: “While explaining this observation is beyond the scope of this work, we note that (1) changing the internal weights suffices to solve the task, and that (2) the extent to which the output weights change during learning depends on the algorithm and specific parametrization [21, 27, 85].”</p><disp-quote content-type="editor-comment"><p>- It is unclear what it means for neural activity to be &quot;aligned&quot; for target outputs that are not continuous time-series, such as the 1D or 2D oscillations used to illustrate most points here.</p><p>Two of the modeled tasks have binary outputs; one has a 3-element binary vector.</p></disp-quote><p>For any dynamics and output, we compare the alignment between the vector of output weights and the main PCs (the leading component of the dynamics). In the extreme of binary internal dynamics, i.e., two points {x_1, x_2}, there would only be one leading PC (the line connecting the two points, i.e. the choice decoder).</p><disp-quote content-type="editor-comment"><p>- It is unclear what criteria are used to assign the analysed neural data to the oblique or aligned regimes of dynamics.</p></disp-quote><p>Such an assignment is indeed difficult to achieve. The RNN models we showed were at the extremes of the two regimes, and these regimes are well characterized in the case of large networks (as described in the methods section). For the neural data, we find different levels of alignment for different experiments. These differences may not be strong enough to assign different regimes. Instead, our measures (correlation and relative fitting dimension) allow us to order the datasets. Here, the BCI data is more aligned than non-BCI data – perhaps unsurprisingly, given the experimental design of the prior and the previous findings for the rotation task [Russo et al, 2018]. We changed the manuscript accordingly, now focusing on the relative measure of alignment, even in the absence of absolute thresholds. We are curious whether future studies with more data, different tasks, or other brain regions might reveal stronger differentiation towards either extreme.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>There's so much interesting content in the supplement - it seemed like a whole other paper! It is interesting to read about the dynamics over the course of learning. Maybe you want to put this somewhere else so that more people read it?</p></disp-quote><p>We are glad the reviewer appreciated this content. We think developing these analysis methods is essential for a more complete understanding of the oblique regime and how it arises, and that it should therefore be part of the current paper.</p><disp-quote content-type="editor-comment"><p>Nice schematic in Figure 1.</p><p>There were some statements in the text highlighting co-rotation in the top 2 PCs for oblique networks. Figure 4a looks like aligned networks might also co-rotate in a particular subspace that is not highlighted. I could be wrong, but the authors should look into this and correct it if so. If both aligned and oblique networks have co-rotation within the top 5 or so PCs, some text should be updated to reflect this.</p></disp-quote><p>This is indeed the case, thanks for pointing this out! For one example, there is co-rotation for the aligned network already in the subspace spanned by PCs 1 and 3, see the figure below. We added a sentence indicating that co-rotation can take place at low-variance PCs for the aligned regime and pointed to this figure, which we added to the appendix (Fig. 17).</p><p>While these observations are an important addition, we don’t think they qualitatively alter our results, particularly the stronger dissociation between output and internal dynamics for oblique than aligned dynamics.</p><disp-quote content-type="editor-comment"><p>Figure 4 color labels were 'dark' and 'light'. I wasn't sure if this was a typo or if it was designed for colorblind readers? Either way, it wasn't too confusing, but adding more description might be useful.</p></disp-quote><p>Fixed to red and yellow.</p><disp-quote content-type="editor-comment"><p>Typo &quot;Aligned networks have a ratio much large than one&quot;</p><p>Typo &quot;just started to be explored&quot; Typo &quot;hence allowing to test&quot;</p></disp-quote><p>Fixed all typos.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>- Explain/discuss in the main text why the initial output weights reliably result in the required internal RNN dynamics (small-&gt;aligned; large-&gt;oblique) after training. The magnitude of the output weights is initially discussed as being fixed, and as far as I can tell all analytical results (sections 4.6-4.9) also assume this. But in all trained models that make up the bulk of the results (Figures 3-6) all three weight vectors/matrices (input, recurrent, and output) are trained by gradient descent. It would be good to see an explanation or results offered in the main text as to why the training always ends up in the same mapping (small-&gt;aligned; large-&gt;oblique) when it could, for example, just optimise the output weights instead.</p></disp-quote><p>See the answer to a similar comment by Reviewer #1 above.</p><disp-quote content-type="editor-comment"><p>- Page 6: explain the 5 tasks.</p></disp-quote><p>We added a link to methods where the tasks are described.</p><disp-quote content-type="editor-comment"><p>- Page 6/Fig 3 &amp; Methods: explain assumptions used to compute a reconstruction R^2 between RNN PCs and a binary or vector target output.</p></disp-quote><p>We added a new methods section, 4.4, where we explain the fitting process in Fig. 3. For all tasks, the target output was a time series with P specified target values in N_out dimensions. We thus always applied regression and did not differentiate between binary and non-binary tasks.</p><disp-quote content-type="editor-comment"><p>- Page 8: methods and predictions are muddled up: paragraph ending &quot;along different directions&quot; should be followed by paragraph starting &quot;Our intuition...&quot;. The intervening paragraph (&quot;We apply perturbations...&quot;) should start after the first sentence of the paragraph &quot;To test this,...&quot;.</p></disp-quote><p>Right, these sentences were muddled up indeed. We put them in the correct order.</p><disp-quote content-type="editor-comment"><p>- Page 10: what are the implications of the differences in noise alignment between the aligned and oblique regimes?</p></disp-quote><p>The noise suppression in the oblique regime is a slow learning process that gradually renders the solution more stable. With a large readout, learning separates into two phases. An early phase, in which a “lazy” solution is learned quickly. This solution is not robust to noise. In a second, slower phase, learning gradually leads to a more robust solution: the oblique solution. The main text emphasizes the result of this process (noise suppression). In the methods, we closely follow this process. This process is possibly related to other slow learning process fine-tuning solutions, e.g., [Blanc et al. 2020, Li et al. 2021, Yang et al. 2023]. Furthermore, it would be interesting to see whether such fine-tuning happens in animals [Ratzon et al. 2024]. We added corresponding sentences to the discussion.</p><disp-quote content-type="editor-comment"><p>- Neural data analysis:</p><p>(i) Page 11 &amp; Fig 7: the assignment of &quot;aligned&quot; or &quot;oblique&quot; to each neural dataset is based on the ratio of D_fit/D_x. But in all cases this ratio is less than 1, indicating fewer dimensions are needed for reconstruction than for explaining variance. Given the example in Figure 2 suggests this is an aligned regime, why assign any of them as &quot;oblique&quot;?</p></disp-quote><p>We weakened the wording in the corresponding section, and now only state that BCI data leans more towards aligned, non-BCI data more towards oblique. This is consistent with the intuition that BCI is by construction aligned (decoder along largest PCs) and non-BCI data already showed signs of oblique dynamics (co-rotating leading PCs in the cycling task, Russo et al. 2018).</p><p>We agree that Fig 2 (and Fig 3) could suggest distinguishing the regimes at a threshold D_fit/D_x = 1, although we hadn’t considered such a formal criterion.</p><disp-quote content-type="editor-comment"><p>(ii) Figure 23 and main text page 11: discuss which outputs for NLB and BCI datasets were used in Figure 7 &amp; and main text; the NLB results vary widely by output type - discuss in the main text; D_fit for NLB-maze-accuracy is missing from panel D; as the criterion is D_fit/D_x, plot this too.</p></disp-quote><p>We now discuss which outputs were used in Fig. 7 in its caption: the velocity of the task-relevant entity (hand/finger/cursor). This was done to have one quantity across studies. We added a sentence to the main text, p. 11, which points to Fig 22 (which used to be Fig 23) and states that results are qualitatively similar for other decoded outputs, despite some fluctuations in numerical values and decodability.</p><p>Regarding Fig 22: D_fit for NLB-maze-accuracy was beyond the manually set y-limit (for visibility of the other data points). We also extended the figure to include D_fit/D_x. We also discovered a small bug in the analysis code which required us to rerun the analysis and reproduce the plots. This also changed some of the numbers in the main text.</p><disp-quote content-type="editor-comment"><p>- Discussion:</p><p>&quot;They do not explain why it [the &quot;irrelevant activity&quot;] is necessary&quot;, implies that the following sentence(s) will explain this, but do not. Instead, they go on to say:</p><p>&quot;Here, we showed that merely ensuring stability of neural dynamics can lead to the oblique regime&quot;: this does not explain why it is necessary, merely that it exists; and it is unclear what results &quot;stability of neural dynamics&quot; is referring to.</p></disp-quote><p>We agree this was not a very clear formulation. We replaced these last three sentences with the following:</p><p>“Our study systematically explains this phenomenon: generating task-related output in the presence of large, task-unrelated dynamics requires large readout weights. Conversely, in the presence of large output weights, resistance to noise or perturbations requires large, potentially task-unrelated neural dynamics (the oblique regime).”</p><disp-quote content-type="editor-comment"><p>- The need for all 27 figures was unclear, especially as some seemed not to be referenced or were referenced out of order. Please check and clarify.</p></disp-quote><p>Fig 16 (Details for network dynamics in cycling tasks) and Fig 21 (loss over learning time for the different tasks) were not referenced, and are now removed.</p><p>We also reordered the figures in the appendix so that they would appear in the order they are referenced. Note that we added another figure (now Fig. 17) following a question from Reviewer #1.</p></body></sub-article></article>