<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72081</article-id><article-id pub-id-type="doi">10.7554/eLife.72081</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Rat sensitivity to multipoint statistics is predicted by efficient coding of natural scenes</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-245598"><name><surname>Caramellino</surname><given-names>Riccardo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2201-8079</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-245597"><name><surname>Piasini</surname><given-names>Eugenio</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0384-7699</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa1">‡</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-245599"><name><surname>Buccellato</surname><given-names>Andrea</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-245600"><name><surname>Carboncino</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-15603"><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6497-3819</contrib-id><email>vijay@physics.upenn.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12467"><name><surname>Zoccolan</surname><given-names>Davide</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7221-4188</contrib-id><email>zoccolan@sissa.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Visual Neuroscience Lab, International School for Advanced Studies</institution><addr-line><named-content content-type="city">Trieste</named-content></addr-line><country>Italy</country></aff><aff id="aff2"><label>2</label><institution>Computational Neuroscience Initiative, University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution>The University of Chicago</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>‡</label><p>Neural Computation Lab, International School for Advanced Studies, Trieste, Italy</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>12</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e72081</elocation-id><history><date date-type="received" iso-8601-date="2021-07-13"><day>13</day><month>07</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-11-18"><day>18</day><month>11</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-05-18"><day>18</day><month>05</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.05.17.444510"/></event></pub-history><permissions><copyright-statement>© 2021, Caramellino et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Caramellino et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72081-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-72081-figures-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="article-reference" xlink:href="10.7554/eLife.03722"/><abstract><p>Efficient processing of sensory data requires adapting the neuronal encoding strategy to the statistics of natural stimuli. Previously, in Hermundstad et al., 2014, we showed that local multipoint correlation patterns that are most variable in natural images are also the most perceptually salient for human observers, in a way that is compatible with the efficient coding principle. Understanding the neuronal mechanisms underlying such adaptation to image statistics will require performing invasive experiments that are impossible in humans. Therefore, it is important to understand whether a similar phenomenon can be detected in animal species that allow for powerful experimental manipulations, such as rodents. Here we selected four image statistics (from single- to four-point correlations) and trained four groups of rats to discriminate between white noise patterns and binary textures containing variable intensity levels of one of such statistics. We interpreted the resulting psychometric data with an ideal observer model, finding a sharp decrease in sensitivity from two- to four-point correlations and a further decrease from four- to three-point. This ranking fully reproduces the trend we previously observed in humans, thus extending a direct demonstration of efficient coding to a species where neuronal and developmental processes can be interrogated and causally manipulated.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>texture perception</kwd><kwd>image statistics</kwd><kwd>pattern vision</kwd><kwd>shape perception</kwd><kwd>efficient coding</kwd><kwd>ideal observer</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011199</institution-id><institution>FP7 Ideas: European Research Council</institution></institution-wrap></funding-source><award-id>616803-LEARN2SEE</award-id><principal-award-recipient><name><surname>Zoccolan</surname><given-names>Davide</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1734030</award-id><principal-award-recipient><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS113241</award-id><principal-award-recipient><name><surname>Piasini</surname><given-names>Eugenio</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Computational Neuroscience Initiative of the University of Pennsylvania</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Visual sensitivity to correlation patterns in rats matches that previously measured in humans, as well as predictions from efficient coding theory based on the statistics of natural images.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>It is widely believed that the tuning of sensory neurons is adapted to the statistical structure of the signals they must encode (<xref ref-type="bibr" rid="bib42">Sterling and Laughlin, 2015</xref>). This normative principle, known as efficient coding, has been successful in explaining many aspects of neural processing in vision (<xref ref-type="bibr" rid="bib2">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="bib8">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib20">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib29">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib33">Pitkow and Meister, 2012</xref>), audition (<xref ref-type="bibr" rid="bib4">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib40">Smith and Lewicki, 2006</xref>) and olfaction (<xref ref-type="bibr" rid="bib44">Teşileanu et al., 2019</xref>), including adaptation (<xref ref-type="bibr" rid="bib26">Młynarski and Hermundstad, 2021</xref>) and gain control (<xref ref-type="bibr" rid="bib39">Schwartz and Simoncelli, 2001</xref>). In <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>, we reported that human sensitivity to visual textures defined by local multipoint correlations depends on the variability of such correlations across natural scenes. This allocation of resources to features that are the most variable in the environment, and thus more informative about its state, is accounted for by efficient coding, demonstrating its role as an organizing principle also at the perceptual level (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Tesileanu et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Tkacik et al., 2010</xref>). However, it remains unknown whether this preferential encoding of texture statistics that are the most variable across natural images is a general principle underlying visual perceptual sensitivity across species. Although some evidence exists for differential neural encoding of multipoint correlations in macaque V2 (<xref ref-type="bibr" rid="bib57">Yu et al., 2015</xref>) and V1 (<xref ref-type="bibr" rid="bib36">Purpura et al., 1994</xref>), the sensitivity ranking we previously reported in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> has not been investigated in any species other than humans (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Tesileanu et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Tkacik et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>). Moreover, while monkeys are standard models of advanced visual processing (<xref ref-type="bibr" rid="bib6">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Kourtzi and Connor, 2011</xref>; <xref ref-type="bibr" rid="bib21">Lehky and Tanaka, 2016</xref>; <xref ref-type="bibr" rid="bib27">Nassi and Callaway, 2009</xref>; <xref ref-type="bibr" rid="bib30">Orban, 2008</xref>), they are less amenable than rodents to causal manipulations (e.g. optogenetic or controlled rearing) to interrogate how neural circuits may adapt to natural image statistics. On the other hand, rodents have emerged as powerful model systems to study visual functions during the last decade (<xref ref-type="bibr" rid="bib11">Glickfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Glickfeld and Olsen, 2017</xref>; <xref ref-type="bibr" rid="bib14">Huberman and Niell, 2011</xref>; <xref ref-type="bibr" rid="bib17">Katzner and Weigelt, 2013</xref>; <xref ref-type="bibr" rid="bib28">Niell and Scanziani, 2021</xref>; <xref ref-type="bibr" rid="bib38">Reinagel, 2015</xref>; <xref ref-type="bibr" rid="bib58">Zoccolan, 2015</xref>). Rats, in particular, are able to employ complex shape processing strategies at the perceptual level (<xref ref-type="bibr" rid="bib1">Alemi-Neissi et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">De Keyser et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Djurdjevic et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Vermaercke and Op de Beeck, 2012</xref>), and rat lateral extrastriate cortex shares many defining features with the primate ventral stream (<xref ref-type="bibr" rid="bib16">Kaliukhovich and Op de Beeck, 2018</xref>; <xref ref-type="bibr" rid="bib23">Matteucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Piasini et al., 2021</xref>; <xref ref-type="bibr" rid="bib43">Tafazoli et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Vermaercke et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Vinken et al., 2017</xref>). More importantly, it was recently shown that rearing newborn rats in controlled visual environments allows causally testing long-standing hypotheses about the dependence of visual cortical development from natural scene statistics (<xref ref-type="bibr" rid="bib24">Matteucci and Zoccolan, 2020</xref>). Establishing the existence of a preferential encoding of less predictable statistics in rodents is therefore crucial to understand the neural substrates of efficient coding and its relationship with postnatal visual experience.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To address this question, we measured rat sensitivity to visual textures defined by local multipoint correlations, training the animals to discriminate binary textures containing structured noise from textures made of white noise (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The latter were generated by independently setting each pixel to black or white with equal probability, resulting in no spatial correlations. Structured textures, on the other hand, were designed to enable precise control over the type and intensity of the correlations they contained. To generate these textures we built and published a software library (<xref ref-type="bibr" rid="bib31">Piasini, 2021</xref>) that implements the method developed in <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>. Briefly, for any given type of multipoint correlation (also termed a statistic in what follows), we sampled from the distribution over binary textures that had the desired probability of occurrence of that statistic, but otherwise contained the least amount of structure (i.e. had maximum entropy). The probability of occurrence of the pattern was parametrized by the intensity of the corresponding statistic, determined by a parity count of white or black pixels inside tiles of 1, 2, 3, or 4 pixels (termed gliders) used as the building blocks of the texture (<xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>). When the intensity is zero, the texture does not contain any structure–it is the same as white noise (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, left). When the intensity is +1, every possible placement of the glider across the texture contains an even number of white pixels, while a level of –1 corresponds to all placements containing an odd number of white pixels. Intermediate intensity levels correspond to intermediate fractions of gliders containing the even parity count. The structure of the glider and the sign of the intensity level dictate the appearance of the final texture. For instance (see examples in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, right), for positive intensity levels, a one-point glider produces textures with increasingly large luminance, a two-point glider produces oriented edges and a four-point glider produces rectangular blocks. A three-point glider produces L-shape patterns, either black or white depending on whether the intensity is negative or positive.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Visual stimuli and behavioral task.</title><p>(<bold>A</bold>) Schematic of the four kinds of texture discrimination tasks administered to the four groups of rats in our study. Each group had to discriminate unstructured binary textures containing white noise (example on the left) from structured binary textures containing specific types of local multipoint correlations among nearby pixels (i.e. 1-, 2-, 3-, or 4-point correlations; examples on the right). The textures were constructed to be as random as possible (maximum entropy), under the constraint that the strength of a given type of correlation matched a desired level. The strength of a correlation pattern was quantified by the value (intensity) of a corresponding statistic (see main text), which could range from 0 (white noise) to 1 (maximum possible amount of correlation). The examples shown here correspond to intensities of 0.85 (one- and two-point statistics) and 0.95 (three- and four-point statistics). (<bold>B</bold>) Schematic representation of a behavioral trial. Left and center: animals initiated the presentation of a stimulus by licking the central response port placed in front of them. This prompted the presentation of either a structured (top) or an unstructured (bottom) texture. Right: in order to receive the reward, animals had to lick either the left or right response port to report whether the stimulus contained the statistic (top) or the noise (bottom). <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> shows the performances attained by four example rats (one per group) during the initial phase of the training (when the animals were required to discriminate the stimuli shown in A), as well as the progressively lower statistic intensity levels that these rats progressively learned to discriminate from white noise during the second phase of the experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72081-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Learning curves of four example rats during the two initial training phases.</title><p>(<bold>A</bold>) Learning curves of four example animals belonging to the first batch of rats (see Materials and methods), each trained with one of the four texture statistics: one-point (red), two-point (blue), three-point (purple), and four-point (green). During this phase, animals had to discriminate textures containing a single high intensity level of the assigned statistic (see examples in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, right) from white noise. Each curve shows the performance of a rat (i.e. the fraction of correct choices), as computed in consecutive blocks of 500 trials. The dashed line indicates the 65% criterion performance that rats of the first batch had to reach in order to be moved to the next experimental phase (see Materials and methods). As exemplified by these four animals, rats trained on different statistics required different lengths of time to learn the discrimination – animals trained on one- and two-point statistics reached and maintained a performance above criterion considerably earlier than those trained on three- and four-point. Specifically, considering only those rats that reached criterion performance, animals trained on one-point correlations required on average 3.5 ± 0.3 blocks of 500 trials (n = 9) to reach criterion; those trained on two-point correlations 5.3 ± 1.3 (n = 8); those trained on 3-point statistic 3.4 ± 1.6 (n = 5); and those trained on four-point correlations 11.8 ± 1.6 (n = 10). (<bold>B</bold>) Minimal intensity levels of the statistics that the same animals shown in A were able to discriminate from white noise across consecutive sessions (colored dots) of the second phase of the experiment, when the rats were exposed to progressively lower intensities through a staircase procedure (see Materials and methods). Sessions where the number of trials performed by the animals was below 50 are not shown in these graphs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72081-fig1-figsupp1-v1.tif"/></fig></fig-group><p>Notably, two-point and three-point gliders are associated to multiple distinct multipoint correlations, corresponding to different spatial glider configurations. For instance, two-point correlations can arise from horizontal (-), vertical (|) or oblique gliders (/, \), while three-point correlations can give rise to L patterns with various orientations (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌝</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>θ</mml:mi><mml:mpadded depth="1.4pt" height="4.9pt" width="5.2pt"><mml:mi mathvariant="normal">¬</mml:mi></mml:mpadded></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌞</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). In our previous study with human participants (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>), we tested all these two-point, three-point, and four-point configurations, as well as 11 of their pairwise combinations, for a total of 20 different texture statistics. In that set of experiments, we did not test textures defined by one-point correlations because, by construction, the method we used to measure the variability of texture statistics across natural images could not be applied to the one-point statistic. In our current study, practical and ethical constraints prevented us from measuring rat sensitivity to a large number of statistic combinations, because a different group of animals had to be trained with each tested statistic (see below), meaning that the number of rats required for the experiments increased rapidly with the number of statistics studied. Therefore, we chose to test the 4-point statistic, as well as one each of the two-point and three-point statistics (those shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). One of the three-point statistics (corresponding to the glider <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) was randomly selected among the four available, since in our previous study no difference was found among the variability of distinct three-point textures across natural images, and aggregate human sensitivity to three-point correlations was measured without distinguishing among glider configurations. As for the two-point statistic, we selected one of the two gliders (the horizontal one) that yielded the largest sensitivity in humans, so as to include in our stimulus set at least an instance of both the most discriminable (two-point -) and least discriminable (three-point <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) textures. In addition, we also tested the one-point statistic because, given the well-established sensitivity of the rat visual system to luminance changes (<xref ref-type="bibr" rid="bib25">Minini and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="bib43">Tafazoli et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Vascon et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Vermaercke and Op de Beeck, 2012</xref>), performance with this statistic served as a useful benchmark against which to compare rat discrimination of the other, more complex textures. Finally, while in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>, both positive and negative values of the statistics were probed against white noise, here we tested only one side of the texture intensity axis (either positive, for one-, two-, and four-point configurations, or negative, for three-point ones) — again, with the goal of limiting the number of rats used in the experiment (see Materials and methods for more details on the rationale behind the choice of statistics and their polarity, and see Discussion for an assessment of the possible impact of these choices on our conclusions).</p><p>For each of the four selected image statistics, we trained a group of rats to discriminate between white noise and structured textures containing that statistic with nonzero intensity (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each trial of the experiment started with the rat autonomously triggering the presentation of a stimulus by licking the central response port within an array of three (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The animal then reported whether the texture displayed over the monitor placed in front of him contained the statistic (by licking the left port) or white noise (by licking the right port). The rat received liquid reward for correct choices and was subjected to a time-out period for incorrect ones (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In the initial phase of the experiment, the intensity of the statistic was set to a single level, close to the maximum (or minimum, in case of the three-point statistic, for which we used only negative values), to make the discrimination between structured textures and white noise as easy as possible for naive rats that had to learn the task from scratch. The learning curves of four example rats, one per group, are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>. In the following phase of the experiment, the intensity of the statistic was gradually reduced using an adaptive staircase procedure (see Materials and methods) to make the task progressively harder. The asymptotic levels of the statistics reached across consecutive training sessions by four example rats, one per group, are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>. Following this training, rats were subjected to: (1) a main testing phase, where textures were sampled at regular intervals along the intensity level axis and were randomly presented to the animals; and (2) a further testing phase, where rats originally trained with a given statistic were probed with a different one (see Materials and methods for details on training and testing).</p><p>The main test phase yielded psychometric curves showing the sensitivity of each animal in discriminating white noise from the structured texture with the assigned statistic (example in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, black dots). To interpret results, we developed an ideal observer model, in which the presentation of a texture with a level of the statistic equal to s produces a percept <inline-formula><mml:math id="inf7"><mml:mi>x</mml:mi></mml:math></inline-formula> sampled from a truncated Gaussian distribution centered on the actual value of the statistic (<inline-formula><mml:math id="inf8"><mml:mi>s</mml:mi></mml:math></inline-formula>) with a fixed standard deviation σ (<xref ref-type="bibr" rid="bib9">Fleming et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Geisler, 2011</xref>). Here, σ measures the ‘blurriness’ in the animal's sensory representation for a particular type of statistic (i.e. the perceptual noise) and, consequently, its inverse 1/σ captures its resolution, or sensitivity — i.e., the perceptual threshold for discriminating a structured texture from white noise. As detailed in the Materials and methods, our ideal observer model yields the psychometric function giving the probability of responding ‘noise’ at any given level of the statistic <inline-formula><mml:math id="inf9"><mml:mi>s</mml:mi></mml:math></inline-formula> as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>report noise</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Rat sensitivity to multipoint correlations.</title><p>(<bold>A</bold>) Psychometric data for an example rat trained on two-point correlations. Black dots: fraction of trials in which a texture with the corresponding intensity of the statistic was correctly classified as ‘structured’. Empty black circle: fraction of trials the rat has judged a white noise texture as containing the statistic. Blue line: psychometric function corresponding to the fitted ideal observer model (see main text). (<bold>B</bold>) Psychometric functions obtained for all the rats tested on the four statistics (n indicates the number of animals in each group). (<bold>C</bold>) Values of the perceptual sensitivity <inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> to each of the four statistics. Filled dots: individual rat estimates. Empty diamonds: group averages. The dashed line emphasizes the sensitivity ranking observed for the four statistics. Significance markers ** and *** indicate, respectively, p &lt; 0.01 and p &lt; 0.001 for a two-sample t-test with Holm-Bonferroni correction. The same analysis was repeated in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> including only the rats that reached a certain performance criterion during the initial training.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72081-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Rat sensitivity to multipoint correlations after excluding animals that did not reach the 65% correct discrimination criterion in phase I of the training.</title><p>Same data as in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, but after excluding one rat from the two-point group, three rats from the three-point group and one rat from the four-point group. Significance markers as in <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72081-fig2-figsupp1-v1.tif"/></fig></fig-group><p>where <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard Normal cumulative density function, <inline-formula><mml:math id="inf12"><mml:mi>α</mml:mi></mml:math></inline-formula> captures the animal’s prior choice bias and <inline-formula><mml:math id="inf13"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the decision boundary used by the animal to divide the perceptual axis into ‘noise’ and ‘structured texture’ regions. The two free parameters of the model (α and σ) parameterize the psychometric function (example in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, blue curve) and can be estimated from behavioral data by maximum likelihood. Prior bias (<inline-formula><mml:math id="inf14"><mml:mi>α</mml:mi></mml:math></inline-formula>) and sensitivity (<inline-formula><mml:math id="inf15"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>) are related, respectively, to the horizontal offset and slope of the curve.</p><p>Fitting this model to the behavioral choices of rats in the four groups led to psychometric functions with a characteristic shape, which depended on the order of the multipoint statistic an animal had to discriminate (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In particular, the sensitivity <inline-formula><mml:math id="inf16"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> followed a specific ranking among the groups (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), being higher for one- and two-point than for three-point (p<sub>1</sub>&lt;0.001 and p<sub>2</sub>&lt;0.001, two-sample t-test with Holm-Bonferroni correction) and four-point (p<sub>1</sub>&lt;0.001, p<sub>2</sub>&lt;0.001) correlations, and larger for four-point than three-point correlations (p&lt;0.01). When focusing on the texture statistics that had been also tested in our previous study (i.e. two-point horizontal, three-point, and four-point correlations), this sensitivity ranking was the same as the one observed in humans and as the variability ranking measured across natural images (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>): two-point horizontal &gt; four-point &gt; three-point. Moreover, for the set of statistics that were studied both here and in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>, the actual values of the rat sensitivity matched, up to a scaling factor, both the human sensitivity and the standard deviation of the statistics in natural images (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This match was quantified with the ‘degree of correspondence’, defined in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>, which takes on values between 0 and 1, with one indicating perfect quantitative match (see Materials and methods for details). The degree of correspondence was 0.986 between rat sensitivity and image statistics (p-value: 0.07, Monte Carlo test), and 0.990 between rat sensitivity and human sensitivity (p-value: 0.05, Monte Carlo test). For reference, <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> reported values between 0.987 and 0.999 for the degree of correspondence between human sensitivity and image statistics. This indicates not only a qualitative but also a quantitative agreement between our findings and the pattern of texture sensitivity predicted by efficient coding.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Quantitative match between rat sensitivities, human sensitivities and texture variability across natural images.</title><p>For the three texture statistics that were tested both in our current study with rats and in our earlier study with humans (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>), rat average sensitivities are compared to human average sensitivities and to the variability of these statistics across natural scenes data from <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>. The three sets of data points have been scaled in such a way that each triplet of sensitivity values had Euclidean norm = 1, so as to allow an easier qualitative comparison (for a quantitative comparison see main text).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72081-fig3-v1.tif"/></fig><p>To further validate these findings, we performed additional within-group and within-subject comparisons. To this end, each group of animals was either tested with a new statistic or was split into two subgroups, each tested with a different statistic. Results of these additional experiments are reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>, comparing the sensitivity to the new statistic(s) with the sensitivity to the originally learned statistic (colored symbols without and with halo, respectively) for each group/subgroup. Rats trained on one- and two-point statistics (the most discriminable ones; see <xref ref-type="fig" rid="fig2">Figure 2C</xref>) performed poorly with higher-order correlations (compare the green and purple star with the red star, and the green and purple cross with the blue cross in <xref ref-type="fig" rid="fig4">Figure 4</xref>), while animals trained on the four-point statistic performed on two-point correlations as well as rats that were originally trained on those textures (compare the blue square to the blue cross). This shows that the better discriminability of textures containing lower order correlations is a robust phenomenon, which is independent of the history of training and observable within individual subjects. Moreover, performance on four-point correlations was higher than performance on three-point correlations for each group of rats (compare the green to the purple symbols connected by a line). This was true, in particular, not only for rats trained on four-point and switching to three-point (green vs. purple square, p &lt; 0.01, paired one-tailed t-test) but even for rats trained on three-points and switching to four-point (green vs. purple triangle, p &lt; 0.05, paired one-tailed t-test). This means that the larger discriminability of the four-point statistic, as compared to the three-point one, is a statistically robust phenomenon within individual subjects.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Rat sensitivity to multipoint correlations – dependence on training history and within-subject analysis.</title><p>Colored points with halo show the average sensitivities (with SEM) of the four groups of rats to the statistics (indicated by the symbols in the key) they were originally trained on (i.e. same data as the colored diamonds in <xref ref-type="fig" rid="fig2">Figure 2C</xref>). The other colored symbols connected by a line show the average sensitivities (with SEM) obtained when subgroups of rats originally trained on a given statistic (as indicated by the symbol in the key) were tested with different statistics (as indicated in abscissa). Specifically: (1) out of the nine rats originally trained/tested with one-point correlations (star), four were tested with three-point (purple star) and four with four-point (green star) correlations (one rat did not reach this test phase); (2) out of the nine rats originally trained/tested with two-point correlations (cross), five were tested with three-point (purple cross) and four with four-point (green cross) correlations; (3) out of the eight rats originally trained/tested with three-point correlations (triangle), seven were tested with four-point (green triangle) correlations (one rat did not reach this test phase); and (4) out of the eleven rats originally trained/tested with four-point correlations (square), eight were tested with three-point (purple square) and three with two-point correlations (blue square). Sensitivities achieved by individual animals are represented as shaded data points with the corresponding symbol/color combination.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72081-fig4-v1.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Overall, our results show that rat sensitivity to multipoint statistics is similar to the one we previously observed in humans and to the variability of multipoint correlations we previously measured across natural images (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Tkacik et al., 2010</xref>). This agreement holds both qualitatively and quantitatively (<xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig4">4</xref>). Importantly, we found the expected sensitivity ranking (two-point horizontal &gt; four-point &gt; three-point) to be robust not only across groups (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) but also for animals that were sequentially tested with multiple texture statistics (<xref ref-type="fig" rid="fig4">Figure 4</xref>) - and even at the within-subject level for the crucial three-point vs. four-point comparison. Moreover, we found a high degree of correspondence between rat and human sensitivities (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>A potential limitation of our study is related to our stimulus choices, both in terms of selected texture statistics and polarity (i.e. negative vs. positive intensity). A first possible issue is whether the three texture statistics that were tested in both the present study and in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> are sufficient to allow a meaningful comparison between rat and human sensitivities, as well as rat sensitivity and texture variability in natural scenes. We addressed this matter at the level of experimental design, by carefully choosing the three statistics that, based on the sensitivity ranking observed in humans, would have yielded the cleanest signature of efficient coding (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>). That is, we selected two statistics that were, respectively, maximally and minimally variable across natural images, and yielded the largest and lowest sensitivities in humans: horizontal two-point correlations and one of the three-point correlations. The four-point correlation was then a natural choice as the third statistic, as it was the only one characterized by a differently shaped glider. Additionally, human sensitivity to this statistic, as well as its variability across natural images, is only slightly larger than for the three-point configurations. Therefore, finding a reliable sensitivity difference between three-point and four-point textures also for rats would have provided strong evidence for matching texture sensitivity across the two species. Due to the experimental limitations discussed in the Results and the Materials and methods sections, we were unable to analyze one of the oblique two-point statistics, for which human sensitivity takes on an intermediate value between the two-point horizontal and three-point correlations, and that in humans allows one to differentiate between the predictions of efficient coding and those stemming from an oblique effect for patterns that are rotated versions of each other (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>).</p><p>The second potential limitation is related to the choice of polarity (positive or negative intensity values for the examined statistics). This choice was guided by different considerations depending on the kind of statistic. For one-point correlations we chose positive intensity values because they yield patterns that are brighter than white noise. Since previous work from our group has shown that rat V1 neurons are very sensitive to increases of luminance (<xref ref-type="bibr" rid="bib43">Tafazoli et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Vascon et al., 2019</xref>), our choice ensured that one-point textures were highly distinguishable from white noise (as indeed observed in our data; see <xref ref-type="fig" rid="fig2">Figure 2B–C</xref>), which was the key requirement for our benchmark statistic. This enabled us to guard against issues in our task design: if the animals had failed to discriminate one-point textures, this would have suggested an overall inadequacy of the behavioral task rather than a lack of perceptual sensitivity to luminance changes. For two-point and four-point statistics we also used positive intensity values — a choice dictated by the need of testing a rodent species that has much lower visual acuity than humans (<xref ref-type="bibr" rid="bib18">Keller et al., 2000</xref>; <xref ref-type="bibr" rid="bib35">Prusky et al., 2002</xref>; <xref ref-type="bibr" rid="bib58">Zoccolan, 2015</xref>). Positive two-point and four-point correlations give rise to large features (thick oriented stripes and wide rectangular blocks made of multiple pixels with the same color), while negative intensities produce higher spatial frequency patterns, where color may change every other pixel (see Figure 2A in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>). Therefore, using negative two-point and four-point statistics would have introduced a possible confound, since low sensitivity to these textures could have been simply due to the low spatial resolution of rat vision. For three-point correlations, polarity does not affect the shape and size of the emerging visual patterns, but it determines their contrast. Positive and negative intensities yield L-shaped patches that are, respectively, white and black. In this case, we chose the latter to make sure that the well-known dominance of OFF responses observed across the visual systems of many mammal species would not play in favor of finding the lowest sensitivity for the three-point statistic. In fact, several studies have shown that primary visual neurons of primates and cats respond more strongly to black than to white spots and oriented bars (<xref ref-type="bibr" rid="bib22">Liu and Yao, 2014</xref>; <xref ref-type="bibr" rid="bib55">Xing et al., 2010</xref>; <xref ref-type="bibr" rid="bib56">Yeh et al., 2009</xref>). A very recent study has shown that this is the case also for the central visual field of mice, although in the periphery OFF and ON response are more balanced (<xref ref-type="bibr" rid="bib54">Williams et al., 2021</xref>). Indeed, the asymmetry begins already in the retina where there are more OFF cells than ON cells (<xref ref-type="bibr" rid="bib37">Ratliff et al., 2010</xref>). Since in our behavioral rigs rats face frontally the stimulus display (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) and maintain their head oriented frontally during stimulus presentation (<xref ref-type="bibr" rid="bib47">Vanzella et al., 2019</xref>), it was important that the L-shaped patterns produced by three-point correlations had the highest saliency. Choosing negative intensity values ensured that this was the case, thus excluding the possibility that the low-sensitivity found for three-point textures (<xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig4">4</xref>) was partially due to presentation at a suboptimal contrast. Notwithstanding these considerations, one could wonder whether probing also the opposite polarities of those tested in our study would be desirable for a tighter test of the efficient coding principle. Previous studies, however, found human sensitivity to be nearly identical for negative and positive intensity variations of each of the statistic tested in our study: one-point, two-point, three-point, and four-point correlations (<xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>), even in the face of asymmetries of the distribution of the corresponding statistic in natural images (see Figure 3—figure supplement 9 in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>). In the present work, we have accordingly decided to focus the available resources on the differences between different statistics, rather than between positive and negative intensities of the same statistic.</p><p>In summary, our choices of texture types and their polarity were all dictated by the need of adapting to a rodent species texture stimuli that, so far, have only been used in psychophysics studies with humans (<xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Tesileanu et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Tkacik et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>) and neurophysiology studies in monkeys (<xref ref-type="bibr" rid="bib36">Purpura et al., 1994</xref>; <xref ref-type="bibr" rid="bib57">Yu et al., 2015</xref>). Our goal was to maximize the sensitivity of the comparison with humans and natural image statistics, while reducing the possible impact of phenomena (such as rat low visual acuity and the dominance of OFF responses) that could have acted as confounding factors. Thanks to these measures, our findings provide a robust demonstration that a rodent species and humans are similarly adapted to process the statistical structure of visual textures, in a way that is consistent with the computational principle of efficient coding. This attests to the fundamental role of natural image statistics in shaping visual processing across species, and opens a path toward a causal test of efficient coding through the altered-rearing experiments that small mammals, such as rodents, allow (<xref ref-type="bibr" rid="bib15">Hunt et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Matteucci and Zoccolan, 2020</xref>; <xref ref-type="bibr" rid="bib53">White and Fitzpatrick, 2007</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Psychophysics experiments</title><sec id="s4-1-1"><title>Subjects</title><p>A total of 42 male adult Long Evans rats (Charles River Laboratories) were tested in a visual texture discrimination task. Animals started the training at 10 weeks, after 1 week of quarantine upon arrival in our institute and 2 weeks of handling to familiarize them with the experimenters. Their weight at arrival was approximately 300 g and they grew to over 600 g over the time span of the experiment. Rats always had free access to food but their access to water was restricted in the days of the behavioral training (5 days a week). They received 10–20 ml of diluted pear juice (1:4) during the execution of the discrimination task, after which they were also given free access to water for the time needed to reach at least the recommended 50 ml/kg intake per day.</p><p>The number of rats was chosen in such a way to yield meaningful statistical analyses (i.e. to have about 10 subjects for each of the texture statistic tested in our study), under the capacity constraint of our behavioral rig. The rig allows to simultaneously test six rats, during the course of 1–1.5 hr (<xref ref-type="bibr" rid="bib58">Zoccolan, 2015</xref>; <xref ref-type="bibr" rid="bib7">Djurdjevic et al., 2018</xref>). Given the need for testing four different texture statistics, we started with a first batch of 24 animals (i.e. 6 per statistics), which required about 6 hr of training per day. This first batch was complemented with a second one of 18 more rats, again divided among the four statistics (see below for details), so as to reach the planned number of about 10 animals per texture type. The first batch arrived in November 2018 and was tested throughout most of 2019; the second group arrived in September 2019 and was tested throughout most of 2020. In the first batch, four animals did not reach the test phase (i.e. the phase yielding the data shown in <xref ref-type="fig" rid="fig2">Figure 2A and B</xref>), because three of them did not achieve the criterion performance during the initial training phase (see below) and one died shortly after the beginning of the study. In the second batch, one rat died before reaching the test phase and two more died before the last test phase with switched statistics (i.e. the phase yielding the data of <xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>All animal procedures were conducted in accordance with the international and institutional standards for the care and use of animals in research and were approved by the Italian Ministry of Health and after consulting with a veterinarian (Project DGSAF 25271, submitted on December 1, 2014 and approved on September 4, 2015, approval 940/2015-PR).</p></sec><sec id="s4-1-2"><title>Experimental setup</title><p>Rats were trained in a behavioral rig consisting of two racks, each equipped with three operant boxes (a picture of the rig and a schematic of the operant box can be found in previous studies [<xref ref-type="bibr" rid="bib58">Zoccolan, 2015</xref>; <xref ref-type="bibr" rid="bib7">Djurdjevic et al., 2018</xref>]). Each box was equipped with a 21.5” LCD monitor (ASUS VEZZHR) for the presentation of the visual stimuli and an array of three stainless-steel feeding needles (Cadence Science), serving as response ports. To this end, each needle was connected to a led-photodiode pair to detect when the nose of the animal approached and touched it (a Phidgets 1203 input/output device was used to collect the signals of the photodiodes). The two lateral feeding needles were also connected to computer-controlled syringe pumps (New Era Pump System NE-500) for delivery of the liquid reward. In each box, one of the walls bore a 4.5 cm-diameter viewing hole, so that a rat could extend its head outside the box, face the stimulus display (located at 30 cm from the hole) and reach the array with the response ports.</p></sec><sec id="s4-1-3"><title>Choice of image statistics to be used in the experiment</title><p>As mentioned in the main text, in our experiment we studied the 1-point and 4-point statistic, as well as one of the two-point and one of the three-point statistics. In the nomenclature introduced by <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>, these are, respectively, the <inline-formula><mml:math id="inf17"><mml:mi>γ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf18"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>β</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> statistics. By comparison, in humans, <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref> studied a total of five statistics (the same we tested, plus <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>β</mml:mi><mml:mo>/</mml:mo></mml:msub></mml:math></inline-formula>), while <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> tested many more, including combinations of statistic pairs, although they did not investigate <inline-formula><mml:math id="inf22"><mml:mi>γ</mml:mi></mml:math></inline-formula>. Our choice of which statistics to test was constrained on practical and ethical grounds by the need to use the minimum possible number of animals in our experiments, which led us to study one representative statistic per order of the glider. We note also that we decided to test the <inline-formula><mml:math id="inf23"><mml:mi>γ</mml:mi></mml:math></inline-formula> statistic, even though this was omitted by <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> (as explained in that paper, the method used to assess the variability of all other multipoint correlation patterns in natural images can’t be applied to <inline-formula><mml:math id="inf24"><mml:mi>γ</mml:mi></mml:math></inline-formula> by construction, because the binarization threshold used for images is such that <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all images in the dataset). The reason for including <inline-formula><mml:math id="inf26"><mml:mi>γ</mml:mi></mml:math></inline-formula> was that it provided a useful control on the effectiveness of our experimental design, as (unlike for the other visual patterns) we expected rats to be able to easily discriminate stimuli differing by average luminosity (<xref ref-type="bibr" rid="bib25">Minini and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="bib43">Tafazoli et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Vascon et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Vermaercke and Op de Beeck, 2012</xref>). As mentioned in the Discussion, failure of the rats to discriminate one-point textures would have indicated a likely issue in the design of the task.</p><p>Human sensitivity to multipoint correlation patterns does not distinguish between positive and negative values of the statistics (<xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>). Therefore, again in order to minimize the number of animals necessary to the experiment, we only collected data for positive values of the <inline-formula><mml:math id="inf27"><mml:mi>γ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf28"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:mi>α</mml:mi></mml:math></inline-formula> statistics, and negative values of the <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> statistic (see below for the specific values used). Unlike two- or four-point statistics, <inline-formula><mml:math id="inf31"><mml:mi>θ</mml:mi></mml:math></inline-formula> statistics change contrast under a sign change (namely, positive <inline-formula><mml:math id="inf32"><mml:mi>θ</mml:mi></mml:math></inline-formula> values correspond to white triangular patterns on a black background, and negative <inline-formula><mml:math id="inf33"><mml:mi>θ</mml:mi></mml:math></inline-formula> values correspond to black triangular patterns on a white background). On the other hand, dominance of OFF responses (elicited by dark spots on a light background) has been reported in mammals, including primates, cats, and rodents (<xref ref-type="bibr" rid="bib37">Ratliff et al., 2010</xref>; <xref ref-type="bibr" rid="bib22">Liu and Yao, 2014</xref>; <xref ref-type="bibr" rid="bib55">Xing et al., 2010</xref>; <xref ref-type="bibr" rid="bib56">Yeh et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Williams et al., 2021</xref>). Therefore we reasoned that if rats, unlike humans, were to have a different sensitivity to positive and negative <inline-formula><mml:math id="inf34"><mml:mi>θ</mml:mi></mml:math></inline-formula> values, the sensitivity to negative <inline-formula><mml:math id="inf35"><mml:mi>θ</mml:mi></mml:math></inline-formula> would be the higher of the two.</p><p>Finally, for the sake of simplicity, whenever in the text we refer to the ‘intensity’ of a statistic, this should be interpreted as the absolute value of the intensity as defined by <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>. This has no effect when describing <inline-formula><mml:math id="inf36"><mml:mi>γ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf37"><mml:mi>β</mml:mi></mml:math></inline-formula>, or <inline-formula><mml:math id="inf38"><mml:mi>α</mml:mi></mml:math></inline-formula> statistics, and only means that any value reported for <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> should be taken with a sign flip (i.e. negative instead of positive values) if trying to connect formally to the system of coordinates in <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>.</p></sec><sec id="s4-1-4"><title>Visual stimuli</title><p>Maximum-entropy textures were generated using the methods described by <xref ref-type="bibr" rid="bib51">Victor and Conte, 2012</xref>. To this end, we implemented a standalone library and software package that we have since made publicly available as free software (<xref ref-type="bibr" rid="bib31">Piasini, 2021</xref>). In the experiment, we used white noise textures as well as textures with positive levels of four different multipoint statistics, as described above (see also <xref ref-type="fig" rid="fig1">Figure 1A</xref>). It should be noted that, with the exception of the extreme value of the <inline-formula><mml:math id="inf40"><mml:mi>γ</mml:mi></mml:math></inline-formula> statistic (<inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> corresponds to a fully white image), the intensity level of a given statistic does not specify deterministically the resulting texture image. In our experiment, for any intensity level of each statistic, multiple, random instances of the textures were built to be presented to the rats during the discrimination task (see below for more details).</p><p>Subjects had to discriminate between visual textures containing one of the four selected statistics and white noise. Each texture had a size of 39 × 22 pixels and occupied the entire monitor (full-field stimuli). The pixels had a dimension of about 2 degrees of visual angle. Given that the maximal resolution of rat vision is about one cycle per degree (<xref ref-type="bibr" rid="bib18">Keller et al., 2000</xref>; <xref ref-type="bibr" rid="bib34">Prusky et al., 2000</xref>; <xref ref-type="bibr" rid="bib35">Prusky et al., 2002</xref>), such a choice of the pixel size guaranteed that the animals could discriminate between neighboring pixels of different color. Textures were showed at full-contrast over the LCD monitors that were calibrated in such a way to have minimal luminance of 0.126 ± 0.004 cd/mm (average ± SD across the six monitors), maximal luminance of 129 ± 5 cd/mm, and an approximately linear luminance response curve.</p></sec><sec id="s4-1-5"><title>Discrimination task</title><p>Each rat was trained to: (1) touch the central response port to trigger stimulus presentation and initiate a behavioral trial; and (2) touch one of the lateral response ports to report the identity of the visual stimulus and collect the reward (all the animals were trained with the following stimulus/response association: structured texture → left response port; white noise texture → right response port). The stimulus remained on the display until the animal responded or for a maximum of 5 s, after which the trial was considered as ignored. In case of a correct response the stimulus was removed, a positive reinforcement sound was played and a white (first animal batch) or gray (second batch) background was shown during delivery of the reward. In case of an incorrect choice, the stimulus was removed and a 1–3 s time-out period started, during which the screen flickered from middle-gray to black at a rate of 10 Hz, while a ‘failure’ sound was played. During this period the rat was not allowed to initiate a new trial. To prevent the rats from making impulsive random choices, trials where the animals responded in less than 300 or 400 ms were considered as aborted: the stimulus was immediately removed and a brief sound was played. In each trial, the visual stimuli had the same probability (50%) of being sampled from the pool of white noise textures or from the pool of structured textures, with the constraint that stimuli belonging to the same category were shown for at most <italic>n</italic> consecutive trials (with <italic>n</italic> varying between 2 and 3 depending on the animal and on the session), so as to prevent the rats from developing a bias toward one of the response ports.</p><p>Stimulus presentation, response collection and reward delivery were controlled via workstations running the open source suite MWorks (<ext-link ext-link-type="uri" xlink:href="https://mworks.github.io">https://mworks.github.io</ext-link>;<xref ref-type="bibr" rid="bib41">Starwarz and Cox, 2021</xref>).</p></sec><sec id="s4-1-6"><title>Experimental design</title><p>Each rat was assigned to a specific statistic, from one- to four-point, for which it was trained in phases I and II and then tested in phase III. Generalization to a different statistic from the one the rat was trained on was assessed in phase IV. Out of the 42 rats, 9 were trained with one-point statistics, 9 with two-point, 12 with three-point, and 12 with four-point. The animals that reached phase III were 9, 9, 8, and 11, respectively, for the four statistics.</p></sec><sec id="s4-1-7"><title>Phase I</title><p>Initially, rats were trained to discriminate unstructured textures made of white noise from structured textures containing a single high-intensity level of one of the statistics (for one-point and two-point: 0.85; for three-point and four-point: 0.95). To make sure that the animals learned a general distinction between structured and unstructured textures (and not between specific instances of the two stimulus categories), in each trial both kinds of stimuli were randomly sampled (without replacement) from a pool of 350 different textures. Since the rats typically performed between 200 and 300 trials in a training session, every single texture was not shown more than once. A different pool of textures was used in each of the five days within a week of training. The same five texture pools were then used again (in the same order) the following week. Therefore, at least 7 days had to pass before a given texture stimulus was presented again to a rat.</p><p>For the first batch of rats, we moved to the second phase of the experiment all the animals that were able to reach at least an average performance of 65% correct choices over a set of 500 trials, collected across a variable number of consecutive sessions (the learning curves of four example rats from this batch, one per group, are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). Based on this criterion, two rats tested with three-point textures and one rat tested with four-point textures were excluded from further testing. For the second batch of rats, we decided to admit all the animals to the following experimental phases after a prolonged period of training in the first phase. In fact, we reasoned that, in case some texture statistic was particularly hard to discriminate, imposing a criterion performance in the first phase of the experiment would bias the pool of rats tested with such very difficult statistic toward including only exceptionally proficient animals. This in turn, could lead to an overestimation of rat typical sensitivity to such difficult statistic. On the other hand, the failure of a rat to reach a given criterion performance could be due to intrinsic limitations of its visual apparatus (such as a malfunctioning retina or particularly low acuity). Therefore, to make sure that our result did not depend on including in our analysis some animals of the second batch that did not reach 65% correct discrimination in the first training phase, the perceptual sensitivities were re-estimated after excluding those rats (i.e. after excluding one rat from the two-point, three rats from the three-point, and one from the four-point groups). As shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, the resulting sensitivity ranking was unchanged (compare to <xref ref-type="fig" rid="fig2">Figure 2C</xref>) and all pairwise comparisons remained statistically significant (two-sample t-test with Holm-Bonferroni correction).</p></sec><sec id="s4-1-8"><title>Phase II</title><p>In this phase, we introduced progressively lower levels of intensity of each statistic, bringing them gradually closer to the zero-intensity level corresponding to white noise. To this end, we applied an adaptive staircase procedure to update the minimum level of the statistic to be presented to a rat based on its current performance. Briefly, in any given trial, the level of the multipoint correlation in the structured textures was randomly sampled between a minimum level (under the control of the staircase procedure) and a maximum level (fixed at the value used in phase I). Within this range, the sampling was not uniform, but was carried out using a geometric distribution (with the peak at the minimum level), so as to make much more likely for rats to be presented with intensity levels at or close to the minimum. The performance achieved by the rats on the current minimum intensity level was computed every ten trials. If such a performance was higher than 70% correct, the minimum intensity level was decreased by a step of 0.05. By contrast, if the performance was lower than 50%, the minimum intensity level was increased of the same amount.</p><p>This procedure allowed the rats to learn to discriminate progressively lower levels of the statistic in a gradual and controlled way (the asymptotic levels of the statistics reached across consecutive training sessions by four example rats of the first batch, one per group, are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). At the end of this phase, the minimum intensity level reached by the animal in the three groups was: 0.21 ± 0.12, 0.2 ± 0.2, 0.70 ± 0.22, and 0.56 ± 0.18 (group average ± SD) for, respectively, one-, two-, three-, and four-point correlations.</p></sec><sec id="s4-1-9"><title>Phase III</title><p>After the training received in phases I and II, the rats were finally moved to the main test phase, where we measured their sensitivity to the multipoint correlations they were trained on. In each trial of this phase, the stimulus was either white noise or a patterned texture with equal probability. If it was a patterned texture, the level of the statistic was randomly selected from the set {0.02, 0.09, 0.16, …, 0.93, 1} (i.e. from 0.02 to 1 in steps of 0.07) with uniform probability. The responses of each rat over this range of intensity levels yielded psychometric curves (see example in <xref ref-type="fig" rid="fig1">Figure 1B</xref>), from which rat sensitivity was measured by fitting the Bayesian ideal observer model described below (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>).</p></sec><sec id="s4-1-10"><title>Phase IV</title><p>To verify the sensitivity ranking observed in phase III, we carried out an additional test phase, where each rat was tested on a new statistic, which was different from the one the animal was previously trained and tested on. The two groups of rats that were originally trained with the statistics yielding the highest sensitivity in phase III (i.e. one- and two-point correlations; see <xref ref-type="fig" rid="fig2">Figure 2B</xref>) were split in approximately equally-sized subgroups and each of these subgroups was tested with the less discriminable statistics (i.e. three- and four-point correlations; leftmost half of <xref ref-type="fig" rid="fig2">Figure 2C</xref>). This allowed assessing that, regardless of the training history, sensitivity to four-point correlations was slightly but consistently higher than sensitivity to three-point correlations. For the group of rats originally tested with the three-point statistic, all the animals were switched to the four-point (third set of points in <xref ref-type="fig" rid="fig2">Figure 2C</xref>). This allowed comparing the sensitivities to these statistics at the within-subject level (notably, these rats were found to be significantly more sensitive to the four-point textures than to the three-point, despite the extensive training they had received with the latter). For the same reason, most of the rats (8/11) of the last group (i.e. the animals originally trained/tested with the four-point correlations; last set of points in <xref ref-type="fig" rid="fig2">Figure 2C</xref>) were switched to the three-point statistic, which yielded again the lowest discriminability. A few animals (3/11) were instead tested with the two-point statistic, thus verifying that the latter was much more discriminable than the four-point one (again, despite the extensive training the animals of this group had received with the four-point textures).</p><sec id="s4-1-10-1"><title>Data Availability</title><p>Experimental data are available at <xref ref-type="bibr" rid="bib3">Caramellino et al., 2021</xref>.</p></sec></sec></sec><sec id="s4-2"><title>Ideal observer model</title><p>In this section we describe the ideal observer model we used to estimate the sensitivity of the rats to the different textures. The approach is a standard one and is inspired by that in <xref ref-type="bibr" rid="bib9">Fleming et al., 2013</xref>. Because our intention is to use an ideal observer as a model for animal behavior, we will write interchangeably ‘rat’, ‘animal’, and ‘ideal observer’ in the following.</p><sec id="s4-2-1"><title>Preliminaries</title><p>The texture discrimination task is a two-alternative forced choice (2AFC) task, where the stimulus can be either a sample of white noise or a sample of textured noise, and the goal of the animal is to correctly report the identity of each stimulus. On any given trial, either stimulus class can happen with equal probability. The texture class is composed of <inline-formula><mml:math id="inf42"><mml:mi>K</mml:mi></mml:math></inline-formula> discrete, positive values of the texture. In practice, <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:math></inline-formula>, and these values are <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.02</mml:mn><mml:mo>,</mml:mo><mml:mn>0.09</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>0.93</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, but we’ll use a generic <inline-formula><mml:math id="inf45"><mml:mi>K</mml:mi></mml:math></inline-formula> in the derivations for clarity. The texture statistics are parametrised such that a statistic value of zero corresponds to white noise. Therefore, if we call <inline-formula><mml:math id="inf46"><mml:mi>s</mml:mi></mml:math></inline-formula> the true level of the statistic, the task is a parametric discrimination task where the animal has to distinguish <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-2"><title>Key assumptions</title><list list-type="order"><list-item><p>each trial is independent from those preceding and following it (both for the generated texture and for the animal’s behavior);</p></list-item><list-item><p>on any given trial, the nominal (true) value of the statistic is some value <inline-formula><mml:math id="inf49"><mml:mi>s</mml:mi></mml:math></inline-formula>. Because the texture has finite size, the empirical value of the statistic in the texture will be somewhat different from <inline-formula><mml:math id="inf50"><mml:mi>s</mml:mi></mml:math></inline-formula>. We lump this uncertainty together with that induced by the animal’s perceptual process, and we say that any given trial results on the production of a <italic>percept</italic> <inline-formula><mml:math id="inf51"><mml:mi>x</mml:mi></mml:math></inline-formula>, sampled from a truncated Normal distribution centered around the nominal value of the statistic and bounded between <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>:<inline-formula><mml:math id="inf54"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>where <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability density function of the standard Normal and <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is its cumulative density function. Setting the bounds to –1 and 1 allows us to account for the fact that the value of a statistic is constrained within this range by construction. We will keep <inline-formula><mml:math id="inf57"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mi>b</mml:mi></mml:math></inline-formula> in some of the expressions below for generality and clarity, and we will substitute their values only at the end.</p></list-item><list-item><p>we assume that each rat has a certain prior over the statistic level that we parametrise by the log prior odds:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>α</mml:mi><mml:mo>:=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf59"><mml:mi>α</mml:mi></mml:math></inline-formula> depends on the rat. More specifically, we assume that each rat assigns a prior probability <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the presentation of a noise sample, and a probability of <inline-formula><mml:math id="inf61"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the presentation of a texture coming from any of the <inline-formula><mml:math id="inf62"><mml:mi>K</mml:mi></mml:math></inline-formula> nonzero statistic values. In formulae: <inline-formula><mml:math id="inf63"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mfrac><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf64"><mml:mi>δ</mml:mi></mml:math></inline-formula> is Kronecker’s delta, and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are the <inline-formula><mml:math id="inf66"><mml:mi>K</mml:mi></mml:math></inline-formula> possible nonzero values of the statistic. Note that this choice of prior matches the distribution actually used in generating the data for the experiment, except that <inline-formula><mml:math id="inf67"><mml:mi>α</mml:mi></mml:math></inline-formula> is a free parameter instead of being fixed at 0.</p></list-item><list-item><p>we assume that the true values of <inline-formula><mml:math id="inf68"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf69"><mml:mi>σ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf70"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mi>b</mml:mi></mml:math></inline-formula> are accessible to the decision making process of the rat.</p></list-item></list></sec><sec id="s4-2-3"><title>Derivation of the ideal observer</title><p>For a particular percept, the ideal observer will evaluate the posterior probability of noise vs texture given that percept. It will report ‘noise’ if the posterior of noise is higher than the posterior of texture, and ‘texture’ otherwise.</p><p>More in detail, for a given percept <inline-formula><mml:math id="inf72"><mml:mi>x</mml:mi></mml:math></inline-formula> we can define a decision variable <inline-formula><mml:math id="inf73"><mml:mi>D</mml:mi></mml:math></inline-formula> as the log posterior ratio:<disp-formula id="equ3"><label>(1)</label><mml:math id="m3"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>With this definition, the rat will report ‘noise’ when <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and ‘texture’ otherwise.</p><p>By plugging in the likelihood functions and our choice of prior, we get<disp-formula id="equ4"><label>(2)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Now, remember that <italic>given a value of the percept x</italic>, the decision rule based on <inline-formula><mml:math id="inf75"><mml:mi>D</mml:mi></mml:math></inline-formula> is fully deterministic (maximum a posteriori estimate). But on any given trial we don’t know the value of the percept — we only know the nominal value of the statistic. On the other hand, our assumptions above specify the distribution <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for any <inline-formula><mml:math id="inf77"><mml:mi>s</mml:mi></mml:math></inline-formula>, so the deterministic mapping <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> means that we can compute the probability of reporting ‘noise’ as,<disp-formula id="equ5"><label>(3)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>report noise</mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>x</mml:mi></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We note at this point that <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is monotonic: indeed,<disp-formula id="equ6"><label>(4)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mtext> for all </mml:mtext><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where for the last inequality we have used the fact that <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> &lt;<italic> b</italic> and therefore, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This result matches the intuitive expectation that a change in percept in the positive direction (i.e. away from zero) should always make it less likely for the observer to report ‘noise’.</p><p>Because <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is monotonic, there will be a unique value of <inline-formula><mml:math id="inf83"><mml:mi>x</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf84"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and the integration region <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> will simply consist of all values of <inline-formula><mml:math id="inf86"><mml:mi>x</mml:mi></mml:math></inline-formula> smaller than that. More formally, if we define<disp-formula id="equ7"><label>(5)</label><mml:math id="m7"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> such that </mml:mtext><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>we can write<disp-formula id="equ8"><label>(6)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>report noise</mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:mi>σ</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where in the last passage we have substituted <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2-4"><title>Example: single-level discrimination case</title><p>To give an intuitive interpetation of the results above, consider the case where <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, so the possible values of the statistic are only two, namely 0 and <italic>s</italic><sub>1</sub>. In this case,<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>β</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>so that we can write <inline-formula><mml:math id="inf90"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> in closed form:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi/><mml:mo>*</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which can be read as saying that the decision boundary is halfway between 0 and <italic>s</italic><sub>1</sub>, plus a term that depends on the prior bias and the effect of the boundaries of the domain of <inline-formula><mml:math id="inf91"><mml:mi>x</mml:mi></mml:math></inline-formula> (but involves the sensitivity too, represented by <inline-formula><mml:math id="inf92"><mml:mi>σ</mml:mi></mml:math></inline-formula>).</p><p>Simplifying things even further, if we remove the domain boundaries (by setting <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>a</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>b</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), we have that <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>β</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this case, by plugging the expression above in <xref ref-type="disp-formula" rid="equ1">Equation 6</xref> we obtain,<disp-formula id="equ12"><label>(7)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>report noise</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mi>σ</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and therefore we recover a simple cumulative Normal form for the psychometric function. By looking at <xref ref-type="disp-formula" rid="equ1">Equation 7</xref> it is clear how the prior bias <inline-formula><mml:math id="inf96"><mml:mi>α</mml:mi></mml:math></inline-formula> introduces a horizontal shift in the psychometric curve, and <inline-formula><mml:math id="inf97"><mml:mi>σ</mml:mi></mml:math></inline-formula> controls the slope (but also affects the horizontal location when <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>α</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-2-5"><title>Fitting the ideal observer model to the experimental data</title><p>Independently for each rat, we infer a value of <inline-formula><mml:math id="inf99"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:mi>σ</mml:mi></mml:math></inline-formula> by maximising the likelihood of the data under the model above. More in detail, for a given rat and a given statistic value <inline-formula><mml:math id="inf101"><mml:mi>s</mml:mi></mml:math></inline-formula> (including 0), we call <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> the number of times the rat reported ‘noise’, and <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> the total number of trials. For a given fixed value of <inline-formula><mml:math id="inf104"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mi>σ</mml:mi></mml:math></inline-formula>, under the ideal observer model the likelihood of <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> will be given by a Binomial probability distribution for <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> trials and probability of success given by the probability of reporting noise in <xref ref-type="disp-formula" rid="equ1">Equation 6</xref>,<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac linethickness="0pt"><mml:msub><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>rep. noise</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>rep. noise</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Assuming that the data for the different values of <inline-formula><mml:math id="inf108"><mml:mi>s</mml:mi></mml:math></inline-formula> is conditionally independent given <inline-formula><mml:math id="inf109"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:mi>σ</mml:mi></mml:math></inline-formula>, the total log likelihood for the data of the given rat is simply the sum of the log likelihoods for the individual values of <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>,<disp-formula id="equ14">.<mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We find numerically the values of <inline-formula><mml:math id="inf112"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf113"><mml:mi>σ</mml:mi></mml:math></inline-formula> that maximise this likelihood, using Matlab’s mle function with initial condition <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula>. Note that evaluating the likelihood for any given value of <inline-formula><mml:math id="inf116"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mi>σ</mml:mi></mml:math></inline-formula> requires finding <inline-formula><mml:math id="inf118"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, defined as the zero of <xref ref-type="disp-formula" rid="equ4">Equation 2</xref>. We do this numerically by using Matlab’s fzero function with initial condition <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2-6"><title>Comparing the estimated sensitivity in rats to sensitivity in humans and variability in natural images</title><p>To compare quantitatively our sensitivity estimates in rat to those in humans and to the variance of the statistics in natural images reported in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>, we computed the <italic>degree of correspondence</italic>, as defined in <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>, between these sets of numbers. Briefly, define <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the array containing the rat sensitivities for the three statistics that were tested both here and by <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> (<inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>⌟</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the notation used by <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref>), <italic>s</italic><sub><italic>h</italic></sub> as the array containing the corresponding values for humans, and <inline-formula><mml:math id="inf123"><mml:mi>v</mml:mi></mml:math></inline-formula> as that containing the standard deviations of the distribution of the corresponding statistics in natural images. For our comparisons, we use the values of <inline-formula><mml:math id="inf124"><mml:mi>v</mml:mi></mml:math></inline-formula> reported by <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> for the image analysis defined by the parameters <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></inline-formula> (i.e. the analysis used for the numbers reported in the table in <xref ref-type="fig" rid="fig3">Figure 3C</xref> in their paper). The degree of correspondence between any two of these arrays is their cosine dissimilarity:</p><p><inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>rat</mml:mtext><mml:mo>,</mml:mo><mml:mtext>human</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>rat</mml:mtext><mml:mo>,</mml:mo><mml:mtext>images</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>v</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>human</mml:mtext><mml:mo>,</mml:mo><mml:mtext>images</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>v</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The degree of correspondence is limited by construction to values between 0 and 1, with one indicating a perfect correspondence up to a scaling factor. <xref ref-type="bibr" rid="bib13">Hermundstad et al., 2014</xref> report values of 0.987–0.999 for <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>human</mml:mtext><mml:mo>,</mml:mo><mml:mtext>images</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, averaging over all texture coordinates and depending on the details of the analysis.</p><p>To assess statistical significance of our values of <inline-formula><mml:math id="inf129"><mml:mi>c</mml:mi></mml:math></inline-formula>, we compare our estimated values with the null probability distribution of the cosine dissimilarity of two unit vectors sampled randomly in the positive orthant of the 3-dimensional Euclidean space. If such vectors are described, in spherical coordinates, as<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo rspace="12.5pt">,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf130"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the cosine of the angle they form with each other is<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mtext>null</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The p-values reported in the text for <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>rat</mml:mtext><mml:mo>,</mml:mo><mml:mtext>human</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>rat</mml:mtext><mml:mo>,</mml:mo><mml:mtext>images</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are computed by sampling 10<sup>7</sup> values of <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>c</mml:mi><mml:mtext>null</mml:mtext></mml:msub></mml:math></inline-formula>, and assessing the fraction of samples with values larger than the empirical estimates.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All animal procedures were conducted in accordance with the international and institutional standards for the care and use of animals in research and were approved by the Italian Ministry of Health and after consulting with a veterinarian (Project DGSAF 25271, submitted on December 1, 2014 and approved on September 4, 2015, approval 940/2015-PR).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-72081-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Experimental data are available at (Caramellino et al., 2021).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Caramellino</surname><given-names>R</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Buccellato</surname><given-names>A</given-names></name><name><surname>Carboncino</surname><given-names>A</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data from &quot;Rat sensitivity to multipoint statistics is predicted by efficient coding of natural scenes&quot;</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.4762567</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We acknowledge the financial support of the European Research Council Consolidator Grant project no. 616803-LEARN2SEE (DZ), the National Science Foundation grant 1734030 (VB), the National Institutes of Health grant R01NS113241 (EP) and the Computational Neuroscience Initiative of the University of Pennsylvania (VB). These funding sources had no role in the design of this study and its execution, as well as in the analyses, interpretation of the data, or decision to submit results.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alemi-Neissi</surname><given-names>A</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multifeatural shape processing in rats engaged in invariant visual object recognition</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>5939</fpage><lpage>5956</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3629-12.2013</pub-id><pub-id pub-id-type="pmid">23554476</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Redlich</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Towards a Theory of Early Visual Processing</article-title><source>Neural Computation</source><volume>2</volume><fpage>308</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1162/neco.1990.2.3.308</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Caramellino</surname><given-names>R</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Buccellato</surname><given-names>A</given-names></name><name><surname>Carboncino</surname><given-names>A</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data from”rat sensitivity to multipoint statistics is predicted by efficient coding of natural scenes</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.4763647</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>NL</given-names></name><name><surname>Ming</surname><given-names>VL</given-names></name><name><surname>Deweese</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sparse codes for speech predict spectrotemporal receptive fields in the inferior colliculus</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002594</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002594</pub-id><pub-id pub-id-type="pmid">22807665</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Keyser</surname><given-names>R</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cue-invariant shape recognition in rats as tested with second-order contours</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.15.14</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Djurdjevic</surname><given-names>V</given-names></name><name><surname>Ansuini</surname><given-names>A</given-names></name><name><surname>Bertolini</surname><given-names>D</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Accuracy of Rats in Discriminating Visual Objects Is Explained by the Complexity of Their Perceptual Strategy</article-title><source>Current Biology</source><volume>28</volume><fpage>1005</fpage><lpage>1015</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.02.037</pub-id><pub-id pub-id-type="pmid">29551414</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1038/35090500</pub-id><pub-id pub-id-type="pmid">11518957</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>SM</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The irrationality of categorical perception</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19060</fpage><lpage>19070</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1263-13.2013</pub-id><pub-id pub-id-type="pmid">24305804</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contributions of ideal observer theory to vision research</article-title><source>Vision Research</source><volume>51</volume><fpage>771</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.09.027</pub-id><pub-id pub-id-type="pmid">20920517</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Andermann</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A mouse model of higher visual cortical function</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>28</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.08.009</pub-id><pub-id pub-id-type="pmid">24492075</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Higher-Order Areas of the Mouse Visual Cortex</article-title><source>Annual Review of Vision Science</source><volume>3</volume><fpage>251</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061331</pub-id><pub-id pub-id-type="pmid">28746815</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Conte</surname><given-names>MM</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Variance predicts salience in central sensory processing</article-title><source>eLife</source><volume>3</volume><elocation-id>03722</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03722</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberman</surname><given-names>AD</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What can mice tell us about how vision works?</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>464</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2011.07.002</pub-id><pub-id pub-id-type="pmid">21840069</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>JJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Goodhill</surname><given-names>GJ</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sparse Coding Can Predict Primary Visual Cortex Receptive Field Changes Induced by Abnormal Visual Input</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003005</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003005</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaliukhovich</surname><given-names>DA</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hierarchical stimulus processing in rodent primary and lateral visual cortex as assessed through neuronal selectivity and repetition suppression</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>926</fpage><lpage>941</lpage><pub-id pub-id-type="doi">10.1152/jn.00673.2017</pub-id><pub-id pub-id-type="pmid">29742022</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Weigelt</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual cortical networks: of mice and men</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>202</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.01.019</pub-id><pub-id pub-id-type="pmid">23415830</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>J</given-names></name><name><surname>Strasburger</surname><given-names>H</given-names></name><name><surname>Cerutti</surname><given-names>DT</given-names></name><name><surname>Sabel</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Assessing spatial vision - automated measurement of the contrast-sensitivity function in the hooded rat</article-title><source>Journal of Neuroscience Methods</source><volume>97</volume><fpage>103</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/s0165-0270(00)00173-4</pub-id><pub-id pub-id-type="pmid">10788664</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural representations for object perception: structure, category, and adaptive coding</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>45</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-060909-153218</pub-id><pub-id pub-id-type="pmid">21438683</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A Simple Coding Procedure Enhances a Neuron’s Information Capacity</article-title><source>Zeitschrift Für Naturforschung C</source><volume>36</volume><fpage>910</fpage><lpage>912</lpage><pub-id pub-id-type="doi">10.1515/znc-1981-9-1040</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehky</surname><given-names>SR</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural representation for object recognition in inferotemporal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>23</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.001</pub-id><pub-id pub-id-type="pmid">26771242</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Yao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Contrast-dependent OFF-dominance in cat primary visual cortex facilitates discrimination of stimuli with natural contrast statistics</article-title><source>The European Journal of Neuroscience</source><volume>39</volume><fpage>2060</fpage><lpage>2070</lpage><pub-id pub-id-type="doi">10.1111/ejn.12567</pub-id><pub-id pub-id-type="pmid">24931049</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matteucci</surname><given-names>G</given-names></name><name><surname>Bellacosa Marotti</surname><given-names>R</given-names></name><name><surname>Riggi</surname><given-names>M</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nonlinear Processing of Shape Information in Rat Lateral Extrastriate Cortex</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>1649</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1938-18.2018</pub-id><pub-id pub-id-type="pmid">30617210</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matteucci</surname><given-names>G</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unsupervised experience with temporal continuity of the visual environment is causally involved in the development of V1 complex cells</article-title><source>Science Advances</source><volume>6</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aba3742</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minini</surname><given-names>L</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Do rats use shape to solve “shape discriminations”?</article-title><source>Learning &amp; Memory</source><volume>13</volume><fpage>287</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1101/lm.84406</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname><given-names>WF</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Efficient and adaptive sensory codes</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>998</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00846-0</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassi</surname><given-names>JJ</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Parallel processing strategies of the primate visual system</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>360</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nrn2619</pub-id><pub-id pub-id-type="pmid">19352403</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How Cortical Circuits Implement Cortical Computations: Mouse Visual Cortex as a Model</article-title><source>Annual Review of Neuroscience</source><volume>44</volume><fpage>517</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-102320-085825</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Higher order visual processing in macaque extrastriate cortex</article-title><source>Physiological Reviews</source><volume>88</volume><fpage>59</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1152/physrev.00008.2007</pub-id><pub-id pub-id-type="pmid">18195083</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Piasini</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>metex — maximum entropy textures</data-title><version designator="Version 1.1.0">Version 1.1.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/5561807#.YadOyHUzZUs">https://zenodo.org/record/5561807#.YadOyHUzZUs</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Soltuzu</surname><given-names>L</given-names></name><name><surname>Muratore</surname><given-names>P</given-names></name><name><surname>Caramellino</surname><given-names>R</given-names></name><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Temporal stability of stimulus representation increases along rodent visual cortical hierarchies</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>4448</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-24456-3</pub-id><pub-id pub-id-type="pmid">34290247</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decorrelation and efficient coding by retinal ganglion cells</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>628</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nn.3064</pub-id><pub-id pub-id-type="pmid">22406548</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prusky</surname><given-names>GT</given-names></name><name><surname>West</surname><given-names>PW</given-names></name><name><surname>Douglas</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Behavioral assessment of visual acuity in mice and rats</article-title><source>Vision Research</source><volume>40</volume><fpage>2201</fpage><lpage>2209</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(00)00081-x</pub-id><pub-id pub-id-type="pmid">10878281</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prusky</surname><given-names>GT</given-names></name><name><surname>Harker</surname><given-names>KT</given-names></name><name><surname>Douglas</surname><given-names>RM</given-names></name><name><surname>Whishaw</surname><given-names>IQ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Variation in visual acuity within pigmented, and between pigmented and albino rat strains</article-title><source>Behavioural Brain Research</source><volume>136</volume><fpage>339</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/s0166-4328(02)00126-2</pub-id><pub-id pub-id-type="pmid">12429395</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purpura</surname><given-names>KP</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Katz</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Striate cortex extracts higher-order spatial correlations from visual textures</article-title><source>PNAS</source><volume>91</volume><fpage>8482</fpage><lpage>8486</lpage><pub-id pub-id-type="doi">10.1073/pnas.91.18.8482</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratliff</surname><given-names>CP</given-names></name><name><surname>Borghuis</surname><given-names>BG</given-names></name><name><surname>Kao</surname><given-names>YH</given-names></name><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Retina is structured to process an excess of darkness in natural scenes</article-title><source>PNAS</source><volume>107</volume><fpage>17368</fpage><lpage>17373</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005846107</pub-id><pub-id pub-id-type="pmid">20855627</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinagel</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Using rats for vision research</article-title><source>Neuroscience</source><volume>296</volume><fpage>75</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2014.12.025</pub-id><pub-id pub-id-type="pmid">25542420</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural signal statistics and sensory gain control</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1038/90526</pub-id><pub-id pub-id-type="pmid">11477428</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>EC</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient auditory coding</article-title><source>Nature</source><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Starwarz</surname><given-names>C</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>mworks</data-title><version designator="bd290b9">bd290b9</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mworks/mworks">https://github.com/mworks/mworks</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Laughlin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Principles of Neural Design</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262028707.001.0001</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Safaai</surname><given-names>H</given-names></name><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Vanzella</surname><given-names>W</given-names></name><name><surname>Riggi</surname><given-names>M</given-names></name><name><surname>Buffolo</surname><given-names>F</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22794</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22794</pub-id><pub-id pub-id-type="pmid">28395730</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teşileanu</surname><given-names>T</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptation of olfactory receptor abundances for efficient coding</article-title><source>eLife</source><volume>8</volume><elocation-id>e39279</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.39279</pub-id><pub-id pub-id-type="pmid">30806351</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tesileanu</surname><given-names>T</given-names></name><name><surname>Conte</surname><given-names>MM</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient coding of natural scene statistics predicts discrimination thresholds for grayscale textures</article-title><source>eLife</source><volume>9</volume><elocation-id>e54347</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54347</pub-id><pub-id pub-id-type="pmid">32744505</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkacik</surname><given-names>G</given-names></name><name><surname>Prentice</surname><given-names>JS</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Local statistics in natural scenes predict the saliency of synthetic textures</article-title><source>PNAS</source><volume>107</volume><fpage>18149</fpage><lpage>18154</lpage><pub-id pub-id-type="doi">10.1073/pnas.0914916107</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanzella</surname><given-names>W</given-names></name><name><surname>Grion</surname><given-names>N</given-names></name><name><surname>Bertolini</surname><given-names>D</given-names></name><name><surname>Perissinotto</surname><given-names>A</given-names></name><name><surname>Gigante</surname><given-names>M</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A passive, camera-based head-tracking system for real-time, three-dimensional estimation of head position and orientation in rodents</article-title><source>Journal of Neurophysiology</source><volume>122</volume><fpage>2220</fpage><lpage>2242</lpage><pub-id pub-id-type="doi">10.1152/jn.00301.2019</pub-id><pub-id pub-id-type="pmid">31553687</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vascon</surname><given-names>S</given-names></name><name><surname>Parin</surname><given-names>Y</given-names></name><name><surname>Annavini</surname><given-names>E</given-names></name><name><surname>D’Andola</surname><given-names>M</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Pelillo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Characterization of visual object representations in rat primary visual cortex</chapter-title><person-group person-group-type="editor"><name><surname>Vascon</surname><given-names>S</given-names></name></person-group><source>Lecture Notes in Computer Science</source><publisher-name>Springer International Publishing</publisher-name><fpage>577</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-11015-4_43</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A multivariate approach reveals the behavioral templates underlying visual discrimination in rats</article-title><source>Current Biology</source><volume>22</volume><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.041</pub-id><pub-id pub-id-type="pmid">22209530</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Gerich</surname><given-names>FJ</given-names></name><name><surname>Ytebrouck</surname><given-names>E</given-names></name><name><surname>Arckens</surname><given-names>L</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional specialization in rat occipital and temporal visual cortex</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1963</fpage><lpage>1983</lpage><pub-id pub-id-type="doi">10.1152/jn.00737.2013</pub-id><pub-id pub-id-type="pmid">24990566</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Conte</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Local image statistics: maximum-entropy constructions and perceptual salience</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>29</volume><fpage>1313</fpage><lpage>1345</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.29.001313</pub-id><pub-id pub-id-type="pmid">22751397</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recent Visual Experience Shapes Visual Processing in Rats through Stimulus-Specific Adaptation and Response Enhancement</article-title><source>Current Biology</source><volume>27</volume><fpage>914</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.02.024</pub-id><pub-id pub-id-type="pmid">28262485</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>LE</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Vision and cortical map development</article-title><source>Neuron</source><volume>56</volume><fpage>327</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.011</pub-id><pub-id pub-id-type="pmid">17964249</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>B</given-names></name><name><surname>Del Rosario</surname><given-names>J</given-names></name><name><surname>Muzzu</surname><given-names>T</given-names></name><name><surname>Peelman</surname><given-names>K</given-names></name><name><surname>Coletta</surname><given-names>S</given-names></name><name><surname>Bichler</surname><given-names>EK</given-names></name><name><surname>Speed</surname><given-names>A</given-names></name><name><surname>Meyer-Baese</surname><given-names>L</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Haider</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spatial modulation of dark versus bright stimulus responses in the mouse visual system</article-title><source>Current Biology</source><volume>31</volume><fpage>4172</fpage><lpage>4179</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.094</pub-id><pub-id pub-id-type="pmid">34314675</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xing</surname><given-names>D</given-names></name><name><surname>Yeh</surname><given-names>CI</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Generation of black-dominant responses in V1 cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>13504</fpage><lpage>13512</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2473-10.2010</pub-id><pub-id pub-id-type="pmid">20926676</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>CI</given-names></name><name><surname>Xing</surname><given-names>D</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>“Black” responses dominate macaque primary visual cortex v1</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>11753</fpage><lpage>11760</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1991-09.2009</pub-id><pub-id pub-id-type="pmid">19776262</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Schmid</surname><given-names>AM</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual processing of informative multipoint correlations arises primarily in V2</article-title><source>eLife</source><volume>4</volume><elocation-id>e06604</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06604</pub-id><pub-id pub-id-type="pmid">25915622</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Invariant visual object recognition and shape processing in rats</article-title><source>Behavioural Brain Research</source><volume>285</volume><fpage>10</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2014.12.053</pub-id><pub-id pub-id-type="pmid">25561421</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72081.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>The University of Chicago</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.05.17.444510" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.17.444510"/></front-stub><body><p>This work will be of interest to neuroscientists who want to understand how visual systems are tuned to and encode natural scenes. It reports that rats share phenomenology with humans in sensitivity to spatial correlations in scenes. This shows that an earlier paper's hypothesis about efficient coding may be more broadly applicable. This work also opens up the possibility of studying this kind of visual tuning in an animal where invasive techniques can be used to study this neural origins of this sensitivity and its development.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72081.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution>The University of Chicago</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.17.444510">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.05.17.444510v1.full">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Rat sensitivity to multipoint statistics is predicted by efficient coding of natural scenes&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>All reviewers thought that the paper was exciting, but needed some revision to clarify the results and presentation.</p><p>1) It would be useful if the manuscript scaled back claims about the alignment between the human and rat data slightly to make them more comparable with the results presented in the paper, and also to emphasize the ranking of sensitivities, qualitatively, is what's established, rather than a precise quantitative match to the full correlation matrix. Some specific points along these lines are:</p><p>– It is unclear why the ranking in rat sensitivity is evidence for efficient coding. In Hermundstad et al., 2014, efficient coding was established by comparing the image-based precision matrix with the human perceptual isodiscrimination contours. There is no such comparison here.</p><p>– Revision prompt (1a) Claims should be softened slightly.</p><p>– The previous paper emphasized that the difference of perceptual sensitivity between horizontal/vertical edges and diagonal edges is not merely an &quot;oblique effect&quot;: Horizontal and vertical pairwise correlation share an edge, while pixels involved in diagonal pairwise correlations only share a corner. One wonders whether rats show any sensitivity difference between horizontal/vertical edges and diagonal edges. The manuscript in its current form misses this important comparison. Without showing this, the rat sensitivity does not fully reproduce the trend previously observed in humans. It seems like acquiring new data from the rats is prohibitively time-consuming, so again, the claims of the paper should be softened a bit.</p><p>Revision prompt (1b) – If possible, it would be useful to see a comparison of the rat sensitivity to different 2-point correlations, and a note about whether it matches the human data or not.</p><p>Revision prompt (1c) – It would be very helpful if the authors can generate analysis as in Figure 3B or 3C in Hermundstad et al., 2014 (3C is maybe easier?). If such analysis is possible, then it shows that the rat sensitivity also quantitatively matches the results from efficient coding. Again, if this is prohibitive, claims should be softened.</p><p>2) One part of the analysis was unclear: Why does it work with this theory to find the sensitivity only to positive parity values?</p><p>It seemed surprising that one would not need to test negative pairwise correlations, negative 3-point correlations, or negative 4-point patterns. For the 3-point glider, in particular, the large correlated patches (triangles) change contrast when parity is inverted, so it is the only correlational stimulus in this set that inverts contrast under parity inversion (besides the trivial 1-point glider). Given the light-dark asymmetries of the natural world, it would seem possible that the three-point sensitivity depends (strongly?) on the parity. This seems to be true of some older point-statistic discrimination tasks in humans (from Chubb?), where the number of black pixels (rather than merely dark gray) seemed to account for human discrimination thresholds. The parity of 3-point gliders clearly makes an impact on motion perception when these are looked at in space-time (i.e., Hu and Victor and various subsequent work in flies and fish), and the percept strength is also different for positive vs. negative parity. So, given the contrast inversion asymmetry in 3-point gliders and prior work on light-dark asymmetries in discriminability, it seems one needs to test whether sensitivity is the same under positive and negative parity for these types of spatial correlations. If the authors contend that this is not necessary given the efficient coding hypothesis being tested, some discussion is warranted of light-dark asymmetries in natural scenes and in this suite of stimuli, and why they are neglected in this framework (if that's the case).</p><p>3) Figure 3 needs revision for clarity. All reviewers found the layout confusing. Perhaps the authors could find a clearer way to present the results, using more figure panels.</p><p>4) The luminance values listed for the visual stimuli seem rather odd, since the mean luminance is not the average of the max and min luminance (the light and dark pixels). This seems to imply that these patterns do contain not equal numbers of light and dark pixels, which they should for all the 2, 3, and 4 point glider stimuli. It's not clear how this is consistent with the described experiments. Please clarify this point in the text.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72081.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>All reviewers thought that the paper was exciting, but needed some revision to clarify the results and presentation.</p><p>1) It would be useful if the manuscript scaled back claims about the alignment between the human and rat data slightly to make them more comparable with the results presented in the paper, and also to emphasize the ranking of sensitivities, qualitatively, is what's established, rather than a precise quantitative match to the full correlation matrix. Some specific points along these lines are:</p><p>– It is unclear why the ranking in rat sensitivity is evidence for efficient coding. In Hermundstad et al., 2014, efficient coding was established by comparing the image-based precision matrix with the human perceptual isodiscrimination contours. There is no such comparison here.</p><p>– Revision prompt (1a) Claims should be softened slightly.</p></disp-quote><p>We thank the reviewers for underscoring the difference between, on the one hand, a quantitative comparison of the sensitivity to the variance of the statistics in natural images, and on the other hand a more qualitative comparison of their rank ordering. In our initial submission, we built our argument based on the rankings in order to better connect not only with Hermundstad et al., 2014, but also with earlier human psychophysics results on the same task (Victor and Conte 2012), where there was no comparison with natural image statistics and therefore only the qualitative ranking among sensitivities was examined. We also note that Hermundstad et al., do, in fact, make ample use of the rank-ordering agreement between natural image statistics and human sensitivity in order to support their argument (“rank-order”, or similar locutions, are used six times between the results and the discussion). In this sense, while it is true that “In Hermundstad et al., 2014, efficient coding was established by comparing the image-based precision matrix with the human perceptual isodiscrimination contours”, it is also true that the rank ordering was presented as part of the evidence for efficient coding.</p><p>Having said this, we nevertheless agree that our argument can be strengthened by presenting both approaches, qualitative and quantitative. We have now added a new figure (Figure 3), where we compare our estimates of psychophysical sensitivity in rats with the corresponding values for human psychophysics and natural image statistics reported in Hermunstad 2014 (note that we were only able to compare three out of the four statistics that we tested, as Hermunstad et al., did not consider 1-point correlations). The comparisons in Figure 3 (and the related quantitative measures reported in the text, lines 146-160) reveal a strong quantitative match, similar to that between the human psychophysics and the image statistic data.</p><p>Finally, in response to a specific point raised by Reviewer 2, point 1 (“Hermundstad et al., 2014 did not include 1<sup>st</sup>-order at all.”), the 1<sup>st</sup> order statistic γ was indeed only studied in Victor and Conte 2012, which only contained phychophysics data, and not in Hermundstad et al., 2014, which connected psychophysics with natural image statistics. Indeed, it is not possible to analyze the variability of γ in natural images with the method established by Hermundstad et al., 2014, because each image is binarized in such a way to guarantee that γ=0 by construction. In this sense, like the use of qualitative ranking discussed above, γ was included to better reflect the approach in Victor and Conte. Moreover, we wanted to include a sensory stimulus condition that we were sure the animals could detect well, in order to ensure that any failure to learn or perform the task was due to limitations in sensory processing and not in the learning or decision-making process. Before performing our experiments, the only statistic that we were confident the rats could be trained to distinguish from noise was γ [Tafazoli et al., 2017, Vascon et al., 2019], and therefore it made sense to include it in the experimental design. We have modified the Results (lines 90-93, 104-108), the Methods (316-321) and the Discussion (212-218) to express this point more clearly.</p><disp-quote content-type="editor-comment"><p>– The previous paper emphasized that the difference of perceptual sensitivity between horizontal/vertical edges and diagonal edges is not merely an &quot;oblique effect&quot;: Horizontal and vertical pairwise correlation share an edge, while pixels involved in diagonal pairwise correlations only share a corner. One wonders whether rats show any sensitivity difference between horizontal/vertical edges and diagonal edges. The manuscript in its current form misses this important comparison. Without showing this, the rat sensitivity does not fully reproduce the trend previously observed in humans. It seems like acquiring new data from the rats is prohibitively time-consuming, so again, the claims of the paper should be softened a bit.</p><p>Revision prompt (1b) – If possible, it would be useful to see a comparison of the rat sensitivity to different 2-point correlations, and a note about whether it matches the human data or not.</p></disp-quote><p>When designing our experiment, we prioritized collecting data for the other statistics as they were closer to the extremes of the measured sensitivity values, therefore offering a clearer signal for a comparison with rat data. For instance, had we found better sensitivity to 3- or 4-point statistics than to (horizontal) 2-point statistics, this would have been a very clear sign that perceptual sensitivity in rat is organized differently than in humans. Conversely, we reasoned that a comparison based on 2-point diagonal instead of 2-point horizontal would have been more easily muddled and made inconclusive by the experimental noise that we expected to observe in rats. We agree that, given the high precision of the quantitative match between rats, humans and image statistics now highlighted by the new Figure 3, it would be interesting to test rats also for their sensitivity to diagonal 2-point correlations and check whether they matched the pattern exhibited by humans. However, as the editor rightly surmises, acquiring new data at this stage would indeed be exceedingly time consuming. Therefore, we have modified the text to better highlight that we did not seek to replicate this particular result in Hermundstad et al., 2014 (as well as that we could not test as many correlation patterns as in Hermundstad et al., 2014 more generally, due to practical and ethical constraints). We also note that, since we did not test 2-point diagonal, we can’t draw conclusions similar to those in Hermundstad 2014 about the difference of an effect due to efficient coding and one due to a hypothetical oblique effect for the specific 2-point horizontal vs. diagonal comparison. These points are now all brought up in the Discussion of our revised manuscript (lines 189-208). It is also worth noting that the oblique effect was a minor point of the Hermundstad et al., paper and the main arguments did not hinge on it.</p><disp-quote content-type="editor-comment"><p>Revision prompt (1c) – It would be very helpful if the authors can generate analysis as in Figure 3B or 3C in Hermundstad et al., 2014 (3C is maybe easier?). If such analysis is possible, then it shows that the rat sensitivity also quantitatively matches the results from efficient coding. Again, if this is prohibitive, claims should be softened.</p></disp-quote><p>Thank you for the suggestion. As mentioned above, we have now added a new figure (Figure 3) where we compare rat sensitivity, human sensitivity, and image statistic data in a way similar to Figure 3B in Hermundstad 2014, for the image statistics that were tested in both our experiment and in Hermundstad et al., 2014. We have also computed the “degree of correspondence” <italic>c</italic> between rat and image data and between rat and human data, using the definition of this metric introduced by Hermundstad et al., and reported by them in Figure 3C and in the main text. The degree of correspondence captures quantitatively the excellent match between rat, human and image data, with c(rat, image)=0.986 and c(rat, human)=0.990, where 0≤c≤1, and c=1 indicates perfect match. These results are reported in the Results section of the updated manuscript (lines 146-160).</p><disp-quote content-type="editor-comment"><p>2) One part of the analysis was unclear: Why does it work with this theory to find the sensitivity only to positive parity values?</p><p>It seemed surprising that one would not need to test negative pairwise correlations, negative 3-point correlations, or negative 4-point patterns. For the 3-point glider, in particular, the large correlated patches (triangles) change contrast when parity is inverted, so it is the only correlational stimulus in this set that inverts contrast under parity inversion (besides the trivial 1-point glider). Given the light-dark asymmetries of the natural world, it would seem possible that the three-point sensitivity depends (strongly?) on the parity. This seems to be true of some older point-statistic discrimination tasks in humans (from Chubb?), where the number of black pixels (rather than merely dark gray) seemed to account for human discrimination thresholds. The parity of 3-point gliders clearly makes an impact on motion perception when these are looked at in space-time (i.e., Hu and Victor and various subsequent work in flies and fish), and the percept strength is also different for positive vs. negative parity. So, given the contrast inversion asymmetry in 3-point gliders and prior work on light-dark asymmetries in discriminability, it seems one needs to test whether sensitivity is the same under positive and negative parity for these types of spatial correlations. If the authors contend that this is not necessary given the efficient coding hypothesis being tested, some discussion is warranted of light-dark asymmetries in natural scenes and in this suite of stimuli, and why they are neglected in this framework (if that’s the case).</p></disp-quote><p>Before addressing the reviewer’s point, we should first clarify that the range of values of the 3-point statistic used in our experiment in fact spans the negative, rather than positive, half of the space of possibilities in the parameterization of Victor and Conte, 2012. We reported these as positive values in our initial submission due to an inversion of the 3-point axis in our code. This is simply a matter of convention and does not affect any of the arguments made in the paper, or the reviewer’s point, but we wanted to clarify this first. We have now explained in the Methods section (lines 334-338) that, although we still refer to 3-point intensities using positive numbers, if the reader is interested in connecting formally to the system of coordinates in Victor and Conte 2012, the sign of the values of 3-point statistics we report should be inverted.</p><p>In humans, the answer to the question raised by the reviewer is already known: Victor and Conte report that “consistently across subjects, thresholds for negative and positive variations of each statistic are closely matched” (Victor and Conte 2012, caption to Figure 7). Similarly, Hermundstad et al., 2014 remark on the same phenomenon and investigate it specifically (Hermundstad et al., 2014, Figure 3 —figure sup)</p><p>Moreover, even foregoing the above argument about human equal sensitivity to the positive and negative variations in the statistics, we observe the following with respect to the mention of the contrast inversion asymmetry in 3-point gliders, in relation to the light-dark asymmetry in natural scenes. Dominance of OFF responses (elicited by dark spots on a light background) has been reported in mammals, including primates, cats, and, more recently, rodents (Liu and Yao 2014; Xing, Yeh, and Shapley 2010; Yeh, Xing, and Shapley 2009; Williams et al., 2021). Therefore, if rats unlike humans had different sensitivity to positive and negative 3-point statistics, one would expect that the sensitivity to the negative 3-point correlations would be the highest of the two (as negative intensities corresponds to dark triangular patterns on white background). Since we are interested in the hypothesis that the 3-point configuration is the statistic with the lowest sensitivity of those we tested, by testing negative intensities we are choosing the stricter test, whereas testing positive values would risk biasing the experiment towards the desired conclusion. Indeed, this was the reason why the negative half of the 3-point axis was chosen in the first place.</p><p>As for the reason we used positive intensity values of the 2-point and 4-point statistics, this choice was dictated by the need of testing rats with textures containing features that were large enough to be processed by their low-resolution visual system. In fact, rat visual acuity is much lower than human acuity and, while positive 2-point and 4-point correlations give rise to, respectively, thick oriented stripes and wide rectangular blocks made of multiple pixels with the same color, negative intensities produce higher spatial frequency patterns, where color may change every other pixel see Figure 2A in Hermundstad et al., 2014. Therefore, using negative 2-point and 4-point statistics would have introduced a possible confound, since low sensitivity to these textures could have been simply due to the low spatial resolution of rat vision. Finally, in the case of the 1-point statistic, positive intensity values were chosen because they yield patterns that are brighter than white noise and, as such (we reasoned), would be highly distinguishable from white noise, given the high sensitivity of rat V1 neurons to increases of luminance.</p><p>All these explanations are now provided in the Methods section (lines 322-338) and a thorough discussion of the possible impact of our stimulus choices (both at the level of texture type and polarity) on our conclusions is now presented in the Discussion of our revised manuscript, including the rationale behind testing only on either positive or negative values of each given statistic (lines 189-249).</p><disp-quote content-type="editor-comment"><p>3) Figure 3 needs revision for clarity. All reviewers found the layout confusing. Perhaps the authors could find a clearer way to present the results, using more figure panels.</p></disp-quote><p>Thank you for this valuable suggestion. This figure (that in our revised manuscript has become Figure 4) has now been redesigned from scratch for better clarity. We still kept all data in a single panel in order to enable easy comparisons between conditions with same test statistic but different training, or same training and different test statistics.</p><disp-quote content-type="editor-comment"><p>4) The luminance values listed for the visual stimuli seem rather odd, since the mean luminance is not the average of the max and min luminance (the light and dark pixels). This seems to imply that these patterns do contain not equal numbers of light and dark pixels, which they should for all the 2, 3, and 4 point glider stimuli. It's not clear how this is consistent with the described experiments. Please clarify this point in the text.</p></disp-quote><p>Thank you for noticing this inconsistency with the luminance values reported in our Method section. In fact, while the minimal and maximal luminance values of the display were correctly reported in our sentence, the luminance that we reported as corresponding to mid-gray was incorrect. The actual value is, as it should, halfway between the maximum and minimum (average among monitors is equal to 61±8 cd/mm). This error was due to the fact that we erroneously reported the luminance of pixel intensity level 128 without taking into account the linearization of the pixel/luminance curve that we carried out before presenting the stimuli. We have now corrected this error in our revised Methods (lines 355-357), where we simply report the average maximal and minimal luminosity levels of the monitors.</p></body></sub-article></article>