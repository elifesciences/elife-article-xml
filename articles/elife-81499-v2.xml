<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81499</article-id><article-id pub-id-type="doi">10.7554/eLife.81499</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Contrasting action and posture coding with hierarchical deep neural network models of proprioception</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-308513"><name><surname>Sandbrink</surname><given-names>Kai J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa1">§</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-308514"><name><surname>Mamidanna</surname><given-names>Pranav</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2095-3314</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa2">#</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-308515"><name><surname>Michaelis</surname><given-names>Claudio</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-122748"><name><surname>Bethge</surname><given-names>Matthias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6417-7812</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-187946"><name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7368-4456</contrib-id><email>mackenzie@post.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-200232"><name><surname>Mathis</surname><given-names>Alexander</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3777-2202</contrib-id><email>alexander.mathis@epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>The Rowland Institute at Harvard, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Tübingen AI Center, Eberhard Karls Universität Tübingen &amp; Institute for Theoretical Physics</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Brain Mind Institute, School of Life Sciences, École Polytechnique Fédérale de Lausanne</institution></institution-wrap><addr-line><named-content content-type="city">Genève</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ba</surname><given-names>Demba</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>§</label><p>Department of Experimental Psychology, University of Oxford, Oxford, United Kingdom</p></fn><fn fn-type="present-address" id="pa2"><label>#</label><p>Department of Health Science and Technology, Aalborg University, Aalborg, Denmark</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>31</day><month>05</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e81499</elocation-id><history><date date-type="received" iso-8601-date="2022-06-30"><day>30</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-05-16"><day>16</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2020-05-08"><day>08</day><month>05</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.05.06.081372"/></event></pub-history><permissions><copyright-statement>© 2023, Sandbrink, Mamidanna et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Sandbrink, Mamidanna et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81499-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-81499-figures-v2.pdf"/><abstract><p>Biological motor control is versatile, efficient, and depends on proprioceptive feedback. Muscles are flexible and undergo continuous changes, requiring distributed adaptive control mechanisms that continuously account for the body’s state. The canonical role of proprioception is representing the body state. We hypothesize that the proprioceptive system could also be critical for high-level tasks such as action recognition. To test this theory, we pursued a task-driven modeling approach, which allowed us to isolate the study of proprioception. We generated a large synthetic dataset of human arm trajectories tracing characters of the Latin alphabet in 3D space, together with muscle activities obtained from a musculoskeletal model and model-based muscle spindle activity. Next, we compared two classes of tasks: trajectory decoding and action recognition, which allowed us to train hierarchical models to decode either the position and velocity of the end-effector of one’s posture or the character (action) identity from the spindle firing patterns. We found that artificial neural networks could robustly solve both tasks, and the networks’ units show tuning properties similar to neurons in the primate somatosensory cortex and the brainstem. Remarkably, we found uniformly distributed directional selective units only with the action-recognition-trained models and not the trajectory-decoding-trained models. This suggests that proprioceptive encoding is additionally associated with higher-level functions such as action recognition and therefore provides new, experimentally testable hypotheses of how proprioception aids in adaptive motor control.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>proprioception</kwd><kwd>somatosensory</kwd><kwd>deep learning</kwd><kwd>biomechanics</kwd><kwd>task-driven modeling</kwd><kwd>sensory systems</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>310030_201057</award-id><principal-award-recipient><name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>310030_212516</award-id><principal-award-recipient><name><surname>Mathis</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009835</institution-id><institution>Rowland Institute at Harvard</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>EPFL</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Mathis</surname><given-names>Alexander</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>To isolate and study proprioception, hierarchical deep neural networks paired with biomechanical models provide a normative approach to test the role of task effects on neural representations, such as the emergence of kinematic tuning and higher-level abstractions of actions.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Proprioception is a critical component of our ability to perform complex movements, localize our body’s posture in space, and adapt to environmental changes (<xref ref-type="bibr" rid="bib46">Miall et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Proske and Gandevia, 2012</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>). Our movements are generated by a large number of muscles and are sensed via a diverse set of receptors, most importantly muscle spindles, which carry highly multiplexed information (<xref ref-type="bibr" rid="bib13">Clark et al., 1985</xref>; <xref ref-type="bibr" rid="bib51">Proske and Gandevia, 2012</xref>). For instance, arm movements are sensed via distributed and individually ambiguous activity patterns of muscle spindles, which depend on relative joint configurations rather than the absolute hand position (<xref ref-type="bibr" rid="bib44">Matthews, 1963</xref>; <xref ref-type="bibr" rid="bib45">Matthews, 1981</xref>). Interpreting this high-dimensional input (around 50 muscles for a human arm) of distributed information at the relevant behavioral level poses a challenging decoding problem for the central nervous system (<xref ref-type="bibr" rid="bib7">Bernstein, 1967</xref>; <xref ref-type="bibr" rid="bib45">Matthews, 1981</xref>). Proprioceptive information from the receptors undergoes several processing steps before reaching somatosensory cortex (<xref ref-type="bibr" rid="bib9">Bosco et al., 1996</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Tuthill and Azim, 2018</xref>) – from the spindles that synapse in Clarke’s nucleus, to the brainstem, thalamus (<xref ref-type="bibr" rid="bib19">Francis et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>), and finally to somatosensory cortex (S1). In cortex, a number of tuning properties have been observed, such as responsiveness to varied combinations of joints and muscle lengths (<xref ref-type="bibr" rid="bib23">Goodman et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Chowdhury et al., 2020</xref>), sensitivity to different loads and angles (<xref ref-type="bibr" rid="bib20">Fromm and Evarts, 1982</xref>), and broad and uni-modal tuning for movement direction during arm movements (<xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>). The proprioceptive information in S1 is then hypothesized to serve as the basis of a wide variety of tasks via its connections to motor cortex and higher somatosensory processing regions (<xref ref-type="bibr" rid="bib46">Miall et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Proske and Gandevia, 2012</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Mathis et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Kumar et al., 2019</xref>).</p><p>One key role of proprioception is to sense the state of the body, that is, posture. This information subserves many other functions, from balance to motor learning. Thus, to gain insights into the computations of the proprioceptive system, we quantitatively compare two different goals in a task-driven fashion: a trajectory-decoding task and an action recognition task (ART) (<xref ref-type="fig" rid="fig1">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx1.F1">1</ext-link></xref>). The trajectory-decoding task represents the canonical view of proprioception (<xref ref-type="bibr" rid="bib51">Proske and Gandevia, 2012</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>). Alternatively, the role of the proprioceptive system might include inference of more abstract actions (i.e., complex sequences of postures). Our hypothesis is motivated by the observation that action segmentation would be an efficient way to represent complex behavior, and it could directly drive the action map in motor cortex (<xref ref-type="bibr" rid="bib24">Graziano, 2016</xref>). These two tasks also represent two different extremes for learning invariances: the trajectory-decoding task enforces invariance to ‘what’ is done, while the ART encourages invariance to ‘where’ something is done. Along this continuum, we also consider a variant of trajectory decoding which also predicts velocity. The ART is also motivated by the following observation: although the animal’s motor system is aware of its own actions (at least during volitional control), it may still be helpful to infer executed actions in order to direct corrective motor actions in the event of disturbances (<xref ref-type="bibr" rid="bib66">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib43">Mathis et al., 2017</xref>) or to serve as targets for action reinforcement (<xref ref-type="bibr" rid="bib42">Markowitz et al., 2023</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Contrasting spindle-based tasks to study proprioception: proprioceptive inputs that correspond to the tracing of individual letters were simulated using a musculoskeletal model of a human arm.</title><p>This scalable, large-scale dataset was used to train deep neural network models of the proprioceptive pathway either to classify the character (action recognition task [ART]) or to decode the posture of the arm (trajectory decoding tasks [TDTs]) based on the input muscle spindle firing rates. Here, we test two variants of the latter, to decode either position-only (canonical-proprioception) or position+velocity (as a control) of the end-effector. We then analyze these models and compare their tuning properties to the proprioceptive system of primates.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig1-v2.tif"/></fig><p>Large-scale datasets like ImageNet (<xref ref-type="bibr" rid="bib56">Russakovsky et al., 2015</xref>), that present a challenging visual object-recognition task, have allowed the training of deep neural networks whose representations closely resemble the tuning properties of single neurons in the ventral pathway of primates (<xref ref-type="bibr" rid="bib33">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib72">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib61">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Storrs et al., 2021</xref>). This goal-driven modeling approach (<xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib54">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Saxe et al., 2020</xref>) has since successfully been applied to other sensory modalities such as touch (<xref ref-type="bibr" rid="bib74">Zhuang et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Sundaram et al., 2019</xref>), thermosensation (<xref ref-type="bibr" rid="bib25">Haesemeyer et al., 2019</xref>), and audition (<xref ref-type="bibr" rid="bib32">Kell et al., 2018</xref>). However, unlike for vision and audition, where large annotated datasets of raw images or sounds are readily available, data for relevant proprioceptive stimuli (as well as task goals) are not.</p><p>To create a large-scale passive movement dataset, we started with human motion data for drawing different Latin characters (<xref ref-type="bibr" rid="bib70">Williams et al., 2006</xref>). Next, we used a musculoskeletal model of the human upper limb (<xref ref-type="bibr" rid="bib59">Saul et al., 2015</xref>) to generate muscle length configurations corresponding to drawing the pen-tip trajectories in multiple horizontal and vertical planes. We converted these into proprioceptive inputs using models of spindle Ia and II (<xref ref-type="bibr" rid="bib16">Dimitriou and Edin, 2008a</xref>; <xref ref-type="bibr" rid="bib17">Dimitriou and Edin, 2008b</xref>). We then used the tasks to train families of neural networks to either decode the full trajectory of the handwritten characters or classify the characters from the generated spindle firing rates. Through an extensive hyper-parameter search, we found neural networks for various architectures that solve the tasks. We then analyzed those models and found that models trained on action recognition, but not trajectory decoding, more closely resemble what is known about tuning properties in the proprioceptive pathway. Collectively, we present a framework for studying the proprioceptive pathway using goal-driven modeling by synthesizing datasets of muscle (spindle) activities in order to test theories of coding.</p><p>As in previous task-driven work for other sensory systems (<xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib32">Kell et al., 2018</xref>), we do not model the system in a closed-loop nature with a motor control model. Of course, tuning properties of proprioception are likely optimized jointly with the motor system (and possibly other systems). Studying proprioception with a basic open-loop model is important to (1) isolate proprioception, and (2) set the stage for comparing to more complex models, such as joint models of proprioception and motor control.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Muscle spindle-based biomechanical tasks</title><p>To model the proprioceptive system, we designed two classes of real-world proprioceptive tasks. The objectives were to either classify or reconstruct Latin alphabet characters (character recognition or trajectory decoding) based on the proprioceptive inputs that arise when the arm is passively moved (<xref ref-type="fig" rid="fig1">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx1.F1">1</ext-link></xref>). Thus, we computationally isolate proprioception from active movement, a challenge in experimental work. We used a dataset of pen-tip trajectories for the 20 characters that can be handwritten in a single stroke (thus excluding <italic>f, i, j, k, t,</italic> and <italic>x</italic>, which are multi-stroke) (<xref ref-type="bibr" rid="bib71">Williams, 2008</xref>; <xref ref-type="bibr" rid="bib70">Williams et al., 2006</xref>). Then, we generated 1 million end-effector (hand) trajectories by scaling, rotating, shearing, translating, and varying the speed, of each original trajectory (<xref ref-type="fig" rid="fig2">Figure 2A–C</xref>; <xref ref-type="table" rid="table1">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T1">1</ext-link></xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Synthetic proprioceptive characters dataset generation.</title><p>(<bold>A</bold>) Multiple examples of pen-tip trajectories for five of the 20 letters are shown. (<bold>B</bold>) The same trajectories as in A, plotted as time courses of Cartesian coordinates. (<bold>C</bold>) Creating (hand) end-effector trajectories from pen-tip trajectories. (<italic>Left</italic>) An example trajectory of character a resized to fit in a 10 × 10 cm<sup>2</sup> grid, linearly interpolated from the true trajectory while maintaining the true velocity profile. (<italic>Right</italic>) This trajectory is further transformed by scaling, rotating, and varying its speed. (<bold>D</bold>) Candidate starting points to write the character in space. (<italic>Left</italic>) A 2-link, 4 degrees of freedom (DoFs) model human arm is used to randomly select several candidate starting points in the workspace of the arm (<italic>right</italic>), such that written characters are all strictly reachable by the arm. (<bold>E</bold>) (<italic>Left to right and down</italic>) Given a sample trajectory in (C) and a starting point in the arm’s workspace, the trajectory is then drawn on either a vertical or horizontal plane that passes through the starting point. We then apply inverse kinematics to solve for the joint angles required to produce the traced trajectory. (<bold>F</bold>) (<italic>Left to right</italic>) The joint angles obtained in (E) are used to drive a musculoskeletal model of the human arm in OpenSim, to obtain equilibrium muscle fiber-length trajectories of 25 relevant upper arm muscles. These muscle fiber lengths and their instantaneous velocities together form the proprioceptive inputs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig2-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-81499-fig2-video1.mp4" id="fig2video1"><label>Figure 2—video 1.</label><caption><title>Supplementary video.</title><p>Video depicts the OpenSim model being passively moved to match the human-drawn character ‘a’ for three different variants; drawn vertically (left, right) and horizontally (middle).</p></caption></media></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Variable range for the data augmentation applied to the original pen-tip trajectory dataset.</title><p>Furthermore, the character trajectories are translated to start at various starting points throughout the arm’s workspace, overall yielding movements in 26 horizontal and 18 vertical planes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Type of variation</th><th align="left" valign="bottom">Levels of variation</th></tr></thead><tbody><tr><td align="left" valign="bottom">Scaling</td><td align="char" char="." valign="bottom">[0.7×, 1×, 1.3×]</td></tr><tr><td align="left" valign="bottom">Rotation</td><td align="char" char="." valign="bottom">[-<italic>π</italic>/6, -<italic>π</italic>/12, 0, <italic>π</italic>/12, <italic>π</italic>/6]</td></tr><tr><td align="left" valign="bottom">Shearing</td><td align="char" char="." valign="bottom">[-<italic>π</italic>/6, -<italic>π</italic>/12, 0, <italic>π</italic>/12, <italic>π</italic>/6]</td></tr><tr><td align="left" valign="bottom">Translation</td><td align="left" valign="bottom">Grid with a spacing of 3 cm</td></tr><tr><td align="left" valign="bottom">Speed</td><td align="char" char="." valign="bottom">[0.8×, 1×, 1.2×, 1.4×]</td></tr><tr><td align="left" valign="bottom">Plane of writing</td><td align="left" valign="bottom">[Horizontal (26), Vertical (18)]</td></tr></tbody></table></table-wrap><p>To translate end-effector trajectories into three-dimensional (3D) arm movements, we computed the joint-angle trajectories through inverse kinematics using a constrained optimization approach (<xref ref-type="fig" rid="fig2">Figure 2D–E</xref> and Methods). We iteratively constrained the solution space by choosing joint angles in the vicinity of the previous configuration in order to eliminate redundancy. To cover a large 3D workspace, we placed the characters in multiple horizontal (26) and vertical (18) planes and calculated corresponding joint-angle trajectories (starting points are illustrated in <xref ref-type="fig" rid="fig2">Figure 2D</xref>). A human upper-limb model in OpenSim (<xref ref-type="bibr" rid="bib59">Saul et al., 2015</xref>) was then used to compute equilibrium muscle lengths for 25 muscles in the upper arm that lead to the corresponding joint-angle trajectory (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>). We did not include hand muscles for simplicity, therefore the location of the end-effector is taken to be the hand location.</p><p>Based on these simulations, we generated proprioceptive inputs composed of muscle length and muscle velocity, which approximate receptor inputs during passive movement (see Methods). From this set, we selected a subset of 200,000 examples with smooth, non-jerky joint angle and muscle length changes, while ensuring that the set is balanced in terms of the number of examples per class (see Methods). Since not all characters take the same amount of time to write, we padded the movements with static postures corresponding to the starting and ending postures of the movement and randomized the initiation of the movement in order to maintain ambiguity about when the writing begins. At the end of this process, each sample consists of simulated proprioceptive inputs from each of the 25 muscles over a period of 4.8 s, simulated at 66.7 Hz. The dataset was split into a training, validation, and test set with a 72-8-20 ratio.</p></sec><sec id="s2-2"><title>Recognizing characters from muscle activity is challenging</title><p>We reasoned that several factors complicate the recognition of a specific character. First, the end-effector position is only present as a distributed pattern of muscle activity. Second, the same character will give rise to widely different proprioceptive inputs depending on different arm configurations.</p><p>To test these hypotheses, we first visualized the data at the level of proprioceptive inputs by using t-distributed stochastic neighbor embedding (t-SNE, <xref ref-type="bibr" rid="bib40">Maaten and Hinton, 2008</xref>). This illustrated that character identity was indeed entangled (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>A</xref>). Then, we trained pairwise support vector machine (SVM) classifiers as baseline models for character recognition. Here, the influence of the specific geometry of each character is notable. On average, the pairwise accuracy is 86.6 ± 12.5 (mean ± SD, <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>190</mml:mn></mml:mrow></mml:math></inline-formula> pairs, <xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>B</xref>). As expected, similar-looking characters were harder to distinguish at the level of the proprioceptive input, that is <italic>e</italic> and <italic>y</italic> were easily distinguishable but <italic>m</italic> and <italic>w</italic> were not (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>B</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Quantifying action recognition and trajectory decoding task performance.</title><p>(<bold>A</bold>) t-Distributed stochastic neighbor embedding (t-SNE) of the end-effector coordinates (left) and proprioceptive inputs (right). (<bold>B</bold>) Classification performance for all pairs of characters with binary support vector machines (SVMs) trained on proprioceptive inputs. Chance level accuracy is 50%. The pairwise accuracy is 86.6 ± 12.5% (mean ± SD, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>190</mml:mn></mml:mrow></mml:math></inline-formula> pairs). A subset of the data is also illustrated as a circular graph, whose edge color denotes the classification accuracy. For clarity, only pairs with performance less than 70% are shown, which corresponds to the bottom 12% of all pairs. (<bold>C</bold>) Performance of baseline models: Multi-class SVM performance computed using a one-vs-one strategy for different types of input/kinematic representations on the action recognition task (left). Performance of ordinary least-squares linear regression on the trajectory decoding (position) task (right). Note that end-effector coordinates, for which this analysis is trivial, are excluded. (<bold>D</bold>) Neural networks are trained on two main tasks: action recognition and trajectory decoding (of position) based on proprioceptive inputs. We tested three families of neural network architectures. Each model is comprised of one or more processing layers, as shown. Processing of spatial and temporal information takes place through a series of one-dimensional (1D) or two-dimensional (2D) convolutional layers or a recurrent layer. (<bold>E</bold>) Performance of neural network models on the tasks: the test performance of the 50 networks of each type is plotted against the number of layers of processing in the networks for the action recognition (left) and trajectory decoding (center) tasks separately and against each other (right). Note we jittered the number of layers (<italic>x</italic>-values) for visibility, but per model it is discrete.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Network performance.</title><p>(<bold>A</bold>) Training vs. test performance for all networks. Shallower networks tend to overfit more. (<bold>B</bold>) Network performance is plotted against the number of parameters. Note: Parameters of the final (fully connected) layer are not counted.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig3-figsupp1-v2.tif"/></fig></fig-group><p>To quantify the separability between all characters, we used a one-against-one strategy with trained pairwise classifiers (<xref ref-type="bibr" rid="bib29">Hsu and Lin, 2002</xref>). The performance of this multi-class decoder was poor regardless of whether the input was end-effector coordinates, joint angles, normalized muscle lengths, or proprioceptive inputs (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>C</xref>). Taken together, these analyses highlight that it is difficult to extract the character class from those representations as illustrated by t-SNE embedding (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>A</xref>) and quantified by SVMs (<xref ref-type="fig" rid="fig3">Figure 3B, C</xref>). In contrast, as expected, accurately decoding the end-effector position (by linear regression) from the proprioceptive input is much simpler, with an average decoding error of 1.72 cm, in a 3D workspace approximately 90 × 90 × 120 cm<sup>3</sup> (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>C</xref>).</p></sec><sec id="s2-3"><title>Neural networks models of proprioception</title><p>We explore the ability of three types of artificial neural network models (ANNs) to solve the proprioceptive character recognition and decoding tasks. ANNs are powerful models for both their performance and for their ability to elucidate neural representations and computations (<xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib26">Hausmann et al., 2021</xref>). An ANN consists of layers of simplified units (neurons) whose connectivity patterns mimic the hierarchical, integrative properties of biological neurons and anatomical pathways (<xref ref-type="bibr" rid="bib55">Rumelhart et al., 1986</xref>; <xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib54">Richards et al., 2019</xref>). As candidate models we parameterized a spatial-temporal convolutional neural network, a spatiotemporal convolutional network (both TCNs; <xref ref-type="bibr" rid="bib39">Lecun et al., 1998</xref>), and a recurrent neural network (a long short-term memory [LSTM] network; <xref ref-type="bibr" rid="bib27">Hochreiter and Schmidhuber, 1997</xref>), which impose different inductive priors on the computations. We refer to these three types as spatial-temporal, spatiotemporal, and LSTM networks (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>D</xref>).</p><p>Importantly, the different models differ in the way they integrate spatial and temporal information along the hierarchy. These two types of information can be processed either sequentially, as is the case for the spatial-temporal network type that contains layers with one-dimensional (1D) filters that first integrate information across the different muscles, followed by an equal number of layers that integrate only in the temporal dimension, or simultaneously, using two-dimensional (2D) kernels, as they are in the spatiotemporal network. In the LSTM networks, spatial information was integrated similarly to the spatial-temporal networks, before entering the LSTM layer.</p><p>Candidate models for each class can be created by varying hyper-parameters such as the number of layers, number and size of spatial and temporal filters, type of regularization, and response normalization (see <xref ref-type="table" rid="table2">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T2">2</ext-link></xref>, Methods). As a first step to restrict the number of models, we performed a hyper-parameter architecture search by selecting models according to their performance on the proprioceptive tasks. We should emphasize that our ANNs integrate along both proprioceptive inputs and time, unlike standard feed-forward CNN models of the visual pathway that just operate on images (<xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>). TCNs have been shown to be excellent for time-series modeling (<xref ref-type="bibr" rid="bib6">Bai et al., 2018</xref>), and therefore naturally describe neurons along a sensory pathway that integrates spatiotemporal inputs.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Hyper-parameters for neural network architecture search.</title><p>To form candidate networks, first a number of layers (per type) is chosen, ranging from 2 to 8 (in multiples of 2) for spatial-temporal models and 1–4 for the spatiotemporal and long short-term memory (LSTM) ones. Next, a spatial and temporal kernel size per Layer is picked where relevant, which remains unchanged throughout the network. For the spatiotemporal model, the kernel size is equal in both the spatial and temporal directions in each layer. Then, for each layer, an associated number of kernels/feature maps is chosen such that it never decreases along the hierarchy. Finally, a spatial and temporal stride is chosen. For the LSTM networks, the number of recurrent units is also chosen. All parameters are randomized independently and 50 models are sampled per network type. Columns 2–4: Hyper-parameter values for the top-performing models in the ART. The values given under the spatial rows count for both the spatial and temporal directions for the spatiotemporal model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Hyper-parameters</th><th align="left" valign="bottom">Spatial-temporal</th><th align="left" valign="bottom">Spatiotemporal</th><th align="left" valign="bottom">LSTM</th></tr></thead><tbody><tr><td align="left" valign="bottom">Num. layers</td><td align="left" valign="bottom">[1, 2, 3, 4]</td><td align="left" valign="bottom">4+4</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">3+1</td></tr><tr><td align="left" valign="bottom">Spatial kernels (pL)</td><td align="left" valign="bottom">[8, 16, 32, 64]</td><td align="left" valign="bottom">[8,16,16,32]</td><td align="left" valign="bottom">[8, 8, 32, 64]</td><td align="left" valign="bottom">[8, 16, 16]</td></tr><tr><td align="left" valign="bottom">Temporal kernels (pL)</td><td align="left" valign="bottom">[8, 16, 32, 64]</td><td align="left" valign="bottom">[32, 32, 64, 64]</td><td align="left" valign="bottom">n/a</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Spatial kernel size</td><td align="left" valign="bottom">[3, 5, 7, 9]</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">3</td></tr><tr><td align="left" valign="bottom">Temporal kernel size</td><td align="left" valign="bottom">[3, 5, 7, 9]</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">n/a</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Spatial stride</td><td align="left" valign="bottom">[1, 2]</td><td align="left" valign="bottom">9</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Temporal stride</td><td align="left" valign="bottom">[1, 2, 3]</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">n/a</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Num. recurrent units</td><td align="left" valign="bottom">[128, 256]</td><td align="left" valign="bottom">n/a</td><td align="left" valign="bottom">n/a</td><td align="left" valign="bottom">256</td></tr></tbody></table></table-wrap></sec><sec id="s2-4"><title>Architecture search and representational changes</title><p>To find models that could solve the proprioceptive tasks, we performed an architecture search and trained 150 models (50 models per type). Notably, we trained the same model (as specified by architectural parameters) on both classes of tasks by modifying the output and the loss function used to train the model. After training, all models were evaluated on an unseen test set (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>).</p><p>Models of all three types achieved excellent performance on the ART (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>E</xref>; multi-class accuracy of 98.86% ± 0.04, mean ± SEM for the best spatial-temporal model, 97.93% ± 0.03 for the best spatiotemporal model, and 99.56% ± 0.04 for the best LSTM model, <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> randomly initialized models). The parameters of the best-performing architectures are displayed in <xref ref-type="table" rid="table2">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T2">2</ext-link></xref>. The same models could also accurately solve the trajectory decoding task (TDT; position decoding) (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>E</xref>; with decoding errors of only 0.22 cm ± 0.005, mean ± SEM for the best spatial-temporal model, 0.13 cm ± 0.005 for the best spatiotemporal model, and 0.05 cm ± 0.01 for the best LSTM model, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> randomly initialized models). This decoding error is substantially lower than the linear readout (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Of the hyper-parameters considered, the depth of the networks influenced performance the most (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). Further, the performance on the two tasks was related: models performing well on one task tend to perform well on the other (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p>Having found models that robustly solve the tasks we sought to analyze their properties. We created five pre-training (untrained) and post-training (trained) pairs of models for the best-performing model architecture for further analysis. We will refer to those as instantiations. As expected, the untrained models performed at chance level (5%) on the ART.</p><p>How did the population activity change across the layers after learning the tasks? Here, we focus on the best spatial-temporal model and then show that our analysis extends to the other model types. We compared the representations across different layers for each trained model to its untrained counterpart by linear centered kernel alignment (CKA, see Methods). This analysis revealed that for all instantiations, the representations remained similar between the trained and untrained models for the first few layers and then deviated in the middle to final layers of the network (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Furthermore, trained models not only differed from the untrained ones but also across tasks, and the divergence appeared earlier (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>A</xref>). Therefore, we found that both training and the task substantially changed the representations. Next, we aimed to understand how the tasks are solved, that is, how the different stimuli are transformed across the hierarchy.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Low-dimensional embedding of network layers reveals structure.</title><p>(<bold>A</bold>) Similarity in representations (centered kernel alignment [CKA]) between the trained and the untrained models for each of the five instantiations of the best-performing spatial-temporal models (left and center). CKA between models trained on recognition vs. decoding (right). (<bold>B</bold>) t-Distributed stochastic neighbor embedding (t-SNE) for each layer of one instantiation of the best-performing spatial-temporal model trained on both tasks. Each data point is a random stimulus sample (<italic>N</italic>=2000, 50 per stimulus). (<bold>C</bold>) Representational dissimilarity matrices (RDMs). Character level representation is visualized using percentile RDMs for proprioceptive inputs (left) and final layer features (right) of one instantiation of the best-performing spatio-temporal model trained on the recognition task. (<bold>D</bold>) Similarity in stimulus representations between RDMs of an Oracle (ideal observer) and each layer for the five instantiations of the action recognition task (ART)-trained models and their untrained counterparts. (<bold>E</bold>) Decoding error (in cm) along the hierarchy for each model type on the trajectory decoding task. (<bold>F</bold>) Centered kernel alignment (CKA) between models trained on recognition vs. decoding for the five instantiations of all network types (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Extended analysis of network models.</title><p>(<bold>A</bold>) t-Distributed stochastic neighbor embedding (t-SNE) for each layer of the best spatiotemporal and long short-term memory (LSTM) model. Each data point is a random stimulus sample (<italic>N</italic>=4000, 200 per character). (<bold>B</bold>) Representational dissimilarity matrices (RDMs) of an ideal observer ‘Oracle’, which by definition has low dissimilarity for different samples of the same character and high dissimilarity for different samples of different characters. Character level representation are calculated through percentile RDMs for proprioceptive inputs and final layer features of one instantiation of the best-performing spatiotemporal and LSTM model trained on recognition task. (<bold>C</bold>) Centered kernel alignment (CKA) between models trained on recognition vs decoding for all network types (<inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> per network type).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig4-figsupp1-v2.tif"/></fig></fig-group><p>To illustrate the geometry of the ANN representations and how the different characters are disentangled across the hierarchy, we used t-SNE to visualize the structure of the hidden layer representations. For the ART, the different characters separate in the final layers of the processing hierarchy (spatial-temporal model: <xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>B</xref>; for the other model classes, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). To quantify this, we computed representational dissimilarity matrices (RDMs; see Methods). We found that different instances of the same characters were not represented similarly at the level of proprioceptive inputs, but rather at the level of the last convolutional layer for the trained models (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>C</xref>; for other model classes, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). To quantify how the characters are represented across the hierarchy, we computed the similarity to an Oracle’s RDM, where an Oracle (or ideal observer) would have a block structure, with dissimilarity 0 for all stimuli of the same class and 1 (100th percentile) otherwise (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). We found for all model instantiations that similarity only increased toward the last layers (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>D</xref>). This finding corroborates the visual impression gained via t-SNE that different characters are disentangled near the end of the processing hierarchy (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A, C</xref>).</p><p>How is the TDT solved across the hierarchy? In contrast to the ART-trained models, as expected, representations of characters remained entangled throughout (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). We found that the end-effector position can be decoded across the hierarchy (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>E</xref>). This result is expected, as even from the proprioceptive input a linear readout achieves good performance (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>C</xref>). We quantified CKA scores across the different architecture classes and found that with increasing depth the representations diverge between the two tasks (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>F</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>). Collectively, this suggests that characters are not immediately separable in ART models, but the end-effector can be decoded well in TDT models throughout the architecture.</p></sec><sec id="s2-5"><title>Single unit encoding properties and decodability</title><p>To gain insight into why ART- and TDT-trained models differ in their representations, we examined single unit tuning properties. In primates, these have been described in detail (<xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>), and thus present an ideal point of comparison. Specifically, we analyzed the units for end-effector position, speed, direction, velocity, and acceleration tuning. We performed these analyses by relating variables (such as movement direction) to the activity of single units during the continuous movement (see Methods). Units with a test <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> were considered tuned to that feature (this is a conservative value in comparison to experimental studies, e.g., 0.07 for <xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>).</p><p>Given the precedence in the literature, we focused on direction tuning in <italic>all horizontal</italic> planes. We fit directional tuning curves to the units with respect to the instantaneous movement direction. As illustrated in examples, the ART spatial-temporal model (as well as proprioceptive inputs), directional tuning can be observed for the typical units shown (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>). Spindle afferents are known to be tuned to motion, that is velocity and direction (<xref ref-type="bibr" rid="bib53">Ribot-Ciscar et al., 2003</xref>). We verified the tuning of the spindles and found that the spindle component tuned for muscle length is primarily tuned for position (median <inline-formula><mml:math id="inf7"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.36</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>) rather than kinematics (median direction <inline-formula><mml:math id="inf9"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, median velocity <inline-formula><mml:math id="inf10"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0026</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>), whereas the spindle component tuned for changes in muscle length were primarily tuned for kinematics (median direction <inline-formula><mml:math id="inf12"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.58</mml:mn></mml:mrow></mml:math></inline-formula>, velocity <inline-formula><mml:math id="inf13"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.83</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>), and poorly tuned for position (median <inline-formula><mml:math id="inf15"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0024</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>). For the ART model, direction selectivity was prominent in middle layers 1–6 before decreasing by layer 8, and a fraction of units exhibited tuning to other kinematic variables with <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx4">5</ext-link>C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A, B</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Analysis of single unit tuning properties for spatial-temporal models.</title><p>(<bold>A</bold>) Polar scatter plots showing the activation of units (radius <inline-formula><mml:math id="inf18"><mml:mi>r</mml:mi></mml:math></inline-formula>) as a function of end-effector direction, as represented by the angle <italic>θ</italic> for directionally tuned units in different layers of the top-performing spatial-temporal model trained on the action recognition task, where direction corresponds to that of the end-effector while tracing characters in the model workspace. The activation strengths of one (velocity-dependent) muscle spindle one unit each in layers 3, 5, and 8 are shown. (<bold>B</bold>) Similar to (A), except that now radius describes velocity and color represents activation strength. The contours are determined following linear interpolation, with gaps filled in by neighbor interpolation and smoothed using a Gaussian filter. Examples of one muscle spindle, one unit each in layers 3, 5, and 8, are shown. (<bold>C</bold>) For each layer of one trained instantiation, the units are classified into types based on their tuning. A unit was classified as belonging to a particular type if its tuning had a test <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Tested features were direction tuning, speed tuning, velocity tuning, Cartesian and polar position tuning, acceleration tuning, and label tuning (18/5446 scores excluded for action recognition task [ART]-trained, 430/5446 for trajectory decoding task [TDT]-trained; see Methods). (<bold>D</bold>) The same plot but for the spatial-temporal model of the same architecture but trained on the trajectory decoding task. (<bold>E</bold>) For an example instantiation, the distribution of test <inline-formula><mml:math id="inf20"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> scores for both the ART- and TDT-trained models are shown as vertical histograms (split-violins), for five kinds of kinematic tuning for each layer: direction tuning, speed tuning, Cartesian position tuning, polar position tuning, and label specificity indicated by different shades and arranged left-right for each layer including spindles. Tuning scores were excluded if they were equal to 1, indicating a constant neuron, or less than −0.1, indicating an improper fit (12/3890 scores excluded for ART, 285/3890 for TDT; see Methods). (<bold>F</bold>) The means of 90% quantiles over all five model instantiations of models trained on ART and TDT are shown for direction tuning (dark) and position tuning (light). 95% confidence intervals are shown over instantiations (<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Extended kinematic tuning of single neurons.</title><p>(<bold>A</bold>) For an example spatial-temporal model instantiation, the distribution of test <inline-formula><mml:math id="inf22"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> scores for both the action recognition task (ART)- and trajectory decoding task (TDT)-trained models are shown, for direction, speed, velocity, acceleration, and labels (12/3890 scores excluded over all layers for ART-trained, 290/3890 for TDT-trained; see Methods). (<bold>B</bold>) The individual traces (faint) as well as the means (dark) of 90% quantiles over all five model instantiations of models trained on action recognition and trajectory decoding are shown for direction tuning (solid line) and acceleration tuning (dashed line). (<bold>C, D</bold>) Same as (A, B) but for the spatiotemporal model (0/2330 scores excluded for ART-trained, 138/2330 for TDT; see Methods). (<bold>E, F</bold>) Same as (A, B) but for the long short-term memory (LSTM) model (10/6530 scores excluded for ART-trained, 1052/6530 for TDT-trained; see Methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Analysis of single unit tuning properties for action recognition task (ART)-trained and untrained models.</title><p>(<bold>A</bold>) For an example instantiation of the top-performing spatial-temporal model, the distribution of test <inline-formula><mml:math id="inf23"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> scores for both the trained and untrained model are shown, for five kinds of kinematic tuning for each layer: direction tuning, speed tuning, Cartesian position tuning, polar position tuning, and label specificity. The solid line connects the 90% quantiles of two of the tuning curve types, direction tuning (dark) and position tuning (light) (12/3890 scores excluded summed over all layers for ART-trained, 351/3890 for untrained; see Methods). (<bold>B</bold>) The means of 90% quantiles over all five model instantiations of models trained on action recognition and trajectory decoding are shown for direction tuning (dark) and position tuning (light). 95% confidence intervals are shown over instantiations (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) The same plot as in (<bold>A</bold>) but for the top-performing spatiotemporal model (0/2330 scores excluded for ART-trained, 60/2330 for the untrained; see Methods). (<bold>D</bold>) The same plot as (B) for the spatiotemporal model. (<bold>E</bold>) The same plot as in (<bold>A</bold>) but for the top-performing long short-term memory (LSTM) model (10/6530 scores excluded for ART-trained, 395/6530 for the untrained; see Methods). (<bold>F</bold>) The same plot as (B) for the LSTM model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Analysis of single unit tuning properties for trajectory decoding task (TDT)-trained and untrained models.</title><p>(<bold>A</bold>) For an example instantiation of the top-performing spatial-temporal model, the distribution of test <inline-formula><mml:math id="inf25"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> scores for both the trained and untrained model are shown, for five kinds of kinematic tuning for each layer: direction tuning, speed tuning, Cartesian position tuning, polar position tuning, and label specificity. The solid line connects the 90% quantiles of two of the tuning curve types, direction tuning (dark) and position tuning (light) (285/3890 scores excluded summed over all layers for TDT-trained, 351/3890 for untrained; see Methods). (<bold>B</bold>) The means of 90% quantiles over all five model instantiations of models trained on action recognition and trajectory decoding are shown for direction tuning (dark) and position tuning (light). 95% confidence intervals are shown over instantiations (<inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) The same plot as in (<bold>A</bold>) but for the top-performing spatiotemporal model (138/2330 scores excluded for TDT-trained, 60/2330 for the untrained; see Methods). (<bold>D</bold>) The same plot as (B), for the spatiotemporal model. (<bold>E</bold>) The same plot as in (<bold>A</bold>) but for the top-performing long short-term memory (LSTM) model (1052/6530 scores excluded for TDT-trained, 395/6530 for the untrained; see Methods). (<bold>F</bold>) The same plot as (B), for the LSTM model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Analysis of single unit tuning properties for spatiotemporal and long short-term memory (LSTM) models.</title><p>(<bold>A</bold>) For an example instantiation of the top-performing spatiotemporal model, the distribution of test <inline-formula><mml:math id="inf27"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> scores for both the action recognition task (ART)-trained and trajectory decoding task (TDT)-trained model are shown, for five kinds of kinematic tuning for each layer: direction tuning, speed tuning, Cartesian position tuning, polar position tuning, and label specificity. The solid line connects the 90% quantiles of two of the tuning curve types, direction tuning (dark) and position tuning (light) (0/2330 scores excluded summed over all layers for ART-trained, 138/2330 for TDT; see Methods). (<bold>B</bold>) The means of 90% quantiles over all five model instantiations of models trained on action recognition and trajectory decoding are shown for direction tuning (dark) and position tuning (light). 95% confidence intervals are shown over instantiations (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) The same plot as in (<bold>A</bold>) but for the top-performing LSTM model (10/6530 scores excluded for ART-trained, 1052/6530 for TDT; see Methods). (<bold>D</bold>) The same plot as (B), for the LSTM model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig5-figsupp4-v2.tif"/></fig></fig-group><p>In contrast, for the TDT model, no directional tuning was observed, but positional tuning was (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). These observations are further corroborated when comparing the distributions of tuning properties (<xref ref-type="fig" rid="fig5">Figure 5E</xref>) and 90% quantiles for all the instantiations (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). The difference in median tuning score between the two differently trained groups of models across the five model instantiations becomes significant starting in the first layer for both direction and position [Direction: (layer 1 <inline-formula><mml:math id="inf29"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>10.44</mml:mn></mml:mrow></mml:math></inline-formula>, p=0.0005; layer 2 <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>17.15</mml:mn></mml:mrow></mml:math></inline-formula>, p=6.78e-05; layer 3 <inline-formula><mml:math id="inf31"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>41.46</mml:mn></mml:mrow></mml:math></inline-formula>, p=2.02e-06; layer 4 <inline-formula><mml:math id="inf32"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>37.63</mml:mn></mml:mrow></mml:math></inline-formula>, p=2.98e-06; layer 5 <inline-formula><mml:math id="inf33"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>25.05</mml:mn></mml:mrow></mml:math></inline-formula>, p=1.51e-06; layer 6 <inline-formula><mml:math id="inf34"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>14.32</mml:mn></mml:mrow></mml:math></inline-formula>, p=0.0001; layer 7 <inline-formula><mml:math id="inf35"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.47</mml:mn></mml:mrow></mml:math></inline-formula>, p=0.026; layer 8 <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>7.61</mml:mn></mml:mrow></mml:math></inline-formula>, p=0.0016); Position: (layer 1 <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10.00</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.0006; layer 2 <inline-formula><mml:math id="inf38"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>24.62</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=1.62e-05; layer 3 <inline-formula><mml:math id="inf39"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>19.15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=4.38e-05; layer 4 <inline-formula><mml:math id="inf40"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>13.08</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.0002; layer 5 <inline-formula><mml:math id="inf41"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>21.57</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=2.73e-05; layer 6 <inline-formula><mml:math id="inf42"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>57.55</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=5.46e-07; layer 7 <inline-formula><mml:math id="inf43"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>20.80</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=3.16e-05; layer 8 <inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>16.08</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=8.76–05)].</p><p>Given that the ART models are trained to recognize characters, we asked if single units are well tuned for specific characters. To test this, we trained an SVM to classify characters from the single unit activations. Even in the final layer (before the readout) of the spatial-temporal model, the median classification performance over the five model instantiations as measured by the normalized area under the ROC curve-based selectivity index for single units was 0.210 ± 0.006 (mean ± SEM, <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> instantiations), and was never higher than 0.40 for any individual unit across all model instantiations (see Methods). Thus, even in the final layer, there are effectively no single-character-specific units. Of course, combining the different units of the final fully connected layer gives a high-fidelity readout of the character and allows the model to achieve high classification accuracy. Thus, character identity is represented in a distributed way. In contrast, and as expected, character identity is poorly encoded in single cells for the TDT model (<xref ref-type="fig" rid="fig5">Figure 5D and F</xref>). These main results also hold for the other architecture classes (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p><p>In spatiotemporal models, in which both spatial and temporal processing occurs between all layers, we observe a monotonic decrease in the directional tuning across the four layers for the ART task and a quick decay for the TDT task (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4A, B</xref>). Speed and acceleration tuning are present in the ART, but not in the TDT models (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B, C</xref>). Conversely, we find that positional coding is stable for TDT models and not the ART models. The same results hold true for LSTM models (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1E, F</xref> and <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4C, D</xref>). The differences in directional and Cartesian positional tuning were statistically significant for all layers according to a paired t-test with 4 degrees of freedom (DoFs) for both model types. Thus, for all architecture classes, we find that strong direction selective tuning is present in early layers of models trained with the ART task, but not the TDT task.</p><p>Our results suggest that the primate proprioceptive pathway is consistent with the action recognition hypothesis, but to corroborate this, we also assessed decoding performance, which measures representational information. For all architecture types, movement direction and speed can be better decoded from ART than from TDT-trained models (<xref ref-type="fig" rid="fig6">Figure 6A, C, E</xref>). In contrast, for all architectures, position can be better decoded for TDT- than for ART-trained models (<xref ref-type="fig" rid="fig6">Figure 6B, D, F</xref>). These results are consistent with the single-cell encoding results and again lend support for the proprioceptive system’s involvement in action representation.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Population decoding analysis of action recognition task (ART) vs. trajectory decoding task (TDT) models.</title><p>(<bold>A</bold>) Population decoding of speed (<italic>light</italic>) and direction (<italic>dark</italic>) for spatial-temporal models for the ART- and TDT-trained models. The faint line shows the <inline-formula><mml:math id="inf46"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Population decoding of end-effector position (<italic>X</italic> and <italic>Y</italic> coordinates) for spatial-temporal models. The faint line shows the <inline-formula><mml:math id="inf48"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) Same as (A) but for spatiotemporal models. (<bold>D</bold>) Same as (B) but for spatiotemporal models. (<bold>E</bold>) Same as (A) but for long short-term memory (LSTM) models. (<bold>F</bold>) Same as (B) but for LSTM models.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Analysis of population decoding for action recognition task (ART)-trained and untrained models.</title><p>(<bold>A</bold>) Population decoding of speed (<italic>light</italic>) and direction (<italic>dark</italic>) for the ART-trained and untrained for spatial-temporal models (<italic>left</italic>), spatiotemporal (<italic>middle</italic>), and long short-term memory (LSTM) (<italic>right</italic>) models. The faint line shows the <inline-formula><mml:math id="inf50"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Population decoding of end-effector position (<italic>X</italic> and <italic>Y</italic> coordinates) for spatial-temporal models. The faint line shows the <inline-formula><mml:math id="inf52"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Analysis of population decoding for trajectory decoding task (TDT)-trained and untrained models.</title><p>(<bold>A</bold>) Population decoding of speed (<italic>light</italic>) and direction (<italic>dark</italic>) for the TDT-trained and untrained for spatial-temporal models (<italic>left</italic>), spatiotemporal (<italic>middle</italic>), and long short-term memory (LSTM) (<italic>right</italic>) models. The faint line shows the <inline-formula><mml:math id="inf54"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Population decoding of end-effector position (<italic>X</italic> and <italic>Y</italic> coordinates) for spatial-temporal models (<italic>left</italic>), spatiotemporal (<italic>middle</italic>), and LSTM (<italic>right</italic>) models. The faint line shows the <inline-formula><mml:math id="inf56"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Results for the position and velocity trajectory decoding task (TDT-PV).</title><p>(<bold>A</bold>) For an example instantiation, the fraction of neurons that are tuned for a particular feature (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> on the relevant encoding model). Model architectures: (<italic>left</italic>) spatial-temporal, (<italic>middle</italic>) spatiotemporal, (<italic>right</italic>) long short-term memory (LSTM). Tested features were direction tuning, speed tuning, velocity tuning, Cartesian and polar position tuning, acceleration tuning, and label tuning (328/5446 scores excluded for TDT-PV-trained spatial-temporal model, 140/3262 for spatiotemporal, and 1150/9142 for LSTM; see Methods). (<bold>B</bold>) The means of 90% quantiles over all five model instantiations of models trained on action recognition task (ART) and TDT-PV are shown for direction tuning (dark) and position tuning (light). 95% confidence intervals are shown over instantiations (<inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) Population decoding of direction (<italic>dashed</italic>) and Cartesian coordinates (<italic>solid</italic>; mean over individually computed scores for <italic>X</italic> and <italic>Y</italic> directions taken) for the ART-trained and TDT-PV-trained for spatial-temporal models (<italic>left</italic>), spatiotemporal (<italic>middle</italic>), and LSTM (<italic>right</italic>) models. The faint line shows the <inline-formula><mml:math id="inf60"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> score for an individual model; the dark one the mean over all instantiations (<inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>D</bold>) For quantifying uniformity, we calculated the total absolute deviation from the corresponding uniform distribution over the bins in the histogram (red line in inset) for the spatial-temporal model (<italic>left</italic>), the spatiotemporal model (<italic>middle</italic>), and the LSTM model (<italic>right</italic>). Normalized absolute deviation from uniform distribution for preferred directions per instantiation is shown (<inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, faint lines) for TDT-PV-trained and untrained models as well as mean and 95% confidence intervals over instantiations (solid line; <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig6-figsupp3-v2.tif"/></fig></fig-group><p>So far, we have directly compared TDT and ART models. This does not address task training as such. Namely, we found directional selective units in ART models and positional-selective units in TDT models, but how do those models compare to randomly initialized (untrained) models? Remarkably, directional selectivity is similar for ART-trained and untrained models (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). In contrast to untrained models, ART-trained models have fewer positionally tuned units. The situation is reversed for TDT-trained models – those models gain positionally tuned units and lose directionally selective units during task training (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Consistent with those encoding results, position can be decoded less well from ART-trained models than from untrained models, and direction and speed similarly well (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Conversely, direction and speed can be worse and position better decoded from TDT-trained than untrained models (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>).</p><p>We found that while TDT models unlearn directional selective units (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>), ART models retain them (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). As an additional control, we wanted to test a task that does not only predict the position of the end-effector but also the velocity. We therefore trained the best five instantiations of all three model types on the position and velocity trajectory decoding task (TDT-PV) task (see Methods). We found that they could accurately predict both the location and the velocity (root mean squared error: 0.11, mean ± SEM for the best spatial-temporal model, 0.09 for the best spatiotemporal model, and 0.04 for the best LSTM model, <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> best models) and produced similar position decoding errors as the positional TDT-trained models (decoding errors for the TDT-PV-trained models: 0.26 cm ± 0.01, for the best spatial-temporal model, 0.20 cm ± 0.01 for the best spatiotemporal model, and 0.09 cm ±0.006 for the best LSTM model). What kind of tuning curves do these models have? We found that, for all architecture types, the models also unlearn directional selectivity and have similarly tuned units as for the positional TDT task, with only slightly more velocity-tuned neurons in the intermediate layers (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3A</xref>). Compared with the ART-trained models, the TDT-PV-trained models are tuned more strongly for end-effector position and less strongly for direction (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3B</xref>). As revealed by the decoding analysis, this difference also holds for the distributed representations of direction and position in the networks (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3C</xref>). Further, we found that even when additionally predicting velocity, the TDT task-type models represent velocity less well than ART-trained ones. Next, we looked at the statistics of preferred directions.</p></sec><sec id="s2-6"><title>Uniformity and coding invariance</title><p>We compared population coding properties to further elucidate the similarity to S1. We measured the distributions of preferred directions and whether coding properties are invariant across different workspaces (reaching planes). Prud’homme and Kalaska found a relatively uniform distribution of preferred directions in primate S1 during a center-out 2D manipulandum-based arm movement task (Figure 7A from <xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>). In contrast, most velocity-tuned spindle afferents have preferred directions located along one major axis pointing frontally and slightly away from the body (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Qualitatively, it appears that the ART-trained model had more uniformly distributed preferred directions in the middle layers compared to untrained models (<xref ref-type="fig" rid="fig7">Figure 7C</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Distribution of preferred directions and invariance of representation across workspaces.</title><p>(<bold>A</bold>) Adopted from <xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>; distribution of preferred directions in primate S1. (<bold>B</bold>) Distribution of preferred directions for spindle input. (<bold>C</bold>) Distribution of preferred directions for one spatial-temporal model instantiation (all units with <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> are included). Bottom: the corresponding untrained model. For visibility, all histograms are scaled to the same size and the colors indicate the number of tuned neurons. (<bold>D</bold>) For quantifying uniformity, we calculated the total absolute deviation from the corresponding uniform distribution over the bins in the histogram (red line in inset) for the spatial-temporal model (<italic>left</italic>), the spatiotemporal model (<italic>middle</italic>), and the long short-term memory (LSTM) model (<italic>right</italic>). Normalized absolute deviation from uniform distribution for preferred directions per instantiation is shown (<inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, faint lines) for trained and untrained models as well as mean and 95% confidence intervals over instantiations (solid line; <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). Note that there is no data for layers 7 and 8 of the trained spatial-temporal model, layer 8 of the untrained spatial-temporal model, and layer 4 of the spatiotemporal model as they have no direction-selective units (<inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). (<bold>E</bold>) For quantifying invariance, we calculated mean absolute deviation in preferred orientation for units from the central plane to each other vertical plane (for units with <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Results are shown for each instantiation (<inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, faint lines) for trained and untrained models plus mean (solid) and 95% confidence intervals over instantiations (<inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). Note that there is no data for layer 4 of the trained spatiotemporal model, as it has no direction-selective units (<inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Invariance of preferred orientations.</title><p>(<bold>A</bold>) To quantify invariance, we calculated mean absolute deviation in preferred orientation for units from a central plane at <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to each other horizontal plane (for units with <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Results are shown for each instantiation (<inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, faint lines) for trained and untrained models plus mean (solid) and 95% confidence intervals over instantiations (<inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) for the spatial-temporal (<italic>left</italic>), spatiotemporal (<italic>right</italic>), and long short-term memory (LSTM) (<italic>right</italic>) networks. Note that there is no data for layer 4 of the trained spatiotemporal model, as it has no direction-selective units (<inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). (<bold>B</bold>) Deviation in preferred direction for individual spindles (<italic>N</italic>=25). The preferred directions are fit for each plane and displayed in relation to a central horizontal (<italic>left</italic>) and vertical plane (<italic>right</italic>). Individual gray lines are for all units (spindles) with <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the thick red line marks the mean. (<bold>C</bold>) Same as (B), but for direction tuning in vertical planes for units in layer 5 of one instantiation of the best spatial-temporal model for the trained (<italic>left</italic>) and untrained model (<italic>right</italic>). Individual gray lines are for units with <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the red line is the plane-wise mean. (<bold>D</bold>) Same as in (B) but for layer 5 of the trained spatial-temporal network. (<bold>E</bold>) Same as in (D) but for layer 5 of the corresponding untrained network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81499-fig7-figsupp1-v2.tif"/></fig></fig-group><p>To quantify uniformity, we calculated the total absolute deviation from uniformity in the distribution of preferred directions. The results indicate that the distribution of preferred directions becomes more uniform in middle layers for all instantiations of the different model architectures (<xref ref-type="fig" rid="fig7">Figure 7D</xref>), and that this difference is statistically significant for the spatial-temporal model beginning in layer 3 (layer 3 <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3.55</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.024; layer 4 <inline-formula><mml:math id="inf81"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>5.50</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.005; layer 5 <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>4.60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.010; layer 6 <inline-formula><mml:math id="inf83"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>6.12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.004). This analysis revealed that while randomly initialized models also have directionally selective units, those units are less uniformly distributed than in models trained with the ART task. Similar results hold for the spatiotemporal model, for which the difference is statistically significant beginning in layer 1 (layer 1 <inline-formula><mml:math id="inf84"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>4.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.013; layer 2 <inline-formula><mml:math id="inf85"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>2.46</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.070), and for the LSTM beginning in layer 2 (layer 2 <inline-formula><mml:math id="inf86"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>2.88</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.045; layer 3 <inline-formula><mml:math id="inf87"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.031). We also analyzed the preferred directions of the TDT-PV task and found that the distributions deviated from uniform as much, or more, than the untrained distribution (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3D</xref>). Furthermore, positional TDT-trained modules have almost no directionally selective units, lending further support to our hypothesis that ART models are <italic>more consistent</italic> with Prud’homme and Kalaska’s findings (<xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>).</p><p>Lastly, we tested directly if preferred tuning directions (of tuned units) were maintained across different planes due to the fact that we created trajectories in multiple vertical and horizontal planes. We hypothesized that preferred orientations would be preserved more for trained than untrained models. In order to examine how an individual unit’s preferred direction changed across different planes, directional tuning curve models were fit in each horizontal/vertical plane separately (examples in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A, B</xref>). To measure representational invariance, we took the mean absolute deviation (MAD) of the preferred tuning direction for directionally tuned units (<inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) across planes (see Methods) and averaged over all planes (<xref ref-type="fig" rid="fig5">Figure 5E</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>). For the spatial-temporal model across vertical workspaces, layers 3–6 were indeed more invariant in their preferred directions (layer 3: t(4)= –10.30, p=0.0005; layer 4: t(4) = –10.40, p=0.0005; layer 5: t(4) = –10.17, p=0.0005; layer 6: t(4) = –7.37, p=0.0018; <xref ref-type="fig" rid="fig5">Figure 5E</xref>; variation in preferred direction illustrated for layer 5 in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1D</xref> for trained model and in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref> for untrained). The difference in invariance for the horizontal planes was likewise statistically significant for layers 4–6 (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>). A possible reason that the difference in invariance might only become statistically significant one layer later in this setting is that the spindles are already more invariant in the horizontal planes (MAD: <inline-formula><mml:math id="inf89"><mml:mrow><mml:mrow><mml:mn>0.225</mml:mn><mml:mo/><mml:mrow><mml:mn>2.78</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:math></inline-formula>, mean ± SEM, <italic>N</italic>=25; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1B</xref>) than the vertical workspaces (MAD: <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mn>0.439</mml:mn><mml:mo/><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:math></inline-formula>, mean ± SEM, <italic>N</italic>=16; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1C</xref>), meaning that it takes a greater amount of invariance in the trained networks for differences with the untrained networks to become statistically apparent. Across vertical workspaces, the difference in invariance between the ART-trained and untrained models was statistically significant for layers 2–3 for the spatiotemporal model (layer 2 <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>4.10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.0149; layer 3 <inline-formula><mml:math id="inf92"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>8.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.0009) and for layers 3–4 the LSTM (layer 3 <inline-formula><mml:math id="inf93"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>5.73</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.0046; layer 4 <inline-formula><mml:math id="inf94"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>13.18</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, p=0.0002). For these models, the relatively slower increase in invariance in the horizontal direction is exaggerated even more. For the spatiotemporal model, the difference in invariance in the horizontal workspaces becomes statistically significant in layer 3. For the LSTM model, the neuron tuning does not become stronger for the horizontal planes until the recurrent layer (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Task-driven modeling of proprioception</title><p>For various anatomical and experimental reasons, recording proprioceptive activity during natural movements is technically challenging (<xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Kibleur et al., 2020</xref>). Furthermore, ‘presenting’ particular proprioceptive-only stimuli is difficult, which poses substantial challenges for systems identification approaches. This highlights the importance of developing accurate, normative models that can explain neural representations across the proprioceptive pathway, as has been successfully done in the visual system (<xref ref-type="bibr" rid="bib33">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib72">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib61">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Storrs et al., 2021</xref>). To tackle this, we combined human movement data, biomechanical modeling, as well as deep learning to provide a blueprint for studying the proprioceptive pathway.</p><p>We presented a task-driven approach to study the proprioceptive system based on our hypothesis that proprioception can be understood normatively, probing two different extremes of learning targets: the trajectory-decoding task encourages the encoding of ‘where’ information, while the ART focuses on ‘what’ is done. We created a passive character recognition task for a simulated human biomechanical arm paired with a muscle spindle model and found that deep neural networks can be trained to accurately solve the ART. Inferring the character from passive arm traces was chosen as it is a type of task that humans can easily perform and because it covers a wide range of natural movements of the arm. The perception is also likely fast, so feed-forward processing is a good approximation (while we also find similar results with recurrent models). Additionally, character recognition is an influential task for studying ANNs, for instance MNIST (<xref ref-type="bibr" rid="bib39">Lecun et al., 1998</xref>; <xref ref-type="bibr" rid="bib30">Illing et al., 2019</xref>). Moreover, when writing movements were imposed onto the ankle with a fixed knee joint, the movement trajectory could be decoded from a few spindles using a population vector model, suggesting that spindle information is accurate enough for decoding (<xref ref-type="bibr" rid="bib4">Albert et al., 2005</xref>). Lastly, while the underlying movements are natural and of ethological importance for humans, the task itself is only a small subset of human upper-limb function. Thus, it posed an interesting question whether such a task would be <italic>sufficient</italic> to induce representations similar to biological neurons.</p><p>We put forth a normative model of the proprioceptive system, which is experimentally testable. This builds on our earlier work (<xref ref-type="bibr" rid="bib57">Sandbrink et al., 2020</xref>), which used a different receptor model (<xref ref-type="bibr" rid="bib49">Prochazka and Gorassini, 1998a</xref>). Here, we also include positional sensing and an additional task to directly test our hypothesis of action coding vs. the canonical view of proprioception (trajectory decoding). We also confirm (for different receptor models) that in ART-trained models, but not in untrained models, the intermediate representations contain directionally selective neurons that are uniformly distributed (<xref ref-type="bibr" rid="bib57">Sandbrink et al., 2020</xref>). Furthermore, we had predicted that earlier layers, and in particular muscle spindles, have a biased, bidirectionally tuned distribution. This distribution was later experimentally validated for single units in the cuneate nucleus (<xref ref-type="bibr" rid="bib68">Versteeg et al., 2021</xref>). Here, we still robustly find this result but with different spindle models (<xref ref-type="bibr" rid="bib17">Dimitriou and Edin, 2008b</xref>). However, interestingly, these PD distribution results do not hold when neural networks with identical architectures are trained on trajectory decoding. In those models, directionally tuned neurons do not emerge but are ‘unlearned’ in comparison to untrained models for TDT (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p><p>The distribution of preferred directions becomes more uniform over the course of the processing hierarchy for the ART-trained models, similar to the distribution of preferred tuning in somatosensory cortex (<xref ref-type="fig" rid="fig5">Figure 5A–D</xref>, <xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>). This does not occur either in the untrained (<xref ref-type="fig" rid="fig5">Figure 5C–D</xref>) or the PV-TDT models (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>), which instead maintain an input distribution centered on the primary axis of preferred directions of the muscular tuning curves. Furthermore, the ART-trained models make a prediction about the distribution of preferred directions along the proprioceptive pathway. For instance, we predict that in the brainstem – that is cuneate nucleus – preferred directions are aligned along major axes inherited from muscle spindles that correspond to biomechanical constraints (consistent with <xref ref-type="bibr" rid="bib68">Versteeg et al., 2021</xref>). A key element of robust object recognition is invariance to task-irrelevant variables (<xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib62">Serre, 2019</xref>). In our computational study, we could probe many different workspaces (26 horizontal and 18 vertical) to reveal that training on the character recognition task makes directional tuning more invariant (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). This, together with our observation that directional tuning is simply inherited from muscle spindles, highlights the importance of sampling the movement space well, as also emphasized by pioneering experimental studies (<xref ref-type="bibr" rid="bib31">Jackson et al., 2007</xref>). We also note that the predictions depend on the musculoskelatal model and the movement statistics. In fact, we predict that, for example, distributions of tuning and invariances might be different in mice, a species that has a different body orientation from primates.</p></sec><sec id="s3-2"><title>Limitations and future directions</title><p>Using task-driven modeling we could show that tuning properties consistent with known biological tuning curve properties emerged from models trained on a higher-order goal, namely action recognition. This has also been shown in other sensory systems, where models were typically trained on complex, higher-order tasks (<xref ref-type="bibr" rid="bib33">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib72">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib61">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Storrs et al., 2021</xref>). While the ART task likely captures aspects of higher-order tasks that would be needed to predict the ‘where’ and the ‘what’ (to use the visual system analogy), it is not exhaustive, and it is likely that proprioception multiplexes goals, such as postural representation and higher-order tasks (like action recognition). Both the ‘what’ and the ‘where’ are important for many other behaviors (such as in state estimation for motor control). Therefore, future work can design new tasks that make experimentally testable predictions for coding in the proprioceptive pathway.</p><p>Our model only encompasses proprioception and was trained in a supervised fashion. However, it is quite natural to interpret the supervised feedback stemming from other senses. For instance, the visual system could naturally provide information about hand localization or about the type of character. The motor system could also provide this information during voluntary movement. Thus, one future direction should be multi-modal integration not only from the motor system, but from critical systems like vision.</p><p>We used different types of temporal convolutional and recurrent network architectures. In future work, it will be important to investigate emerging, perhaps more biologically relevant architectures to better understand how muscle spindles are integrated in upstream circuits. While we used spindle Ia and II models, it is known that multiple receptors, namely cutaneous, joint, and muscle receptors, play a role for limb localization and kinesthesia (<xref ref-type="bibr" rid="bib21">Gandevia et al., 2002</xref>; <xref ref-type="bibr" rid="bib47">Mileusnic et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Aimonetti et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Blum et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Delhaye et al., 2018</xref>). For instance, a recent simulation study by <xref ref-type="bibr" rid="bib34">Kibleur et al., 2020</xref> highlighted the complex spatiotemporal structure of proprioceptive information at the level of the cervical spinal cord. Furthermore, due to fusimotor drive receptor activity can be modulated by other modalities, for example, vision (<xref ref-type="bibr" rid="bib2">Ackerley et al., 2019</xref>). In the future, models for other afferents, Golgi tendon organ including the fusimotor drive as well as cutaneous receptors can be added to study their role in the context of various tasks (<xref ref-type="bibr" rid="bib26">Hausmann et al., 2021</xref>). Furthermore, as highlighted in the introduction, we studied proprioception as an open-loop system. Future work should study the effect of active motor control on proprioception.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Proprioceptive character trajectories: dataset and tasks</title><sec id="s4-1-1"><title>The character trajectories dataset</title><p>The movement data for our task was obtained from the UCI Machine Learning Repository character trajectories dataset (<xref ref-type="bibr" rid="bib71">Williams, 2008</xref>; <xref ref-type="bibr" rid="bib70">Williams et al., 2006</xref>). In brief, the dataset contains 2858 pen-tip trajectories for 20 single-stroke characters (excluding f, i, j, k, t, and x, which were multi-stroke in this dataset) in the Latin alphabet, written by a single person on an Intuos 3 Wacom digitization tablet providing pen-tip position and pressure information at 200 Hz. The size of the characters was such that they all approximately fit within a 1 × 1 cm<sup>2</sup> grid. Since we aimed to study the proprioception of the whole arm, we first interpolated the trajectories to lie within a 10 × 10 cm<sup>2</sup> grid and discarded the pen-tip pressure information. Trajectories were interpolated linearly while maintaining the velocity profiles of the original trajectories. Empirically, we found that on average it takes three times longer to write a character in the 10 × 10 cm<sup>2</sup> grid than in the small 1 × 1 one. Therefore, the time interval between samples was increased from 5 ms (200 Hz) to 15 ms (66.7 Hz) when interpolating trajectories. The resulting 2858 character trajectories served as the basis for our end-effector trajectories.</p></sec><sec id="s4-1-2"><title>Computing joint angles and muscle length trajectories</title><p>Using these end-effector trajectories, we sought to generate realistic proprioceptive inputs for passively executed movements. For this purpose, we used an open-source musculoskeletal model of the human upper limb, the upper extremity dynamic model by <xref ref-type="bibr" rid="bib59">Saul et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Holzbaur et al., 2005</xref>. The model includes 50 Hill-type muscle-tendon actuators crossing the shoulder, elbow, forearm, and wrist. While the kinematic foundations of the model enable it with 15 DoFs, 8 DoFs were eliminated by enforcing the hand to form a grip posture. We further eliminated 3 DoFs by disabling the model to have elbow rotation, wrist flexion, and rotation. The four remaining DoFs are elbow flexion (<inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ef</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), shoulder rotation (<inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sr</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), shoulder elevation, that is, thoracohumeral angle (<inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">se</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and elevation plane of the shoulder (<inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sep</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>The first step in extracting the spindle activations involved computing the joint angles for the 4 DoFs from the end-effector trajectories using constrained inverse kinematics. We built a 2-link 4 DoF arm with arm-lengths corresponding to those of the upper extremity dynamic model (<xref ref-type="bibr" rid="bib28">Holzbaur et al., 2005</xref>). To determine the joint-angle trajectories, we first define the forward kinematics equations that convert a given joint-angle configuration of the arm to its end-effector position. For a given joint-angle configuration of the arm <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ef</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sr</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">se</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sep</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the end-effector position <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in an absolute frame of reference {<inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>} centered on the shoulder is given by<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">l</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=:</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with position of the end-effector (hand) <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and elbow <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">l</mml:mi><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> when the arm is at rest and rotation matrices<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">se</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sep</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">se</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sr</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ef</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thereby, <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the rotation matrix at the shoulder joint, <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the rotation matrix at the elbow obtained by combinations of intrinsic rotations around the <italic>X</italic>, <italic>Y</italic>, and <italic>Z</italic> axes which are defined according to the upper extremity dynamic model (<xref ref-type="bibr" rid="bib28">Holzbaur et al., 2005</xref>), treating the joint angles as Euler angles and <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – the three basic rotation matrices.</p><p>Given the forward kinematics equations, the joint angles <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for an end-effector position <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> can be obtained by iteratively solving a constrained inverse kinematics problem for all times <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>…</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mtext>minimize</mml:mtext></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mtext>subject to</mml:mtext><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">min</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>θ</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">max</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mi mathvariant="normal">∀</mml:mi><mml:mtext> </mml:mtext><mml:mrow/><mml:mi>θ</mml:mi><mml:mtext> </mml:mtext><mml:mrow/><mml:mo>∈</mml:mo><mml:mtext> </mml:mtext><mml:mrow/><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ef</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sr</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">se</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sep</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a natural pose in the center of the workspace (see <xref ref-type="fig" rid="fig2">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F2">2</ext-link>D</xref>) and each <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a posture pointing to <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, while being close to the previous posture <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thereby, <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">min</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">max</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> define the limits for each joint angle. For a given end-effector trajectory <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, joint-angle trajectories are thus computed from the previous time point in order to generate smooth movements in joint space. This approach is inspired by <xref ref-type="bibr" rid="bib18">D’Souza et al., 2001</xref>.</p><p>Finally, for a given joint trajectory <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we passively moved the arm through the joint-angle trajectories in the OpenSim 3.3 simulation environment (<xref ref-type="bibr" rid="bib15">Delp et al., 2007</xref>; <xref ref-type="bibr" rid="bib63">Seth et al., 2011</xref>), computing at each time point the equilibrium muscle lengths <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, since the actuation of the 4 DoFs is achieved by 25 muscles. For simplicity, we computed equilibrium muscle configurations given joint angles as an approximation to passive movement.</p></sec><sec id="s4-1-3"><title>Proprioceptive inputs</title><p>While several mechanoreceptors provide proprioceptive information, including joint receptors, Golgi tendon organs and skin stretch receptors, the muscle spindles are regarded as the most important for conveying position and movement-related information (<xref ref-type="bibr" rid="bib41">Macefield and Knellwolf, 2018</xref>; <xref ref-type="bibr" rid="bib51">Proske and Gandevia, 2012</xref>; <xref ref-type="bibr" rid="bib49">Prochazka and Gorassini, 1998a</xref>; <xref ref-type="bibr" rid="bib50">Prochazka and Gorassini, 1998b</xref>). Here, we are inspired by Dimitriou and Edin’s recordings from human spindles (<xref ref-type="bibr" rid="bib16">Dimitriou and Edin, 2008a</xref>). They found that both Ia and II units are well predicted by combinations (for parameters <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) of muscle length <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, muscle velocity <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, acceleration <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and EMG:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:math></disp-formula></p><p>As we model passive movement, the associated EMG activity is negligible. To simplify the aggregate information flowing from one muscle (via multiple Ia and II spindles), we consider a more generic/functional representation of proprioceptive information as consisting of muscle length and velocity signals, which are approximately conveyed by muscle spindles during passive movements. Therefore, in addition to the equilibrium muscle lengths <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we input the muscle velocity <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> signal obtained by taking the first derivative. Taken together, <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> form the proprioceptive inputs that we use to train models of the proprioceptive system.</p></sec><sec id="s4-1-4"><title>A scalable proprioceptive character trajectories dataset</title><p>We move our arms in various configurations and write at varying speeds. Thus, several axes of variation were added to each (original) trajectory by (1) applying affine transformations such as scaling, rotation, and shear, (2) modifying the speed at which the character is written, (3) writing the character at several locations (chosen from a grid of candidate starting points) in the 3D workspace of the arm, and (4) writing the characters on either transverse (horizontal) or frontal (vertical) planes, of which there were 26 and 18, respectively, placed at a spatial distance of 3 cm from each other (see <xref ref-type="table" rid="table1">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T1">1</ext-link></xref> for parameter ranges). We first generated a dataset of end-effector trajectories of 1 million samples by generating variants of each original trajectory, by scaling, rotating, shearing, translating, and varying its speed. For each end-effector trajectory, we compute the joint-angle trajectory by performing inverse kinematics. Subsequently, we simulate the muscle length and velocity trajectories. Since different characters take different amounts of time to be written, we pad the movements with static postures corresponding to the starting and ending postures of the movement, and jitter the beginning of the writing to maintain ambiguity about when the writing begins.</p><p>From this dataset of trajectories, we selected a subset of trajectories such that the integral of joint-space jerk (third derivative of movement) was less than 1 rad/s<sup>3</sup> to ensure that the arm movement is sufficiently smooth. Among these, we picked the trajectories for which the integral of muscle-space jerk was minimal, while making sure that the dataset was balanced in terms of the number of examples per class, resulting in 200,000 samples. The final dataset consists of muscle length and velocity trajectories from each of the 25 muscles over a period of 320 time points, simulated at 66.7 Hz (i.e., 4.8 s). In other words, the dimensionality of the proprioceptive inputs in our tasks is 25 × 320 × 2. The dataset was then split into a training, validation, and test set with a 72-8-20 ratio.</p></sec><sec id="s4-1-5"><title>ART and TDT</title><p>Having simulated a large scale dataset of proprioceptive character trajectories, we designed two tasks: (1) the ART to classify the identity of the character based on the proprioceptive inputs, and (2) the TDT to decode the end-effector coordinates (at each time step), from proprioceptive inputs. Baseline models (SVMs for the ART and linear regression for TDT) were first trained to investigate the difficulty of the task, followed by a suite of deep neural networks that aim to model the proprioceptive pathway.</p><p>As a control, we also implemented the trajectory decoding task (TDT-PV) to decode both the end-effector coordinates and velocity (at each time step) from proprioceptive inputs.</p></sec><sec id="s4-1-6"><title>Low-dimensional embedding of population activity</title><p>To visualize population activity (of kinematic or network representations), we created low-dimensional embeddings of the proprioceptive inputs as well as the internal representations the neural network models, along time, and space/muscles dimensions. To this end, we first used principal components analysis to reduce the space to 50 dimensions, typically retaining around 75–80% of the variance. We then used t-SNE (<xref ref-type="bibr" rid="bib40">Maaten and Hinton, 2008</xref>) using sklearn (<xref ref-type="bibr" rid="bib48">Pedregosa et al., 2011</xref>) with a perplexity of 40 for 300 iterations, to reduce these 50 dimensions down to 2 for visualization.</p></sec><sec id="s4-1-7"><title>SVM analysis for action recognition</title><p>To establish a baseline performance for multi-class recognition, we used pairwise SVMs with the one-against-one method (<xref ref-type="bibr" rid="bib29">Hsu and Lin, 2002</xref>). That is, we train <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mn>20</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> pairwise (linear) SVM classifiers (<xref ref-type="fig" rid="fig3">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.F3">3</ext-link>C</xref>) and at test time implement a voting strategy based on the confidences of each classifier to determine the class identity. We trained SVMs for each input modality (end-effector trajectories, joint-angle trajectories, muscle fiber-length trajectories, and proprioceptive inputs) to determine how the format affects performance. All pairwise classifiers were trained using a hinge loss, and cross-validation was performed with nine regularization constants spaced logarithmically between 10<sup>−4</sup> and 10<sup>4</sup>.</p></sec><sec id="s4-1-8"><title>Baseline linear regression model for trajectory decoding</title><p>To establish how well one could decode end-effector coordinates from the joint, muscle, and proprioceptive inputs, we trained linear regressors with ordinary least-squares loss using stochastic gradient descent until the validation loss saturated (with a tolerance of 10<sup>−3</sup>). Inputs and outputs to the model were first transformed using a standard scalar to center (remove the mean) and scale to unit variance over each feature in order to train faster. At test time, the same scalars were reused. Decoding error was determined as the squared error (L-2 norm) of the predicted and true end-effector coordinates in 3D.</p></sec></sec><sec id="s4-2"><title>Models of the proprioceptive system</title><p>We trained two types of convolutional networks and one type of recurrent network on the two tasks. Each model is characterized by the layers used – convolutional and/or recurrent – which specify how the spatial and temporal information in the proprioceptive inputs is processed and integrated.</p><p>Each convolutional layer contains a set of convolutional filters of a given kernel size and stride, along with response normalization and a point-wise non-linearity. The convolutional filters can either be 1D, processing only spatial or temporal information, or 2D, processing both types of information simultaneously. For response normalization we use layer normalization (<xref ref-type="bibr" rid="bib5">Ba et al., 2016</xref>), a commonly used normalization scheme to train deep neural networks, where the response of a neuron is normalized by the response of all neurons of that layer. As point-wise non-linearity, we use rectified linear units. Each recurrent layer contains a single LSTM cell with a given number of units that process the input one time step at a time.</p><p>Depending on what type of convolutional layers are used and how they are arranged, we classify convolutional models into two subtypes: (1) spatial-temporal and (2) spatiotemporal networks. Spatial-temporal networks are formed by combining multiple 1D spatial and temporal convolutional layers. That is, the proprioceptive inputs from different muscles are first combined to attain a condensed representation of the ‘spatial’ information in the inputs, through a hierarchy of spatial convolutional layers. This hierarchical arrangement of the layers leads to increasingly larger receptive fields in spatial (or temporal) dimension that typically (for most parameters) gives rise to a representation of the whole arm at some point in the hierarchy. The temporal information is then integrated using temporal convolutional layers. In the spatiotemporal networks, multiple 2D convolutional layers where convolutional filters are applied simultaneously across spatial and temporal dimensions are stacked together. The LSTM models on the other hand are formed by combining multiple 1D spatial convolutional layers and a single LSTM layer at the end of a stack of spatial filters that recurrently processes the temporal information. For each network, the features at the final layer are mapped by a single fully connected layer onto either a 20D (logits) or a 3D output (end-effector coordinates).</p><p>For each specific network type, we experimented with the following hyper-parameters: number of layers, number and size of spatial and temporal filters, and their corresponding stride (see <xref ref-type="table" rid="table2">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T2">2</ext-link></xref>). Using this set of architectural parameters, 50 models of each type were randomly generated. Notably, we trained the same model (as specified by the architecture) on both the ART and the TDT position tasks.</p><sec id="s4-2-1"><title>Network training and evaluation procedure</title><p>The action-recognition trained models were trained by minimizing the softmax cross entropy loss using the Adam Optimizer (<xref ref-type="bibr" rid="bib35">Kingma and Ba, 2014</xref>) with an initial learning rate of 0.0005, batch size of 256, and decay parameters (<inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) of 0.9 and 0.999. During training, we monitored performance on the held-out validation set. When the validation error did not improve for five consecutive epochs, we decreased the learning rate by a factor of 4. After the second time the validation error saturated, we ended the training and evaluated accuracy of the networks on the test set. Overall, we observe that the trained networks generalized well to the test data, even though the shallower networks tended to overfit (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>).</p><p>The TDT-trained models, on the other hand, were trained to minimize the mean squared error between predicted and true trajectories. Hyper-parameter settings for the optimizer, batch size, and early stopping procedure used during training remained the same across both tasks. Here, we observe that train and test decoding errors were highly correlated, and thereby achieve excellent generalization to test data.</p><p>Note that only the best five instantiations of all three model types were trained on the TDT-PV task. Here, the trajectories consisted of concatenated position and velocity targets. Since the position and velocity magnitudes differed, each target dimension was scaled by their range on the training set (min-max scaling) to ensure the trained models decoded both variables similarly.</p></sec><sec id="s4-2-2"><title>Comparison with untrained models</title><p>For each of the three types of models, the architecture belonging to the best performing model on the ART (as identified via the hyper-parameter search) was chosen as the basis of the analysis (<xref ref-type="table" rid="table2">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T2">2</ext-link></xref>). The resulting sizes of each layer’s representation across the hierarchy are given in <xref ref-type="table" rid="table3">Table <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.T3">3</ext-link></xref>. For each different model type, five sets of random weights were initialized and saved. Then, each instantiation was trained on both ART and TDT using the same training procedure as described in the previous section, and the weights were saved again after training. This gives a before and after structure for each run that allows us to isolate the effect of task training.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Size of representation at each layer for best-performing architecture of each network type (spatial × temporal × filter dimensions).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Layer</th><th align="left" valign="top">Dimension</th><th align="left" valign="top">Layer</th><th align="left" valign="top">Dimension</th><th align="left" valign="top">Layer</th><th align="left" valign="top">Dimension</th></tr></thead><tbody><tr><td align="left" valign="top">Input</td><td align="char" char="." valign="top">25 × 320 × 2</td><td align="left" valign="top">Input</td><td align="char" char="." valign="top">25 × 320 × 2</td><td align="left" valign="top">Input</td><td align="char" char="." valign="top">25 × 320 × 2</td></tr><tr><td align="left" valign="top">SC0</td><td align="char" char="." valign="top">13 × 320 × 8</td><td align="left" valign="top">STC0</td><td align="char" char="." valign="top">13 × 160 × 8</td><td align="left" valign="top">SC0</td><td align="char" char="." valign="top">25 × 320 × 8</td></tr><tr><td align="left" valign="top">SC1</td><td align="char" char="." valign="top">7 × 320 × 16</td><td align="left" valign="top">STC1</td><td align="char" char="." valign="top">7 × 80 × 8</td><td align="left" valign="top">SC1</td><td align="char" char="." valign="top">25 × 320 × 16</td></tr><tr><td align="left" valign="top">SC2</td><td align="char" char="." valign="top">4 × 320 × 16</td><td align="left" valign="top">STC2</td><td align="char" char="." valign="top">4 × 40 × 32</td><td align="left" valign="top">SC2</td><td align="char" char="." valign="top">25 × 320 × 16</td></tr><tr><td align="left" valign="top">SC3</td><td align="char" char="." valign="top">2 × 320 × 32</td><td align="left" valign="top">STC3</td><td align="char" char="." valign="top">2 × 20 × 64</td><td align="left" valign="top">R</td><td align="char" char="." valign="top">256 × 320</td></tr><tr><td align="left" valign="top">TC0</td><td align="char" char="." valign="top">2 × 107 × 32</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">TC1</td><td align="char" char="." valign="top">2 × 36 × 32</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">TC2</td><td align="char" char="." valign="top">2 × 12 × 64</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">TC3</td><td align="char" char="." valign="top">2 × 4 × 64</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr></tbody></table></table-wrap></sec></sec><sec id="s4-3"><title>Population comparisons</title><sec id="s4-3-1"><title>Centered kernel alignment</title><p>In order to provide a population-level comparison between the trained and untrained models (<xref ref-type="fig" rid="fig4">Figure <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx2.SSx3">4</ext-link>A</xref>), we used linear CKA for a high-level comparison of each layer’s activation patterns (<xref ref-type="bibr" rid="bib36">Kornblith et al., 2019</xref>). CKA is an alternative that extends canonical correlation analysis (CCA) by weighting activation patterns by the eigenvalues of the corresponding eigenvectors (<xref ref-type="bibr" rid="bib36">Kornblith et al., 2019</xref>). As such, it maintains CCA’s invariance to orthogonal transformations and isotropic scaling, yet retains a greater sensitivity to similarities. Using this analysis, we quantified the similarity of the activation of each layer of the trained models with those of the respective untrained models in response to identical stimuli comprising 50% of the test set for each of the five model instantiations.</p></sec><sec id="s4-3-2"><title>Representational similarity analysis</title><p>Representational similarity analysis (RSA) is a tool to investigate population level representations among competing models (<xref ref-type="bibr" rid="bib37">Kriegeskorte et al., 2008</xref>). The basic building block of RSA is a RDM. Given stimuli <inline-formula><mml:math id="inf128"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"/><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and vectors of population responses <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the RDM is defined as:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mtext>RDM</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mtext>var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mtext>var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>One of the main advantages of RDMs is that they characterize the geometry of stimulus representation in a way that is independent of the dimensionality of the feature representations, so we can easily compare between arbitrary representations of a given stimulus set. Example RDMs for proprioceptive inputs as well as the final layer before the readout for the best models of each type are shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>. Each RDM is computed for a random sample of 4000 character trajectories (200 from each class) by using the correlation distance between corresponding feature representations. To compactly summarize how well a network disentangles the stimuli we compare the RDM of each layer to the RDM of the ideal observer, which has an RDM with perfect block structure (with dissimilarity values 0 for all stimuli of the same class and 1 (100 percentile) otherwise; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>).</p></sec></sec><sec id="s4-4"><title>Single unit analysis</title><sec id="s4-4-1"><title>Comparing the tuning curves</title><p>To elucidate the emerging coding properties of single units, we determined label specificity and fit tuning curves. Specifically, we focused on kinematic properties such as direction, velocity, acceleration, and position of the end-effector (<xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig7">Figure 7</xref>). For computational tractability, 20,000 of the original trajectories were randomly selected for the spatial-temporal and spatiotemporal models, and 10,000 for the LSTM models. In convolutional layers in which the hidden layers had a reduced temporal dimensionality, the input trajectory was downsampled. Only those time points were kept that correspond to the center of the receptive fields of the units in the hidden layers.</p><p>A train-test split of 80-20 was used, split between trajectories (so that a single trajectory was only used for training or only used for testing, but never both, eliminating the possibility of correlations between train and test resulting from temporal network dynamics). The tuning curves were fit and tested jointly on all movements in planes with a common orientation, vertical or horizontal. The analysis was repeated for each of the five trained and untrained models. For each of the five different types of tuning curves (the four biological ones and label specificity) and for each model instantiation, distributions of test scores were computed.</p><p>When plotting comparisons between different types of models (ART, TDT, and untrained), the confidence interval for the mean (CLM) using an <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> significance level based on the t-statistic was displayed.</p></sec><sec id="s4-4-2"><title>Label tuning (selectivity index)</title><p>The networks’ ability to solve the proprioceptive task poses the question if individual units serve as character detectors. To this end, SVMs were fit with linear kernels using a one-vs.-rest strategy for multi-class classification based on the firing rate of each unit, resulting in linear decision boundaries for each letter. Each individual SVM classifies whether the trajectory belongs to a certain character or not, based on that unit’s firing rates. For each SVM, auROC was calculated, giving a measure of how well the label can be determined based on the firing rate of an individual unit alone. The label specificity of that unit was then determined by taking the maximum over all characters. Finally, the auROC score was normalized into a selectivity index: 2((auROC)–0.5).</p></sec><sec id="s4-4-3"><title>Position, direction, velocity, and acceleration</title><p>For the kinematic tuning curves, the coefficient of determination <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> on the test set was used as the primary metric of evaluation. These tuning curves were fit using ordinary least squares linear regression, with regularization proving unnecessary due to the high number of data points and the low number of parameters (2-3) in the models.</p></sec><sec id="s4-4-4"><title>Position tuning</title><p>Position <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is initially defined with respect to the center of the workspace. For trajectories in a <italic>horizontal</italic> plane (workspace), a position vector was defined with respect to the starting position <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of each trace, <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula>. This was also represented in polar coordinates <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the angle measured with the counterclockwise direction defined as positive between the position vector and the vector <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is the vector extending away from the body, and <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. Positional tuning of the neural activity <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of node <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was evaluated by fitting models both using Cartesian coordinates,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p><p>as well as polar ones,<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a parameter representing a neuron’s preferred direction for position. For trajectories in the <italic>vertical</italic> plane, all definitions are equivalent, but with coordinates <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-4-5"><title>Direction</title><p>In order to examine the strength of kinematic tuning, tuning curves relating direction, velocity, and acceleration to neural activity were fitted. Since all trajectories take place either in a horizontal or vertical plane, the instantaneous velocity vector at time <inline-formula><mml:math id="inf143"><mml:mi>t</mml:mi></mml:math></inline-formula> can be described in two components as <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula>, or <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for trajectories in a vertical plane, or alternately in polar coordinates, <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> representing the angle between the velocity vector and the <italic>x</italic>-axis, and <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> representing the speed.</p><p>First, a tuning curve was fit that excludes the magnitude of velocity but focuses on the instantaneous direction, putting the angle of the polar representation of velocity <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in relation to each neuron’s preferred direction <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p><p>To fit this model, <xref ref-type="disp-formula" rid="equ9">Equation <ext-link ext-link-type="uri" xlink:href="https://resources.kriyadocs.com/resources/elife/elife/81499/resources/89c1ac7f-7928-48ef-ad0c-4de0d32d7767.html#Sx4.E9">9</ext-link></xref> was re-expressed as a simple linear sum using the cosine sum and difference formula <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>α</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>α</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, a reformulation that eases the computational burden of the analysis significantly (<xref ref-type="bibr" rid="bib22">Georgopoulos et al., 1982</xref>). In this formulation, the equation for directional tuning becomes:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The preferred direction <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is now contained in the coefficients <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The quality of fit of this type of tuning curve was visualized using polar scatter plots in which the angle of the data point corresponds to the angle <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the polar representation of velocity and the radius corresponds to the node’s activation. In the figures the direction of movement was defined so that 0° (<italic>Y</italic>) corresponds to movement to the right of the body and progressing counterclockwise, a movement straight (forward) away from the body corresponds to 90° (<italic>X</italic>) (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="fig" rid="fig7">Figure 7</xref>).</p></sec><sec id="s4-4-6"><title>Speed</title><p>Two linear models for activity <inline-formula><mml:math id="inf156"><mml:mi>N</mml:mi></mml:math></inline-formula> at a node <inline-formula><mml:math id="inf157"><mml:mi>ν</mml:mi></mml:math></inline-formula> for velocity were fit.</p><p>The first is based on its magnitude, speed,<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>α</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-7"><title>Velocity</title><p>The second velocity-based tuning curve factors in both directional and speed components:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>PD</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The quality of fit of this type of tuning curve was visualized using polar filled contour plots in which the angle of the data point corresponds to the angle <inline-formula><mml:math id="inf158"><mml:mi>θ</mml:mi></mml:math></inline-formula> in the polar representation of velocity, the radius corresponds to the speed, and the node’s activation is represented by the height. For the visualizations (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), to cover the whole range of angle and radius given a finite number of samples, the activation was first linearly interpolated. Then, missing regions were filled in using nearest neighbor interpolation. Finally, the contour was smoothed using a Gaussian filter.</p></sec><sec id="s4-4-8"><title>Acceleration</title><p>Acceleration is defined analogously to velocity by <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. A simple linear relationship with acceleration magnitude was tested:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula></p><p>In subsequent analyses, scores were excluded if they were equal to 1 (indicating a dead neuron whose output was constant) or if they were less than –0.1 (indicating a fit that did not converge).</p></sec><sec id="s4-4-9"><title>Classification of neurons into different types</title><p>The neurons were classified as belonging to a certain type if the corresponding kinematic model yielded a test <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Seven different model types were evaluated:</p><list list-type="order"><list-item><p>Direction tuning</p></list-item><list-item><p>Velocity tuning</p></list-item><list-item><p>Direction and velocity tuning</p></list-item><list-item><p>Position (Cartesian)</p></list-item><list-item><p>Position (polar)</p></list-item><list-item><p>Acceleration tuning</p></list-item><list-item><p>Label specificity</p></list-item></list><p>These were treated as distinct classes for the purposes of classification.</p></sec></sec><sec id="s4-5"><title>Population decoding analysis</title><p>We also performed population-level decoding analysis for the kinematic tuning curve types. The same datasets were used as for the single-cell encoding analysis, except with switched predictors and targets. The firing rates of all neurons in a hidden layer at a single time point were jointly used as predictors for the kinematic variable at the center of the receptive field at the corresponding input layer.</p><p>This analysis was repeated for each of the following kinematic variables:</p><list list-type="order"><list-item><p>Direction</p></list-item><list-item><p>Speed</p></list-item><list-item><p><italic>X</italic> position (Cartesian)</p></list-item><list-item><p><italic>Y</italic> position (Cartesian)</p></list-item></list><p>For each of these, the accuracy was evaluated using <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> score. The encoding strength for <italic>X</italic> and <italic>Y</italic> position in Cartesian coordinates was additionally jointly evaluated by calculating the average distance between true and predicted points of the trajectory. To prevent over-fitting, ridge regularization was used with a regularization strength of 1.</p></sec><sec id="s4-6"><title>Distribution of preferred directions</title><p>Higher-order features of the models were also evaluated and compared between the trained models and their untrained counterparts. The first property was the distribution of preferred directions fit for all horizontal planes in each layer. If a neuron’s direction-only tuning yields a test <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, its preferred direction was included in the distribution. Within a layer, the preferred directions of all neurons were binned into 18 equidistant intervals in order to enable a direct comparison with the findings by <xref ref-type="bibr" rid="bib52">Prud’homme and Kalaska, 1994</xref>. They found that the preferred directions of tuning curves were relatively evenly spread in S1; our analysis showed that this was not the case for muscle spindles. Thus, we formed the hypothesis that the preferred directions in the trained networks were more uniform in the trained networks than in the random ones. For quantification, absolute deviation from uniformity was used as a metric. To calculate this metric, the deviation from the mean height of a bin in the circular histograms was calculated for each angular bin. Then, the absolute value of this deviation was summed over all bins. We then normalize the result by the number of significantly directionally tuned neurons in a layer, and compare the result for the trained and untrained networks.</p></sec><sec id="s4-7"><title>Preferred direction invariance</title><p>We also hypothesized that the representation in the trained network would be more invariant across different horizontal and vertical planes, respectively. To test this, directional tuning curves were fit for each individual plane. A central plane was chosen as a basis of comparison (plane at <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for the horizontal planes and at <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for vertical). Changes in preferred direction of neurons are shown for spindles, as well as for neurons of layer 5 of one instantiation of the trained and untrained spatial-temporal model. Generalization was then evaluated as follows: for neurons with <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the average deviation of the neurons’ preferred directions over all different planes from those in the central plane was summed up and normalized by the number of planes and neurons, yielding a total measure for the neurons’ consistency in preferred direction in any given layer. If a plane had fewer than three directionally tuned neurons, its results were excluded.</p></sec><sec id="s4-8"><title>Statistical testing</title><p>To test whether differences were statistically significant between trained and untrained models, paired t-tests were used with a pre-set significance level of <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-9"><title>Software</title><p>We used the scientific Python3 stack (<ext-link ext-link-type="uri" xlink:href="https://www.python.org/">python.org</ext-link>): Numpy, Pandas, Matplotlib, SciPy (<xref ref-type="bibr" rid="bib69">Virtanen et al., 2020</xref>), and scikit-learn (<xref ref-type="bibr" rid="bib48">Pedregosa et al., 2011</xref>). OpenSim (<xref ref-type="bibr" rid="bib15">Delp et al., 2007</xref>; <xref ref-type="bibr" rid="bib63">Seth et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Saul et al., 2015</xref>) was used for biomechanics simulations and Tensorflow was used for constructing and training the neural network models (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>).</p></sec><sec id="s4-10"><title>Code and data</title><p>Code and data is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/amathislab/DeepDraw">https://github.com/amathislab/DeepDraw</ext-link>, (<xref ref-type="bibr" rid="bib58">Sandbrink et al., 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:484bf1cacf8bfba55cddf5671534884edc88a87b;origin=https://github.com/amathislab/DeepDraw;visit=swh:1:snp:8f58ea30d586c3833dcfb32284fdb16003019a9f;anchor=swh:1:rev:5785af7b25375e58c1d26a7ccd1787596474287f">swh:1:rev:5785af7b25375e58c1d26a7ccd1787596474287f</ext-link>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Methodology</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-81499-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The computational dataset and code to create it is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/amathislab/DeepDraw">https://github.com/amathislab/DeepDraw</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib58">Sandbrink et al., 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to the Mathis Lab, Mathis Group, and Bethge Group for comments on earlier versions of the manuscript, and Travis DeWolf for suggestions regarding the constrained inverse kinematics. Funding: KJS: Werner Siemens Fellowship of the Swiss Study Foundation; PM: Smart Start I, Bernstein Center for Computational Neuroscience; MWM: the Rowland Fellowship from the Rowland Institute at Harvard, and SNSF grant (310030_201057). AM: SNSF grant (310030_212516); AM and MWM funding from EPFL.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: A system for large-scale machine learning</article-title><conf-name>In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackerley</surname><given-names>R</given-names></name><name><surname>Chancel</surname><given-names>M</given-names></name><name><surname>Aimonetti</surname><given-names>JM</given-names></name><name><surname>Ribot-Ciscar</surname><given-names>E</given-names></name><name><surname>Kavounoudias</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Seeing your foot move changes muscle Proprioceptive feedback</article-title><source>ENeuro</source><volume>6</volume><elocation-id>ENEURO.0341-18.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0341-18.2019</pub-id><pub-id pub-id-type="pmid">30923738</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aimonetti</surname><given-names>JM</given-names></name><name><surname>Hospod</surname><given-names>V</given-names></name><name><surname>Roll</surname><given-names>JP</given-names></name><name><surname>Ribot-Ciscar</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cutaneous afferents provide a neuronal population vector that Encodes the orientation of human ankle movements</article-title><source>The Journal of Physiology</source><volume>580</volume><fpage>649</fpage><lpage>658</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2006.123075</pub-id><pub-id pub-id-type="pmid">17255169</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>F</given-names></name><name><surname>Ribot-Ciscar</surname><given-names>E</given-names></name><name><surname>Fiocchi</surname><given-names>M</given-names></name><name><surname>Bergenheim</surname><given-names>M</given-names></name><name><surname>Roll</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Proprioceptive feedback in humans expresses motor Invariants during writing</article-title><source>Experimental Brain Research</source><volume>164</volume><fpage>242</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2246-5</pub-id><pub-id pub-id-type="pmid">15856208</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ba</surname><given-names>JL</given-names></name><name><surname>Kiros</surname><given-names>JR</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Layer Normalization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1607.06450</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>S</given-names></name><name><surname>Kolter</surname><given-names>JZ</given-names></name><name><surname>Koltun</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.01271">https://arxiv.org/abs/1803.01271</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1803.01271</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>The Co-Ordination and Regulation of Movements</source><publisher-loc>Oxford, New York</publisher-loc><publisher-name>Pergamon Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>KP</given-names></name><name><surname>Lamotte D’Incamps</surname><given-names>B</given-names></name><name><surname>Zytnicki</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>LH</given-names></name><name><surname>Ayers</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Force Encoding in muscle Spindles during stretch of passive muscle</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005767</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005767</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosco</surname><given-names>G</given-names></name><name><surname>Rankin</surname><given-names>A</given-names></name><name><surname>Poppele</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of passive hindlimb postures in cat Spinocerebellar activity</article-title><source>Journal of Neurophysiology</source><volume>76</volume><fpage>715</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.76.2.715</pub-id><pub-id pub-id-type="pmid">8871193</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep Convolutional models improve predictions of Macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Glaser</surname><given-names>JI</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Area 2 of primary Somatosensory cortex Encodes Kinematics of the whole arm</article-title><source>eLife</source><volume>9</volume><elocation-id>e48198</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48198</pub-id><pub-id pub-id-type="pmid">31971510</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to Spatio-temporal cortical Dynamics of human visual object recognition reveals Hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>FJ</given-names></name><name><surname>Burgess</surname><given-names>RC</given-names></name><name><surname>Chapin</surname><given-names>JW</given-names></name><name><surname>Lipscomb</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Role of Intramuscular receptors in the awareness of limb position</article-title><source>Journal of Neurophysiology</source><volume>54</volume><fpage>1529</fpage><lpage>1540</lpage><pub-id pub-id-type="doi">10.1152/jn.1985.54.6.1529</pub-id><pub-id pub-id-type="pmid">4087047</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delhaye</surname><given-names>BP</given-names></name><name><surname>Long</surname><given-names>KH</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural basis of touch and Proprioception in Primate cortex</article-title><source>Comprehensive Physiology</source><volume>8</volume><fpage>1575</fpage><lpage>1602</lpage><pub-id pub-id-type="doi">10.1002/cphy.c170033</pub-id><pub-id pub-id-type="pmid">30215864</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delp</surname><given-names>SL</given-names></name><name><surname>Anderson</surname><given-names>FC</given-names></name><name><surname>Arnold</surname><given-names>AS</given-names></name><name><surname>Loan</surname><given-names>P</given-names></name><name><surname>Habib</surname><given-names>A</given-names></name><name><surname>John</surname><given-names>CT</given-names></name><name><surname>Guendelman</surname><given-names>E</given-names></name><name><surname>Thelen</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Opensim: open-source software to create and analyze dynamic Simulations of movement</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>54</volume><fpage>1940</fpage><lpage>1950</lpage><pub-id pub-id-type="doi">10.1109/TBME.2007.901024</pub-id><pub-id pub-id-type="pmid">18018689</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimitriou</surname><given-names>M</given-names></name><name><surname>Edin</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Discharges in human muscle receptor afferents during block grasping</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>12632</fpage><lpage>12642</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3357-08.2008</pub-id><pub-id pub-id-type="pmid">19036957</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimitriou</surname><given-names>M</given-names></name><name><surname>Edin</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Discharges in human muscle spindle afferents during a key-pressing task</article-title><source>The Journal of Physiology</source><volume>586</volume><fpage>5455</fpage><lpage>5470</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2008.160036</pub-id><pub-id pub-id-type="pmid">18801840</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>D’Souza</surname><given-names>A</given-names></name><name><surname>Vijayakumar</surname><given-names>S</given-names></name><name><surname>Schaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Learning inverse kinematics</article-title><conf-name>Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium</conf-name><conf-loc>Maui, HI, USA</conf-loc><fpage>298</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1109/IROS.2001.973374</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>JT</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Chapin</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Proprioceptive and cutaneous representations in the rat ventral Posterolateral thalamus</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>2291</fpage><lpage>2304</lpage><pub-id pub-id-type="doi">10.1152/jn.01206.2007</pub-id><pub-id pub-id-type="pmid">18287546</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fromm</surname><given-names>C</given-names></name><name><surname>Evarts</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Pyramidal tract neurons in Somatosensory cortex: central and peripheral inputs during voluntary movement</article-title><source>Brain Research</source><volume>238</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(82)90781-8</pub-id><pub-id pub-id-type="pmid">6805854</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gandevia</surname><given-names>SC</given-names></name><name><surname>Refshauge</surname><given-names>KM</given-names></name><name><surname>Collins</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Proprioception: peripheral inputs and perceptual interactions</chapter-title><person-group person-group-type="editor"><name><surname>Gandevia</surname><given-names>SC</given-names></name><name><surname>Proske</surname><given-names>U</given-names></name></person-group><source>In Sensorimotor Control of Movement and Posture</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer</publisher-name><fpage>61</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1007/978-1-4615-0713-0_8</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name><name><surname>Caminiti</surname><given-names>R</given-names></name><name><surname>Massey</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>On the relations between the direction of two-dimensional arm movements and cell discharge in Primate motor cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>1527</fpage><lpage>1537</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-11-01527.1982</pub-id><pub-id pub-id-type="pmid">7143039</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>JM</given-names></name><name><surname>Tabot</surname><given-names>GA</given-names></name><name><surname>Lee</surname><given-names>AS</given-names></name><name><surname>Suresh</surname><given-names>AK</given-names></name><name><surname>Rajan</surname><given-names>AT</given-names></name><name><surname>Hatsopoulos</surname><given-names>NG</given-names></name><name><surname>Bensmaia</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Postural representations of the hand in the Primate sensorimotor cortex</article-title><source>Neuron</source><volume>104</volume><fpage>1000</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.004</pub-id><pub-id pub-id-type="pmid">31668844</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MSA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Ethological action maps: a paradigm shift for the motor cortex</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>121</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.10.008</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haesemeyer</surname><given-names>M</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Convergent temperature representations in artificial and biological neural networks</article-title><source>Neuron</source><volume>103</volume><fpage>1123</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.07.003</pub-id><pub-id pub-id-type="pmid">31376984</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hausmann</surname><given-names>SB</given-names></name><name><surname>Vargas</surname><given-names>AM</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Measuring and modeling the motor system with machine learning</article-title><source>Current Opinion in Neurobiology</source><volume>70</volume><fpage>11</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.04.004</pub-id><pub-id pub-id-type="pmid">34116423</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holzbaur</surname><given-names>KRS</given-names></name><name><surname>Murray</surname><given-names>WM</given-names></name><name><surname>Delp</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A model of the upper extremity for Simulating musculoskeletal surgery and analyzing neuromuscular control</article-title><source>Annals of Biomedical Engineering</source><volume>33</volume><fpage>829</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1007/s10439-005-3320-7</pub-id><pub-id pub-id-type="pmid">16078622</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>CW</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A comparison of methods for Multiclass support vector machines</article-title><source>IEEE Transactions on Neural Networks</source><volume>13</volume><fpage>415</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1109/72.991427</pub-id><pub-id pub-id-type="pmid">18244442</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Illing</surname><given-names>B</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Brea</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Biologically plausible deep Learningbut how far can we go with shallow networks</article-title><source>Neural Networks</source><volume>118</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2019.06.001</pub-id><pub-id pub-id-type="pmid">31254771</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>A</given-names></name><name><surname>Mavoori</surname><given-names>J</given-names></name><name><surname>Fetz</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Correlations between the same motor cortex cells and arm muscles during a trained task, free behavior, and natural sleep in the Macaque monkey</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>360</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1152/jn.00710.2006</pub-id><pub-id pub-id-type="pmid">17021028</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A task-Optimized neural network Replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not Unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kibleur</surname><given-names>P</given-names></name><name><surname>Tata</surname><given-names>SR</given-names></name><name><surname>Greiner</surname><given-names>N</given-names></name><name><surname>Conti</surname><given-names>S</given-names></name><name><surname>Barra</surname><given-names>B</given-names></name><name><surname>Zhuang</surname><given-names>K</given-names></name><name><surname>Kaeser</surname><given-names>M</given-names></name><name><surname>Ijspeert</surname><given-names>A</given-names></name><name><surname>Capogrosso</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatiotemporal maps of Proprioceptive inputs to the Cervical spinal cord during Three- dimensional reaching and grasping</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>28</volume><fpage>1668</fpage><lpage>1677</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2020.2986491</pub-id><pub-id pub-id-type="pmid">32396093</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Similarity of Neural Network Representations Revisited</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.00414">https://arxiv.org/abs/1905.00414</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1905.00414</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis-connecting the branches of systems Neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>N</given-names></name><name><surname>Manning</surname><given-names>TF</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Somatosensory cortex participates in the consolidation of human motor memory</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000469</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000469</pub-id><pub-id pub-id-type="pmid">31613874</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Gradient-based learning applied to document recognition</article-title><source>Proceedings of the IEEE</source><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maaten</surname><given-names>Lvd</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing data using t-SNE</article-title><source>Journal of Machine Learning Research</source><volume>9</volume><elocation-id>25792605</elocation-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macefield</surname><given-names>VG</given-names></name><name><surname>Knellwolf</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional properties of human muscle Spindles</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>452</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1152/jn.00071.2018</pub-id><pub-id pub-id-type="pmid">29668385</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Jay</surname><given-names>M</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>RW</given-names></name><name><surname>Cieszkowski</surname><given-names>R</given-names></name><name><surname>Scott</surname><given-names>R</given-names></name><name><surname>Brann</surname><given-names>D</given-names></name><name><surname>Koveal</surname><given-names>D</given-names></name><name><surname>Kula</surname><given-names>T</given-names></name><name><surname>Weinreb</surname><given-names>C</given-names></name><name><surname>Osman</surname><given-names>MAM</given-names></name><name><surname>Pinto</surname><given-names>SR</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spontaneous behaviour is structured by reinforcement without explicit reward</article-title><source>Nature</source><volume>614</volume><fpage>108</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05611-2</pub-id><pub-id pub-id-type="pmid">36653449</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Somatosensory cortex plays an essential role in Forelimb motor adaptation in mice</article-title><source>Neuron</source><volume>93</volume><fpage>1493</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.049</pub-id><pub-id pub-id-type="pmid">28334611</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>The response of de-Efferented muscle spindle receptors to stretching at different velocities</article-title><source>The Journal of Physiology</source><volume>168</volume><fpage>660</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1963.sp007214</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="1981">1981</year><chapter-title>Muscle Spindles: Their Messages and Their Fusimotor Supply. Handbook of Physiology: I. The Nervous System</chapter-title><source>Comprehensive Physiology</source><publisher-name>American Physiological Society</publisher-name><pub-id pub-id-type="doi">10.1002/cphy.cp010206</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miall</surname><given-names>RC</given-names></name><name><surname>Kitchen</surname><given-names>NM</given-names></name><name><surname>Nam</surname><given-names>S-H</given-names></name><name><surname>Lefumat</surname><given-names>H</given-names></name><name><surname>Renault</surname><given-names>AG</given-names></name><name><surname>Ørstavik</surname><given-names>K</given-names></name><name><surname>Cole</surname><given-names>JD</given-names></name><name><surname>Sarlegna</surname><given-names>FR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Proprioceptive loss and the perception, control and learning of arm movements in humans: evidence from sensory Neuronopathy</article-title><source>Experimental Brain Research</source><volume>236</volume><fpage>2137</fpage><lpage>2155</lpage><pub-id pub-id-type="doi">10.1007/s00221-018-5289-0</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mileusnic</surname><given-names>MP</given-names></name><name><surname>Brown</surname><given-names>IE</given-names></name><name><surname>Lan</surname><given-names>N</given-names></name><name><surname>Loeb</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mathematical models of Proprioceptors. I. control and Transduction in the muscle spindle</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>1772</fpage><lpage>1788</lpage><pub-id pub-id-type="doi">10.1152/jn.00868.2005</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><elocation-id>28252830</elocation-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prochazka</surname><given-names>A</given-names></name><name><surname>Gorassini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998a</year><article-title>Ensemble firing of muscle afferents recorded during normal locomotion in cats</article-title><source>The Journal of Physiology</source><volume>507 ( Pt 1)</volume><fpage>293</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7793.1998.293bu.x</pub-id><pub-id pub-id-type="pmid">9490855</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prochazka</surname><given-names>A</given-names></name><name><surname>Gorassini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998b</year><article-title>Models of ensemble firing of muscle spindle afferents recorded during normal locomotion in cats</article-title><source>The Journal of Physiology</source><volume>507 ( Pt 1)</volume><fpage>277</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7793.1998.277bu.x</pub-id><pub-id pub-id-type="pmid">9490851</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proske</surname><given-names>U</given-names></name><name><surname>Gandevia</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The Proprioceptive senses: their roles in signaling body shape, body position and movement, and muscle force</article-title><source>Physiological Reviews</source><volume>92</volume><fpage>1651</fpage><lpage>1697</lpage><pub-id pub-id-type="doi">10.1152/physrev.00048.2011</pub-id><pub-id pub-id-type="pmid">23073629</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prud’homme</surname><given-names>MJ</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Proprioceptive activity in Primate primary Somatosensory cortex during active arm reaching movements</article-title><source>Journal of Neurophysiology</source><volume>72</volume><fpage>2280</fpage><lpage>2301</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.72.5.2280</pub-id><pub-id pub-id-type="pmid">7884459</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ribot-Ciscar</surname><given-names>E</given-names></name><name><surname>Bergenheim</surname><given-names>M</given-names></name><name><surname>Albert</surname><given-names>F</given-names></name><name><surname>Roll</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Proprioceptive population coding of limb position in humans</article-title><source>Experimental Brain Research</source><volume>149</volume><fpage>512</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1384-x</pub-id><pub-id pub-id-type="pmid">12677332</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for Neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>A general framework for parallel distributed processing</article-title><source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</source><volume>1</volume><fpage>45</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.7551/mitpress/5236.001.0001</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Imagenet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sandbrink</surname><given-names>KJ</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Task-Driven Hierarchical Deep Neural Network Models of the Proprioceptive Pathway</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.06.081372</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sandbrink</surname><given-names>KJ</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Contrasting action and posture coding with Hierarchical deep neural network models of Proprioception</data-title><version designator="swh:1:rev:5785af7b25375e58c1d26a7ccd1787596474287f">swh:1:rev:5785af7b25375e58c1d26a7ccd1787596474287f</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:484bf1cacf8bfba55cddf5671534884edc88a87b;origin=https://github.com/amathislab/DeepDraw;visit=swh:1:snp:8f58ea30d586c3833dcfb32284fdb16003019a9f;anchor=swh:1:rev:5785af7b25375e58c1d26a7ccd1787596474287f">https://archive.softwareheritage.org/swh:1:dir:484bf1cacf8bfba55cddf5671534884edc88a87b;origin=https://github.com/amathislab/DeepDraw;visit=swh:1:snp:8f58ea30d586c3833dcfb32284fdb16003019a9f;anchor=swh:1:rev:5785af7b25375e58c1d26a7ccd1787596474287f</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saul</surname><given-names>KR</given-names></name><name><surname>Hu</surname><given-names>X</given-names></name><name><surname>Goehler</surname><given-names>CM</given-names></name><name><surname>Vidt</surname><given-names>ME</given-names></name><name><surname>Daly</surname><given-names>M</given-names></name><name><surname>Velisar</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Benchmarking of dynamic simulation predictions in two software platforms using an upper limb musculoskeletal model</article-title><source>Computer Methods in Biomechanics and Biomedical Engineering</source><volume>18</volume><fpage>1445</fpage><lpage>1458</lpage><pub-id pub-id-type="doi">10.1080/10255842.2014.916698</pub-id><pub-id pub-id-type="pmid">24995410</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>If deep learning is the answer, what is the question?</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-00395-8</pub-id><pub-id pub-id-type="pmid">33199854</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning: the good, the bad, and the ugly</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>399</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014951</pub-id><pub-id pub-id-type="pmid">31394043</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>A</given-names></name><name><surname>Sherman</surname><given-names>M</given-names></name><name><surname>Reinbolt</surname><given-names>JA</given-names></name><name><surname>Delp</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Opensim: a musculoskeletal modeling and simulation framework for in Silico investigations and Exchange</article-title><source>Procedia IUTAM</source><volume>2</volume><fpage>212</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.piutam.2011.04.021</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Diverse deep neural networks all predict human inferior temporal cortex well, after training and fitting</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2044</fpage><lpage>2064</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01755</pub-id><pub-id pub-id-type="pmid">34272948</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sundaram</surname><given-names>S</given-names></name><name><surname>Kellnhofer</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>JY</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Matusik</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning the signatures of the human grasp using a Scalable tactile glove</article-title><source>Nature</source><volume>569</volume><fpage>698</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1234-z</pub-id><pub-id pub-id-type="pmid">31142856</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Optimal feedback control as a theory of motor coordination</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1226</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.1038/nn963</pub-id><pub-id pub-id-type="pmid">12404008</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuthill</surname><given-names>JC</given-names></name><name><surname>Azim</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Proprioception</article-title><source>Current Biology</source><volume>28</volume><fpage>R194</fpage><lpage>R203</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.064</pub-id><pub-id pub-id-type="pmid">29510103</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Versteeg</surname><given-names>C</given-names></name><name><surname>Rosenow</surname><given-names>JM</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Encoding of limb state by single neurons in the Cuneate nucleus of awake monkeys</article-title><source>Journal of Neurophysiology</source><volume>126</volume><fpage>693</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1152/jn.00568.2020</pub-id><pub-id pub-id-type="pmid">34010577</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Scipy 1.0: fundamental Algorithms for scientific computing in python</article-title><source>Nature Methods</source><volume>17</volume><elocation-id>352</elocation-id><pub-id pub-id-type="doi">10.1038/s41592-020-0772-5</pub-id><pub-id pub-id-type="pmid">32094914</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>BH</given-names></name><name><surname>Toussaint</surname><given-names>M</given-names></name><name><surname>Storkey</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Extracting motion primitives from natural handwriting data</article-title><conf-name>In International Conference on Artificial Neural Networks</conf-name><fpage>634</fpage><lpage>643</lpage></element-citation></ref><ref id="bib71"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><data-title>Character Trajectories</data-title><source>UC Irvine Machine Learning Repository</source><pub-id pub-id-type="doi">10.24432/C58G7V</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-Optimized Hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hartmann</surname><given-names>MJ</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Toward goal-driven neural network models for the rodent Whisker-trigeminal system</article-title><conf-name>In Advances in Neural Information Processing Systems</conf-name><fpage>2555</fpage><lpage>2565</lpage></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81499.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ba</surname><given-names>Demba</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2020.05.06.081372" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2020.05.06.081372"/></front-stub><body><p>This article proposes a combination of biomechanical modeling and in-silico experiments, on a newly-curated passive-movement dataset, to elucidate the nature of computations in the proprioceptive pathway. The authors find that, in addition to its canonical role in representing the body state, the proprioceptive pathway may have evolved to recognize actions. Overall, the authors' findings lead to new hypotheses about proprioception that future in-vivo experiments could test.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81499.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ba</surname><given-names>Demba</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.05.06.081372">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.05.06.081372v3">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Contrasting action and posture coding with hierarchical deep neural network models of proprioception&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Nikolaus Kriegeskorte (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>The reviewers found the topic of the paper and its contributions interesting.</p><p>At the same time, overall, the reviewers</p><p>1. Expressed reservations at the lack of quantitative evaluation of neural data, and more importantly</p><p>2. Suggest that to lend support to the current claim that the function of the proprioceptive system is action recognition, the authors would need to train with losses intermediate between posture and action that, in addition to action can decode velocity as well.</p><p>The Reviewing Editor agrees with the reviewers that 2. would strengthen the manuscript. In its absence, the Reviewing Editor would find it difficult to fully support the scientific findings of the manuscript. The Reviewing Editor encourages the authors to consider additional experiments based on the detailed comments from the reviewers.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>As I have explained in the public review, I don't agree with the main conclusion of the paper. The end effector position estimation task is too simple and is not the right model to contrast the action recognition model with. Ideally, I would argue that one should simulate a full closed-loop control model to be able to make a claim like this for proprioception. At the very least, they could have a state estimation network that estimates effector velocities in addition to position, which would likely learn speed, velocity, and direction-selective hidden layer representations. As far as I can see, the lack of such cells is the criterion that they use to rule out the TDT task.</p><p>I don't know if it exists, but if there is relevant data available, they could make more detailed comparisons to data. For example, do the trends they see in the change of selectivity across network layers match that in the brain?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1) The title refers to &quot;posture&quot;, but this term is not at all mentioned in the abstract, which instead contrasts action and trajectory decoding. It would be good to choose consistent terminology or at least to relate the term posture to the trajectory decoding objective in the abstract.</p><p>2) Some more motivation is needed, even in the abstract, for the idea that the function of the proprioceptive pathway is to recognize what the brain already knows: the action it is trying to perform. This is fascinating and plausible to me, but it is not at all obvious. The introduction and discussion should contain the authors' best guesses (even if speculative) to better motivate the ART objective and interpret the central result.</p><p>3) Please address the two weaknesses I mention in the public review (no quantitative evaluation, unclear what about the ART drives the main result) in the discussion with a view to guiding future follow-up studies (including ones that build on the data set and code from this study).</p><p>4) It might be clearer to refer to the control models as untrained models (as different other kinds of control model could have been used).</p><p>5) Figure 4A: &quot;CKA score&quot; is not a conceptually informative label for the color bar. &quot;similarity between untrained and trained model representations [CKA]&quot; would be better.</p><p>6) Figure 4B: The t-SNE plots seem to show little more than that the ART, but not the TDT model shown achieves the ART model's objective. If models trained with different objectives are to be compared, then the visualization should not be partial to one of the objectives. Coloring dots by trajectory similarity might provide an alternative scheme that could be added. More generally, t-SNE requires setting hyperparameters and its objective function is hard to state concisely (beyond maintaining neighbor relationships), making interpretation more difficult than for say metric-scale MDS. t-SNE may also show classes as clustered that are not linearly separable, failing to visualize the gradual disentangling across layers, as in Figure 5 here: https://arxiv.org/abs/2107.00731. I have no definite suggestion but did not find this panel particularly informative in the present form.</p><p>7) Figure 4C: With one tick mark per letter, some readers will miss the many conditions per letter that are shown in the RDMs (despite this being apparent in the proprioceptive RDM). It might be better to show the whole matrix much larger in a separate supplemental figure and to focus on a, b, c, d, and e in Figure 4C and to add tickmarks for, say 5 instances for each of these letters. The number of conditions shown should also be stated in the figure legend and ideally should match the number of tickmarks on the matrix.</p><p>8) Figure 5E: The unconventional lines connecting percentiles across the vertical distribution plots add more clutter than clarity. It might be better to plot the distributions and point summaries separately or choose one. An exacerbating factor might be that the coding of the different tuning properties in the shading is not clearly discernible, and I end up having to count which of the five tuning variables I am looking at. Using a separate plot for each tuning type might help.</p><p>9) The paper contains some ungrammatical English and typos and would benefit from careful proofreading and editing.</p><p>10) CKA stands for &quot;centered kernel alignment&quot; (not &quot;centered kernel analysis&quot;).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81499.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The reviewers found the topic of the paper and its contributions interesting.</p><p>At the same time, overall, the reviewers</p><p>1. Expressed reservations at the lack of quantitative evaluation of neural data, and more importantly</p><p>2. Suggest that to lend support to the current claim that the function of the proprioceptive system is action recognition, the authors would need to train with losses intermediate between posture and action that, in addition to action can decode velocity as well.</p><p>The Reviewing Editor agrees with the reviewers that 2. would strengthen the manuscript. In its absence, the Reviewing Editor would find it difficult to fully support the scientific findings of the manuscript. The Reviewing Editor encourages the authors to consider additional experiments based on the detailed comments from the reviewers.</p></disp-quote><p>We thank all reviewers for their insightful comments. Firstly, we agree overall that we should tone-down the perception that we claimed proprioception is doing action recognition at the cost of trajectory estimation. We don’t want to claim that – we want to simply put forth that normative models can yield hypotheses, and proprioception might do action recognition. Thus, firstly we tuned our writing. Also, as suggested by the reviewers, we conducted additional experiments to include an intermediate model that predicts location and velocity, as well. We find that this generalized TDT model (that also predicts velocity), still has much fewer directional selective units than the ART model (see Suppl. Figure 6-3). Thanks for this suggestion, we believe it did improve the manuscript.</p><p>Furthermore, we do agree that a comparison to neural data is of great interest. However, here we presented a model based on human biomechanical models. We thus feel that predicting neural data is beyond the scope of this paper (as we are not aware of single unit recordings of the proprioceptive pathway in humans). We do note in the revision that since the time of our preprint, one study did test our hypothesis and show cuneate nucleus has more intermediate-layer neurons<xref ref-type="bibr" rid="bib68">Versteeg et al., 2021</xref>, and we note this in the revision. And, over the past two years we have started follow up work with a Macaque arm model and are currently comparing data in the brain stem and sensory cortex, but this is much beyond the scope of this manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>As I have explained in the public review, I don't agree with the main conclusion of the paper. The end effector position estimation task is too simple and is not the right model to contrast the action recognition model with. Ideally, I would argue that one should simulate a full closed-loop control model to be able to make a claim like this for proprioception. At the very least, they could have a state estimation network that estimates effector velocities in addition to position, which would likely learn speed, velocity, and direction-selective hidden layer representations. As far as I can see, the lack of such cells is the criterion that they use to rule out the TDT task.</p></disp-quote><p>We thank reviewer #1 for their insightful comments. In this paper, we are intentionally focusing on proprioception as its own system (and not entangled with motor control). Much like vision is studied without motor (which notably changes visual properties) we aim to study this “sensory system&quot; in isolation. We fully agree with you that studying proprioception in a closed-loop fashion (with a motor system) will be a key next step, but is beyond the scope of this manuscript, and our aim is not to overthrow the canonical role of proprioception, rather put forth a way to isolate and test hypotheses with a normative approach. We have toned-down the language and rather aim to highlight here is an example (i.e., propose proprioception could involve action recognition, much like vision can also do face recognition). We hope this revision clarifies our intent better.</p><p>Furthermore, we believe that there is value in studying proprioception in isolation to later contrast the results for closed-loop system. To emphasize this limitation, we have updated the introduction (and discussion), where we state: “We emphasize, that as in previous task-driven work for other sensory systems (Yamins and DiCarlo 2016, Kell et al. 2018), we do not model the closed-loop nature. Of course, in reality, the motor system is not constant but the tuning properties of proprioception are optimized jointly with the motor system and other changing influences. Studying proprioception with a basic open-loop model is important to set the stage for more complex models such as joint models of proprioception and motor control.&quot;</p><p>Our goal in this paper is not to assert that either trajectory decoding or action recognition is the &quot;true&quot; or even the &quot;canonical&quot; model of the proprioceptive system. Rather, we view trajectory decoding as representative of the set of tasks whose goal would be to recover information that can be used by the rest of the brain to accomplish higher-order functions. We also note that we added another trajectory decoding task as a control (see below). We select action recognition as a paradigmatic case of one of these higher-order cognitive tasks (although we by no means view it as the true, or only, function of the proprioceptive system). Our aim by doing this is to show that training a network on a higher-order task is sufficient for recovering the kinds of neural representations that have been observed in biological measurements of the proprioceptive system. We adjusted the writing in the paper to make this conceptual distinction clearer.</p><p>We agree that the comparison between the two tasks could be enriched by including velocity prediction as task for the artificial proprioceptive system. As suggested by the reviewer we also added a new task predicting both the position and the velocity of the end-effector. Interestingly, we also do not find many direction selective units in this case (in comparison to the purely positional TDT,see Suppl. Figure 6-3) and indeed also this task unlearns direction selectivity in comparison to the untrained initializations. The ART-trained models have significantly more direction selective units.</p><disp-quote content-type="editor-comment"><p>I don't know if it exists, but if there is relevant data available, they could make more detailed comparisons to data. For example, do the trends they see in the change of selectivity across network layers match that in the brain?</p></disp-quote><p>As we noted in the response to both reviewers, we are not aware of human-recordings in the proprioceptive system that are available. We believe that comparing directly to neural data is beyond the scope of the current manuscript.</p><p>In lieu of that, we compare the emergent representations in the networks with neural data qualitatively where this is appropriate, and indeed some findings have already been validated experimentally now that our pre-print has been available for 3 years (as in the case of the distribution of preferred directions).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1) The title refers to &quot;posture&quot;, but this term is not at all mentioned in the abstract, which instead contrasts action and trajectory decoding. It would be good to choose consistent terminology or at least to relate the term posture to the trajectory decoding objective in the abstract.</p></disp-quote><p>Thanks for that observation, we have now utilized posture in the abstract and the introduction. There we state: “One key role of proprioception is to sense the state of the body—i.e., posture. This information subserves many other functions, from balance to motor learning. [..]&quot;</p><disp-quote content-type="editor-comment"><p>2) Some more motivation is needed, even in the abstract, for the idea that the function of the proprioceptive pathway is to recognize what the brain already knows: the action it is trying to perform. This is fascinating and plausible to me, but it is not at all obvious. The introduction and discussion should contain the authors' best guesses (even if speculative) to better motivate the ART objective and interpret the central result.</p></disp-quote><p>Thanks for this feedback. As described in the response to Reviewer 1 above, we do not take our work in this paper to imply that action recognition is the &quot;true&quot; model of the proprioceptive system. Rather, we consider it as a paradigmatic case of one of the (many) higher-order cognitive tasks that the proprioceptive system can give access to. Our aim by doing this is to show that training a network on a higher-order task is sufficient for recovering the kinds of neural representations that have been observed in biological measurements of the proprioceptive system. We will adjust the writing in the paper to make this conceptual distinction clearer.</p><disp-quote content-type="editor-comment"><p>3) Please address the two weaknesses I mention in the public review (no quantitative evaluation, unclear what about the ART drives the main result) in the discussion with a view to guiding future follow-up studies (including ones that build on the data set and code from this study).</p><p>4) It might be clearer to refer to the control models as untrained models (as different other kinds of control model could have been used).</p></disp-quote><p>Thanks for the suggestion, we call it untrained model now.</p><disp-quote content-type="editor-comment"><p>5) Figure 4A: &quot;CKA score&quot; is not a conceptually informative label for the color bar. &quot;similarity between untrained and trained model representations [CKA]&quot; would be better.</p></disp-quote><p>Thanks for the comment, we changed the label.</p><disp-quote content-type="editor-comment"><p>6) Figure 4B: The t-SNE plots seem to show little more than that the ART, but not the TDT model shown achieves the ART model's objective. If models trained with different objectives are to be compared, then the visualization should not be partial to one of the objectives. Coloring dots by trajectory similarity might provide an alternative scheme that could be added. More generally, t-SNE requires setting hyperparameters and its objective function is hard to state concisely (beyond maintaining neighbor relationships), making interpretation more difficult than for say metric-scale MDS. t-SNE may also show classes as clustered that are not linearly separable, failing to visualize the gradual disentangling across layers, as in Figure 5 here: https://arxiv.org/abs/2107.00731. I have no definite suggestion but did not find this panel particularly informative in the present form.</p></disp-quote><p>We agree that the criticism of t-SNE is well taken, and we only use it as a visualization. The gradual disentanglement is quantified with RDMs in the same figure (4D-F).</p><p>However, we respectfully disagree with the reviewer. Of course the TDT model does not disentangle the characters, but we have found this panel a useful illustration for some readers/listeners. That said, we do agree, it’s not very deep.</p><disp-quote content-type="editor-comment"><p>7) Figure 4C: With one tick mark per letter, some readers will miss the many conditions per letter that are shown in the RDMs (despite this being apparent in the proprioceptive RDM). It might be better to show the whole matrix much larger in a separate supplemental figure and to focus on a, b, c, d, and e in Figure 4C and to add tickmarks for, say 5 instances for each of these letters. The number of conditions shown should also be stated in the figure legend and ideally should match the number of tickmarks on the matrix.</p></disp-quote><p>That’s a great idea, we made this change.</p><disp-quote content-type="editor-comment"><p>8) Figure 5E: The unconventional lines connecting percentiles across the vertical distribution plots add more clutter than clarity. It might be better to plot the distributions and point summaries separately or choose one. An exacerbating factor might be that the coding of the different tuning properties in the shading is not clearly discernible, and I end up having to count which of the five tuning variables I am looking at. Using a separate plot for each tuning type might help.</p></disp-quote><p>We thank the reviewer for this comment, we have dropped the percentile lines and adjusted the colors and labels for clarity.</p><disp-quote content-type="editor-comment"><p>9) The paper contains some ungrammatical English and typos and would benefit from careful proofreading and editing.</p></disp-quote><p>We have carefully read the paper again and hopefully fixed all errors.</p><disp-quote content-type="editor-comment"><p>10) CKA stands for &quot;centered kernel alignment&quot; (not &quot;centered kernel analysis&quot;).</p></disp-quote><p>Thank you, we have changed the text accordingly.</p></body></sub-article></article>