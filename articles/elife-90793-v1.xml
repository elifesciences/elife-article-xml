<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">90793</article-id><article-id pub-id-type="doi">10.7554/eLife.90793</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90793.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Selective consolidation of learning and memory via recall-gated plasticity</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0930-7327</contrib-id><email>jackwlindsey@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2422-6576</contrib-id><email>a.litwin-kumar@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gjorgjieva</surname><given-names>Julijana</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kkvpp62</institution-id><institution>Technical University of Munich</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>18</day><month>07</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP90793</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-07-14"><day>14</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-07-06"><day>06</day><month>07</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.12.08.519638"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-11-24"><day>24</day><month>11</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90793.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-02"><day>02</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90793.2"/></event></pub-history><permissions><copyright-statement>Â© 2023, Lindsey and Litwin-Kumar</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Lindsey and Litwin-Kumar</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-90793-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-90793-figures-v1.pdf"/><abstract><p>In a variety of species and behavioral contexts, learning and memory formation recruits two neural systems, with initial plasticity in one system being consolidated into the other over time. Moreover, consolidation is known to be selective; that is, some experiences are more likely to be consolidated into long-term memory than others. Here, we propose and analyze a model that captures common computational principles underlying such phenomena. The key component of this model is a mechanism by which a long-term learning and memory system prioritizes the storage of synaptic changes that are consistent with prior updates to the short-term system. This mechanism, which we refer to as recall-gated consolidation, has the effect of shielding long-term memory from spurious synaptic changes, enabling it to focus on reliable signals in the environment. We describe neural circuit implementations of this model for different types of learning problems, including supervised learning, reinforcement learning, and autoassociative memory storage. These implementations involve synaptic plasticity rules modulated by factors such as prediction accuracy, decision confidence, or familiarity. We then develop an analytical theory of the learning and memory performance of the model, in comparison to alternatives relying only on synapse-local consolidation mechanisms. We find that recall-gated consolidation provides significant advantages, substantially amplifying the signal-to-noise ratio with which memories can be stored in noisy environments. We show that recall-gated consolidation gives rise to a number of phenomena that are present in behavioral learning paradigms, including spaced learning effects, task-dependent rates of consolidation, and differing neural representations in short- and long-term pathways.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>memory</kwd><kwd>consolidation</kwd><kwd>theory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI-1707398</award-id><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000015</institution-id><institution>Department of Energy</institution></institution-wrap></funding-source><award-id>CSGF (DE-SC0020347)</award-id><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000861</institution-id><institution>Burroughs Wellcome Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005270</institution-id><institution>McKnight Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001229</institution-id><institution>Mathers Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EB029858</award-id><principal-award-recipient><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A theory of memory consolidation illustrates the benefits of communication between short- and long-term memory systems to prioritize the storage of reliable memories.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Systems that learn and remember confront a tradeoff between memory acquisition and retention. Plasticity enables learning but can corrupt previously stored information. Consolidation mechanisms, which stabilize or render more resilient certain plasticity events associated with memory formation, are key to navigating this tradeoff (<xref ref-type="bibr" rid="bib34">Kandel et al., 2014</xref>). Consolidation may be mediated both by molecular dynamics at the synapse level (synaptic consolidation) and dynamics at the neural population level (systems consolidation).</p><p>In this work, we present a model and theoretical analysis of selective systems consolidation, with a focus on understanding the computational advantages it offers in terms of long-term learning and memory storage. Several prior theoretical studies have studied synaptic consolidation models from this perspective, providing descriptions of how synaptic consolidation affects the strength and lifetime of memories (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib36">Lahiri and Ganguli, 2013</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>). In these studies, synapses are modeled with multiple internal variables, operating at distinct timescales, which enable individual synapses to exist in more labile or more rigid (âconsolidatedâ) states. Such models can prolong memory lifetime and recapitulate certain memory-related phenomena, notably power-law forgetting curves. Moreover, this line of work has established theoretical limits on the memory retention capabilities of any such synaptic model, and shown that biologically realistic models can approximately achieve these limits (<xref ref-type="bibr" rid="bib36">Lahiri and Ganguli, 2013</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>). These theoretical frameworks leave open the question of what computational benefit is provided by <italic>systems</italic> consolidation mechanisms that take place in a coordinated fashion across populations of neurons.</p><p>The term systems consolidation most often refers to the process by which memories stored in the hippocampus are transferred to the neocortex (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib49">Squire and Alvarez, 1995</xref>; <xref ref-type="bibr" rid="bib18">Frankland and Bontempi, 2005</xref>; <xref ref-type="bibr" rid="bib39">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib40">McClelland and Goddard, 1996</xref>). Prior work has described the hippocampus and the neocortex as âcomplementary learning systemsâ, emphasizing their distinct roles: the hippocampus stores information about individual experiences, and the neocortex extracts structure from many experiences (<xref ref-type="bibr" rid="bib39">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib40">McClelland and Goddard, 1996</xref>). Related phenomena also occur in other brain systems. In rodents, distinct pathways underlie the initial acquisition and long-term execution of some motor skills, with motor cortex apparently passing off responsibility to basal ganglia structures as learning progresses (<xref ref-type="bibr" rid="bib35">Kawai et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">Dhawale et al., 2021</xref>). A similar consolidation process is observed during vocal learning in songbirds, where song learning is dependent on the region LMAN but song execution can, after multiple days of practice, become LMAN-independent and rely instead on the song motor pathway (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib56">Warren et al., 2011</xref>). Some insects also display a form of systems consolidation. Olfactory learning experiments in fruit flies reveal that short-term memory (STM) and long-term memory (LTM) retrieval recruit distinct neurons within the mushroom body, and the short-term pathway is necessary for LTM formation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib13">Cervantes-Sandoval et al., 2013</xref>; <xref ref-type="bibr" rid="bib17">Dubnau and Chiang, 2013</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic of short- and long-term memory systems across species and brain areas.</title><p>(<bold>A</bold>) In mice and other mammals, hippocampal memories are consolidated into neocortex. (<bold>B</bold>) Zebrafinch song learning initially depends on LMAN but later requires only HVC-to-RA synapses in the song motor pathway. (<bold>C</bold>) In the <italic>Drosophila</italic> mushroom body (inset), short- and long-term memories depend on dopamine-dependent plasticity in the <italic>Î³</italic> and <italic>Î±</italic> lobes, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig1-v1.tif"/></fig><p>The examples above are all characterized by two essential features: the presence of two systems involved in learning similar information and an asymmetric relationship between them, such that learning in one system (the âLTMâ) is facilitated by the other (the âSTMâ). Moreover, mounting evidence indicates that across all these systems, there exist mechanisms that selectively modulate or gate consolidation into long-term storage. In flies, for instance, recent work has shown that short-term olfactory memory recall gates LTM storage via a disinhibitory circuit, such that repeated stimulus-outcome pairings are consolidated into LTM but once-presented pairings are not (<xref ref-type="bibr" rid="bib4">Awata et al., 2019</xref>). A recent study in songbirds indicates that the rate at which song learning is consolidated into the song motor pathway is modulated by performance quality (<xref ref-type="bibr" rid="bib51">Tachibana et al., 2022</xref>). Finally, a large body of work has shown that propensity of hippocampal memories to be cortically consolidated is modulated by a variety of factors including repetition, reliability, and novelty (<xref ref-type="bibr" rid="bib52">Terada et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">Gorriz et al., 2023</xref>; <xref ref-type="bibr" rid="bib32">Jackson et al., 2006</xref>; <xref ref-type="bibr" rid="bib9">Brodt et al., 2016</xref>).</p><p>The ubiquity of the systems consolidation motif across species, neural circuits, and behaviors suggests that it offers broadly useful computational advantages that are complementary to those offered by synaptic mechanisms. In this work, we propose that the ability to selectively consolidate memories is the key computational feature that distinguishes systems from synaptic memory consolidation. To formalize this idea, we generalize prior theoretical studies studies by considering environments in which some memories should be prioritized more than others for long-term retention. We then introduce a model of selective systems consolidation and show that it can provide substantial performance advantages in such environments. In the model, synaptic updates are consolidated into LTM depending on their consistency with knowledge already stored in STM. We term this mechanism ârecall-gated consolidationâ. This model is well-suited to prioritize storage of reliable patterns of synaptic updates which are consistently reinforced over time. We derive neural circuit implementations of this model for several tasks of interest. These involve plasticity rules modulated by globally broadcast factors such as prediction accuracy, confidence, and familiarity. We develop an analytical theory that describes the limits on learning performance achievable by synaptic consolidation mechanisms and shows that recall-gated systems consolidation exhibits qualitatively different and complementary benefits. Our theory depends on a quantitative treatment of environmental statistics, in particular the statistics with which similar events recur over time. Different model parameter choices suit different environmental statistics, and give rise to different learning behavior, including spaced training effects. The model makes predictions about the dependence of consolidation rate on the consistency of features in an environment, and the amount of time spent in it. It also predicts that STM benefits from employing sparser representations compared to LTM. A variety of experimental data support predictions of the model, which we review in the Discussion.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Following prior work (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>), we consider a network of neurons connected by an ensemble of <italic>N</italic> synapses whose weights are described by a vector <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (our analysis also generalizes to synapses that store additional auxiliary information besides their weight; see Methods). For now, we are agnostic as to the structure of the network and its synaptic connections. The networkâs synapses are subject to a stream of patterns of candidate synaptic potentiation and depression events. We refer to such a pattern as a <italic>memory</italic>, defined by a vector <inline-formula><mml:math id="inf2"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mi>â</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. Synaptic weights are updated by memories according to a plasticity rule (see Methods). The plasticity rule always updates <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to be more aligned with the memory vector <inline-formula><mml:math id="inf4"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>; thus, the memory may be interpreted as a âtargetâ value for the synaptic weights. One simple example of a synaptic update rule is a âbinary switchâ model, in which synapses can exist in two states (active or inactive), and candidate synaptic updates are binary (potentiation or depression). In this model, inactive synapses activate (resp. active synapses inactivate) in response to potentiation (resp. depression) events with some probability <italic>p</italic>. However, our systems consolidation model can be used with any underlying synaptic mechanisms, and we will consider a variety of synaptic plasticity rules as underlying substrates.</p><p>In our framework, the same memory can be encountered repeatedly over time, and we will refer to such repeated encounters as âreinforcementâ of a memory (not to be confused with reward-contingent notions of reinforcement). We distinguish memories by the <italic>reliability</italic>, or frequency, with which they are reinforced. The notion of reliability in our framework is meant to capture the idea that the structure of events in the world which drive synaptic plasticity is in some cases consistent over time, and in other cases inconsistent. For now, we focus on a simple environment model which captures this essential distinction, in which there are two kinds of memories: âreliableâ and âunreliableâ. Reliable memories are consistent patterns of synaptic updates that are reinforced regularly over time, whereas unreliable memories are spurious, randomly sampled patterns of synaptic updates. Concretely, in simulations, we assume that a given reliable memory is reinforced with independent probability <italic>Î»</italic> at each timestep, and otherwise a randomly sampled unreliable memory is encountered.</p><p>A useful measure of system performance is the memory <italic>recall factor</italic>, defined as the overlap <inline-formula><mml:math id="inf5"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> between a memory <inline-formula><mml:math id="inf6"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> and the current synaptic state <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Specifically, we are interested in the signal-to-noise ratio (SNR) of the recall factor for reliable memories, (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>), which normalizes the recall strength relative to the expected value of the fluctuations in recall factors for random memory vectors <inline-formula><mml:math id="inf8"><mml:semantics><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>. We assume for simplicity that memories and weight vectors are mean-centered, so that the SNR may be written as follows:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><sec id="s2-1"><title>Recall-gated systems consolidation</title><p>In our model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), we propose that the population of <italic>N</italic> synapses is split into two subpopulations which we call the âshort-term memoryâ (STM) and âlong-term memoryâ (LTM). Upon every presentation of a memory <inline-formula><mml:math id="inf9"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, the STM recall <inline-formula><mml:math id="inf10"><mml:semantics><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> is computed. Learning in the LTM is modulated by a factor <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We refer to <italic>g</italic> as the âgating functionâ. For now we assume <italic>g</italic> to be a simple threshold function, equal to 0 for <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>Î¸</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and 1 for <inline-formula><mml:math id="inf13"><mml:semantics><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub><mml:mo>â¥</mml:mo><mml:mi>Î¸</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, for some suitable threshold <italic>Î¸</italic>. This means that consolidation occurs only when a memory is reinforced at a time when it can be recalled sufficiently strongly by the STM. Later we will consider different choices of the gating function <italic>g</italic>, which may be more appropriate depending on the statistics of memory recurrence in the environment.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Recall-gated systems consolidation model.</title><p>(<bold>A</bold>) Schematic of systems consolidation model. Top and bottom rows illustrate different examples, in which a memory is not consolidated or consolidated, respectively. Memories <inline-formula><mml:math id="inf14"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> correspond to patterns of candidate potentiation and depression events (dashed arrows) applied to a synaptic population with weights w (solid arrows). The synaptic population is divided into an STM (left) and LTM (right). Memories that provoke strong recall in the STM â that is, overlap strongly with the present synaptic state â enable plasticity (consolidation) in the LTM; otherwise plasticity in the LTM is gated (gray shaded rectangle). Note that the synaptic weights and the components of the memory corresponding to the LTM need not be linked to those of the STM (i.e. the patterns of arrows are different between the left and right columns). (<bold>B</bold>) Schematic of the environmental statistics. A reliable memory (green) arrives repeatedly with probability <italic>Î»</italic> at each time step, with randomly sampled âunreliableâ memories (gray) interspersed. The LTM is exposed to a filtered subset of consolidated memory traces with a higher proportion of reliable memories. (<bold>C</bold>) Simulation of recall performance of a single reliable memory with time as it is presented with probability <inline-formula><mml:math id="inf15"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> at each time step, <italic>N</italic>=2000 synapses (1000 each in the STM and LTM). The STM and LTM learning rates (binary switching probabilities) are <italic>p</italic>=0.25 and <italic>p</italic>=0.05, respectively, and the synaptic state is initialized randomly, each synapse initially active with probability 0.5. In the recall-gated model, the gating threshold is set at <inline-formula><mml:math id="inf16"><mml:semantics><mml:mrow><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. Shaded regions indicate standard deviation across 1000 simulations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Same as <xref ref-type="fig" rid="fig2">Figure 2C</xref>, but with multiple reliable memories simultaneously learned, each recurring equally often at a rate <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all reliable memories <italic>i</italic>.</title><p>Here <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> synapses, and the STM and LTM learning rates are 0.05 and 0.01, respectively. In the recall-gated model, the gating threshold is set at <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each plot corresponds to a different number <italic>P</italic> of reliable memories being stored (the SNR shown is averaged across the reliable memories). The behavior of the SNR of an individual reliable memory is approximately the same as in the single-memory case for small values of <italic>Î»</italic><sub><italic>to</italic>t</sub> but diverges from it when <italic>Î»</italic><sub><italic>to</italic>t</sub> grows large.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>SNR as a function of repetitions for single populations without consolidation, varying the parameter <italic>k</italic> of the Weibull distribution governing interarrival times (and defining the learnable timescale in terms of the expected interarrival time).</title><p>The behavior of the systems scales similarly for diverse values of <italic>k</italic>, justifying the use of the deterministic approximation <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">â</mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for theoretical calculations. The learning rate for the binary model here is set at 0.1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We refer to this mechanism as <italic>recall-gated consolidation</italic>. Its function is to filter out unreliable memories, preventing them from affecting LTM synaptic weights. With an appropriately chosen gating function, reliable memories will pass through the gate at a higher rate than unreliable memories. Consequently, events that trigger plasticity in the LTM will consist of a higher proportion of reliable memories (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and hence the LTM will attain a higher SNR than the STM. The cost of this gating is to incur some false negativesâreliable memory presentations that fail to update the LTM. However, some false negatives can be tolerated given that we expect reliable memories to recur multiple times, and information about these events is still present in the STM. As a proof of concept of the efficacy of recall-gated consolidation, we conducted a simulation in which memories correspond to random binary patterns and plasticity follows a binary switch rule (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This simulation implements an âideal observerâ model in which we assume the system has direct access to the memory vector and can compute the recall factor exactly (realistic implementations are discussed below). Notably, recall-dependent consolidation results in reliable memory recall with a much higher SNR than an alternative model in which LTM weight updates proceed independently of STM recall.</p></sec><sec id="s2-2"><title>Neural circuit implementations of recall-gated consolidation</title><p>Our model requires a computation of recall strength, which we defined as the overlap between a memory and the current state of the synaptic population. From this definition, it is not clear how recall strength can be computed biologically. The mechanisms underlying computation of recall strength will depend on the task, network architecture, and learning rule giving rise to memory vectors. A simple example is the case of a population of input neurons connected to a single downstream output neuron, subject to a plasticity rule that potentiates synapses corresponding to active inputs. In this case, the recall strength quantity corresponds exactly to the total input received by the output neuron, which acts as a familiarity detector. Below, we give the corresponding recall factors for other learning and memory tasks: supervised learning, reinforcement learning, and unsupervised auto-associative memory, summarized in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. The expressions for the recall factors are derived for each learning rule of interest in the Appendix. We emphasize that our use of the term ârecallâ refers to the familiarity of <italic>synaptic update patterns</italic> (specifically, memory vectors), and does not necessarily correspond to familiarity of stimuli or other task variables. Thus, although we will continue to use the term ârecall factorâ, for a given task the recall factor quantity may have a different semantic interpretation, summarized in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. Note also that we use the term âlearning ruleâ to refer to how the memory vector is constructed from task quantities, while âplasticity ruleâ is reserved for the mechanism by which memory vectors update synaptic weights.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Circuit architectures for recall-gated consolidation model.</title><p>(<bold>A</bold>) Description of learning rules corresponding to different types of learning problems and corresponding expressions for the recall factor used in the recall-gated consolidation model. (<bold>B</bold>) Schematic indicating a possible implementation of the model in a supervised learning problem, where LTM plasticity is modulated by the consistency between STM predictions and ground-truth labels. (<bold>C</bold>) Like B, but for a reinforcement learning problem. LTM plasticity is gated by both STM action confidence and the presence of reward. (<bold>D</bold>) Like B and C, but for an autoassociative unsupervised learning problem. As above, <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to neural activity and <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to the network weights, which here are recurrent. LTM plasticity is gated by familiarity detection in the STM module. (<bold>E</bold>) Simulation of a binary classification problem, <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:mn>0.125</mml:mn><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. There are 20 total stimuli each associated with a random binary (Â±1) label and each appearing with probability <inline-formula><mml:math id="inf24"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> at each timestep (otherwise a random stimulus is presented, with a random binary label). Plot shows the classification accuracy over time, given by the outputs of the STM and LTM of the consolidation model. Shaded region indicates standard deviation over 50 simulations. (<bold>F</bold>) Simulation of a reinforcement learning problem, <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:mn>0.125</mml:mn><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. There are five total stimuli, each appearing with probability <inline-formula><mml:math id="inf26"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> at each timestep (otherwise a random stimulus is presented), and three possible actions. Each stimulus has a corresponding action that yields reward (the reward is randomly sampled for the random stimuli). The plot shows average reward per step over time, evaluated using the actions given by the STM or LTM (during learning, the STM action was always used). (<bold>G</bold>) Simulation of an autoassociative learning problem. <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>4000</mml:mn><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. A single stimulus appears with probability <inline-formula><mml:math id="inf28"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> at each timestep, and otherwise a random stimulus appears. Recall performance is evaluated by exposing the system to a noisy version of the reliable stimulus seen during training, allowing the recurrent dynamics of the network to run for 5 timesteps, and measuring the correlation of the final state of the network with the ground-truth pattern.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig3-v1.tif"/></fig><sec id="s2-2-1"><title>Supervised learning</title><p>Suppose a population of neurons with activity <bold>x</bold> representing stimuli is connected via feedforward weights to a readout population with activity <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The goal of the system is to predict ground-truth outputs <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. A simple choice of the form of the memory <inline-formula><mml:math id="inf31"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> (written with a capital letter since the synaptic weights are being interpreted as a matrix) that will train the system is <inline-formula><mml:math id="inf32"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, corresponding to a associative Hebbian learning rule <xref ref-type="bibr" rid="bib27">Hebb, 1949</xref>. The corresponding recall factor is <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, corresponding to prediction accuracy (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; see Appendix for derivation).</p></sec><sec id="s2-2-2"><title>Reinforcement learning</title><p>Suppose a population of neurons with activity <italic>x</italic> representing an animalâs state is connected to a population with activity <inline-formula><mml:math id="inf34"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Ï</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> which controls the action selection probabilities. Specifically, the log probability of selecting action <italic>a</italic> is proportional to <italic>Ï<sub>a</sub></italic> (see Methods). Following action selection, the animal receives a reward, the value of which depends on the state and chosen action. A simple approach to reinforcement learning is to use a memory vector arising from a âthree-factorâ rule (<xref ref-type="bibr" rid="bib33">Joel et al., 2002</xref>; <xref ref-type="bibr" rid="bib19">FrÃ©maux and Gerstner, 2015</xref>; <xref ref-type="bibr" rid="bib38">Lindsey et al., 2024</xref>) <inline-formula><mml:math id="inf35"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mtext>reward</mml:mtext><mml:mo>â</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector with 1 in the index corresponding to the selected action and 0 elsewhere. This learning rule reinforces actions that lead to reward. For this model, the corresponding recall factor is <inline-formula><mml:math id="inf37"><mml:semantics><mml:mrow><mml:mtext>reward</mml:mtext><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, a multiplicative combination of reward and the animalâs confidence in its selected action, as measured by its a priori log likelihood of selecting that action (see Appendix for derivation). Intuitively, the recall factor will be high when a confidently chosen action leads to reward (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p></sec><sec id="s2-2-3"><title>Unsupervised autoassociative memory</title><p>Suppose a population of neurons with activity <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and recurrent weights <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> stores memories as attractors according to an autoassociative Hebbian rule, where memories correspond to <inline-formula><mml:math id="inf40"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, similar to a Hopfield network <xref ref-type="bibr" rid="bib29">Hopfield, 1982</xref>. In this case, the recall factor can be expressed as <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, a comparison between stimulus input <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and recurrent input <inline-formula><mml:math id="inf43"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> (see Appendix for derivation). Intuitively, the recall factor measures the familiarity of the stimulus, as highly familiar stimuli will exhibit attractor behavior, making <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> highly correlated. Such a quantity could in principle be computed directly, for instance if separate dendritic compartments represent the feedforward inputs and the recurrent inputs, though such a mechanism is speculative. This quantity can also be approximated using a separate novelty readout trained alongside the recurrent weights, which is the implementation we use in our simulation. In this approach, a set of familiarity readout weights <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> receive the neural population activity as input, and outputs a scalar signal indicating the familiarity of that activity pattern. These familiarity readouts are updated according to their own corresponding memory vector <inline-formula><mml:math id="inf47"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>. The output of these weights, <inline-formula><mml:math id="inf48"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>, estimates the familiarity of the activity pattern, and is used as the recall factor (see Appendix for more details on when this approximation is equal to the ideal recall factor).</p><p>To verify that the advantages of recall-gated consolidation illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref> apply in these tasks, we simulated the three architectures and learning rules described above (see Methods for simulation details). In each case, learning takes place online, with reliable task-relevant stimuli appearing a fraction <italic>Î»</italic> of the time, interspersed among randomly sampled unreliable stimuli. In the case of supervised and reinforcement learning tasks, unreliable stimuli are paired with random labels and random rewards, respectively. Reliable stimuli are associated with consistent labels or action-reward contingencies. We find that recall-gated consolidation provides significant benefits in each case, illustrating that the theoretical benefits of increased SNR in memory storage translate to improved performance on meaningful tasks (<xref ref-type="fig" rid="fig3">Figure 3E, F and G</xref>).</p></sec></sec><sec id="s2-3"><title>An analytical theory of the recall of repeatedly reinforced memories</title><p>We now turn to analyzing the behavior of the recall-gated systems consolidation model more systematically, to understand the source of its computational benefits and characterize other predictions it makes. To do so, we developed an analytic theory of memory system performance, with and without recall-gated consolidation. To make the analysis tractable, our subsequent results assume an ideal observer model as in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, where we assume the system has direct access to memory vectors and can compute the recall factor exactly. Importantly, our framework differs from prior work (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>) in considering environments with intermittent repeated presentations of the same memory. We adopt several assumptions for analytical tractability. First, as in previous studies, we assume that inputs have been preprocessed so that the representations of different memories are random and uncorrelated with one another (<xref ref-type="bibr" rid="bib23">Gluck and Myers, 1993</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>). We also assume, for now, that each memory consists of an equal number of positive and negative entries, although later we will relax this assumption. We are interested in tracking the SNR of recall for a given reliable memory. We emphasize that this quantity is an abstract measure of system performance reflecting the degree to which a specific set of synaptic changes (a memory trace) is retained in the system, and its interpretation varies according to the task in question (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>The dynamics of memory storage depend strongly on the underlying synapse model and plasticity rule. Given a synaptic model, an important quantity to consider is its associated âforgetting curveâ <italic>m</italic>(<italic>t</italic>), defined as the average SNR of recall for a memory <inline-formula><mml:math id="inf49"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> at <italic>t</italic> timesteps following its first presentation, assuming a new randomly sampled memory has been presented at each timestep since. For example, the binary switch model with transition probability <italic>p</italic> has an associated forgetting curve <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mi>p</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>). More sophisticated synapse models, such as the cascade model of <xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref> and multivariable model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref> achieve power-law forgetting curves (see Methods). In the limit of large system size <italic>N</italic> and under the assumption that memories are random, uncorrelated patterns, the forgetting curve is an exact description of the decay of recall strength.</p><p>Forgetting curves capture the behavior of a system in response to a single presentation of a memory, but we are concerned with the behavior of memory systems in response to multiple reinforcements of the same memory trace. Thus, another key quantity in our theory is the interarrival distribution <italic>p</italic>(<italic>I</italic>), which describes the distribution of intervals between repeated presentations of the same memory, and its expected value <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the average interval length. Our simplest case of interest is the case in which a given memory recurs according to a Poisson process; that is, it is reinforced with probability <italic>Î»</italic> at each timestep, independent of the history of recent reinforcements (as in the simulation in <xref ref-type="fig" rid="fig2">Figure 2C</xref>). This case corresponds to an exponential interarrival distribution <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Î»</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with mean interarrival time <inline-formula><mml:math id="inf53"><mml:semantics><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>Î»</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>We now quantify the recall strength for a memory that has been reinforced <italic>R</italic> times. For the synapse models we consider, this quantity can be approximated accurately (see Appendix) by summing the strengths of preceding forgetting curves, that is:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>t<sub>i</sub></italic> is the time elapsed since the <italic>i</italic>th reinforcement of the memory. This quantity is a random variable whose value depends on the history of interarrival intervals of the memory, and the specific unreliable memories that have been stored in intervening timesteps. To more concisely characterize a systemâs memory performance, we introduce a new summary metric, the <italic>learnable timescale</italic> of the system. For a given target SNR value and allowable probability of error <italic>Ïµ</italic>, the learnable timescale <inline-formula><mml:math id="inf54"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>Î²</mml:mi><mml:mi>Ïµ</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> is defined as the maximum interarrival timescale <italic>Ï</italic> for which the <inline-formula><mml:math id="inf55"><mml:semantics><mml:mrow><mml:mtext>SNR</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> of recall will exceed <italic>Î²</italic> with probability <inline-formula><mml:math id="inf56"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ïµ</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. We fix <inline-formula><mml:math id="inf57"><mml:semantics><mml:mrow><mml:mi>Ïµ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> throughout this work; this choice has no qualitative effect on our results. Learnable timescale captures the systemâs ability to reliably recall memories that are presented intermittently. We note that there exists a close relationship between learnable timescale and the memory capacity of the system (the number of memories it can store), with the two quantities becoming linearly related in environments with a high frequency of unreliable memory presentations (see Appendix and <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>).</p><p>The quantifications of recall SNR and learnable timescale we present in figures are computed numerically, as deriving exact analytical expressions for learnable timescale is difficult due to the randomness of the interarrival distribution. However, to gain theoretical intuition, we find it useful to consider the following approximation, corresponding to an environment in which memories are reinforced at deterministic intervals of length <italic>Ï</italic>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This approximation is an upper bound on the true SNR in the limit of small <italic>Ïµ</italic>, and empirically provides a close match to the true dependence of <inline-formula><mml:math id="inf58"><mml:semantics><mml:mrow><mml:mtext>SNR</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> on <italic>R</italic> (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). Using this approximation allows us to provide closed-form analytical estimates of the behavior of SNR and learnable timescale as a function of system and environment parameters.</p></sec><sec id="s2-4"><title>Theory of recall-gated consolidation</title><p>In the recall-gated consolidation model, the behavior of the STM is identical to that of a model without systems consolidation. The LTM, on the other hand, behaves differently, updating only in response to the subset of memory presentations that exceed a threshold level of recall in the STM. From the perspective of the LTM, this phenomenon has the effect of changing the distribution of interval lengths between repeated reinforcements of a reliable memory. For exponentially distributed interarrival times, the induced effective interarrival distribution in the LTM is also exponential with new time constant <inline-formula><mml:math id="inf59"><mml:semantics><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> given by<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>Ï</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>I</italic> is the (stochastic) length of intervals between presentations of the same reliable memory, <italic>Î¸</italic> is the consolidation threshold, and <inline-formula><mml:math id="inf60"><mml:semantics><mml:mi>Î¦</mml:mi></mml:semantics></mml:math></inline-formula> is the cumulative distribution function of the Gaussian distribution with mean 0 and variance 1. This approximation is valid in the limit of large system sizes <italic>N</italic>, where responses to unreliable memories are nearly Gaussian. For general (non-exponential) interarrival distributions, the shape of the effective LTM interarrival distribution may change, but the above expression for <inline-formula><mml:math id="inf61"><mml:semantics><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> remains valid.</p><p>We note that although the consolidation threshold <italic>Î¸</italic> can be chosen arbitrarily, setting it to too high a value has the effect of reducing the probability with which reliable memories are consolidated, by a factor of <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For large values of <italic>Î¸</italic> this reduction can become unacceptably small. For a given number of memory repetitions <italic>R</italic>, we restrict ourselves to values of <italic>Î¸</italic> for which the probability that no consolidation takes place after <italic>R</italic> repetitions is smaller than the allowable probability of error <italic>Ïµ</italic>. Where <italic>R</italic> is not explicitly reported, we set <italic>R</italic>=2, which corresponds to analyzing the behavior of the model when a memory is reinforced once following its initial presentation.</p></sec><sec id="s2-5"><title>Recall-gated consolidation increases SNR and learnable timescale of memories</title><p>For fixed statistics of memory presentations, as the SNR of the STM increases (say, due to increasing <italic>N</italic>), stricter thresholds can be chosen for consolidation which filter out an increasing proportion of unreliable memory presentations, without reducing the consolidation rate of reliable memories (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). Consequently, the SNR of the LTM can grow much larger than that of the STM, and the amplification of SNR increases with the SNR of the STM. Notably, the SNR of the LTM in the recall-gated consolidation model also exceeds that of a control model in which STM and LTM modules are both present but do not interact (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>), which performs comparably to the STM by itself due to the lack of selective consolidation.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Properties of the recall-gated consolidation model.</title><p>(<bold>A</bold>) Distribution (probability density function) of reliable and unreliable memory overlaps (on a log scale), varying the number of synapses <italic>N</italic>, <inline-formula><mml:math id="inf63"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> Shaded regions indicate consolidation thresholds that preserve 10% of reliable memory presentations. Units are standard deviations of the distribution of recall for randomly sampled memories. (<bold>B</bold>) LTM SNR induced by consolidation (with threshold set as in A, to consolidate 10% of reliable memory presentations) as <italic>N</italic> varies. The parallel model uses aslower learning rate (the value of <italic>p</italic> in the binary switching synapse model is a factor of 10 smaller) in the LTM than the STM. (<bold>C</bold>) Learnable timescale as a function of target SNR, for several values of N, using the binary switching synapse model with <inline-formula><mml:math id="inf64"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula>. (<bold>D</bold>) Distribution of reliable and unreliable memory overlaps, with various potential gating thresholds indicated, <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>E</bold>) Fraction of memory presentations consolidated (log scale) vs recall threshold for consolidation, <inline-formula><mml:math id="inf66"><mml:semantics><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. (<bold>F</bold>) LTM SNR induced by consolidation vs. the expected number of repetitions before consolidation occurs, <inline-formula><mml:math id="inf67"><mml:semantics><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, same color legend as panel E. Increasing the expected number of repetitions corresponds to setting a more stringent consolidation threshold which filters a higher proportion of reliable memory presentations. (<bold>G</bold>) Learnable timescale at a target SNR of 10 as a function of number of reliable memory repetitions for several underlying synapse models, <inline-formula><mml:math id="inf68"><mml:semantics><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. (<bold>H</bold>) Same as <bold>G</bold>, considering only the multivariable model as the underlying synapse model, and varying the interarrival interval regularity factor <italic>k</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Effects of varying rate of reliable memory presentations.</title><p>(<bold>A</bold>) Same as <xref ref-type="fig" rid="fig4">Figure 4A</xref>, also varying the presentation rate Î» of reliable memories. (<bold>B</bold>) Same as <xref ref-type="fig" rid="fig4">Figure 4B</xref>, also varying the presentation rate Î» of reliable memories.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>Comparison of different synaptic learning rules.</title><p>(<bold>A</bold>) Same as <xref ref-type="fig" rid="fig4">Figure 4C</xref>, also varying the underlying synaptic learning rule. (<bold>B</bold>) Same learnable timescale information as panel <italic>A</italic>, presented as a function of the synaptic population size <italic>N</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 3.</label><caption><title>Same information as <xref ref-type="fig" rid="fig4">Figure 4G</xref>, varying the population size <italic>N</italic> and the desired <italic>SNR</italic>.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 4.</label><caption><title>Same information as <xref ref-type="fig" rid="fig4">Figure 4H</xref>, varying the population size <italic>N</italic> and the desired <italic>SNR.</italic></title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig4-figsupp4-v1.tif"/></fig></fig-group><p>We may also view the benefits of consolidation in terms of the learnable timescale of the system. Recall-gated consolidation enables longer learnable timescales, particularly at high target SNRs (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>). We note that our definition of SNR considers only noise arising from random memory sampling and presentation order. High SNR values may be essential for adequate task performance in the face of additional sources of noise, or when the system is asked to generalize by recalling partially overlapping memory traces (<xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>).</p></sec><sec id="s2-6"><title>Recall-gated consolidation enables better scaling of memory retention with repeated reinforcement</title><p>As mentioned previously, higher consolidation thresholds reduce the rate at which reliable memories are consolidated. However, the consolidation rate of <italic>unreliable</italic> memories decreases even more quickly as a function of the threshold (<xref ref-type="fig" rid="fig4">Figure 4D and E</xref>). Hence, higher thresholds increase the fraction of consolidated memories which are reliable, at the expense of reducing the rate of consolidation into LTM. This tradeoff may be acceptable if reliable memories are reinforced a large number of times, as in this case they can still be consolidated despite infrequent LTM plasticity. In other words, as the number of anticipated repetitions <italic>R</italic> of a single reliable memory increases, higher thresholds can be used in the gating function, without preventing the eventual consolidation of that memory. Doing so allows more unreliable memory presentations to be filtered out and consequently increases the SNR in the LTM (<xref ref-type="fig" rid="fig4">Figure 4F</xref>).</p><p>Assuming, as we have so far, that reliable memories are reinforced at independently sampled times at a constant rate, we show (calculations in Appendix) that the dependence of learnable timescale on <italic>R</italic> is linear, regardless of the underlying synaptic model (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, <xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3</xref>). Synaptic models with a small number of states, such as binary switch or cascade models, are unable to achieve this scaling without systems consolidation (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). In particular, the learnable timescale is roughly invariant to <italic>R</italic> for the binary switch model, and scales approximately logarithmically with <italic>R</italic> for the cascade model (see Appendix for derivation). Synaptic models employing a large number of internal states (growing exponentially with the intended timescale of memory retention), like the multivariable model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>, can also achieve linear scaling of learnable timescale on <italic>R</italic>. However, these models still suffer a large constant factor reduction in learnable timescale compared to models employing recall-gated consolidation (<xref ref-type="fig" rid="fig4">Figure 4G</xref>).</p></sec><sec id="s2-7"><title>Consolidation dynamics depend on the statistics of memory recurrence</title><p>The benefit of recall-gated consolidation is even more notable when the reinforcement of reliable memories does not occur at independently sampled times, but rather in clusters. Such irregular interarrival times might naturally arise in real-world environments. For instance, exposure to one environmental context might induce a burst of high-frequency reinforcement of the same pattern of synaptic updates, followed by a long drought when the context changes. Intentional bouts of study or practice could also produce such effects. The systems consolidation model can capitalize on such bursts of reinforcement to consolidate memories while they can still be recalled.</p><p>To formalize this intuition, we extend our theoretical framework to allow for more general patterns of memory recurrence. In particular, we let <italic>p</italic>(<italic>I</italic>) indicate the probability distribution of interarrival intervals <italic>I</italic> between reliable memory presentations. So far, we have considered the case of reliable memories whose occurence times follow Poisson statistics, corresponding to an exponentially distributed interval distribution. To consider more general occurrence statistics, we consider a family of interrarival distributions known as Weibull distributions. This class allows control over an additional parameter <italic>k</italic> which modulates âburstinessâ of reinforcement, and contains the exponential distribution as a special case (<italic>k</italic>=1). For <italic>k</italic>&lt;1, reliable memory presentations occur with probability that decays with time since the last presentation. In this regime, the same memory is liable to recur in bursts separated by long gaps (details in Methods).</p><p>Without systems consolidation, the most sophisticated synapse model we consider, the multivariable model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>, achieves a scaling of learnable timescale that is linear with <italic>R</italic> regardless of the regularity factor <italic>k</italic>. In fact, we show (see Appendix) that the best possible learnable timescale that can achieved by any synaptic consolidation mechanism scales approximately linearly in <italic>R</italic>, up to logarithmic factors. However, for the recall-gated consolidation model, the learnable timescale scales as <inline-formula><mml:math id="inf69"><mml:semantics><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> when <inline-formula><mml:math id="inf70"><mml:semantics><mml:mrow><mml:mi>k</mml:mi><mml:mo>â¤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4H</xref>, <xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3</xref>). In this sense, recall-gated consolidation outperforms any form of synaptic consolidation at learning from irregularly spaced memory reinforcement.</p></sec><sec id="s2-8"><title>Alternative gating functions suit different environmental statistics and predict spaced training effects</title><p>Thus far, we have considered a threshold gating function, which is well-suited to environments in which unreliable memories are each only encountered once. We may also imagine an environment in which unreliable memories tend to recur multiple times, but over a short timescale (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, top). In such an environment, the strongest evidence for a memoryâs reliability is if it overlaps to an <italic>intermediate</italic> degree with the synaptic state (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, bottom). The appropriate gating function in this case is no longer a threshold, but rather a non-monotonic function of STM memory overlap, meaning that memories are most likely to be consolidated if reinforced at intermediate-length intervals (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Such a mechanism is straightforward to implement using neurons tuned to particular ranges of recall strengths. This model behavior is consistent with spaced learning effects reported in flies (<xref ref-type="bibr" rid="bib6">Beck et al., 2000</xref>), rodents (<xref ref-type="bibr" rid="bib22">Glas et al., 2021</xref>), and humans (<xref ref-type="bibr" rid="bib46">Rovee-Collier et al., 1995</xref>; <xref ref-type="bibr" rid="bib55">Verkoeijen et al., 2005</xref>), which all show a characteristic inverted U-shaped dependence of memory performance on spacing interval.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Alternative memory gating functions.</title><p>(<bold>A</bold>) Top: Example sequence of memory presentations where unreliable memories (gray) can repeat multiple times, but only within within a short timescale (note gradient of light to dark). Bottom: Distribution of reliable and unreliable memory overlaps induced by such memory presentation statistics (log scale on x axis). Shaded region indicates overlap values that are at least ten times as likely for reliable memories as for unreliable memories. (<bold>B</bold>) Probability of consolidation, with the gating function chosen such that only overlaps within the shaded region of panel A are consolidated, as a function of interarrival interval. (<bold>C</bold>) SNR at 8 timesteps following 5 spaced repetitions of a memory, with spacing interval indicated on the x axis, for the multivariable synapse model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref> with no systems consolidation. Spaced training effects are present at short timescales, but not if other memories are presented during the interpresentation intervals. (<bold>D</bold>) Distribution of recall strengths corresponding to different kinds of memories, in an environment with many reliable memories. In the environment model, reliable memories are reinforced with different interarrival interval distributions, and the timescales of these distributions for different memories are distributed log-uniformly. The environment also has as a background rate of unreliable memory presentations appearing at fraction 0.9 of timesteps. (<bold>E</bold>) Depiction of a generalization of the model in which memories can be consolidated into different LTM sub-modules, according to gating functions tuned to different recall strengths (intended to target reliable memories with different timescales of recurrence). (<bold>F</bold>) A consequence of the model outlined in panel E is a smooth positive dependence of memory lifetime on the spacing of repetitions, up to some threshold spacing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Same information as <xref ref-type="fig" rid="fig5">Figure 5C</xref>, varying the learning rate (scale of potentiation/depression impulses, relative to the maximum/minimum threshold values in the model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>), and the length of time following spaced training at which the systemâs recall SNR is evaluated.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Same as <xref ref-type="fig" rid="fig5">Figure 5D</xref> (top row) and <xref ref-type="fig" rid="fig5">Figure 5F</xref> (bottom row), for different population sizes <italic>N</italic>.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 3.</label><caption><title>Same as <xref ref-type="fig" rid="fig5">Figure 5D</xref> (top row) and <xref ref-type="fig" rid="fig5">Figure 5F</xref> (bottom row), for different memory recurrence regularity factors (Weibull distribution parameter <italic>k</italic>).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig5-figsupp3-v1.tif"/></fig></fig-group><p>While some synapse-level models (such as the multivariable synapse model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>) can also give rise to spaced training effects, these effects require that a synapse undergoes few additional potentiation or depression events between the spaced reinforcements (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). This is because spacing effects in such models arise when synapse-local variables are saturated, and saturation effects are disrupted when other events are interspersed between repeated presentations of the same memory. Hence, the spacing effects arising from such models are unlikely to be robust over long timescales. Recall-gated systems consolidation, on the other hand, can yield spaced training effects robustly in the presence of many intervening plasticity events.</p></sec><sec id="s2-9"><title>Heterogeneous gating functions suit complex environments with multiple memories reinforced at different timescales</title><p>Thus far we has assumed a dichotomy between unreliable, one-off memories and reliable memories which recur according to particular statistics. In more realistic scenarios, there will exist many repeatedly reinforced memories, which may be reinforced at distinct timescales. We may be interested in ensuring good recall performance over a distribution of memories with varying recurrence statistics. For concreteness, we consider the specific case of an environment with a large number of distinct reliably reinforced memories, whose characteristic interarrival timescales are log-uniformly distributed. As before, unreliable memories are also presented with a constant probability per timestep.</p><p>The recall-gated plasticity model already described, using a threshold function for consolidation, still provides the benefit of filtering unreliable memory traces from the LTM. However, further improved memory recall performance is achieved with a simple extension to the model. The LTM can be subdivided into a set of subpopulations, each with distinct gating functions that specialize for different memory timescales by selecting for different recall strengths (<xref ref-type="fig" rid="fig5">Figure 5D and E</xref>). That is, one subpopulation consolidates strongly recalled memories, another consolidates weakly recalled memories, and others lie on a spectrum between these extremes. The effect of this arrangement is to assign infrequently reinforced memory traces to subpopulations which experience less plasticity, allowing these traces to persist until their next reinforcement. This heterogeneity of timescales is consistent with observations in a variety of species of intermediate timescale memory traces (<xref ref-type="bibr" rid="bib45">Rosenzweig et al., 1993</xref>; <xref ref-type="bibr" rid="bib12">Cepeda et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Davis, 2011</xref>).</p><p>Studies of spaced training effects have found that the optimal spacing interval during training depends on the interval between training and evaluation (<xref ref-type="bibr" rid="bib11">Cepeda et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Cepeda et al., 2008</xref>). In particular, the timescale of memory retention is observed to increase smoothly with the spacing interval used during training. Our extended model naturally gives rise to this behavior (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5âfigure supplement 3</xref>), due to the fact that the lifetime of a consolidated memory scales inversely with the frequency with which memories are consolidated into its corresponding LTM subpopulation.</p></sec><sec id="s2-10"><title>Predicted features of memory representations and consolidation dynamics</title><p>The recall-gated consolidation model makes a number of key predictions. The most basic consequence of the model is that responsibility for recalling a memory will gradually shift from the STM to the LTM as consolidation progresses, rendering the recall performance of the system increasingly robust to the inactivation of the STM (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). A more specific prediction of the model is the <italic>rate</italic> of updates to the LTM increases with time, as STM recall grows stronger (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The rate of LTM updates also increases with reliability of the environment (operationalized as the proportion of synaptic update events which correspond to reliable memories; <xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Predictions of the model.</title><p>(<bold>A</bold>) Top: Recall performance for a single reliable memory (true positive rate, with a decision threshold set to yield a 10% false positive rate) as learning progresses. Simulation environment is the same as in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Bottom: difference between combined recall performance and LTM-only performance. The STM makes diminishing contributions to recall over time. (<bold>B</bold>) Probability of consolidation into LTM increases with experience and with the reliability of the environment (parameterized here by the recurrence frequency <italic>Î»</italic> of reliable memories). Simulation environment is the same as in panel A. (<bold>C</bold>) For a single population of binary synapses (no consolidation) and Poisson memory recurrence, mean SNR as a function of reliable memory recurrence frequency <italic>Î»</italic> and memory sparsity <italic>f</italic>. Dots indicate simulation results and solid lines indicate analytical approximation. <italic>N</italic>=1,024. (<bold>D</bold>) For the systems consolidation model using binary synapses, total system SNR (<italic>N</italic>=256) as a function of memory sparsity in the STM and LTM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90793-fig6-v1.tif"/></fig><p>The recall-gated consolidation model also makes predictions regarding neural representations in the STM and LTM. Until now we have assumed that memories consist of balanced potentiation and depression events distributed across the population. However, memories may involve only a sparse subset of synapses, for instance if synaptic plasticity arises from neural activity which is itself sparse. To formalize this notion, we consider memories that potentiate a fraction <italic>f</italic> of synapses, and a correspondingly modified binary switch plasticity rule such that potentiation activates synapses with probability <italic>p</italic> and depression inactivates synapses with probability <inline-formula><mml:math id="inf72"><mml:semantics><mml:mrow><mml:mfrac><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mi>p</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. We show analytically (see Appendix) that in the limit of low <italic>f</italic>, the SNR-optimizing choice of <italic>f</italic> is proportional to the rate <italic>Î»</italic> of reliable memory reinforcement (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Other factors, such as energetic constraints and noise-robustness, may also affect the optimal coding level. In general, however, our analysis shows that environments with infrequent reinforcement of a given reliable memory incentivize sparser representations. As the effective value of <italic>Î»</italic> is amplified in the LTM module, it follows that the LTM benefits from a denser representation than the STM. Interestingly, we also find that the optimal sparsity in the STM decreases when optimizing for the overall SNR of the systemâthat is, the optimal STM representation is even more sparse in the context of supporting LTM consolidation than it would be in isolation. Taken together, these two effects result in much denser representations being optimal in the LTM than in the STM (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). One consequence of denser representations is greater generalization in the face of input noise (<xref ref-type="bibr" rid="bib5">Babadi and Sompolinsky, 2014</xref>), implying that an optimal STM/LTM system should employ more robust and generalizable representations in the LTM.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have presented a theory of systems memory consolidation via recall-gated long-term plasticity, which provides complementary benefits to synaptic consolidation mechanisms in terms of memory lifetime and retrieval accuracy. Its advantage arises from the ability to integrate over the information present in an entire neuronal population, rather than individual synapses, in order to decide which memory traces are consolidated. This capability is important in environments that induce a mixture of reliable and unreliable synaptic updates, in which a system must prioritize which updates to store long-term.</p><sec id="s3-1"><title>Experimental evidence for recall-gated consolidation</title><p>The recall-gated consolidation model is by design agnostic to the underlying neural circuit and hence potentially applicable to a wide variety of species and brain regions. Here, we summarize evidence consistent with recall-gated consolidation in several model organisms. As our proposal is new, the experiments we describe were not designed to directly test our model predictions, and thus provide incomplete evidence for them. We hope that future work will more directly clarify the relevance of our model to these systems as well as others, the mechanisms by which it is implemented, and the shortcomings it may have in accounting for experimental results.</p><sec id="s3-1-1"><title>Associative learning in insects</title><p>In the <italic>Drosophila</italic> mushroom body, plasticity is driven by activity of dopamine neurons innervating a collection of anatomically defined compartments. These contain mushroom body output neurons (MBONs) that drive learned behavioral responses, such as approach or avoidance, to sensory stimuli (<xref ref-type="bibr" rid="bib3">Aso et al., 2014</xref>). The compartments are grouped into anatomically defined lobes referred to by Greek letters: <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>,</mml:mo><mml:mi>Î±</mml:mi><mml:mo>,</mml:mo><mml:mi>Î²</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>Î±</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Î²</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. In general the <italic>Î³</italic> lobe compartments are implicated in STM while the <inline-formula><mml:math id="inf74"><mml:semantics><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>/</mml:mo><mml:mi>Î²</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> compartments are implicated in LTM (<xref ref-type="bibr" rid="bib3">Aso et al., 2014</xref>). Mushroom body dopamine neurons receive a wide variety of inputs, including from MBONs themselves (<xref ref-type="bibr" rid="bib37">Li et al., 2020</xref>). Such inputs provide a substrate by which long-term learning can be modulated by the outputs of short-term pathways. To implement recall-gated consolidation, the activity of dopamine neurons modulating plasticity in LTM compartments should be gated by learning in corresponding short-term pathways. A recent study found an instance of this motif (<xref ref-type="bibr" rid="bib4">Awata et al., 2019</xref>). Short-term aversive learning decreases the activity of the <inline-formula><mml:math id="inf75"><mml:semantics><mml:mrow><mml:mi>Î³</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> MBON (implicated in short-term aversive memory). This <inline-formula><mml:math id="inf76"><mml:semantics><mml:mrow><mml:mi>Î³</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> MBON is inhibitory and synapses onto a dopamine neuron innervating the <inline-formula><mml:math id="inf77"><mml:semantics><mml:mrow><mml:mi>Î±</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> compartment (which is associated with long-term aversive learning). Thus, short-term aversive learing in the <inline-formula><mml:math id="inf78"><mml:semantics><mml:mrow><mml:mi>Î³</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> MBON disinhibits the <inline-formula><mml:math id="inf79"><mml:semantics><mml:mrow><mml:mi>Î±</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> dopamine neuron, allowing for learning to proceed in the LTM <inline-formula><mml:math id="inf80"><mml:semantics><mml:mrow><mml:mi>Î±</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> compartment. This circuit is a precise mechanistic implementation of our recall-gated consolidation model. More work is needed to determine if other examples of this motif can be found in <italic>Drosophila</italic> or other insects.</p></sec><sec id="s3-1-2"><title>Motor learning</title><p>Several lines of work have indicated that the neural substrate of motor skills can shift with practice. In songbirds, learned changes to song performance are initially driven by a cortico-basal ganglia circuit called the anterior forebrain pathway (AFB) but eventually are consolidated into the song motor pathway (SMP) and become AFB-independent (<xref ref-type="bibr" rid="bib2">Andalman and Fee, 2009</xref>; <xref ref-type="bibr" rid="bib56">Warren et al., 2011</xref>). Using transient inactivations of LMAN, a region forming part of the AFB, a recent study quantified the degree of AFB-to-SMP consolidation over time and found that it strongly correlated with the birdâs motor performance at the time (<xref ref-type="bibr" rid="bib51">Tachibana et al., 2022</xref>). Although this finding does not establish the mechanism for this phenomena, the behavioral result is consistent with our modelâs prediction that the <italic>rate</italic> of consolidation should increase as learning progresses in the short-term pathway.</p><p>A related motor consolidation process has been observed during motor learning in rats. Experiments have shown that motor cortex disengages from heavily practiced skills (<xref ref-type="bibr" rid="bib35">Kawai et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Hwang et al., 2019</xref>), transferring control at least in part to the basal ganglia (<xref ref-type="bibr" rid="bib15">Dhawale et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Dhawale et al., 2021</xref>), and that the degree of cortical disengagement tracks motor performance, as measured by the variability of learned trajectories (<xref ref-type="bibr" rid="bib31">Hwang et al., 2021</xref>). This finding is broadly consistent with recall-consolidation, with short-term learning being mediated by motor cortex and long-term learning being mediated by basal ganglia. However, we note that unlike in the song learning study referenced above, it neither confirms nor rejects our stronger prediction that the <italic>rate</italic> (rather than overall extent) of motor consolidation increases with learning.</p></sec><sec id="s3-1-3"><title>Spatial learning and hippocampal replay</title><p>Hippocampal replay is thought to be crucial to the consolidation of episodic memories to cortex (<xref ref-type="bibr" rid="bib10">Carr et al., 2011</xref>; <xref ref-type="bibr" rid="bib43">ÃlafsdÃ³ttir et al., 2018</xref>). Replay has many proposed computational functions, such as enabling continual learning (<xref ref-type="bibr" rid="bib54">van de Ven et al., 2020</xref>), or optimizing generalization performance (<xref ref-type="bibr" rid="bib50">Sun et al., 2021</xref>), which are outside the scope of our model. However, under the assumption that replay enables LTM storage in cortex, the recall-gated consolidation model makes predictions about which memories should be replayedânamely, replay should disproportionately emphasize memories that are familiar to the hippocampus. That is, we would predict more frequent replay of events or associations that are frequently encountered than of those that were experienced only once, or unreliably.</p><p>Recent experimental work supports this hypothesis. A recent study found that CA3 axonal projections to CA1, those that respond visual cues associated with a fixed spatial location are recruited more readily in sharp-wave ripple events than those that respond to the randomly presented cues (<xref ref-type="bibr" rid="bib52">Terada et al., 2022</xref>). This observation is consistent with our modelâs prediction that repeatedly experienced patterns of activity are more likely to be consolidated, though other interpretations are possible. Earlier work found that sharp-wave ripple events occur more frequently during maze navigation sessions with regular trajectories, and increase in frequency over the course of session, similar to the behavior of our model in <xref ref-type="fig" rid="fig6">Figure 6B</xref>; <xref ref-type="bibr" rid="bib32">Jackson et al., 2006</xref>. Thus, existing evidence suggests that hippocampal replay is biased toward familiar patterns of activity, consistent with a form recall-gated consolidation. Other experiments provide preliminary evidence for signatures of such a bias in cortical plasticity. For instance, fMRI study of activity in hippocampus and posterior parietal cortex (PPC) during a human virtual navigation experiment found that that the recruitment of PPC during the task, which was linked with memory performance, tended to strengthen with experience in a static environment, but did not strengthen when subjects were exposed to an constantly changing environment, consistent with consolidation of only reliable memories (<xref ref-type="bibr" rid="bib9">Brodt et al., 2016</xref>).</p></sec></sec><sec id="s3-2"><title>Comparison with synaptic consolidation mechanisms</title><p>Recall-gated consolidation improves memory performance regardless of the underlying synapse model (<xref ref-type="fig" rid="fig4">Figure 4</xref>), indicating that its benefits are complementary to those of synaptic consolidation. Our theory quantifies these benefits in terms of the scaling behavior of the modelâs maximum learnable timescale with respect to other parameters. First, for any underlying synapse model, recall-gated consolidation allows the learnable timescale to decay much more slowly as a function of the desired SNR of memory storage. Second, recall-gated consolidation achieves (at worst) linear scaling of learnable timescale as a function of the number of memory reinforcements <italic>R</italic>. For models with a fixed, finite number of internal states per synapse, this scaling is at best logarithmic. Our results therefore illustrate that systems-level consolidation mechanisms allow relatively simple synaptic machinery to support LTM storage. We note that more sophisticated synaptic models, which involve a large number of internal states that scales with the memory timescale of interest (<xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>), can also achieve linear scaling of learnable timescale with <italic>R</italic> (although recall-gated consolidation still improves their performance by a large constant factor). However, for environmental statistics characterized by concentrated bursts of repeated events separated by long gaps, recall-gated consolidation achieves superlinear power-law scaling, which we showed is not achievable by any synapse-local consolidation mechanism.</p><p>Our model provides an explanation for spaced training effects (<xref ref-type="fig" rid="fig5">Figure 5</xref>) based on optimal gating of LTM consolidation depending on the recurrence statistics of reliable stimuli. It is important to note that, depending on the specific form of internal dynamics present in individual synapses, synaptic consolidation models can also reproduce spacing effects. For example, the initial improvement of memory strength with increased spacing arises in the model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref> due to saturation of fast synaptic variables, meaning that the timescale of these internal variables determines optimal spacing, and that intervening stimuli can block the effect by preventing saturation (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In contrast, in our model this timescale is set by population-level forgetting curves, rendering spacing effects robust over long timescales and in the presence of intervening events. It is likely that mechanisms at both the synaptic and systems level contribute to spacing effects; our results suggest that effects observed at longer timescales are likely to arise from memory recall mechanisms at the systems level.</p></sec><sec id="s3-3"><title>Other models of systems consolidation</title><p>Unlike previous theories, our study emphasizes the role of repeated memory reinforcement and selective consolidation. As such, our model has novel capabilities, but also has limitations compared to other models of consolidation. As the key insight of our model is distinct from most other theoretical work on the subject, we believe that future work will be able to fruitfully integrate the notion of recall-gated plasticity into other models of consolidation and attain the benefits of both.</p><p>Much prior work focuses on consolidation via hippocampal replay. Prior work has proposed that replay (or similar mechanisms) can prolong memory lifetimes (<xref ref-type="bibr" rid="bib48">Shaham et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">Remme et al., 2021</xref>), alleviate the problem of catastrophic forgetting of previously learned information (<xref ref-type="bibr" rid="bib54">van de Ven et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">GonzÃ¡lez et al., 2020</xref>; <xref ref-type="bibr" rid="bib48">Shaham et al., 2021</xref>), and facilitate generalization of learned information (<xref ref-type="bibr" rid="bib39">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib50">Sun et al., 2021</xref>). One prior theoretical study (<xref ref-type="bibr" rid="bib47">Roxin and Fusi, 2013</xref>), which uses replayed activity to consolidate synaptic changes from short to long-term modules, explored how systems consolidation extends forgetting curves. Unlike our work, this model (and related work, such as that of <xref ref-type="bibr" rid="bib8">Brea et al., 2023</xref>) essentially models consolidation as âcopyingâ synaptic weights from one system to another. While such a mechanism has potentially useful consequences, such as enabling decoding of the age of a memory (<xref ref-type="bibr" rid="bib8">Brea et al., 2023</xref>), it does not involve gating of memory consolidation, and consequently provides no additional benefit in consolidating repeatedly reinforced memories. Our model is thus distinct from, but also complementary to, these prior studies. In particular, recall-gated consolidation can be implemented in real-time, without replay of old memories. However, as discussed above, selective replay of familiar memories is one possible implementation of recall-gated consolidation. Selective replay is a feature of some of the work cited above (<xref ref-type="bibr" rid="bib48">Shaham et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Sun et al., 2021</xref>), which suggests it can provide advantages for retention and generalization (<xref ref-type="bibr" rid="bib48">Shaham et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Sun et al., 2021</xref>).</p><p>Other work has proposed models of consolidation, particularly in the context of motor learning, in which one module âtutorsâ another to perform learned behaviors by providing it with target outputs (<xref ref-type="bibr" rid="bib41">Murray and Escola, 2017</xref>; <xref ref-type="bibr" rid="bib53">TeÅileanu et al., 2017</xref>). <xref ref-type="bibr" rid="bib42">Murray and Escola, 2020</xref> proposes a fast-learning pathway (which learns using reward or supervision) which tutors the slow-learning long-term module via a Hebbian learning rule. In machine learning, a similar concept has become popular (typically referred to âknowledge distillationâ), in which the outputs of a trained neural network are used to supervise the learning of a second neural network on the same task (<xref ref-type="bibr" rid="bib28">Hinton et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Gou et al., 2021</xref>). Empirically, this procedure is found to improve generalization performance and enable the use of smaller networks. Our model can be interpreted as a form of partial tutoring of the LTM by the STM, as learning in the LTM is partially dictated by outputs of the STM. In this sense, our work provides a theoretical justification for the use of tutoring signals between two neural populations.</p></sec><sec id="s3-4"><title>Limitations and future work</title><p>In addition to motivating new experiments to test the predictions of a recall-gated consolidation model, our work leaves open a number of theoretical questions that future modeling could address. Our theory assumes fixed and random representations of memory traces. Subject to this assumption, we showed that STM benefits from sparser representations than LTM. In realistic scenarios, synaptic updates are likely to be highly structured, and the optimal representations in each module could differ in more sophisticated ways. Moreover, adapting representations onlineâfor instance, in order to decorrelate consolidated memory tracesâmay improve learning performance further. Addressing these questions requires extending our theory to handle memory statistics with nontrivial correlations. Another possibility we left unaddressed is that of more complex interactions between memory modulesâfor instance, reciprocal rather than unidirectional interactionsâor the use of more than two interacting systems.</p><p>Finally, in this work we considered only a limited family of ways in which long-term consolidation may be modulatedânamely, according to threshold-like functions of recall in the short-term pathway. Considering richer relationships between recall and consolidation rate may enable improved memory performance and/or better fits to experimental data. Moreover, in real neural circuits, additional factors besides recall, such as reward or salience, are likely to influence consolidation as well. For instance, a sufficiently salient event should be stored in LTM even if encountered only once. Furthermore, while in our model familiarity drives consolidation, certain forms of novelty may also incentivize consolidation, raising the prospect of a non-monotonic relationship between consolidation probability and familiarity. Unlike our notion of recall, which can be modeled in task-agnostic fashion, the impact of such additional factors on learning likely depends strongly depend on the behavior in question. Our work provides a theoretical framework that will facilitate more detailed models of the rich dynamics of consolidation in specific neural systems of interest.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Theoretical framework</title><p>We consider a population of <italic>N</italic> synapses, indexed by <inline-formula><mml:math id="inf81"><mml:semantics><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> each with a synaptic weight <inline-formula><mml:math id="inf82"><mml:semantics><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mi>â</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. The set of synaptic weights across the population can be denoted by the vector <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The synapses may retain additional information besides strength as well; if each synapse carries <italic>d</italic>-dimensional state information in addition to its strength, the synaptic state can be written as <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">Ë</mml:mo></mml:mrow></mml:mover><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with the scalar synaptic strengths <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> defined as a function of the high-dimensional state <inline-formula><mml:math id="inf86"><mml:semantics><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle></mml:msub></mml:mrow><mml:mo stretchy="true">Ë</mml:mo></mml:mover></mml:mrow></mml:semantics></mml:math></inline-formula>. We define memories as patterns of target synaptic weights, following prior work (<xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>; <xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>). More specifically, we model each memory as a vector <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. By defining memories in this fashion, our analysis can remain agnostic to the network architecture and plasticity rule that give rise to synaptic modifications. We will typically model memories as binary potentiation/depression events for simplicity, but in principle, memories can be continuous valued. Synaptic are updated by memories according to a plasticity rule <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which maps the synaptic state at the time of a memory event to the subsequent synaptic state.</p><p>For theoretical calculations, we assume as in prior work (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>), that the components of each memory <inline-formula><mml:math id="inf89"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> are independent and uncorrelated with those of other memories (although this assumption is violated in our task learning simulations). We also assume for simplicity that memories are mean-centered so that <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> over randomly sampled memories <inline-formula><mml:math id="inf91"><mml:semantics><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>We define the recall strength associated with memory as the overlap <inline-formula><mml:math id="inf92"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. This definition reflects an âideal observerâ perspective, as it requires direct and complete access to the state of the synaptic population. The ideal observer perspective provides an upper bound on the recall performance of a real system, and should be a fairly good approximation assuming that memory readout mechanisms are sophisticated enough. We are particularly interested in the normalized recall strength<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:semantics><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where the expectation is taken over randomly sampled memories <inline-formula><mml:math id="inf93"><mml:semantics><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>. We refer to this quantity as the signal-to-noise ratio (SNR) of memory recall.</p></sec><sec id="s4-2"><title>Synaptic models and plasticity rules</title><p>In this paper, we primarily consider three synaptic models and corresponding plasticity rules, taken from prior work.</p><p>The first and simplest is is a âbinary switchâ model in which synapses take on binary ( Â± 1) values and stochastically activate (resp. inactivate) in response to positive (resp. negative) values of a memory vector with probability <italic>p</italic> (<xref ref-type="bibr" rid="bib1">Amit and Fusi, 1994</xref>). No auxiliary state variables are used in this model.</p><p>The second is the âcascadeâ model of <xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>, in which synapses are modeled as a Markov chain with a finite number 2 k of discrete states with transition probabilities dependent on the kind of memory event (potentiation or depression). Half the states (states <inline-formula><mml:math id="inf94"><mml:semantics><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) are considered potentiated (strength +1) and half (states <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are depressed (strength â1). Intuitively, states of the same potentiation level correspond to different propensities for plasticity in the synapse, enabling a form of synaptic consolidation. Formally, for <italic>i</italic>&lt;<italic>k</italic>, the potentiated state <italic>a<sub>i</sub></italic> (resp. depressed state <italic>b<sub>i</sub></italic>) transitions to state <inline-formula><mml:math id="inf96"><mml:semantics><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (resp. <inline-formula><mml:math id="inf97"><mml:semantics><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>) with probability <inline-formula><mml:math id="inf98"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>Î±</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î±</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula> following a potentiation (resp. depression) event. And for <italic>i</italic>&lt;<italic>k</italic>, the potentiated state <italic>a<sub>i</sub></italic> (resp. depressed state <italic>b<sub>i</sub></italic>) transitions to state <italic>b</italic><sub>1</sub> (resp. <italic>a</italic><sub>1</sub>) with probability <inline-formula><mml:math id="inf99"><mml:semantics><mml:mrow><mml:msup><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> following a depression (resp. potentiation) event. For <italic>i</italic>=<italic>k</italic> this latter transition occurs with probability <inline-formula><mml:math id="inf100"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î±</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>; as described in <xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>, this choice is made for convenience to ensure equal occupancy of the different synaptic states. We assume <inline-formula><mml:math id="inf101"><mml:semantics><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> throughout.</p><p>The third synaptic model is the model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>, which we refer to as the âmultivariableâ model. In this model, synapses are described by a chain of <italic>m</italic> interacting continuous-valued variables <inline-formula><mml:math id="inf102"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, the first of which corresponds to synaptic strength. Potentiation and depression events increment or decrement the value of the first synaptic variable, and a set of difference equations governs the evolution of the multidimensional state at each time step:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>Î±</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>Î±</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <italic>n</italic> and <italic>Î±</italic> a parameter of the model (we assume <inline-formula><mml:math id="inf103"><mml:semantics><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> throughout). This model also provides the ability for synapses to store information at different timescales, due to the information retained in auxiliary variables.</p></sec><sec id="s4-3"><title>Model implementation for example tasks</title><sec id="s4-3-1"><title>Supervised Hebbian learning</title><p>We simulated a single-layer feedforward network with a population of N=1,000 input neurons (activity denoted by <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and a single output neuron, (activity denoted by <inline-formula><mml:math id="inf105"><mml:semantics><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:semantics></mml:math></inline-formula>), connected with a 1ÃN binary weight matrix <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In each simulation, a set of <italic>P</italic>=20 reliable stimuli were randomly generated, which corresponded to binary (<inline-formula><mml:math id="inf108"><mml:semantics><mml:mrow><mml:mo>Â±</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>) random <italic>N</italic>-dimensional activity patterns in the input neurons. Note that due to the scaling of the inputs and use of binary synaptic weights, the activity <inline-formula><mml:math id="inf109"><mml:semantics><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:semantics></mml:math></inline-formula> is constrained to lie in the interval <inline-formula><mml:math id="inf110"><mml:semantics><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>. Each reliable stimulus was associated with a randomly chosen (but consistent across the simulation) label <italic>y</italic>, 1 or â1. At each time step, one of the reliable stimuli (along with its label) was presented to the network with probability <inline-formula><mml:math id="inf111"><mml:semantics><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for all <inline-formula><mml:math id="inf112"><mml:semantics><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. Otherwise (with probability <inline-formula><mml:math id="inf113"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>â</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>), a randomly sampled unreliable stimulus was presented with a randomly chosen label. Memory vectors (written as matrices since the synaptic weights are interpreted as a matrix) were given by a Hebbian learning rule <inline-formula><mml:math id="inf114"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, corresponding to the product of the binary input neuron activity and the corresponding label. Learning followed the binary switch rule with <italic>p</italic>=0.1; that is, positive entries in the memory vector resulted in potentiation with probability <italic>p</italic>, and likewise for negative entries and depression. At each timestep, the product of the STM output and the Â±1 label was computed, and if it exceeded the consolidation threshold <inline-formula><mml:math id="inf115"><mml:semantics><mml:mrow><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:mn>0.125</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>, plasticity was permitted in the LTM network.</p></sec><sec id="s4-3-2"><title>Reinforcement learning</title><p>We used the same setup as in the supervised learning task, with the following modifications. Instead of a single readout, the network had <italic>A</italic>=3 output neurons corresponding to different possible actions. The activity of the output neuron <italic>a</italic> (denoted by <italic>Ï<sub>a</sub></italic>) represented the unnormalized log probabiliy of taking action <italic>a</italic>: <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î²</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î²</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>Î²</italic> is a parameter controlling the stochasticity of the action selection (we set <italic>Î²</italic>=10 in our simulations). For the purposes of learning, the STM outputs were used to compute action probabilities, but both the STM and the LTM were evaluated throughout training. Each of the 5 reliable stimuli was associated with a correct action. Taking the correct action yielded a reward of 1, while taking the other action yielded a reward of 0. Unreliable stimuli were associated with randomly sampled correct actions. Memory vectors were derived from the following three-factor learning rule: <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>reward</mml:mtext><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a one-hot vector with a value of 1 at the index of the chosen action. At each timestep the product <inline-formula><mml:math id="inf119"><mml:semantics><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mtext>reward</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> was computed, and if it exceeded the consolidation threshold, plasticity was permitted in the LTM network. All other parameters were the same as in the supervised learning simulation.</p></sec><sec id="s4-3-3"><title>Unsupervised Hebbian learning</title><p>We simulated two recurrent neural networks with N=1,000 binary neurons each and with binary recurrent weight matrices <inline-formula><mml:math id="inf120"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, respectively. Memories consisted of binary (entries equal to <inline-formula><mml:math id="inf122"><mml:semantics><mml:mrow><mml:mo>Â±</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>) random <italic>N</italic>-dimensional vectors that provided direct input <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to the network neurons at each timestep. The network state <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> evolved for <italic>T</italic>=5 timesteps according to the following dynamics equation:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>h</mml:mi></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>h</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf125"><mml:semantics><mml:mi>Ï</mml:mi></mml:semantics></mml:math></inline-formula> is a binary threshold nonlinearity with threshold set so that 50% of neurons were active at each time step (corresponding to a mechanism that normalizes activity across the network). The weights <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of the network were binary and initialized as binary random variables with equal on/off probability. On each trial a stimulus was presented, which with probability <inline-formula><mml:math id="inf127"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> was a (randomly sampled but consistent) reliable stimulus, and otherwise was a newly randomly sampled unreliable stimulus. The network weights <italic>W<sub>ij</sub></italic> were subjected to potentiation events when <italic>x<sub>i</sub></italic> and <italic>x<sub>j</sub></italic> were both active at <italic>t</italic>=0, and otherwise subjected to depression events. Synaptic updates followed the binary switch rule with probability <italic>P</italic>=1.0.</p><p>Additionally, a set of <italic>N</italic> weights <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> connected the STM neurons to a single readout neuron that measured familiarity. These weights were also binary and updated according to their own memory vector <inline-formula><mml:math id="inf129"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. They experienced candidate potentiation/depression events when their corresponding stimulus input neuron was active/inactive, respectively (i.e. the memory entry <inline-formula><mml:math id="inf130"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> was equal to <italic>x<sub>i</sub></italic>). These weights followed the binary switch rule with probability p=1.0.</p><p>Plasticity in the LTM proceeded according to the same rule as in the STM but was gated by recall strength <inline-formula><mml:math id="inf131"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, according to a threshold function with threshold equal to 0.25.</p><p>The performance of the network was determined by presenting noise-corrupted versions of the single reliable stimulus and measuring the correlation between the network state and the uncorrupted memory after <italic>T</italic>=5 time steps. The corrupted patterns were obtained by adding Gaussian noise of variance <inline-formula><mml:math id="inf132"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> to the ground-truth pattern, and binarizing the result by choosing the fraction 0.5 of neurons with the highest values to be active.</p></sec></sec><sec id="s4-4"><title>Forgetting curves for different synaptic plasticity rules</title><p>Prior work (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>) has considered environments in which a given memory is presented to the system only once. In this case, the performance of a single population of synapses with a given plasticity rule depends crucially on the memory trace function <italic>m</italic>(<italic>t</italic>). This is defined as<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:semantics><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>the recall SNR at time <italic>t</italic> for a memory <inline-formula><mml:math id="inf133"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> presented at <italic>t</italic>=0, assuming randomly sampled memories have been presented in the intervening timesteps. For the binary switch model, <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mi>p</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. More sophisticated synaptic models, like the cascade and multivariable models, can achieve power-law scalings (<xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>). The key feature of these models that enables power-law forgetting is that their synapses maintain additional information besides their weight, which encodes their propensity to change state. In this fashion, memories can be consolidated at the synaptic level into more stable, slowly decaying traces. The cascade model of Fusi et al. achieves<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>for some characteristic timescale <italic>T</italic> which can be chosen as a model parameter. Hence, its performance is upper bounded by<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The model of Benna and Fusi can achieve<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:msqrt><mml:mfrac><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:msqrt><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>which is upper bounded by<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msqrt><mml:mfrac><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Benna and Fusi also show that <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> scaling is an upper bound on the performance of any synapse model with finite dynamic range.</p></sec><sec id="s4-5"><title>Implementation of SNR and learnable timescale computations</title><p>To compute recall strengths associated with single synaptic populations, we first sampled interarrival intervals <italic>I</italic> from the environmental statistics <italic>p(I)</italic>. Given a number of repetitions <italic>R</italic>, we computed recall strength samples <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>m</italic> is the forgetting curve associated with the underlying synaptic plasticity rule, <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>â¥</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the <italic>I</italic><sub><italic>j</italic></sub> are independent samples from <italic>p(I)</italic>. We scaled recall strengths by a factor of <inline-formula><mml:math id="inf138"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula> to compute the recall SNR (an approximation that is exact in the large-<italic>N</italic> limit).</p><p>To compute recall strengths associated with the recall-gated consolidation model, we repeated the above procedure using a new interarrival distribution <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> induced by the gating model. The induced distribution <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained by drawing as samples the lengths of intervals between consecutive interarrival interval samples for which the corresponding recall SNR in the STM exceeds the gating threshold <italic>Î¸</italic> (corresponding to the interval between consolidated reliable memory presentations), and rescaling it by the fraction of unreliable memories that are consolidated. Strictly speaking, in the general case this distribution is nonstationary, as the probability of STM recall exceeding the threshold can change as synaptic updates accumulate across repetitions for sophisticated synapse models like that of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>. We adopt a conservative approximation that ignores such effects and thus slightly underestimates the rate of consolidation when such synaptic models are used (and consequently underestimates the SNR and learnable timescale of the recall-gated consolidation model). With this approximation, the random variable <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined as as the following mixture distribution<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>.</mml:mo><mml:mtext>Â </mml:mtext></mml:mrow><mml:mi>q</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mn>1</mml:mn><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>Î¸</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where each <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>Î¶</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <italic>q</italic> indicates the probability of a reliable memory presentation inducing consolidation, and <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes an indicator function, equal to 1 or 0 depending on whether the condition is met. The value of <italic>j</italic> corresponds to the number of reinforcements that go by between instances of consolidation. For sufficiently large <italic>Ï</italic> this distribution can be approximated by<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mi>q</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf144"><mml:semantics><mml:mi>Î¦</mml:mi></mml:semantics></mml:math></inline-formula> is the CDF of the standard normal distribution. For large <italic>N</italic>, the probability of consolidation <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We note that for an exponential interarrival distribution with mean <italic>Ï</italic>, the induced distribution of <inline-formula><mml:math id="inf146"><mml:semantics><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is also exponential, with mean <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is because the sums of <italic>j</italic> independent samples <italic>I<sub>i</sub></italic> are distributed according to a Gamma distribution with shape parameter <italic>j</italic>, and the mixture of such Gamma distributions with geometrically distributed mixture weights <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>q</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is itself an exponential distribution with mean <inline-formula><mml:math id="inf149"><mml:semantics><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>/</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>For a given number of expected memory repetitions <italic>R</italic>, the gating threshold <italic>Î¸</italic> was set such that at least one of the <italic>R</italic> repetitions would be consolidated with probability <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ïµ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>Ïµ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Where <italic>R</italic> is not reported, we assume it equal to 2, the minimum number of repetitions for the notion of consolidation to be meaningful in our model.</p><p>To compute learnable timescales, we repeated the above SNR computations over a range of mean interarrival times <inline-formula><mml:math id="inf151"><mml:semantics><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mi>I</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula>, keeping the interarrival distribution family (Weibull distributions with a fixed value of <italic>k</italic>, see below) constant. We report the maximum value of <italic>Ï</italic> for which the SNR exceeds the designated target threshold with probability <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ïµ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>Ïµ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Throughout, for our interarrival distributions we use Weibull distributions with regularity parameter <italic>k</italic>. The corresponding cumulative distribution function is<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>Î</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and Î is the Gamma function. In the case <italic>k</italic>=1, this reduces to an exponential distribution of interarrival intervals, which corresponds to memory reinforcements that occur according to a Poisson process with rate <inline-formula><mml:math id="inf154"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. In the limit <inline-formula><mml:math id="inf155"><mml:semantics><mml:mrow><mml:mi>k</mml:mi><mml:mo>â</mml:mo><mml:mi>â</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, it corresponds to interarrival intervals of deterministic length <italic>Ï</italic>. For <italic>k</italic>&lt;1, the interarrival distribution is âburstyâ, with periods of dense reinforcement separated by long gaps.</p></sec><sec id="s4-6"><title>Spacing effect simulations</title><p>We simulated the multivariable synapse model of <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>, in which each synapse is described by <italic>m</italic> continuous-valued dynamical variables <inline-formula><mml:math id="inf156"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> which evolve as follows:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>Î±</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>Î±</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>For the first variable <italic>u</italic><sub>1</sub>, in place of <inline-formula><mml:math id="inf157"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> we substitute components <italic>m<sub>j</sub></italic> of the memory traces. For the last variable <italic>u<sub>m</sub></italic>, in place of <inline-formula><mml:math id="inf158"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> we substitute 0. The strength of each synapse corresponds to the value of its first dynamical variable. For our simulations, we chose <italic>m</italic>=10 dynamical variables, n=2, <inline-formula><mml:math id="inf159"><mml:semantics><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>, and N=400 synapses. The value of <italic>Î±</italic> is also varied in <xref ref-type="fig" rid="fig5s3">Figure 5âfigure supplement 3</xref>. A spacing interval Î was selected and a randomly drawn reliable memory was presented at Î-length intervals (the same pattern at each presentation). In the case without intervening memories, the dynamics of each synapse ran unimpeded between these presentations. In the case with intervening memories, new randomly drawn patterns were presented to the system at each timestep between the reliable memory presentations. Each pattern was drawn with values equal to Â± 1/2, with equal probability.</p></sec><sec id="s4-7"><title>Generalized model with multiple memory timescales</title><p>In our generalized environment model, the environment contains a variety of distinct reliable memories <italic>x<sub>i</sub></italic> which recur with Poisson statistics at a variety of rates <italic>Î»<sub>i</sub></italic>. Timescales <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are distributed as <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â¼</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <italic>A</italic> is a large constant. This corresponds to the value of <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> being uniformly distributed in <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, or equivalently to <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â¼</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and bounded between <inline-formula><mml:math id="inf165"><mml:semantics><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> and 1. The environment also contains an additional fraction of unreliable memories as before, sampled randomly and presented with a fixed probability at each timestep. The natural generalization of learnable timescale to this setting is the maximum interarrival interval timescale for which the lifetime of a corresponding memory (the time following last reinforcement its recall strength decays to an SNR below the target SNR) exceeds that timescale.</p><p>The distribution of interarrival intervals for memory <italic>i</italic> is<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Integrating across the distribution of <italic>Î»</italic>, we get the distribution of interarrival intervals for reliable memories observed by the system:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>reliable</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>Î»</mml:mi><mml:mo>â¼</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>Î»</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>â</mml:mo><mml:msubsup><mml:mo>â«</mml:mo><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Î»</mml:mi></mml:mfrac><mml:mi>Î»</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>Î»</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>â</mml:mo><mml:msubsup><mml:mo>â«</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>Î»</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>â</mml:mo><mml:msubsup><mml:mo>â«</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>Î»</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>I</mml:mi></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>I</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>for large <italic>I</italic>.</p><p>The full distribution of interval strengths (including unreliable memories) is a mixture of <inline-formula><mml:math id="inf166"><mml:semantics><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>reliable</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and a delta function at <inline-formula><mml:math id="inf167"><mml:semantics><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mi>â</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, with the latterâs weight corresponding to the probability with which an unreliable memory is sampled at a given timestep (in our simulations we chose 0.9).</p><p>From here we can compute a distribution of STM recall strengths <italic>r</italic><disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>â«</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>I</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>We simulated a model in which an ensemble of LTM subpopulations are assigned gating functions <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> equal 1 for <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and 0 elsewhere, with the <italic>A<sub>i</sub></italic> spaced evenly over <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The expected lifetime of a memory reinforced with a given interval <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is given approximately by the STM lifetime divided by the fraction of memory presentations for which the recall strength lies in the same interval as <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This quantity reflects the proportion of memories presentations that are consolidated into the same LTM subpopulation as the memory in question.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-90793-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a theoretical study, so no data have been generated for this manuscript.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Stefano Fusi and Samuel Muscinelli for helpful discussions and comments on the manuscript. ALK and JL were supported by the Gatsby Charitable Foundation, NSF award DBI-1707398. JL was also supported by the DOE CSGF (DE-SC0020347). ALK was also supported by the Burroughs Wellcome Foundation, the McKnight Endowment Fund, the Mathers Foundation, and NIH award R01EB029858.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Learning in neural networks with material synapses</article-title><source>Neural Computation</source><volume>6</volume><fpage>957</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1162/neco.1994.6.5.957</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andalman</surname><given-names>AS</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A basal ganglia-forebrain circuit in the songbird biases motor output to avoid vocal errors</article-title><source>PNAS</source><volume>106</volume><fpage>12518</fpage><lpage>12523</lpage><pub-id pub-id-type="doi">10.1073/pnas.0903214106</pub-id><pub-id pub-id-type="pmid">19597157</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Hattori</surname><given-names>D</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Johnston</surname><given-names>RM</given-names></name><name><surname>Iyer</surname><given-names>NA</given-names></name><name><surname>Ngo</surname><given-names>TTB</given-names></name><name><surname>Dionne</surname><given-names>H</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Tanimoto</surname><given-names>H</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neuronal architecture of the mushroom body provides a logic for associative learning</article-title><source>eLife</source><volume>3</volume><elocation-id>e04577</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04577</pub-id><pub-id pub-id-type="pmid">25535793</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Awata</surname><given-names>H</given-names></name><name><surname>Takakura</surname><given-names>M</given-names></name><name><surname>Kimura</surname><given-names>Y</given-names></name><name><surname>Iwata</surname><given-names>I</given-names></name><name><surname>Masuda</surname><given-names>T</given-names></name><name><surname>Hirano</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural circuit linking mushroom body parallel circuits induces memory consolidation in <italic>Drosophila</italic></article-title><source>PNAS</source><volume>116</volume><fpage>16080</fpage><lpage>16085</lpage><pub-id pub-id-type="doi">10.1073/pnas.1901292116</pub-id><pub-id pub-id-type="pmid">31337675</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparseness and expansion in sensory representations</article-title><source>Neuron</source><volume>83</volume><fpage>1213</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id><pub-id pub-id-type="pmid">25155954</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>CD</given-names></name><name><surname>Schroeder</surname><given-names>B</given-names></name><name><surname>Davis</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Learning performance of normal and mutant <italic>Drosophila</italic> after repeated conditioning trials with discrete stimuli</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>2944</fpage><lpage>2953</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-08-02944.2000</pub-id><pub-id pub-id-type="pmid">10751447</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benna</surname><given-names>MK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational principles of synaptic memory consolidation</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1038/nn.4401</pub-id><pub-id pub-id-type="pmid">27694992</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brea</surname><given-names>J</given-names></name><name><surname>Clayton</surname><given-names>NS</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Computational models of episodic-like memory in food-caching birds</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>2979</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-38570-x</pub-id><pub-id pub-id-type="pmid">37221167</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodt</surname><given-names>S</given-names></name><name><surname>PÃ¶hlchen</surname><given-names>D</given-names></name><name><surname>Flanagin</surname><given-names>VL</given-names></name><name><surname>Glasauer</surname><given-names>S</given-names></name><name><surname>Gais</surname><given-names>S</given-names></name><name><surname>SchÃ¶nauer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rapid and independent memory formation in the parietal cortex</article-title><source>PNAS</source><volume>113</volume><fpage>13251</fpage><lpage>13256</lpage><pub-id pub-id-type="doi">10.1073/pnas.1605719113</pub-id><pub-id pub-id-type="pmid">27803331</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname><given-names>MF</given-names></name><name><surname>Jadhav</surname><given-names>SP</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>147</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1038/nn.2732</pub-id><pub-id pub-id-type="pmid">21270783</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cepeda</surname><given-names>NJ</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name><name><surname>Vul</surname><given-names>E</given-names></name><name><surname>Wixted</surname><given-names>JT</given-names></name><name><surname>Rohrer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Distributed practice in verbal recall tasks: A review and quantitative synthesis</article-title><source>Psychological Bulletin</source><volume>132</volume><fpage>354</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.132.3.354</pub-id><pub-id pub-id-type="pmid">16719566</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cepeda</surname><given-names>NJ</given-names></name><name><surname>Vul</surname><given-names>E</given-names></name><name><surname>Rohrer</surname><given-names>D</given-names></name><name><surname>Wixted</surname><given-names>JT</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spacing effects in learning: A temporal ridgeline of optimal retention</article-title><source>Psychological Science</source><volume>19</volume><fpage>1095</fpage><lpage>1102</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02209.x</pub-id><pub-id pub-id-type="pmid">19076480</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cervantes-Sandoval</surname><given-names>I</given-names></name><name><surname>Martin-PeÃ±a</surname><given-names>A</given-names></name><name><surname>Berry</surname><given-names>JA</given-names></name><name><surname>Davis</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>System-like consolidation of olfactory memories in <italic>Drosophila</italic></article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>9846</fpage><lpage>9854</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0451-13.2013</pub-id><pub-id pub-id-type="pmid">23739981</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Traces of <italic>Drosophila</italic> memory</article-title><source>Neuron</source><volume>70</volume><fpage>8</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.012</pub-id><pub-id pub-id-type="pmid">21482352</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Wolff</surname><given-names>SBE</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Ãlveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Basal Ganglia Can Control Learned Motor Sequences Independently of Motor Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/827261</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Wolff</surname><given-names>SBE</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Ãlveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The basal ganglia control the detailed kinematics of learned motor skills</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1256</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00889-3</pub-id><pub-id pub-id-type="pmid">34267392</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubnau</surname><given-names>J</given-names></name><name><surname>Chiang</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Systems memory consolidation in <italic>Drosophila</italic></article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>84</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.09.006</pub-id><pub-id pub-id-type="pmid">23084099</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>PW</given-names></name><name><surname>Bontempi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The organization of recent and remote memories</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1038/nrn1607</pub-id><pub-id pub-id-type="pmid">15685217</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FrÃ©maux</surname><given-names>N</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules</article-title><source>Frontiers in Neural Circuits</source><volume>9</volume><elocation-id>85</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2015.00085</pub-id><pub-id pub-id-type="pmid">26834568</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Drew</surname><given-names>PJ</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cascade models of synaptically stored memories</article-title><source>Neuron</source><volume>45</volume><fpage>599</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.02.001</pub-id><pub-id pub-id-type="pmid">15721245</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The space of interactions in neural network models</article-title><source>Journal of Physics A</source><volume>21</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1088/0305-4470/21/1/030</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glas</surname><given-names>A</given-names></name><name><surname>HÃ¼bener</surname><given-names>M</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Goltstein</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spaced training enhances memory and prefrontal ensemble stability in mice</article-title><source>Current Biology</source><volume>31</volume><fpage>4052</fpage><lpage>4061</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.085</pub-id><pub-id pub-id-type="pmid">34324833</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluck</surname><given-names>MA</given-names></name><name><surname>Myers</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Hippocampal mediation of stimulus representation: A computational theory</article-title><source>Hippocampus</source><volume>3</volume><fpage>491</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1002/hipo.450030410</pub-id><pub-id pub-id-type="pmid">8269040</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>GonzÃ¡lez</surname><given-names>OC</given-names></name><name><surname>Sokolov</surname><given-names>Y</given-names></name><name><surname>Krishnan</surname><given-names>GP</given-names></name><name><surname>Delanois</surname><given-names>JE</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Can sleep protect memories from catastrophic forgetting?</article-title><source>eLife</source><volume>9</volume><elocation-id>e51005</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51005</pub-id><pub-id pub-id-type="pmid">32748786</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gorriz</surname><given-names>MH</given-names></name><name><surname>Takigawa</surname><given-names>M</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The Role of Experience in Prioritizing Hippocampal Replay</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.28.534589</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gou</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Maybank</surname><given-names>SJ</given-names></name><name><surname>Tao</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Knowledge distillation: A survey</article-title><source>International Journal of Computer Vision</source><volume>129</volume><fpage>1789</fpage><lpage>1819</lpage><pub-id pub-id-type="doi">10.1007/s11263-021-01453-z</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><source>The Organization of Behavior: A Neuropsychological Theory</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distilling the Knowledge in a Neural Network</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>EJ</given-names></name><name><surname>Dahlen</surname><given-names>JE</given-names></name><name><surname>Hu</surname><given-names>YY</given-names></name><name><surname>Aguilar</surname><given-names>K</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Mukundan</surname><given-names>M</given-names></name><name><surname>Mitani</surname><given-names>A</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Disengagement of motor cortex from movement control during long-term learning</article-title><source>Science Advances</source><volume>5</volume><elocation-id>eaay0001</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aay0001</pub-id><pub-id pub-id-type="pmid">31693007</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>EJ</given-names></name><name><surname>Dahlen</surname><given-names>JE</given-names></name><name><surname>Mukundan</surname><given-names>M</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Disengagement of motor cortex during long-term learning tracks the performance level of learned movements</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>7029</fpage><lpage>7047</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3049-20.2021</pub-id><pub-id pub-id-type="pmid">34244359</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>JC</given-names></name><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Hippocampal sharp waves and reactivation during awake states depend on repeated sequential experience</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>12415</fpage><lpage>12426</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4118-06.2006</pub-id><pub-id pub-id-type="pmid">17135403</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joel</surname><given-names>D</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Ruppin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Actor-critic models of the basal ganglia: new anatomical and computational perspectives</article-title><source>Neural Networks</source><volume>15</volume><fpage>535</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(02)00047-3</pub-id><pub-id pub-id-type="pmid">12371510</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kandel</surname><given-names>ER</given-names></name><name><surname>Dudai</surname><given-names>Y</given-names></name><name><surname>Mayford</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The molecular and systems biology of memory</article-title><source>Cell</source><volume>157</volume><fpage>163</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.03.001</pub-id><pub-id pub-id-type="pmid">24679534</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawai</surname><given-names>R</given-names></name><name><surname>Markman</surname><given-names>T</given-names></name><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Fantana</surname><given-names>AL</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Ãlveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Motor cortex is required for learning but not for executing a motor skill</article-title><source>Neuron</source><volume>86</volume><fpage>800</fpage><lpage>812</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.024</pub-id><pub-id pub-id-type="pmid">25892304</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lahiri</surname><given-names>S</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A memory frontier for complex synapses</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1034</fpage><lpage>1042</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Lindsey</surname><given-names>JW</given-names></name><name><surname>Marin</surname><given-names>EC</given-names></name><name><surname>Otto</surname><given-names>N</given-names></name><name><surname>Dreher</surname><given-names>M</given-names></name><name><surname>Dempsey</surname><given-names>G</given-names></name><name><surname>Stark</surname><given-names>I</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Pleijzier</surname><given-names>MW</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Takemura</surname><given-names>S-Y</given-names></name><name><surname>Eckstein</surname><given-names>N</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Francis</surname><given-names>A</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Parekh</surname><given-names>R</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Waddell</surname><given-names>S</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The connectome of the adult <italic>Drosophila</italic> mushroom body provides insights into function</article-title><source>eLife</source><volume>9</volume><elocation-id>e62576</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.62576</pub-id><pub-id pub-id-type="pmid">33315010</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Dynamics of Striatal Action Selection and Reinforcement Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.02.14.580408</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>OâReilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Goddard</surname><given-names>NH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Considerations arising from a complementary learning systems perspective on hippocampus and neocortex</article-title><source>Hippocampus</source><volume>6</volume><fpage>654</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1996)6:6&lt;654::AID-HIPO8&gt;3.0.CO;2-G</pub-id><pub-id pub-id-type="pmid">9034852</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JM</given-names></name><name><surname>Escola</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning multiple variable-speed sequences in striatum via cortical tutoring</article-title><source>eLife</source><volume>6</volume><elocation-id>e26084</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26084</pub-id><pub-id pub-id-type="pmid">28481200</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JM</given-names></name><name><surname>Escola</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Remembrance of things practiced with fast and slow learning in cortical and subcortical pathways</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>6441</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-19788-5</pub-id><pub-id pub-id-type="pmid">33361766</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ÃlafsdÃ³ttir</surname><given-names>HF</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The role of hippocampal replay in memory andplanning</article-title><source>Current Biology</source><volume>28</volume><fpage>R37</fpage><lpage>R50</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.073</pub-id><pub-id pub-id-type="pmid">29316421</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remme</surname><given-names>MWH</given-names></name><name><surname>Bergmann</surname><given-names>U</given-names></name><name><surname>Alevi</surname><given-names>D</given-names></name><name><surname>Schreiber</surname><given-names>S</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Kempter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hebbian plasticity in parallel synaptic pathways: A circuit mechanism for systems memory consolidation</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009681</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009681</pub-id><pub-id pub-id-type="pmid">34874938</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenzweig</surname><given-names>MR</given-names></name><name><surname>Bennett</surname><given-names>EL</given-names></name><name><surname>Colombo</surname><given-names>PJ</given-names></name><name><surname>Lee</surname><given-names>DW</given-names></name><name><surname>Serrano</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Short-term, intermediate-term, and long-term memories</article-title><source>Behavioural Brain Research</source><volume>57</volume><fpage>193</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(93)90135-d</pub-id><pub-id pub-id-type="pmid">8117424</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rovee-Collier</surname><given-names>C</given-names></name><name><surname>Evancio</surname><given-names>S</given-names></name><name><surname>Earley</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The time window hypothesis: Spacing effects</article-title><source>Infant Behavior and Development</source><volume>18</volume><fpage>69</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/0163-6383(95)90008-X</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roxin</surname><given-names>A</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Efficient partitioning of memory systems and its importance for memory consolidation</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003146</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003146</pub-id><pub-id pub-id-type="pmid">23935470</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shaham</surname><given-names>N</given-names></name><name><surname>Chandra</surname><given-names>J</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Stochastic Consolidation of Lifelong Memory</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.08.24.457446</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squire</surname><given-names>LR</given-names></name><name><surname>Alvarez</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Retrograde amnesia and memory consolidation: a neurobiological perspective</article-title><source>Current Opinion in Neurobiology</source><volume>5</volume><fpage>169</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(95)80023-9</pub-id><pub-id pub-id-type="pmid">7620304</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Advani</surname><given-names>M</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Organizing Memories for Generalization in Complementary Learning Systems</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.10.13.463791</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tachibana</surname><given-names>RO</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Kai</surname><given-names>K</given-names></name><name><surname>Kojima</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Performance-dependent consolidation of learned vocal changes in adult songbirds</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>1974</fpage><lpage>1986</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1942-21.2021</pub-id><pub-id pub-id-type="pmid">35058370</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terada</surname><given-names>S</given-names></name><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>OâHare</surname><given-names>J</given-names></name><name><surname>Vancura</surname><given-names>B</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Adaptive stimulus selection for consolidation in the hippocampus</article-title><source>Nature</source><volume>601</volume><fpage>240</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04118-6</pub-id><pub-id pub-id-type="pmid">34880499</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>TeÅileanu</surname><given-names>T</given-names></name><name><surname>Ãlveczky</surname><given-names>B</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rules and mechanisms for efficient two-stage learning in neural circuits</article-title><source>eLife</source><volume>6</volume><elocation-id>e20944</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.20944</pub-id><pub-id pub-id-type="pmid">28374674</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>GM</given-names></name><name><surname>Siegelmann</surname><given-names>HT</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Brain-inspired replay for continual learning with artificial neural networks</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4069</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17866-2</pub-id><pub-id pub-id-type="pmid">32792531</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verkoeijen</surname><given-names>PPJL</given-names></name><name><surname>Rikers</surname><given-names>RMJP</given-names></name><name><surname>Schmidt</surname><given-names>HG</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Limitations to the spacing effect: demonstration of an inverted u-shaped relationship between interrepetition spacing and free recall</article-title><source>Experimental Psychology</source><volume>52</volume><fpage>257</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1027/1618-3169.52.4.257</pub-id><pub-id pub-id-type="pmid">16302534</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>TL</given-names></name><name><surname>Tumer</surname><given-names>EC</given-names></name><name><surname>Charlesworth</surname><given-names>JD</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mechanisms and time course of vocal learning and consolidation in the adult songbird</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>1806</fpage><lpage>1821</lpage><pub-id pub-id-type="doi">10.1152/jn.00311.2011</pub-id><pub-id pub-id-type="pmid">21734110</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Derivation of recall strength quantity for specific plasticity rules</title><p>In the following derivations, we derive the recall factors for various learning rules, which correspond to different choices of <inline-formula><mml:math id="inf173"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. The recall factor is defined as the elementwise dot product between <bold>W</bold> and <inline-formula><mml:math id="inf174"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, which we denote as <inline-formula><mml:math id="inf175"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. We make use of the fact that this elementwise dot product between the matrices is equal to <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><sec sec-type="appendix" id="s8-1"><title>Supervised Hebbian learning</title><p>Let <bold>x</bold> be the input population activity, <bold>W</bold> be the prediction weights, <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the output population activity (predicted probabilities), and <bold>y</bold> indicate ground-truth target values. A supervised Hebbian plasticity rule gives rise to a memory vector (interpreted here as a matrix since the synaptic weights form a matrix) <inline-formula><mml:math id="inf178"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, and thus the recall strength <inline-formula><mml:math id="inf179"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> can be written as<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>corresponding to the accuracy of the prediction <inline-formula><mml:math id="inf180"><mml:semantics><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:semantics></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s8-2"><title>Reinforcement learning</title><p>Let <bold>x</bold> be the input population activity representing state information, <bold>W</bold> be the output weights, and <inline-formula><mml:math id="inf181"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Ï</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> be the output population activity representing unnormalized log probabilities of taking different actions <inline-formula><mml:math id="inf182"><mml:semantics><mml:mrow><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î²</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î²</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>Î²</italic> is a parameter controlling the stochasticity of the action selection. Let <bold>a</bold> be a one-hot vector indicating the sampled action, and <inline-formula><mml:math id="inf184"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>Â±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> be the scalar reward that results. For a reinforcement learning rule giving rise to a memory vector <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>reward</mml:mtext><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the recall strength <inline-formula><mml:math id="inf186"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> can be written as<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Following the same steps as the derivation for the supervised learning case, with <bold>a</bold> in place of <bold>y</bold>, gives<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î²</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>corresponding to the unnormalized log probability with which the action <italic>a</italic> was selected (which can be interpreted as the confidence in the selection), modulated by reward.</p><p>Computing this factor requires preserving the networkâs action probability distribution, extracting from it the probability of the sampled action, and multiplicatively scaling the result by the obtained reward.</p></sec><sec sec-type="appendix" id="s8-3"><title>Autoassociative memory</title><p>Let <bold>x</bold> be the population activity and <bold>W</bold> be the recurrent weight matrix. For an autoassociative memory storage rule with memory vector <inline-formula><mml:math id="inf187"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, assuming that the weight matrix <bold>W</bold> can be approximated as a sum <inline-formula><mml:math id="inf188"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>â</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> over prior plasticity-driven updates, then the recall strength <inline-formula><mml:math id="inf189"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>â</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> can be written as<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>corresponding the familiarity of the current pattern <bold>x</bold> relative to all previously seen patterns <inline-formula><mml:math id="inf190"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>Familiarity also be computed with a separate familiarity readout trained alongside the recurrent weights. If the familiarity readout employs a Hebbian rule, the resulting estimate of familiarity will be equal to<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>â</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>For uncorrelated patterns in a network below capacity, this strategy corresponds exactly to the true recall factor in the limit of large network size.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Relationship between learnable timescale and capacity</title><p>We note that theoretical work on memory systems often focuses on memory <italic>capacity</italic>, the number of memories that can be reliably stored in the system (<xref ref-type="bibr" rid="bib21">Gardner, 1988</xref>; <xref ref-type="bibr" rid="bib20">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>). Our learnable timescale metric is distinct from capacity. However, the two are closely linked in a particular regime. Suppose <italic>P</italic> distinct reliable memories are reinforced independently at rates <inline-formula><mml:math id="inf191"><mml:semantics><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. In the regime in which the overall rate of reliable memory presentation <inline-formula><mml:math id="inf192"><mml:semantics><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>â</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> is small, the SNR of memory recall for memory <italic>i</italic> will be the same as in the case of a single reliable memory with <inline-formula><mml:math id="inf193"><mml:semantics><mml:mrow><mml:mi>Î»</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). Hence, for a fixed <inline-formula><mml:math id="inf194"><mml:semantics><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, and for simplicity assuming that distinct reliable memories are presented at equal rates <inline-formula><mml:math id="inf195"><mml:semantics><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> for all <italic>i</italic>, the learnable timescale <inline-formula><mml:math id="inf196"><mml:semantics><mml:mrow><mml:msup><mml:mi>Ï</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> of the system dictates its capacity, equal to <inline-formula><mml:math id="inf197"><mml:semantics><mml:mrow><mml:msup><mml:mi>Ï</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. We note that this correspondence does not hold in the case where most observed memories are reliable. In this work, however, we are interested primarily in the regime of scarce reliability, where recall-gated consolidation provides the most benefit. In this regime, we regard the learnable timescale as the most natural measure of system performance, as the primary obstacle to memory storage is the presence of long gaps between reinforcements of reliable memories.</p></sec><sec sec-type="appendix" id="s10"><title>The effect of repeated reinforcement on memory dynamics without recall-gated consolidation</title><p>When memories can recur multiple times, the memory trace function <italic>m</italic>(<italic>t</italic>) is no longer an adequate description of system behavior, as the synaptic updates from multiple presentations can combine. For the synaptic plasticity rules we consider here â the binary switch, the cascade model of Fusi et al., and the multivariable model of Benna &amp; Fusi, this combination is approximately additive (<xref ref-type="bibr" rid="bib7">Benna and Fusi, 2016</xref>). This is because for each of these plasicity rules, the change in distribution of synaptic states following the presentation of a memory is approximately independent of the existing synaptic state. The only dependencies are saturation effects â synapses which have reached the edge of their dynamic range â which can only lead to sub-additive behavior. Saturation effects can be avoided by making the dynamic range of synapses sufficiently large. Thus for these plasticity rules of interest we may consider additive memory trace combination to represent a close approximation (and a tight upper bound) on the combined memory trace strength.</p><p>For a reliable memory presented at times <inline-formula><mml:math id="inf198"><mml:semantics><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, and a population of synapses using additive plasticity rules, the current SNR at time <italic>t</italic> can therefore be approximated as<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:semantics><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:mi>m</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>If memory presentations occur separated by regular intervals of length <inline-formula><mml:math id="inf199"><mml:semantics><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Î»</mml:mi></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>, we have<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:semantics><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>m</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>For the binary switch model, <italic>m</italic>(<italic>t</italic>) decays exponentially with time constant <inline-formula><mml:math id="inf200"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, and so the second term is negligible compared to the first. Hence the learnable timescale of the system is the same as the memory lifetime, approximately <inline-formula><mml:math id="inf201"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. For target SNR threshold <italic>Î´</italic>, we require <inline-formula><mml:math id="inf202"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¥</mml:mo><mml:mi>Î´</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula>, so the best possible learnable timescale, optimizing over <italic>p</italic>, is <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Î´</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For the cascade model, <italic>m</italic>(<italic>t</italic>) decays as <inline-formula><mml:math id="inf204"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>log</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. For <inline-formula><mml:math id="inf205"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>â«</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> the exponential factor dominates, resulting in the same behavior as the binary switch model. For <inline-formula><mml:math id="inf206"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>âª</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, the exponential term approximately vanishes, so the following expression for <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a close approximation and tight upper bound:<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>â</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msubsup><mml:mo>â«</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mi>Ï</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>R</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Again, for computing learnable timescale we are interested in when <inline-formula><mml:math id="inf208"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, in which case:<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mi>Ï</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>Ï</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the multivariable model, <italic>m</italic>(<italic>t</italic>) decays as <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mfrac><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:msqrt><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Again we are primarily interested in the <inline-formula><mml:math id="inf210"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>âª</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> regime, in which the expression for <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is approximately<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>â</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:msqrt><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msubsup><mml:mo>â«</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:msqrt><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mrow><mml:msqrt><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:msqrt><mml:mi>Ï</mml:mi></mml:msqrt><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This SNR is maximized for <inline-formula><mml:math id="inf212"><mml:semantics><mml:mrow><mml:mi>T</mml:mi><mml:mo>â</mml:mo><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. And for computing learnable timescale we are interested in when <inline-formula><mml:math id="inf213"><mml:semantics><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. So we have<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msqrt><mml:mi>Ï</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msqrt><mml:mi>R</mml:mi></mml:msqrt><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To compute the learnable timescale at target SNR <italic>Î´</italic> for <inline-formula><mml:math id="inf214"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>âª</mml:mo><mml:mi>R</mml:mi><mml:mo>âª</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>4</mml:mn><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>â¥</mml:mo><mml:mi>Ï</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>Î´</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the solution of which is within logarithmic factors of <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The above calculations assume deterministic interarrival intervals of length <italic>Ï</italic>. In general, we are interested in an interarrival distribution <italic>p</italic>(<italic>I</italic>) with mean <italic>Ï</italic>. However, we show numerically that for Weibull distributions with reasonable values of <italic>k</italic> (not too close to zero), the true learnable timescale figures are very bounded very closely to our results above (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). Moreover, for the purpose of computing learnable timescale <inline-formula><mml:math id="inf217"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>Ïµ</mml:mi><mml:mi>Î²</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> with error probability tolerance <italic>Ïµ</italic>, for sufficiently small <italic>Ïµ</italic> the deterministic approximation represents an upper bound on the SNR for distributions with mean <italic>Ï</italic>. This is because to ensure high SNR with very high probability, deterministic intervals are a best case scenario, as stochastic interval lengths will with some nonzero probability deviate far above the mean.</p></sec><sec sec-type="appendix" id="s11"><title>Bounds on an ideal synaptic consolidation model</title><p>In this section we show that no realistic synapse-local mechanism can achieve significantly better learnable timescale than <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and hence that the ability of recall-gated systems consolidation to achieve learnable timescale scaling superlinearly with <italic>R</italic> in some environments (see previous section) represents a qualitative advantage.</p><p>We consider a very general class of synaptic plasticity rules. In particular, we suppose a synapse can main tain a history of sequences of potentiation and depression events for arbitrarily long time windows and track the number of windows for which Î, the difference in number of potentiation and depression events, exceeds a threshold <italic>Î´</italic>. Let <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>;</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> refer to the probability distribution of values of Î after <italic>Ï</italic> timesteps, given that a synapse is potentiated by the reliable memory of interest â and <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> refers to the analogous distribution for synapses subject only to potentiation by unreliable memories. After <inline-formula><mml:math id="inf221"><mml:semantics><mml:mrow><mml:mfrac><mml:mi>T</mml:mi><mml:mi>Ï</mml:mi></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula> intervals of length <italic>Ï</italic>, for a synapse potentiated by the reliable memory, we have that<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:semantics><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>reliable</mml:mtext><mml:mo>|</mml:mo><mml:mtext>data</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>unreliable</mml:mtext><mml:mo>|</mml:mo><mml:mtext>data</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mi>Ï</mml:mi></mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>reliable</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Î</mml:mi><mml:mo>;</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>unreliable</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Î</mml:mi><mml:mo>;</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The memory can be considered retrievable with SNR of order <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> once the expression above exceeds <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (since evidence can be accumulated across the <italic>N</italic> synapses) for any choice of <italic>Ï</italic> (since we are interested in the best achievable performance).</p><p>Now, for large enough <italic>Ï</italic>, <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>unreliable</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>;</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is approximately Gaussian with mean 0 and standard deviation <inline-formula><mml:math id="inf225"><mml:semantics><mml:mrow><mml:msqrt><mml:mi>Ï</mml:mi></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula>. Conditioned on the reliable memory being presented <italic>r</italic> times in <italic>Ï</italic> timesteps, <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>reliable</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is approximately Gaussian with mean <italic>r</italic> and standard deviation <inline-formula><mml:math id="inf227"><mml:semantics><mml:mrow><mml:msqrt><mml:mi>Ï</mml:mi></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula>. The KL divergence between these two distributions is <inline-formula><mml:math id="inf228"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>. Now consider the distribution <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of number of repetitions <italic>r</italic> that occur in a time window <italic>Ï</italic>. We want to find a value <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> such that <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â¥</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mfrac><mml:mi>Ï</mml:mi><mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. From there we can assume that <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in any of the <italic>Ï</italic>-length intervals, since after <italic>T</italic> timesteps we cannot reliably count on <italic>r</italic> exceeding <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in any of the intervals.</p><p>For <inline-formula><mml:math id="inf234"><mml:semantics><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¤</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, the median of the distribution <italic>p</italic>(<italic>I</italic>), note that <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup><mml:mo>â¤</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For <inline-formula><mml:math id="inf236"><mml:semantics><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¤</mml:mo><mml:mi>c</mml:mi><mml:mo>â</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, if <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> then at least one interval of less than length <inline-formula><mml:math id="inf238"><mml:semantics><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> contains at least <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> repetitions. Hence <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup><mml:mo>â¤</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. So conservatively we can take <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Thus, our log probability expression above is bounded as follows<disp-formula id="equ33"><mml:math id="m33"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mi>Ï</mml:mi></mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>;</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>;</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>â¤</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mi>Ï</mml:mi></mml:mfrac><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mi>Ï</mml:mi></mml:mfrac><mml:mfrac><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Hence the KL divergence criterion becomes<disp-formula id="equ34"><label>(33)</label><mml:math id="m34"><mml:mrow><mml:mi>T</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>â¥</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>or equivalently,<disp-formula id="equ35"><label>(34)</label><mml:math id="m35"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>T</mml:mi><mml:mo>â¥</mml:mo><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The number of repetitions is <inline-formula><mml:math id="inf242"><mml:semantics><mml:mrow><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mi>I</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula>, giving<disp-formula id="equ36"><label>(35)</label><mml:math id="m36"><mml:mrow><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¥</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ37"><label>(36)</label><mml:math id="m37"><mml:mrow><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¥</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>assuming <inline-formula><mml:math id="inf243"><mml:semantics><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>~</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mi>I</mml:mi> <mml:mo>]</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>. For the interarrival distributions we consider (of the Weibull family), <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> so this is a conservative assumption. Hence the learnable timescale of any population using only synapse-local plasticity rules is no greater than the solution for <inline-formula><mml:math id="inf245"><mml:semantics><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mi>I</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> of the equation above. We have<disp-formula id="equ38"><label>(37)</label><mml:math id="m38"><mml:mrow><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mo>â¥</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>the solution of which is within logarithmic factors of <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12"><title>Scaling behavior of the STM/LTM model</title><p>In the recall-gated consolidation model, the overlap <inline-formula><mml:math id="inf247"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mtext>STM</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> indicates the recall strength of memory <italic>x</italic> given the current synaptic state of the STM. LTM plasticity is modulated by a factor <italic>g</italic>(<italic>r</italic>) â we refer to <italic>g</italic> as the âgating functionâ and <italic>r</italic> as the STM recall strength. We assume for now that the gating function <italic>g</italic>(<italic>r</italic>) is chosen to be a threshold function, <inline-formula><mml:math id="inf248"><mml:semantics><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, where <italic>r</italic> is the SNR of the memory overlap, <italic>H</italic> is the Heaviside step function, and <italic>Î¸</italic> is referred to as the âconsolidation threshold.â With this choice, unreliable memories will be consolidated at a rate of <inline-formula><mml:math id="inf249"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="inf250"><mml:semantics><mml:mi>Î¦</mml:mi></mml:semantics></mml:math></inline-formula> is the CDF of the normal distribution, in the limit of large system size <italic>N</italic>.</p><p>Suppose a memory <inline-formula><mml:math id="inf251"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> is presented twice with interval <italic>I</italic>. Then the SNR at the second presentation will be lower-bounded by <italic>m</italic>(<italic>I</italic>), in expectation. It follows that the rate at which reliable memories will be consolidated at for the gating function above is lower bounded by <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. After <italic>R</italic> repetitions of the reliable memory, the probability that consolidation has occurred will be at least<disp-formula id="equ39"><label>(38)</label><mml:math id="m39"><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We are interested in the maximum <italic>Î¸</italic> for which this expression exceeds <inline-formula><mml:math id="inf253"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ïµ</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> â this is the most stringest consolidation threshold we can set while still ensuring consolidation of the reliable memory with high probability. This value of <italic>Î¸</italic> is given by<disp-formula id="equ40"><label>(39)</label><mml:math id="m40"><mml:mrow><mml:mi>R</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>Ïµ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>If <italic>R</italic> is large then the solution will be such that <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is small, enabling the approximation:<disp-formula id="equ41"><label>(40)</label><mml:math id="m41"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>Ïµ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math></disp-formula></p><p>For tractability we consider, as our family of interarrival distributions, Weibull distributions with regularity parameter <italic>k</italic>. The cumulative distribution function is<disp-formula id="equ42"><label>(41)</label><mml:math id="m42"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&lt;&lt;</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, this is approximated as<disp-formula id="equ43"><label>(42)</label><mml:math id="m43"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>Î</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Importantly, <inline-formula><mml:math id="inf256"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> decays as <italic>t<sup>k</sup></italic>. Thus, increasing the number of repetitions <italic>R</italic> has the effect of scaling the <italic>Ï</italic> that satisfies <xref ref-type="disp-formula" rid="equ41">Equation 40</xref> by <inline-formula><mml:math id="inf257"><mml:semantics><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>. That is, for a fixed <italic>Î¸</italic>, and hence a fixed degree of amplification <inline-formula><mml:math id="inf258"><mml:semantics><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> of the effective rate of reliable memories in the LTM, the maximum <italic>Ï</italic> achieving that SNR with probability <inline-formula><mml:math id="inf259"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ïµ</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> (i.e. the learnable timescale <inline-formula><mml:math id="inf260"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>Î²</mml:mi><mml:mi>Ïµ</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>) scales as <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For a gating function threshold <italic>Î¸</italic>, the corresponding SNR in the LTM will be the SNR induced by an interarrival distribution <inline-formula><mml:math id="inf262"><mml:semantics><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> with mean value<disp-formula id="equ44"><label>(43)</label><mml:math id="m44"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf263"><mml:semantics><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> decays much more rapidly than any power of <inline-formula><mml:math id="inf264"><mml:semantics><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, it follows that <inline-formula><mml:math id="inf265"><mml:semantics><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> can be made <italic>O</italic>(1), and thus the SNR of the LTM can become <inline-formula><mml:math id="inf266"><mml:semantics><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, for relatively small values of <italic>Î¸</italic> (and hence a small number of required repetitions). In other words, for a fixed number of expected memory repetitions, the learnable timescale of the LTM decreases only slightly as the target SNR is raised from <italic>O</italic>(1) to <inline-formula><mml:math id="inf267"><mml:semantics><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>Note that if <italic>P</italic> different reliable memories are present, then <inline-formula><mml:math id="inf268"><mml:semantics><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LTM</mml:mtext></mml:mrow></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> for any given reliable memory will be lower-bounded by <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The induced SNR for any given reliable memory in the LTM will in this case be of order <italic>m</italic>(<italic>P</italic>), rather than <inline-formula><mml:math id="inf271"><mml:semantics><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s13"><title>Optimal sparsity calculations</title><p>We now consider the case of sparse memories â those which potentiate a fraction <italic>f</italic> of synapses. Consider the behavior of a single population of binary synapses employing the binary switch plasticity rule. We modify the plasticity rule slightly so that potentiation flips the state of a synapse with probability <italic>p</italic> and depression with probability <inline-formula><mml:math id="inf272"><mml:semantics><mml:mrow><mml:mfrac><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mi>p</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, to ensure that the fractions of potentiated and depressed synapses remain balanced.</p><p>We consider an environment with a single reliable memory that is presented with probability <italic>Î»</italic> at each time step (otherwise, a randomly sampled unreliable memory is presented). We can compute the behavior analytically by tracking how the distributions of <italic>u</italic> (the output neuron response to true stimuli) and <italic>v</italic> (the output neuron response to noise) evolve over time. We assume that the coding level <italic>f</italic> is sufficiently small that terms of order <inline-formula><mml:math id="inf273"><mml:semantics><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> may be ignored.</p><p>Due to the balanced plasticity rule, <inline-formula><mml:math id="inf274"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula> of synapses are strong at any given time, so the mean response <inline-formula><mml:math id="inf275"><mml:semantics><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> to a randomly sampled noise pattern is <inline-formula><mml:math id="inf276"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>. The variance of <italic>v</italic> is also constant and equal to <inline-formula><mml:math id="inf277"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>The evolution of <italic>u</italic> is a stochastic process that, in the limit of large <italic>Nf</italic> (i.e. a large number of active neurons for each stimulus), can be described as an Ornstein-Uhlenbeck (OU) process:<disp-formula id="equ45"><label>(44)</label><mml:math id="m45"><mml:semantics><mml:mrow><mml:mi>Î</mml:mi><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>â</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Ïµ</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ïµ</mml:mi><mml:mo>â¼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>In the limit of small <italic>f</italic> we have:<disp-formula id="equ46"><label>(45)</label><mml:math id="m46"><mml:semantics><mml:mrow><mml:mi>Î¸</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ47"><label>(46)</label><mml:math id="m47"><mml:semantics><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>Î»</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The quantity <inline-formula><mml:math id="inf279"><mml:semantics><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> determines the asymptotic mean of <italic>u</italic> and the quantity <italic>Î¸</italic> determines the rate at which <italic>u</italic> converges to this mean. Immediately we see that <inline-formula><mml:math id="inf280"><mml:semantics><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> scales with the frequency <italic>Î»</italic> with which the true stimulus is presented, and that the rate of convergence (speed of learning) is proportional to <italic>p</italic>.</p><p>By well-known properties of OU processes, the asymptotic variance of <italic>u</italic> is equal to <inline-formula><mml:math id="inf281"><mml:semantics><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Î¸</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>. In the small-p limit, this quantity comes out to<disp-formula id="equ48"><label>(47)</label><mml:math id="m48"><mml:semantics><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Note that in the low-p limit (slow learning rate) this is the same as the variance of <italic>v</italic>. Thus in the limit of slow learning, we have that<disp-formula id="equ49"><label>(48)</label><mml:math id="m49"><mml:semantics><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>u</mml:mi><mml:mo>â</mml:mo><mml:mi>v</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>Î»</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ50"><label>(49)</label><mml:math id="m50"><mml:semantics><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>â</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>And thus<disp-formula id="equ51"><label>(50)</label><mml:math id="m51"><mml:semantics><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:msup><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>From this expression we can see that for a given <italic>f</italic>, the asymptotic SNR always increases with <italic>Î»</italic> and <italic>N</italic>. For a given <italic>Î»</italic>, we would like to maximize this expression with respect to <italic>f</italic>.<disp-formula id="equ52"><label>(51)</label><mml:math id="m52"><mml:semantics><mml:mrow><mml:msub><mml:mo>â</mml:mo><mml:mi>f</mml:mi></mml:msub><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>This expression equals zero when<disp-formula id="equ53"><label>(52)</label><mml:math id="m53"><mml:semantics><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ54"><label>(53)</label><mml:math id="m54"><mml:semantics><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>Î»</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mi>f</mml:mi></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="equ55"><label>(54)</label><mml:math id="m55"><mml:semantics><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>Î»</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>so the asymptotic SNR is maximized for <inline-formula><mml:math id="inf282"><mml:semantics><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>Î»</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. That is, the optimal coding level is proportional to the frequency with which reliable (as opposed to unreliable) stimuli are observed in the environment.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90793.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gjorgjieva</surname><given-names>Julijana</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Technical University of Munich</institution><country>Germany</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Fundamental</kwd></kwd-group></front-stub><body><p>This <bold>fundamental</bold> work proposes a novel mechanism for memory consolidation where short-term memory provides a gating signal for memories to be consolidated into long-term storage. The work combines extensive analytical and numerical work applied to three different scenarios and provides a <bold>convincing</bold> analysis of the benefits of the proposed model, although some of the analyses are limited to the type of memory consolidation the authors consider (and don't consider), which limits the impact. The work will be of interest to neuroscientists and many other researchers interested in the mechanistic underpinnings of memory.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90793.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In the manuscript the authors suggest a computational mechanism called recall-gated consolidation, which prioritizes the storage of previously experienced synaptic updates in memory. The authors investigate the mechanism with different types of learning problems including supervised learning, reinforcement learning, and unsupervised auto-associative memory. They rigorously analyse the general mechanism and provide valuable insights into its benefits.</p><p>Strengths:</p><p>The authors establish a general theoretical framework, which they translate into three concrete learning problems. For each, they define an individual mathematical formulation. Finally, they extensively analyse the suggested mechanism in terms of memory recall, consolidation dynamics, and learnable timescales.</p><p>The presented model of recall-gated consolidation covers various aspects of synaptic plasticity, memory recall, and the influence of gating functions on memory storage and retrieval. The model's predictions align with observed spaced learning effects.</p><p>The authors conduct simulations to validate the recall-gated consolidation model's predictions, and their simulated results align with theoretical predictions. These simulations demonstrate the model's advantages over consolidating any memory and showcase its potential application to various learning tasks.</p><p>The suggestion of a novel consolidation mechanism provides a good starting point to investigate memory consolidation in diverse neural systems and may inspire artificial learning algorithms.</p><p>Weaknesses:</p><p>I appreciate that the authors devoted a specific section to the model's predictions, and point out how the model connects to experimental findings in various model organisms. However, the connection is rather weak and the model needs to make more specific predictions to be distinguishable from other theories of memory consolidation (e.g. those that the authors discuss) and verifiable by experimental data.</p><p>The model is not compared to other consolidation models in terms of performance and how much it increases the signal-to-noise ratio. It is only compared to a simple STM or a parallel LTM, which I understand to be essentially the same as the STM but with a different timescale (so not really an alternative consolidation model). It would be nice to compare the model to an actual or more sophisticated existing consolidation model to allow for a fairer comparison.</p><p>The article is lengthy and dense and it could be clearer. Some sections are highly technical and may be challenging to follow. It could benefit from more concise summaries and visual aids to help convey key points.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90793.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In their article Jack Lindsey and Ashok Litwin-Kumar describe a new model for systems memory consolidation. Their idea is that a short-term memory acts not as a teacher for a long-term memory - as is common in most complementary learning systems -, but as a selection module that determines which memories are eligible for long term storage. The criterion for the consolidation of a given memory is a sufficient strength of recall in the short term memory.</p><p>The authors provide an in-depth analysis of the suggested mechanism. They demonstrate that it allows substantially higher SNRs than previous synaptic consolidation models, provide an extensive mathematical treatment of the suggested mechanism, show that the required recall strength can be computed in a biologically plausible way for three different learning paradigms, and illustrate how the mechanism can explain spaced training effects.</p><p>Strengths:</p><p>The suggested consolidation mechanism is novel and provides a very interesting alternative to the classical view of complementary learning systems. The analysis is thorough and convincing.</p><p>Weaknesses:</p><p>The main weakness of the paper is the equation of recall strength with the synaptic changes brought about by the presentation of a stimulus. In most models of learning, synaptic changes are driven by an error signal and hence cease once the task has been learned. The suggested consolidation mechanism would stop at that point, although recall is still fine. The authors should discuss other notions of recall strength that would allow memory consolidation to continue after the initial learning phase. Aside from that, I have only a few technical comments that I'm sure the authors can address with a reasonable amount of work.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90793.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the original reviews.</p><p>In light of some reviewer comments requesting more clarity on the relationship between our model and prior theoretical studies of systems consolidation, we propose a modification to the title of our manuscript: âSelective consolidation of learning and memory via recall-gated plasticity.â We believe this title better reflects the key distinguishing feature of our model, that it <italic>selectively</italic> consolidates only a subset of memories, and also highlights the modelâs applicability to task learning as well as memory storage.</p><p>Major comments:</p><disp-quote content-type="editor-comment"><p>Reviewer #3âs primary concern with the paper is the following: âThe main weakness of the paper is the equation of recall strength with the synaptic changes brought about by the presentation of a stimulus. In most models of learning, synaptic changes are driven by an error signal and hence cease once the task has been learned. The suggested consolidation mechanism would stop at that point, although recall is still fine. The authors should discuss other notions of recall strength that would allow memory consolidation to continue after the initial learning phase.â</p></disp-quote><p>We thank the reviewer for drawing attention to this issue, which primarily results from a poor that memories should be interpreted as actual synaptic weight updates, âð¤ and thus in the context choice of notation on our part. Our decision to denote memories as gives the impression of supervised learning would go to zero when the task is learned. However, in the formalism of our model, memories are in fact better interpreted as <italic>target</italic> values of synaptic weights, and the synaptic model/plasticity rule is responsible for converting these target values into synaptic weight updates. We were unclear on this point in our initial submission, because our paper primarily considers binary synaptic weights, where target synaptic weights have a one-to-one correspondence with candidate synaptic weight updates. We have updated the paper to use w* to refer to memories, which we hope resolves this confusion, and have updated our introduction to the term âmemoryâ to reflect their interpretation as target synaptic weight values. We have also updated the paperâs language to more clearly disambiguate between the âlearning rule,â which determines how the memory vector (target synaptic weight vectors) are derived from task variables, and the âplasticity rule,â which governs how these are translated into actual synaptic weight updates. We acknowledge that our manuscript still does not explicitly consider a plasticity rule that is sensitive to continuous error error signals, as our analysis is restricted to binary weights. However, we believe that the updated notation and exposition makes it more clear that our model could be applied in such a case.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 brought up that our framework cannot capture âsingle-shot learning, for example, under fear conditioning or if a presented stimulus is astonishing.â Reviewer #2 raised a related question of how our model ârelates to the opposite more intuitive idea, that novel surprising experiences should be stored in memory, as the familiar ones are presumably already stored.â</p></disp-quote><p>We agree that the built-in inability to consolidate memories after a single experience is a limitation of our model, and that extreme novelty is one factor (among others, such as salience or reward) that might incentivize one-shot consolidation. We have added a comment to the discussion to acknowledge these points (added text in bold): â Moreover, in real neural circuits, additional factors besides recall, such as reward or salience, are likely to influence consolidation as well. For instance, a sufficiently salient event should be stored in long-term memory even if encountered only once. Furthermore, while in our model familiarity drives consolidation, certain forms of novelty may also incentivize consolidation, raising the prospect of a non-monotonic relationship between consolidation probability and familiarity.â We agree that future work should address the combined influence of recall (as in our model) and other factors on the propensity to consolidate a memory.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 requested, âa comparison/discussion of the wide range of models on synaptic tagging for consolidation by various types of signals. Notably, studies from Wulfram Gerstner's group (e.g., Brea, J., Clayton, N. S., &amp; Gerstner, W. (2023). Computational models of episodic-like memory in food-caching birds. Nature Communications, 14(1); and studies on surprise).â</p></disp-quote><p>We thank the reviewer for the reference, which we have added to the manuscript. The model of Brea et al.(2023) is similar to that of Roxin &amp; Fusi (2013), in that consolidation consists of âcopyingâ synaptic weights from one population to another. As a result, just like the model of Roxin &amp; Fusi (2013), this model does not provide the benefit that our model offers in the context of consolidating repeatedly recurring memories. However, the model of Brea et al. does have other interesting properties â for instance, it affords the ability to decode the age of a memory, which our model does not. We have added a comment on this point in the subsection of the Discussion tilted âOther models of systems consolidation.â</p><disp-quote content-type="editor-comment"><p>Reviewer #2 noted, âWhile the article extensively discusses the strengths and advantages of the recall-gated consolidation model, it provides a limited discussion of potential limitations or shortcomings of the model, such as the missing feature of generalization, which is part of previous consolidation models. The model is not compared to other consolidation models in terms of performance and how much it increases the signal-to-noise ratio.â</p></disp-quote><p>We agree that our work does not consider the notion of generalization and associated changes to representational geometry that accompany consolidation, which is the focus of many other studies on consolidation. We have further highlighted this limitation in the discussion. Regarding the comparison to other models, this is a tricky point as the desiderata we emphasize in this study (the ability to recall memories that are intermittently reinforced) is not the focus of other studies. Indeed, our focus is primarily on the ability of systems consolidation to be <italic>selective</italic> in which memories are consolidated, which is somewhat orthogonal to the focus of many other theoretical studies of consolidation. We have updated some wording in the introduction to emphasize this focus.</p><p>Additional comments made by reviewer #1</p><disp-quote content-type="editor-comment"><p>Reviewer #1 pointed out issues in the clarity of Fig. 2A. We have added substantial clarifying text to the figure caption.</p><p>Reviewer #1 pointed out lack of clarity in our introduction to the terms âreliabilityâ and âreinforcement.â We have now made it more clear what we mean by these terms the first time they are used.</p></disp-quote><p>We have updated our definition of ârecallâ to use the term ârecall factor,â which is how we refer to it subsequently in the paper.</p><p>We have made explicit in the main text our simplifying assumption that memories are mean-centered.</p><p>We have made consistent our use of âforgetting curveâ and âmemory traceâ.</p><p>Additional comments made by reviewer #2</p><p>We have added a comment in the discussion acknowledging alternative interpretations of the result of Terada et al. (2021)</p><p>We have significantly expanded the discussion of findings about the mushroom body to make it accessible to readers who do not specialize in this area. We hope this clarifies the nature of the experimental finding, which uncovered a circuit that performs a strikingly clean implementation of our model.</p><p>The reviewer expresses concern that the songbird study (Tachibana et al., 2022) does not provide direct evidence for consolidation being gated by familiarity of patterns of activity. Indeed, the experimental finding is one-step removed from the direct predictions of our model. That said, the finding â that the <italic>rate</italic> of consolidation increases with performance â is highly nontrivial, and is predicted by our model when applied to reinforcement learning tasks. We have added a comment to the discussion acknowledging that this experimental support for our model is behavioral and not mechanistic.</p><p>We do not regard it as completely trivial that the parallel LTM model performs roughly the same as the STM model, since a slower learning rate can achieve a higher SNR (as in Fig. 2C). Nevertheless we have added wording to the main text around Fig. 4B to note that the result is not too surprising.</p><p>We have added a sentence that clarifies the goal / question of our paper earlier on in the introduction.</p><p>We have updated Figure 3 by labeling the key components of the schematics and adding more detail to the legend, as suggested by the reviewer. We also reordered the figure panels as suggested.</p><p>Additional comments made by reviewer #3:</p><p>We have clarified in the main text that Fig. 2C and all results from Fig. 4 onward are derived from an ideal observer model (which we also more clearly define).</p><p>We have now emphasized in the main text that the derivations of the recall factors for specific learning rules are derived in the Supplementary Information.</p><p>We have highlighted more clearly in the main text that the recall factors associated with specific learning rules may correspond to other notions that do not intuitively correspond to ârecall,â and have added a pointer to Fig. 3A where these interpretations are spelled out.</p><p>We have added references corresponding to the types of learning rules we consider.</p><p>The cutoffs / piecewise-looking behavior of plots in Fig. 4 are primarily the result of finite N, which limits the maximum SNR of the system, rather than coarse sampling of parameter values.</p><p>Thank you for pointing out the error in the legend in Fig. 5D (also affected Supp Fig. S7/S8), which is now fixed.</p><p>The reference to the nonexistence panel Fig. 5G has been removed.</p><p>As the reviewer points out, the use of a binary action output in our reinforcement learning task renders it quite similar to the supervised learning task, making the example less compelling. In the revised manuscript we have updated the RL simulation to use three actions. Note also that in our original submission the network outputs represented action probabilities directly (which is straightforward to do for binary actions, but not for more than two available actions). In order to parameterize a policy when more than two actions are available, we sample actions using a softmax policy, as is more standard in the field and as the reviewer suggested. The associated recall factor is still a product of reward and a âconfidence factor,â and the confidence factor is still the value of the network output in the unit corresponding to the chosen action, but in the updated implementation this factor is equal to <inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Ï</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, similar (though with a sign difference) to the reviewerâs suggestion. We believe these updates make our RL implementation and simulation more compelling, as it allows them to be applied to tasks with arbitrary numbers of actions.</p><p>Additional minor comments</p><p>The reviewers made a number of other specific line-by-line wording suggestions, typo corrections,</p></body></sub-article></article>