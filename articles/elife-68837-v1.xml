<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">68837</article-id><article-id pub-id-type="doi">10.7554/eLife.68837</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Fast and accurate annotation of acoustic signals with deep neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-254113"><name><surname>Steinfath</surname><given-names>Elsa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8455-9092</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-233073"><name><surname>Palacios-Muñoz</surname><given-names>Adrian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9335-7767</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-233074"><name><surname>Rottschäfer</surname><given-names>Julian R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3741-8358</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-233075"><name><surname>Yuezak</surname><given-names>Deniz</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-30090"><name><surname>Clemens</surname><given-names>Jan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4200-8097</contrib-id><email>clemensjan@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/></contrib><aff id="aff1"><label>1</label><institution>European Neuroscience Institute - A Joint Initiative of the University Medical Center Göttingen and the Max-Planck-Society</institution><addr-line><named-content content-type="city">Göttingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>International Max Planck Research School and Göttingen Graduate School for Neurosciences, Biophysics, and Molecular Biosciences (GGNB) at the University of Göttingen</institution><addr-line><named-content content-type="city">Göttingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Bernstein Center for Computational Neuroscience</institution><addr-line><named-content content-type="city">Göttingen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>11</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e68837</elocation-id><history><date date-type="received" iso-8601-date="2021-03-26"><day>26</day><month>03</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-10-04"><day>04</day><month>10</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-03-29"><day>29</day><month>03</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.03.26.436927"/></event></pub-history><permissions><copyright-statement>© 2021, Steinfath et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Steinfath et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-68837-v1.pdf"/><abstract><p>Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, and fast.</p><p>We here introduce <italic>DeepAudioSegmenter</italic> (<italic>DAS)</italic>, a method that annotates acoustic signals across species based on a deep-learning derived hierarchical presentation of sound. We demonstrate the accuracy, robustness, and speed of <italic>DAS</italic> using acoustic signals with diverse characteristics from insects, birds, and mammals. <italic>DAS</italic> comes with a graphical user interface for annotating song, training the network, and for generating and proofreading annotations. The method can be trained to annotate signals from new species with little manual annotation and can be combined with unsupervised methods to discover novel signal types. <italic>DAS</italic> annotates song with high throughput and low latency for experimental interventions in realtime. Overall, <italic>DAS</italic> is a universal, versatile, and accessible tool for annotating acoustic communication signals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>acoustic communication</kwd><kwd>annotation</kwd><kwd>song</kwd><kwd>deep learning</kwd><kwd>bird</kwd><kwd>fly</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd><kwd>Mouse</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>329518246</award-id><principal-award-recipient><name><surname>Clemens</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>430158535</award-id><principal-award-recipient><name><surname>Clemens</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>851210</award-id><principal-award-recipient><name><surname>Clemens</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value><italic>DAS</italic> is a universal tool for segmenting and identifying acoustic signals in single and multi channel recordings robustly, reliably, and at low latency.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animals produce sounds to foster group cohesion (<xref ref-type="bibr" rid="bib37">Haack et al., 1983</xref>; <xref ref-type="bibr" rid="bib44">Janik and Slater, 1998</xref>; <xref ref-type="bibr" rid="bib14">Chaverri et al., 2013</xref>), to signal the presence of food, friend, or foe (<xref ref-type="bibr" rid="bib12">Cäsar et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Clay et al., 2012</xref>), and to find and evaluate mating partners (<xref ref-type="bibr" rid="bib5">Baker et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Behr and von Helversen, 2004</xref>; <xref ref-type="bibr" rid="bib40">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>). Studying acoustic communication not only provides insight into social interactions within and across species; it can also reveal the mechanisms driving complex behaviors: The genetics and evolution of signal production and recognition (<xref ref-type="bibr" rid="bib28">Ding et al., 2016</xref>), the genes and circuits driving song learning (<xref ref-type="bibr" rid="bib47">Kollmorgen et al., 2020</xref>), or the fast and precise sensorimotor transformations involved in vocal interactions (<xref ref-type="bibr" rid="bib23">Coen et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Cator et al., 2009</xref>; <xref ref-type="bibr" rid="bib31">Fortune et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Okobi et al., 2019</xref>). The first step in many studies of acoustic communication is song annotation: the segmentation and labeling of individual elements in a recording. Acoustic signals are diverse and range from the repetitive long-distance calling songs of crickets, grasshoppers, and anurans (<xref ref-type="bibr" rid="bib32">Gerhardt and Huber, 2002</xref>), the dynamic and context-specific courtship songs of vinegar flies or rodents (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Neunuebel et al., 2015</xref>; <xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>), to the complex vocalizations produced by some birds and primates (<xref ref-type="bibr" rid="bib52">Lipkind et al., 2013</xref>; <xref ref-type="bibr" rid="bib86">Weiss et al., 2014</xref>; <xref ref-type="bibr" rid="bib51">Landman et al., 2020</xref>).</p><p>This diversity in signal structure has spawned a zoo of annotation tools (<xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref>; <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Koumura and Okanoya, 2016</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>), but existing methods still face challenges: First, assessing vocal repertoires and their relation to behavioral and neural dynamics (<xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Neunuebel et al., 2015</xref>; <xref ref-type="bibr" rid="bib31">Fortune et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Okobi et al., 2019</xref>) requires annotations to be complete and temporally precise even at low signal levels, but annotation can fail when signals are weak (<xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib76">Stern et al., 2017</xref>). Second, analyses of large datasets and experimental interventions during behavior (<xref ref-type="bibr" rid="bib31">Fortune et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Okobi et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Bath et al., 2014</xref>; <xref ref-type="bibr" rid="bib80">Tschida and Mooney, 2012</xref>; <xref ref-type="bibr" rid="bib77">Stowers et al., 2017</xref>) need annotations to be fast, but existing methods are often slow. Last, annotation methods should be flexible and adaptable (<xref ref-type="bibr" rid="bib28">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Ding et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Clemens and Hennig, 2013</xref>), but existing methods often work only for restricted types of signals or adapting them to new signals requires tedious manual tuning (<xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>).</p><p>In brief, an accurate, fast, and flexible framework for annotating song across species is missing. A general framework would not only improve upon existing methods but would also facilitate the study of species for which automated methods do not yet exist. Deep neural networks have emerged as powerful and flexible tools for solving data annotation tasks relevant for neuroscience such as object recognition, pose tracking, or speech recognition (<xref ref-type="bibr" rid="bib50">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Graves and Jaitly, 2014</xref>; <xref ref-type="bibr" rid="bib55">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib69">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Graving et al., 2019</xref>). These methods are not only fast and accurate but also easily adapted to novel signals by non-experts since they only require annotated examples for learning. Recently, deep neural networks have also been used for annotating animal vocalizations (<xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Arthur et al., 2021</xref>; <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>).</p><p>We here present a new deep-learning-based framework for annotating acoustic signals, called <italic>Deep Audio Segmenter</italic> (<italic>DAS</italic>). We test the framework on a diverse set of recordings from insects, birds, and mammals, and show that <italic>DAS</italic> annotates song in single- and multi-channel recordings with high accuracy. The framework produces annotations with low latency on standard PCs and is therefore ideally suited for closed-loop applications. Small-to-moderate amounts of manual annotations suffice for adapting the method to a new species and annotation work can be simplified by combining <italic>DAS</italic> with unsupervised methods. We provide <italic>DAS</italic> as an open-source software package with a graphical user interface for manually annotating audio, training the network, and inferring and proofreading annotations. Integration into existing frameworks for signal analysis or experimental control is possible using a programmatic interface. The code and documentation for <italic>DAS</italic> are available at <ext-link ext-link-type="uri" xlink:href="https://janclemenslab.org/das/">https://janclemenslab.org/das/</ext-link>.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Architecture and working principle of <italic>DAS</italic></title><p>Acoustic signals are defined by features on multiple timescales—the fast harmonic oscillations of the sound carrier (&lt;10 ms), modulations of amplitude (AM) and frequency (FM) (10–100 ms), and the sequencing of different AM and FM patterns into bouts, syllables, or phrases (10–1000 ms). These patterns are typically made explicit using a hand-tuned pre-processing step based on time-resolved Fourier or wavelet transforms (<xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>; <xref ref-type="bibr" rid="bib82">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>). Most deep-learning-based methods then treat this pre-defined spectrogram as an image and use methods derived from computer vision to extract the AM and FM features relevant for annotation (<xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>). Recurrent units are sometimes used to track the sound features over time (<xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>). This approach can produce accurate annotations but has drawbacks: First, the spectrogram constitutes a strong and proven pre-processing step, but it is unsuitable for some signal types, like short pulsatile signals. Second, the pre-processing transform is typically tuned by hand and may therefore require expert knowledge for it to produce optimal results. Lastly, the recurrent units used in some methods (<xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>) excel at combining information over time to provide the context information necessary to annotate spectrally complex signals, but they can be hard to train and slow to run (<xref ref-type="bibr" rid="bib4">Bai et al., 2018</xref>).</p><p><italic>DAS</italic> solves these limitations in three ways: First, the pre-processing step is optional. This makes <italic>DAS</italic> more flexible, since signals for which a time-resolved Fourier transform is not appropriate—for instance, short pulsatile signals—can now also be processed. Second, the optional preprocessing step is integrated and optimized with the rest of the network. This removes the need to hand-tune this step and allows the network to learn a preprocessing that deviates from a time-resolved Fourier or wavelet transform if beneficial (<xref ref-type="bibr" rid="bib16">Choi et al., 2017</xref>). Integrating the preprocessing into the network also increases inference speed due to the efficient implementation and hardware acceleration of deep-learning frameworks. Third and last, <italic>DAS</italic> learns a task-specific representation of sound features using <italic>temporal convolutional networks</italic> (TCNs) (<xref ref-type="bibr" rid="bib4">Bai et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">van den Oord et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Guirguis et al., 2021</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A–E</xref>). At the core of TCNs are so-called <italic>dilated convolutions</italic> (<xref ref-type="bibr" rid="bib87">Yu and Koltun, 2016</xref>). In standard convolutions, short templates slide over the signal and return the similarity with the signal at every time point. In <italic>dilated</italic> convolutions, these templates have gaps, allowing to analyze features on longer timescales without requiring more parameters to specify the template. Stacking dilated convolutions with growing gap sizes results in a hierarchical, multi-scale representation of sound features, which is ideally suited for the hierarchical and harmonic structure of animal vocalizations.</p><p>The output of the deep neural network in <italic>DAS</italic> is a set of confidence scores for each audio sample, corresponding to the probability of each song type (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Annotation labels for the different song types are mutually exclusive and are produced by comparing the confidence score to a threshold or by choosing the most probable song type. Brief gaps in the annotations are closed and short spurious detections are removed to smoothen the annotation. For song types that are described as events, like the pulses in fly song (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), the event times are extracted as local maxima that exceed a confidence threshold.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>DAS performance for fly song.</title><p>(<bold>A</bold>) Fly song (black, top) with manual annotations of sine (blue) and pulse (red) song. The spectrogram (bottom) shows the signal’s frequency content over time (see color bar). (<bold>B</bold>) <italic>DAS</italic> builds a hierarchical presentation of song features relevant for annotation using a deep neural network. The network consists of three TCN blocks, which extract song features at multiple timescales. The output of the network is a confidence score for each sample and song type. (<bold>C</bold>) Confidence scores (top) for sine (blue) and pulse (red) for the signal in A. The confidence is transformed into annotation labels (bottom) based on a confidence threshold (0.5 for sine, 0.7 for pulse). Ground truth (bottom) from manual annotations shown for comparison. (<bold>D</bold>) Confusion matrix for pulse from the test data set. Color indicates the percentage (see color bar) and text labels indicate the number of pulses for each quadrant. All confusion matrices are normalized such that columns sum to 100%. The concentration of values along the diagonal indicates high annotation performance. (<bold>E</bold>) Precision-recall curve for pulse depicts the performance characteristics of <italic>DAS</italic> for different confidence thresholds (from 0 to 1, black arrow points in the direction of increasing threshold). Recall decreases and precision increases with the threshold. The closer the curve to the upper and right border, the better. The red circle corresponds to the performance of <italic>DAS</italic> for a threshold of 0.7. The black circle depicts the performance of FlySongSegmenter (<italic>FSS</italic>) and gray circles the performance of two human annotators. (<bold>F</bold>) Probability density function of temporal errors for all detected pulses (red shaded area), computed as the distance between each pulse annotated by <italic>DAS</italic> and the nearest manually annotated pulse. Lines depict the median temporal error for <italic>DAS</italic> (red line, 0.3 ms) and <italic>FSS</italic> (gray line, 0.1 ms). (G, H) Recall of <italic>DAS</italic> (red line) and <italic>FSS</italic> (gray line) as a function of the pulse carrier frequency (<bold>G</bold>) and signal-to-noise ratio (SNR) (<bold>H</bold>). Red shaded areas show the distributions of carrier frequencies (<bold>G</bold>) and SNRs (<bold>H</bold>) for all pulses. <italic>DAS</italic> outperforms <italic>FSS</italic> for all carrier frequencies and SNRs. (<bold>I</bold>) Same as in D but for sine. Color indicates the percentage (see color bar) and text labels indicate seconds of sine for each quadrant. (<bold>J</bold>) Same as in E but for sine. The blue circle depicts the performance for the confidence threshold of 0.5. (<bold>K</bold>) Distribution of temporal errors for all detected sine on- and offsets. Median temporal error is 12 ms for <italic>DAS</italic> (blue line) and 22 ms for <italic>FSS</italic> (gray line). (<bold>L, M</bold>) Recall for <italic>DAS</italic> (blue line) and <italic>FSS</italic> (gray line) as a function of sine duration (<bold>L</bold>) and SNR (<bold>M</bold>). Blue-shaded areas show the distributions of durations and SNRs for all sine songs. <italic>DAS</italic> outperforms <italic>FSS</italic> for all durations and SNRs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title><italic>DAS</italic> architecture and evaluation.</title><p>(<bold>A-D</bold>) Network architectures for annotating fly song from single (<bold>A</bold>) and multi-channel (<bold>B</bold>) recordings, mouse USVs and marmoset song (<bold>C</bold>), and bird song (Bengalese and Zebra finches) (<bold>D</bold>). See legend to the right. Each TCN block consists of stacks of residual blocks shown in E. See <xref ref-type="table" rid="table4">Table 4</xref> for all network parameters. (<bold>E</bold>) A TCN block (left) consists of a stack of five residual blocks (right). Residual blocks process the input with a sequence of dilated convolution, rectification (ReLU) and normalization. The output of this sequence of steps is then added to the input. In successive residual blocks, the dilation rate of the convolution filters doubles from 1x in the first to 16x in the last layer (see numbers to the left of each block). The output of the last residual block is passed as an input to the next TCN block in the network. In addition, the outputs of all residual blocks in a network are linearly combined to predict the song. (<bold>F</bold>) Annotation performance is evaluated by comparing manual annotations (top) with labels produced by <italic>DAS</italic> (bottom). Gray indicates no song, orange song. True negatives (TN) and true positives (TP) are samples for which <italic>DAS</italic> matches the manual labels. False negatives (FNs) are samples for which the song was missed (blue frame) and reduce recall (TP/(FN+TP)). False positives (FP) correspond to samples that were falsely predicted as containing song (green frames) and reduce precision (TP/(TP+FP)). (<bold>G</bold>) Precision and recall are calculated from a confusion matrix which tabulates TP (orange), TN (gray), FP (green), FN (blue). In the example, precision is 3/(3+2) and recall is 3/(1+3).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Performance and the role of context for annotating fly pulse song.</title><p>(<bold>A</bold>) Recall (blue) and precision (orange) for fly pulse song for different distance thresholds. The distance threshold determines the maximal distance to a true pulse for a detected pulse to be a true positive. (<bold>B</bold>) Waveforms of true positive (left), false positive (middle), and false negatives (right) pulses in fly song. Pulses were aligned to the peak, adjusted for sign, and their amplitude was normalized to have unit norm (see <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>). (<bold>C</bold>) Waveforms (top) and confidence scores (bottom, see color bar) for pulses in different contexts. <italic>DAS</italic> exploits context effects to boost the detection of weak signals. An isolated ('Isolated', first row) weak pulse-like waveform is detected with low confidence, since similar waveforms often arise from noise. Manual annotators exploit context information—the fact that pulses often occur in trains at an interval of 40 ms—to annotate weak signals. <italic>DAS</italic> does the same: the same waveform is detected with much higher confidence due to the presence of a nearby pulse train ('Correct IPI', 2nd row). If the pulse is too close to another pulse ('Too close', 3rd row), it is likely noise and <italic>DAS</italic> detects it with lower confidence. Context effects do not affect strong signals. For instance, a missing pulse within a pulse train ('Missing', last row) does not reduce detection confidence of nearby pulses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Performance for multi-channel recordings of fly courtship song.</title><p>(<bold>A</bold>) Fly song (black) with manual annotation indicating sine (blue) and pulse (red). Traces (top) and spectrogram (bottom, see color bar) show data from the loudest of the nine audio channels. (<bold>B</bold>) Confidence scores (top) for sine (blue) and pulse (red). The confidence is transformed into annotation labels (bottom) based on a confidence threshold (0.5 for sine and pulse). Ground truth (bottom) from manual annotations shown for comparison. <italic>DAS</italic> annotations were generated using separate networks for pulse and for sine. (<bold>C</bold>) Confusion matrix for pulse from a test data set. Color indicates the percentage (see color bar) and text labels indicate number of pulses for each quadrant. All confusion matrices are normalized such that columns sum to 100%. The concentration of values along the diagonal indicates high annotation performance. (<bold>D</bold>) Precision-recall curve for pulse depicts the performance characteristics of <italic>DAS</italic> for different confidence threshold (from 0 to 1). Recall decreases and precision increases with the threshold. The closer the curve to the upper and right border, the better. The red circle corresponds to the performance of <italic>DAS</italic> for a threshold of 0.5. The gray circle depicts the performance of <italic>FlySongSegmenter</italic> (<italic>FSS</italic>). (<bold>E</bold>) Probability density function (PDF) of temporal errors for all detected pulses (red shaded area), computed as the distance between each pulse annotated by <italic>DAS</italic> and its nearest manual pulse. Lines depict median temporal error for <italic>DAS</italic> (red line, 0.3 ms) and <italic>FSS</italic> (gray line, 0.1 ms). (<bold>F, G</bold>) Recall of <italic>DAS</italic> (red line) and <italic>FSS</italic> (gray line) as a function of the pulse carrier frequency (<bold>F</bold>) and signal-to-noise ratio (SNR) (<bold>G</bold>). Red areas show the distributions of carrier frequencies (<bold>F</bold>) and SNRs (<bold>G</bold>) for all pulses. (<bold>H</bold>) Same as in C but for sine. Color indicates the percentage (see color bar) and text labels indicate seconds of sine for each quadrant. (<bold>I</bold>) Same as in D but for sine. The blue circle depicts the performance for the confidence threshold of 0.5 used in A. (<bold>J</bold>) Distribution of temporal errors for all detected sine on- and offsets. Median temporal error is 9 ms for <italic>DAS</italic> (blue line) and 14 ms for <italic>FSS</italic> (gray line). (<bold>K, L</bold>) Recall for <italic>DAS</italic> (blue line) and <italic>FSS</italic> (gray line) as a function of sine duration (<bold>K</bold>) and SNR (<bold>L</bold>). Blue-shaded areas show the distributions of durations (<bold>K</bold>) and SNRs (<bold>L</bold>) for all sine songs. <italic>DAS</italic> outperforms <italic>FSS</italic> for sine songs with short durations and SNRs &lt;1.0.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig1-figsupp3-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title><italic>DAS</italic> accurately annotates song from a diverse range of species</title><sec id="s2-2-1"><title>Fly courtship song</title><p>We first tested <italic>DAS</italic> on the courtship song of <italic>Drosophila melanogaster</italic>, which consists of two major modes (<xref ref-type="fig" rid="fig1">Figure 1A</xref>): The sine song, which corresponds to sustained oscillations with a species-specific carrier frequency (150 Hz), and two types of pulse song, which consists of trains of short (5–10 ms) pulses with carrier frequencies between 180 and 500 Hz, produced with a species-specific interval (35–45 ms in <italic>D. melanogaster</italic>). Males dynamically choose the song modes based on sensory feedback from the female (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Calhoun et al., 2019</xref>). Despite the relative simplicity of the individual song elements, an accurate annotation of fly song is challenging because of low signal-to-noise ratio (SNR): The song attenuates rapidly with distance (<xref ref-type="bibr" rid="bib9">Bennet-Clark, 1998</xref>) and is highly directional (<xref ref-type="bibr" rid="bib62">Morley et al., 2018</xref>), which can lead to weak signals if the male is far from the microphone (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Moreover, the interactions between the flies introduce pulsatile noise and complicate the accurate and complete annotation of the pulse song.</p><p>We first trained <italic>DAS</italic> to detect the pulse and the sine song recorded using a single microphone (data from <xref ref-type="bibr" rid="bib75">Stern, 2014</xref>) and compared the performance of <italic>DAS</italic> to that of the current state-of-the-art in fly song segmentation, <italic>FlySongSegmenter</italic> (<italic>FSS</italic>) (<xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>). Annotation performance was quantified using <italic>precision</italic>, the fraction of correct detections, and <italic>recall</italic>, the fraction of true song that is detected (<xref ref-type="fig" rid="fig1">Figure 1E,J</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F,G</xref>). We counted detected pulses within 10 ms of a true pulse as correct detections. Ten ms corresponds to 1/4th of the typical interval between pulses in a train and results are robust to the choice of this value (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). <italic>DAS</italic> detects pulses with a high precision of 97% - only 3% of all detected pulses are false detections - and a high recall of 96% - it misses only 4% of all pulses. This is a substantial improvement in recall over <italic>FSS</italic>, which has slightly higher precision (99%) but misses 13% of all pulses (87% recall) (<xref ref-type="fig" rid="fig1">Figure 1D,E</xref>). In <italic>DAS</italic>, the balance between precision and recall can be controlled via the confidence threshold, which corresponds to the minimal confidence required for labeling a pulse (<xref ref-type="fig" rid="fig1">Figure 1C</xref>): Lowering this threshold from 0.7 to 0.5 yields a recall of 99% for pulse song with a modest reduction in precision to 95%. The performance gain of <italic>DAS</italic> over <italic>FSS</italic> for pulse stems from better recall at high frequencies (&gt;400 Hz) and low SNR (<xref ref-type="fig" rid="fig1">Figure 1G,H</xref>). To assess <italic>DAS</italic> performance for sine song, we evaluated the sample-wise precision and recall. <italic>DAS</italic> has similar precision to <italic>FSS</italic> (92% vs 91%) but higher recall (98% vs. 91%) (<xref ref-type="fig" rid="fig1">Figure 1I,J</xref>). Recall is higher in particular for short sine songs (&lt;100 ms) and at low SNR (&lt;1.0) (<xref ref-type="fig" rid="fig1">Figure 1L,M</xref>). The performance boost for pulse and sine arises because <italic>DAS</italic> exploits context information, similar to how humans annotate song: For instance, <italic>DAS</italic> discriminates soft song pulses from pulsatile noise based on the pulse shape but also because song pulses occur in regular trains while noise pulses do not (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). A comparison of <italic>DAS</italic>’ performance to that of human annotators reveals that our methods exceeds human-level performance for pulse and sine (<xref ref-type="fig" rid="fig1">Figure 1E,J</xref>, <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Comparison to human annotators for fly song.</title><p>See also <xref ref-type="fig" rid="fig1">Figure 1E,J</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Annotator</th><th>Sine recall [%]</th><th>Sine precision [%]</th><th>Pulse recall [%]</th><th>Pulse precision [%]</th></tr></thead><tbody><tr><td><bold>Human A</bold></td><td>89</td><td>98</td><td>99</td><td>93</td></tr><tr><td><bold>Human B</bold></td><td>93</td><td>91</td><td>98</td><td>88</td></tr><tr><td><bold><italic>FSS</italic></bold></td><td>91</td><td>91</td><td>87</td><td>99</td></tr><tr><td><bold><italic>DAS</italic></bold></td><td>98</td><td>92</td><td>96</td><td>97</td></tr></tbody></table></table-wrap><p>Temporally precise annotations are crucial, for instance when mapping sensorimotor transformations based on the timing of behavioral or neuronal responses relative to individual song elements (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="bibr" rid="bib74">Srivastava et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Long and Fee, 2008</xref>; <xref ref-type="bibr" rid="bib8">Benichov and Vallentin, 2020</xref>). We therefore quantified the temporal error of the annotations produced by <italic>DAS</italic>. For pulse song, the temporal error was taken as the distance of each pulse annotated by <italic>DAS</italic> to the nearest true pulse. The median temporal error for pulse is 0.3 ms which is negligible compared to the average duration of a pulse (5–10 ms) or of a pulse interval (35–45 ms) (<xref ref-type="bibr" rid="bib26">Deutsch et al., 2019</xref>). For sine song, the median temporal error for on- and offsets was 12 ms, which is almost half of that of <italic>FSS</italic> (22 ms). Sine song can have low SNR (<xref ref-type="fig" rid="fig1">Figure 1M</xref>) and fades in and out, making the precise identification of sine song boundaries difficult even for experienced manual annotators (see <xref ref-type="fig" rid="fig1">Figure 1A,C</xref>).</p><p>Recording song during naturalistic interactions in large behavioral chambers often requires multiple microphones (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="bibr" rid="bib64">Neunuebel et al., 2015</xref>). To demonstrate that <italic>DAS</italic> can process multi-channel audio, we trained <italic>DAS</italic> to annotate recordings from a chamber tiled with nine microphones (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). <italic>DAS</italic> processes multi-channel audio by using filters that take into account information from all channels simultaneously. As is the case for existing methods (<xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>), we achieved maximal performance by training separate networks for the pulse and for the sine song (<xref ref-type="table" rid="table2">Table 2</xref>). In multi-channel recordings, <italic>DAS</italic> annotates pulse song with 98% precision and 94% recall, and sine song with 97% precision and 93% recall, and matches the performance of <italic>FSS</italic> (<italic>FSS</italic> pulse precision/recall 99/92%, sine 95/93%) (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3D–L</xref>). Annotations of multi-channel audio have high temporal precision for pulse (<italic>DAS</italic> 0.3 ms, <italic>FSS</italic> 0.1 ms) and sine (<italic>DAS</italic> 8 ms, <italic>FSS</italic> 15 ms) (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3E,J</xref>). Overall, <italic>DAS</italic> performs better or as well as the current state-of-the-art method for annotating single and multi-channel recordings of fly song.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Precision, recall, and temporal error of <italic>DAS</italic>.</title><p>Precision and recall values are sample-wise for all except fly pulse song, for which it is event-wise. The number of classes includes the ‘no song’ class. (p) Pulse, (s) Sine.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Species</th><th>Trained</th><th>Classes</th><th>Threshold</th><th>Precision [%]</th><th>Recall [%]</th><th>Temporal error [ms]</th></tr></thead><tbody><tr><td>Fly single channel</td><td>Pulse (p) and sine (s)</td><td>3</td><td>0.7</td><td>97/92 (p/s)</td><td>96/98 (p/s)</td><td>0.3/12 (p/s)</td></tr><tr><td>Fly multi channel</td><td>Pulse (p)</td><td>2</td><td>0.5</td><td>98</td><td>94</td><td>0.3</td></tr><tr><td>Fly multi channel</td><td>Sine (s)</td><td>2</td><td>0.5</td><td>97</td><td>93</td><td>8</td></tr><tr><td>Mouse</td><td>Female</td><td>2</td><td>0.5</td><td>98</td><td>99</td><td>0.3</td></tr><tr><td>Marmoset</td><td>five male-female pairs</td><td>5</td><td>0.5</td><td>85</td><td>91</td><td>4.4</td></tr><tr><td>Bengalese finch</td><td>four males</td><td>49 (38 in test set)</td><td>0.5</td><td>97</td><td>97</td><td>0.3</td></tr><tr><td>Zebra finch</td><td>one males</td><td>7</td><td>0.5</td><td>98</td><td>97</td><td>1.2</td></tr></tbody></table></table-wrap></sec><sec id="s2-2-2"><title>Mouse ultrasonic vocalizations</title><p>Mice produce ultrasonic vocalizations (USVs) in diverse social contexts ranging from courtship to aggression (<xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>; <xref ref-type="bibr" rid="bib84">Warren et al., 2020</xref>; <xref ref-type="bibr" rid="bib64">Neunuebel et al., 2015</xref>). We tested <italic>DAS</italic> using audio from an intruder assay, in which an anesthetized female was put into the home cage and the USVs produced by a resident female or male were recorded (<xref ref-type="bibr" rid="bib43">Ivanenko et al., 2020</xref>). The female USVs from this assay typically consist of pure tones with weak harmonics and smooth frequency modulations that are often interrupted by frequency steps (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>). The male USVs are similar but also contain complex frequency modulations not produced by the females in this assay (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>). Recording noise from animal movement and interaction as well as the frequency steps often challenge spectral threshold-based annotation methods and tend to produce false positive syllables (<xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>). Moreover, weak signals often lead to missed syllables or imprecisely delimited syllables. We first trained and tested <italic>DAS</italic> on recordings of a female mouse interacting with an anesthetized female intruder (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). <italic>DAS</italic> annotates the female USVs with excellent precision (98%) and recall (99%) (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) and low median temporal error (0.3 ms) (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). <italic>DAS</italic> is robust to noise: Even for weak signals (SNR 1/16) the recall is 90% (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). These performance values are on par with that of methods specialized to annotate USVs (<xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Van Segbroeck et al., 2017</xref>) (see <xref ref-type="table" rid="table3">Table 3</xref>). USVs of female and male residents have similar characteristics (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>) and the female-trained <italic>DAS</italic> network also accurately annotated the male vocalizations (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). Notably, even the male syllables with characteristics not seen in females in the paradigm were detected (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Overall, <italic>DAS</italic> accurately and robustly annotates mouse USVs and generalizes across sexes.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>DAS performance for mouse ultrasonic vocalizations.</title><p>(<bold>A</bold>) Waveform (top) and spectrogram (bottom) of USVs produced by a female mouse in response to an anesthetized female intruder. Shaded areas (top) show manual annotations. (<bold>B</bold>) Confidence scores (top) and <italic>DAS</italic> and manual annotations (bottom) for the female USVs in A. Brief gaps in confidence are filled smooth annotations. (<bold>C</bold>) Example of male USVs with sex-specific characteristics produced in the same assay. (<bold>D</bold>) Confidence scores (top) and <italic>DAS</italic> and manual annotations (bottom) for the male USVs in C from a <italic>DAS</italic> network trained to detect female USVs. (<bold>E</bold>) Confusion matrix from a female-trained network for a test set of female USVs. Color indicates the percentage (see color bar) and text labels the seconds of song in each quadrant. (<bold>F</bold>) Distribution of temporal errors for syllable on- and offsets in female USVs. The median temporal error is 0.3 ms for <italic>DAS</italic> (brown line) and 0.4 ms for <italic>USVSEG</italic> <xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref>, a method developed to annotate mouse USVs (gray line). (<bold>G</bold>) Recall of the female-trained network (brown line) as a function of SNR. The brown shaded area represents the distribution of SNRs for all samples containing USVs. Recall is high even at low SNR. (<bold>H</bold>) Confusion matrix of the female-trained <italic>DAS</italic> network for a test set of male USVs (see <bold>C, D</bold> for examples). Color indicates the percentage (see color bar) and text labels the seconds of song in each quadrant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Performance for marmoset vocalizations.</title><p>(<bold>A, C</bold>) Waveform (top) and spectrogram (bottom) of vocalizations from male and female marmosets. Shaded areas (top) show manual annotations of the different vocalization types, colored by type. Recordings are noisy (A, left), clipped (orange), and individual vocalization types are variable (<bold>C</bold>). (<bold>B, D</bold>) <italic>DAS</italic> and manual annotation labels for the vocalizations types in the recordings in A and C (see color bar in C). <italic>DAS</italic> annotates the syllable boundaries and types with high accuracy. Note the false negative in D. (<bold>E, F</bold>) Confusion matrices for the four vocalization types in the test set (see color bar), using the syllable boundaries from the manual annotations (<bold>E</bold>) or from <italic>DAS</italic> (<bold>F</bold>) as reference. Rows depict the probability with which <italic>DAS</italic> annotated each syllable as one of the four types in the test dataset. The type of most syllables were correctly annotated, resulting in the concentration of probability mass along the main diagonal. False positives and false negatives correspond to the first row in E and the first column in F, respectively. When using the true syllables for reference, there are no false positives (<bold>E</bold>, x=‘noise’, gray labels) since all detections are positives. By contrast, when using the predicted syllables as reference, there are no true negatives (<bold>F</bold>, y=‘noise’, gray labels), since all reference syllables are (true or false) positives. (<bold>G</bold>) Distribution of temporal errors for the on- and offsets of all detected syllables (purple-shaded area). The median temporal error is 4.4 ms for <italic>DAS</italic> (purple line) and 12.5 ms for the method by <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref> developed to annotate marmoset calls (gray line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig2-figsupp1-v1.tif"/></fig></fig-group><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Comparison to alternative methods.</title><p>Methods used for comparisons: (1) <xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>, (2) <xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref>, (3) <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>, (4) <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>. (A,B) <italic>DAS</italic> was trained on 1825/15970 syllables which contained 4/7 of the call types from <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>. (B) The method by <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref> produces an annotation every 50 ms of the recording - since the on/offset can occur anywhere within the 50 ms, the expected error of the method by <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref> is at least 12.5 ms. (C) The method by <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref> annotates 60 minutes of recordings in 8 minutes. (D) Throughput assessed on the CPU, since the methods by <xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref> and <xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref> do not run on a GPU. (E) Throughput assessed on the GPU. The methods by <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref> and <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref> use a GPU.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">Precision [%]</th><th colspan="2">Recall [%]</th><th colspan="2">Jitter [ms]</th><th colspan="2">Throughput [s/s]</th></tr><tr><th>Species</th><th><italic>DAS</italic></th><th>Other</th><th><italic>DAS</italic></th><th>Other</th><th><italic>DAS</italic></th><th>Other</th><th><italic>DAS</italic></th><th>Other</th></tr></thead><tbody><tr><td>Fly single (1)</td><td>97/92 (p/s)</td><td>99/91</td><td>96/98 (p/s)</td><td>87/91</td><td>0.3/12 (p/s)</td><td>0.1/22</td><td>15</td><td>4 (D)</td></tr><tr><td>Fly multi (1)</td><td>98</td><td>99</td><td>94</td><td>92</td><td>0.3</td><td>0.1</td><td>8 (p)</td><td>0.4 (p+s) (D)</td></tr><tr><td>Fly multi (1)</td><td>97</td><td>95</td><td>93</td><td>93</td><td>8.0</td><td>15.0</td><td>8 (s)</td><td>0.4 (p+s) (D)</td></tr><tr><td>Mouse (2)</td><td>98</td><td>98</td><td>99</td><td>99</td><td>0.3</td><td>0.4</td><td>12</td><td>4 (D)</td></tr><tr><td>Marmoset (3)</td><td>96</td><td>85 (A)</td><td>92</td><td>77 (A)</td><td>4.4</td><td>12.5 (B)</td><td>82</td><td>7.5 (C, E)</td></tr><tr><td>Bengalese finch (4)</td><td>99</td><td>99</td><td>99</td><td>99</td><td>0.3</td><td>1.1</td><td>15</td><td>5 (E)</td></tr><tr><td>Zebra finch (4)</td><td>100</td><td>100</td><td>100</td><td>100</td><td>1.3</td><td>2.0</td><td>18</td><td>5 (E)</td></tr></tbody></table></table-wrap></sec><sec id="s2-2-3"><title>Marmoset vocalizations</title><p>We next examined the robustness of annotations produced by <italic>DAS</italic> to noisy recordings and variable vocalization types, by training a network to annotate vocalization from pairs of marmosets (<xref ref-type="bibr" rid="bib51">Landman et al., 2020</xref>). The recordings contain lots of background noises like faint calls from nearby animals, overdrive from very loud calls of the recorded animals, and large variability within syllable types (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A–D</xref>). Recently, a deep-learning-based method was shown to produce good performance (recall 77%, precision 85%, 12.5 ms temporal error) when trained on 16,000 syllables to recognize seven vocalization types (<xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>). We trained <italic>DAS</italic> on 1/9th of the data (1800 syllables) containing four of the seven vocalization types. Despite the noisy and variable vocalizations, <italic>DAS</italic> achieves high syllable-wise precision and recall (96%, 92%, (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E,F</xref>)). Note that <italic>DAS</italic> obtains this higher performance at millisecond resolution (temporal error 4.4 ms, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1G</xref>), while the method by <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref> only produces annotations with a resolution of 50 ms (<xref ref-type="table" rid="table2">Table 2</xref>).</p></sec><sec id="s2-2-4"><title>Bird song</title><p>Bird song is highly diverse and can consist of large, individual-specific repertoires. The spectral complexity and large diversity of the song complicates the annotation of syllable types. Traditionally, syllable types are annotated based on statistics derived from the segmented syllable spectrogram. Recently, good annotation performance has been achieved with unsupervised methods (<xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>) and deep neural networks (<xref ref-type="bibr" rid="bib49">Koumura and Okanoya, 2016</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>). We first trained <italic>DAS</italic> to annotate the song from four male Bengalese finches (data and annotations from <xref ref-type="bibr" rid="bib65">Nicholson et al., 2017</xref>). The network was then tested on a random subset of the recordings from all four individuals which contained 37 of the 48 syllable types from the training set (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A,B</xref>). <italic>DAS</italic> annotates the bird song with high accuracy: Sample-wise precision and recall are 97% and syllable on- and offsets are detected with sub-millisecond precision (median temporal error 0.3 ms, <xref ref-type="fig" rid="fig3">Figure 3C</xref>). The types of 98.5% the syllables are correctly annotated, with only 0.3% false positives (noise annotated as a syllable), 0.2% false negatives (syllables annotated as noise), and 1% type confusions (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C–D</xref>). This results in a low sequence error (corresponding to the minimal number of substitutions, deletions, or insertions required to transform the true sequence of syllables into the inferred one) of 0.012. Overall, <italic>DAS</italic> performs as well as specialized deep-learning-based methods for annotating bird song (<xref ref-type="bibr" rid="bib49">Koumura and Okanoya, 2016</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>, <xref ref-type="table" rid="table2">Table 2</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>DAS performance for the song of Bengalese finches.</title><p>(<bold>A</bold>) Waveform (top) and spectrogram (bottom) of the song of a male Bengalese finch. Shaded areas (top) show manual annotations colored by syllable type. (<bold>B</bold>) <italic>DAS</italic> and manual annotation labels for the different syllable types in the recording in A (see color bar). <italic>DAS</italic> accurately annotates the syllable boundaries and types. (<bold>C</bold>) Confusion matrix for the different syllables in the test set. Color was log-scaled to make the rare annotation errors more apparent (see color bar). Rows depict the probability with which <italic>DAS</italic> annotated each syllable as any of the 37 types in the test dataset. The type of 98.5% of the syllables were correctly annotated, resulting in the concentration of probability mass along the main diagonal. (<bold>D</bold>) Distribution of temporal errors for the on- and offsets of all detected syllables (green-shaded area). The median temporal error is 0.3 ms for <italic>DAS</italic> (green line) and 1.1 ms for TweetyNet <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>, a method developed to annotate bird song (gray line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Performance for the song of Bengalese finches.</title><p>(<bold>A, B</bold>) Number of syllables (log scaled) in the train (<bold>A</bold>) and the test (<bold>B</bold>) sets for each syllable type present in the test set. (<bold>C</bold>) Precision (blue) and recall (orange) for each syllable type, computed from the confusion matrix in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. (<bold>D</bold>) Confusion matrices when using true (left, same as <xref ref-type="fig" rid="fig3">Figure 3C</xref>) and predicted (right) syllables as a reference. Colors were log-scaled to make the rare annotation errors more apparent (see color bar). The reference determines the syllable bounds and the syllable label is then given by the most frequent label found in the samples within the syllable bounds. When using the true syllables for reference, there are no false positives (left, y=0 (no song), gray line) since all detections are positives. By contrast, when using the predicted syllables as reference, there are no true negatives (right, x=0 (no song), gray line), since all reference syllables are (true or false) positives. The average false negative and positive rates are 0.3% and 0.2%, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Performance for the song of a Zebra finch.</title><p>(<bold>A</bold>) Waveform (top) and spectrogram (bottom) of the song of a male Zebra finch. Shaded areas (top) show manual annotations of the six syllables of the male’s motif, colored by syllable type. (<bold>B</bold>) <italic>DAS</italic> and manual annotation labels for the six syllable types in the recording in A (see color bar). <italic>DAS</italic> accurately annotates the syllable boundaries and types. (<bold>C</bold>) Confusion matrix for the six syllables in the test set (see color bar). Rows depict the probability with which <italic>DAS</italic> annotated each syllable as any of the six types in the test dataset. The type of 100% (54/54) of the syllables were correctly annotated, resulting in the concentration of probability mass along the main diagonal. (<bold>D</bold>) Distribution of temporal errors for the on- and offsets of all detected syllables in the test set (blue-shaded area). The median temporal error is 1.3 ms for <italic>DAS</italic> (blue line) and 2 ms for <italic>TweetyNet</italic> (<xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>), a method developed to annotate bird song (gray line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig3-figsupp2-v1.tif"/></fig></fig-group><p>To further demonstrate the robustness of <italic>DAS</italic>, we trained a network to annotate song from Zebra finches. In Zebra finch males, individual renditions of a given syllable type tend to be more variable (<xref ref-type="bibr" rid="bib30">Fitch et al., 2002</xref>). Moreover, the particular recordings used here (<xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>) contain background noise from the bird’s movement. Despite the variability and noise, <italic>DAS</italic> annotates the six syllables from a male’s main motif with excellent precision and recall, and low temporal error, demonstrating that <italic>DAS</italic> is robust to song variability and recording noise (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>In summary, <italic>DAS</italic> accurately and robustly annotates a wide range of signals—from the pulsatile song pulses of flies to the spectrally complex syllables of mammals and birds. <italic>DAS</italic> therefore constitutes a universal method for annotating acoustic signals that is as good as or better than methods specialized for particular types of signals (<xref ref-type="table" rid="table2">Table 2</xref>).</p></sec></sec><sec id="s2-3"><title><italic>DAS</italic> is fast</title><p>To efficiently process large corpora of recordings and to be suitable for closed-loop applications, <italic>DAS</italic> needs to infer annotations quickly. We therefore assessed the throughput and latency of <italic>DAS</italic>. Throughput measures the rate at which <italic>DAS</italic> annotates song and high throughput means that large datasets are processed quickly. Across the five species tested here, <italic>DAS</italic> has a throughput of 8-82x realtime on a CPU and of 24-267x on a GPU (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This means that a 60 min recording is annotated in less than five minutes on a standard desktop PC and in less than 1.5 minutes using a GPU, making the annotation of large datasets feasible (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A–H</xref>). The differences in throughput arise from the different sample rates and network architectures: The marmoset network is fastest because of a relatively shallow architecture with only 2 TCN blocks and a low sampling rate (44.1 kHz). By contrast, the multi-channel <italic>Drosophila</italic> networks have the lowest throughput because of multi-channel inputs (9 channels at 10.0 kHz) and a comparatively deep architecture with four TCN blocks (<xref ref-type="table" rid="table4">Table 4</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>DAS annotates song with high throughput and low latency and requires little data.</title><p>(<bold>A, B</bold>) Throughput (<bold>A</bold>) and latency (<bold>B</bold>) of <italic>DAS</italic> (see also <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Throughput (<bold>A</bold>) was quantified as the amount of audio data in seconds annotated in one second of computation time. Horizontal lines in A indicate throughputs of 1, 10, 40, and 100. Throughput is &gt;8x realtime on a CPU (dark shades) and &gt;24x or more on a GPU (light shades). Latency (<bold>B</bold>) corresponds to the time it takes to annotate a single chunk of audio and is similar on a CPU (dark shades) and a GPU (light shades). Multi-channel audio from flies was processed using separate networks for pulse and sine. For estimating latency of fly song annotations, we used networks with 25 ms chunks, not the 410 ms chunks used in the original network (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). (<bold>C</bold>) Flow diagram of the iterative protocol for fast training <italic>DAS</italic>. (<bold>D</bold>) Number of manual annotations required to reach 90% of the performance of <italic>DAS</italic> trained on the full data set shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> (see also <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). Performance was calculated as the F1 score, the geometric mean of precision and recall. For most tasks, <italic>DAS</italic> requires small to modest amounts of manual annotations. (<bold>E</bold>) Current best validation loss during training for fly pulse song recorded on a single channel for 10 different training runs (red lines, 18 min of training data). The network robustly converges to solutions with low loss after fewer than 15 min of training (40 epochs).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Throughput and latency of inference.</title><p>Median throughput (<bold>A–H</bold>) and latency (<bold>I–P</bold>) across 10 inference runs for different song types. Batch duration is the product of chunk duration and batch size. Error bars are smaller than the marker size (see <xref ref-type="fig" rid="fig4">Figure 4A/B</xref>). Solid and dashed lines correspond to values when running <italic>DAS</italic> on a CPU and GPU, respectively. Squares and circles in the plots for fly song correspond to networks with short and long chunk durations, respectively (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). In <bold>A-H</bold>, horizontal lines mark 10x and 100x realtime and vertical lines mark the batch duration used in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. In <bold>I-P</bold>, the vertical line marks the batch duration used in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. Differences in the throughput and latency between species arise from differences in the sample rate and the complexity of the network (number and duration of the filters, number of TCN blocks) (see <xref ref-type="table" rid="table4">Table 4</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Reducing the chunk duration reduces latency and comes with minimal performance penalties.</title><p>(<bold>A</bold>) F1 scores of networks with different chunk durations. Reduction of performance due to loss of context information with shorter chunks is minimal. (<bold>B</bold>) Latency of networks with different chunk durations. Dashed vertical lines indicate the chunk durations of the networks in <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. Solid vertical lines indicated the ‘short’ chunk durations used in <xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title><italic>DAS</italic> requires small to moderate amounts of data for training.</title><p>(<bold>A</bold>) Performance as a function of the number of manual annotations in the training set. Individual dots correspond to individual fits using different subsets of the data. For flies and mice, dots indicate the number of annotations for each type and performance is given by the F1 score—the geometric mean of precision and recall. For the marmosets and birds (rightmost panels), dots correspond to the median number of syllables per type in the training set and performance is given by the median accuracy over syllables per fit (see C for per-type statistics for Bengalese finches). Colored curves in A and B were obtained by locally weighted scatter plot smoothing (lowess) of the individual data points. Vertical lines correspond to the number of syllables required to surpass 90% of the maximal performance. (<bold>B</bold>) Temporal error given as the median temporal distance to syllable on- and offsets or to pulses. (<bold>C</bold>) Number of annotations required per syllable type for Bengalese finches (range 2–64, median 8, mean 17). One outlier type requires 200 annotations and consists of a mixture of different syllable types (black bar, see D-F). (<bold>D, E</bold>) UMAP embedding of the bird syllables with the outlier type from C labeled according to the manual annotations (D, black) and the unsupervised clustering (<bold>F</bold>). The unsupervised clustering reveals that the outlier type splits into two distinct clusters of syllables (cyan and pink) and three mislabeled syllables (red). (<bold>F</bold>) Spectrograms of different types of syllable exemplars from the outlier type in C grouped based on the unsupervised clustering in E. (<bold>G</bold>) Confusion matrix mapping manual to unsupervised cluster labels (see <xref ref-type="fig" rid="fig5">Figure 5F,G</xref>) for marmosets. Green boxes mark the most likely call type for each unsupervised cluster. While there is a good correspondence between manual and unsupervised call types, most call types are split into multiple clusters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Example of fast training mouse USVs.</title><p><italic>DAS</italic> can be trained using an iterative procedure, in which the network is trained after annotating few syllables. This initial network is then used to create a larger dataset of annotations, by predicting and manually correcting annotation labels for a longer stretch of audio. This procedure is repeated to create ever larger training datasets until the performance is satisfactory. Shown is an example from mouse USVs. (<bold>A</bold>) Example of the test recording for mouse USVs (top - waveform, bottom - spectrogram) (<bold>B</bold>) Manual labels (true, black) and correct (blue) and erroneous (red) annotations from <italic>DAS</italic> for three different stages of training. Even for the first round of fast training, the majority of onsets is detected with low temporal error, requiring only few corrections. Number of syllables, seconds of song and F1-score for the different iterations (1st/2nd/final): 72/248/2706 syllables, 6/26/433 s of song, F1 score 96.6/97.9/98.9%.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title><italic>DAS</italic> performance is robust to changes in the structural parameters of the network.</title><p>(<bold>A, B</bold>) Performance (F1 score) for networks with different structural parameters for fly pulse (A, red) and sine (B, blue). The starting point for each modified network was the network given in <xref ref-type="table" rid="table4">Table 4</xref>. We then trained networks with modified structural parameters from scratch. As long as the network has chunks longer than 100 ms, filters longer than 2 ms, more than eight filters, and at least one TCN block, the resulting model has an F1 score of 95% for pulse and sine. (<bold>C</bold>) Same in in B but for Bengalese finches. Network performance is robust to changes in structural parameters. Convergence is inconsistent for shallow networks with 1–2 TCN blocks (rightmost panel). F1 scores smaller than 80% in <bold>A-C</bold> where set to 80% to highlight small changes in performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig4-figsupp5-v1.tif"/></fig></fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>DAS can be combined with unsupervised methods for song classification.</title><p>(<bold>A</bold>) Low-dimensional representation obtained using the UMAP (<xref ref-type="bibr" rid="bib59">McInnes and Healy, 2018</xref>) method of all pulse and sine song waveforms from <italic>Drosophila melanogaster</italic> annotated by <italic>DAS</italic> in a test data set. Data points correspond to individual waveforms and were clustered into three distinct types (colors) using the density-based method HDBSCAN (<xref ref-type="bibr" rid="bib58">McInnes et al., 2017</xref>). (<bold>B, C</bold>) All waveforms (<bold>B</bold>) and cluster centroids (<bold>C</bold>) from A colored by the cluster assignment. Waveforms cluster into one sine (blue) and two pulse types with symmetrical (red, <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and asymmetrical (orange, <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) shapes. (<bold>D, E</bold>) Low-dimensional representation of the spectrograms of mouse USVs (<bold>D</bold>) and mean spectrogram for each cluster in D (<bold>E</bold>). Individual syllables (points) form a continuous space without distinct clusters. Song parameters vary continuously within this space, and syllables can be grouped by the similarity of their spectral contours using k-means clustering. (<bold>F, G</bold>) Low-dimensional representation of the spectrograms of the calls from marmosets (<bold>F</bold>) and mean spectrogram for each cluster in F (<bold>G</bold>). Calls separate into distinct types and density-based clustering (colors) produces a classification of syllables that recovers the manual annotations (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3G</xref>, homogeneity score 0.88, completeness score 0.57, v-score 0.69). Most types split into multiple clusters, reflecting the variability of the call types in marmosets. Colored bars on top of each spectrogram in G correspond to the colors for the individual clusters in F. The dashed line shows the boundary separating trills and twitters. (<bold>H, I</bold>) Low-dimensional representation of the spectrograms of the syllables from one Zebra finch male, mean spectrogram for each cluster in H (I, top), and example of each clustered syllable within the motif (I, bottom). Density-based clustering (colors) recovers the six syllable types forming the male’s motif. Colored bars on top of each spectrogram in I correspond to the colors for the individual clusters in H. (<bold>J, K</bold>) Low-dimensional representation of the spectrograms of the syllables from four Bengalese finch males (<bold>J</bold>) and mean spectrogram for each cluster in J (<bold>K</bold>). Syllables separate into distinct types and density-based clustering (colors) produces a classification of syllables that closely matches the manual annotations (homogeneity score 0.96, completeness score 0.89, v-score 0.92). X-axes of the average spectrograms for each cluster do not correspond to linear time, since the spectrograms of individual syllables were temporally log-rescaled and padded prior to clustering. This was done to reduce the impact of differences in duration between syllables.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68837-fig5-v1.tif"/></fig><p>A measure of speed crucial for closed-loop experiments is latency, which quantifies the time it takes to annotate a chunk of song and determines the delay for experimental feedback. Latencies are short, between 7 and 15 ms (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1I–P</xref>) on CPUs and GPUs. One network parameter impacting latency is the chunk size—the duration of audio processed at once—and we find that for fly song, latency can be optimized by reducing chunk size with a minimal impact on accuracy (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The low latency of annotation makes <italic>DAS</italic> well suited for triggering realtime optogenetic or acoustic feedback upon the detection of specific vocalizations (<xref ref-type="bibr" rid="bib6">Bath et al., 2014</xref>; <xref ref-type="bibr" rid="bib77">Stowers et al., 2017</xref>).</p><p>We also compared the speed of <italic>DAS</italic> to that of other methods that were specifically developed to annotate the types of signals tested here. Since most existing methods are not suitable for estimating latency due to constraints in their design and interface, we only compared throughput. We find that <italic>DAS</italic> achieves 3x to 10x higher throughput than existing methods (<xref ref-type="table" rid="table2">Table 2</xref>). This has three main reasons: First, the relatively simple, purely convolutional architecture exploits the parallel processing capabilities of modern CPUs and GPUs. Second, Fourier or wavelet-like preprocessing steps are integrated into the network and profit from a fast implementation and hardware acceleration. Third, for multi-channel data, <italic>DAS</italic> combines information from all audio channels early, which increases throughput by reducing the data bandwidth.</p><p>Overall, <italic>DAS</italic> annotates audio with high throughput (&gt;8x realtime) and low latency (&lt;15 ms) and is faster than the alternative methods tested here. The high speed renders<italic> DAS</italic> suitable for annotating large corpora and for realtime applications without requiring specialized hardware.</p></sec><sec id="s2-4"><title><italic>DAS</italic> requires little manual annotation</title><p>To be practical, <italic>DAS</italic> should achieve high performance with little manual annotation effort. We find that <italic>DAS</italic> can be efficiently trained using an iterative protocol (<xref ref-type="bibr" rid="bib69">Pereira et al., 2019</xref>, <xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>): Annotate a small set of recordings and train the network for a few epochs; then generate annotations on a larger set of recordings and correct these annotations. Repeat the predict-correct-train cycle on ever larger datasets until performance is satisfactory. To estimate the amount of manual annotations required to achieve high performance, we evaluated <italic>DAS</italic> trained on subsets of the full training data sets used above (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). We then took the number of manual annotations needed to reach 90% of the performance of <italic>DAS</italic> trained on the full data sets as an upper bound on the data requirements (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). With a performance threshold of 90%, the resulting networks will produce sufficiently accurate annotations for creating a larger body of training data with few corrections. Performance was taken as the F1 score, the geometric mean of precision and recall. For single-channel recordings of fly song, fewer than 50 pulses and 20 sine songs are needed to reach 90% of the performance achieved with the full data set. For mouse vocalizations, <italic>DAS</italic> achieves 90% of its peak performance with fewer than 25 manually annotated syllables. Even for the six syllables from a zebra finch, <italic>DAS</italic> reaches the 90% threshold with only 48 manually annotated syllables (eight per type). Manually annotating such small amounts of song for flies, mice, or zebra finches takes less than 5 min. Likewise, for the song of Bengalese finches, 1–51 (median 8, mean 17) manual annotations are required per syllable type, with one outlier requiring 200 syllables (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref>). Closer inspection reveals that the outlier results from an annotation error and consists of a mixture of three distinct syllable types (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3D–F</xref>). Even with this outlier, only 626 manually annotated syllabes (424 without) are required in total to reach 90% of the test performance of a network trained on &gt;3000 annotated syllables. Data requirements are higher for the multi-channel recordings of fly song (270 pulses and sine songs), and for the noisy and variable marmoset data (1610 annotations, 400 per type), but even in these cases, the iterative training protocol can reduce the manual annotation work.</p><p>Overall, <italic>DAS</italic> requires small to moderate amounts of data for reaching high performance. High throughput (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and small training data sets (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) translate to short training times (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). The single-channel data sets typically achieve 90% of the performance after less than 10 min of training on a GPU. Training on the full data sets typically finishes in fewer than five hours. Thus, <italic>DAS</italic> can be adapted to novel species in short time and with little manual annotation work.</p></sec><sec id="s2-5"><title><italic>DAS</italic> can be combined with unsupervised methods</title><p><italic>DAS</italic> is a supervised annotation method: It discriminates syllable types that have been manually assigned different labels during training. By contrast, unsupervised methods can determine in unlabelled data whether syllables fall into distinct types and if so, classify the syllables (<xref ref-type="bibr" rid="bib78">Tabler et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Arthur et al., 2021</xref>). While <italic>DAS</italic> does not require large amounts of manual annotations (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), manual labeling of syllable types can be tedious when differences between syllable types are subtle (<xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>) or when repertoires are large (<xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>). In these cases, combining <italic>DAS</italic> with unsupervised methods facilitates annotation work. To demonstrate the power of this approach, we use common procedures for unsupervised classification, which consist of an initial preprocessing (e.g. into spectograms) and normalization (e.g. of amplitude) of the syllables, followed by dimensionality reduction and clustering (see Materials and methods) (<xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>).</p><p>For fly song, <italic>DAS</italic> was trained to discriminate two major song modes, pulse and sine. However, <italic>Drosophila melanogaster</italic> males produce two distinct pulse types, termed <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>), and unsupervised classification robustly discriminates the two pulse types as well as the sine song in the <italic>DAS</italic> annotations (<xref ref-type="fig" rid="fig5">Figure 5A–C</xref>). Mouse USVs do not fall into distinct types (<xref ref-type="bibr" rid="bib78">Tabler et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>). In this case, unsupervised clustering produces a low-dimensional representation that groups the syllables by the similarity of their spectrograms (<xref ref-type="bibr" rid="bib78">Tabler et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>, <xref ref-type="fig" rid="fig5">Figure 5D,E</xref>). For marmosets, unsupervised classification recovers the four manually defined call types (<xref ref-type="fig" rid="fig5">Figure 5F,G</xref>). However, most call types are split into multiple clusters, and the clusters for trills and twitters tend to separate poorly (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3G</xref>), which reflects the large variability of the marmoset vocalizations. This contrasts with the song of Zebra finches, for which the unsupervised method produces a one-to-one mapping between manually defined and unsupervised syllable types (<xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="fig" rid="fig5">Figure 5H,I</xref>). For the song of Bengalese finches, the unsupervised classification recovers the manual labeling (<xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="fig" rid="fig5">Figure 5J,K</xref>) and reveals manual annotation errors: For instance, the song syllable that required &gt;200 manual annotations to be annotated correctly by <italic>DAS</italic> is a mixture of three distinct syllable types (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C–F</xref>).</p><p>Overall, unsupervised methods simplify annotation work: <italic>DAS</italic> can be trained using annotations that do not discriminate between syllable types and the types can be determined <italic>post hoc</italic>. If distinct types have been established, <italic>DAS</italic> can be retrained to directly annotate these types using the labels produced by the unsupervised method as training data.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We here present <italic>Deep Audio Segmenter</italic> (<italic>DAS</italic>), a method for annotating acoustic signals. <italic>DAS</italic> annotates song in single- and multi-channel recordings from flies (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), mammals ( <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), and birds (Figs <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) accurately, robustly, and quickly (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>). <italic>DAS</italic> performs as well as or better than existing methods that were designed for specific types of vocalizations (<xref ref-type="bibr" rid="bib49">Koumura and Okanoya, 2016</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>, <xref ref-type="table" rid="table2">Table 2</xref>). <italic>DAS</italic> performs excellently for signals recorded on single and multiple channels (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), with different noise levels, and with diverse characteristics. This suggests that <italic>DAS</italic> is a general method for accurately annotating signals from a wide range of recording setups and species.</p><p>Using a user-friendly graphical interface, our method can be optimized for new species without requiring expert knowledge and with little manual annotation work (<xref ref-type="fig" rid="fig4">Figure 4C–E</xref>). Network performance is robust to changes in the structural parameters of the network, like filter number and duration, or the network depth (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). Thus, the structural parameters do <italic>not</italic> need to be finely tuned to obtain a performant network for a new species. We have trained networks using a wide range of signal types (<xref ref-type="table" rid="table4">Table 4</xref>) and these networks constitute good starting points for adapting <italic>DAS</italic> to novel species. We provide additional advice for the design of novel networks in Methods. This makes the automatic annotation and analysis of large corpora of recordings from diverse species widely accessible.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Structural parameters of the tested networks.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Species</th><th>Rate [kHz]</th><th>Chunk [samples]</th><th>Channels</th><th>STFT downsample</th><th>Separable conv.</th><th>TCN stacks</th><th>Kernel size [samples]</th><th>Kernel</th></tr></thead><tbody><tr><td>Fly single channel</td><td>10.0</td><td>4096</td><td>1</td><td>-</td><td>-</td><td>3</td><td>32</td><td>32</td></tr><tr><td>Fly multi channel (pulse)</td><td>10.0</td><td>2048</td><td>9</td><td>-</td><td>TCN blocks 1+2</td><td>4</td><td>32</td><td>32</td></tr><tr><td>Fly multi channel (sine)</td><td>10.0</td><td>2048</td><td>9</td><td>-</td><td>TCN blocks 1+2</td><td>4</td><td>32</td><td>32</td></tr><tr><td>Mouse</td><td>300.0</td><td>8192</td><td>1</td><td>16x</td><td>-</td><td>2</td><td>16</td><td>32</td></tr><tr><td>Marmoset</td><td>44.1</td><td>8192</td><td>1</td><td>16x</td><td>-</td><td>2</td><td>16</td><td>32</td></tr><tr><td>Bengales finch</td><td>32.0</td><td>1024</td><td>1</td><td>16x</td><td>-</td><td>4</td><td>32</td><td>64</td></tr><tr><td>Zebra finch</td><td>32.0</td><td>2048</td><td>1</td><td>16x</td><td>-</td><td>4</td><td>32</td><td>64</td></tr></tbody></table></table-wrap><p>We show that the annotation burden can be further reduced using unsupervised classification of syllable types, in particular for species with large or individual-specific repertoires (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>; <xref ref-type="bibr" rid="bib78">Tabler et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>; <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Arthur et al., 2021</xref>). In the future, incorporating recent advances in the self-supervised or semi-supervised training of neural networks will likely further reduce data requirements (<xref ref-type="bibr" rid="bib56">Mathis et al., 2021</xref>; <xref ref-type="bibr" rid="bib71">Raghu et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Devlin et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Chen and He, 2020</xref>). These approaches use unlabeled data to produce networks with a general and rich representation of sound features that can then be fine-tuned for particular species or individuals using few annotated samples. <italic>DAS</italic> currently does not work well with recordings in which the signals produced by multiple animals overlap. In the future, <italic>DAS</italic> will be extended with methods for multi-speaker speech recognition to robustly annotate vocalizations from animal groups.</p><p>Lastly, the high inference speed (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>) allows integration of <italic>DAS</italic> in closed-loop systems in which song is detected and stimulus playback or optogenetic manipulation is triggered with low latency (<xref ref-type="bibr" rid="bib6">Bath et al., 2014</xref>; <xref ref-type="bibr" rid="bib77">Stowers et al., 2017</xref>). In combination with realtime pose tracking (<xref ref-type="bibr" rid="bib55">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib69">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Graving et al., 2019</xref>), <italic>DAS</italic> provides unique opportunities to tailor optogenetic manipulations to specific behavioral contexts, for instance to dissect the neural circuits underlying acoustic communication in interacting animals (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Fortune et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Okobi et al., 2019</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Instructions for installing and using <italic>DAS</italic> can be found at <ext-link ext-link-type="uri" xlink:href="https://janclemenslab.org/das">https://janclemenslab.org/das</ext-link>. Code for the <italic><monospace>das</monospace></italic> python module is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das">https://github.com/janclemenslab/das</ext-link>, code for the unsupervised methods is at <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das_unsupervised">https://github.com/janclemenslab/das_unsupervised</ext-link>. All fitted models (with example data and code) can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie">https://github.com/janclemenslab/das-menagerie</ext-link>.</p><p>We also provide instructions for training <italic>DAS</italic> using google colab, which provides a GPU-accelerated python environment. Colab removes the need to install GPU libraries: Annotations can be made locally in the GUI without a GPU and training and prediction are done on GPU-accelerated nodes in the cloud. See this notebook for details: <ext-link ext-link-type="uri" xlink:href="http://janclemenslab.org/das/tutorials/colab.html">http://janclemenslab.org/das/tutorials/colab.html</ext-link>.</p><sec id="s4-1"><title>Data sources</title><p>All data used for testing <italic>DAS</italic> were published previously. Sources for the original data sets, for the data and annotations used for training and testing, and for the fitted models are listed in <xref ref-type="table" rid="table5">Table 5</xref>. Single-channel recordings of <italic>Drosophila melanogaster</italic> (strain OregonR) males courting females were taken from <xref ref-type="bibr" rid="bib75">Stern, 2014</xref>. The multi-channel data from <italic>Drosophila melanogaster</italic> (strain NM91) males courting females were recorded in a chamber tiled with nine microphones (<xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>) and was previously published in <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>. Annotations for fly song were seeded with <italic>FlySongSegmenter</italic> (<xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>) and then manually corrected. Recordings of mouse USVs were previously published in <xref ref-type="bibr" rid="bib43">Ivanenko et al., 2020</xref>. The USVs were manually labeled using the <italic>DAS</italic> graphical interface. Marmoset recordings were taken from the data published with <xref ref-type="bibr" rid="bib51">Landman et al., 2020</xref>. Since we required more precise delineation of the syllable boundaries than was provided in the published annotations, we manually fixed annotations for a subset of the data that then was used for training and testing. The network was trained and tested on a subset of four vocalization types (eks/trills/tsiks/twitters, N=603/828/115/868). The remaining vocalization types were excluded since they had 60 or fewer instances in our subset. To test <italic>DAS</italic> on bird songs, we used a publicly available, hand-labeled collection of song from four male Bengalese finches (<xref ref-type="bibr" rid="bib65">Nicholson et al., 2017</xref>) and recordings of female-directed song from a male Zebra finch from <xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref>. For training and testing the Zebra finch network, we manually labeled 473 syllables of six types (320 s of recordings).</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Sources of all data used for testing <italic>DAS</italic>.</title><p>‘Data’ refers to the data used for <italic>DAS</italic> and to annotations that were either created from scratch or modified from the original annotations (deposited under <ext-link ext-link-type="uri" xlink:href="https://data.goettingen-research-online.de/dataverse/das">https://data.goettingen-research-online.de/dataverse/das</ext-link>). ‘Original data’ refers to the recordings and annotations deposited by the authors of the original publication. ‘Model’ points to a directory with the model files as well as a small test data set and demo code for running the model (deposited under <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie">https://github.com/janclemenslab/das-menagerie</ext-link>).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Species</th><th>Reference</th><th>Data and model repositories</th></tr></thead><tbody><tr><td rowspan="3">Fly single channel</td><td rowspan="3"><xref ref-type="bibr" rid="bib75">Stern, 2014</xref></td><td>data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25625/TP4ODR">https://doi.org/10.25625/TP4ODR</ext-link></td></tr><tr><td>original data: <ext-link ext-link-type="uri" xlink:href="https://www.janelia.org/lab/stern-lab/tools-reagents-data">https://www.janelia.org/lab/stern-lab/tools-reagents-data</ext-link></td></tr><tr><td>model: <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie/dmel_single">https://github.com/janclemenslab/das-menagerie/dmel_single</ext-link></td></tr><tr><td rowspan="2">Fly multi channel</td><td rowspan="2"><xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref></td><td>data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25625/8KAKHJ">https://doi.org/10.25625/8KAKHJ</ext-link></td></tr><tr><td>model: <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie/dmel_multi">https://github.com/janclemenslab/das-menagerie/dmel_multi</ext-link></td></tr><tr><td rowspan="3">Mouse</td><td rowspan="3"><xref ref-type="bibr" rid="bib43">Ivanenko et al., 2020</xref></td><td>data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25625/VVSKCH">https://doi.org/10.25625/VVSKCH</ext-link></td></tr><tr><td>original data: <ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891">https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891</ext-link></td></tr><tr><td>model: <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie/mouse">https://github.com/janclemenslab/das-menagerie/mouse</ext-link></td></tr><tr><td rowspan="3">Marmoset</td><td rowspan="3"><xref ref-type="bibr" rid="bib51">Landman et al., 2020</xref></td><td>data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25625/DYG3KV">https://doi.org/10.25625/DYG3KV</ext-link></td></tr><tr><td>original data: <ext-link ext-link-type="uri" xlink:href="https://osf.io/q4bm3/">https://osf.io/q4bm3/</ext-link></td></tr><tr><td>model: <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie/marmoset">https://github.com/janclemenslab/das-menagerie/marmoset</ext-link></td></tr><tr><td rowspan="3">Bengalese finch</td><td rowspan="3"><xref ref-type="bibr" rid="bib65">Nicholson et al., 2017</xref></td><td>data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25625/ENKMJS">https://doi.org/10.25625/ENKMJS</ext-link></td></tr><tr><td>original data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4805749.v6">https://doi.org/10.6084/m9.figshare.4805749.v6</ext-link></td></tr><tr><td>model: <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie/bengalese_finch">https://github.com/janclemenslab/das-menagerie/bengalese_finch</ext-link></td></tr><tr><td rowspan="3">Zebra finch</td><td rowspan="3"><xref ref-type="bibr" rid="bib33">Goffinet et al., 2021</xref></td><td>data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25625/ZXJJJY">https://doi.org/10.25625/ZXJJJY</ext-link></td></tr><tr><td>original data: <ext-link ext-link-type="uri" xlink:href="https://research.repository.duke.edu/concern/datasets/9k41zf38g">https://research.repository.duke.edu/concern/datasets/9k41zf38g</ext-link></td></tr><tr><td>model: <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie/zebra_finch">https://github.com/janclemenslab/das-menagerie/zebra_finch</ext-link></td></tr></tbody></table></table-wrap></sec><sec id="s4-2"><title><italic>DAS</italic> network</title><p><italic>DAS</italic> is implemented in Keras (<xref ref-type="bibr" rid="bib17">Chollet, 2015</xref>) and Tensorflow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>). At its core, <italic>DAS</italic> consists of a stack of temporal convolutional blocks, which transform an input sequence of audio data into an output sequence of labels.</p><sec id="s4-2-1"><title>Inputs</title><p><italic>DAS</italic> takes as input raw, single or multi-channel audio. Pre-processing of the audio using a wavelet or short-time Fourier transform (STFT) is optional and integrated into the network. <italic>DAS</italic> processes audio in overlapping chunks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A–D</xref>). The chunking accelerates annotations since multiple samples are annotated in a single computational step. Edge effects are avoided by processing overlapping chunks and by discarding a number of samples at the chunk boundaries. The overlap depends on the number of layers and the duration of filters in the network.</p></sec><sec id="s4-2-2"><title>STFT frontend</title><p>The trainable STFT frontend is an optional step and was implemented using kapre (<xref ref-type="bibr" rid="bib16">Choi et al., 2017</xref>). Each frequency channel in the output of the frontend is the result of two, one-dimensional strided convolutions which are initialized with the real and the imaginary part of discrete Fourier transform kernels: <disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>∗</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>τ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>τ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℜ</mml:mi><mml:mo>⁡</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℑ</mml:mi><mml:mo>⁡</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf5"><mml:mi>f</mml:mi></mml:math></inline-formula> is the frequency, <inline-formula><mml:math id="inf6"><mml:mi>s</mml:mi></mml:math></inline-formula> is the stride, and <italic>T</italic> is the filter duration. The stride results in downsampling of the input by a factor <inline-formula><mml:math id="inf7"><mml:mi>s</mml:mi></mml:math></inline-formula>.</p><p>The STFT kernels are optimized with all other parameters of the network during training. The STFT frontend was used for mammal and bird signals, but not for fly song. In the mammal and bird networks, we used 33 STFT filter pairs with a duration <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> samples and a stride <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula> samples. For mouse and bird song, the STFT frontend sped up training and inference, and had a small positive impact on performance. For fly song, the STFT frontend tended to reduce performance and was omitted.</p></sec><sec id="s4-2-3"><title>Temporal convolutional blocks</title><p>Temporal convolutional network (TCN) blocks are central to <italic>DAS</italic> and produce a task-optimized hierarchical representation of sound features at high temporal resolution (<xref ref-type="bibr" rid="bib4">Bai et al., 2018</xref>). Each TCN block consists of a stack of so-called residual blocks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>, <xref ref-type="bibr" rid="bib39">He et al., 2016</xref>):</p><p>A <italic>dilated convolutional layer</italic> filters the input with a number of kernels of a given duration: <inline-formula><mml:math id="inf10"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf12"><mml:mi>i</mml:mi></mml:math></inline-formula> th kernel, <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the input on channel <inline-formula><mml:math id="inf14"><mml:mi>γ</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf15"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the output, and <inline-formula><mml:math id="inf17"><mml:mi>a</mml:mi></mml:math></inline-formula> the gap or skip size (<xref ref-type="bibr" rid="bib87">Yu and Koltun, 2016</xref>). In old-fashioned convolution <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Increasing <inline-formula><mml:math id="inf19"><mml:mi>a</mml:mi></mml:math></inline-formula> allows the kernel to span a larger range of inputs with the same number of parameters and without a loss of output resolution. The number of parameters is further reduced for networks processing multi-channel audio, by using <italic>separable</italic> dilated convolutions in the first two TCN blocks (<xref ref-type="bibr" rid="bib54">Mamalet and Garcia, 2012</xref>). In separable convolutions, the full two-dimensional <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> convolution over times and channels is decomposed into two one-dimensional convolutions. First, a temporal convolution, <inline-formula><mml:math id="inf21"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is applied to each channel and then <inline-formula><mml:math id="inf22"><mml:mi>N</mml:mi></mml:math></inline-formula> channel convolutions, <inline-formula><mml:math id="inf23"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mi>γ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, combine information across channels. Instead of <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>×</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> parameters, the separable convolution only requires <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> parameters. Note that each temporal convolution is applied to each channel, leading to a sharing of filter parameters across channels. This makes explicit the intuition that some operations should be applied to all channels equally. We also tested an alternative implementation, in which individual channels were first processed separately by a single-channel TCN, the outputs of the TCN blocks for each channel were concatenated, and then fed into a stack of standard TCNs with full two-dimensional convolutions. While this architecture slightly increased performance it was also much slower and we therefore chose the architecture with separable convolutions. Architecture choice ultimately depends on speed and performance requirements of the annotation task.</p><p>A <italic>rectifying linear unit</italic> transmits only the positive inputs from the dilated convolutional layer by setting all negative inputs to 0: <inline-formula><mml:math id="inf26"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>A <italic>normalization layer</italic> rescales the inputs to have a maximum absolute value close to 1: <inline-formula><mml:math id="inf27"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The output of the residual block, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is then routed to two targets: First, it is added to the input: <inline-formula><mml:math id="inf29"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and fed into subsequent layers. Second, via so-called skip connections, the outputs of all residual blocks are linearly combined to produce the network's final output (<xref ref-type="bibr" rid="bib81">van den Oord et al., 2016</xref>).</p><p>A single TCN block is composed of a stack of five residual blocks. Within a stack, the skip size <inline-formula><mml:math id="inf30"><mml:mi>a</mml:mi></mml:math></inline-formula> doubles - from one in the first to <inline-formula><mml:math id="inf31"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>5</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula> in the final residual block of a stack. This exponential increase in the span of the filter <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>*</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> allows the TCN block to produce a hierarchical representation of its inputs, from relatively low-level features on short timescales in early stacks to more derived features on longer timescales in late stacks. Finally, a full network consists of a stack of 2 to 4 TCN blocks, which extract ever more derived features (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A–D</xref>).</p></sec><sec id="s4-2-4"><title>Output</title><p>The network returns a set of confidence scores—one for each song type (and for 'no song')—for each sample from a linear combination of the output of each residual block in the network, by using a single dense layer and a softmax activation function. Re-using information from all blocks via so-called skip connections ensures that downstream layers can discard information from upstream layers and facilitates the generation of specialized higher order presentations. If the input recording got downsampled by a STFT frontend, a final upsampling layer restores the confidence scores to the original audio rate by repeating values. The parameters of all used networks are listed in <xref ref-type="table" rid="table4">Table 4</xref>.</p></sec></sec><sec id="s4-3"><title>Choice of structural network parameters</title><p><italic>DAS</italic> performance is relatively robust to the choice of structural network parameters like filter duration and number, or network depth (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). The networks tested here are good starting points for adapting <italic>DAS</italic> to your own data (<xref ref-type="table" rid="table4">Table 4</xref>). In our experience, a network with 32 filters, filter duration 32 samples, 3 TCN blocks, and a chunk duration of 2048 samples will produce good results for most signals. A STFT downsampling layer with 32 frequency bands and 16x downsampling should be included for most signals except when the signals have a pulsatile character. These parameters have been set as defaults when creating a new <italic>DAS</italic> network. Given that <italic>DAS</italic> trains quickly (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), network structure can be optimized by training <italic>DAS</italic> networks over a grid of structural parameters, for instance to find the simplest network (in terms of the number of filters and TCN blocks) that saturates performance but has the shortest latency. We here provide additional guidelines for choosing a network’s key structural parameters:</p><p>The chunk duration corresponds to the length of audio the network processes in one step and constitutes an upper bound for the context available to the network. Choose chunks sufficiently long so that the network has access to key features of your signal. For instance, for fly song, we ensured that a single chunk encompasses several pulses in a train, so the network can learn to detect song pulses based on their regular occurrence in trains. Longer chunks relative to this timescale can reduce short false positive detections, for instance for fly sine song and for bird song. Given that increasing chunk duration does not increase the number of parameters for training, we recommend using long chunks unless low latency is of essence (see below).</p><p>Downsampling/STFT weakly affects performance but strongly accelerates convergence during training. This is because (A) the initialization with STFT filters is a good prior that reduces the number of epochs it takes to learn the optimal filters, and (B) the downsampling reduces the data bandwidth and thereby the time it takes to finish one training epoch. The overall increase in performance from adding the STFT layer is low because convolutional layers in the rest of the network can easily replicate the computations of the STFT layer. For short pulsatile signals or signals with low sampling rates, STFT and downsampling should be avoided since they can decrease performance due to the loss of temporal resolution.</p><p>The number of TCN blocks controls the network’s depth. A deeper network can extract more high-level features, though we found that even for the spectro-temporally complex song of Bengalese finches, deeper networks only weakly improved performance (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>).</p><p>Multi-channel audio can be processed with multi-channel filters via full convolutions or with shared channel-wise filters via time-channel separable convolutions. This can be set on a per-TCN-block basis. We recommend to use separable convolutions in the first 1–2 layers, since basic feature extraction is typically the same for each channel. Later layers can then have full multi-channel filters to allow more complex combination of information across channels.</p><p>Real-time performance can be optimized by reducing networks complexity and chunk duration (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). We recommend starting with the default parameters suggested above and then benchmarking latency. If required, latency can then be further reduced by reducing chunk duration, the number and duration of filters, and the number of TCN blocks.</p></sec><sec id="s4-4"><title>Training</title><p>Networks were trained using the categorical cross-entropy loss and the Adam optimizer (<xref ref-type="bibr" rid="bib45">Kingma and Ba, 2015</xref>) with a batch size of 32. Prediction targets were generated from fully annotated recordings and one-hot-encoded: Segments were coded as binary vectors, with <inline-formula><mml:math id="inf33"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if a segment of type <inline-formula><mml:math id="inf34"><mml:mi>i</mml:mi></mml:math></inline-formula> occurred at time <inline-formula><mml:math id="inf35"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. To encode uncertainty in the timing of fly song pulses, the pulses were represented as Gaussian bumps with a standard deviation of 1.6 ms. A 'no song' type was set to <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mtext>no song</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. That way, <inline-formula><mml:math id="inf38"><mml:mi>y</mml:mi></mml:math></inline-formula> corresponds to the probability of finding any of the annotated song types or no song. For bird song, short gaps (6.25 ms, 200 samples at 32 kHz) were introduced between adjacent syllables to aid the detection of syllable on- and offsets after inference. That way, syllable on- and offsets could be unequivocally detected as changes from ‘no song’ to any of the syllables. This reduced the amount of false positive on- and offsets from switches in the inferred label within a syllable.</p><p>Typically, multiple fully annotated recordings were combined in a data set. Each recording was split 80:10:10 into a training, validation, and test set. The validation and test data were randomly taken from the first, the middle or the last 10% of each recording. Given the uneven temporal distribution of call types in the marmoset recordings, we split the data 60:20:20 to ensure that each call type was well represented in each split. For all networks, training was set to stop after 400 epochs or earlier if the validation loss was not reduced for at least 20 epochs. Training typically stopped within 40–80 epochs depending on the dataset. The test set was only used after training, for evaluating the model performance.</p></sec><sec id="s4-5"><title>Generation of annotations from the network output</title><p>The confidence scores produced by the model correspond to the sample-wise probability for each song type. To produce an annotation label for each sample, the confidence scores were further processed to extract event times and syllable segments. In the resulting annotations, song types are mutually exclusive, that is, each sample is labeled as containing a single song type even if song types overlap.</p><p>Event times for event-like song types like fly pulse song were determined based on local maxima in the confidence score, by setting a threshold value between 0 and 1 and a minimal distance between subsequent peaks (using <monospace>peakutils,</monospace> <xref ref-type="bibr" rid="bib63">Negri and Vestri, 2017</xref>). For the pulse song of flies, we set a minimal distance of 10 ms and a threshold of 0.7 for single channel data (<xref ref-type="fig" rid="fig1">Figure 1</xref>) and 0.5 for multi-channel data (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</p><p>For segment-like song types like fly sine song or the syllables of mouse, marmoset, and bird song, we first transformed the sample-wise probability into a sequence of labels using <inline-formula><mml:math id="inf39"><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mtext>argmax</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The resulting annotation of segments was then smoothed by filling short gaps (flies 20 ms, mice 10 ms, marmosets and birds 5 ms) and removing very short detections (flies 20 ms, mice 5 ms, marmosets and birds 30 ms). These values were chosen based on the statistics of song found in the training data. Syllable on- and offsets were detected as changes from no-song to song and song to no-song, respectively. For bird and marmoset vocalizations, syllable labels were determined based on a majority vote, by calculating the mode of the sample-wise labels for each detected syllable.</p></sec><sec id="s4-6"><title>Evaluation</title><p><italic>DAS</italic> was evaluated on segments of recordings that were not used during training.</p><sec id="s4-6-1"><title>Events</title><p>For events—fly pulse song, or the on- and offsets of segments—we matched each true event with its nearest neighbor in the list of true events and counted as true positives only events within a specified distance from a true event. For the pulse song of flies as well as for the onsets and offsets of mouse, marmoset, and bird syllables, this distance was set to 10 ms. Results were robust to the specific choice of the distance threshold (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). For the onsets and offsets of fly sine song and of the marmoset vocalizations, we set this distance to 40 ms, since these signals tended to fade in and out, making the delineation of exact boundaries difficult. False positive events were counted if the distance from a detected event to the nearest true event exceeded the distance threshold or if another detected event was closer to each true event within the distance threshold. If several detected pulses shared the same nearest true pulses, only the nearest of those was taken as a true positive, while the remaining detections were matched with other true pulses within the distance threshold or counted as false positives.</p><p>False negatives were counted as all true events without nearby detected events. For pulse, pseudo true negative events were estimated as the number of tolerance distances (2x tolerance distance) fitting into the recording, minus the number of pulses. These true negatives for pulse do not influence precision, recall, and F1-scores and are only used to fill the confusion matrices in <xref ref-type="fig" rid="fig1">Figure 1D,I</xref> and <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3C,H</xref>. Pulse and sine song were evaluated only up to the time of copulation.</p></sec><sec id="s4-6-2"><title>Matching segment labels</title><p>For songs with only one syllable type, we compared the predicted and true labels for each sample to compute the confusion matrix (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F,G</xref>). In the case of multiple syllable types, the mode of the true and predicted labels for the samples of each detected syllable were compared. A true positive was counted if the mode of the true labels was the same for the samples covered by the detected syllable. Using the true syllables as reference produces similar results (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E,F</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D,E</xref>).</p></sec><sec id="s4-6-3"><title>Performance scores</title><p>From the false negative (FN), false positive (FP), and true positive (TP) counts we extracted several scores: Precision (P)—the fraction of true positive out of all detections TP/(FP+TP)—and recall (R)—the fraction of true positives out of all positives TP/(TP+FN). The F1 score combines precision and recall via their geometric mean: <inline-formula><mml:math id="inf40"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For datasets with many different syllable types, we also used as a summary measure of performance the accuracy—the fraction of correctly labelled syllables: (TP+TN)/(TP+TN+FP+FN). For comparison with other studies, we additionally provide the error rate for the song of Bengalese finches, which is based on the Levenshtein edit distance and corresponds to the minimal number of inserts, deletions, and substitutions required to transform the sequence of true syllable labels into the sequence of predicted syllable labels normalized by the length of the true sequence (<xref ref-type="bibr" rid="bib49">Koumura and Okanoya, 2016</xref>; <xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>).</p></sec><sec id="s4-6-4"><title>Temporal precision</title><p>The temporal precision for events (pulses, syllable onsets and offsets) was calculated as the median absolute distance between all matched events.</p></sec><sec id="s4-6-5"><title>Annotation errors for Bengalese finches</title><p>The network for annotating bird song was trained on all syllable types. We removed from the test data one syllable type with only a single instance in the test set (which was correctly classified), because the performance could not be assessed reliably based on a single instance. We also excluded as annotation error a syllable type that contained syllables of more than six distinct types.</p></sec></sec><sec id="s4-7"><title>Estimation of signal-to-noise ratio from audio recordings</title><p>To assess the robustness of annotation performance to noise, we assessed the recall of <italic>DAS</italic> for epochs with different signal-to-noise ratios (SNRs) for the fly and the mouse networks. Because of fundamental differences in the nature of the signals, SNR values were computed with different methods and are therefore not directly comparable across species.</p><sec id="s4-7-1"><title>Pulse</title><p>Pulse waveforms were 20 ms long and centered on the peak of the pulse energy. The root-mean square (RMS) amplitudes of the waveform margins (first and last 5 ms) and center (7.5–12.5 ms) were taken as noise and signal, respectively. RMS is defined as <inline-formula><mml:math id="inf41"><mml:msqrt><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:math></inline-formula>. For multi-channel recordings, the pulse waveform from the channel with the highest center RMS was chosen to calculate the SNR.</p></sec><sec id="s4-7-2"><title>Sine</title><p>Signal was given by the RMS amplitudes of the recording during sine song. Noise is the RMS amplitude in the 200 ms before and after each sine song, with a 10 ms buffer. For instance, if a sine song ended at 1000 ms, the recording between 1010 and 1210 ms was taken as noise. From the 200 ms of noise, we excluded samples that were labeled as sine or pulse and included intervals between pulses. For multi-channel recordings, the SNR was calculated for the channel with the largest signal amplitude.</p></sec><sec id="s4-7-3"><title>Mouse</title><p>We assumed an additive noise model: <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the squared signal averaged over a window of 1 ms. Since noise variance changed little relative to the signal variance in our recordings, we can assume constant noise over time to calculate the signal strength: <inline-formula><mml:math id="inf43"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The sample-wise SNR is then given by <inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="s4-8"><title>Speed benchmarks</title><p>Inference speed was assessed using throughput and latency. Throughput is the number of samples annotated per second and latency is the time it takes to annotate a single chunk. Throughput and latency depend on the chunk duration—the duration of a recording snippet processed by the network at once—and on the batch size—the number of chunks processed during one call. Larger batches maximize throughput by more effectively exploiting parallel computation in modern CPUs and GPUs and reducing overheads from data transfer to the GPU. This comes at the cost of higher latency, since results are available only after all chunks in a batch have been processed. Using small batch sizes and short chunks therefore reduces latency, since results are available earlier, but this comes at the cost of reduced throughput because of overhead from data transfer or under-utilized parallel compute resources. To assess throughput and latency, run times of <monospace>model.predict</monospace> were assessed for batch sizes ranging from 1 to 1024 (log spaced) with 10 repetitions for each batch size after an initial warmup run (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A–L</xref>). Results shown in the main text are from a batch size corresponding to 1 s of recording for throughput (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and a batch size of 1 for latency (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, see also <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). For fly song, latency was optimized by reducing the chunk size to 25.6 ms (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Benchmarks were run on Windows 10, Tensorflow 2.1, with the network either running on a CPU (Intel i7-7770, 3.6 GHz) or on a GPU (GTX1050 Ti 4 GB RAM).</p><p>We also benchmarked the throughput of existing methods for comparison with <italic>DAS</italic> (<xref ref-type="table" rid="table2">Table 2</xref>). Since neither of the methods considered are designed to be used in ways in which latency can be fairly compared to that of <italic>DAS</italic>, we did not assess latency. The throughput values include all pre-processing steps (like calculation of a spectrogram) and comparisons to <italic>DAS</italic> were done using the same hardware (CPU for <italic>FSS</italic> and <italic>USVSEG</italic>, GPU for <italic>TweetyNet</italic> and <xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>). The throughput of <italic>FSS</italic> (<xref ref-type="bibr" rid="bib2">Arthur et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Coen et al., 2014</xref>) was tested using 400 s of single-channel and 9-channel recordings in Matlab2019a. <italic>USVSEG</italic> <xref ref-type="bibr" rid="bib79">Tachibana et al., 2020</xref> was tested on a 72 s recording in Matlab2019a. <italic>TweetyNet</italic> (<xref ref-type="bibr" rid="bib25">Cohen et al., 2020</xref>) was tested using a set of 4 recordings (total duration 35 s). Throughput for <italic>TweetyNet</italic> was given by the combined runtimes of the pre-processing steps (calculating of spectrograms from raw audio and saving them as temporary files) and the inference steps (running the network on a GPU). For the previously published network for annotating marmoset calls (<xref ref-type="bibr" rid="bib66">Oikarinen et al., 2019</xref>), we relied on published values for estimating throughput: A processing time of 8 min for a 60 min recording corresponds to a throughput of 7.5 s/s.</p></sec><sec id="s4-9"><title>Data economy</title><p>For estimating the number of manual annotations required to obtain accurate annotations, we trained the networks using different fractions of the full training and validation sets (for instance, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0). Performance of all networks trained on the different subsets was evaluated on the full test sets. The number of manual annotations in each subset was determined after training from the training and validation sets. The number of annotations required to exceed 90% of the F1 score of a model trained on the full data sets was calculated based on a lowess fit (<xref ref-type="bibr" rid="bib21">Cleveland, 1979</xref>) to the data points (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>).</p></sec><sec id="s4-10"><title>Unsupervised classification</title><p>Segmented signals were clustered using unsupervised methods described previously in <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref>, <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>, and <xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>. First, signals were pre-processed: For fly song, pulse and sine waveforms of duration 15 ms were extracted from the recording, aligned to their peak energy, normalized to unit norm, and adjusted for sign (see <xref ref-type="bibr" rid="bib19">Clemens et al., 2018</xref> for details). For mouse, marmoset, and bird vocalizations, we adapted the procedures described in <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref>: Noise was reduced in the bird song recordings using the <monospace>noisereduce</monospace> package (<ext-link ext-link-type="uri" xlink:href="https://github.com/timsainb/noisereduce">https://github.com/timsainb/noisereduce</ext-link>). For mouse and marmoset vocalizations, noise reduction tended to blur the spectral contours and was omitted. Then, syllable spectrograms were extracted from mel spectrograms of the recordings. The noise floor of the spectrogram at each frequency was estimated as the median spectrogram over time and each spectral band was then divided by the frequency-specific noise floor value. Finally, the spectrogram values were log-transformed and thresholded at zero for mice and two for marmosets and birds after visual inspection of the spectrograms to further remove background noise. To reduce differences in the duration of different syllables, all syllables were first log resized in time (scaling factor 8) and then padded with zeros to the duration of the longest syllable in the data set. Lastly, the frequency axis of the spectrograms for mouse syllables were aligned to the peak frequency, to make clustering robust to jitter in the frequency of the thin spectral contours (<xref ref-type="bibr" rid="bib73">Sangiamo et al., 2020</xref>). The peak frequency of each mouse syllable was calculated from its time-averaged spectrogram, and only the 40 spectrogram frequencies around the peak frequency were retained.</p><p>The dimensionality of the pre-processed waveforms (fly) or spectrograms (mouse, marmoset, birds) was then reduced to two using the UMAP method (<xref ref-type="bibr" rid="bib59">McInnes and Healy, 2018</xref>) (<monospace>mindist </monospace>= 0.5, 0.1 for marmosets to improve separation of clusters). Finally, signals were grouped using unsupervised clustering. For the fly, marmoset, and bird signals, the UMAP distribution revealed distinct groups of syllables and we used a density-based method to cluster the syllables (<xref ref-type="bibr" rid="bib11">Campello et al., 2013</xref>, <monospace>min_samples </monospace>= 10, <monospace>min_cluster_size </monospace>= 20). For mouse USVs, no clusters were visible in the UMAP distribution and density-based clustering failed to identify distinct groups of syllables. Syllables were therefore split into 40 groups using k-means clustering.</p></sec><sec id="s4-11"><title>Open source software used</title><list list-type="bullet"><list-item><p>avgn <ext-link ext-link-type="uri" xlink:href="https://github.com/timsainb/avgn_paper">https://github.com/timsainb/avgn_paper</ext-link> <xref ref-type="bibr" rid="bib72">Sainburg et al., 2020</xref></p></list-item><list-item><p>hdbscan <xref ref-type="bibr" rid="bib58">McInnes et al., 2017</xref></p></list-item><list-item><p>ipython <xref ref-type="bibr" rid="bib70">Perez and Granger, 2007</xref></p></list-item><list-item><p>jupyter <xref ref-type="bibr" rid="bib46">Kluyver et al., 2016</xref></p></list-item><list-item><p>kapre <xref ref-type="bibr" rid="bib16">Choi et al., 2017</xref></p></list-item><list-item><p>keras <xref ref-type="bibr" rid="bib17">Chollet, 2015</xref></p></list-item><list-item><p>keras-tcn <ext-link ext-link-type="uri" xlink:href="https://github.com/philipperemy/keras-tcn">https://github.com/philipperemy/keras-tcn</ext-link></p></list-item><list-item><p>librosa <xref ref-type="bibr" rid="bib57">McFee et al., 2015</xref></p></list-item><list-item><p>matplotlib <xref ref-type="bibr" rid="bib42">Hunter, 2007</xref></p></list-item><list-item><p>noisereduce <ext-link ext-link-type="uri" xlink:href="https://github.com/timsainb/noisereduce">https://github.com/timsainb/noisereduce</ext-link></p></list-item><list-item><p>numpy <xref ref-type="bibr" rid="bib38">Harris et al., 2020</xref></p></list-item><list-item><p>pandas <xref ref-type="bibr" rid="bib60">McKinney, 2010</xref></p></list-item><list-item><p>peakutils <xref ref-type="bibr" rid="bib63">Negri and Vestri, 2017</xref></p></list-item><list-item><p>scikit-learn <xref ref-type="bibr" rid="bib68">Pedregosa et al., 2011</xref></p></list-item><list-item><p>scipy <xref ref-type="bibr" rid="bib83">Virtanen et al., 2020</xref></p></list-item><list-item><p>seaborn <xref ref-type="bibr" rid="bib85">Waskom et al., 2017</xref></p></list-item><list-item><p>snakemake <xref ref-type="bibr" rid="bib48">Köster and Rahmann, 2018</xref></p></list-item><list-item><p>tensorflow <xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref></p></list-item><list-item><p>UMAP <xref ref-type="bibr" rid="bib59">McInnes and Healy, 2018</xref></p></list-item><list-item><p>zarr <xref ref-type="bibr" rid="bib61">Miles et al., 2020</xref></p></list-item><list-item><p>xarray <xref ref-type="bibr" rid="bib41">Hoyer and Hamman, 2017</xref></p></list-item></list></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Kurt Hammerschmidt for providing mouse data prior to publication. We thank Mala Murthy, David Stern, and all members of the Clemens lab for feedback on the manuscript.</p><p>This work was supported by the DFG through grants 329518246 (Emmy Noether) and 430158535 (SPP2205) and by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Starting Grant agreement No. 851210 NeuSoSen).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Software, Funding acquisition, Visualization, Methodology, Writing - original draft, Project administration</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-68837-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Any code and data used during this study is deposited at <ext-link ext-link-type="uri" xlink:href="https://data.goettingen-research-online.de/dataverse/das">https://data.goettingen-research-online.de/dataverse/das</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das">https://github.com/janclemenslab/das</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:0cab8a136523bcfd18e419a2e5f516fce9aa4abf">https://archive.softwareheritage.org/swh:1:rev:0cab8a136523bcfd18e419a2e5f516fce9aa4abf</ext-link>). All fitted models are deposited at <ext-link ext-link-type="uri" xlink:href="https://github.com/janclemenslab/das-menagerie">https://github.com/janclemenslab/das-menagerie</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:c41f87f8fd77ca122ca6f2dcc4676717526aaf24">https://archive.softwareheritage.org/swh:1:rev:c41f87f8fd77ca122ca6f2dcc4676717526aaf24</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset7" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Steinfath</surname><given-names>E</given-names></name><name><surname>Palacios-Muñoz</surname><given-names>A</given-names></name><name><surname>Rottschäfer</surname><given-names>JR</given-names></name><name><surname>Yuezak</surname><given-names>D</given-names></name><name><surname>Clemens</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data and models for Steinfath et al. 2021</data-title><source>Goettingen</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://data.goettingen-research-online.de/dataverse/das">das</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>D</given-names></name><name><surname>Queen</surname><given-names>JE</given-names></name><name><surname>Sober</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Bengalese finch song repository</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.4805749.v5</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Ivanenko</surname><given-names>A</given-names></name><name><surname>Watkins</surname><given-names>P</given-names></name><name><surname>Gerven</surname><given-names>MAJ</given-names></name><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</data-title><source>Donders Repository</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891?0">di.dcn.DSC_620840_0003_891</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Landman</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Data from: Close range vocal interaction through trill calls in the common marmoset (Callithrix jacchus)</data-title><source>Open Science Framework</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://osf.io/q4bm3/">10.17605/OSF.IO/PSWQD</pub-id></element-citation></p><p><element-citation id="dataset4" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Stern</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>Data from: Reported Drosophila courtship song rhythms are artifacts of data analysis.</data-title><source>Janelia</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="http://research.janelia.org/sternlab/rawData.tar">sternlab/rawData.tar</pub-id></element-citation></p><p><element-citation id="dataset5" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Goffinet</surname><given-names>J</given-names></name><name><surname>Brudner</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data from: Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</data-title><source>Duke Digital Repository</source><pub-id assigning-authority="other" pub-id-type="doi">10.7924/r4gq6zn8w</pub-id></element-citation></p><p><element-citation id="dataset6" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Clemens</surname><given-names>J</given-names></name><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Roemschied</surname><given-names>FA</given-names></name><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Mazumder</surname><given-names>D</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Pacheco</surname><given-names>DA</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Data from: Discovery of a New Song Mode in Drosophila Reveals Hidden Structure in the Sensory and Neural Drivers of Behavior.</data-title><source>Goettingen Research Online</source><pub-id assigning-authority="other" pub-id-type="doi">10.25625/8KAKHJ</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name><name><surname>Ghemawat</surname> <given-names>S</given-names></name><name><surname>Irving</surname> <given-names>G</given-names></name><name><surname>Isard</surname> <given-names>M</given-names></name><name><surname>Kudlur</surname> <given-names>M</given-names></name><name><surname>Levenberg</surname> <given-names>J</given-names></name><name><surname>Monga</surname> <given-names>R</given-names></name><name><surname>Moore</surname> <given-names>S</given-names></name><name><surname>Murray</surname> <given-names>DG</given-names></name><name><surname>Steiner</surname> <given-names>B</given-names></name><name><surname>Tucker</surname> <given-names>P</given-names></name><name><surname>Vasudevan</surname> <given-names>V</given-names></name><name><surname>Warden</surname> <given-names>P</given-names></name><name><surname>Wicke</surname> <given-names>M</given-names></name><name><surname>Yu</surname> <given-names>Y</given-names></name><name><surname>Zheng</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Tensorflow: A System for Large-Scale Machine Learning OSDI’16</source><ext-link ext-link-type="uri" xlink:href="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Sunayama-Morita</surname> <given-names>T</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multi-channel acoustic recording and automated analysis of <italic>Drosophila</italic> courtship songs</article-title><source>BMC biology</source><volume>11</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1186/1741-7007-11-11</pub-id><pub-id pub-id-type="pmid">23369160</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Ding</surname> <given-names>Y</given-names></name><name><surname>Sosale</surname> <given-names>M</given-names></name><name><surname>Khalif</surname> <given-names>F</given-names></name><name><surname>Kim</surname> <given-names>E</given-names></name><name><surname>Waddell</surname> <given-names>P</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Songexplorer: a deep learning workflow for discovery and segmentation of animal acoustic communication signals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.26.437280</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bai</surname> <given-names>S</given-names></name><name><surname>Kolter</surname> <given-names>JZ</given-names></name><name><surname>Koltun</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.01271">https://arxiv.org/abs/1803.01271</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>CA</given-names></name><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Acoustic Pattern Recognition and Courtship Songs: Insights from Insects</article-title><source>Annual review of neuroscience</source><volume>42</volume><fpage>129</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-080317-061839</pub-id><pub-id pub-id-type="pmid">30786225</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bath</surname> <given-names>DE</given-names></name><name><surname>Stowers</surname> <given-names>JR</given-names></name><name><surname>Hörmann</surname> <given-names>D</given-names></name><name><surname>Poehlmann</surname> <given-names>A</given-names></name><name><surname>Dickson</surname> <given-names>BJ</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>FlyMAD: rapid thermogenetic control of neuronal activity in freely walking <italic>Drosophila</italic></article-title><source>Nature Methods</source><volume>11</volume><fpage>756</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2973</pub-id><pub-id pub-id-type="pmid">24859752</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behr</surname> <given-names>O</given-names></name><name><surname>von Helversen</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bat serenades—complex courtship songs of the sac-winged bat (Saccopteryx bilineata)</article-title><source>Behavioral Ecology and Sociobiology</source><volume>56</volume><fpage>106</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1007/s00265-004-0768-7</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benichov</surname> <given-names>JI</given-names></name><name><surname>Vallentin</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inhibition within a premotor circuit controls the timing of vocal turn-taking in zebra finches</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-13938-0</pub-id><pub-id pub-id-type="pmid">31924758</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bennet-Clark</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Size and scale effects as constraints in insect sound communication</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>353</volume><fpage>407</fpage><lpage>419</lpage><pub-id pub-id-type="doi">10.1098/rstb.1998.0219</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname> <given-names>AJ</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unsupervised identification of the internal states that shape natural behavior</article-title><source>Nature neuroscience</source><volume>22</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0533-x</pub-id><pub-id pub-id-type="pmid">31768056</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Campello</surname> <given-names>R</given-names></name><name><surname>Moulavi</surname> <given-names>D</given-names></name><name><surname>Sander</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Density-Based Clustering Based on Hierarchical Density Estimates</chapter-title><person-group person-group-type="editor"><name><surname>Moulavi</surname> <given-names>D</given-names></name></person-group><source>Advances in Knowledge Discovery and Data Mining</source><publisher-loc>Heidelberg, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>160</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-75768-7</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cäsar</surname> <given-names>C</given-names></name><name><surname>Zuberbühler</surname> <given-names>K</given-names></name><name><surname>Young</surname> <given-names>RJ</given-names></name><name><surname>Byrne</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Titi monkey call sequences vary with predator location and type</article-title><source>Biology letters</source><volume>9</volume><elocation-id>20130535</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2013.0535</pub-id><pub-id pub-id-type="pmid">24004492</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cator</surname> <given-names>LJ</given-names></name><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Harrington</surname> <given-names>LC</given-names></name><name><surname>Hoy</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Harmonic convergence in the love songs of the dengue vector mosquito</article-title><source>Science</source><volume>323</volume><elocation-id>1166541</elocation-id><pub-id pub-id-type="doi">10.1126/science.1166541</pub-id><pub-id pub-id-type="pmid">19131593</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaverri</surname> <given-names>G</given-names></name><name><surname>Gillam</surname> <given-names>EH</given-names></name><name><surname>Kunz</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A call-and-response system facilitates group cohesion among disc-winged bats</article-title><source>Behavioral Ecology</source><volume>24</volume><fpage>481</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1093/beheco/ars188</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>He</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Exploring simple siamese representation learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.10566">https://arxiv.org/abs/2011.10566</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Choi</surname> <given-names>K</given-names></name><name><surname>Joo</surname> <given-names>D</given-names></name><name><surname>Kim</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Kapre: on-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.05781">https://arxiv.org/abs/1706.05781</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chollet</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Keras</source><ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clay</surname> <given-names>Z</given-names></name><name><surname>Smith</surname> <given-names>CL</given-names></name><name><surname>Blumstein</surname> <given-names>DT</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Food-associated vocalizations in mammals and birds: what do these calls really mean?</article-title><source>Animal Behaviour</source><volume>83</volume><fpage>323</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2011.12.008</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Roemschied</surname> <given-names>FA</given-names></name><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Mazumder</surname> <given-names>D</given-names></name><name><surname>Aldarondo</surname> <given-names>DE</given-names></name><name><surname>Pacheco</surname> <given-names>DA</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Discovery of a New Song Mode in <italic>Drosophila</italic> Reveals Hidden Structure in the Sensory and Neural Drivers of Behavior</article-title><source>Current biology : CB</source><volume>28</volume><fpage>2400</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.06.011</pub-id><pub-id pub-id-type="pmid">30057309</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Hennig</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Computational principles underlying the recognition of acoustic signals in insects</article-title><source>Journal of computational neuroscience</source><volume>35</volume><fpage>75</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1007/s10827-013-0441-0</pub-id><pub-id pub-id-type="pmid">23417450</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cleveland</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Robust locally weighted regression and smoothing scatterplots</article-title><source>Journal of the American Statistical Association</source><volume>74</volume><fpage>829</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1080/01621459.1979.10481038</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Weinstein</surname> <given-names>AJ</given-names></name><name><surname>Pacheco</surname> <given-names>DA</given-names></name><name><surname>Deng</surname> <given-names>Y</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic sensory cues shape song structure in <italic>Drosophila</italic></article-title><source>Nature</source><volume>507</volume><fpage>233</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1038/nature13131</pub-id><pub-id pub-id-type="pmid">24598544</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Xie</surname> <given-names>M</given-names></name><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sensorimotor Transformations Underlying Variability in Song Intensity during <italic>Drosophila</italic> Courtship</article-title><source>Neuron</source><volume>89</volume><fpage>629</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.035</pub-id><pub-id pub-id-type="pmid">26844835</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffey</surname> <given-names>KR</given-names></name><name><surname>Marx</surname> <given-names>RG</given-names></name><name><surname>Neumaier</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title><source>Neuropsychopharmacology : official publication of the American College of Neuropsychopharmacology</source><volume>44</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id><pub-id pub-id-type="pmid">30610191</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>Y</given-names></name><name><surname>Nicholson</surname> <given-names>D</given-names></name><name><surname>Sanchioni</surname> <given-names>A</given-names></name><name><surname>Mallaber</surname> <given-names>EK</given-names></name><name><surname>Skidanova</surname> <given-names>V</given-names></name><name><surname>Gardner</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>TweetyNet: a neural network that enables high-throughput, automated annotation of birdsong</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.28.272088</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deutsch</surname> <given-names>D</given-names></name><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Thiberge</surname> <given-names>SY</given-names></name><name><surname>Guan</surname> <given-names>G</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared Song Detector Neurons in <italic>Drosophila</italic> Male and Female Brains Drive Sex-Specific Behaviors</article-title><source>Current biology : CB</source><volume>29</volume><fpage>3200</fpage><lpage>3215</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.08.008</pub-id><pub-id pub-id-type="pmid">31564492</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Devlin</surname> <given-names>J</given-names></name><name><surname>Chang</surname> <given-names>M-W</given-names></name><name><surname>Lee</surname> <given-names>K</given-names></name><name><surname>Toutanova</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bert: pre-training of deep bidirectional transformers for language understanding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>Y</given-names></name><name><surname>Berrocal</surname> <given-names>A</given-names></name><name><surname>Morita</surname> <given-names>T</given-names></name><name><surname>Longden</surname> <given-names>KD</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Natural courtship song variation caused by an intronic retroelement in an ion channel gene</article-title><source>Nature</source><volume>536</volume><fpage>329</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1038/nature19093</pub-id><pub-id pub-id-type="pmid">27509856</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>Y</given-names></name><name><surname>Lillvis</surname> <given-names>JL</given-names></name><name><surname>Cande</surname> <given-names>J</given-names></name><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Long</surname> <given-names>X</given-names></name><name><surname>Xu</surname> <given-names>M</given-names></name><name><surname>Dickson</surname> <given-names>BJ</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural evolution of Context-Dependent fly song</article-title><source>Current Biology</source><volume>29</volume><fpage>1089</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.02.019</pub-id><pub-id pub-id-type="pmid">30880014</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname> <given-names>WT</given-names></name><name><surname>Neubauer</surname> <given-names>J</given-names></name><name><surname>Herzel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Calls out of chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production</article-title><source>Animal Behaviour</source><volume>63</volume><fpage>407</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1006/anbe.2001.1912</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fortune</surname> <given-names>ES</given-names></name><name><surname>Rodríguez</surname> <given-names>C</given-names></name><name><surname>Li</surname> <given-names>D</given-names></name><name><surname>Ball</surname> <given-names>GF</given-names></name><name><surname>Coleman</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural mechanisms for the coordination of duet singing in wrens</article-title><source>Science</source><volume>334</volume><fpage>666</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1126/science.1209867</pub-id><pub-id pub-id-type="pmid">22053048</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerhardt</surname> <given-names>CH</given-names></name><name><surname>Huber</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Acoustic Communication in Insects and Anurans</source><publisher-loc>Illinois, United States</publisher-loc><publisher-name>University of Chicago Press</publisher-name><pub-id pub-id-type="doi">10.1093/icb/42.5.1080</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goffinet</surname> <given-names>J</given-names></name><name><surname>Brudner</surname> <given-names>S</given-names></name><name><surname>Mooney</surname> <given-names>R</given-names></name><name><surname>Pearson</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title><source>eLife</source><volume>10</volume><elocation-id>e67855</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67855</pub-id><pub-id pub-id-type="pmid">33988503</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Jaitly</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Towards End-To-End speech recognition with recurrent neural networks</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>1764</fpage><lpage>1772</lpage><ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v32/graves14.pdf">http://proceedings.mlr.press/v32/graves14.pdf</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname> <given-names>JM</given-names></name><name><surname>Chae</surname> <given-names>D</given-names></name><name><surname>Naik</surname> <given-names>H</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Koger</surname> <given-names>B</given-names></name><name><surname>Costelloe</surname> <given-names>BR</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Guirguis</surname> <given-names>K</given-names></name><name><surname>Schorn</surname> <given-names>C</given-names></name><name><surname>Guntoro</surname> <given-names>A</given-names></name><name><surname>Abdulatif</surname> <given-names>S</given-names></name><name><surname>Yang</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Seld-Tcn: sound event localization &amp; detection via temporal convolutional networks</article-title><conf-name>2020 28th European Signal Processing Conference (EUSIPCO)</conf-name><fpage>16</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.23919/Eusipco47968.2020.9287716</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haack</surname> <given-names>B</given-names></name><name><surname>Markl</surname> <given-names>H</given-names></name><name><surname>Ehret</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1983">1983</year><chapter-title>Sound communication between parents and offspring</chapter-title><person-group person-group-type="editor"><name><surname>Willott</surname> <given-names>J. F</given-names></name></person-group><source>The Auditory Psychobiology of the Mouse</source><publisher-loc>Springfield, Illinois</publisher-loc><publisher-name>CC Thomas</publisher-name><fpage>57</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.18725/OPARU-1174</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>CR</given-names></name><name><surname>Millman</surname> <given-names>KJ</given-names></name><name><surname>van der Walt</surname> <given-names>SJ</given-names></name><name><surname>Gommers</surname> <given-names>R</given-names></name><name><surname>Virtanen</surname> <given-names>P</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Wieser</surname> <given-names>E</given-names></name><name><surname>Taylor</surname> <given-names>J</given-names></name><name><surname>Berg</surname> <given-names>S</given-names></name><name><surname>Smith</surname> <given-names>NJ</given-names></name><name><surname>Kern</surname> <given-names>R</given-names></name><name><surname>Picus</surname> <given-names>M</given-names></name><name><surname>Hoyer</surname> <given-names>S</given-names></name><name><surname>van Kerkwijk</surname> <given-names>MH</given-names></name><name><surname>Brett</surname> <given-names>M</given-names></name><name><surname>Haldane</surname> <given-names>A</given-names></name><name><surname>del Río</surname> <given-names>JF</given-names></name><name><surname>Wiebe</surname> <given-names>M</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name><name><surname>Gérard-Marchant</surname> <given-names>P</given-names></name><name><surname>Sheppard</surname> <given-names>K</given-names></name><name><surname>Reddy</surname> <given-names>T</given-names></name><name><surname>Weckesser</surname> <given-names>W</given-names></name><name><surname>Abbasi</surname> <given-names>H</given-names></name><name><surname>Gohlke</surname> <given-names>C</given-names></name><name><surname>Oliphant</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holy</surname> <given-names>TE</given-names></name><name><surname>Guo</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ultrasonic songs of male mice</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id><pub-id pub-id-type="pmid">16248680</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoyer</surname> <given-names>S</given-names></name><name><surname>Hamman</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Xarray: n-d labeled arrays and datasets in python</article-title><source>Journal of Open Research Software</source><volume>5</volume><elocation-id>148</elocation-id><pub-id pub-id-type="doi">10.5334/jors.148</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: a 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivanenko</surname> <given-names>A</given-names></name><name><surname>Watkins</surname> <given-names>P</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name><name><surname>Hammerschmidt</surname> <given-names>K</given-names></name><name><surname>Englitz</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007918</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007918</pub-id><pub-id pub-id-type="pmid">32569292</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janik</surname> <given-names>VM</given-names></name><name><surname>Slater</surname> <given-names>PJB</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Context-specific use suggests that bottlenose dolphin signature whistles are cohesion calls</article-title><source>Animal behaviour</source><volume>56</volume><fpage>829</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1006/anbe.1998.0881</pub-id><pub-id pub-id-type="pmid">9790693</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: a method for stochastic optimization</article-title><conf-name>Conference Paper at ICLR 2015</conf-name><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1412.6980.pdf">https://arxiv.org/pdf/1412.6980.pdf</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kluyver</surname> <given-names>T</given-names></name><name><surname>Ragan-Kelley</surname> <given-names>B</given-names></name><name><surname>Pérez</surname> <given-names>F</given-names></name><name><surname>Granger</surname> <given-names>B</given-names></name><name><surname>Bussonnier</surname> <given-names>M</given-names></name><name><surname>Frederic</surname> <given-names>J</given-names></name><name><surname>Kelley</surname> <given-names>K</given-names></name><name><surname>Hamrick</surname> <given-names>J</given-names></name><name><surname>Grout</surname> <given-names>J</given-names></name><name><surname>Corlay</surname> <given-names>S</given-names></name><name><surname>Ivanov</surname> <given-names>P</given-names></name><name><surname>Avila</surname> <given-names>D</given-names></name><name><surname>Abdalla</surname> <given-names>S</given-names></name><name><surname>Willing</surname> <given-names>C</given-names></name><name><surname>development team</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Jupyter notebooks - a publishing format for reproducible computational workflows</chapter-title><person-group person-group-type="editor"><name><surname>Loizides</surname> <given-names>F</given-names></name><name><surname>Scmidt</surname> <given-names>B</given-names></name></person-group><source>Positioning and Power in Academic Publishing: Players, Agents and Agendas</source><publisher-loc>Netherlands</publisher-loc><publisher-name>IOS Press</publisher-name><fpage>87</fpage><lpage>90</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kollmorgen</surname> <given-names>S</given-names></name><name><surname>Hahnloser</surname> <given-names>RHR</given-names></name><name><surname>Mante</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nearest neighbours reveal fast and slow components of motor learning</article-title><source>Nature</source><volume>577</volume><fpage>526</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1892-x</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Köster</surname> <given-names>J</given-names></name><name><surname>Rahmann</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Snakemake-a scalable bioinformatics workflow engine</article-title><source>Bioinformatics</source><volume>34</volume><elocation-id>3600</elocation-id><pub-id pub-id-type="doi">10.1093/bioinformatics/bty350</pub-id><pub-id pub-id-type="pmid">29788404</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koumura</surname> <given-names>T</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Automatic Recognition of Element Classes and Boundaries in the Birdsong with Variable Sequences</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0159188</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0159188</pub-id><pub-id pub-id-type="pmid">27442240</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems 25 (NIPS 2012)</conf-name><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landman</surname> <given-names>R</given-names></name><name><surname>Sharma</surname> <given-names>J</given-names></name><name><surname>Hyman</surname> <given-names>JB</given-names></name><name><surname>Fanucci-Kiss</surname> <given-names>A</given-names></name><name><surname>Meisner</surname> <given-names>O</given-names></name><name><surname>Parmar</surname> <given-names>S</given-names></name><name><surname>Feng</surname> <given-names>G</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Close-range vocal interaction in the common marmoset (Callithrix jacchus)</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0227392</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0227392</pub-id><pub-id pub-id-type="pmid">32298305</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipkind</surname> <given-names>D</given-names></name><name><surname>Marcus</surname> <given-names>GF</given-names></name><name><surname>Bemis</surname> <given-names>DK</given-names></name><name><surname>Sasahara</surname> <given-names>K</given-names></name><name><surname>Jacoby</surname> <given-names>N</given-names></name><name><surname>Takahasi</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>K</given-names></name><name><surname>Feher</surname> <given-names>O</given-names></name><name><surname>Ravbar</surname> <given-names>P</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name><name><surname>Tchernichovski</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stepwise acquisition of vocal combinatorial capacity in songbirds and human infants</article-title><source>Nature</source><volume>498</volume><fpage>104</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/nature12173</pub-id><pub-id pub-id-type="pmid">23719373</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>MA</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Using temperature to analyse temporal dynamics in the songbird motor pathway</article-title><source>Nature</source><volume>456</volume><fpage>189</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1038/nature07448</pub-id><pub-id pub-id-type="pmid">19005546</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mamalet</surname> <given-names>F</given-names></name><name><surname>Garcia</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Simplifying ConvNets for Fast Learning</chapter-title><person-group person-group-type="editor"><name><surname>Garcia</surname> <given-names>C</given-names></name></person-group><source>Artificial Neural Networks and Machine Learning – ICANN 2012</source><publisher-loc>Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>58</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-33266-1_8</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Biasi</surname> <given-names>T</given-names></name><name><surname>Schneider</surname> <given-names>S</given-names></name><name><surname>Yüksekgönül</surname> <given-names>M</given-names></name><name><surname>Rogers</surname> <given-names>B</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pretraining boosts Out-of-Domain robustness for pose estimation</article-title><conf-name>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name><pub-id pub-id-type="doi">10.1109/WACV48630.2021.00190</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McFee</surname> <given-names>B</given-names></name><name><surname>Raffel</surname> <given-names>C</given-names></name><name><surname>Liang</surname> <given-names>D</given-names></name><name><surname>Ellis</surname> <given-names>DP</given-names></name><name><surname>McVicar</surname> <given-names>M</given-names></name><name><surname>Battenberg</surname> <given-names>E</given-names></name><name><surname>Nieto</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Librosa: audio and music signal analysis in python</article-title><conf-name>Proceedings of the 14th Python in Science Conference</conf-name><ext-link ext-link-type="uri" xlink:href="https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf">https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname> <given-names>L</given-names></name><name><surname>Healy</surname> <given-names>J</given-names></name><name><surname>Astels</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hdbscan: hierarchical density based clustering</article-title><source>The Journal of Open Source Software</source><volume>2</volume><elocation-id>205</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00205</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McInnes</surname> <given-names>L</given-names></name><name><surname>Healy</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Umap: uniform manifold approximation and projection for dimension reduction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKinney</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Data structures for statistical computing in python</article-title><conf-name>Proc. of the 9th Python in Science Conf. (SCIPY 2010)</conf-name><ext-link ext-link-type="uri" xlink:href="https://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf">https://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Miles</surname> <given-names>A</given-names></name><name><surname>Kirkham</surname> <given-names>J</given-names></name><name><surname>Durant</surname> <given-names>M</given-names></name><name><surname>Bourbeau</surname> <given-names>J</given-names></name><name><surname>Onalan</surname> <given-names>T</given-names></name><name><surname>Hamman</surname> <given-names>J</given-names></name><name><surname>Patel</surname> <given-names>Z</given-names></name><name><surname>shikharsg</surname> <given-names>R</given-names></name><name><surname>Schut</surname> <given-names>V</given-names></name><name><surname>de Andrade</surname> <given-names>ES</given-names></name><name><surname>Abernathey</surname> <given-names>R</given-names></name><name><surname>Noyes</surname> <given-names>C</given-names></name><name><surname>Tran</surname> <given-names>T</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name><name><surname>Swaney</surname> <given-names>J</given-names></name><name><surname>Moore</surname> <given-names>J</given-names></name><name><surname>Jevnik</surname> <given-names>J</given-names></name><name><surname>Kelleher</surname> <given-names>J</given-names></name><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Sakkis</surname> <given-names>G</given-names></name><name><surname>Barnes</surname> <given-names>C</given-names></name><name><surname>Banihirwe</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Zarr-Developers/zarr-Python, Zenodo, v2.4.0</source><ext-link ext-link-type="uri" xlink:href="https://github.com/zarr-developers/zarr-python">https://github.com/zarr-developers/zarr-python</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morley</surname> <given-names>EL</given-names></name><name><surname>Jonsson</surname> <given-names>T</given-names></name><name><surname>Robert</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Auditory sensitivity, spatial dynamics, and amplitude of courtship song in <italic>Drosophila melanogaster</italic></article-title><source>The Journal of the Acoustical Society of America</source><volume>144</volume><fpage>734</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1121/1.5049791</pub-id><pub-id pub-id-type="pmid">30180716</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Negri</surname> <given-names>LH</given-names></name><name><surname>Vestri</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Lucashn/peakutils, Zenodo, v1.1.0</source><ext-link ext-link-type="uri" xlink:href="https://github.com/lucashn/peakutils">https://github.com/lucashn/peakutils</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neunuebel</surname> <given-names>JP</given-names></name><name><surname>Taylor</surname> <given-names>AL</given-names></name><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Egnor</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Female mice ultrasonically interact with males during courtship displays</article-title><source>eLife</source><volume>4</volume><elocation-id>e06203</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06203</pub-id><pub-id pub-id-type="pmid">26020291</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Nicholson</surname> <given-names>D</given-names></name><name><surname>Queen</surname> <given-names>JE</given-names></name><name><surname>Sober, S</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Bengalese finch song repository</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.4805749.v5</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oikarinen</surname> <given-names>T</given-names></name><name><surname>Srinivasan</surname> <given-names>K</given-names></name><name><surname>Meisner</surname> <given-names>O</given-names></name><name><surname>Hyman</surname> <given-names>JB</given-names></name><name><surname>Parmar</surname> <given-names>S</given-names></name><name><surname>Fanucci-Kiss</surname> <given-names>A</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Landman</surname> <given-names>R</given-names></name><name><surname>Feng</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep convolutional network for animal sound classification and source attribution using dual audio recordings</article-title><source>The Journal of the Acoustical Society of America</source><volume>145</volume><fpage>654</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1121/1.5087827</pub-id><pub-id pub-id-type="pmid">30823820</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okobi</surname> <given-names>DE</given-names></name><name><surname>Banerjee</surname> <given-names>A</given-names></name><name><surname>Matheson</surname> <given-names>AMM</given-names></name><name><surname>Phelps</surname> <given-names>SM</given-names></name><name><surname>Long</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Motor cortical control of vocal interaction in neotropical singing mice</article-title><source>Science</source><volume>363</volume><fpage>983</fpage><lpage>988</lpage><pub-id pub-id-type="doi">10.1126/science.aau9480</pub-id><pub-id pub-id-type="pmid">30819963</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Michel</surname> <given-names>V</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Grisel</surname> <given-names>O</given-names></name><name><surname>Blondel</surname> <given-names>M</given-names></name><name><surname>Prettenhofer</surname> <given-names>P</given-names></name><name><surname>Weiss</surname> <given-names>R</given-names></name><name><surname>Dubourg</surname> <given-names>V</given-names></name><name><surname>Vanderplas</surname> <given-names>J</given-names></name><name><surname>Passos</surname> <given-names>A</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Brucher</surname> <given-names>M</given-names></name><name><surname>Perrot</surname> <given-names>M</given-names></name><name><surname>Duchesnay</surname> <given-names>É</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Aldarondo</surname> <given-names>DE</given-names></name><name><surname>Willmore</surname> <given-names>L</given-names></name><name><surname>Kislin</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>SS</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature methods</source><volume>16</volume><fpage>1</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez</surname> <given-names>F</given-names></name><name><surname>Granger</surname> <given-names>BE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>IPython: a system for interactive scientific computing</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>21</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.53</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Raghu</surname> <given-names>M</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Kleinberg</surname> <given-names>J</given-names></name><name><surname>Bengio</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Transfusion: understanding transfer learning for medical imaging</data-title><source>NeurIPS</source><ext-link ext-link-type="uri" xlink:href="https://paperswithcode.com/paper/transfusion-understanding-transfer-learning">https://paperswithcode.com/paper/transfusion-understanding-transfer-learning</ext-link></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname> <given-names>T</given-names></name><name><surname>Thielk</surname> <given-names>M</given-names></name><name><surname>Gentner</surname> <given-names>TQ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008228</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008228</pub-id><pub-id pub-id-type="pmid">33057332</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sangiamo</surname> <given-names>DT</given-names></name><name><surname>Warren</surname> <given-names>MR</given-names></name><name><surname>Neunuebel</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultrasonic signals associated with different types of social behavior of mice</article-title><source>Nature neuroscience</source><volume>23</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0584-z</pub-id><pub-id pub-id-type="pmid">32066980</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname> <given-names>KH</given-names></name><name><surname>Holmes</surname> <given-names>CM</given-names></name><name><surname>Vellema</surname> <given-names>M</given-names></name><name><surname>Pack</surname> <given-names>AR</given-names></name><name><surname>Elemans</surname> <given-names>CP</given-names></name><name><surname>Nemenman</surname> <given-names>I</given-names></name><name><surname>Sober</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Motor control by precisely timed spike patterns</article-title><source>PNAS</source><volume>114</volume><fpage>1171</fpage><lpage>1176</lpage><pub-id pub-id-type="doi">10.1073/pnas.1611734114</pub-id><pub-id pub-id-type="pmid">28100491</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stern</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reported <italic>Drosophila</italic> courtship song rhythms are artifacts of data analysis</article-title><source>BMC Biology</source><volume>12</volume><elocation-id>38</elocation-id><pub-id pub-id-type="doi">10.1186/1741-7007-12-38</pub-id><pub-id pub-id-type="pmid">24965095</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stern</surname> <given-names>DL</given-names></name><name><surname>Clemens</surname> <given-names>J</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Calhoun</surname> <given-names>AJ</given-names></name><name><surname>Hogenesch</surname> <given-names>JB</given-names></name><name><surname>Arthur</surname> <given-names>BJ</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Experimental and statistical reevaluation provides no evidence for <italic>Drosophila</italic> courtship song rhythms</article-title><source>PNAS</source><volume>114</volume><fpage>9978</fpage><lpage>9983</lpage><pub-id pub-id-type="doi">10.1073/pnas.1707471114</pub-id><pub-id pub-id-type="pmid">28851830</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname> <given-names>JR</given-names></name><name><surname>Hofbauer</surname> <given-names>M</given-names></name><name><surname>Bastien</surname> <given-names>R</given-names></name><name><surname>Griessner</surname> <given-names>J</given-names></name><name><surname>Higgins</surname> <given-names>P</given-names></name><name><surname>Farooqui</surname> <given-names>S</given-names></name><name><surname>Fischer</surname> <given-names>RM</given-names></name><name><surname>Nowikovsky</surname> <given-names>K</given-names></name><name><surname>Haubensak</surname> <given-names>W</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Tessmar-Raible</surname> <given-names>K</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual reality for freely moving animals</article-title><source>Nature methods</source><volume>14</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><pub-id pub-id-type="pmid">28825703</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabler</surname> <given-names>JM</given-names></name><name><surname>Rigney</surname> <given-names>MM</given-names></name><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Gopalakrishnan</surname> <given-names>S</given-names></name><name><surname>Heude</surname> <given-names>E</given-names></name><name><surname>Al-Lami</surname> <given-names>HA</given-names></name><name><surname>Yannakoudakis</surname> <given-names>BZ</given-names></name><name><surname>Fitch</surname> <given-names>RD</given-names></name><name><surname>Carter</surname> <given-names>C</given-names></name><name><surname>Vokes</surname> <given-names>S</given-names></name><name><surname>Liu</surname> <given-names>KJ</given-names></name><name><surname>Tajbakhsh</surname> <given-names>S</given-names></name><name><surname>Egnor</surname> <given-names>SR</given-names></name><name><surname>Wallingford</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cilia-mediated hedgehog signaling controls form and function in the mammalian larynx</article-title><source>eLife</source><volume>6</volume><elocation-id>e19153</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19153</pub-id><pub-id pub-id-type="pmid">28177282</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tachibana</surname> <given-names>RO</given-names></name><name><surname>Kanno</surname> <given-names>K</given-names></name><name><surname>Okabe</surname> <given-names>S</given-names></name><name><surname>Kobayasi</surname> <given-names>KI</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>USVSEG: A robust method for segmentation of ultrasonic vocalizations in rodents</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0228907</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0228907</pub-id><pub-id pub-id-type="pmid">32040540</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschida</surname> <given-names>K</given-names></name><name><surname>Mooney</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of auditory feedback in vocal learning and maintenance</article-title><source>Current opinion in neurobiology</source><volume>22</volume><fpage>320</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.11.006</pub-id><pub-id pub-id-type="pmid">22137567</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van den Oord</surname> <given-names>A</given-names></name><name><surname>Dieleman</surname> <given-names>S</given-names></name><name><surname>Zen</surname> <given-names>H</given-names></name><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Kalchbrenner</surname> <given-names>N</given-names></name><name><surname>Senior</surname> <given-names>A</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Wavenet: a generative model for raw audio</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Segbroeck</surname> <given-names>M</given-names></name><name><surname>Knoll</surname> <given-names>AT</given-names></name><name><surname>Levitt</surname> <given-names>P</given-names></name><name><surname>Narayanan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MUPET-Mouse Ultrasonic Profile ExTraction: A Signal Processing Tool for Rapid and Unsupervised Analysis of Ultrasonic Vocalizations</article-title><source>Neuron</source><volume>94</volume><fpage>465</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id><pub-id pub-id-type="pmid">28472651</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname> <given-names>P</given-names></name><name><surname>Gommers</surname> <given-names>R</given-names></name><name><surname>Oliphant</surname> <given-names>TE</given-names></name><name><surname>Haberland</surname> <given-names>M</given-names></name><name><surname>Reddy</surname> <given-names>T</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Burovski</surname> <given-names>E</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name><name><surname>Weckesser</surname> <given-names>W</given-names></name><name><surname>Bright</surname> <given-names>J</given-names></name><name><surname>van der Walt</surname> <given-names>SJ</given-names></name><name><surname>Brett</surname> <given-names>M</given-names></name><name><surname>Wilson</surname> <given-names>J</given-names></name><name><surname>Millman</surname> <given-names>KJ</given-names></name><name><surname>Mayorov</surname> <given-names>N</given-names></name><name><surname>Nelson</surname> <given-names>ARJ</given-names></name><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Kern</surname> <given-names>R</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Carey</surname> <given-names>CJ</given-names></name><name><surname>Polat</surname> <given-names>İ</given-names></name><name><surname>Feng</surname> <given-names>Y</given-names></name><name><surname>Moore</surname> <given-names>EW</given-names></name><name><surname>VanderPlas</surname> <given-names>J</given-names></name><name><surname>Laxalde</surname> <given-names>D</given-names></name><name><surname>Perktold</surname> <given-names>J</given-names></name><name><surname>Cimrman</surname> <given-names>R</given-names></name><name><surname>Henriksen</surname> <given-names>I</given-names></name><name><surname>Quintero</surname> <given-names>EA</given-names></name><name><surname>Harris</surname> <given-names>CR</given-names></name><name><surname>Archibald</surname> <given-names>AM</given-names></name><name><surname>Ribeiro</surname> <given-names>AH</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>van Mulbregt</surname> <given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>MR</given-names></name><name><surname>Clein</surname> <given-names>RS</given-names></name><name><surname>Spurrier</surname> <given-names>MS</given-names></name><name><surname>Roth</surname> <given-names>ED</given-names></name><name><surname>Neunuebel</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultrashort-range, high-frequency communication by female mice shapes social interactions</article-title><source>Scientific Reports</source><volume>10</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s41598-020-59418-0</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Waskom</surname> <given-names>M</given-names></name><name><surname>Botvinnik</surname> <given-names>O</given-names></name><name><surname>O’Kane</surname> <given-names>D</given-names></name><name><surname>Hobson</surname> <given-names>P</given-names></name><name><surname>Lukauskas</surname> <given-names>S</given-names></name><name><surname>Gemperline</surname> <given-names>DC</given-names></name><name><surname>Augspurger</surname> <given-names>T</given-names></name><name><surname>Halchenko</surname> <given-names>Y</given-names></name><name><surname>Cole</surname> <given-names>JB</given-names></name><name><surname>Warmenhoven</surname> <given-names>J</given-names></name><name><surname>de Ruiter</surname> <given-names>J</given-names></name><name><surname>Pye</surname> <given-names>C</given-names></name><name><surname>Hoyer</surname> <given-names>S</given-names></name><name><surname>Vanderplas</surname> <given-names>J</given-names></name><name><surname>Villalba</surname> <given-names>S</given-names></name><name><surname>Kunter</surname> <given-names>G</given-names></name><name><surname>Quintero</surname> <given-names>E</given-names></name><name><surname>Bachant</surname> <given-names>P</given-names></name><name><surname>Martin</surname> <given-names>M</given-names></name><name><surname>Meyer</surname> <given-names>K</given-names></name><name><surname>Miles</surname> <given-names>A</given-names></name><name><surname>Ram</surname> <given-names>Y</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name><name><surname>Williams</surname> <given-names>ML</given-names></name><name><surname>Evans</surname> <given-names>C</given-names></name><name><surname>Fitzgerald</surname> <given-names>C</given-names></name><name><surname>Brian</surname> <given-names>F</given-names></name><name><surname>Lee, A</surname> <given-names>C</given-names></name><name><surname>Qalieh</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Mwaskom/seaborn, Zenodo, v0.8.1</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mwaskom/seaborn">https://github.com/mwaskom/seaborn</ext-link></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname> <given-names>M</given-names></name><name><surname>Hultsch</surname> <given-names>H</given-names></name><name><surname>Adam</surname> <given-names>I</given-names></name><name><surname>Scharff</surname> <given-names>C</given-names></name><name><surname>Kipper</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The use of network analysis to study complex animal communication systems: a study on nightingale song</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>281</volume><elocation-id>20140460</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2014.0460</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>F</given-names></name><name><surname>Koltun</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multi-scale context aggregation by dilated convolutions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.07122">https://arxiv.org/abs/1511.07122</ext-link></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68837.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Egnor</surname><given-names>SE Roian</given-names> </name><role>Reviewer</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Troyer</surname><given-names>Todd</given-names> </name><role>Reviewer</role><aff><institution>UT San Antonio</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.03.26.436927">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.03.26.436927v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper presents and evaluates a machine learning method for segmenting and annotating animal acoustic communication signals. The paper presents results from applying the method to signals from <italic>Drosophila</italic>, mice, marmosets, and songbirds, but the method should be useful for a broad range of researchers who record animal vocalizations. The method appears to be easily generalizable and has high throughput and modest training times.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Fast and accurate annotation of acoustic signals with deep neural networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Ronald Calabrese as the Senior and Reviewing Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Roian Egnor (Reviewer #1); Todd Troyer (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>There are two main concerns with this paper that all three reviewers shared in their own way. The reviews offer details of how to address these concerns.</p><p>1. The authors claim that the method compares favorably to other machine learning methods, but they only provide head-to-head performance comparisons for <italic>Drosophila</italic> songs. There are no direct comparisons against other methods for mouse ultrasonic or songbird vocalizations, nor is there a readable summary of performance numbers from the literature. This makes it difficult for the reader to assess the authors' claim that DeepSS is a significant advance over the state of the art.</p><p>2. The authors provide little discussion about optimizing network parameters for a given data set. If the software is to be useful for the non-expert, a broader discussion of considerations for setting parameters would be useful. How should one choose the stack number, or the size or number of kernels? Moreover, early in the paper DeepSS is claimed as a method that learns directly from the raw audio data, in contrast to methods that rely on Fourier or Wavelet transforms. Yet in the Methods section it is revealed that a short-term Fourier front-end was used for both the mouse and songbird data.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>General comments:</p><p>This is a reasonable piece of software. Not super polished, but pretty straightforward. Once I got it installed (which was a little involved) it worked out of the box on raw mouse vocalizations. There could be a little more background on other methods/approaches and their strengths and weaknesses. In addition, I think more explanation about tuning parameters is necessary if the goal is for users without machine learning expertise to be able to use this.</p><p>I. Results</p><p>A. How good is it?</p><p>1. How fast is it?</p><p>a. How long to train?</p><p>Train time depends on the amount of data, but the ranges quotes (10 minutes to 5 hours) are quite reasonable. It works on reasonable hardware (I tested on a laptop with a GPU).</p><p>b. How long to classify?</p><p>Latency to classification is between 7-15ms, which is a little long for triggered optogenetics, but not bad, and certainly reasonable for acoustic feedback.</p><p>2. How accurate is it?</p><p>a. Accuracy is improved relative to Fly Song Segmenter, particularly in recall (Arthur et al., 2013; Coen et al., 2014).</p><p>Pulse song:</p><p>DeepSS precision: 97%, recall: 96%</p><p>FlySongSegmenter: precision: 99%, recall 87%.</p><p>Sine song:</p><p>DeepSS precision: 92%, recall: 98%</p><p>FlySongSegmenter: precision: 91%, recall: 91%.</p><p>b. One main concern I have is that all the signals described, with the exception of pulse song, are relatively simple tonally. Bengalese finch song is much less noisy than zebra finch song. Mouse vocalizations are quite tonal. How would this method work on acoustic signals with noise components, like zebra finches or some non-human primate signals? Some signals can have variable spectrotemporal structure based on the distortion due to increased intensity of the signal (see, for example, Fitch, Neubauer, and Hertzel, 2002).</p><p>W.Tecumseh Fitch, Jürgen Neubauer and Hanspeter Herzel (2002) &quot;Calls out of chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production&quot; Animal Behaviour, 63: 407-418. doi:10.1006/anbe.2001.1912</p><p>B. How easy to use?</p><p>0. &quot;our method can be optimized for new species without requiring expert knowledge and with little manual annotation work.&quot; There isn't a lot of explanation, either in the paper or in the associated documentation, of how to select network parameters for a new vocalization type. However, it does appear that small amounts of annotation are sufficient to train a reasonable classifier.</p><p>1. How much pre-processing of signals is necessary?</p><p>All the claims of the paper are based on pre-processed audio data, although they state, in the Methods section that preprocessing is not necessary. It's not clear how important this pre-processing is for achieving the kinds of accuracy observed. Certainly I would expect the speed to drop if high frequency signals like mouse vocalizations aren't downsampled. However, I tried it on raw, un-preprocessed mouse vocalizations, without downsampling and using very few training examples, and it worked quite well, only missing low signal-to-noise vocalizations.</p><p>C. How different from other things out there?</p><p>It would strengthen the paper to include some numbers on other mouse and birdsong methods, rather than simple and vague assertions &quot;These performance values compare favorably to that of methods specialized to annotate USVs (Coffey et al., 2019; Tachibana et al., 2020; Van Segbroeck et al., 2017).&quot; &quot;Thus, DeepSS performs as well as or better than specialized deep learning-based methods for annotating bird song (Cohen et al., 2020; Koumura and Okanoya, 2016).</p><p>D. Miscellaneous comments</p><p>1. Interestingly, the song types don't appear to be mutually exclusive. One can have pulse song in the middle of sine song. That might be useful to be able to toggle…I can imagine cases where it would be nice to be able to label things that overlap, but in general if something is sine song, it can't be pulse song. And my assumption certainly was that song types would be mutually exclusive. Adding some explanation of that to the text/user's manual would be useful.</p><p>2. How information is combined across channels is aluded to several times but not described well in the body of the manuscript, though it is mentioned in the methods in vague terms:</p><p>&quot;several channel convolutions, 𝑘𝛾(1, 𝛾), combine information across channels.&quot;</p><p>II. Usability</p><p>A. Getting it installed</p><p>Installing on Windows 10, was a bit involved if you were not already using python: Anaconda, python, tensorflow, CUDA libraries, create an account to download cuDNN, and update NVIDIA drivers.</p><p>However, it went OK until I hit this:</p><p>error:</p><p>File &quot;…\miniconda3\envs\dss\lib\ctypes\__init__.py&quot;, line 373, in __init__</p><p>self._handle = _dlopen(self._name, mode)</p><p>FileNotFoundError: Could not find module '…\miniconda3\envs\dss\lib\site-packages\scipy\.libs\libbanded5x.3OIBJ6VWWPY6GDLEMSTXSIPCHHWASXGT.gfortran-win_amd64.dll' (or one of its dependencies). Try using the full path with constructor syntax.</p><p>rummaging around in Stackoverflow I found this (answer #4):</p><p>https://stackoverflow.com/questions/59330863/cant-import-dll-module-in-python</p><p>which was to edit ~\Miniconda3\envs\dss\lib\ctypes\ _init_.py to change winmode=None to winmode=0.</p><p>This worked, but did slow me down a fair bit. I'm not sure who the target audience is for this software, but it may be a bit much for the average biologist.</p><p>B. Using it</p><p>It wasn't clear to me what &quot; complete annotation&quot; meant: &quot;Once you have completely annotated the song in the first 18 seconds of the tutorial recording…&quot;. Does this mean that all instances of pulse and sine song must be labeled? What are the consequences if this is not true?</p><p>I also wasn't clear how to know when it finished training and I could try predicting.</p><p>I was a bit confused by there being 2 menu headings called &quot;DeepSS&quot;, one of which (the rightmost) has a Predict option and one of which doesn't. The first time through I used the leftmost DeepSS menu heading for Make dataset and Train, and it didn't work, the second time through I used the rightmost one and it did work (but this might have been user error).</p><p>III. General usability:</p><p>1. It would be nice if the oscillogram vertical axis didn't autoscale, it made it hard to recognize sine song when there were no pulses.</p><p>2. It would be nice to be able to change the playback rate, or the size of the jumps in the navigation bar at the bottom.</p><p>3. It was not very clear in the instructions how to fix errors and use those annotations to train again, although in the body of the text this is explicitly stated as important.</p><p>&quot;We find that DeepSS can be efficiently trained using an iterative protocol (Pereira et al., 2018) (Figure 4C, S8): Annotate a small set of recordings and train the network for a few epochs; then generate annotations on a larger set of recordings and correct these annotations.&quot;</p><p>&quot;Correct any prediction errors-add missing annotations, remove false positive annotations, adjust the timing of annotations.&quot;</p><p>4. The instructions for correcting could be more clear. It took a little fiddling to figure out I needed to switch to &quot;add pulse_proposals&quot; in order to remove a pulse proposal.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In general, I recommend publication after major revision.</p><p>Apart from my comments above, my excitement for this toolbox is a little bit hampered by a few aspects:</p><p>Regarding the comparisons to other tools, it would be great to compare (or at least discuss the performance of other recent tools on the bird and mouse dataset). Existing models are described as slow (Line 50), but no quantitative comparisons are given.</p><p>What are the human recall/precision for Figure 1 (compare to MARS, where a nice classifier vs. human comparison is given: https://www.biorxiv.org/content/10.1101/2020.07.26.222299v1). Is FSS a tunable algorithm and how was it tuned?</p><p>While the authors emphasize that the architecture does not require any pre-processing, they end up using the &quot;optional&quot; Short-time Fourier transform frontend for all, but the sine wave detection for flies (i.e. mice and bird). So I think this emphasis is ill advised.</p><p>In the end, the authors use different network parameters for all datasets (see Table S1A; with/without STFT, # stacks, kernel size…). This begs the question is this necessary. What happens if one trains the same architectures on the different datasets, does the performance suffer? -- I would suppose that is not the case, and with respect to the usability of this toolbox this seems to be an important question. I.e. how should users pick the architecture and hyperparameters?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The most important issue I had with the paper is that it provided the reader with little sense of DeepSS's performance relative to other software. The paper would be much better if other approaches were implemented on publicly available data sets and compared head-to-head.</p><p>The methods should state very clearly where the data sets used are available for download. While the authors may believe in DeepSS, other challengers will inevitably arise. These authors should be given the opportunity to report head-to-head performance against DeepSS on the same data.</p><p>The text references in the mouse section are to figure 3, but the relevant figure is now figure 2.</p><p>There are several places where the color scale used is useless, namely in the confusion matrices (Figure 1D, 1I, 2E, 2H, 3C and some supplemental figs). At these performance levels, color just indicates the correct vs. incorrect parts of the graph – relative shades of the heat map are not perceptible.</p><p>The birdsong confusion matrices are similarly uninformative, since all that is seen is a bunch of boxes along the diagonal. Worse yet, the discretization of the figure makes it appear that boxes along the diagonal have different sizes/offsets, suggesting structure where there is none. Showing marginals of the matrix like Figure S4E can be useful.</p><p>In Figure S4C, precision and recall are plotted whereas in S4E false positive and false negatives are plotted. Are both plots needed?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68837.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>There are two main concerns with this paper that all three reviewers shared in their own way. The reviews offer details of how to address these concerns.</p><p>1. The authors claim that the method compares favorably to other machine learning methods, but they only provide head-to-head performance comparisons for <italic>Drosophila</italic> songs. There are no direct comparisons against other methods for mouse ultrasonic or songbird vocalizations, nor is there a readable summary of performance numbers from the literature. This makes it difficult for the reader to assess the authors' claim that DeepSS is a significant advance over the state of the art.</p></disp-quote><p>We addressed this issue in two ways: By testing DAS on two more species and by directly comparing accuracy and speed of DAS to that of other methods.</p><p>First, we included two more species – Zebra finches and marmosets – to further demonstrate the versatility and robustness of DAS. Both species were suggested by Reviewer #1 because they produced more variable signals. Annotations for the marmoset calls are particularly challenging because of a lot of background noise in the recordings that we used. We find that DAS annotates signals from both species with high accuracy. The results of these tests are now discussed in two new sections Results and shown in two new Figure 2 —figure supplement 1 (marmoset) and Figure 3 —figure supplement 2 (zebra finch).</p><p>For the marmosets (Results, p6, l200):</p><p>“Marmoset vocalizations</p><p>We next examined the robustness of annotations produced by DAS to noisy recordings and variable vocalization types, by training a network to annotate vocalization from pairs of marmosets (Landman et al., 2020). The recordings contain lots of background noises like faint calls from nearby animals, overdrive from very loud calls of the recorded animals, and large variability within syllable types (Figure 2 —figure supplement 1A-D). Recently, a deep-learning based method was shown to produce good performance (recall 77%, precision 85%, 12.5 ms temporal error) when trained on 16000 syllables to recognize seven vocalization types (Oikarinen et al., 2019). We trained DAS on 1/9th of the data (1800 syllables) containing 4 of the 7 vocalization types. Despite the noisy and variable vocal- izations, DAS achieves high syllable-wise precision and recall (96%, 92%, (Figure 2 —figure supplement 1E, F)). Note that DAS obtains this higher performance at millisecond resolution (temporal error 4.4 ms, Figure 2 —figure supplement 1G), while the method by Oikarinen et al., (2019) only produces annotations with a resolution of 50 ms (Table 2).”</p><p>For the Zebra finches (Results, p7, l232):</p><p>“To further demonstrate the robustness of DAS, we trained a network to annotate song from Zebra finches. In Zebra finch males, Individual renditions of a given syllable type tend to be more variable (Fitch et al., 2002). Moreover, the particular recordings used here (Goffinet et al., 2021) contain background noise from the bird’s movement. Despite the variability and noise, DAS annotates the six syllables from a male’s main motif with excellent precision and recall, and low temporal error, demonstrating that DAS is robust to song variability and recording noise (Figure 3 —figure supplement 2).”</p><p>These two species are now also included in all analyses of the throughput and latency, of number of annotations required for training, and of the unsupervised classification of syllable types. With the inclusion of these two new species, we now demonstrate state-of-the-art performance for DAS using recordings from five different species with highly diverse signal structures and signal-to-noise ratios – including the pulse song of <italic>Drosophila</italic>, the spectrotemporally complex syllables of birds, and noisy recordings of the variable calls of marmosets. We are not aware of a single method with demonstrably good performance on such a diverse set of annotation tasks.</p><p>Second, we compared the annotation performance and speed of our method to those of existing state-of-the-art methods for <italic>all</italic> species tested. This confirms our statement in the original paper: that DAS performs as well as or better than existing, specialized methods. DAS performance for mouse (against USVSEG) and bird song (against TweetyNet) is on par with that of the state of the art. DAS outperforms the state of the art for marmoset (against the Deep Learning method from Oikarinen et al., (2019)) and fly song (against FlySongSegmenter). DAS has also 3x-20x higher throughput than these methods. Overall, this established DAS as a fast, accurate, and universal method for annotating acoustic signals.</p><p>We revised the wording throughout the manuscript to more accurately reflect the outcome of these comparisons. For instance, in the comparison with bird song we now state that our method “performs as well as” the state of the art.</p><p>We added a new paragraph, describing the results of the speed comparisons (Results, p8, l264):</p><p>“We also compared the speed of DAS to that of other methods that were specifically developed to annotate the types of signals tested here. Since most existing methods are not suitable for estimating latency due to constraints in their design and interface, we only compared throughput. We find that DAS achieves 3x to 10x higher throughput than existing methods (Table 2). This has three main reasons: First, the relatively simple, purely convolutional architecture exploits the parallel processing capabilities of modern CPUs and GPUs. Second, Fourier or wavelet-like preprocessing steps are integrated into the network and profit therefore from a fast implementation and hardware acceleration. Third, for multi-channel data, DAS combines information from all audio channels early, which increases throughput by reducing the data bandwidth.</p><p>Overall, DAS annotates audio with high throughput (&gt;8x realtime) and low latency (&lt;15 ms), and is faster than the alternative methods tested here. The high speed renders suitable for annotating large corpora and for realtime applications without requiring specialized hardware.”</p><disp-quote content-type="editor-comment"><p>2. The authors provide little discussion about optimizing network parameters for a given data set. If the software is to be useful for the non-expert, a broader discussion of considerations for setting parameters would be useful. How should one choose the stack number, or the size or number of kernels? Moreover, early in the paper DeepSS is claimed as a method that learns directly from the raw audio data, in contrast to methods that rely on Fourier or Wavelet transforms. Yet in the Methods section it is revealed that a short-term Fourier front-end was used for both the mouse and songbird data.</p></disp-quote><p>We addressed this issue in two ways: First, by demonstrating that performance is relatively robust to changes in network parameters. Second, by providing advice on how to choose network parameters in Methods.</p><p>First, we performed parameter sweeps for the networks trained on the song of flies and of Bengalese finches demonstrating that DAS performance is relatively robust to changes in individual network parameters, like the number of stacks, the number/duration of filters (new Figure 4 —figure supplement 5). This means that the exact choice of parameters is not crucial to obtain performant networks. The parameters of the networks tested in the manuscript serve as good starting points for fitting networks to new species.</p><p>Second, we added a new section to Methods in which we provide advice for choosing the structural parameters of the network for a new species. The results from the parameter sweeps and the new section in Methods are now referred to in Discussion (p11, l355):</p><p>“Network performance is robust to changes in the structural parameters of the network, like filter number and duration, or the network depth (Figure 4 —figure supplement 5). Thus, the structural parameters do not need to be finely tuned to obtain a performant network for a new species. We have trained networks using a wide range of signal types (Table 4) and these networks constitute good starting points for adapting DAS to novel species. We provide additional advice for the design of novel network in Methods.”</p><p>From Methods (new section on p15, l475):</p><p>“Choice of structural network parameters</p><p>DAS performance is relatively robust to the choice of structural network parameters like filter duration and number, or network depth (Figure 4 —figure supplement 5). […] If required, latency can then be further reduced by reducing chunk duration, the number and duration of filters, and the number of TCN blocks.”</p><p>Overall, this shows that DAS is easy to adapt to novel species: annotation performance does not crucially depend on the networks’ structural parameters, and that defaults given in the new section in Methods perform well across species in our experience.</p><disp-quote content-type="editor-comment"><p>Moreover, early in the paper DeepSS is claimed as a method that learns directly from the raw audio data, in contrast to methods that rely on Fourier or Wavelet transforms. Yet in the Methods section it is revealed that a short-term Fourier front-end was used for both the mouse and songbird data.</p></disp-quote><p>We agree that our wording was a misleading. In DAS, raw audio is provided as an input to the network, and a trainable downsampling layer which is initialized with STFT filters can be optionally added. This has several advantages</p><p>1. The downsampling can be omitted. This makes DAS more flexible, since it allows DAS to annotate signals for which an STFT-like transform is not appropriate (fly pulse song).</p><p>2. The downsampling is initialized with STFT filters but is then trained, removing the need to hand-tune this step and allowing the network to learn filters that greatly deviate from the initialization.</p><p>3. By integrating downsampling into the network, it profits from an efficient implementation and hardware acceleration, increasing throughout and reducing latency.</p><p>We have de-emphasized and clarified this point in the beginning of Results (p3, l95):</p><p>“… First, the pre-processing step is optional. This makes DAS more flexible, since signals for which a time-resolved Fourier transform is not appropriate—for instance, short pulsatile signals—can now also be processed. Second, the optional preprocessing step is integrated and optimized with the rest of the network. This removes the need to hand-tune this step and allows the network to learn a preprocessing that deviates from a time-resolved Fourier or wavelet transform if beneficial (Choi et al., 2017). Integrating the preprocessing into the network also increases inference speed due to the efficient implementation and hardware acceleration of deep-learning frameworks.”</p><p>And in Methods (p13, l414)</p><p>“DAS takes as input raw, single or multi-channel audio. Pre-processing of the audio using a Wavelet or short-time Fourier transform is optional and integrated into the network.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>This is a reasonable piece of software. Not super polished, but pretty straightforward. Once I got it installed (which was a little involved) it worked out of the box on raw mouse vocalizations. There could be a little more background on other methods/approaches and their strengths and weaknesses. In addition, I think more explanation about tuning parameters is necessary if the goal is for users without machine learning expertise to be able to use this.</p></disp-quote><p>We now discuss key architectural differences to existing approaches in Results (p2, l82):</p><p>“These [AM and FM] patterns are typically made explicit using a hand-tuned pre-processing step based on time-resolved Fourier or wavelet transforms (Arthur et al., 2013; Coffey et al., 2019; Cohen et al., 2020a; Oikarinen et al., 2019; Van Segbroeck et al., 2017). Most deep-learning based methods then treat this pre-defined spectrogram as an image and use methods derived from computer vision to extract the AM and FM features relevant for annotation (Coffey et al., 2019; Cohen et al., 2020a; Oikarinen et al., 2019). Recurrent units are sometimes used to track the sound features over time (Cohen et al., 2020a). This approach can produce accurate annotations but has drawbacks: First, the spectrogram constitutes a strong and proven pre-processing step, but it is unsuitable for some signal types, like short pulsatile signals. Second, the pre-processing transform is typically tuned by hand and may therefore require expert knowledge for it to produce optimal results. Lastly, the recurrent layers used in some methods (Cohen et al., 2020a) excel at combining information over time to provide the context information necessary to annotate spectrally complex signals, but they can be hard to train and slow to run (Bai et al., 2018).”</p><disp-quote content-type="editor-comment"><p>I. Results</p><p>A. How good is it?</p><p>1. How fast is it?</p><p>a. How long to train?</p><p>Train time depends on the amount of data, but the ranges quotes (10 minutes to 5 hours) are quite reasonable. It works on reasonable hardware (I tested on a laptop with a GPU).</p><p>b. How long to classify?</p><p>Latency to classification is between 7-15ms, which is a little long for triggered optogenetics, but not bad, and certainly reasonable for acoustic feedback.</p><p>2. How accurate is it?</p><p>a. Accuracy is improved relative to Fly Song Segmenter, particularly in recall (Arthur et al., 2013; Coen et al., 2014).</p><p>Pulse song:</p><p>DeepSS precision: 97%, recall: 96%</p><p>FlySongSegmenter: precision: 99%, recall 87%.</p><p>Sine song:</p><p>DeepSS precision: 92%, recall: 98%</p><p>FlySongSegmenter: precision: 91%, recall: 91%.</p><p>b. One main concern I have is that all the signals described, with the exception of pulse song, are relatively simple tonally. Bengalese finch song is much less noisy than zebra finch song. Mouse vocalizations are quite tonal. How would this method work on acoustic signals with noise components, like zebra finches or some non-human primate signals? Some signals can have variable spectrotemporal structure based on the distortion due to increased intensity of the signal (see, for example, Fitch, Neubauer, and Hertzel, 2002).</p><p>W.Tecumseh Fitch, Jürgen Neubauer and Hanspeter Herzel (2002) &quot;Calls out of chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production&quot; Animal Behaviour, 63: 407-418. doi:10.1006/anbe.2001.1912</p></disp-quote><p>See Essential Revision #1 for details.</p><p>We tested DAS on signals from Zebra finches and marmosets. The marmoset recordings are particularly challenging due to high background noise. DAS annotates the signals from both species accurately: F1 score of 94% for marmosets (new Figure 2 —figure supplement 1, reproduced on page 3), and of 100% for the main motif of a Zebra finch (new Figure 3 —figure supplement 2, reproduced on page 4).</p><disp-quote content-type="editor-comment"><p>B. How easy to use?</p><p>0. &quot;our method can be optimized for new species without requiring expert knowledge and with little manual annotation work.&quot; There isn't a lot of explanation, either in the paper or in the associated documentation, of how to select network parameters for a new vocalization type. However, it does appear that small amounts of annotation are sufficient to train a reasonable classifier.</p></disp-quote><p>See Essential Rev #2 for details.</p><p>Using a parameter sweep for the fly and Bengalese finch networks, we show that DAS performance is relatively robust to changes in individual parameters – so exact parameter values do not matter (Figure 4 —figure supplement 5, reproduced on page 7). We provide recommendations for parameter choice in a new section in Methods (p15, l475, reproduced on page 7).</p><disp-quote content-type="editor-comment"><p>1. How much pre-processing of signals is necessary?</p><p>All the claims of the paper are based on pre-processed audio data, although they state, in the Methods section that preprocessing is not necessary. It's not clear how important this pre-processing is for achieving the kinds of accuracy observed. Certainly I would expect the speed to drop if high frequency signals like mouse vocalizations aren't downsampled. However, I tried it on raw, un-preprocessed mouse vocalizations, without downsampling and using very few training examples, and it worked quite well, only missing low signal-to-noise vocalizations.</p></disp-quote><p>See Essential Rev #2.</p><p>We clarified the wording in the beginning of the Results section. We de-emphasized the misleading statement that no there is no pre-processing (Results, p3, l95, reproduced on page 9).</p><p>The pre-processing is not a manual step but an optional part of the network, which is recommended for most signals. It acts as a down-sampling layer, which is in essence a convolutional layer that is initialized with STFT filters. These filters are optimized during training. This abrogates the need for manually tuning the STFT parameters as in other methods. We did not use it for fly song, since (1) an STFT-like transform is not appropriate for the highly transient pulses in pulse song and (2) the audio rate was already low (10 kHz)–downsampling would have impaired temporal accuracy.</p><p>And yes, the pre-processing has only a small impact on performance (we observed this with mouse and bird song). We find that it accelerates convergence during training because the STFT filters are a good initialization and it accelerates inference because the downsampling reduces the bandwidth of the data processed in the network.</p><p>The above considerations are now included in our recommendation for choosing model parameters (Methods, p15, l495):</p><p>“Downsampling/STFT weakly affects performance but strongly accelerates convergence during training. This is because (A) the initialization with STFT filters is a good prior that reduces the number of epochs it takes to learn the optimal filters, and (B) the downsampling reduces the data bandwidth and thereby the time it takes to finish one training epoch. The overall increase in performance from adding the STFT layer is low because convolutional layers in the rest of the network can easily replicate the computations of the STFT layer. For short pulsatile signals or signals with low sampling rates, STFT and downsampling should be avoided since they can decrease performance due to the loss of temporal resolution.”</p><disp-quote content-type="editor-comment"><p>C. How different from other things out there?</p><p>It would strengthen the paper to include some numbers on other mouse and birdsong methods, rather than simple and vague assertions &quot;These performance values compare favorably to that of methods specialized to annotate USVs (Coffey et al., 2019; Tachibana et al., 2020; Van Segbroeck et al., 2017).&quot; &quot;Thus, DeepSS performs as well as or better than specialized deep learning-based methods for annotating bird song (Cohen et al., 2020; Koumura and Okanoya, 2016).</p></disp-quote><p>See Essential Rev #1 for details (pages 1-6 of the reply).</p><p>See Essential Revisions #1 for details (pages 1-6 of the reply).</p><p>We now compared the accuracy and speed of DAS to that of other tools for mice (USVSEG), Bengalese and Zebra finches (TweetyNet), and marmosets (Oikarinen et al., 2019). DAS is as good as or better then these specialized methods, and between 3x and 20x faster. The comparisons are shown in a new Table 2 (reproduced on page 5).</p><p>We have also tested DAS on two more species – marmosets and Zebra finches. For both species, DAS performs excellently (F1 scores 94% for marmosets and 100% for Zebra finches). The two new figures and the two new sections from Results are reproduced on pages 2-4.</p><p>Our wording is now more accurate throughout, to reflect the outcome of these comparisons. For instance, when discussing our results on bird song (p7, l229):</p><p>“Thus, DAS performs as well as specialized deep learning-based methods for annotating bird song (Cohen et al., 2020a; Koumura and Okanoya, 2016) (Table 2).”</p><disp-quote content-type="editor-comment"><p>D. Miscellaneous comments</p><p>1. Interestingly, the song types don't appear to be mutually exclusive. One can have pulse song in the middle of sine song. That might be useful to be able to toggle…I can imagine cases where it would be nice to be able to label things that overlap, but in general if something is sine song, it can't be pulse song. And my assumption certainly was that song types would be mutually exclusive. Adding some explanation of that to the text/user's manual would be useful.</p></disp-quote><p>Yes, song types are mutually exclusive at the moment – we currently choose labels using an argmax operation over the confidence scores for each song type. In a future version one could detect overlapping song types by using an absolute threshold on the confidence scores or by training type-specific networks. We now make the fact that labels are mutually exclusive explicit</p><p>In Results (p3, l112):</p><p>“Annotation labels for the different song types are mutually exclusive and are produced by comparing the confidence score to a threshold or by choosing the most probable song type.”</p><p>And in Methods (p16, l538):</p><p>“In the resulting annotations, song types are mutually exclusive, that is, each sample is labelled as containing a single song type even if song types overlap.”</p><disp-quote content-type="editor-comment"><p>2. How information is combined across channels is aluded to several times but not described well in the body of the manuscript, though it is mentioned in the methods in vague terms:</p><p>&quot;several channel convolutions, ky(1,y), combine information across channels.&quot;</p></disp-quote><p>This is learned by the network during training: Different audio channels are treated like different color channels are treated in vision-based networks – they are filtered by multi-channel filters and the filter weights determine how information is combined across channels. There is no manual step in which information is explicitly combined across channels. In addition to the technical description in the methods, this is now also stated in Results when describing the network multi-channel recordings of fly song (p5, l170):</p><p>“DAS processes multi-channel audio by using filters that take into account information from all channels simultaneously.”</p><disp-quote content-type="editor-comment"><p>II. Usability</p><p>A. Getting it installed</p><p>Installing on Windows 10, was a bit involved if you were not already using python: Anaconda, python, tensorflow, CUDA libraries, create an account to download cuDNN, and update NVIDIA drivers.</p></disp-quote><p>Indeed, getting a GPU accelerated version of tensorflow running is quite involved atm. Installing the CUDA and cuDNN libraries using conda is currently broken. Instead, we now provide instructions for training and inference on colab, which provides server-based GPU instances – see https://janclemenslab.org/das/tutorials/colab.html. This is now mentioned in methods: (p13, l386):</p><p>“We also provide instructions for training DAS using google colab, which provides a GPU-accelerated python environment for the network. Colab removes the need to install GPU libraries: Annotations can be made locally in the GUI without a GPU and training and predicting is done on fast GPU nodes in the cloud. See this notebook for more details: [http://janclemenslab.org/das/tutorials/colab.html].”</p><disp-quote content-type="editor-comment"><p>However, it went OK until I hit this:</p><p>error:</p><p>File &quot;…\miniconda3\envs\dss\lib\ctypes\__init__.py&quot;, line 373, in __init__</p><p>self._handle = _dlopen(self._name, mode)</p><p>FileNotFoundError: Could not find module '…\miniconda3\envs\dss\lib\site-packages\scipy\.libs\libbanded5x.3OIBJ6VWWPY6GDLEMSTXSIPCHHWASXGT.gfortran-win_amd64.dll' (or one of its dependencies). Try using the full path with constructor syntax.</p><p>rummaging around in Stackoverflow I found this (answer #4):</p><p>https://stackoverflow.com/questions/59330863/cant-import-dll-module-in-python</p><p>which was to edit ~\Miniconda3\envs\dss\lib\ctypes\ _init_.py to change winmode=None to winmode=0.</p><p>This worked, but did slow me down a fair bit. I'm not sure who the target audience is for this software, but it may be a bit much for the average biologist.</p></disp-quote><p>Sorry about that. We test using CI on linux/OSX/windows and use the software in the lab on all three OSes without problems. We never encountered this particular issue. We have created a conda package (https://anaconda.org/ncb/das), which should make the installation of a set of inter-compatible packages more reliable.</p><disp-quote content-type="editor-comment"><p>B. Using it</p><p>It wasn't clear to me what &quot; complete annotation&quot; meant: &quot;Once you have completely annotated the song in the first 18 seconds of the tutorial recording…&quot;. Does this mean that all instances of pulse and sine song must be labeled? What are the consequences if this is not true?</p></disp-quote><p>Yes, all instances in the segment must be annotated for training. Unlabelled instances would constitute false negatives in the training data and reduce performance.</p><p>This is now clarified as meaning “all pulses and sine song segments in this stretch of the recording” in the documentation.</p><disp-quote content-type="editor-comment"><p>I also wasn't clear how to know when it finished training and I could try predicting.</p></disp-quote><p>Training is now indicated using a dialog window in the GUI which will close once training is finished and prediction can start.</p><disp-quote content-type="editor-comment"><p>I was a bit confused by there being 2 menu headings called &quot;DeepSS&quot;, one of which (the rightmost) has a Predict option and one of which doesn't. The first time through I used the leftmost DeepSS menu heading for Make dataset and Train, and it didn't work, the second time through I used the rightmost one and it did work (but this might have been user error).</p></disp-quote><p>This is now fixed.</p><disp-quote content-type="editor-comment"><p>III. General usability:</p></disp-quote><p>Thank you for testing the software so thoroughly and for the constructive feedback!</p><disp-quote content-type="editor-comment"><p>1. It would be nice if the oscillogram vertical axis didn't autoscale, it made it hard to recognize sine song when there were no pulses.</p></disp-quote><p>Sine can indeed be hard to detect when there are pulses or when there is low noise. In those cases, we use the spectrogram view to annotate sine. We will add manual vertical scaling to a future version of the software.</p><disp-quote content-type="editor-comment"><p>2. It would be nice to be able to change the playback rate, or the size of the jumps in the navigation bar at the bottom.</p></disp-quote><p>We added a scrollbar and a text field for entering the time to jump to for more easily navigating long recordings.</p><disp-quote content-type="editor-comment"><p>3. It was not very clear in the instructions how to fix errors and use those annotations to train again, although in the body of the text this is explicitly stated as important.</p><p>&quot;We find that DeepSS can be efficiently trained using an iterative protocol (Pereira et al., 2018) (Figure 4C, S8): Annotate a small set of recordings and train the network for a few epochs; then generate annotations on a larger set of recordings and correct these annotations.&quot;</p><p>&quot;Correct any prediction errors-add missing annotations, remove false positive annotations, adjust the timing of annotations.&quot;</p></disp-quote><p>The documentation now contains more information on how to do that:</p><p>https://janclemenslab.org/das/quickstart.html#proof-reading</p><disp-quote content-type="editor-comment"><p>4. The instructions for correcting could be more clear. It took a little fiddling to figure out I needed to switch to &quot;add pulse_proposals&quot; in order to remove a pulse proposal.</p></disp-quote><p>Thanks! We have updated the documentation accordingly:</p><p>https://janclemenslab.org/das/quickstart.html#proof-reading</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In general, I recommend publication after major revision.</p><p>Apart from my comments above, my excitement for this toolbox is a little bit hampered by a few aspects:</p><p>Regarding the comparisons to other tools, it would be great to compare (or at least discuss the performance of other recent tools on the bird and mouse dataset). Existing models are described as slow (Line 50), but no quantitative comparisons are given.</p></disp-quote><p>See Essential Revisions #1 for details (pages 1-6 of the reply).</p><p>We now compared the accuracy and speed of DAS to that of other tools for mice (USVSEG), Bengalese and Zebra finches (TweetyNet), and marmosets (Oikarinen et al., 2019). DAS is as good as or better then these specialized methods, and between 3x and 20x faster. The comparisons are shown in a new Table 2 (reproduced on page 5).</p><p>We have also tested DAS on two more species – marmosets and Zebra finches. For both species, DAS performs excellently (F1 scores 94% for marmosets and 100% for Zebra finches). The two new figures and the two new sections from Results are reproduced on pages 2-4.</p><p>Our wording is now more accurate throughout, to reflect the outcome of these comparisons. For instance, when discussing our results on bird song (p7, l229):</p><p>“Thus, DAS performs as well as specialized deep learning-based methods for annotating bird song (Cohen et al., 2020a; Koumura and Okanoya, 2016) (Table 2). “</p><disp-quote content-type="editor-comment"><p>What are the human recall/precision for Figure 1 (compare to MARS, where a nice classifier vs. human comparison is given: https://www.biorxiv.org/content/10.1101/2020.07.26.222299v1).</p></disp-quote><p>To estimate human recall/precision, two humans annotated the test data for fly song. The results are presented in a new Table 3.</p><p>Discrepancies between human annotators arise largely in the boundaries of sine song (DAS also struggles with these) and whether isolated pulses are counted or not. DAS is as good as or better than FlySongSegmenter and human annotators. Human recall and precision are now marked in Figures 1E and J and addressed in Results (p5, l152):</p><p>“A comparison of DAS’ performance to that of human annotators reveals that our methods exceeds human-level performance for pulse and sine (Figure 1E, J, Table 3).”</p><disp-quote content-type="editor-comment"><p>Is FSS a tunable algorithm and how was it tuned?</p></disp-quote><p>We tested the version of FSS currently used by several labs (found at https://github.com/murthylab/MurthyLab_FlySongSegmenter). This version was hand-tuned for optimal performance for the single and multi-channel recordings used here and is used in several publications (Coen et al., 2014, 2016, Clemens et al., 2018).</p><disp-quote content-type="editor-comment"><p>While the authors emphasize that the architecture does not require any pre-processing, they end up using the &quot;optional&quot; Short-time Fourier transform frontend for all, but the sine wave detection for flies (i.e. mice and bird). So I think this emphasis is ill advised.</p></disp-quote><p>See Essential Revisions #2 for details (page 9 of the reply).</p><p>We now de-emphasize and clarify this point. The crucial difference to existing methods, in which the Short-time Fourier transform (STFT) is a mandatory and separate pre-processing step that is tuned manually, DAS makes this step optional, integrates it into the network, and optimizes the STFT filters with the rest network.</p><disp-quote content-type="editor-comment"><p>In the end, the authors use different network parameters for all datasets (see Table S1A; with/without STFT, # stacks, kernel size…). This begs the question is this necessary. What happens if one trains the same architectures on the different datasets, does the performance suffer? -- I would suppose that is not the case, and with respect to the usability of this toolbox this seems to be an important question. I.e. how should users pick the architecture and hyperparameters?</p></disp-quote><p>See Essential Revisions #2 for details (pages 6-8 of the reply).</p><p>We now demonstrate that DAS performance is fairly robust to changes in individual parameters (so exact parameter choices do not matter, Figure 4 —figure supplement 5), recommend default parameters that should produce performant networks for any type of signal and provide additional details on parameter choices (Methods p15, l475).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The most important issue I had with the paper is that it provided the reader with little sense of DeepSS's performance relative to other software. The paper would be much better if other approaches were implemented on publicly available data sets and compared head-to-head</p></disp-quote><p>See Essential Revisions #1 for details.</p><p>We now compared the accuracy and speed of DAS to that of other tools for mice (USVSEG), Bengalese and Zebra finches (TweetyNet), and marmosets (Oikarinen et al., 2019). DAS is as good as or better then these specialized methods, and between 3x and 20x faster. The comparisons are shown in a new Table 2 (reproduced on page 5).</p><p>We have also tested DAS on two more species – marmosets and Zebra finches. For both species, DAS performs excellently (F1 scores 94% for marmosets and 100% for Zebra finches). The two new figures and the two new sections from Results are reproduced on pages 2-4.</p><p>Our wording is now more accurate throughout, to reflect the outcome of these comparisons. For instance, when discussing our results on bird song (p7, l229):</p><p>“Thus, DAS performs as well as specialized deep learning-based methods for annotating bird song (Cohen et al., 2020a; Koumura and Okanoya, 2016) (Table 2).”</p><disp-quote content-type="editor-comment"><p>The methods should state very clearly where the data sets used are available for download. While the authors may believe in DeepSS, other challengers will inevitably arise. These authors should be given the opportunity to report head-to-head performance against DeepSS on the same data.</p></disp-quote><p>We do hope that others improve on our method! To aid direct performance comparisons, we deposited all data (audio and annotations) used for training and testing DAS here: https://data.goettingen-research-online.de/dataverse/das. In addition, we added a new Table 5 listing (1) all original data sources, (2) direct links to the data used by DAS for each species tested, and (3) links to the trained models including sample data and code for running the models (https://github.com/janclemenslab/das-menagerie).</p><disp-quote content-type="editor-comment"><p>The text references in the mouse section are to figure 3, but the relevant figure is now figure 2.</p></disp-quote><p>Thank you! We fixed all figure references.</p><disp-quote content-type="editor-comment"><p>There are several places where the color scale used is useless, namely in the confusion matrices (Figure 1D, 1I, 2E, 2H, 3C and some supplemental figs). At these performance levels, color just indicates the correct vs. incorrect parts of the graph – relative shades of the heat map are not perceptible.</p></disp-quote><p>We agree that the confusion matrices as shown are not very informative given the performance levels. We still prefer to use color instead of plain text tables to make the information indicated in the text labels of the confusion matrices more glanceable.</p><disp-quote content-type="editor-comment"><p>The birdsong confusion matrices are similarly uninformative, since all that is seen is a bunch of boxes along the diagonal. Worse yet, the discretization of the figure makes it appear that boxes along the diagonal have different sizes/offsets, suggesting structure where there is none. Showing marginals of the matrix like Figure S4E can be useful.</p></disp-quote><p>We now log-scaled the color code for the confusion matrix in Figure 3C and Figure 3 —figure supplement 1D (was S4D) to make annotations errors apparent. We also fixed the issue with the discretization of the figure.</p><disp-quote content-type="editor-comment"><p>In Figure S4C, precision and recall are plotted whereas in S4E false positive and false negatives are plotted. Are both plots needed?</p></disp-quote><p>Given that false positive and false negative rates are now more visible from the log-scaled confusion matrices in Figure 3 —figure supplement 1D, we removed panel S4E.</p></body></sub-article></article>