<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74971</article-id><article-id pub-id-type="doi">10.7554/eLife.74971</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A neural mechanism for detecting object motion during self-motion</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-260733"><name><surname>Kim</surname><given-names>HyungGoo R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9106-4960</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-138210"><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9650-8962</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-134181"><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1635-1273</contrib-id><email>gdeangelis@ur.rochester.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04q78tk20</institution-id><institution>Department of Biomedical Engineering, Sungkyunkwan University</institution></institution-wrap><addr-line><named-content content-type="city">Suwon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>Department of Brain and Cognitive Sciences, Center for Visual Science, University of Rochester</institution></institution-wrap><addr-line><named-content content-type="city">Rochester</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00y0zf565</institution-id><institution>Center for Neuroscience Imaging Research, Institute for Basic Science</institution></institution-wrap><addr-line><named-content content-type="city">Suwon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Clark</surname><given-names>Damon A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e74971</elocation-id><history><date date-type="received" iso-8601-date="2021-10-25"><day>25</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-17"><day>17</day><month>05</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-11-19"><day>19</day><month>11</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.11.16.468843"/></event></pub-history><permissions><copyright-statement>© 2022, Kim et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Kim et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-74971-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-74971-figures-v1.pdf"/><abstract><p>Detection of objects that move in a scene is a fundamental computation performed by the visual system. This computation is greatly complicated by observer motion, which causes most objects to move across the retinal image. How the visual system detects scene-relative object motion during self-motion is poorly understood. Human behavioral studies suggest that the visual system may identify local conflicts between motion parallax and binocular disparity cues to depth and may use these signals to detect moving objects. We describe a novel mechanism for performing this computation based on neurons in macaque middle temporal (MT) area with incongruent depth tuning for binocular disparity and motion parallax cues. Neurons with incongruent tuning respond selectively to scene-relative object motion, and their responses are predictive of perceptual decisions when animals are trained to detect a moving object during self-motion. This finding establishes a novel functional role for neurons with incongruent tuning for multiple depth cues.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>depth perception</kwd><kwd>binocular vision</kwd><kwd>motion perception</kwd><kwd>central visual pathways</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>EY013644</award-id><principal-award-recipient><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>NS118246</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>EY001319</award-id><principal-award-recipient><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural recordings from macaque area MT reveal a novel mechanism for detecting moving objects during self-motion, involving neurons with incongruent tuning for depth from motion parallax and binocular disparity cues.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When an observer moves through the environment, image motion on the retina generally includes components caused by self-motion and objects that move relative to the scene, both of which depend on the depth structure of the scene. Because self-motion typically causes a complex pattern of image motion across the visual field (optic flow, <xref ref-type="bibr" rid="bib21">Gibson et al., 1959</xref>; <xref ref-type="bibr" rid="bib36">Koenderink and van Doorn, 1987</xref>), detecting the movement of objects relative to the world can be a difficult task for the brain to solve. An object that is moving in the world might appear to move faster or slower in the image than objects that are stationary in the scene, depending on the specific viewing geometry. Thus, a critical computational challenge for detecting scene-relative object motion is to identify components of image motion that are not caused by one’s self-motion and the static depth structure of the scene. This is a form of causal inference problem (<xref ref-type="bibr" rid="bib67">Shams and Beierholm, 2010</xref>; <xref ref-type="bibr" rid="bib20">French and DeAngelis, 2020</xref>).</p><p>Object movement may be relatively easy to distinguish from self-motion when the object’s temporal motion profile is clearly different from that of image motion resulting from self-motion (<xref ref-type="bibr" rid="bib39">Layton and Fajen, 2016a</xref>) or when the object moves in a direction that is incompatible with self-motion (<xref ref-type="bibr" rid="bib54">Royden and Connors, 2010</xref>). Neural mechanisms with center-surround interactions in velocity space have been proposed as potential solutions to the problem of detecting object motion under these types of conditions (<xref ref-type="bibr" rid="bib56">Royden and Holloway, 2014</xref>; <xref ref-type="bibr" rid="bib57">Royden et al., 2015</xref>). However, the brain has a remarkable ability to detect object motion even under conditions in which the image velocity of a moving object is very similar to that of stationary background elements during self-motion. <xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref> demonstrated that object movement relative to the scene ‘pops out’ when 3D structure is specified by binocular disparity cues but not in the absence of disparity cues. They suggested that disparity cues help the visual system to discount the global flow field resulting from self-motion, thereby identifying object motion. How the brain might achieve this computation has remained a mystery.</p><p>We previously reported that many neurons in area MT have incongruent tuning for depth defined by binocular disparity and motion parallax cues (<xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>). We speculated that such neurons might play a role in detecting object motion during self-motion by responding selectively to local conflicts between disparity and motion parallax cues (<xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>; <xref ref-type="bibr" rid="bib33">Kim et al., 2016a</xref>). Here, we test this hypothesis directly by recording from MT neurons while monkeys perform a task that requires detecting object motion during self-motion. We show that monkeys perform this task based mainly on local differences in depth as cued by disparity and motion parallax. We demonstrate that MT neurons with incongruent tuning for depth based on disparity and motion parallax are generally more sensitive to scene-relative object motion than neurons with congruent tuning. We show that MT neurons that respond with a consistent preference for scene-relative object motion are predictive of the animals’ perceptual decisions and that training a linear decoder to detect object motion based on MT responses largely reproduces our main empirical results. We also show that selectively decoding neurons with incongruent tuning yields better performance than decoding congruent neurons. Our findings establish a novel mechanism for detecting moving objects during self-motion, thus revealing a sensory substrate for a specific form of causal inference. Because this mechanism relies on sensitivity to local discrepancies between disparity and motion parallax cues, it allows detection of object motion without the need for more complex computations that discount the global flow field. Thus, this local mechanism may be relatively economical for the nervous system to implement and likely provides a complementary approach to mechanisms for computing scene-relative object motion based on optic flow parsing (<xref ref-type="bibr" rid="bib59">Rushton and Warren, 2005</xref>; <xref ref-type="bibr" rid="bib73">Warren and Rushton, 2008</xref>; <xref ref-type="bibr" rid="bib75">Warren and Rushton, 2009b</xref>, a; <xref ref-type="bibr" rid="bib40">Layton and Fajen, 2016b</xref>; <xref ref-type="bibr" rid="bib50">Niehorster and Li, 2017</xref>; <xref ref-type="bibr" rid="bib41">Layton and Niehorster, 2019</xref>; <xref ref-type="bibr" rid="bib42">Layton and Fajen, 2020</xref>; <xref ref-type="bibr" rid="bib51">Peltier et al., 2020</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We recorded from 123 well-isolated single neurons in area MT of two macaques that were trained to perform an object motion detection task during self-motion (53 neurons from monkey 1 [M1] and 70 from monkey 2 [M2]). We begin by describing the task and behavioral data, followed by analysis of the responses of isolated MT neurons during this task. Finally, we demonstrate that a simple linear decoder trained to perform the task based on responses of our MT population can recapitulate our main findings.</p><sec id="s2-1"><title>Stimulus configuration and behavioral task</title><p>During neural recordings, monkeys viewed a display consisting of two square planar objects that were defined by random dot patterns (<xref ref-type="fig" rid="fig1">Figure 1A, B</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>; <xref ref-type="video" rid="video1">Video 1</xref>; see Methods for details). The animal viewed these objects while being translated (0.5 Hz modified sinusoid, see Methods) along an axis in the fronto-parallel plane which corresponded with the preferred-null motion axis of the neuron under study. In the base condition of the task with no cue conflict between depth from disparity and motion parallax, both objects were simulated to be stationary in the world, such that their image motion was determined by the self-motion trajectory and the location of the objects in depth. When the objects were stationary in the world, their depth defined by motion parallax and disparity cues was the same, hence the difference in depth between the two cues was zero (ΔDepth = d<sub>MP</sub> – d<sub>BD</sub> = 0).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Object detection task and behavior.</title><p>(<bold>A</bold>) Schematic illustration of the moving object detection task. Once the animal fixated on a center target, objects were presented while the animal experienced self-motion. Saccade targets then appeared at the center of each object, and the animal indicated the dynamic object (moving relative to the scene) by making a saccade. (<bold>B</bold>) Schematic illustration of stimulus generation from behind and above the observer. A stationary far object that lies within the neuron’s receptive field (RF, dashed circle) has rightward image motion when the observer moves to the right. The other (dynamic) object moves rightward independently in space (cyan arrow) such that the object’s net motion suggests a far depth while binocular disparity cues suggest a near depth. Gray shaded region indicates the display screen; cross indicates the fixation point. (<bold>C</bold>) Average behavioral performance across recording sessions for each animal (n=47 sessions from monkey 1 [M1] and n=57 sessions from monkey 2 [M2], excluding two sessions for which the standard set of ΔDepth values was not used). Error bars denote 95% CIs. (<bold>D</bold>) Normalized regression coefficients for depth from disparity (β<sub>BD</sub>), depth from motion parallax (β<sub>MP</sub>), and ΔDepth (β<sub>Δ</sub>) are shown separately for chosen locations and not-chosen locations (see text for details). Gray and black bars denote data for M1 (n=44) and M2 (n=53), respectively. (<bold>E</bold>) Proportion of fits for which each regression coefficient was significantly different from zero (alpha = 0.05). Format as in panel D.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Visual display and motion trajectories.</title><p>(<bold>A</bold>) A screen shot of the visual stimulus. It consisted of fixation target at the center, two or four objects (two objects shown), and background dots, which were masked within the central region of the display. Red and green dots represent images shown to the left and right eyes. (<bold>B</bold>) Schematic drawing (top view) depicting the rendering geometry for a far stationary object (left) and a near dynamic object (right). Left: the location of a stationary object was initially defined by the horizontal and vertical coordinates on the screen and then was ray-traced onto a virtual plane located at the depth of the object. A moving object was initially positioned at the depth defined by binocular disparity (d<sub>BD</sub>, cyan) in the same way as the stationary object. The location of the object at a different depth (d<sub>MP</sub>) was ray-traced (blue star). Right: once the self-motion trajectory of the animal was determined, the location of the virtual object (blue star) was ray-traced onto the plane defined by binocular disparity (d<sub>BD</sub>) to compute the trajectory of independent movement at d<sub>BD</sub>. The resultant retinal motion mimics motion parallax at depth d<sub>MP.</sub> We used the depth difference (ΔDepth) to manipulate the difficulty of the task. (<bold>C</bold>) Time courses of position (left) and velocity (right) of the animal during the modified sinusoidal translational motion (see Methods for details).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Results from a control experiment including monocularly presented objects.</title><p>Psychometric data are shown from control experiments (n=3 sessions, 1180 total trials) in which dynamic and stationary objects were shown with (binocular) and without (monocular) disparity cues. These data were collected in the version of the task with four objects (see also <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), such that chance level performance was 25% (dashed horizontal line). Monocular conditions were only presented for the largest (easiest) ΔDepth values (16% of total trials). While performance in the binocular condition shows a robust V-shaped curve similar to the main dataset (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), performance in the monocular condition is very poor. Error bars represent 95% CIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Behavioral performance in the more generalized task with four objects.</title><p>(<bold>A</bold>) An example session from monkey 1 (M1) in which the animal performed the detection task with four objects and three pedestal depths (red, green, and blue colors). (<bold>B</bold>) An example session from monkey 2 (M2) prior to neural recordings. (<bold>C</bold>) Another example session from M2 after neural recording experiments were completed. (<bold>D</bold>) Normalized beta coefficients from logistic regression analysis for the four-object task; format as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Data include 35 sessions from M1 from before neural recordings, 27 sessions from M2 from before neural recordings, and 10 sessions from M2 after neural recordings. (<bold>E</bold>) Proportion of beta coefficients that are significantly different from zero. Format as in <xref ref-type="fig" rid="fig1">Figure 1E</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig1-figsupp3-v1.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-74971-video1.mp4"><label>Video 1.</label><caption><title>Visual stimuli used in the dynamic object detection task.</title><p>Examples of visual stimuli in the two-object task, assuming that the receptive field of a neuron is located on the horizontal meridian. The video shows a sequence of seven stimuli, which are sorted by their ΔDepth values (ΔDepth = −1.53, −0.57, −0.21, 0, 0.21, 0.57, and 1.53 deg). The depth of the stationary object in each stimulus is labeled and was chosen randomly. In the actual experiment, the fixation target was stationary in the world, and the motion platform moved the animal and screen sinusoidally along an axis in the fronto-parallel plane (here a horizontal axis). Thus, the video shows the scene from the viewpoint of the moving observer. The stimulus sequences are equivalent to a situation in which the observer remains stationary, and the entire scene is translated in front of the observer. For each ΔDepth value, two full cycles of the stimulus are shown for display purposes; in the actual experiment, each trial consisted of just one cycle. During the second cycle of each stimulus in the video, the text label indicates whether the dynamic object was on the left or right side of the display. Red and green dots in the video denote the stereo half-images for the left and right eyes. Note that, without viewing the images stereoscopically and tracking the fixation target, it is generally not possible to determine the location of the dynamic object from the image motion on the display.</p></caption></media><p>In other conditions (ΔDepth ≠ 0), one of the objects was stationary in the world while the second ‘dynamic’ object moved in space such that its depth defined by motion parallax, d<sub>MP</sub>, was not consistent with its depth defined by binocular disparity, d<sub>BD</sub> (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>; see Methods for details). As a result of this cue conflict between disparity and motion parallax, the dynamic object should appear to be moving in the world based on previous work in humans (<xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref>). As ΔDepth becomes greater in magnitude, it should be easier for the animal to correctly determine which object is the dynamic object. Animals indicated their decision by making a saccade to one of two targets that appeared at the locations of the two objects at the end of the trial (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Critically, due to the experimental design (see Methods for details), animals could not simply detect the dynamic object based on its retinal image velocity since the stationary object(s) in the display also moved on the retina due to self-motion combined with depth variation.</p><p>Average psychometric functions for the two animals across 104 recording sessions are shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. As expected, the animals perform at chance when ΔDepth = 0, and their percent correct increases with the magnitude of ΔDepth. This demonstrates that monkeys can perform the task as expected from human behavioral work (<xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref>). Furthermore, we found that performance was near chance levels in a control experiment without binocular disparity cues (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), as also expected from previous work (<xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref>).</p><p>The ranges of depths of the stationary and dynamic objects were overlapping but not identical (see Methods). To determine whether the animals primarily made their decisions based on ΔDepth and not based upon the individual depths specified by disparity or motion parallax, we performed a logistic regression analysis to determine how animals perceptually weighted depth from motion parallax (|d<sub>MP</sub>|), depth from binocular disparity (|d<sub>BD</sub>|), and the magnitude of ΔDepth (<inline-formula><mml:math id="inf1"><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> ; see Methods for details). Results show that animals primarily weighted the |ΔDepth| cue to make their decisions (<xref ref-type="fig" rid="fig1">Figure 1D, E</xref>), although there were small contributions from the individual depth cues. We initially trained each animal to perform the task with four objects present in the display (three stationary objects and one dynamic object), as well as three different pedestal depths, to make it more difficult for animals to rely on d<sub>MP</sub> or d<sub>BD</sub>. Indeed, we found that the logistic regression weights were also strongly biased in favor of |ΔDepth| in the four-object version of the task (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3D, E</xref>). To increase the number of stimulus repetitions we could perform during recording experiments, we simplified the task to the two-object case.</p></sec><sec id="s2-2"><title>Congruency of depth preferences and responses to dynamic objects</title><p>We measured the tuning of well-isolated MT neurons for depth defined by either binocular disparity or motion parallax cues, as described previously (<xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>, see also Methods). Receptive fields (RFs) and direction preferences of the population of MT neurons are summarized in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows data for a typical ‘congruent’ cell, which prefers near depth defined by both disparity and motion parallax cues (see Methods for definition of congruent and opposite cells). Note that motion parallax stimuli are presented monocularly, such that selectivity for depth from motion parallax cannot be a consequence of binocular cues. In contrast, <xref ref-type="fig" rid="fig2">Figure 2B, C</xref> show data for two examples of ‘opposite’ cells that prefer near depths defined by motion parallax and moderate far depths defined by binocular disparity. Such neurons would, in principle, respond more strongly to some stimuli with discrepant disparity and motion parallax cues. Note that, for all of the example cells in <xref ref-type="fig" rid="fig2">Figure 2A–C</xref>, responses to binocular disparity are substantially greater than responses to motion parallax. This is mainly because binocular disparity tuning was measured with constant-velocity stimuli at the preferred speed, whereas the range of speeds used to measure depth tuning based on motion parallax is generally lower (and covaries with depth magnitude).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Responses of representative MT neurons.</title><p>(<bold>A</bold>) Depth tuning curves for an example ‘congruent’ neuron preferring near depths based on both binocular disparity (magenta) and motion parallax (cyan) cues (DSDI<sub>BD</sub> = −0.81; DSDI<sub>MP</sub> = −0.70 [DSDI, depth-sign discrimination index]; p&lt;0.05 for both, permutation test; correlation R<sub>MP_BD</sub> = 0.76, p=0.016). Dashed horizontal lines indicate baseline activity for each tuning curve. (<bold>B</bold>) Tuning curves for an example ‘opposite’ neuron preferring small far depths based on binocular disparity but preferring near depths based on motion parallax (DSDI<sub>BD</sub> = 0.41; DSDI<sub>MP</sub> = −0.67; p&lt;0.05 for both, permutation test; R<sub>MP_BD</sub> = −0.36, p=0.32). (<bold>C</bold>) Another example opposite cell preferring far depths based on binocular disparity but near depths based on motion parallax (DSDI<sub>BD</sub> = 0.46; DSDI<sub>MP</sub> = −0.56; p&lt;0.05 for both, permutation test; R<sub>MP_BD</sub> = −0.73, p=0.025). (<bold>D</bold>) Responses of the neuron in panel A to stationary objects (red) and dynamic objects (blue) during performance of the detection task. Stationary objects were presented at various depths (bottom abscissa). Dynamic objects generally have conflicts (ΔDepth ≠ 0) between depth from motion parallax (bottom abscissa) and binocular disparity (top abscissa). The pedestal depth at which ΔDepth = 0 is shown as an unfilled blue triangle. (<bold>E</bold>) Responses during the detection task for the opposite cell of panel B. Format as in panel D. (<bold>F</bold>) Responses during detection for the neuron of panel C. Error bars in all panels represent s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Distribution of receptive field (RF) properties.</title><p>(<bold>A</bold>) Positions and sizes of the RFs of our sample of MT neurons (n=123). Each circle represents the contour of the RF, defined as the center and diameter obtained from the RF mapping and size tuning protocols (see Methods). (<bold>B</bold>) Distribution of preferred directions for the sample of neurons (n=123), where 0 deg corresponds to rightward motion and 90 deg corresponds to upward motion.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig2-figsupp1-v1.tif"/></fig></fig-group><p>As done previously (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Nadler et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Kim et al., 2015a</xref>, <xref ref-type="bibr" rid="bib32">Kim et al., 2015b</xref>; <xref ref-type="bibr" rid="bib35">Kim et al., 2017</xref>), we quantified the depth-sign preference of each MT neuron using a depth-sign discrimination index (DSDI, see Methods), which takes on negative values for neurons with near preferences and positive values for neurons with far preferences. Across the population of 123 neurons, depth-sign preferences for motion parallax tended to be strongly biased toward near-preferring neurons, as reported previously (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>), whereas depth-sign preferences for binocular disparity were rather well balanced (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Importantly, there are roughly equal numbers of neurons in the lower-left and upper-left quadrants of <xref ref-type="fig" rid="fig3">Figure 3A</xref>, indicating that congruent and opposite cells were roughly equally prevalent in our sample of MT neurons (see also <xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>). Thus, there are many opposite cells in MT that might respond selectively to dynamic objects over static objects.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Relationship between selectivity for moving objects and congruency between depth cues.</title><p>(<bold>A</bold>) Population summary of congruency of depth tuning for disparity and motion parallax. The depth-sign discrimination index (DSDI) value for binocular disparity tuning (DSDI<sub>BD</sub>) is plotted as a function of the DSDI value for motion parallax tuning (DSDI<sub>MP</sub>) for each neuron (n=123). Triangles and squares denote data for monkey 1 (M1) (n=53) and monkey 2 (M2) (n=70), respectively. (<bold>B</bold>) Population summary of relationship between relative responses to dynamic and stationary objects as a function of depth tuning congruency. The ordinate shows the ratio of peak responses for dynamic:stationary stimuli. The abscissa shows the correlation coefficient (R<sub>MP_BD</sub>) between depth tuning for motion parallax and disparity. Dashed line is a linear fit using type 2 regression (n=106; n=47 from M1 and n=59 from M2; sample includes all neurons for which we completed the detection task). (<bold>C</bold>) Population summary (n=106) of the relationship between the preference for ΔDepth of the dynamic object (as quantified by DSDI<sub>dyn</sub>, see Methods) and the difference between DSDI<sub>MP</sub> and DSDI<sub>BD</sub>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig3-v1.tif"/></fig><p><xref ref-type="fig" rid="fig2">Figure 2D</xref> shows responses of the example congruent cell (from <xref ref-type="fig" rid="fig2">Figure 2A</xref>) that were obtained during the object detection task. Responses to the stationary object (red) are plotted as a function of the depth values specified by motion parallax (which are necessarily equal to binocular disparity values for a stationary object). Responses to the dynamic object (blue) are plotted as a function of both depth defined by motion parallax (lower abscissa) and depth defined by disparity (upper blue abscissa). This allows the reader to determine the depth value for each cue that is associated with a dynamic object having a particular ΔDepth value. For this example congruent cell (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), responses to stationary objects with large near depths substantially exceeded responses to any dynamic object.</p><p>A strikingly different pattern of results is seen for the example opposite cell in <xref ref-type="fig" rid="fig2">Figure 2E</xref>. In this case, there are a few dynamic objects for which the neuron’s response (blue) clearly exceeds the response to stationary objects of all different depth values (red). More specifically, this incongruent cell responds most strongly to dynamic objects that have large near depths defined by motion parallax and depths near the plane of fixation (0 deg) as defined by binocular disparity. This pattern of results is expected from the individual tuning curves in <xref ref-type="fig" rid="fig2">Figure 2B</xref> and demonstrates that this opposite cell is preferentially activated by a subset of dynamic objects. The second example opposite cell in <xref ref-type="fig" rid="fig2">Figure 2C, F</xref> shows a generally similar pattern of results. For this cell, peak responses to stationary and dynamic objects are similar, but the neuron responds more strongly to dynamic objects over most of the stimulus range. Since we applied our ΔDepth manipulation around a fixed pedestal depth of –0.45 deg (to facilitate decoding, see Methods), we do not expect dynamic objects to preferentially activate every opposite cell. However, cells that are preferentially activated by dynamic objects should tend to be neurons with mismatched depth tuning for motion parallax and binocular disparity cues.</p><p><xref ref-type="fig" rid="fig3">Figure 3B</xref> shows that this expected relationship holds across our population of MT neurons. The ratio of peak responses for dynamic:stationary objects is plotted as a function of the correlation coefficient, R<sub>MP_BD</sub>, between depth tuning curves for disparity and motion parallax. Neurons with R<sub>MP_BD</sub> &lt; 0 (opposite cells) tend to have peak response ratios that lie in the upper-left quadrant, indicating that opposite cells tend to be preferentially activated by dynamic objects. In contrast, neurons with R<sub>MP_BD</sub> &gt; 0 (congruent cells) tend to have peak response ratios in the lower-right quadrant, indicating that they tend to be preferentially activated by stationary objects. Across the population, peak response ratio is significantly anti-correlated with R<sub>MP_BD</sub> (n=106, Spearman rank correlation, R = −0.39, p=2.8×10<sup>–5</sup>), indicating that the hypothesized relationship between tuning congruency and response to scene-relative object motion is observed.</p><p>We further tested whether differences in depth tuning curves for binocular disparity and motion parallax can predict whether neurons prefer positive or negative ΔDepth values. Using responses to the dynamic object, we quantified each neuron’s preference for positive/negative ΔDepth values using a variant of the DSDI metric, DSDI<sub>dyn</sub> (see Methods), and found that it is robustly correlated with the difference in DSDI values (ΔDSDI) computed from depth tuning curves for disparity and motion parallax (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <italic>R</italic>=0.54, p=2.7×10<sup>–9</sup>, n=106, Spearman correlation). Thus, selectivity for ΔDepth during the detection task is reasonably predictable from the congruency of depth tuning measured during a fixation task.</p></sec><sec id="s2-3"><title>Correlation with perceptual decisions</title><p>If neurons with mismatched depth tuning for disparity and motion parallax cues are selectively involved in detecting scene-relative object motion, we hypothesized that neurons that respond preferentially to dynamic objects should have responses that are more strongly correlated with perceptual decisions. To measure the correlation of neural activity with perceptual decisions, we took advantage of the fact that our design included a subset of trials in which both objects were stationary in the world and were presented at the pedestal depth of –0.45 deg (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). These conditions allowed us to quantify choice-related activity, for a fixed stimulus, by sorting responses into two groups: trials in which the monkey chose the object in the neuron’s RF, and trials in which the monkey chose the object in the opposite hemi-field.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Relationship between MT responses and detection of object motion.</title><p>(<bold>A</bold>) When ΔDepth = 0, two stationary objects at the pedestal depth had identical retinal motion and depth cues. Animals still were required to report one of the objects as dynamic. (<bold>B</bold>) To compute detection probability (DP), responses to the ΔDepth = 0 condition were z-scored and sorted into two groups according to the animal’s choice. Filled and open bars show distributions of z-scored responses of an example MT neuron when the animal reported that the moving object was in and out of the receptive field (RF), respectively. (<bold>C</bold>) Distribution of DP values for a sample of 92 MT neurons, including all neurons tested in the detection task for which the animal made at least five choices in each direction (see Methods). Arrowhead shows the mean DP value of 0.56, which was significantly greater than 0.5 (p=6.0×10<sup>–5</sup>, n=92, <italic>t</italic>-test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Time courses of choice-related activity.</title><p>Average population responses in the ambiguous trials (ΔDepth = 0) sorted by the animal’s choice. Neurons showing positive detection probability values were analyzed (n=64 neurons). (<bold>A</bold>) Average time courses of z-scored responses for the subset of trials in which self-motion began toward the neuron’s preferred direction (phase 0). We first computed the moving averages of spiking activity (150 ms window) for each neuron. The results were then z-scored based on the mean and SD of the moving averages for each neuron. (<bold>B</bold>) Responses when self-motion began toward the neuron’s anti-preferred direction (phase 180). (<bold>C</bold>) The differential response between the two choice groups shown in panel A. Gray marks denote time points at which the differential response is significantly different from zero (alpha = 0.05, n=64, Wilcoxon signed-rank test). (<bold>D</bold>) Differential response between the two choice groups shown in panel B. Shadings represent s.e.m.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Data for an example neuron (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) show somewhat greater responses when the monkey chose the object located in the neuron’s RF. We quantified this effect by applying ROC analysis to the two choice distributions (see Methods for details), which yielded a detection probability (DP) metric. DP will be greater than 0.5 when responses are greater on trials in which the monkey reported that the stimulus in the RF was the dynamic object. Because DP is only computed from the subset of trials with ΔDepth=0, this measure need not have any relationship with a neuron’s preference for dynamic vs. stationary stimuli. For the example neuron of <xref ref-type="fig" rid="fig4">Figure 4B</xref>, the DP value was 0.75, which is significantly greater than chance by permutation test (p=0.006, see Methods). Across a population of 92 neurons for which there were sufficient numbers of choices toward each stimulus (see Methods), the mean DP value of 0.56 was significantly greater than chance (p=6×10<sup>–5</sup>, t(91) = 4.21, n=92, <italic>t</italic>-test) with 13 of 92 neurons showing individually significant DP values (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, filled bars). All neurons with significant DP values had effects in the expected direction, with DP&gt;0.5. In addition, the mean DP value was significantly greater than chance for each monkey individually (M1: n=39, mean=0.59, p=5.7×10<sup>–4</sup>, t(38) = 3.76; M2: n=53, mean = 0.53, p=0.03, t(52) = 2.21, <italic>t</italic>-test).</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows that many MT neurons have responses that are correlated with detection choices in the task. We hypothesized that neurons with DP&gt;0.5 are more likely to be those that respond preferentially to dynamic objects over stationary objects. To obtain a signal-to-noise measure of each neuron’s selectivity for dynamic vs. stationary objects, we again applied ROC analysis as illustrated for an opposite cell in <xref ref-type="fig" rid="fig5">Figure 5A–C</xref>. This neuron responded more strongly to dynamic objects than stationary objects across most of the depth range (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). To quantify this selectivity, for each value of ΔDepth, responses were sorted into two groups: trials in which the dynamic object was in the RF, and trials in which the dynamic object was located in the opposite hemi-field and a stationary object was in the RF (regardless of the depth of the stationary object). Thus, the ROC value computed for each ΔDepth value gave an indication of how well the neuron discriminated between that particular dynamic object and stationary objects of any depth. By convention, ROC values &gt;0.5 indicate greater responses for a dynamic object in the RF.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Relationship between detection probability (DP) and neurometric performance for dynamic objects.</title><p>(<bold>A</bold>) Responses of an example opposite neuron to dynamic and stationary objects during the detection task. Format as in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. (<bold>B</bold>) ROC values comparing responses to a dynamic object at each value of ΔDepth with responses to stationary objects, for the neuron of panel A. Neurometric performance (NP = 0.78 for this neuron) is defined as the average ROC area for all ΔDepth ≠ 0. (<bold>C</bold>) Distribution of z-scored responses sorted by choice for the same neuron as in panels A,B. Format as in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. (<bold>D–F</bold>) Data from an example congruent cell, plotted in the same format as panels A–C. (<bold>G</bold>) Relationship between DP and NP for a population of 92 MT neurons. Dashed line: linear fit using type 2 regression (slope = 1.06, slope CI = [0.80 1.48]; intercept = –0.04, intercept CI = [–0.31 0.11]).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Relationships between preference for dynamic objects, neurometric performance (NP), and detection probability (DP).</title><p>(<bold>A</bold>) NP is plotted as a function of peak response ratio (dynamic:stationary objects). Triangles and squares denote data for monkey 1 (M1) (n=47) and monkey 2 (M2) (n=59), respectively. NP is significantly correlated with peak response ratio (<italic>R</italic>=0.43, p=4.5×10<sup>–6</sup>, Spearman rank correlation), such that neurons with greater peak response ratios tend to have greater NP. (<bold>B</bold>) DP is plotted as a function of peak response ratio for the subset of 92 neurons for which DP values could be computed (M1: n=39; M2: n=53). The positive trend in this relationship did not reach significance (<italic>R</italic>=0.12, p=0.25).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Results of this analysis for the example opposite cell (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) show that ROC values were greater than 0.5 for all ΔDepth ≠ 0; thus, this neuron reliably responded more strongly to dynamic objects than to stationary objects. To obtain a single metric for each neuron, we simply averaged the ROC metrics for each non-zero ΔDepth value, yielding a neurometric performance (NP) value of 0.78 for this neuron. The corresponding DP value for this neuron was 0.77 (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, p=0.0015, permutation test), indicating that this neuron shows both strong selectivity for dynamic objects when ΔDepth ≠ 0 and stronger responses when the animal reports a dynamic object in the RF when ΔDepth = 0.</p><p>Data for an example congruent cell (<xref ref-type="fig" rid="fig5">Figure 5D–F</xref>) show a very different pattern of results. This neuron generally responds more strongly to stationary objects of any depth than to dynamic objects (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). As a result, ROC values are consistently &lt;0.5 when comparing responses to dynamic vs. stationary objects in the RF (ΔDepth ≠ 0, <xref ref-type="fig" rid="fig5">Figure 5E</xref>), yielding an NP value of 0.23. The corresponding DP value for this neuron (<xref ref-type="fig" rid="fig5">Figure 5F</xref>) was 0.39 (p=0.26, permutation test), indicating that it responded slightly more to ambiguous stimuli when the monkey reports that the object in the RF was stationary. Thus, the data from these two examples neurons support the hypothesis that neurons with preferences for dynamic objects are selectively correlated with perceptual decisions.</p><p>To examine whether this hypothesis holds at the population level, we plotted the DP value for each neuron against the corresponding NP value. These two metrics, which are computed from completely different sets of trials (ΔDepth = 0 for DP; ΔDepth ≠ 0 for NP), are strongly correlated (<xref ref-type="fig" rid="fig5">Figure 5G</xref>, <italic>R</italic>=0.47, p=3.2×10<sup>–6</sup>, n=92, Spearman rank correlation) such that neurons with DP values substantially greater than 0.5 tend to be neurons that are selective for dynamic objects (NP &gt;0.5). In addition, we observed a significant positive correlation for each animal individually (M1: n=39, <italic>R</italic>=0.59, p=7.9×10<sup>–5</sup>; M2: n=53, <italic>R</italic>=0.33, p=0.015, Spearman correlation). It is also worth noting that all neurons with large DP values (&gt;0.7) also have NP values substantially greater than 0.5. Thus, the MT neurons that most strongly predict decisions to detect the dynamic object (on ambiguous trials) are those with a consistent preference for dynamic objects.</p><p>It is worth noting that the distribution of NP values in <xref ref-type="fig" rid="fig5">Figure 5G</xref> is biased toward values &gt;0.5; indeed, the mean NP value (0.56) is significantly greater than 0.5 (one-sample <italic>t</italic>-test, <italic>t(105</italic>) = 4.98, and p=2.5×10<sup>–6</sup>). This effect likely arises due to the distribution of stimulus values involved in the dynamic object condition. Because neurons were generally tested with a pedestal depth of –0.45 deg, the depth values for both disparity and motion parallax tend to be mostly negative for the dynamic object condition (see blue and black x-axes in <xref ref-type="fig" rid="fig2">Figure 2D–F</xref>). This bias toward negative (near) depth values of dynamic objects, combined with the fact that most neurons have a near preference for depth from motion parallax (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), means that many neurons (including congruent cells) tend to have mean responses to dynamic objects that are greater than the mean response to stationary objects (e.g. <xref ref-type="fig" rid="fig2">Figure 2D</xref>). This asymmetry leads to a mean NP value &gt;0.5.</p><p>How is the result of <xref ref-type="fig" rid="fig5">Figure 5G</xref> related to neurons’ preference for dynamic relative to stationary objects? <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref> shows that NP is robustly correlated with peak response ratio (<italic>R</italic>=0.43, p=4.5×10<sup>–6</sup>, Spearman rank correlation). Neurons with peak response ratios substantially greater than unity almost always have NP &gt;0.5. However, some neurons with peak response ratios near unity also have high NP values (including the neuron in <xref ref-type="fig" rid="fig2">Figure 2C, F</xref>). In contrast, we did not find a significant correlation between DP and peak response ratio (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>: <italic>R</italic>=0.12, p=0.25). A main reason for this appears to be that there is a cluster of neurons with large peak response ratios that have DP values near 0.5. These neurons generally have tuning properties similar to the example cell in <xref ref-type="fig" rid="fig2">Figure 2B, E</xref>. While incongruent tuning creates a clear preference for dynamic objects for these neurons, that preference is limited to a narrow range of depth values, such that NP values tend to be only modestly &gt;0.5. To achieve high NP values, neurons need to have a consistent preference for dynamic objects (e.g. <xref ref-type="fig" rid="fig2">Figures 2F</xref> and <xref ref-type="fig" rid="fig5">5A</xref>). Thus, what seems to be crucial for producing high DP values is that a neuron consistently prefers dynamic objects over the range tested. This explains the robust correlation between DP and NP (<xref ref-type="fig" rid="fig5">Figure 5G</xref>) and the weak correlation between DP and peak response ratio (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). Thus, incongruent tuning tends to lead to a preference for dynamic objects (high peak response ratio, <xref ref-type="fig" rid="fig3">Figure 3B</xref>), but does not always lead to high DP values (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>).</p><p>We also examined the time course of choice-related activity and found that it appeared within a few hundred ms after the onset of self-motion (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This choice-related activity was largely sustained throughout the rest of the stimulus period, even when motion of the object was in the anti-preferred direction.</p></sec><sec id="s2-4"><title>Decoding model</title><p>The results described above suggest that perceptual detection of dynamic objects might be driven by the activity of MT neurons with depth tuning curves that make them respond preferentially to dynamic objects. To further probe this hypothesis, we trained a simple linear decoder to detect dynamic objects based on simulated responses of a population of neurons that is closely based on our data, and we examined whether performance of the decoder shows a similar relationship between DP and NP (see Methods for details).</p><p>In the simulation (as in the experiments), the dynamic object could appear in either the right or left hemi-field, and the decoder was trained to report the location of the dynamic object. For each neuron in the population, responses were simulated to have the same mean and SD as empirically measured responses. Since neurons were recorded separately and we could not measure correlated noise, we simulated responses based on either independent noise or correlated noise (see Methods for details).</p><p>The decoder was trained to report the location of the dynamic object based on simulated population responses from the subset of trials for which ΔDepth ≠ 0. The trained decoder was then used to predict responses for the completely ambiguous (ΔDepth = 0) trials in which identical objects were presented in both hemi-fields. Responses to ambiguous trials were then sorted according to the decoder output to compute predicted DP values (DP<sub>pred</sub>) for each neuron in the population.</p><p>We first compared DP<sub>pred</sub> with NP values for simulations in which all neurons were assumed to have independent noise. This decoder performs very well based on a sample of 97 MT neurons (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, see Methods for selection criteria), indicating that there is extensive information available in a moderately sized sample of MT neurons. We find a significant positive correlation between DP<sub>pred</sub> and NP (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, <italic>R</italic>=0.53, p=4.9×10<sup>–8</sup>, n=97, Spearman correlation) in this simulation, consistent with the empirical observations of <xref ref-type="fig" rid="fig5">Figure 5G</xref>. The decoding weights provide an indication of how neurons with different properties contribute to the classification outcomes. With independent noise, we find a strong relationship (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, <italic>R</italic>=0.92, p&lt;1×10<sup>–15</sup>, n=97, Spearman correlation) between DP<sub>pred</sub> and decoding weights, with positive readout weights being associated with DP<sub>pred</sub> values greater than 0.5.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Linear decoding reproduces the relationship between detection probability (DP) and neurometric performance (NP).</title><p>(<bold>A</bold>) Performance of a linear decoder that was trained to detect moving objects based on simulated population responses with independent noise (see Methods for details). Error bars represent 95% CIs (n=100 simulations). (<bold>B</bold>) Neural responses were sorted by the output of the decoder to compute a predicted DP (DP<sub>pred</sub>) for each unit in the simulated population (n=97, including all neurons recorded in the detection task using identical ΔDepth and stationary depth values, see Methods). DP<sub>pred</sub> is plotted as a function of the measured NP for each neuron. Error bars represent 95% CIs (n=100 simulations). (<bold>C</bold>) Relationship between DP<sub>pred</sub> and the readout weight (β) for each unit in the decoded population. Error bars represent 95% CIs. (<bold>D–F</bold>) Analogous results for a decoder that was trained based on population responses with modest correlated noise (see text and Methods for details). Format as in panels A-C.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig6-v1.tif"/></fig><p>While the relationship between DP<sub>pred</sub> and NP in <xref ref-type="fig" rid="fig6">Figure 6B</xref> has a positive slope, DP<sub>pred</sub> values tend to be substantially closer to 0.5 than the values observed experimentally (<xref ref-type="fig" rid="fig5">Figure 5G</xref>). However, this is not surprising given that neurons in this simulation were assumed to have independent noise. It is well established that neurons in MT exhibit correlated noise (e.g. <xref ref-type="bibr" rid="bib80">Zohary et al., 1994</xref>; <xref ref-type="bibr" rid="bib27">Huang and Lisberger, 2009</xref>) and that choice-related activity is expected to be stronger in the presence of correlated noise (<xref ref-type="bibr" rid="bib5">Britten et al., 1996</xref>; <xref ref-type="bibr" rid="bib66">Shadlen et al., 1996</xref>; <xref ref-type="bibr" rid="bib26">Haefner et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Gu et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Pitkow et al., 2015</xref>). Thus, we also simulated responses with a moderate level of correlated noise (median R<sub>noise</sub> = 0.15, see Methods for details), which had little impact on decoder performance (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). In the presence of correlated noise, DP<sub>pred</sub> values show a greater spread around 0.5 and are much more strongly correlated with NP values (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, <italic>R</italic>=0.89, p&lt;3×10<sup>–16</sup>, n=92, Spearman correlation). While correlated noise enhances the relationship between DP<sub>pred</sub> and NP, it also weakens the relationship between DP<sub>pred</sub> and decoding weights (<xref ref-type="fig" rid="fig6">Figure 6F</xref>, <italic>R</italic>=0.46, p=3.4×10<sup>–6</sup>, n=92, Spearman correlation), as expected from theoretical studies (<xref ref-type="bibr" rid="bib26">Haefner et al., 2013</xref>).</p><p>These simulations show that our main experimental finding is recapitulated by a simple linear decoder that is trained to distinguish between dynamic and static objects based on MT responses. Note, however, that we have not attempted to find parameters of our decoding simulations that would best match the empirical data (<xref ref-type="fig" rid="fig5">Figure 5G</xref>). This would almost certainly be possible, but we do not feel that it is a worthwhile exercise given that we would have to make assumptions about the structure of correlated noise that we cannot sufficiently constrain.</p><p>To further assess whether neurons with incongruent tuning for disparity and motion parallax can provide a greater contribution to detecting scene-relative object motion, we performed additional decoding simulations after dividing the population into subgroups based on peak response ratio. Four subgroups were defined: lowest third, middle third, highest third, and a random subsample of the same size from all neurons. <xref ref-type="fig" rid="fig7">Figure 7A</xref> shows that decoding performance is significantly greater for the subgroup of neurons with the highest peak response ratios, as compared with the low and middle subgroups (error bars denote 95% CIs). We also performed a similar analysis after dividing the population based on the correlation between depth tuning from disparity and motion parallax cues, R<sub>MP_BD</sub>. This revealed parallel results in which neurons with the most negative values of R<sub>MP_BD</sub> achieved significantly greater decoding performance (proportion correct: 0.88±0.008, mean ±95% CIs) than neurons with intermediate (0.82±0.01) or high (0.84±0.009) values of R<sub>MP_BD</sub>.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Decoder performance depends on selectivity for dynamic objects.</title><p>Decoding was performed separately for three subgroups of MT neurons based on their peak response ratios: lowest third (n=33), middle third (n=32), and highest third (n=32) of peak response ratio values. In addition, decoding was performed for mixed subgroups of the same size (n=32) that were selected randomly from the population. (<bold>A</bold>) Decoder performance for each subgroup based on peak response ratio. For each subgroup, 500 decoding simulations were performed (gray dots) using different samples of correlated noise, as described in Methods. For the mixed subgroup, each of the 500 simulations also involved drawing (without replacement) a new random subset of 32 neurons from the population. Black filled symbols denote mean proportion correct across the 500 simulations, and error bars represent 95% CIs. (<bold>B</bold>) Predicted detection probability (DP<sub>pred</sub>) for each neuron in each subgroup (gray symbols), along with mean DP<sub>pred</sub> values (black symbols, error bars denote s.e.m). For the mixed subgroup, data are shown for all 97 neurons since each neuron was included in many different subsamplings. In this case, gray data points represent the mean DP<sub>pred</sub> value for all random subsamplings (165, on average) that included each neuron. Median DP<sub>pred</sub> was not significantly different from 0.5 for the lowest third subgroup (p=0.59, n=33), was marginally significant for the middle third subgroup (p=0.043, n=32), and was significantly greater than 0.5 for the highest third (p=2.2×10<sup>–5</sup>, n=32) and mixed (p=3.2×10<sup>–5</sup>, n=97) subgroups (signed-rank tests comparing median DP<sub>pred</sub> with 0.5).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74971-fig7-v1.tif"/></fig><p>We further examined whether detection probabilities predicted by the decoder depend on tuning congruency, using the same subgroups based on peak response ratio. We find that DP<sub>pred</sub> values from these decoding simulations are greatest for neurons with the highest peak response ratios, and not significantly above chance for neurons with the lowest peak response ratios (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, see caption for details). These results demonstrate that neurons with incongruent tuning carry enhanced information for detecting moving objects during self-motion.</p><p>Together, these decoding simulations demonstrate that a population of MT neurons with the depth tuning properties that we have described could be utilized to detect scene-relative object motion, and that such a read-out could produce the relationship between DP and NP that we have observed empirically. The simulations further indicate that neurons with larger peak response ratios provide better decoding performance, consistent with the idea that incongruent tuning is adaptive for detecting object motion during self-motion.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We find that neurons having incongruent depth tuning for binocular disparity and motion parallax cues often respond more strongly to objects that move in the world than to stationary objects. Moreover, neurons with a consistent preference for dynamic objects tend to more strongly predict perceptual decisions regarding object motion relative to the scene. While it has been established that humans can detect object motion based on cue conflicts between binocular disparity and motion parallax (<xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref>), in the absence of other cues to object motion, the neural basis of this capacity has remained unknown. Our findings establish a simple neural mechanism for detecting moving objects, which can be computed locally and is complementary to flow-parsing mechanisms that involve more global computations (discussed further below). In addition, our findings establish another important function for neurons with mismatched tuning for multiple stimulus cues, building on recent studies (<xref ref-type="bibr" rid="bib34">Kim et al., 2016b</xref>; <xref ref-type="bibr" rid="bib22">Goncalves and Welchman, 2017</xref>; <xref ref-type="bibr" rid="bib62">Sasaki et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Sasaki et al., 2019</xref>; <xref ref-type="bibr" rid="bib78">Zhang et al., 2019a</xref>). Our task involves a form of causal inference (<xref ref-type="bibr" rid="bib38">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Shams and Beierholm, 2010</xref>), and our findings support the idea that a sensory representation consisting of a mixture of congruent and opposite cells provides a useful sensory substrate for causal inference (<xref ref-type="bibr" rid="bib53">Rideaux et al., 2021</xref>). To our knowledge, these findings provide the first empirical evidence for a specific contribution of opposite neurons to perceptual inference about causes of sensory signals.</p><sec id="s3-1"><title>Comparison to other types of mechanisms for detecting object motion</title><p>In many instances, scene-relative object motion produces components of image motion that differ clearly in velocity or timing from the background optic flow at the corresponding location. Human observers can detect object motion when there are sufficient differences in local direction of motion between object and background (<xref ref-type="bibr" rid="bib54">Royden and Connors, 2010</xref>). Humans can also detect object motion based on local differences in speed when there are sufficient depth cues (<xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref>; <xref ref-type="bibr" rid="bib58">Royden et al., 2016</xref>) or when the image speed of an object is outside the range of background speeds in a particular task context (<xref ref-type="bibr" rid="bib55">Royden and Moore, 2012</xref>).</p><p><xref ref-type="bibr" rid="bib56">Royden and Holloway, 2014</xref> have shown that a model built on MT-like operators with surround suppression can effectively detect object motion when there are sufficient directional differences between object and background motion, or when object speed is outside the range of background speeds. However, such a model would not be able to detect object motion under task conditions like ours or those of <xref ref-type="bibr" rid="bib60">Rushton et al., 2007</xref>, because our dynamic object had the same motion axis as the stationary distractors and because the speeds of our dynamic objects were well within the range of speeds of stationary objects. More recently, <xref ref-type="bibr" rid="bib57">Royden et al., 2015</xref> have added disparity-tuned operators to their model, which allow detection of object motion even when it is aligned with background flow lines. This model computes local differences in response of separate velocity and disparity-tuned operators. It then applies an arbitrary threshold to detect cases for which there are differences and identifies these as possible object motion. While this model shows that differences in signals related to motion and disparity can be used to identify object motion in more general cases, it does not provide a biologically plausible neural mechanism.</p><p>Our findings demonstrate a key, and apparently thus far unappreciated, neural mechanism for identifying local discrepancies between binocular disparity and motion parallax cues that accompany moving objects, even in difficult cases for which there are no local differences in the direction or timing of image motion. The activity of MT neurons with incongruent depth tuning for motion parallax and disparity provides a critical signal about these local discrepancies. Moreover, our simulations indicate that these signals can be easily read out by a linear decoder to detect object motion during self-motion.</p></sec><sec id="s3-2"><title>Relationship to flow-parsing mechanism for computing scene-relative object motion</title><p>A more general approach to computing scene-relative object motion during self-motion is flow parsing, in which global patterns of background motion related to self-motion are discounted (i.e. subtracted off) such that the remaining signal represents object motion relative to the scene (<xref ref-type="bibr" rid="bib59">Rushton and Warren, 2005</xref>). Several studies <xref ref-type="bibr" rid="bib72">Warren and Rushton, 2007</xref>; <xref ref-type="bibr" rid="bib73">Warren and Rushton, 2008</xref>; <xref ref-type="bibr" rid="bib74">Warren and Rushton, 2009a</xref>; <xref ref-type="bibr" rid="bib76">Warren et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Foulkes et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Rushton et al., 2018</xref> have provided strong behavioral support for the flow-parsing hypothesis in humans, including some which suggest strongly that it involves a global motion process (<xref ref-type="bibr" rid="bib75">Warren and Rushton, 2009b</xref>). In addition, a recent study has demonstrated flow parsing in macaque monkeys (<xref ref-type="bibr" rid="bib51">Peltier et al., 2020</xref>). If flow parsing completely discounts background motion due to self-motion, then the computations for detecting scene-relative object motion would be greatly simplified and would be essentially the same as when there is no self-motion. However, flow parsing alone may not be sufficient to detect scene-relative object motion. Recent evidence (<xref ref-type="bibr" rid="bib50">Niehorster and Li, 2017</xref>; <xref ref-type="bibr" rid="bib51">Peltier et al., 2020</xref>) indicates that the gain of flow parsing can be well below unity, such that background motion is only partially discounted. In this case, the output of a flow-parsing mechanism may not be sufficient to detect scene-relative object motion, and a mechanism such as we found in area MT would be valuable.</p><p>An advantage of our proposed mechanism over flow parsing is that it does not require estimation of the global flow field, nor a complicated mechanism (<xref ref-type="bibr" rid="bib40">Layton and Fajen, 2016b</xref>; <xref ref-type="bibr" rid="bib42">Layton and Fajen, 2020</xref>) for implementing the flow-parsing computation at each location in the visual field. Thus, our proposed mechanism may provide a valuable complement to flow parsing. On the other hand, the mechanism that we have described clearly does not obviate the need for flow-parsing mechanisms. Unlike our local mechanism, flow parsing does not require binocular disparity signals to operate. Furthermore, flow parsing allows for the estimation of scene-relative object velocity, rather than just facilitating object motion detection. Thus, our results do not discount the contributions of center-surround or flow-parsing mechanisms to computation of object motion, nor the contributions of mechanisms that may rely on multisensory signals (<xref ref-type="bibr" rid="bib34">Kim et al., 2016b</xref>). Rather, our findings provide evidence of an additional complementary mechanism that is likely to be synergistic. Indeed, since our stimuli involved peripheral background dots (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>), it is possible that our task also engaged flow-parsing mechanisms to some extent.</p><p>We have recently demonstrated flow parsing in macaque monkeys (<xref ref-type="bibr" rid="bib51">Peltier et al., 2020</xref>), including the observation that optic flow in one visual hemi-field can influence perception of object direction in the opposite hemi-field, as demonstrated previously in humans (<xref ref-type="bibr" rid="bib75">Warren and Rushton, 2009b</xref>). These observations imply a global contribution to flow parsing, as might be implemented through feedback from the dorsal subdivision of the medial superior temporal area (MSTd) (<xref ref-type="bibr" rid="bib40">Layton and Fajen, 2016b</xref>; <xref ref-type="bibr" rid="bib42">Layton and Fajen, 2020</xref>). Indeed, in ongoing work, we have observed that flow parsing modulates responses of MT neurons (unpublished). In contrast, the mechanism described here can be computed locally within each portion of the visual field. Thus, we can speculate that the current findings and flow parsing involve distinct neural mechanisms in area MT.</p><p>Our findings establish the first direct (albeit correlational) evidence for a neural mechanism that is involved in perceptual dissociation of object and self-motion. However, the broader problem is more complex, as it is also necessary to flexibly compensate for self-motion to compute object motion in different coordinate frames, such as head-centered or world-centered reference frames (<xref ref-type="bibr" rid="bib16">Fajen et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Sasaki et al., 2020</xref>). Some of these computations are likely to also rely on non-visual signals including vestibular signals about self-motion (<xref ref-type="bibr" rid="bib15">Fajen and Matthis, 2013</xref>; <xref ref-type="bibr" rid="bib16">Fajen et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Dokka et al., 2015a</xref>; <xref ref-type="bibr" rid="bib13">Dokka et al., 2015b</xref>; <xref ref-type="bibr" rid="bib62">Sasaki et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Sasaki et al., 2020</xref>). Thus, the mechanism proposed here is one part of a larger set of neural computations that remain to be fully understood.</p></sec><sec id="s3-3"><title>Functional roles of area MT and computational roles of opposite cells</title><p>Area MT has traditionally been considered to hold a retinotopic representation of retinal image motion. Many studies still make this assumption, despite the fact that MT is known to be modulated by attention (<xref ref-type="bibr" rid="bib68">Treue and Maunsell, 1996</xref>; <xref ref-type="bibr" rid="bib69">Treue and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib44">Martínez-Trujillo and Treue, 2002</xref>; <xref ref-type="bibr" rid="bib77">Womelsdorf et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Lee and Maunsell, 2010</xref>), eye movements (<xref ref-type="bibr" rid="bib49">Newsome et al., 1988</xref>; <xref ref-type="bibr" rid="bib3">Bremmer et al., 1997</xref>; <xref ref-type="bibr" rid="bib28">Inaba et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Chukoskie and Movshon, 2009</xref>; <xref ref-type="bibr" rid="bib47">Nadler et al., 2009</xref>; <xref ref-type="bibr" rid="bib29">Inaba et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Kim et al., 2017</xref>), and stimulus expectation (<xref ref-type="bibr" rid="bib65">Schlack and Albright, 2007</xref>). Our previous work has shown that MT neurons integrate retinal image motion with smooth eye movement (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Nadler et al., 2009</xref>) and global background motion (<xref ref-type="bibr" rid="bib31">Kim et al., 2015a</xref>) signals to compute depth from motion parallax. In addition, most MT neurons are well known to be tuned for binocular disparity (<xref ref-type="bibr" rid="bib45">Maunsell and Van Essen, 1983</xref>; <xref ref-type="bibr" rid="bib10">DeAngelis and Newsome, 1999</xref>; <xref ref-type="bibr" rid="bib11">DeAngelis and Uka, 2003</xref>). Recent studies (<xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Kim et al., 2015a</xref>) revealed the existence of many MT neurons that have mismatched depth tuning for motion parallax and binocular disparity cues. Such neurons would presumably not be useful for cue integration in depth perception, and their functional role has thus far remained unclear. In the present study, we demonstrate that such ‘opposite’ neurons provide valuable signals for detecting object motion during self-motion by selectively responding to local inconsistencies between binocular disparity and motion parallax cues. Thus, our findings provide novel evidence that the functional roles of MT go well beyond representing retinal image motion; they suggest that some MT neurons play fundamental roles in helping to infer the origins, or causes, of retinal image motion.</p><p>Our findings have parallels to the potential function of neurons in area MSTd and the ventral intraparietal (VIP) area that have mismatched heading tuning for visual and vestibular cues (<xref ref-type="bibr" rid="bib23">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib24">Gu et al., 2008</xref>; <xref ref-type="bibr" rid="bib6">Chen et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2013</xref>). Studies of cue integration and cue re-weighting in heading perception have demonstrated that activity of congruent cells can account for behavioral performance (<xref ref-type="bibr" rid="bib24">Gu et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Fetsch et al., 2011</xref>), but the functional role of opposite cells remained unclear from those studies. More recent work has suggested that opposite neurons may play a role in helping parse the retinal image into signals related to self-motion and object motion (<xref ref-type="bibr" rid="bib34">Kim et al., 2016b</xref>; <xref ref-type="bibr" rid="bib62">Sasaki et al., 2017</xref>), although they did not link opposite cell activity to a relevant behavior. Thus, mismatched tuning, whether unisensory or multisensory, may be a common motif for performing computations that involve parsing sensory signals into components that reflect different causes in the world (<xref ref-type="bibr" rid="bib78">Zhang et al., 2019a</xref>).</p><p>More generally, the parsing of retinal image motion into components related to object motion and self-motion is a causal inference problem (<xref ref-type="bibr" rid="bib38">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Shams and Beierholm, 2010</xref>; <xref ref-type="bibr" rid="bib20">French and DeAngelis, 2020</xref>), and recent psychophysical work in humans has demonstrated that perception of heading in the presence of object motion follows predictions of a Bayesian causal inference model (<xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>). While the neural mechanisms of causal inference are still largely unknown (but see <xref ref-type="bibr" rid="bib17">Fang et al., 2019</xref>), recent computational work has suggested that the relative activity of congruent and opposite cells may provide a critical signal for carrying out causal inference operations (<xref ref-type="bibr" rid="bib79">Zhang et al., 2019b</xref>; <xref ref-type="bibr" rid="bib53">Rideaux et al., 2021</xref>). By providing an empirical link between the activity of opposite cells and detection of object motion during self-motion, our results provide novel evidence for a sensory substrate that may be used to perform causal inference in the domain of object motion and self-motion perception. Elucidating the neural substrates and mechanisms of causal inference regarding object motion is the topic of ongoing studies in our laboratories.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects and surgery</title><p>Two male monkeys (<italic>macaca mulatta</italic>, 8–12 kg) participated in these experiments. Standard aseptic surgical procedures under gas anesthesia were performed to implant a head restraint device. A Delrin (Dupont) ring was attached to the skull using a combination of dental acrylic, bone screws, and titanium inverted T-bolts (see <xref ref-type="bibr" rid="bib23">Gu et al., 2006</xref> for details). To monitor eye movements using the magnetic search coil technique, a scleral coil was implanted under the conjunctiva of one eye.</p><p>A recording grid made of Delrin was affixed inside the ring using dental acrylic. The grid (2×4×0.5 cm) contains a dense array of holes spaced 0.8 mm apart. Under anesthesia and using sterile technique, small burr holes (~0.5 mm diameter) were drilled vertically through the recording grid to allow the penetration of microelectrodes into the brain via a transdural guide tube. All surgical procedures and experimental protocols were approved by University Committee on Animal Resources at the University of Rochester.</p></sec><sec id="s4-2"><title>Experimental apparatus</title><p>In each experimental session, animals were seated in a custom-built primate chair that was secured to a six degree-of-freedom motion platform (MOOG 6DOF2000E). The motion platform was used to generate passive body translation along an axis in the fronto-parallel plane, and the trajectory of the platform was controlled in real time at 60 Hz over a dedicated Ethernet link (see <xref ref-type="bibr" rid="bib23">Gu et al., 2006</xref> for details). A field coil frame (C-N-C engineering) was mounted on top of the motion platform to measure eye movements.</p><p>Visual stimuli were rear-projected onto a 60×60 cm tangent screen using a stereoscopic projector (Christie Digital Mirage S+3 K) which was also mounted on the motion platform (<xref ref-type="bibr" rid="bib23">Gu et al., 2006</xref>). The display screen was attached to the front side of the field coil frame. To restrict the animal’s field of view to visual stimuli displayed on the tangent screen, the sides and top of the field coil frame were covered with matte black enclosures. Viewed from a distance of ~30 cm, the display subtended ~90°×90° of visual angle.</p><p>To generate accurate visual simulations of the animal’s movement through a virtual environment, an OpenGL camera was placed at the location of one eye, and the camera moved precisely according to the movement trajectory of the platform. Since the motion platform has its own dynamics, we characterized the transfer function of the motion platform, as described previously (<xref ref-type="bibr" rid="bib23">Gu et al., 2006</xref>), and we generated visual stimuli according to the predicted motion of the platform. To account for a delay between the command signal and the actual movement of the platform, we adjusted a delay parameter to synchronize visual motion with platform movement. Synchronization was confirmed by presenting a world-fixed target in the virtual environment and superimposing a small spot by a room-mounted laser pointer while the platform is in motion (<xref ref-type="bibr" rid="bib23">Gu et al., 2006</xref>).</p></sec><sec id="s4-3"><title>Electrophysiological recordings</title><p>We recorded extracellular single unit activity using single-contact tungsten microelectrodes (FHC Inc) having a typical impedance of 1–3 MΩ. The electrode was loaded into a transdural guide tube and was manipulated with a hydraulic micro-manipulator (Narishige). The voltage signal was amplified and filtered (1 kHz–6 kHz) using conventional hardware (BAK Electronics). Single unit spikes were detected using a window discriminator (BAK Electronics), whose output was time stamped with 1 ms resolution.</p><p>Eye position signals were digitized at 1 kHz, then digitally filtered and down sampled to 200 Hz (TEMPO, Reflective Computing). The raw voltage signal from the microelectrode was digitized and recorded to disk at 25 kHz using a Power1401 data acquisition system (Cambridge Electronic Design). If necessary, single units were re-sorted off-line using a template-based method (Spike2, Cambridge Electronic Design).</p><p>The location of area MT was initially identified in each animal through analysis of structural MRI scans, which were segmented, flattened, and registered with a standard macaque atlas using CARET software (<xref ref-type="bibr" rid="bib71">Van Essen et al., 2001</xref>). The position of area MT in the posterior bank of the superior temporal sulcus (STS) was then projected onto the horizontal plane, and grid holes around the projection area were explored systematically in mapping experiments. In addition to the MRI scans, the physiological properties of neurons and the patterns of gray matter and white matter encountered along electrode penetrations provided essential evidence for identifying MT. In a typical electrode penetration through the STS that encounters area MT, we first encounter neurons with large RFs and visual motion sensitivity (as expected for area MSTd). This is typically followed by a very quiet region as the electrode passes through the lumen of the STS, and then area MT is the next region of gray matter. As expected from previous studies, RFs of MT neurons are much smaller than those in MSTd (<xref ref-type="bibr" rid="bib37">Komatsu and Wurtz, 1988</xref>) and some MT neurons exhibit strong surround suppression (<xref ref-type="bibr" rid="bib11">DeAngelis and Uka, 2003</xref>) which is typically not seen in MSTd. Confirming a putative localization of the electrode to MT, we observed gradual changes in the preferred direction, preferred disparity, and RF location of multiunit activity, consistent with those described previously (<xref ref-type="bibr" rid="bib1">Albright et al., 1984</xref>; <xref ref-type="bibr" rid="bib10">DeAngelis and Newsome, 1999</xref>).</p></sec><sec id="s4-4"><title>Visual stimuli</title><p>Visual stimuli were generated by a custom-written C++ program using the OpenGL 3D graphics library (<xref ref-type="bibr" rid="bib30">Kim, 2013</xref>, <ext-link ext-link-type="uri" xlink:href="https://github.com/hkim09/MoogDots_2013">https://github.com/hkim09/MoogDots_2013</ext-link>) and were displayed using a hardware-accelerated OpenGL graphics card (NVIDIA Quadro FX 1700). The location of the OpenGL camera was matched to the location of the animal’s eye, and images were generated using perspective projection. We calibrated the display such that the virtual environment had the same spatial scale as the physical space through which the platform moved the animal. To view stimuli stereoscopically, animals wore anaglyphic glasses with red and green filters (Kodak Wratten 2 Nos. 29 and 61, respectively). The crosstalk between eyes was measured using a photometer and found to be very small (0.3% for the green filter and 0.1% for the red filter).</p><sec id="s4-4-1"><title>Stimulus to measure depth tuning from motion parallax</title><p>We used an established procedure to generate random-dot stimuli to measure depth tuning from motion parallax (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>). A circular aperture having slightly greater (~10%) diameter than optimal size was located over the center of the RF of the neuron under study. The position of each dot in the image plane was generated by independently choosing random horizontal and vertical locations within the aperture. To present stimuli such that they appear to lie in depth at a specific equivalent disparity, the set of random dots within the circular aperture was ray-traced onto a cylinder corresponding to the desired equivalent disparity, as described in detail previously (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>). This ray-tracing procedure ensured that the size, location, and density of the random dot patch were constant across simulated depths. Size and occlusion cues were eliminated by rendering transparent dots with a constant retinal size (0.39 deg). Critically, this procedure removed pictorial depth cues and rendered the visual stimulus depth-sign ambiguous, thus requiring interaction of retinal object motion with either extra-retinal signals (<xref ref-type="bibr" rid="bib47">Nadler et al., 2009</xref>) or global visual motion cues (<xref ref-type="bibr" rid="bib31">Kim et al., 2015a</xref>) that specify eye rotation relative to the scene.</p><p>The above description assumes lateral translation of the observer in the horizontal plane. In our experiments, animals were translated along an axis in the fronto-parallel plane (i.e. a vertical plane that includes the interaural axis and is parallel to the plane of the display screen) that was aligned with the preferred-null axis of the neuron under study (to elicit robust responses). In this case, we rotated the virtual stimulus cylinder about the naso-occipital axis (normal to the display screen) such that the axis of translation of the observer was always orthogonal to the long axis of the cylinder. This ensures that dots having the same equivalent disparities produce the same retinal speeds regardless of the axis of observer translation (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>).</p></sec><sec id="s4-4-2"><title>Stimulus for object detection task</title><p>Visual stimuli for the main task consisted of a dynamic target object (which could be either moving or stationary in the world), one or three stationary objects (distractors), and a cloud of background dots that appeared outside of a central masked region (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). Background dots were masked out of this central region around the target and distractor objects to avoid having the background dots directly stimulate the RF of the neuron under study. The two-object version of the task (one dynamic target and one stationary distractor) was used in all neural recording experiments, whereas the four-object task (one dynamic target and three stationary distractors) was used during training and in some behavioral control experiments.</p><p>For the two-object task, one object was located in the center of the RF of the neuron under study, and the other object was presented on the opposite side of the fixation target (180 deg apart) at the same eccentricity (<xref ref-type="fig" rid="fig1">Figure 1A, B</xref>). For the four-object task, one object was centered on the RF, and the other three objects were distributed equally (90 deg apart) around the fixation target at equal eccentricities. To present each object at the same retinal position regardless of its depth, the positions of objects were initially determined in screen coordinates and then were ray-traced onto surfaces in the simulated environment (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, left).</p><p>Each object was rendered as a square-shaped ‘plate’ of random dots (density: 1.1 dots/deg<sup>2</sup>) and was displayed binocularly as a red-green anaglyph. The retinal size of dots was constant (0.15 deg) regardless of object depth, such that dot size was not a depth cue. The target and distractor objects were all of the same retinal size (which was tailored to the RF of the neuron under study) regardless of their location in depth, such that the image size of objects was also not a depth cue. Thus, the only reliable cues to object depth were binocular disparity and motion parallax.</p><p>Dynamic target objects had two independent depth parameters, one based on binocular disparity (d<sub>BD</sub>) and the other based on motion parallax (d<sub>MP</sub>). The left-eye and right-eye half-images of the dynamic object were rendered based on the depth defined by binocular disparity, d<sub>BD</sub>. We then computed the image motion of the dynamic object during translation of the monkey such that it had motion parallax that was consistent with a different depth, d<sub>MP</sub>. Based on the predicted trajectory of the camera on each video frame, we ray-traced the position of the dynamic object (at d<sub>MP</sub>) onto the depth plane defined by binocular disparity, d<sub>BD</sub> (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, right). This procedure ensures that the dynamic object had a particular difference in depth (ΔDepth, in equivalent disparity units) specified by (d<sub>MP</sub> – d<sub>BD</sub>), but that it was not possible to detect the dynamic object solely based on its relative motion in the scene (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). In other words, when viewed monocularly, the image motion of the dynamic object would be consistent with that of a stationary object at d<sub>MP</sub>. When viewed binocularly, if ΔDepth ≠ 0, the dynamic object’s image motion would not be consistent with its depth specified by disparity, d<sub>BD</sub>.</p></sec></sec><sec id="s4-5"><title>Experimental protocol</title><sec id="s4-5-1"><title>Preliminary measurements</title><p>After isolating the action potential of a single neuron, the RF was explored manually using a small (typically 2–3 deg) patch of random dots. The direction, speed, position, and binocular disparity of the random-dot patch were manipulated using a computer mouse, and instantaneous firing rates were plotted on a display interface that represents the spatial location of the patch in visual space and the stimulus velocity in a direction-speed space. This procedure was used to estimate the location and size of the RF as well as to estimate the neuron’s preferences for direction, speed, and binocular disparity.</p><p>After these qualitative tests, we measured the direction, speed, binocular disparity, and size tuning of each neuron using quantitative protocols (<xref ref-type="bibr" rid="bib11">DeAngelis and Uka, 2003</xref>). Each of these measurements was performed in a separate block of trials, and each distinct stimulus was repeated 3–5 times. Direction tuning was measured with random dots that moved in eight different directions separated by 45 deg. Speed tuning was measured, at the preferred direction, with random dot stimuli that moved at speeds of 0, 0.5, 1, 2, 4, 8, 16, and 32 deg/s. The stimuli in our main task contained speeds of motion that were &lt;7 deg/s. If a neuron gave very little response (&lt;5 spk/s) to these slow speeds, the neuron was not studied further. Next, the spatial profile of the RF was measured by presenting a patch of random dots at all locations on a 4×4 grid that covered the RF. The height and width of the grid were 1.5–2.5 times larger than the estimated RF size, and each small patch was approximately ¼ the size of the RF. Responses were fitted by a 2D Gaussian function to estimate the center location and size of the RF. To measure binocular disparity tuning, a random dot stereogram was presented at binocular disparities ranging from –2 deg to +2 deg in steps of 0.5 deg. For this disparity tuning measurement, dots moved in the neuron’s preferred direction and speed. Finally, size tuning was measured with random-dot patches having diameters of 0.5, 1, 2, 4, 8, 16, and 32 deg.</p><p>Depth tuning from motion parallax was then measured as described previously (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>). Dots were presented monocularly and were rendered at one of nine simulated depths based on their motion (–2 deg to +2 deg of equivalent disparity in steps of 0.5 deg), in addition to the null condition in which only the fixation target was presented. Each distinct stimulus was repeated 6–10 times. During measurement of depth tuning from motion parallax, animals underwent passive whole-body translation which followed a modified sinusoidal trajectory along an axis in the fronto-parallel plane (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). To smooth the onset and offset, the 2 s sinusoidal trajectory was multiplied by a Gaussian function that was exponentiated to a large power as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:semantics><mml:mrow><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where t<sub>0</sub>=1.0 s, σ=0.92, and n=22. On half of the trials, platform movement started toward the neuron’s preferred direction. On the other half, motion started toward the neuron’s null direction (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). During body translation, animals were required to maintain fixation on a world-fixed target, which required a compensatory smooth eye movement in the direction opposite to head movement.</p></sec><sec id="s4-5-2"><title>Moving object detection task</title><p>We presented one dynamic (i.e. moving) object and one (or three) stationary object(s) while the animal experienced the modified sinusoidal lateral motion as described above. The animal was trained to identify the dynamic object by making a saccadic eye movement to it (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). At the beginning of each trial, the fixation target first appeared at the center of the screen. After the animal established fixation for 0.2 s, the dynamic object, stationary object(s), and background cloud of dots appeared and began to move as the animal was translated sinusoidally for 2.1 s (see <xref ref-type="video" rid="video1">Video 1</xref>). Because the fixation target was world-fixed, translation of the animal required a counter-active smooth eye movement to maintain visual fixation. An electronic window around the fixation target was used to monitor and enforce pursuit accuracy. The initial size of the target window was 3–4 deg, and it shrank to 2.1–2.8 deg after 250 ms of translation. This allowed the animal a brief period of time to initiate pursuit and execute a catch-up saccade to arrive on target. At the end of visual stimulation, both the fixation target and the visual stimuli disappeared and a choice target (0.4 deg in diameter) appeared at the center location of each object. The animal then attempted to make a saccadic eye movement to the location of the dynamic object and received a liquid reward (0.2–0.4 ml) for correct answers.</p><p>Based on the preliminary tests described above, we set the axis of translation within the fronto-parallel plane to align with the preferred-null axis of the neuron under study. In the main detection task, we systematically varied the depth discrepancy (ΔDepth) between disparity and motion parallax cues for the dynamic object to manipulate task difficulty (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). ΔDepth is defined as the difference between depths specified by motion parallax and binocular disparity cues, (d<sub>MP</sub> – d<sub>BD</sub>). Different values of ΔDepth were applied to the dynamic object around a fixed ‘pedestal depth’ (red line, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). For the vast majority of recording sessions, the pedestal depth was fixed at –0.45 deg (103/106 sessions), although it deviated from this value slightly in a few early experiments. We elected to use a fixed pedestal depth such that all neurons were tested with the same stimulus values, thereby allowing for decoding analyses (described below). The pedestal depth was chosen as the average midpoint between the preferred depths obtained from tuning curves for disparity and motion parallax, based on data from a previous study (<xref ref-type="bibr" rid="bib48">Nadler et al., 2013</xref>). We used the following ΔDepth values: −1.53, –0.57, –0.21, 0, 0.21, 0.57, and 1.53 deg. Stationary objects were presented at one of seven possible depths (–1.6 deg to +1.6 deg in steps of 0.4 deg). The vast majority of recording sessions were conducted using these ‘standard’ pedestal depth, ΔDepth, and stationary depth values (101/106 sessions). Thus, the maximum range of depths of dynamic objects (–1.215 to +0.315 deg) was well within the range of depths for stationary objects, which ensured that the animals could not perform the task solely based on depth outliers (either in binocular disparity or motion parallax). The identity of each object (dynamic/stationary) and its depth values were chosen from the above ranges randomly on each trial. Each ΔDepth value of the dynamic object was repeated at least 14 times (mean: 35 and SD: 9.6).</p><p>For three sessions, a monkey performed the object detection task without binocular disparity cues in a fraction of trials (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, monocular condition). In this control condition, the visual stimulus (except for the fixation point) was displayed to only one eye in 16% of trials, while the rest of the task structure remained the same. Monocular conditions were presented in a small percentage of trials in order not to frustrate the animal, given that performance was poor on these monocular trials.</p></sec></sec><sec id="s4-6"><title>Animal training procedure</title><p>Although the object detection task is conceptually simple, it required extensive behavioral training, involving a number of steps. Here, we outline a series of operant conditioning steps required to teach animals to perform the task. Following basic chair training and habituation to the laboratory, animals were trained to maintain visual fixation on a target during sinusoidal translation of the motion platform.</p><p>Once smooth eye movements tracked the fixation target with pursuit gains approaching 0.9, we initially trained animals to detect a moving object without any self-motion, such that any motion of an object on the display resulted from object motion relative to the scene. After fixation, four objects appeared on the display and only one of them moved sinusoidally along a horizontal trajectory for 2.1 s. In the early stages of this training, a saccade target appeared only at the location of the moving object. Subsequently, we introduced a fraction of trials in which saccade targets appeared at the locations of all four objects, and we gradually increased the proportion of these trials. During this phase of training, the depths of the objects, as defined solely by binocular disparity since there was no self-motion, were randomly drawn from a uniform distribution spanning the range from –1.6 to +1.6 deg, to help animals generalize the task.</p><p>Once animals performed the task well in the absence of self-motion, we began to introduce small amounts of sinusoidal self-motion, which induced subtle retinal image motion of all objects. During the initial stages of this training period, the dynamic object had a large motion amplitude such that it was quite salient relative to the motion of stationary objects that was due to self-motion. As the animals became accustomed to performing the task during self-motion, we gradually increased the magnitude of self-motion (up to 2.8 cm) and decreased the motion amplitude of the dynamic object. Once the retinal motion amplitude of the dynamic object became comparable to that of stationary objects, we began to introduce a depth discrepancy between disparity and motion parallax (ΔDepth). That is, the motion trajectory of the dynamic object began to follow that of an object at a different depth, d<sub>MP</sub> (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, right). We used a staircase procedure to train animals over a range of values of ΔDepth. During this phase of training, we interleaved three different pedestal depths (–0.51 deg, 0 deg, and 0.51 deg) to help animals generalize the task, and we randomly chose the depths of the three stationary objects from the range –1.6 to +1.6 deg.</p><p>Once we observed stable ‘v-shaped’ psychometric functions for all three pedestal depths over a span of more than 10 days (e.g. <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A, B</xref>), we transitioned to the final stimulus configuration for recording experiments. To keep the number of stimulus conditions manageable for recording, this configuration included one pedestal depth and two objects (one dynamic and one stationary). Following recording experiments, we revisited the more general version of the task involving four objects and three pedestal depths to make sure that behavioral performance did not reflect any change in strategy (e.g. <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3C</xref>).</p></sec><sec id="s4-7"><title>Data analyses</title><sec id="s4-7-1"><title>Regression analysis of behavior</title><p>We used multinomial regression to assess the relative contributions of d<sub>BD</sub>, d<sub>MP</sub>, and ΔDepth to perceptual decisions. If animals perform the task primarily based on the discrepancy between disparity and motion parallax cues to depth, we expect to see a much greater contribution of ΔDepth relative to d<sub>BD</sub> and d<sub>MP</sub>. For each possible choice location, i, (i.e. a chosen location or a not-chosen location), we performed the following regression:<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow/></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where j denotes the locations of objects on the screen, and N is the total number of objects (two or four). Once beta values were obtained, we averaged betas across the two (or four) possible choice locations and also averaged betas across the two (or 12) not chosen locations (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3D, E</xref>).</p><p>We also quantified the proportion of fits that produced significant values of each beta coefficient (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). The number of beta values significantly different from zero (alpha = 0.05) were summed across locations (two or four) and across sessions. The results were then divided by the total number of beta values (2 * number of valid sessions or 4 * number of valid sessions, respectively). For not-chosen objects in the four-object task, the number of significant fits were summed across three locations and then divided by 12 * number of valid sessions.</p></sec><sec id="s4-7-2"><title>Depth-sign tuning and discrimination index</title><p>Average firing rates during stimulus presentation were plotted as a function of simulated depth (<xref ref-type="fig" rid="fig2">Figure 2A–C</xref>) to construct depth tuning curves. To quantify the relative strength of neural responses to near and far depths defined by binocular disparity or motion parallax, we computed a DSDI from each tuning curve (<xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Nadler et al., 2009</xref>).<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For each pair of depths symmetrical around zero (for example, ±2 deg), the difference in mean response between far (R<sub>far</sub>) and near (R<sub>near</sub>) depths was computed relative to response variability (σ<sub>avg</sub>, the average SD of responses to the two depths). This quantity was then averaged across the four pairs of depth magnitudes to obtain the DSDI (–1&lt;DSDI&lt;+1). Near-preferring neurons have negative DSDI values, whereas far-preferring neurons have positive DSDI values. Statistical significance of DSDI values was evaluated using a permutation test in which DSDI values were computed 1000 times after shuffling responses across depths. If the measured DSDI value is negative, the p value is the proportion of shuffled DSDIs less than the measured DSDI value. If the measured DSDI is positive, the p value is the proportion of DSDIs greater than the measured DSDI value.</p></sec><sec id="s4-7-3"><title>Depth sign discrimination index for dynamic object tuning</title><p>Average firing rates during stimulus presentation were plotted as a function of depth difference (<xref ref-type="fig" rid="fig2">Figure 2D–F</xref>) to construct dynamic object tuning curves. To quantify the relative strength of neural responses to negative and positive values of ΔDepth, we computed a DSDI metric for the dynamic object responses (DSDI<sub>dyn</sub>) as follows:<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For each pair of ΔDepth values symmetrical around zero (e.g. ±1.53 deg), the difference in mean response between positive (R<sub>pos</sub>) and negative (R<sub>neg</sub>) ΔDepth was computed relative to response variability (σ<sub>avg</sub>, the average SD of responses to the two ΔDepth values). This quantity was then averaged across the three pairs of ΔDepth values to obtain DSDI<sub>dyn</sub> (–1&lt;DSDI<sub>dyn</sub>&lt;+1).</p></sec><sec id="s4-7-4"><title>Depth tuning congruency</title><p>Congruency of depth tuning curves obtained by manipulating binocular disparity and motion parallax cues was quantified using a correlation coefficient. The Pearson correlation was computed between the two cues using the average responses across nine depths (−2 to 2 deg in steps of 0.5 deg) for each cue; this coefficient is noted as R<sub>MP_BD</sub> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Neurons were classified as ‘congruent’ or ‘opposite’ if their value of R<sub>MP_BD</sub> was significantly greater or less than zero, respectively.</p></sec><sec id="s4-7-5"><title>Neurometric performance</title><p>We used an ideal observer analysis to measure how reliably single neurons can signal whether an object is dynamic or stationary. For each value of ΔDepth, the distribution of firing rates across trials was sorted into two groups according to the type of object in the RF (dynamic vs. stationary). An ROC curve was computed from the pair of response distributions for each ΔDepth (<xref ref-type="bibr" rid="bib4">Britten et al., 1992</xref>), and performance of the ideal observer was defined as the area under the ROC curve. ROC areas were then plotted as a function of ΔDepth to construct a neurometric function (<xref ref-type="fig" rid="fig5">Figure 5B, E</xref>). To obtain a single measure of NP, we then averaged the ROC areas across non-zero values of ΔDepth to obtain a single metric for each neuron. This average ROC area will be &gt;0.5 if a neuron responds preferentially to dynamic objects overall and &lt;0.5 if it responds preferentially to stationary objects overall.</p></sec><sec id="s4-7-6"><title>Detection probability</title><p>DP is a measure of the relationship between neural responses and perceptual decisions in a detection task (<xref ref-type="bibr" rid="bib2">Bosking and Maunsell, 2011</xref>) and is similar to the choice probability metric (<xref ref-type="bibr" rid="bib5">Britten et al., 1996</xref>). The procedure for computing DP is analogous to the ROC analysis described above, except that responses are sorted into two groups according to the animal’s perceptual decision (dynamic vs. stationary object in the RF). To eliminate any contamination from stimulus effects, only ambiguous trials (ΔDepth = 0) were used to compute DP (<xref ref-type="fig" rid="fig4">Figure 4</xref>). A permutation test was used to determine whether each DP value was significantly different from the chance level of 0.5 (<xref ref-type="bibr" rid="bib70">Uka and DeAngelis, 2004</xref>).</p></sec><sec id="s4-7-7"><title>Decoding analyses</title><p>We constructed an optimal linear decoder to detect moving objects based on simulated responses from a population of 97 model neurons. Model neurons correspond to the dominant subset of recorded neurons for which data were collected under identical stimulus conditions, thus allowing us to construct pseudo-population responses. We randomly selected 100,000 samples of stimulus conditions from the datasets with replacement (16 unique stimulus conditions within the RF). The mean and SD of measured responses to each stimulus condition were then used to generate simulated responses according to the following equation (<xref ref-type="bibr" rid="bib66">Shadlen et al., 1996</xref>; <xref ref-type="bibr" rid="bib9">Cohen and Newsome, 2009</xref>; <xref ref-type="bibr" rid="bib25">Gu et al., 2014</xref>):<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>σ</mml:mi></mml:math></disp-formula></p><p>where <italic>µ</italic> and <italic>σ</italic> are vectors of means and SDs of the population across stimulus conditions, <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a vector of standard normal deviates (MATLAB ‘normrnd’ function with zero mean and unity standard deviation), and Q is the square root of the correlation matrix. The correlation matrix was modeled such that pairs of neurons with similar NP values have stronger correlated noise, and pairs of neurons with dissimilar NP values show weaker correlated noise:<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mi>r</mml:mi><mml:mo>_</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.1</mml:mn><mml:mi> </mml:mi><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>-</mml:mo><mml:msqrt><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:msqrt><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the neurometric performance of neuron i. This generated noise correlations (0.15±0.17, mean ± SD) of roughly similar strength to those observed in empirical studies of MT neurons (<xref ref-type="bibr" rid="bib80">Zohary et al., 1994</xref>; <xref ref-type="bibr" rid="bib27">Huang and Lisberger, 2009</xref>).</p><p>Total trials were divided into training (90%) and test (10%) sets. A linear decoder was trained to classify whether the stimulus in the RF was a dynamic or stationary object based on population responses in the training set. We used linear discriminant analysis (MATLAB ‘classify’ function) to determine the weights of the decoder. Ambiguous trials (ΔDepth = 0) were excluded from the training set.</p><p>The test set was used to validate performance of the decoder. A DP<sub>pred</sub> was computed for each neuron in the model in the same way we computed DP from the empirical data, except that the decoder’s ‘choice’ for each trial was used instead of the monkey’s behavioral choice. Specifically, responses to ambiguous stimuli (ΔDepth = 0) in the test set were sorted according to the decoder’s output (dynamic vs. stationary object prediction).</p></sec><sec id="s4-7-8"><title>Time course of choice-related responses</title><p>Spikes in the ambiguous trials (ΔDepth = 0) were aligned to stimulus onset, compiled into peri-stimulus time histograms, and then smoothed using a 150 ms boxcar window. Trials were first sorted by the phase of self-motion (phase 0 or phase 180), and then sorted by the animal’s choice (whether the animal chose an object within the RF or not). Average responses were z-scored using a session-wide mean and SD. We plotted the mean and SE of the z-scored responses, as well as the difference in z-scored responses between choices (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). For each phase, we tested whether the median responses for the two choices at each time point were significantly different or not (alpha = 0.05, Wilcoxon signed-rank test).</p></sec><sec id="s4-7-9"><title>Neuron samples and selection criteria</title><p>We analyzed data from a total of 123 single units (53 neurons from M1 across 73 recording sessions and 70 neurons from M2 across 82 recording sessions) for which we completed the basic tuning measurements, including tuning for direction, speed, RF position, size, depth from binocular disparity, and depth from motion parallax. Among these, we completed the object detection task for 106 neurons (47 from M1 and 59 from M2). This set of 106 neurons constitutes the sample for the single neuron analyses of <xref ref-type="fig" rid="fig3">Figure 3</xref>. Except for two neurons, 104 of these 106 neurons were tested using a standard set of ΔDepth values, including zero (47 from M1 and 57 from M2).</p><p>To compute detection probability, we analyzed a subset of these 104 neurons for which the monkey made at least five choices in favor of both target locations when ΔDepth = 0 (92 neurons, 39 from M1 and 53 from M2). For population decoding (<xref ref-type="fig" rid="fig6">Figure 6</xref>), we required that each dataset contain responses to objects at all of the standard depth values for the stationary object. Three neurons were excluded because they were tested with slightly different stationary depth values, and four neurons were excluded because they did not have responses to stationary objects at all of the standard depth values (which can occur because the depths of stationary objects were chosen randomly from the standard values in each trial). Thus, with these exclusions, 97 neurons contributed to the population decoding analysis (45 from M1 and 52 from M2).</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Funding acquisition, Project administration, Supervision, Validation, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All surgical procedures and experimental protocols were approved by the University Committee on Animal Resources at the University of Rochester (#100682) and were performed in accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-74971-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data have been made available on Figshare: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.19783624">https://doi.org/10.6084/m9.figshare.19783624</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>HyungGoo</surname><given-names>K</given-names></name><name><surname>Angelaki</surname><given-names>ED</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>A neural mechanism for detecting object motion during self-motion</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.19783624</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Johnny Wen for programming assistance, as well as Swati Shimpi, Emily Murphy, and Dina Graf for assistance with training animals. This work was supported by NEI R01 grant EY013644, NINDS U19 grant NS118246, and by an NEI Core grant (EY001319).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname><given-names>TD</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Columnar organization of directionally selective cells in visual area MT of the macaque</article-title><source>Journal of Neurophysiology</source><volume>51</volume><fpage>16</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1152/jn.1984.51.1.16</pub-id><pub-id pub-id-type="pmid">6693933</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosking</surname><given-names>WH</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Effects of stimulus direction on the correlation between behavior and single units in area MT during a motion detection task</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>8230</fpage><lpage>8238</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0126-11.2011</pub-id><pub-id pub-id-type="pmid">21632944</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Ilg</surname><given-names>UJ</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Distler</surname><given-names>C</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Eye position effects in monkey cortex. I. Visual and pursuit-related activity in extrastriate areas MT and MST</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>944</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.2.944</pub-id><pub-id pub-id-type="pmid">9065860</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage><pub-id pub-id-type="pmid">1464765</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Celebrini</surname><given-names>S</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A relationship between behavioral choice and the visual responses of neurons in macaque MT</article-title><source>Visual Neuroscience</source><volume>13</volume><fpage>87</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1017/s095252380000715x</pub-id><pub-id pub-id-type="pmid">8730992</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Representation of vestibular and visual cues to self-motion in ventral intraparietal cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12036</fpage><lpage>12052</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0395-11.2011</pub-id><pub-id pub-id-type="pmid">21849564</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Functional specializations of the ventral intraparietal area for multisensory heading discrimination</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>3567</fpage><lpage>3581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4522-12.2013</pub-id><pub-id pub-id-type="pmid">23426684</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chukoskie</surname><given-names>L</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Modulation of visual signals in macaque MT and MST neurons during pursuit eye movement</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>3225</fpage><lpage>3233</lpage><pub-id pub-id-type="doi">10.1152/jn.90692.2008</pub-id><pub-id pub-id-type="pmid">19776359</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Estimates of the contribution of single neurons to perception depend on timescale and noise correlation</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>6635</fpage><lpage>6648</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5179-08.2009</pub-id><pub-id pub-id-type="pmid">19458234</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Organization of disparity-selective neurons in macaque area MT</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>1398</fpage><lpage>1415</lpage><pub-id pub-id-type="pmid">9952417</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Uka</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Coding of horizontal disparity and velocity by MT neurons in the alert macaque</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>1094</fpage><lpage>1111</lpage><pub-id pub-id-type="doi">10.1152/jn.00717.2002</pub-id><pub-id pub-id-type="pmid">12574483</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Multisensory Integration of Visual and Vestibular Signals Improves Heading Discrimination in the Presence of a Moving Object</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13599</fpage><lpage>13607</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2267-15.2015</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>MacNeilage</surname><given-names>PR</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Multisensory Self-Motion Compensation During Object Trajectory Judgments</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>619</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht247</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Jansen</surname><given-names>M</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference accounts for heading perception in the presence of object motion</article-title><source>PNAS</source><volume>116</volume><fpage>9060</fpage><lpage>9065</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820373116</pub-id><pub-id pub-id-type="pmid">30996126</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fajen</surname><given-names>BR</given-names></name><name><surname>Matthis</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual and non-visual contributions to the perception of object motion during self-motion</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e55446</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0055446</pub-id><pub-id pub-id-type="pmid">23408983</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fajen</surname><given-names>BR</given-names></name><name><surname>Parade</surname><given-names>MS</given-names></name><name><surname>Matthis</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Humans perceive object motion in world coordinates during obstacle avoidance</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.1167/13.8.25</pub-id><pub-id pub-id-type="pmid">23887048</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Qi</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical inference of body representation in the macaque brain</article-title><source>PNAS</source><volume>116</volume><fpage>20151</fpage><lpage>20157</lpage><pub-id pub-id-type="doi">10.1073/pnas.1902334116</pub-id><pub-id pub-id-type="pmid">31481617</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/nn.2983</pub-id><pub-id pub-id-type="pmid">22101645</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foulkes</surname><given-names>AJ</given-names></name><name><surname>Rushton</surname><given-names>SK</given-names></name><name><surname>Warren</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Flow parsing and heading perception show similar dependence on quality and quantity of optic flow</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>7</volume><elocation-id>49</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2013.00049</pub-id><pub-id pub-id-type="pmid">23801945</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>RL</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multisensory neural processing: from cue integration to causal inference</article-title><source>Current Opinion in Physiology</source><volume>16</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2020.04.004</pub-id><pub-id pub-id-type="pmid">32968701</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>EJ</given-names></name><name><surname>Gibson</surname><given-names>JJ</given-names></name><name><surname>Smith</surname><given-names>OW</given-names></name><name><surname>Flock</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Motion parallax as a determinant of perceived depth</article-title><source>Journal of Experimental Psychology</source><volume>58</volume><fpage>40</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1037/h0043883</pub-id><pub-id pub-id-type="pmid">13664883</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goncalves</surname><given-names>NR</given-names></name><name><surname>Welchman</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>“What Not” Detectors Help the Brain See in Depth</article-title><source>Current Biology</source><volume>27</volume><fpage>1403</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.03.074</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Watkins</surname><given-names>PV</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Visual and nonvisual contributions to three-dimensional heading selectivity in the medial superior temporal area</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>73</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2356-05.2006</pub-id><pub-id pub-id-type="pmid">16399674</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates of multisensory cue integration in macaque MSTd</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1201</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1038/nn.2191</pub-id><pub-id pub-id-type="pmid">18776893</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Contribution of correlated noise and selective decoding to choice probability measurements in extrastriate visual cortex</article-title><source>eLife</source><volume>3</volume><elocation-id>e670</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.02670</pub-id><pub-id pub-id-type="pmid">24986734</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Gerwinn</surname><given-names>S</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inferring decoding strategies from choice probabilities in the presence of correlated variability</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>235</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/nn.3309</pub-id><pub-id pub-id-type="pmid">23313912</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Lisberger</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Noise correlations in cortical area MT and their potential impact on trial-by-trial variation in the direction and speed of smooth-pursuit eye movements</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>3012</fpage><lpage>3030</lpage><pub-id pub-id-type="doi">10.1152/jn.00010.2009</pub-id><pub-id pub-id-type="pmid">19321645</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inaba</surname><given-names>N</given-names></name><name><surname>Shinomoto</surname><given-names>S</given-names></name><name><surname>Yamane</surname><given-names>S</given-names></name><name><surname>Takemura</surname><given-names>A</given-names></name><name><surname>Kawano</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>MST neurons code for visual motion in space independent of pursuit eye movements</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>3473</fpage><lpage>3483</lpage><pub-id pub-id-type="doi">10.1152/jn.01054.2006</pub-id><pub-id pub-id-type="pmid">17329625</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inaba</surname><given-names>N</given-names></name><name><surname>Miura</surname><given-names>K</given-names></name><name><surname>Kawano</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Direction and speed tuning to visual motion in cortical areas MT and MSTd during smooth pursuit eye movements</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>1531</fpage><lpage>1545</lpage><pub-id pub-id-type="doi">10.1152/jn.00511.2010</pub-id><pub-id pub-id-type="pmid">21273314</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>MoogDots_2013, GitHub</data-title><version designator="688db8e">688db8e</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/hkim09/MoogDots_2013">https://github.com/hkim09/MoogDots_2013</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>A Functional Link between MT Neurons and Depth Perception Based on Motion Parallax</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>2766</fpage><lpage>2777</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3134-14.2015</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>A novel role for visual perspective cues in the neural computation of depth</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>129</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1038/nn.3889</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>The neural basis of depth perception from motion parallax</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>371</volume><elocation-id>20150256</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0256</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>A simple approach to ignoring irrelevant variables by population decoding based on multisensory neurons</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>1449</fpage><lpage>1467</lpage><pub-id pub-id-type="doi">10.1152/jn.00005.2016</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gain Modulation as a Mechanism for Coding Depth from Motion Parallax in Macaque Area MT</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8180</fpage><lpage>8197</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0393-17.2017</pub-id><pub-id pub-id-type="pmid">28739582</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname><given-names>JJ</given-names></name><name><surname>van Doorn</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Facts on optic flow</article-title><source>Biological Cybernetics</source><volume>56</volume><fpage>247</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1007/BF00365219</pub-id><pub-id pub-id-type="pmid">3607100</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komatsu</surname><given-names>H</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Localization and Visual Properties of Neurons</article-title><source>Journal of Neurophysiol</source><volume>60</volume><fpage>580</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1152/jn.1988.60.2.580</pub-id><pub-id pub-id-type="pmid">3171643</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Layton</surname><given-names>OW</given-names></name><name><surname>Fajen</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>A Neural Model of MST and MT Explains Perceived Object Motion during Self-Motion</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>8093</fpage><lpage>8102</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4593-15.2016</pub-id><pub-id pub-id-type="pmid">27488630</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Layton</surname><given-names>OW</given-names></name><name><surname>Fajen</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>The temporal dynamics of heading perception in the presence of moving objects</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>286</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1152/jn.00866.2015</pub-id><pub-id pub-id-type="pmid">26510765</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Layton</surname><given-names>OW</given-names></name><name><surname>Niehorster</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A model of how depth facilitates scene-relative object motion perception</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007397</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007397</pub-id><pub-id pub-id-type="pmid">31725723</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Layton</surname><given-names>OW</given-names></name><name><surname>Fajen</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computational Mechanisms for Perceptual Stability using Disparity and Motion Parallax</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>996</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0036-19.2019</pub-id><pub-id pub-id-type="pmid">31699889</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attentional modulation of MT neurons with single or multiple stimuli in their receptive fields</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>3058</fpage><lpage>3066</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3766-09.2010</pub-id><pub-id pub-id-type="pmid">20181602</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martínez-Trujillo</surname><given-names>J</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Attentional modulation strength in cortical area MT depends on stimulus contrast</article-title><source>Neuron</source><volume>35</volume><fpage>365</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00778-x</pub-id><pub-id pub-id-type="pmid">12160753</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Functional properties of neurons in middle temporal visual area of the macaque monkey</article-title><source>II. Binocular Interactions and Sensitivity to Binocular Disparity. J Neurophysiol</source><volume>49</volume><fpage>1148</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1152/jn.1983.49.5.1148</pub-id><pub-id pub-id-type="pmid">6864243</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A neural representation of depth from motion parallax in macaque visual cortex</article-title><source>Nature</source><volume>452</volume><fpage>642</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1038/nature06814</pub-id><pub-id pub-id-type="pmid">18344979</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Nawrot</surname><given-names>M</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>MT neurons combine visual motion with a smooth eye movement signal to code depth-sign from motion parallax</article-title><source>Neuron</source><volume>63</volume><fpage>523</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.029</pub-id><pub-id pub-id-type="pmid">19709633</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Barbash</surname><given-names>D</given-names></name><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Shimpi</surname><given-names>S</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Joint representation of depth from motion parallax and binocular disparity cues in macaque area MT</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>14061</fpage><lpage>14074</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0251-13.2013</pub-id><pub-id pub-id-type="pmid">23986242</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Relation of cortical areas MT and MST to pursuit eye movements</article-title><source>II. Differentiation of Retinal from Extraretinal Inputs. J Neurophysiol</source><volume>60</volume><fpage>604</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1152/jn.1988.60.2.604</pub-id><pub-id pub-id-type="pmid">3171644</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niehorster</surname><given-names>DC</given-names></name><name><surname>Li</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accuracy and Tuning of Flow Parsing for Visual Perception of Object Motion During Self-Motion</article-title><source>I-Perception</source><volume>8</volume><elocation-id>204166951770820</elocation-id><pub-id pub-id-type="doi">10.1177/2041669517708206</pub-id><pub-id pub-id-type="pmid">28567272</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peltier</surname><given-names>NE</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Optic flow parsing in the macaque monkey</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.10.8</pub-id><pub-id pub-id-type="pmid">33016983</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>How Can Single Sensory Neurons Predict Behavior?</article-title><source>Neuron</source><volume>87</volume><fpage>411</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.033</pub-id><pub-id pub-id-type="pmid">26182422</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rideaux</surname><given-names>R</given-names></name><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Maiello</surname><given-names>G</given-names></name><name><surname>Welchman</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How multisensory neurons solve causal inference</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2106235118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2106235118</pub-id><pub-id pub-id-type="pmid">34349023</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Connors</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The detection of moving objects by moving observers</article-title><source>Vision Research</source><volume>50</volume><fpage>1014</fpage><lpage>1024</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.03.008</pub-id><pub-id pub-id-type="pmid">20304002</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Moore</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Use of speed cues in the detection of moving objects by moving observers</article-title><source>Vision Research</source><volume>59</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.02.006</pub-id><pub-id pub-id-type="pmid">22406544</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Holloway</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Detecting moving objects in an optic flow field using direction- and speed-tuned operators</article-title><source>Vision Research</source><volume>98</volume><fpage>14</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2014.02.009</pub-id><pub-id pub-id-type="pmid">24607912</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Sannicandro</surname><given-names>SE</given-names></name><name><surname>Webber</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Detection of moving objects using motion- and stereo-tuned operators</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.1167/15.8.21</pub-id><pub-id pub-id-type="pmid">26129859</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Parsons</surname><given-names>D</given-names></name><name><surname>Travatello</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The effect of monocular depth cues on the detection of moving objects by moving observers</article-title><source>Vision Research</source><volume>124</volume><fpage>7</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.05.002</pub-id><pub-id pub-id-type="pmid">27264029</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushton</surname><given-names>SK</given-names></name><name><surname>Warren</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Moving observers, relative retinal motion and the detection of object movement</article-title><source>Current Biology</source><volume>15</volume><fpage>R542</fpage><lpage>R543</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.07.020</pub-id><pub-id pub-id-type="pmid">16051158</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushton</surname><given-names>SK</given-names></name><name><surname>Bradshaw</surname><given-names>MF</given-names></name><name><surname>Warren</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The pop out of scene-relative object movement against retinal motion due to self-movement</article-title><source>Cognition</source><volume>105</volume><fpage>237</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2006.09.004</pub-id><pub-id pub-id-type="pmid">17069787</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushton</surname><given-names>SK</given-names></name><name><surname>Niehorster</surname><given-names>DC</given-names></name><name><surname>Warren</surname><given-names>PA</given-names></name><name><surname>Li</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Primary Role of Flow Processing in the Identification of Scene-Relative Object Movement</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1737</fpage><lpage>1743</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3530-16.2017</pub-id><pub-id pub-id-type="pmid">29229707</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>R</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dissociation of Self-Motion and Object Motion by Linear Population Decoding That Approximates Marginalization</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>11204</fpage><lpage>11219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1177-17.2017</pub-id><pub-id pub-id-type="pmid">29030435</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>R</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Processing of object motion and self-motion in the lateral subdivision of the medial superior temporal area in macaques</article-title><source>Journal of Neurophysiology</source><volume>121</volume><fpage>1207</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1152/jn.00497.2018</pub-id><pub-id pub-id-type="pmid">30699042</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>R</given-names></name><name><surname>Anzai</surname><given-names>A</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Flexible coding of object motion in multiple reference frames by parietal cortex neurons</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1004</fpage><lpage>1015</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0656-0</pub-id><pub-id pub-id-type="pmid">32541964</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlack</surname><given-names>A</given-names></name><name><surname>Albright</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remembering visual motion: neural correlates of associative plasticity and motion recall in cortical area MT</article-title><source>Neuron</source><volume>53</volume><fpage>881</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.02.028</pub-id><pub-id pub-id-type="pmid">17359922</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A computational analysis of the relationship between neuronal and behavioral responses to visual motion</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1486</fpage><lpage>1510</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-04-01486.1996</pub-id><pub-id pub-id-type="pmid">8778300</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Beierholm</surname><given-names>UR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Causal inference in perception</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>425</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.07.001</pub-id><pub-id pub-id-type="pmid">20705502</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Attentional modulation of visual motion processing in cortical areas MT and MST</article-title><source>Nature</source><volume>382</volume><fpage>539</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1038/382539a0</pub-id><pub-id pub-id-type="pmid">8700227</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>7591</fpage><lpage>7602</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-17-07591.1999</pub-id><pub-id pub-id-type="pmid">10460265</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uka</surname><given-names>T</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Contribution of area MT to stereoscopic depth perception: choice-related response modulations reflect task strategy</article-title><source>Neuron</source><volume>42</volume><fpage>297</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(04)00186-2</pub-id><pub-id pub-id-type="pmid">15091344</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Drury</surname><given-names>HA</given-names></name><name><surname>Dickson</surname><given-names>J</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Hanlon</surname><given-names>D</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrated software suite for surface-based analyses of cerebral cortex</article-title><source>Journal of the American Medical Informatics Association</source><volume>8</volume><fpage>443</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1136/jamia.2001.0080443</pub-id><pub-id pub-id-type="pmid">11522765</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>PA</given-names></name><name><surname>Rushton</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Perception of object trajectory: parsing retinal motion into self and object movement components</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/7.11.2</pub-id><pub-id pub-id-type="pmid">17997657</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>PA</given-names></name><name><surname>Rushton</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Evidence for flow-parsing in radial flow displays</article-title><source>Vision Research</source><volume>48</volume><fpage>655</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.10.023</pub-id><pub-id pub-id-type="pmid">18243274</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>PA</given-names></name><name><surname>Rushton</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2009">2009a</year><article-title>Optic flow processing for the assessment of object movement during ego movement</article-title><source>Current Biology</source><volume>19</volume><fpage>1555</fpage><lpage>1560</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.07.057</pub-id><pub-id pub-id-type="pmid">19699091</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>PA</given-names></name><name><surname>Rushton</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2009">2009b</year><article-title>Perception of scene-relative object movement: Optic flow parsing and the contribution of monocular depth cues</article-title><source>Vision Research</source><volume>49</volume><fpage>1406</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.01.016</pub-id><pub-id pub-id-type="pmid">19480063</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>PA</given-names></name><name><surname>Rushton</surname><given-names>SK</given-names></name><name><surname>Foulkes</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Does optic flow parsing depend on prior estimation of heading?</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/12.11.8</pub-id><pub-id pub-id-type="pmid">23064244</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>8934</fpage><lpage>8944</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4030-07.2008</pub-id><pub-id pub-id-type="pmid">18768687</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>WH</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>TS</given-names></name><name><surname>Wong</surname><given-names>KM</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Complementary congruent and opposite neurons achieve concurrent multisensory integration and segregation</article-title><source>eLife</source><volume>8</volume><elocation-id>e43753</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43753</pub-id><pub-id pub-id-type="pmid">31120416</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>A normative theory for causal inference and Bayes factor computation in neural circuits</article-title><source>NeurIPS</source><volume>32</volume><fpage>3804</fpage><lpage>3813</lpage></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohary</surname><given-names>E</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title><source>Nature</source><volume>370</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1038/370140a0</pub-id><pub-id pub-id-type="pmid">8022482</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74971.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Clark</surname><given-names>Damon A</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.11.16.468843" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.16.468843"/></front-stub><body><p>This paper will be of broad interest to readers in the field of visual processing. The authors use concurrent psychophysics and single unit recordings, along with modeling, to investigate how visual signals in primate cortical area MT can distinguish between visual motion induced by self-motion and the motion of other objects in the world. The experiments provide an explanation for otherwise puzzling discrepancies in the depth tuning of MT cells.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74971.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Clark</surname><given-names>Damon A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Nandy</surname><given-names>Anirvan S</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Layton</surname><given-names>Oliver W</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00fvyjk73</institution-id><institution>Colby College</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.16.468843">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.11.16.468843v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A neural mechanism for detecting object motion during self-motion&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Anirvan S Nandy (Reviewer #2); Oliver W. Layton (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers agreed that this was an interesting study and with careful controls. During the discussion, there were 3 consensus points that will require addressing in your revision, as you will also see in the individual reviews below.</p><p>1) There should be more discussion of the relationship between the incongruency mechanism explored here and the more general mechanism of flow parsing. How would this mechanism relate to or be combined with or substitute for flow parsing under different conditions?</p><p>2) The authors should perform neurometric analyses and modeling to more directly substantiate their claims. In particular, the population of cells included in this analysis could be more directly tied to populations of cells that include no incongruent cells, include only incongruent cells, or include some range of congruencies, to see directly how these different tunings in a population of cells affect decoding. This would go further than the current less direct analyses, and strengthen conclusions about the role of incongruent cells.</p><p>3) In Figure S2, two reviewers were a bit confused about the implications of a monkey getting the monocular presentation wrong 90-95% of the time, if we are reading that plot correctly. Reviewers expected this to be more like 50% performance. How should this be interpreted?</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) In this paper, starting in the abstract, the authors emphasize the role played by these incongruent MP/BD neurons. In the discussion, they say (quoting in full because there are no line numbers to reference): &quot;our findings support the idea that a sensory representation consisting of a mixture of congruent and opposite cells provides a useful sensory substrate for causal inference (Rideaux et al. 2021). To our knowledge, these findings provide the first empirical evidence for a specific contribution of opposite neurons to perceptual inference about causes of sensory signals.&quot; However, as far as I could tell, the analyses and modeling never actually pulled out the incongruent neurons in particular to analyze them in isolation or analyze the performance without them. Thus, I had a hard time feeling this claim was supported.</p><p>In particular, in Figure 5G, the authors conclude: &quot;Thus, the MT neurons that most strongly predict decisions to detect the dynamic object (on ambiguous trials) are those with incongruent tuning that makes them selective for dynamic objects.&quot; But this is a bit indirect, because the figure shows correlation between detection probability and neurometric performance, and the neurometric performance is correlated with congruency. Why not just correlate DP with a direct measure of non-congruence?</p><p>(On that same plot (Figure 5G), I'm puzzled why there are so few DP and NP values &lt;0.5, when roughly half of all neurons were congruent in their BD and MP tuning – wouldn't one expect those neurons to have NP&lt;0.5 and DP&lt;0.5?)</p><p>Most generally, I'm still unclear about how much congruent neurons can contribute to these decisions compared to the incongruent ones. Given how well the population decoding works, it would be interesting to know if it still worked this well with *only* congruent or *only* incongruent neurons included. Is there an alternative hypothesis where having a range of different MP vs. BD tunings is what allows correct identification of the moving object, and it's the tuning variability rather than the incongruent neurons per se that matters? One could test this directly by including different ranges of neuronal tunings into a predictive model. Overall, it seems like there should be more direct analyses of the incongruent neuron population to provide support for the strong claims the authors are making about their role.</p><p>2) Figure S2: I'm a little puzzled at how a monkey could get it wrong 95% of the time in the monocular +1.5 degree \Δ Depth condition. What's going on there? Doesn't this mean that there *are* monocular cues for solving this task and they're just being interpreted entirely wrongly by the monkey? How should this be interpreted?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I would like the authors to consider these two points:</p><p>(A) In each experimental session, the axis of translation was aligned with the preferred axis of the neuron under observation in order to elicit robust responses. What would be the role of these incongruent neurons when self-motion is not aligned to this axis? Will it be possible to show that a population code of such neurons is sufficient to detect dynamic objects in the face of an arbitrary axis of self-motion?</p><p>(B) The authors state that it is not a worthwhile exercise to find model parameters that best match the empirical data, noting that they would have to make assumptions about the structure of correlated noise. It appears that in their dataset they should have several sets of simultaneously recorded neurons (e.g. 70 neurons from 57 sessions in M2). Might these give a hint to this correlation structure?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I have a few questions and some suggestions that I think would improve the manuscript and better contextualize the findings.</p><p>1. The stimulus in the present study contains background dots in the periphery outside the masked region that produce retinal motion consistent with the observer's self-motion. It would be helpful if the authors clarified the extent to which this peripheral motion may activate MSTd neurons and potentially recruit a flow parsing mechanism. In other words, is it possible that the background motion engages flow parsing, perhaps interacting with the proposed mechanism to some extent?</p><p>2. In several places in the paper (e.g. bottom of p.3, p. 26, and p. 28) the authors' descriptions may be taken to mean that the proposed local mechanism circumvents the need for flow parsing. Some examples: &quot;it allows detection of object motion without the need for more complex computations that discount the global flow field&quot; (p.3); &quot;this mechanism… may be relatively economical for the nervous system to implement&quot; (p.3); &quot; An advantage of our proposed mechanism over flow parsing is that it does not require estimation of the global flow field, nor a complicated mechanism …&quot; (p. 28). Given that the authors acknowledge elsewhere in the manuscript that the proposed mechanism likely complements flow parsing, I think the authors would agree that the proposed mechanism is not a direct substitute in general and rather is likely synergistic (e.g. multiple solutions to the same problem enhance robustness in different contexts, the proposed mechanism may be especially useful when quick reaction time is needed, etc.). Flow parsing is a much broader process and addresses far more than moving object detection. I would suggest rephrasing some of these statements and being cautious with language like &quot;advantages over&quot; flow parsing because the proposed local mechanism is more specialized than flow parsing.</p><p>3. I think the limitations from the public review should be referenced/discussed in the paper.</p><p>4. Warren and Rushton (2009) have demonstrated that humans parse object motion from self-motion to a similar extent regardless of whether the background motion surrounding a moving object is present in a monocular display. Niehorster and Li (2017) found a similar result for stereo optic flow. Simulations by Layton and Fajen (2016; 2020) and Layton and Niehorster (2019) support the hypothesis that a global mechanism plays a primary role. Together, these findings suggest that local mechanisms may play a much smaller role, at least for flow parsing. How do these findings relate to the proposed mechanism?</p><p>5. I found Supplementary Figure 1b very helpful to develop an intuition about the displays (as was the included video). I know figure space is at a premium, but if possible, including Supplementary Figure 1b in the main manuscript would likely help readers because the stimulus design is rather complex.</p><p>6. Aspects of the paragraph related to opposite at the bottom of p. 29 sounded repetitive after reading the previous parts of the paper and I thought they could be condensed/removed.</p><p>7. I found the motion parallax light blue color difficult to see in Figure 2. I would appreciate it if the authors replaced it with a more salient color.</p><p>8 Supplementary Figure 2: Why is the proportion correct so much less than 50% on the monocular task? Given that the monocular task is very challenging and there are two possibilities, I would have expected the proportion correct to be roughly 50%.</p><p>9. p. 7. I don't think I understand the following statement and would appreciate clarification: &quot;…animals were translated along an axis in the fronto-parallel plane that was aligned with the preferred-null axis of the neuron under study.&quot; Does fronto-parallel plane mean rotation about the Z axis (axis toward the stimulus)?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74971.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers agreed that this was an interesting study and with careful controls. During the discussion, there were 3 consensus points that will require addressing in your revision, as you will also see in the individual reviews below.</p><p>1) There should be more discussion of the relationship between the incongruency mechanism explored here and the more general mechanism of flow parsing. How would this mechanism relate to or be combined with or substitute for flow parsing under different conditions?</p><p>2) The authors should perform neurometric analyses and modeling to more directly substantiate their claims. In particular, the population of cells included in this analysis could be more directly tied to populations of cells that include no incongruent cells, include only incongruent cells, or include some range of congruencies, to see directly how these different tunings in a population of cells affect decoding. This would go further than the current less direct analyses, and strengthen conclusions about the role of incongruent cells.</p><p>3) In Figure S2, two reviewers were a bit confused about the implications of a monkey getting the monocular presentation wrong 90-95% of the time, if we are reading that plot correctly. Reviewers expected this to be more like 50% performance. How should this be interpreted?</p></disp-quote><p>We have addressed and resolved all three of these issues. Here is a brief summary of how the consensus points have been addressed, with further details described below. (1) We have added considerable discussion of the relationship between this new mechanism and flow parsing. We agree with Reviewer #3 that they are complementary mechanisms. (2) We have added new single-cell-level analyses of the relationships between incongruency and both detection probability and neurometric performance (new Figure 5—figure supplement 1). We have also now separately decoded incongruent and congruent neurons and demonstrate better performance by incongruent neurons (new Figure 7). (3) We have cleared up the issue of the behavioral control performance. Performance was less 50% because these data came from the 4-alternative version of the task (25% chance level). In addition to addressing these consensus issues, we have addressed all of the other specific points of the reviewers, as detailed below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) In this paper, starting in the abstract, the authors emphasize the role played by these incongruent MP/BD neurons. In the discussion, they say (quoting in full because there are no line numbers to reference): &quot;our findings support the idea that a sensory representation consisting of a mixture of congruent and opposite cells provides a useful sensory substrate for causal inference (Rideaux et al. 2021). To our knowledge, these findings provide the first empirical evidence for a specific contribution of opposite neurons to perceptual inference about causes of sensory signals.&quot; However, as far as I could tell, the analyses and modeling never actually pulled out the incongruent neurons in particular to analyze them in isolation or analyze the performance without them. Thus, I had a hard time feeling this claim was supported.</p><p>In particular, in Figure 5G, the authors conclude: &quot;Thus, the MT neurons that most strongly predict decisions to detect the dynamic object (on ambiguous trials) are those with incongruent tuning that makes them selective for dynamic objects.&quot; But this is a bit indirect, because the figure shows correlation between detection probability and neurometric performance, and the neurometric performance is correlated with congruency. Why not just correlate DP with a direct measure of non-congruence?</p></disp-quote><p>These are very reasonable criticisms, and have led us to explore the relationships between incongruency, neurometric performance, and detection probability further. First, we now show (Figure 5—figure supplement 1A) that neurometric performance is significantly correlated with peak response ratio. This result shows that neurons with peak response ratios substantially greater than unity almost always have neurometric performance &gt; 0.5. However, this result also shows that there are neurons with peak response ratios near unity (including slightly less than unity) that also have high neurometric performance (such as the example neuron in Figure 2C,F).</p><p>On the reviewer’s suggestion, we also looked at the relationship between DP and peak response ratio directly, as now shown in Figure 5—figure supplement 1B. Although there is a trend in the expected direction, it is not significant. The main reason for this appears to be that there is a cluster of neurons with large peak response ratios that have DP values near 0.5. Digging into this further, we discovered that these neurons generally have tuning properties similar to the example cell in Figure 2B,E. In cases like this, while incongruent tuning creates a clear preference for dynamic objects, it is limited to a narrow range of depth values, and some dynamic objects elicit responses that are weaker than many stationary objects. Thus, what seems to be crucial for producing high DP values is that a neuron <italic>consistently</italic> prefers dynamic objects over the range tested. Thus, the monkey may have adopted a strategy of selectively reading out neurons that consistently prefer dynamic objects across the stimulus range. In this case, neurons with high peak response ratios but strongly varying responses to dynamic objects (Figure 2 B,E) do not appear to be well correlated with decisions.</p><p>We have now expanded the Results section (pp. 11-12) to present and discuss these findings. We have also modified text throughout the manuscript to reflect our finding that correlation with decisions depends mainly on a consistent preference for dynamic objects rather than the presence of incongruency per se.</p><disp-quote content-type="editor-comment"><p>(On that same plot (Figure 5G), I'm puzzled why there are so few DP and NP values &lt;0.5, when roughly half of all neurons were congruent in their BD and MP tuning – wouldn't one expect those neurons to have NP&lt;0.5 and DP&lt;0.5?)</p></disp-quote><p>We think there are two parts of an answer to this question. First, the reviewer seems to assume that if NP &lt; 0.5 then DP should also be &lt;0.5. But this doesn’t necessarily need to be the case. For neurons that prefer stationary objects, it is certainly possible that they are mainly uncorrelated with decisions rather than being anticorrelated. Second, the reviewer asks why NP values are not symmetrically distributed around 0.5 in Figure 5G. This is a good question; our analyses suggest that this arises because of the distribution of our stimulus values for the dynamic object condition. Because almost all neurons were tested with a pedestal depth of -0.45 deg, the depth values for both disparity and motion parallax tend to be mostly negative for the dynamic object conditions (this can be observed by looking at the two x-axes in Figure 2D-F). This bias toward negative (near) values of dynamic objects, combined with the fact that most neurons have a near preference for depth from motion parallax (Figure 3A), means that most neurons, including most congruent cells, tended to have mean responses to dynamic objects that were above the mean response to stationary objects (e.g., Figure 2D). And this is captured by the neurometric performance. We now discuss this asymmetry in the text on p. 11.</p><disp-quote content-type="editor-comment"><p>Most generally, I'm still unclear about how much congruent neurons can contribute to these decisions compared to the incongruent ones. Given how well the population decoding works, it would be interesting to know if it still worked this well with *only* congruent or *only* incongruent neurons included. Is there an alternative hypothesis where having a range of different MP vs. BD tunings is what allows correct identification of the moving object, and it's the tuning variability rather than the incongruent neurons per se that matters? One could test this directly by including different ranges of neuronal tunings into a predictive model. Overall, it seems like there should be more direct analyses of the incongruent neuron population to provide support for the strong claims the authors are making about their role.</p></disp-quote><p>We thank the reviewer for making this suggestion. We have now performed decoding simulations after breaking up the population into three groups based on the peak response ratio measure of preference for dynamic objects. These new results show that neurons with high peak response ratios produce better decoding performance and greater predicted detection probabilities than neurons with low peak response ratios. These results are shown in a new Figure 7 and described in the associated text on p. 14. We also found similar results when dividing the population into groups based on the correlation between depth tuning for disparity and motion parallax R<sub>MP_BD</sub> (see text). We think that these new findings address the reviewer’s concern by demonstrating directly that neurons with incongruent tuning enable better performance for decoding scene-relative object motion.</p><disp-quote content-type="editor-comment"><p>2) Figure S2: I'm a little puzzled at how a monkey could get it wrong 95% of the time in the monocular +1.5 degree \Δ Depth condition. What's going on there? Doesn't this mean that there *are* monocular cues for solving this task and they're just being interpreted entirely wrongly by the monkey? How should this be interpreted?</p></disp-quote><p>We thank Reviewers #1 and #3 for questioning this. It turns out that these control data were from the version of the task with 4 objects, rather than the version with two objects. Thus, the chance level of performance (now indicated in the figure) is 25%. This accounts for most of the mystery regarding the monocular condition. The remaining variation around the chance level is presumably just somewhat random as the monkey was only exposed to the monocular condition in 16% of trials across 3 sessions. The figure and caption have been modified to clarify this issue (now Figure 1—figure supplement 2).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I would like the authors to consider these two points:</p><p>(A) In each experimental session, the axis of translation was aligned with the preferred axis of the neuron under observation in order to elicit robust responses. What would be the role of these incongruent neurons when self-motion is not aligned to this axis? Will it be possible to show that a population code of such neurons is sufficient to detect dynamic objects in the face of an arbitrary axis of self-motion?</p></disp-quote><p>We don’t have any data to address this question directly because we did not record from neurons while we intentionally misaligned the stimulus axis with the preferred direction. That said, MT neurons are rather broadly tuned, and so we would expect their response modulations to fall off gradually with such a misalignment. In this sense, we would not expect this case to be substantially different from any analogous situation involving a population of neurons with broadly distributed preferences. As the axis of motion changes, some neurons will become less informative while others will become more informative.</p><disp-quote content-type="editor-comment"><p>(B) The authors state that it is not a worthwhile exercise to find model parameters that best match the empirical data, noting that they would have to make assumptions about the structure of correlated noise. It appears that in their dataset they should have several sets of simultaneously recorded neurons (e.g. 70 neurons from 57 sessions in M2). Might these give a hint to this correlation structure?</p></disp-quote><p>Unfortunately, there seems to be a misunderstanding here. There were not 70 neurons recorded from 57 sessions in M2 during the discrimination task. In fact, we didn’t have any well-isolated cell pairs that were recorded through the main detection task. For M2, the 70 single units that were recorded during the preliminary tests came from 82 recording sessions (as now clarified in Methods, p. 34, for both animals). Thus, we don’t have sufficient data to make any reasonable attempt at constraining the correlation structure.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I have a few questions and some suggestions that I think would improve the manuscript and better contextualize the findings.</p><p>1. The stimulus in the present study contains background dots in the periphery outside the masked region that produce retinal motion consistent with the observer's self-motion. It would be helpful if the authors clarified the extent to which this peripheral motion may activate MSTd neurons and potentially recruit a flow parsing mechanism. In other words, is it possible that the background motion engages flow parsing, perhaps interacting with the proposed mechanism to some extent?</p></disp-quote><p>We have not recorded from MSTd neurons using these stimuli, so we do not have a direct answer to this question based on data. However, we think it is reasonable to assume that the background motion would drive at least some MSTd neurons. Thus, it is possible that flow parsing could also be engaged to some extent in our task, as now acknowledged explicitly on p. 17. However, as the reviewer notes in a previous comment, flow parsing does not require disparity signals whereas the local mechanism demonstrated by our findings does. Hence, it seems unlikely that flow parsing plays a major role here.</p><disp-quote content-type="editor-comment"><p>2. In several places in the paper (e.g. bottom of p.3, p. 26, and p. 28) the authors' descriptions may be taken to mean that the proposed local mechanism circumvents the need for flow parsing. Some examples: &quot;it allows detection of object motion without the need for more complex computations that discount the global flow field&quot; (p.3); &quot;this mechanism… may be relatively economical for the nervous system to implement&quot; (p.3); &quot; An advantage of our proposed mechanism over flow parsing is that it does not require estimation of the global flow field, nor a complicated mechanism …&quot; (p. 28). Given that the authors acknowledge elsewhere in the manuscript that the proposed mechanism likely complements flow parsing, I think the authors would agree that the proposed mechanism is not a direct substitute in general and rather is likely synergistic (e.g. multiple solutions to the same problem enhance robustness in different contexts, the proposed mechanism may be especially useful when quick reaction time is needed, etc.). Flow parsing is a much broader process and addresses far more than moving object detection. I would suggest rephrasing some of these statements and being cautious with language like &quot;advantages over&quot; flow parsing because the proposed local mechanism is more specialized than flow parsing.</p></disp-quote><p>We do agree that the two mechanisms are synergistic and complementary, and hadn’t meant to imply that this mechanism could replace flow parsing (nor did we say that explicitly). Nevertheless, we see the reviewer’s point that some of our language might be construed in that direction. We have now modified the text in all of the places noted by the reviewer, as well as several others throughout the manuscript, to emphasize the complementarity of the two mechanisms.</p><disp-quote content-type="editor-comment"><p>3. I think the limitations from the public review should be referenced/discussed in the paper.</p></disp-quote><p>As noted in responses to other comments, we have made several revisions to address these points, including a greatly expanded section about flow parsing in the Discussion (pp. 17-18).</p><disp-quote content-type="editor-comment"><p>4. Warren and Rushton (2009) have demonstrated that humans parse object motion from self-motion to a similar extent regardless of whether the background motion surrounding a moving object is present in a monocular display. Niehorster and Li (2017) found a similar result for stereo optic flow. Simulations by Layton and Fajen (2016; 2020) and Layton and Niehorster (2019) support the hypothesis that a global mechanism plays a primary role. Together, these findings suggest that local mechanisms may play a much smaller role, at least for flow parsing. How do these findings relate to the proposed mechanism?</p></disp-quote><p>We agree that a key difference is that flow parsing involves a global mechanism, as described by Warren and Rushton (2009), and we have also demonstrated this finding in monkeys (Peltier et al. 2020). In contrast, the mechanism described in this study can be computed locally within each portion of the visual field. We now devote a paragraph of the discussion (p. 17) to this issue.</p><disp-quote content-type="editor-comment"><p>5. I found Supplementary Figure 1b very helpful to develop an intuition about the displays (as was the included video). I know figure space is at a premium, but if possible, including Supplementary Figure 1b in the main manuscript would likely help readers because the stimulus design is rather complex.</p></disp-quote><p>We appreciate the suggestion, as we considered it carefully, but we prefer to keep this figure in the supplement.</p><disp-quote content-type="editor-comment"><p>6. Aspects of the paragraph related to opposite at the bottom of p. 29 sounded repetitive after reading the previous parts of the paper and I thought they could be condensed/removed.</p></disp-quote><p>With the substantially expanded discussion of flow parsing, we think that this paragraph is worth retaining and is not redundant, as it makes distinct points regarding reference frames and multisensory contributions. We have trimmed it down some, however (now on p. 18).</p><disp-quote content-type="editor-comment"><p>7. I found the motion parallax light blue color difficult to see in Figure 2. I would appreciate it if the authors replaced it with a more salient color.</p></disp-quote><p>Thanks for the suggestion. We have made the cyan color darker so that it is more visible.</p><disp-quote content-type="editor-comment"><p>8 Supplementary Figure 2: Why is the proportion correct so much less than 50% on the monocular task? Given that the monocular task is very challenging and there are two possibilities, I would have expected the proportion correct to be roughly 50%.</p></disp-quote><p>We thank Reviewers #1 and #3 for questioning this. It turns out that these control data were from the version of the task with 4 objects, rather than the version with two objects. Thus, the chance level of performance (now indicated in the figure) is 25%. This accounts for most of the mystery regarding the monocular condition. The remaining variation around the chance level is presumably just somewhat random as the monkey was only exposed to the monocular condition in 16% of trials across 3 sessions. The figure and caption have been modified to clarify this issue (now Figure 1—figure supplement 2).</p><disp-quote content-type="editor-comment"><p>9. p. 7. I don't think I understand the following statement and would appreciate clarification: &quot;…animals were translated along an axis in the fronto-parallel plane that was aligned with the preferred-null axis of the neuron under study.&quot; Does fronto-parallel plane mean rotation about the Z axis (axis toward the stimulus)?</p></disp-quote><p>No, the fronto-parallel plane does not imply any rotation about the Z-axis. The fronto-parallel plane is a pretty standard term for the vertical plane that includes the interaural axis, and which is parallel to the plane of the display screen. This is now stated on p. 24 for additional context. To align the stimulus axis with the preferred-null axis, the entire stimulus was rotated around the naso-occipital axis (which is what we presume the reviewer means by the Z-axis), as was stated in Methods previously. We have also added some clarification of this axis (p. 24).</p></body></sub-article></article>