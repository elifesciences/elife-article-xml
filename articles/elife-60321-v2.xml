<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">60321</article-id><article-id pub-id-type="doi">10.7554/eLife.60321</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Graphical-model framework for automated annotation of cell identities in dense cellular images</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-196113"><name><surname>Chaudhary</surname><given-names>Shivesh</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1928-0933</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196114"><name><surname>Lee</surname><given-names>Sol Ah</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196115"><name><surname>Li</surname><given-names>Yueyi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196116"><name><surname>Patel</surname><given-names>Dhaval S</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-115800"><name><surname>Lu</surname><given-names>Hang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6881-660X</contrib-id><email>hang.lu@gatech.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>School of Chemical &amp; Biomolecular Engineering, Georgia Institute of Technology</institution><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Petit Institute for Bioengineering and Bioscience, Georgia Institute of Technology</institution><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>24</day><month>02</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e60321</elocation-id><history><date date-type="received" iso-8601-date="2020-06-23"><day>23</day><month>06</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-02-23"><day>23</day><month>02</month><year>2021</year></date></history><permissions><copyright-statement>Â© 2021, Chaudhary et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Chaudhary et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-60321-v2.pdf"/><abstract><p>Although identifying cell names in dense image stacks is critical in analyzing functional whole-brain data enabling comparison across experiments, unbiased identification is very difficult, and relies heavily on researchersâ€™ experiences. Here, we present a probabilistic-graphical-model framework, CRF_ID, based on Conditional Random Fields, for unbiased and automated cell identification. CRF_ID focuses on maximizing intrinsic similarity between shapes. Compared to existing methods, CRF_ID achieves higher accuracy on simulated and ground-truth experimental datasets, and better robustness against challenging noise conditions common in experimental data. CRF_ID can further boost accuracy by building atlases from annotated data in highly computationally efficient manner, and by easily adding new features (e.g. from new strains). We demonstrate cell annotation in <italic>Caenorhabditis elegans</italic> images across strains, animal orientations, and tasks including gene-expression localization, multi-cellular and whole-brain functional imaging experiments. Together, these successes demonstrate that unbiased cell annotation can facilitate biological discovery, and this approach may be valuable to annotation tasks for other systems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cell annotation</kwd><kwd>automation</kwd><kwd>probabilistic graphical model</kwd><kwd>whole-brain</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>C. elegans</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21DC015652</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS096581</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01GM088333</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1764406</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1707401</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01GM108962</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P40OD010440</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Hang</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Unbiased and automatic annotation using structured prediction framework with efficiently built data-driven atlases is more accurate than registration-based methods for cell identifications in dense images and enables fast whole-brain analysis.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Annotation of anatomical structures at cellular resolution in large image sets is a common data analysis step in many studies in <italic>Caenorhabditis elegans</italic> such as gene expression pattern analysis (<xref ref-type="bibr" rid="bib39">Long et al., 2009</xref>; <xref ref-type="bibr" rid="bib45">Murray, 2008</xref>), lineage tracing (<xref ref-type="bibr" rid="bib4">Bao et al., 2006</xref>), multi-cell calcium imaging and whole-brain imaging (<xref ref-type="bibr" rid="bib65">SchrÃ¶del et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Nguyen et al., 2016</xref>). It is necessary for cellular resolution comparison of data across animals, trials, and experimental conditions. Particularly in whole-brain functional imaging, meaningful interpretation of population activity critically depends on cell identities as they facilitate the incorporation of existing knowledge about the system (<xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>). Cell identities are also needed for applying common statistical data analysis methods such as Principal Component Analysis, Tensor Component Analysis, demixed-Principal Component AnalysisÂ (<xref ref-type="bibr" rid="bib83">Williams et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Kobak et al., 2016</xref>)Â etc as data across experiments needs to be indexed and pooled by cell identities before applying these methods.</p><p>While accurate annotation of cell identities in images is critical, this task is difficult. Typically, the use of cell-specific markers as landmarks delivers good accuracy, but has the cost of having to engineer cell-specific reagents without interfering with phenotypes of interest, which is not guaranteed. Further, even with markers such as the recently developed impressive reagents in the NeuroPAL collection (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>), there is stillÂ a need to automate the cell identification process. In the absence of markers, cells are identified by comparing images to a reference atlas such as WormAtlas (<xref ref-type="bibr" rid="bib2">Altun and Hall, 2009</xref>) and OpenWorm (<xref ref-type="bibr" rid="bib72">Szigeti et al., 2014</xref>) atlas. However, there are severe limitations from both using reference atlas and the presence of noise in data. Reference atlases assume a static and often single view of the anatomy; in contrast, anatomical features vary across individuals. Moreover, due to variations in experimental conditions during acquisition such as exact resolution and orientation of animals, image data often do not match the static atlases, making manual cell identification extremely difficult if not infeasible. Separately, two kinds of noise are prevalent in data. First, individual-to-individual variability in cell positions compared to positions in atlas (position noise). Second, mismatch between number of cells in image and atlas (count noise). Count noise is primarily caused by variability in the expression levels of the reporter used to label cells across animals (i.e. mosaicism), incomplete coverage of promoter to label desired cells, and limits in the computational methods to detect cells. In each of these cases, fewer cells are detected in the images than cells in the atlas. Empirical data have shown that in normalized coordinates, a cellâ€™s position can deviate from the atlas position by more than the cellâ€™s distance to its tenthâ€™ nearest neighbor in the image (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>; <xref ref-type="bibr" rid="bib75">Toyoshima et al., 2016</xref>). Further, our data, as well as data from other labs, have shown that 30â€“50% of cells in atlases may be missing from images (<xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Nguyen et al., 2016</xref>). As a result of theÂ large position and count noise common in data, identifying densely packed cells in head ganglion images of <italic>C. elegans</italic> by manually comparing images to the atlas is extremely difficult, even for experienced researchers. Further, manual annotation is labor intensive. Therefore, there is a critical need for automated methods for cell identification.</p><p>Previous computational methods for cell identification in <italic>C. elegans</italic> images (<xref ref-type="bibr" rid="bib39">Long et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Long et al., 2008</xref>; <xref ref-type="bibr" rid="bib58">Qu et al., 2011</xref>; <xref ref-type="bibr" rid="bib1">Aerni et al., 2013</xref>) focused on identifying sparsely distributed cells with stereotypical positions in young larvae animals. Tools for identification of cells in whole-brain datasets, thatÂ is in dense head ganglion, do not exist. Further, previous methods (<xref ref-type="bibr" rid="bib39">Long et al., 2009</xref>; <xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>; <xref ref-type="bibr" rid="bib58">Qu et al., 2011</xref>; <xref ref-type="bibr" rid="bib1">Aerni et al., 2013</xref>; <xref ref-type="bibr" rid="bib76">Toyoshima, 2019</xref>; <xref ref-type="bibr" rid="bib64">Scholz, 2018</xref>) do not explicitly address the challenges imposed by the presence of position and count noise in theÂ data. All previous methods either areÂ registration-based or formulate a linear assignment problem; objective functions in these methods minimize a first-order constraint such as the distances between cell-specific features in images and atlases. Thus, these methods maximize only extrinsic similarity (<xref ref-type="bibr" rid="bib7">Bronstein et al., 2007</xref>) between images and atlas, which is highly sensitive to count noise, position noise, and pre-alignment of spaces in which the image and the atlas exist (i.e. orientations of animals in images and atlases). With the amount of position and count noise commonly observed in experimental data, registration-based methods produce large matching errors.</p><p>An alternative criterion proposed for topology-invariant matching of shapes is to maximize intrinsic similarity (<xref ref-type="bibr" rid="bib7">Bronstein et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Bronstein et al., 2009</xref>), orthogonal to extrinsic similarity. This approach has advantages because noise that affects extrinsic similarity does not necessarily imply worse intrinsic similarity. For instance, although cell positions in an image may deviate from their positions in the atlas (large extrinsic noise), geometrical relationships among them are largely maintained (low intrinsic noise). As a specific example, although absolute positions of the cell bodies of AIBL and RIML in an image may deviate greatly from their atlas positions, AIBL somaÂ stays anterior to RIMLÂ soma. ThereforeÂ intrinsic similarity is more robust against noises, independent of the pre-alignment of spaces, and inherently captures dependencies between cell label assignments that registration methods do not consider.</p><p>To directly optimize for intrinsic similarity and dependencies between label assignments, we cast the cell annotation problem as a Structured Prediction Problem (<xref ref-type="bibr" rid="bib3">Bakir, 2007</xref>; <xref ref-type="bibr" rid="bib51">Nowozin, 2010</xref>; <xref ref-type="bibr" rid="bib9">Caelli and Caetano, 2005</xref>; <xref ref-type="bibr" rid="bib24">Kappes et al., 2015</xref>) and build a Conditional Random Fields (CRF) model (<xref ref-type="bibr" rid="bib33">Lafferty et al., 2001</xref>) to solve it. The model directly optimizes cell-label dependencies by maximizing intrinsic and extrinsic similarities between images and atlases. One major advantage, as shown using both synthetic data with realistic properties (e.g. statistics from real data) and manually annotated experimental ground-truth datasets, is that CRF_ID achieves higher accuracy compared to existing methods. Further, CRF_ID outperforms existing methods in handling both position noise and count noise common in experimental data across all challenging noise levels.</p><p>To further improve accuracy, we took two approaches. First, we took advantage of spatially distributed (fluorescently labeled) landmark cells. These landmark cells act as additional constraints on the model, thus aiding in optimization, and helping in pre- as well post-prediction analysis. Second, we developed a methodology to build data-driven atlases that capture the statistics of the experimentally observed data for better prediction. We provide a set of computational tools for automatic and unbiased annotation of cell identities in fluorescence image data, and efficient building of data-driven atlases using fully or partially annotated image sets. We show the utility of our approach in several contexts: determining gene expression patterns with no prior expectations, tracking activities of multiple cells during calcium imaging, and identifying cells in whole-brain imaging videos. For the whole-brain imaging experiments, our annotation framework enabled us to analyze the simultaneously recorded response of <italic>C. elegans</italic> head ganglion to food stimulus and identify two distinct groups of cells whose activities correlated with distinct variables â€“ food sensation and locomotion.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Cell annotation formulation using structured prediction framework</title><p>Our automated cell annotation algorithm is formulated using Conditional Random Fields (CRF). CRF is a graphical model-based framework widely used for structured/relational learning tasks in Natural Language Processing and Computer Vision community (<xref ref-type="bibr" rid="bib3">Bakir, 2007</xref>; <xref ref-type="bibr" rid="bib51">Nowozin, 2010</xref>; <xref ref-type="bibr" rid="bib33">Lafferty et al., 2001</xref>; <xref ref-type="bibr" rid="bib71">Sutton and McCallum, 2010</xref>). The goal of structured learning tasks is to predict labels for structured objects such as graphs. In our neuron annotation problem, we assume that our starting point is a 3D image stack of the <italic>C. elegans</italic> head ganglion (<xref ref-type="fig" rid="fig1">Figure 1A(i)</xref>) in which neurons have alreadyÂ been detected (<xref ref-type="fig" rid="fig1">Figure 1A(ii)</xref>), either manually or by automated segmentation, and we want to match each neuronal cell body or nucleus to an identity label (a biological name). Hence, we have <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi></mml:math></inline-formula> detected neuronal cell bodies <inline-formula><mml:math id="inf2"><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> that form the set of observed variables <inline-formula><mml:math id="inf3"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, and their 3D coordinates, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>N</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We also have a neuron atlas that provides a set of labels <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (biological names) of the neurons and positional relationships among them. Note that the number of neurons in the atlas is greater than the number of neurons detected in the image stack in all datasets,Â that is <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The goal is to annotate a label <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to each neuron in the image stack. The problem is similar to structured labeling (<xref ref-type="bibr" rid="bib51">Nowozin, 2010</xref>) since the labels to be assigned to neurons are dependent on each other. For example, if a certain neuron is assigned label AVAL, then the neurons that can be assigned label RMEL become restricted since only the cells anterior to AVAL can be assigned RMEL label.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>CRF_ID annotation framework automatically predicts cell identities in image stacks.</title><p>(<bold>A</bold>)Â Steps in CRF_ID framework applied to neuron imaging in <italic>C. elegans</italic>. (i) Max-projection of a 3D image stack showing head ganglion neurons whose biological names (identities) are to be determined. (ii) Automatically detected cells (MaterialsÂ andÂ methods) shown as overlaid colored regions on the raw image. (iii) Coordinate axes are generated automatically (Note S1). (iv) Identities of landmark cells if available are specified. (v) Unary and pairwise positional relationship features are calculated in data. These features are compared against same features in atlas. (vi) Atlas can be easily built from fully or partially annotated dataset from various sources using the tools provided with framework. (vii) An example of unary potentials showing the affinity of each cell taking the label RMGL. (viii) An example of dependencies encoded by pairwise potentials, showing the affinity of each cell taking the label ALA given the arrow-pointed cell is assigned the label RMEL. (ix) Identities are predicted by simultaneous optimization of all potentials such that assigned labels maximally preserve the empirical knowledge available from atlases. (x) Predicted identities. (xi) Duplicate assignment of labels is handled using a label consistency score calculated for each cell (Appendix 1â€“Extended methods S1). (xii) The process is repeated with different combinations of missing cells to marginalize over missing cells (Note S1). Finally, top candidate label list is generated for each cell. (<bold>B</bold>)Â An example of automatically predicted identities (top picks) for each cell.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1â€”figure supplement 1.</label><caption><title>Schematic description of various features in the CRF model that relate to intrinsic similarity and extrinsic similarity.</title><p>(<bold>A</bold>)Â An example of binary positional relationship feature (Appendix 1â€“Extended methods S1.2.2) illustrated for positional relationships along AP axis. The table lists feature value for some exemplary assignment of labels <bold>â€˜Aâ€™</bold>, <bold>â€˜Bâ€™</bold>, and <bold>â€˜Câ€™</bold> from the atlas to cells â€˜1â€™ and â€˜2â€™ in the image data. ForÂ example since cell â€˜1â€™ is anterior to cell â€˜2â€™ in image, if labels assigned to these cells are consistent with the anterior-posterior positional relationship (e.g. <bold>â€˜A-Bâ€™</bold>, <bold>â€˜A-Câ€™</bold>, <bold>â€˜B-Câ€™</bold>), then the feature value is high (1); else low (0). CRF_ID model assigns identities to cells in image by maximizing the feature values for each pair of cells in image over all possible label assignments. The table also illustrates the difference between using a static atlas (or single data source) and a data-driven atlas built using available annotated data. In case of static atlas, the CRF model assumes that the cell â€˜<bold>A</bold>â€™ is anterior to cell â€˜<bold>B</bold>â€™ with 100% probability. In contrast, in experimental data cell â€˜<bold>A</bold>â€™ may be anterior to cell â€˜Bâ€™ with 80% probability (8 out of 10 datasets) and cell â€˜<bold>B</bold>â€™ may be anterior to cell â€˜<bold>C</bold>â€™ with 50% probability (5 out of 10 datasets). Thus, data-driven atlases relaxes the hard constraint and uses statistics from experimental data. The feature values are changed accordingly. Note, unlike registration based methods for building data-driven atlas, in CRF model data-driven atlases record only probabilistic positional relationship among cells and not probabilistic positions of cells. Thus CRF_ID does not build spatial atlas of cells. (<bold>B</bold>)Â An example of angular relationship feature (Appendix 1â€“Extended methods S1.2.4). The table lists feature value for some exemplary assignment of labels. For example, the feature value is highest for assigning labels â€˜<bold>A</bold>â€™ and â€˜<bold>C</bold>â€™ to cells â€˜1â€™ and â€˜2â€™â€™ because the vector joining cells â€˜<bold>A</bold>â€™ and â€˜<bold>C</bold>â€™ in atlas (<inline-formula><mml:math id="inf8"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula>) is most directionally similar to vector joining cells â€˜1â€™ and â€˜2â€™ in image (<inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mi>u</mml:mi><mml:mtext>12</mml:mtext></mml:msub><mml:mo stretchy="false">â†’</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) as measured by dot product of vectors. For data-driven atlas, average vectors in atlas are used. (<bold>C</bold>)Â An example of proximity relationship feature (Appendix 1â€“Extended methods S1.2.3). The table lists feature value for some exemplary assignment of labels. For example, the feature value is low for assigning labels â€˜Bâ€™ and â€˜Câ€™ to cells â€˜1â€™ and â€˜2â€™â€™ because the distance between cells â€˜Bâ€™ and â€˜Câ€™ in atlas (<inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) is least similar to distance between cells â€˜1â€™ and â€˜2â€™ in image (<inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). The distance metric can be Euclidean distance or geodesic distance. For data-driven atlas, average distances in atlas are used. (<bold>D</bold>)Â An example illustrating the cell annotation performed by maximizing extrinsic similarity in contrast to intrinsic similarity. Registration based methods maximize extrinsic similarity by minimizing registration cost function <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, a transformation <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is applied to the atlas and labels are annotated to cells in image by minimizing the assignment cost that is sum of distances between cell coordinates in image and transformed coordinates of cells in atlas. For data-driven atlas, a spatial atlas is built using annotated data that is used for registration. Note, in contrast, CRF_ID method does not build any spatial atlas of cells because it uses intrinsic similarity features. CRF_ID only builds atlases of intrinsic similarity features shown in panels (<bold>A</bold>-<bold>C</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1â€”figure supplement 2.</label><caption><title>Additional examples of unary and pairwise potentials and label consistency scores calculated for each cell.</title><p>(<bold>A</bold>)Â Unary potentials encode affinities of each cell to take specific labels in atlas. Here, affinities of all cells to take the label specified on the top right corner of images are shown. Randomly selected examples are shown here. In practice, unary potentials are calculated for all cells for every label. (<bold>B</bold>)Â Pairwise potentials encode affinities of pair of cells in head ganglion to get two labels from atlas. Here, we show the affinity of all cells taking the label specified on the top right corner of images given the cell marked by the arrow is assigned the given label. Randomly selected examples are shown here. In practice, pairwise potentials are calculated for all pairs of cells for all pairs of labels. (<bold>C</bold>)Â Examples of label-consistency score of cells that were assigned duplicate labels (specified on the top right corner of the image) in an intermediate step in framework. To remove duplicate assignments, only the cell with the highest consistency score is assigned the label. Optimization is run again to assign labels to all unlabeled cells while keeping the identities of labeled cells fixed. (<bold>D</bold>)Â Comparison of label-consistency scores for accurately predicted cells and incorrectly predicted cells. Correctly predicted cells have a higher binary positional relationship consistency score, close to one angular relationship consistency score (smaller angular deviation between labels in image and atlas) and close to 0 proximity consistency score (smaller Gromov-Wasserstein discrepancy). Scores shown for all 130 predicted cells in synthetic data acrossÂ ~1100 runs. Thus, n â‰ˆ 150,000. *** denotes p&lt;0.001, Bonferroni paired comparison test. Each run differed from the other in terms of random position noise and count noise applied to synthetic data to mimic real images. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig1-figsupp2-v2.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-60321-fig1-video1.mp4"><label>Figure 1â€”video 1.</label><caption><title>Identities predicted automatically by the CRF_ID framework in head ganglion stack.</title><p>Top five identities predicted are shown sorted by consistency score. Scale bar 5 Âµm.</p></caption></media></fig-group><p>We use CRF-based formulation to directly optimize for such dependencies and automatically assign names to each cell. Briefly, a node <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is associated with each observed variable <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (i.e. segmented neuron in image data) forming the set of variables <inline-formula><mml:math id="inf16"><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> in the model. Then, CRF models a conditional joint probability distribution <inline-formula><mml:math id="inf17"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula> over product space <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>Ã—</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>Ã—</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of labels assigned to <inline-formula><mml:math id="inf19"><mml:mi>V</mml:mi></mml:math></inline-formula> given observations <inline-formula><mml:math id="inf20"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula>, where each <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a particular assignment of labels to <inline-formula><mml:math id="inf23"><mml:mi>V</mml:mi></mml:math></inline-formula>. <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> contains non-optimal and optimal assignments. In CRF, label dependencies among various nodes are encoded by the structure of an underlying undirected graph <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> defined over nodes <inline-formula><mml:math id="inf26"><mml:mi>V</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the set of cliques in graph <inline-formula><mml:math id="inf28"><mml:mi mathvariant="normal">G</mml:mi></mml:math></inline-formula>. With the underlying graph structure, the conditional joint probability distribution <inline-formula><mml:math id="inf29"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula> over full label space <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> factorizes over cliques in <inline-formula><mml:math id="inf31"><mml:mi mathvariant="normal">G</mml:mi></mml:math></inline-formula>, making it tractable to the model:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:munder><mml:mo>âˆ</mml:mo><mml:mi>c</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf32"><mml:mi>Z</mml:mi></mml:math></inline-formula> is the normalization constantÂ with <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes clique potential if nodes in clique <inline-formula><mml:math id="inf35"><mml:mi>c</mml:mi></mml:math></inline-formula> are assigned label <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>Ã—</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>Ã—</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In our model, the fully connected graph structure considers only pairwise dependencies between every pair of neurons. Thus, the graph structure of our model becomes <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, with the node set <inline-formula><mml:math id="inf38"><mml:mi>V</mml:mi></mml:math></inline-formula> containing nodes <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> associated with each segmented neuron and pairwise edges between all nodes that form the edge set <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The potential functions in our model are node-potentials <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and edge-potentials <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. These functions are parameterized with unary feature functions <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>Ã—</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">â†’</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and pairwise feature functions <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>Ã—</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>Ã—</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">â†’</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>âŸ®</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>âŸ¯</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>âŸ®</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>âŸ¯</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are labels in atlas. Note, there are <inline-formula><mml:math id="inf47"><mml:mi>a</mml:mi></mml:math></inline-formula> unary features and <inline-formula><mml:math id="inf48"><mml:mi>b</mml:mi></mml:math></inline-formula> pairwise features with weights <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> respectively to define node and edge potentials.</p><p>While unary features account for extrinsic similarity and cell-specific features, pairwise features account for intrinsic similarity. To maximize accuracy, we encode pairwise dependencies between all pairs of cells in the form of several geometrical relationship features (<xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1</xref>). Optimal identities of all neurons <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained by maximizing the joint-distribution <inline-formula><mml:math id="inf52"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula>. This is equivalent to maximizing the following energy function.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">y</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Optimizing this energy function over fully connected graphs (more specifically graphs with loops) is known to be an NP-hard problem (<xref ref-type="bibr" rid="bib28">Kohli et al., 2009</xref>). However, approximate inference algorithms are widely used in CRF community as they provide reasonable solutions. We implemented a popular method called Loopy Belief Propagation (<xref ref-type="bibr" rid="bib44">Murphy et al., 1999</xref>) to infer the most probable labeling over all cells, as well as marginal distributions of label assignments for each cell.</p><p>The features used in the base version of the model are geometrical relationship features that ensure identities assigned to cells in image are consistent with the atlas in terms of satisfying pairwise geometrical relationships. These features include binary positional relationship feature, proximity relationship feature, and angular relationship feature (<xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1</xref>, Appendix 1â€“Extended methods S1.2). All these features are a variant of the quadratic Gromov-Wasserstein distance used in matching metric spaces (<xref ref-type="bibr" rid="bib8">Bronstein et al., 2009</xref>; <xref ref-type="bibr" rid="bib55">Peyre et al., 2016</xref>) and shapes (<xref ref-type="bibr" rid="bib67">Solomon et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">MÃ©moli, 2011</xref>). Briefly, binary positional relationship features encode that as an example, if cell <inline-formula><mml:math id="inf53"><mml:mi>i</mml:mi></mml:math></inline-formula> is anterior, dorsal and to the right of cell <inline-formula><mml:math id="inf54"><mml:mi>j</mml:mi></mml:math></inline-formula> in image stack, then identities assigned to these cells should satisfy these relationships in the atlas. Proximity relationship features ensure that if cell <inline-formula><mml:math id="inf55"><mml:mi>i</mml:mi></mml:math></inline-formula> is spatially near to cell <inline-formula><mml:math id="inf56"><mml:mi>j</mml:mi></mml:math></inline-formula> in image stack, then identities of spatially distant cells in atlas would not be assigned to these cells. Finally, angular relationships ensure that identities assigned to cells <inline-formula><mml:math id="inf57"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mi>j</mml:mi></mml:math></inline-formula> should satisfy fine-scale directional relationships as well, and not just simple binary relationships. We show that the CRF model can be easily updated to include additional features such as cells with known identities (landmark cells) and fluorescent spectral codes of cells. We demonstrate this by incorporating landmark cells and spectral information of cells in the model and show improvement in accuracy.</p><p>A critical component for the success of automated cell identification methods is data-driven atlas. Static atlases such as OpenWorm atlas provide a single observation of positional relationships among cells. For instance, if cell RMEL is to the left of cell AVAL in OpenWorm atlas, then the model assumes that RMEL is to the left of AVAL with 100% probability. In contrast, in observed experimental data RMEL may be observed to be left of AVAL with 80% probability (e.g. in 8 out of 10 annotated experimental datasets). Thus, data-driven atlases relax the hard-coded constraint of 100% probability imposed by static atlas and accounts for the statistics that is observed experimentally for all positional relationship features (<xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1</xref>). Note, data-driven atlas built in our framework is considerably different from those built by registration-based methods. While the latter atlases store probabilistic positions of cells, atlases built by our framework store only probabilistic pairwise positional relationship features among cells, thus more generalizable. We show that building such data-driven atlases is easy for our CRF model (Appendix 1â€“Extended methods S1.7). We demonstrate this by building several data-driven atlases from different data sources containing various features, showing considerable improvement in accuracy. Further, building data-driven atlases is computationally cheap in CRF_ID, requiring only simple averaging operations; thus, it is scalable to build atlases from large-scale annotated data that may become available in future.</p></sec><sec id="s2-2"><title>Computational workflow for automatic cell identification</title><p>Our annotation framework consists of four major steps (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; Appendix 1â€“Extended methods S1). First, cells are automatically detected in input image channels using a Gaussian Mixture-based segmentation method (see Materials and methods â€“ Whole-brain data analysis). Cells with known identities (landmarks cells) are also detected in this step and their identities are specified. We designed the framework to be flexible on several fronts: (1) easily using manual segmentations of image channels or segmenting on the run; (2) integrating landmark information from any number of image channels; (3) specifying identities of landmark cells on the run or from existing fully or partially annotated files generated with other tools such as Vaa3D (<xref ref-type="bibr" rid="bib53">Peng et al., 2010</xref>). In the second step, a head coordinate is generated by solving an optimization problem with considerations of the directional consistency of axes (see Appendix 1-Extended methods S1.3). With this coordinate system, we next define cell-specific features (unary potentials) and co-dependent features (pairwise potentials) in the data (<xref ref-type="fig" rid="fig1s2">Figure 1â€”figure supplement 2A,B</xref>). The base version of the model uses only pairwise relationship features for all pairs of cells, including binary positional relationships, angular relationship, and proximity relationship between cells in images (<xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1</xref>). However, additional unary features such as landmarks and color information can be easily added in the model. By encoding these features among all pairs of cells, our fully connected CRF model accounts for label dependencies between each cell pair to maximize accuracy. The atlas used for prediction may be a standard atlas such as the OpenWorm (<xref ref-type="bibr" rid="bib72">Szigeti et al., 2014</xref>) atlas or it can be easily built from fully or partially annotated datasets from various sources using the tools provided with our framework (see Appendix 1â€“Extended methods S1.7). In the third step, identities are automatically predicted for all cells by optimizing the CRF energy function consisting of unary and pairwise potentials, which in our formulation is equivalent to maximizing the intrinsic similarity between data and the atlas (see Appendix 1â€“Extended methods S1.4). Duplicate assignments are resolved by calculating a label-consistency score for each neuron, removing duplicate assignments with low scores (<xref ref-type="fig" rid="fig1s2">Figure 1â€”figure supplement 2C,D</xref>, see Appendix 1â€“Extended methods S1.5) and re-running the optimization. After the third step, the code outputs top predicted label for each cell. Next, an optional fourth step can be performed to account for missing neurons in image stack. In this step, full atlas is subsampled to remove fixed number of labels from atlas by either sampling uniformly or based on prior confidence values available on missing rate of labels in images (see Appendix 1â€“Extended methods S1.5). Subsampled atlas assumes that labels removed are missing from the image and thus ensures that those labels cannot be assigned to any cell in the image. The sampling procedure is repeated, and identities are predicted in each run. We perform these runs in parallel on computing clusters. Lastly, identities predicted across each run are pooled to generate top candidate identities for each cell (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="video" rid="fig1video1">Figure 1â€”video 1</xref>; see Appendix 1â€“Extended methods S1.6). Thus, there are two modes of running the framework â€“ single-run mode that outputs only top label for each cell and parallel-run mode that outputs multiple top candidate labels for each cell. We make the software suite freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/shiveshc/CRF_Cell_ID">https://github.com/shiveshc/CRF_Cell_ID</ext-link> (<xref ref-type="bibr" rid="bib11">Chaudhary, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f852385572fef1c1df5dd410e7a54fa07378e3af;origin=https://github.com/shiveshc/CRF_Cell_ID.git;visit=swh:1:snp:4cc3097dd2ace051a4a2404108355b9abbc551ed;anchor=swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526/">swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526</ext-link>).</p></sec><sec id="s2-3"><title>Prediction accuracy is bound by position and count noise in data</title><p>Given the broad utility of image annotation, we envision our workflow to apply to a variety of problems where experimental constraints and algorithm performance requirements may be diverse. For example, experimental data across different tasks inherently contains noise contributed by various sources in varying amounts that can affect annotation accuracy. These sources of noises include the following: (1) deviation between cell positions in images and positions in atlases, which is position noise, (2) smaller count of cells in images than number of cells in atlas due to missing cells in images, which is count noise, and (3) absence of cells with known identities, i.e. known landmarks. We set out to determine general principles of how these noises may affect cell identification accuracy across various tasks. We used two different kinds of data: synthetic data generated from OpenWorm 3D atlas (<xref ref-type="bibr" rid="bib72">Szigeti et al., 2014</xref>;Â <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1A,B</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2</xref>) and experimental data generated using NeuroPAL strains (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>), consisting of annotated ground-truth of nine animals withÂ ~100 uniquely identified neurons (<xref ref-type="fig" rid="fig2s3">Figure 2â€”figure supplement 3</xref>). While experimental data enables the assessment of prediction accuracy in real scenarios, synthetic data enable us to tune the amount of noise contributed from various sources and dissect their effects on accuracy independently.</p><p>To assess the effects of position noise and count noise on prediction accuracy, we simulated four scenarios using the synthetic data (<xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C</xref>). In the absence of any noise, relative positional relationship features predicted neuron identities with perfect accuracy (scenario one in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C</xref>), thus demonstrating the suitability of co-dependent features and CRF_ID framework for the annotation task. We found that both position noise and count noise affect accuracy significantly (<xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C,D</xref>) with position noise having a larger effect (compare scenarios 1â€“2 with 3â€“4 in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C</xref>). As mentioned before, count noise is primarily caused by inefficiencies of either the reporter used to label cells or inaccuracies of the cell detection algorithm used, thus leading to fewer cells detected in the images than cells in atlases. Results on both synthetic data and real data show that 10â€“15% improvement in prediction accuracy can be attained by simply improving reagents and eliminating count noise (<xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1D</xref>). Next, we tested the effect of landmarks (cells with known identities) on annotation accuracy (<xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1E</xref>). We hypothesized that landmarks will improve accuracy by acting as additional constraints on the optimization while the algorithm searches for the optimal arrangement of labels for non-landmark cells. Indeed, we found, in both experimental data and synthetic data, randomly chosen landmarks increased prediction accuracy byÂ ~10â€“15%. It is possible that strategic choices of landmarks could further improve accuracy.</p><p>Another advantage of simulations using synthetic data is that by quantifying accuracy across the application of extreme-case of empirically observed noises, they can be used to obtain expected accuracy bounds for real scenarios. We obtained such bounds (shown as gray regions in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1F</xref>) based on observed position noise in experimental data (<xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2</xref>). Notably, the prediction results for experimental data lay close to the estimated bounds using synthetic data (<xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1F</xref>). Together, good agreement between results obtained on synthetic and experimental data suggest that the general trends uncovered using synthetic data of how various noises affect accuracy are applicable to experimental data.</p><p>Next, with this knowledge, we tuned the features in the model, and we compared prediction accuracy for several combinations of positional relationship features. Among all co-dependent positional relationship features, the angular relationship feature by itself or when combined with PA, LR, and DV binary position relationship features performed best (<xref ref-type="fig" rid="fig2s4">Figure 2â€”figure supplement 4A</xref>). To account for missing cells, we developed a method that considers missing neurons as a latent state in the model (similar to hidden-state CRF <xref ref-type="bibr" rid="bib59">Quattoni et al., 2007</xref>) and predicts identities by marginalizing over latent states (see Appendix 1â€“Extended methods S1.6). Compared to the base case that assumes all cells are present in data, simulating missing neurons significantly increased the prediction accuracy (<xref ref-type="fig" rid="fig2s4">Figure 2â€”figure supplement 4B</xref>) on experimental data.</p></sec><sec id="s2-4"><title>Identity assignment using intrinsic features in CRF_ID outperforms other methods</title><p>We next characterized the performance of our CRF_ID framework by predicting the identities of cells in manually annotated ground-truth datasets (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). To specify prior knowledge, we built data-driven atlases combining positional information of cells from annotated ground-truth datasets and OpenWorm atlas (Appendix 1â€“Extended methods S1.7). To predict cell identities in each ground-truth dataset, separate leave-one-out atlases were built keeping the test dataset held out. Building such data-driven atlases for our framework is extremely computationally efficient, requiring simple averaging operations; thus, new atlases can be built from thousands of annotated images very quickly. With data-driven atlases (of only eight annotated set, one for each test dataset), 74% of cells were correctly identified by the top label prediction in the ground-truth data set, which exceeds the state of the art. Further, 88% and 94% of cells had true identities within the top 3 and the top 5 predicted labels, respectively (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Note that with using only positional relationship features in the data-driven atlas, this case is equivalent to predicting identities in experimental whole-brain datasets without color information. More importantly, automated annotation is unbiased because, in principle, the framework can combine manual annotations of cell identities of several users (possibly across labs) in the form of data-driven atlases and can predict identities such that positional relationships in the atlas are maximally preserved. Thus, automated annotation removes individual biases in annotating cells. Further, it greatly supports researchers with no prior experience.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>CRF_ID annotation framework outperforms other approaches.</title><p>(<bold>A</bold>) CRF_ID framework achieves high prediction accuracy (average 73.5% for top labels) using data-driven atlases without using color information. Results shown for whole-brain experimental ground truth data (nÂ =Â 9 animals). Prediction was performed using separateÂ leave-one-out data-driven atlases built for each animal dataset with test datasetÂ heldÂ out. Gray regions indicate bounds on prediction accuracy obtained using simulations on synthetic data (see <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1F</xref>). Experimental data comes from strain OH15495. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>B</bold>) Schematic highlighting key difference between registration-based methods and our CRF_ID framework.Â (<bold>C</bold>) Prediction accuracy comparison across methods for ground truth experimental data (nÂ =Â 9, *p&lt;0.05, Bonferroni paired comparison test) and synthetic data (nÂ =Â 190â€“200 runs for each method, ***p&lt;0.001, Bonferroni paired comparison test). OpenWorm atlas was used for predictions. Accuracy results shown for top predicted labels. Experimental data comes from strain OH15495. For synthetic data, random but realistic levels of position and count noise applied in each run. Gray regions indicate bounds on prediction accuracy obtained using simulations on synthetic data (see <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1F</xref>). Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>D</bold>) Comparison of methods across count noise levels (defined as percentage of cells in atlas that are missing from data) using synthetic data. (nÂ =Â 150â€“200 runs for Rel. Position for each noise level, n =Â ~1000 runs for Registration for each noise level, ***p&lt;0.001, Bonferroni paired comparison test). OpenWorm atlas was used for prediction. Accuracy results shown for top predicted labels. For a fixed count noise level, random cells were set as missing in each run. Markers and error bars indicate mean Â± standard deviation.Â (<bold>E</bold>) Comparison of methods across position noise levels using synthetic data. (nÂ =Â 190â€“200 runs for each method for each noise level, ***p&lt;0.001, Bonferroni paired comparison test). OpenWorm atlas was used for prediction. Accuracy results shown for top predicted labels. For a fixed position noise level, random position noise was applied to cells in each run. Different noise levels correspond to different variances of zero-mean gaussian noise added to positions of cells (see section MaterialsÂ andÂ methods â€“ Generating synthetic data for framework tuning and comparison against other methods). Noise levels 3 and 6 correspond to the lower bound and upper bound noise levels shown in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1F</xref>. Markers and error bars indicate mean Â± standard deviation.Â (<bold>F</bold>) Pairwise positional relationships among cells are more consistent with OpenWorm atlas even though the absolute positions of cells vary across worms. (Left) average deviation of angular relationship measured in ground truth data (nÂ =Â 9) from the angular relationship in static atlas. (Right) distribution of all deviations in left panel (total of 8516 relationships) is sparse and centered around 0 deviation, thus indicating angular relationships are consistent with atlas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 1.</label><caption><title>Performance characterization using synthetic data.</title><p>(<bold>A</bold>)Â Freely available open-source 3D atlas (OpenWorm atlas) was used to generate synthetic data. (<bold>B</bold>)Â Four scenarios were simulated using atlas and prediction accuracies were quantified.Â These scenarios include different perturbations observed in experimental data such as count noise (discrepancy between cells in images and atlas), and position noise (variability in cell positions compared to atlas). (<bold>C</bold>)Â Prediction accuracy of framework for four simulated scenarios. For the scenario 1, in which no position noise or count noise perturbation was applied to generate simulated data, CRF_ID framework predicted identities with 99.9% accuracy thus highlighting that identities can be predicted by using relative positional relationships only (without any information about absolute positions of cells) in CRF_ID framework. With application of position and count noise, prediction accuracy decreased (scenarios 2â€“4). Two levels of box plots for scenario 3 and scenario 4 show prediction results for lower bound and upper bound levels of position noise applied to cells (see <xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2</xref>). nÂ =Â 200â€“203 runs for each scenario and for each number of landmarks condition. Each run differed from another with respect to (1) random perturbations applied to positions of cells (2) random combination of landmarks selected in each run (3) random combination of cells set as missing. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively. (D)Scenarios 3 and 4 were simulated for experimental data by applying count noise to data and results were compared to results on synthetic data shown in panel C. Results show good match between synthetic data and experimental ground truth data. For synthetic data simulations, results are same as panel C for 0 landmarks condition and lower bound level of perturbation. Experimental data comes from OH15495 strain. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively. (<bold>E</bold>)Â Effect of number of landmarks on prediction accuracy using both experimental data and synthetic data show similar accuracy trends. Images of nine animals were used as experimental ground truth datasets (strain OH15495). For no landmarks condition, nÂ =Â 9. For non-zero landmarks conditions, nÂ =Â 450 runs across nine datasets were performed for each condition with randomly selected landmarks in each run. For synthetic data, nÂ =Â 200 runs were performed for each condition with randomly selected landmarks in each run. For no landmarks condition in synthetic data â€“ each run is different from another only with respect to random perturbations applied in each run. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively. (<bold>F</bold>)Â Accuracy bounds for Top, Top 3, and Top 5 labels were defined as average prediction accuracies achieved by applying lower bound and upper bound levels of position noise in synthetic data. These accuracy bounds are shown as gray regions in <xref ref-type="fig" rid="fig2">Figure 2</xref>. nÂ =Â 50 runs for both lower and upper bounds levels of noise. Accuracies obtained for top, top 3, and top 5 labels in synthetic data are similar to accuracies in experimental data (nÂ =Â 9). Experimental data comes from OH15495 strain. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 2.</label><caption><title>Method of applying position noise to the atlas to generate synthetic data.</title><p>(<bold>A</bold>, <bold>C</bold>, <bold>E</bold>) Variability in positions of cells were quantified in experimental data using landmark strains GT290 and GT298. Panels here show the variability of landmark cells along AP, LR, DV axes (nÂ =Â 31 animals). Top and bottom lines in box plot indicate 75th percentile and 25th percentile of data, middle (red) line indicates data median, and whiskers indicate range. (<bold>B</bold>, <bold>D</bold>, <bold>F</bold>) Position variability of cells was compared to the inter-cell distances among all cells in the head ganglion. Panels show distributions of inter-cell distances between all pairs of cells in the head along AP, LR, and DV axes. 25th and 75th percentiles of cell position variability were compared to inter-cell distances to define lower bound and upper bound levels of variability in cell positions. (<bold>G</bold>, <bold>H</bold>) To calculate perturbations to be applied to cells in atlas, inter-cell distances in the atlas were calculated as well. Subsequently, the relation between position variability and inter-cell distance calculated in experimental data was used to calculate appropriately scaled perturbations based on inter-cell distance in atlas. This was done to remove the effect of different spatial scales of cell positions in experimental data vs atlas. Scaled noise perturbations were applied to each cell position. Panels show original positions of cells (red) and positions after applying perturbation (blue) in atlas for the lower bound and upper bound level of perturbations. Two views of the atlas are shown.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 3.</label><caption><title>Details of manually annotated experimental ground-truth datasets.</title><p>(<bold>A</bold>)Â Number of cells manually annotated in each of anterior (anterior ganglion), middle (lateral, dorsal, and ventral ganglion) and posterior (retrovesicular ganglion) regions of head ganglion in two kinds of experimental ground-truth datasets: where animal is imaged in lateral orientation, that is LR axis is perpendicular to the image plane (left panel) and where animal is non-rigidly rotated about AP axis (right panel). See <xref ref-type="fig" rid="fig4s2">Figure 4â€”figure supplement 2</xref> for details on grouping of head ganglion in anterior, middle, and posterior regions. Data was collected using NeuroPAL strains (OH15495 and OH15500).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 4.</label><caption><title>Model tuning/characterization â€“ features selection and simulating missing cells.</title><p>(<bold>A</bold>) Feature selection in the model was performed by keeping various feature combinations in the model and assessing prediction accuracy. Left panel â€“ experimental data without using color information (nÂ =Â 9 animals for each condition), Middle panel â€“Â experimental data using color information (nÂ =Â 9 worms for each condition), Right panel â€“ Synthetic data generated from atlas (nÂ =Â 189â€“200 runs for each condition with random position and count noise perturbations applied in each run, see MaterialsÂ andÂ methods). Prediction accuracy across these datasets follow a similar trend for different feature combination. Overall, the angular relationship feature by itself or combined with binary positional relationship features performs best. Experimental data comes from OH15495 strain. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively. (<bold>B</bold>) Accounting for missing neurons improves prediction accuracy (nÂ =Â 9 animals, ***p&lt;0.001, Bonferroni paired comparison test). Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 5.</label><caption><title>CRF_ID framework with relative positional features outperforms registration method.</title><p>(<bold>A</bold>) Prediction accuracies achieved by Top, Top 3, and Top 5 labels predicted by three methods â€“ Registration, CRF_ID framework with Relative Position features and CRF_ID framework with combined features (see Appendix 1â€“Extended methods S2.1-2.3). Relative position features outperform the registration method in both experimental data (left panel) and synthetic data (right panel). For experimental data, nÂ =Â 9 worm datasets, strain OH15495. For synthetic data, nÂ =Â 200 runs for registration method, nÂ =Â 48â€“50 runs for Rel. Position method and combined method. Each run differed from the other in terms of position noise and count noise applied. *** denotes p&lt;0.001, Bonferroni paired comparison test. Part of data is re-plotted in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>B</bold>) Accuracy comparison across count noise levels (defined as percentage of cells in atlas that are missing from data) for various fixed position noise levels. (nÂ =Â 180â€“200 runs for Rel. Position for each count noise level for each condition, n =Â ~500 runs for Registration for each count noise level for each condition, ***p&lt;0.001, Bonferroni paired comparison test). Results for position noise level two are shown in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. For a fixed count noise level, random cells were removed in each run. Markers and error bars indicate mean Â± standard deviation.Â (<bold>C</bold>) Accuracy comparison across position noise levels for various fixed count noise levels. (nÂ =Â 180â€“200 runs for Rel. Position for each position noise level for each condition, n =Â ~500 runs for Registration for each position noise level for each condition, ***p&lt;0.001, Bonferroni paired comparison test). Different noise levels correspond to different variances of zero-mean gaussian noise added to positions of cells (see section MaterialsÂ andÂ methods â€“ Generating synthetic data for framework tuning and comparison against other methods). For a fixed position noise level, random noise was applied to cell positions in each run. Noise levels three corresponds to the lower bound noise level calculated in <xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2</xref>. Markers and error bars indicate mean Â± standard deviation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 6.</label><caption><title>Variability in absolute positions of cells and relative positional features in experimental data compared to the static atlas.</title><p>(<bold>A</bold>) DV view (top) and LR view showing positions of cells across ground truth data (nÂ =Â 9 worms, strain OH15495). Each point cloud of one color represents positions of a specific cell across datasets. Ellipsoid around the point clouds shows 50% confidence interval. Total 98 point-clouds for 98 uniquely identified cells across nine datasets shown here.Â (<bold>B</bold>) Distributions of deviations of absolute positions of cells in experimental data (n â€“ nine animals, strain OH15495) compared to corresponding positions in static OpenWorm atlas. Top, middle, and bottom panels show deviation along anterior-posterior, (AP), left-right (LR), and dorsal-ventral (DV) axes. To put the deviations into perspective, dotted red lines show the median distance of each cell to its 10th nearest neighbor. Thus, in experimental data, cell positions can be deviated from their atlas position by much more than their distances to their 10th nearest neighbor. As a result, registration method often creates mismatches.Â (<bold>C</bold>) Deviation of relative positional relationships among cells compared to the relationship in static atlas. Top panel shows a schematic of how these deviations were calculated. for example each positional relationship is represented as a matrix. Each element in the matrix records the positional relationship between pair of cells. for example in case of static atlas, if a column cell such as RMEL is anterior to a row cell such as AIZR, then the corresponding elements will denote 1 otherwise 0. Similarly, for experimental data (nÂ =Â 9 worms, same as panel A, strain OH15495), each element denotes the average number of times RMEL is observed anterior to AIZR in data. Hence the value can be between 0 and 1. Matrices are calculated in the same manner for other features. A simple subtraction of the two matrices records the deviation of the positional relationship feature. Below panels show deviations of AP, LR, DV, and angular relationships between static atlas and data. Closer to 0 values in PA and angular relationship indicates these relationships are most similar to static atlas. Gray cells in matrix denote relationships that could not be measured across nine datasets (when both cells were never annotated in same data).Â (<bold>D</bold>) Distributions of deviation of relative positional relationship features between static atlas and data. Total 8546 positional relationship features compared for 98 uniquely identified cell across data. Sparseness of the distributions indicate that the relative positional relationships are consistent with the atlas, although absolute positions of cells vary a lot (panel <bold>a, b</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 7.</label><caption><title>Comparison of optimization runtimes of CRF_ID framework with a registration method CPDÂ (<xref ref-type="bibr" rid="bib47">Myronenko and Song, 2010</xref>).</title><p>(<bold>A</bold>)Â Optimization runtimes of CRF method using Loopy Belief Propagation (LBP) as optimization method, and registration method CPD across different number of cells in data to be annotated. Synthetic data was used for simulations and annotation was performed using OpenWorm atlas with 206 head ganglion cells (nÂ =Â 10 runs). Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig2-figsupp7-v2.tif"/></fig></fig-group><p>We next compared our method against registration-based methods popular for automatic cell annotation (<xref ref-type="bibr" rid="bib39">Long et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Long et al., 2008</xref>; <xref ref-type="bibr" rid="bib1">Aerni et al., 2013</xref>; <xref ref-type="bibr" rid="bib76">Toyoshima, 2019</xref>; <xref ref-type="bibr" rid="bib64">Scholz, 2018</xref>) (see Appendixâ€“Â S1.9Â Registration methods do not consider intrinsic similarity features such as relative positional relationships and S2.1Â Registration). For fair comparison across methods, all methods used OpenWorm atlas as reference for prediction. The major difference between our framework and previous methods is the use of intrinsic similarity compared to extrinsic similarities in previous methods in the annotation task (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1</xref>). Remarkably, for both experimental and synthetic data, CRF_ID using relative positional features performs the best (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; <xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5A</xref>). Note that the decrease in accuracy compared to <xref ref-type="fig" rid="fig2">Figure 2A</xref> here is due to using static OpenWorm atlas, further highlighting the importance of building data-driven atlases. Notably, CRF_ID outperforms registration-based method across all levels of count noise and position noise in data (<xref ref-type="fig" rid="fig2">Figure 2D,E</xref>; <xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5B,C</xref>). The accuracy of registration-based methods falls rapidly with increasing count noise levels, whereas CRF_ID is highly robust, maintaining higher accuracy even when up to 75% of cells in atlas were missing from data. This has important practical implications as the amount of count noise observed in experimental data may vary significantly across reagents, imaging conditions etc. Further, neuron positions being highly variable across individual animals have been shown (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>), and confirmed by our datasets as wellÂ (<xref ref-type="fig" rid="fig2s6">Figure 2â€”figure supplement 6A</xref>). Because cell positions on average can deviate from their atlas position by more than the distance to their tenth nearest neighbor (<xref ref-type="fig" rid="fig2s6">Figure 2â€”figure supplement 6B</xref>), we expect that this variability introduces large matching errors in registration-based methods. In contrast, most pair-wise relationships are preserved despite the variability of absolute positions (<xref ref-type="fig" rid="fig2">Figure 2F</xref>; <xref ref-type="fig" rid="fig2s6">Figure 2â€”figure supplement 6C,D</xref>). Interestingly, a hybrid objective function that combines registration using absolute positions with relative position features in CRF_ID framework corrupts the annotation performance (<xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5A</xref>), likely due to competing effects in the objective function. This again highlights the fact that higher accuracy is achieved by positional relationship features in CRF_ID method.</p><p>Next, to compare the computational efficiency of CRF_ID framework with that of registration based methods, we compared the optimization step runtimes of the single-run mode of CRF_ID framework with that of a popular registration method (Coherent Point Drift <xref ref-type="bibr" rid="bib47">Myronenko and Song, 2010</xref>;Â <xref ref-type="fig" rid="fig2s7">Figure 2â€”figure supplement 7A</xref>). The computational speed of both methods scales with theÂ number of cells to be annotated in images and theÂ number of cells in the atlas. As expected, CRF_ID framework is computationally more expensive compared to CPD, because it optimizes both unary and pairwise potentials. Nonetheless, the optimization runtime of CRF_ID framework for multi-cell calcium imaging use-case (10â€“50 cells in image) is on the order of 0.1â€“10 s, on a desktop computer (see Materials and methods â€“ Runtime comparison), when full head ganglion atlas (206 cells) is used for annotation. We emphasize that using full head ganglion atlas for cell identity annotation in whole-brain imaging is important because without prior knowledge of which cells are missing in images, full atlas provides unbiased opportunity to cells in images to take any label from the atlas. In contrast, if only a partial atlas or partially annotated data set is used as atlas, the labels absent in atlas will never get assigned to any cell in images, thus potentially biasing the annotation. In practice, faster runtimes can be achieved in multi-cell calcium imaging and whole-brain imaging case with the use of smaller atlases based on prior knowledge of cells expected in strains. Further, the multiple-run mode of CRF_ID framework can be parallelized using multiple CPU workers. Thus, higher accuracy compared to registration based methods combined with reasonable speeds makes CRF_ID favorable for cell annotation tasks.</p></sec><sec id="s2-5"><title>Cell annotation in gene-expression pattern analysis</title><p>We next demonstrate the utility of our framework for gene-expression localization analyses, which is important for many problems, for example mapping the cellular atlas of neurotransmitters (<xref ref-type="bibr" rid="bib20">Gendrel et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">Pereira et al., 2015</xref>), receptors (<xref ref-type="bibr" rid="bib79">Vidal et al., 2018</xref>), and neuropeptides (<xref ref-type="bibr" rid="bib6">Bentley et al., 2016</xref>). Conventional methods, for exampleÂ screening a list of cell-specific marker lines that overlap in expression with the reporter, are laborious and scale poorly with the number of cells expressing the genes of interest and the number of new genes for which expression patterns are to be determined. Our cell annotation framework can considerably reduce manual efforts by generating a small list of candidate identities for each cell expressing the reporter. Subsequently, researchers can easily verify or prune the candidate list. To demonstrate this use case, we imaged a strain with multiple cells labeled with GFP and predicted candidate identities for each cell (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Determining cell identities in this case is difficult due to large count noise along with position noise: since the full list of labels in the atlas is much bigger than few cells in the reporter strain (equivalent to scenario four in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C</xref>). Thus, several degenerate (equally probable) solutions are possible. To avoid accuracy decrease in such cases, we directly predicted the candidate identities of all cells marked with pan-neuronal red fluorescent protein (RFP) using full whole-brain atlas and subsequently assessed the accuracy of only cells of interest, that is those marked with GFP. Our framework accurately generated a candidate list for cells across all datasets (nÂ =Â 21 animals); 85% of cells had true identities within the top five labels chosen by the framework. In comparison, the candidate list generated by the registration method achieved only 61% accuracy (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>CRF_ID framework predicts identities for gene expression pattern analyses.</title><p>(<bold>A</bold>) (Top) Schematic showing a fluorescent reporter strain with GFP expressed in cells for which names need to be determined. Since no candidate labels are known a priori neurons labels are predicted for all cells marked with pan-neuronally expressed RFP using full whole-brain atlas. (Bottom)Â A proxy strain AML5 [rab-3p(prom1)::2xNLS::TagRFP; odr-2b::GFP] with pan-neuronal RFP and 19 cells labeled with GFP was used to assess prediction accuracy.Â (B) CRF_ID framework with relative position features outperforms registration method (nÂ =Â 21 animals) (***p&lt;0.001, Bonferroni paired comparison test). Accuracy shown for top five labels predicted by both methods. Experimental data comes from strain AML5. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig3-v2.tif"/></fig></sec><sec id="s2-6"><title>Cell annotation in multi-cell functional imaging experiments</title><p>We next demonstrate the utility of our algorithm in another important application - annotating cell identities in multi-cell calcium functional imaging in vivo (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Automation in this case dramatically reduces labor associated with cell annotation for many time points, across trials, animals, and experiments. We used a strain carrying GFP in multiple cells as a proxy for GCaMP-labeled strains for illustration purposes (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Given the known candidate list of labels that can be assigned (i.e. no count noise), the configurational space is small, which makes the task easy (similar to scenario three in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C</xref>). Indeed, our annotation framework identified neurons with high accuracy (98%, nÂ =Â 35 animals). In comparison, the registration method predicted identities with lower accuracy (88%) even with the small label assignment space (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In reality, some neurons may be undetected in the data due to expression mosaicism or low-calcium transients thus adding count noise to data (equivalent to scenario 4 in <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1C</xref>). We thus simulated this case by randomly removing up to a third of total neurons from the images and predicting identities of remaining cells using the full label list (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4â€”figure supplement 1A</xref>). Even under these conditions, the accuracy of our method remains high (88%), significantly outperforming registration method (81%) (<xref ref-type="video" rid="fig4video1">Figure 4â€”video 1</xref>). In practice, the performance can be further compensated for by using multiple frames from each video, which we are not doing here in the mock experiment.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cell identity prediction in mock multi-cell calcium imaging experiments and landmark strain.</title><p>(<bold>A</bold>) (Top) schematic showing automatic identification of cells in multi-cell calcium imaging videos for high-throughput analysis. (Bottom) A mock strain with GFP-labeled cells was used as an illustration of GCaMP imaging. Only green channel of AML5 strain was used for this purpose.Â (<bold>B</bold>) CRF_ID framework outperforms registration method (nÂ =Â 35 animals, ***p&lt;0.001, Bonferroni paired comparison test). OpenWorm atlas was used for prediction. Accuracy results shown for top predicted labels. Experimental data comes from strain AML5 (only green channel used). Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>C</bold>) Prediction accuracy comparison for the case of missing cells in images (count noise). ***p&lt;0.001, Bonferroni paired comparison test. Total nÂ =Â 700 runs were performed across 35 animals for each method with 3 out 16 randomly selected cells removed in each run. For fair comparison, cells removed across methods were the same. OpenWorm atlas was used for prediction. Accuracy results shown for top predicted labels. Experimental data comes from strain AML5 (only green channel used). Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>D</bold>) Max-projection of 3D image stacks showing CyOFP labeled landmark cells in head ganglion (pseudo-colored as cyan): animals carrying [unc47p::NLS::CyOFP1::egl-13NLS] (GT296 strain) with nine landmarks (top), and animals carrying [unc-47p::NLS::CyOFP1::egl-13NLS; gcy-32p::NLS::CyOFP1::egl-13NLS] with 12 landmarks (bottom).Â (<bold>E</bold>) (Left) max-projection of a 3D image stack from whole-brain activity recording showing head ganglion cells and identities predicted by CRF_ID framework (Top labels). Animal is immobilized in a microfluidic device channel and IAA stimulus is applied to the nose tip. (Right) GCaMP6s activity traces extracted by tracking cells over time in the same 108 s recording and their corresponding identities. Blue shaded region shows IAA stimulation period. Experimental data comes from strain GT296.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4â€”figure supplement 1.</label><caption><title>Relative position features perform better than registration in handling missing cells in images.</title><p>(<bold>A</bold>)Â Comparison of prediction accuracies across three methods for different number of missing cells (out of total 16 cells) simulated in experimental data. Experimental data comes from AML5 strain (only GFP channel used). Results for 3 cells missing are shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. ***p&lt;0.001, **p&lt;0.01, all comparisons done with Bonferroni paired comparison test. nÂ =Â 700 runs were performed across data from 35 animals for each number of missing cells and each method. For fair comparison across methods, the same cells were considered missing for all methods for a fixed missing cell number. Each run differed from the other with respect to random subset of cells considered missing. Part of data is re-plotted in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4â€”figure supplement 2.</label><caption><title>Spatially distributed landmarks or landmarks in lateral ganglion perform best in supporting CRF_ID framework for predicting identities.</title><p>(<bold>A</bold>) Top panel â€“ Region-wise prediction accuracy achieved by our CRF_ID framework when landmarks were constrained to lie in specific regions of the head. nÂ =Â 200 runs when landmarks were constrained in anterior, middle and posterior regions, nÂ =Â 1000 runs when landmarks were spatially distributed throughout the head. All data is synthetic data. A random combination of 15 landmarks was selected in each run. Landmarks constrained in the anterior region perform badly in predicting identities of the posterior region, similarly landmarks constrained in the posterior region perform badly in predicting identities of the anterior region. Landmarks in the middle region or spatially distributed throughout the head show balanced accuracy for all regions. Bottom panel â€“ shows the grouping of head regions as anterior, middle, and posterior regions. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4â€”figure supplement 3.</label><caption><title>Microfluidic device used in chemical stimulation experiments and characterization.</title><p>(<bold>A</bold>) Schematic of the microfluidic device <xref ref-type="bibr" rid="bib13">Cho et al., 2020</xref> used in chemical stimulation experiments. The position of nematode in the imaging channel is shown. Temporally varying stimulus is applied to the nose tip of the nematode by switching between food/IAA and buffer streams.Â (<bold>B</bold>) Stimulus characterization was performed using FITC as stimulus and S-basal as buffer. Figures show a zoom-in of the T-junction in the device (where the nose tip of the nematode would be). Colored boxes show the regions used to characterize the stimulus profile.Â (C) Applied stimulus profile and measured stimulus profile for 5 s on and 5 s off stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig4-figsupp3-v2.tif"/></fig><media id="fig4video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-60321-fig4-video1.mp4"><label>Figure 4â€”video 1.</label><caption><title>Comparison between the CRF_ID framework and the registration method for predicting identities in case of missing cells.</title><p>Identities shown in red are incorrect predictions. Scale bar 5 Î¼m.</p></caption></media></fig-group><p>To further facilitate annotation accuracy, we explored the utility of landmarks with known identities. Landmarks can also help in establishing a coordinate system in images and guiding post-prediction correction. Because the combinatorial space of potential landmarks is very large (~10<sup>14</sup> for 10 landmarks out of ~200 cells in the head), we asked what properties landmarks should have. We found that landmarks distributed throughout the head or in lateral ganglion perform better in predicting identities of neurons in all regions of the brain (<xref ref-type="fig" rid="fig4s2">Figure 4â€”figure supplement 2</xref>; MaterialsÂ andÂ methods). As a test case, we developed strains with spatially distributed, sparse neuronal landmarks using CyOFP (see Material and methods - Construction of landmark strains), which by itself can assist researchers in manual cell identification tasks. When crossed with pan-neuronally expressing GCaMP/RFP reagents, the strains can be used for whole-brain imaging (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) by using only two channels. This has two advantages: CyOFP can be imaged 'for free' while imaging GCaMP and RFP simultaneously, thus the landmarks providing a concurrent reference in all frames; this strategy also leaves other channels open for optogenetic manipulations and voltage imaging (<xref ref-type="bibr" rid="bib57">Piatkevich et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Piatkevich et al., 2018</xref>).</p><p>We next tested this strategy in a simple whole-brain imaging experiment. Isoamyl alcohol (IAA) is a well-known component of the bacterial metabolites that <italic>C. elegans</italic> senses and responds to <xref ref-type="bibr" rid="bib10">Chalasani et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">L'Etoile and Bargmann, 2000</xref>; <xref ref-type="bibr" rid="bib5">Bargmann et al., 1993</xref>. We recorded neuronal responses to a step-change in IAA concentration using a microfluidic system (<xref ref-type="bibr" rid="bib13">Cho et al., 2020</xref>;Â <xref ref-type="fig" rid="fig4s3">Figure 4â€”figure supplement 3</xref>). We observed both odor-specific responses and spontaneous activities (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). More importantly, neurons with algorithm-assigned identities demonstrate expected behavior. For instance, we identified the sensory neuron AWC, and detected an off-response to IAA, consistent with known AWC behavior. In addition, the predicted interneurons (e.g. AVA, RIB, and AIB) also demonstrate previously known activity patterns (<xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>).</p><p>We also tested wormsâ€™ responses to periodic stimuli of a more complex and naturalistic input â€“ supernatant of bacterial culture (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="video" rid="fig5video1">Figure 5â€”video 1</xref>). A periodic input (5 s On and 5 s Off for eight cycles) entrains many neurons as expected, therefore allowing us to better separate the odor-elicited responses from spontaneous activities (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We generated the candidate identities for all recorded neurons (<xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1A</xref>). Notably, several highly entrained neurons were identified as sensory neurons known to respond to food stimuli (<xref ref-type="bibr" rid="bib37">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="bib81">Wakabayashi et al., 2009</xref>; <xref ref-type="bibr" rid="bib86">Zaslaver et al., 2015</xref>;Â <xref ref-type="fig" rid="fig5">Figure 5C</xref>), some of which responded to the onset of the stimuli and some to the withdrawal of the stimuli (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). The power spectrum of these neurons showed a strong frequency component at 0.1 Hz as expected (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>CRF_ID framework identifies neurons representing sensory and motor activities in whole-brain recording.</title><p>(<bold>A</bold>) GCaMP6s activity traces of 73 cells automatically tracked throughout a 278 s long whole-brain recording and the corresponding predicted identities (top labels). Periodic stimulus (5 sec-on â€“ 5 sec-off) of bacteria (<italic>E.Â coli</italic> OP50) supernatant was applied starting at 100 s (shaded blue regions). Experimental data comes from strain GT296.Â (<bold>B</bold>) Power spectrum of neuron activity traces during the stimulation period for all cells. Cells entrained by 0.1 Hz periodic stimulus show significant amplitude for 0.1 Hz frequency component (green).Â (<bold>C</bold>) Activity traces of cells entrained by periodic stimulus shown for the stimulation period. Blue shaded regions indicate stimulus ON, unshaded region indicate stimulus OFF. Identities predicted by the framework are labeled.Â (<bold>D</bold>) Average ON and OFF responses of cells entrained by periodic stimulus across trials. The black line indicates mean and gray shading indicates Â± s.e.m.Â (<bold>E</bold>) Average activities of neurons with significant non-zeros weights in the first three sparse principal components (SPCs). Activities within each component are stereotypical and different components show distinct temporal dynamics. Cells with positive weights (blue) and negative weights (red) in SPC2 and SPC3 showed anti-correlated activity. Out of the 67 non-stimulus-tuned cells, 19 had non-zero weights in SPC1, 16 cells had non-zero weights in SPC2, and 5 cells had non-zero weights in SPC3. SPC1, SPC2, and SPC3 weights of cells are shown in <xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1</xref>. Shading indicates mean Â± s.e.m of activity.Â (<bold>F</bold>) Velocity (motion/second) traces of cells along anterior-posterior (AP) axis (blue to red) show phase shift in velocity indicating motion in device shows signatures of wave propagation.Â (<bold>G</bold>) Cells with non-zero weights in SPC2 show high mutual information with worm velocity compared to cells grouped in other SPCs (*** denotes p&lt;0.001, Bonferroni paired comparison test). Median (red line), 25th and 75th percentiles (box) and range (whiskers). Dashed line indicates entropy of velocity (maximum limit of mutual information between velocity and any random variable). Velocity of cell indicated by the black arrow in panel H right was used for mutual information analysis.Â (<bold>H</bold>) Activity traces of 16 cells (with significant non-zero weights) in SPC2 and corresponding identities predicted by the framework. Red traces for cells with negative weights in SPC2, blue traces for cells with positive weights in SPC2. Worm motion/second shown on top. (Right) max projection of 3D image stack showing head ganglion neurons and cells with positive weights (blue) and negative weights (red) in SPC2. Motion/second of cell indicated with arrow is shown in left panel.Â (<bold>I</bold>) Cross-correlation analysis between velocity and cells with non-zero weights in SPC2 shows a strong correlation between neuron activities and velocity. In comparison, other cells show low correlation. Velocity of cell indicated by arrow in panel H right was used for cross-correlation analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5â€”figure supplement 1.</label><caption><title>Further analysis of data in periodic food stimulation and whole-brain imaging experiment.</title><p>(<bold>A</bold>) Identities (top labels) predicted by our CRF_ID framework overlaid on the image (max-projection of image stack shown). Data comes from strain GT296.Â (<bold>B</bold>) Cumulative variance captured by traditional principal components (PCs) and sparse PCs. Sparse PCs capture lower variance as a tradeoff for minimizing mixing of different temporal dynamics across components to improve the interpretability of each PC.Â (<bold>C</bold>) The weights of cells across first three sparse principal components (SPCs). Blue and red bars in SPC2 and SPC3 denote cells with significant non-zero weights in SPC2 and SPC3. Activities of these cells are shown in <xref ref-type="fig" rid="fig5">Figure 5E</xref>.Â (<bold>D</bold>) Left panels top and bottom â€“ Y and X displacement of randomly selected cells in the head ganglion, blue to red ordered from anterior to posterior. Right panels top and bottom show corresponding velocities along Y and X directions. Smooth displacement of cells and phase shift in peak velocities of cells along the AP axis show signatures of wave-propagation in partially immobilized worm in microfluidic device.Â (<bold>E</bold>) Mutual information between worm velocity and lagged activities of cells grouped as SPC2 positive (blue) and SPC2 negative (red). Positive lag of neuron activities at which mutual information is maximum indicates neuron activities precede velocity. Shading indicates mean Â± sem. The horizontal dotted line indicates entropy of velocity that is maximum mutual information any random variable can have with velocity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig5-figsupp1-v2.tif"/></fig><media id="fig5video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-60321-fig5-video1.mp4"><label>Figure 5â€”video 1.</label><caption><title>Whole-brain functional imaging with bacteria supernatant stimulation.</title><p>Circles indicate the tracking of two cells that show ON and OFF response to food stimulus. Scale bar 5 Âµm.</p></caption></media><media id="fig5video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-60321-fig5-video2.mp4"><label>Figure 5â€”video 2.</label><caption><title>Wave propagation in animal and correlation of neuron activities to worm motion.</title><p>Top-left panel shows tracking of cell along the anterior-posterior axis used to calculate the motion of worm. Scale bar 5 Î¼m. Bottom-left panel shows the velocity (px/s) of the cells. Top-right panel shows the velocity of one of the cells. Bottom-right panels show activity of two cells in SPC2 with negative (red) and positive (blue) weights in SPC2.</p></caption></media></fig-group><p>Next, to examine the latent dynamics in the whole-brain activities during the entire experiment, we used traditional Principal Component Analysis (PCA) and Sparse Principal Component Analysis (sPCA) (<xref ref-type="bibr" rid="bib87">Zou et al., 2006</xref>). The overall dynamics are low-dimensional with top three traditional PCs capturing 70% of the variance (<xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1B</xref>). In comparison, while the top 3 sparse PCs (SPCs) explain 43% of the variance in the data, they enable meaningful interpretation of the latent dynamics by eliminating mixing of activity profiles in PCs (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). SPC1 shows a systematic decline of the signals, presumably related to photobleaching of the fluorophores; both SPC2 and SPC3 illustrate spontaneous activities with different temporal dynamics. With automatic annotation, we were able to identify cell classes belonging to each SPC (<xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1C</xref>). We then analyzed the relationship between motion and neuron activities. In our microfluidic device, the animals are not fully immobilized. By tracking landmarks on the body; we observed propagating waves along the body (<xref ref-type="fig" rid="fig5">Figure 5F</xref>; <xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1D</xref>, <xref ref-type="video" rid="fig5video2">Figure 5â€”video 2</xref>). Interestingly, cells participating in SPC2 showed significantly higher mutual information with motion than any other component (<xref ref-type="fig" rid="fig5">Figure 5G</xref>). Examining the connection between activities of neurons that drive SPC2 and animal motion demonstrates that these neurons are indeed correlated or anti-correlated with the motion we detected (<xref ref-type="fig" rid="fig5">Figure 5H</xref>); notably, these neurons included several command interneurons such as AVA, RIM, and motor neurons such as VAÂ and DA (<xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>;Â <xref ref-type="fig" rid="fig5">Figure 5H</xref>). Cross-correlation analysis between motion and neuron activities showed that neurons are activated ahead of motion (<xref ref-type="fig" rid="fig5">Figure 5I</xref>); when aÂ lag is added to the neuron activities, the mutual information of SPC2 neurons with motion is maximum at the same delay observed in the cross-correlationÂ analysis (<xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1E</xref>). These experiments together demonstrate the power of the approach, which enabled previously difficult simultaneous analyses of several sensory, inter-, and motor neuronsâ€™ activities to natural food stimulus. Thus, automatic identity prediction enabled meaningful interpretation of the whole-brain data.</p></sec><sec id="s2-7"><title>CRF framework is broadly applicable to wider conditions</title><p>Another important advantage of the CRF_ID framework is its flexibility to incorporate additional information to improve the identification accuracy, by simply adding new terms in the objective function without disturbing the weights of existing features. Here we demonstrate this idea by using the recently developed NeuroPAL (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>) that provides a unique chromatic code to each neuron (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The chromatic code was included as a unary feature in the model (see Appendix 1â€“Extended methods S2.6). Using manually curated ground-truth data, we compared different methods. These methods included different orthogonal feature combinations, as used by previous approaches, thus providing insights into which features perform best in predicting cell identities (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, see Appendix 1â€“Extended methods S2). For fair comparison across methods, static OpenWorm atlas was used across all methods. For methods that use color information, we built data-driven color atlases (Appendix 1â€“Extended methods S2.4) using all datasets except the test dataset: leave-one-out color atlases. Unsurprisingly, registration performs poorly (with or without color information); color alone is not sufficient, and color combined with spatial features improves the accuracy (whether registration or relative position is used). Notably, the best performing model uses relative position features in combination with color and without registration term (<xref ref-type="fig" rid="fig6">Figure 6B</xref>; <xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1A</xref>), achieving 67.5% accuracy for the top-label prediction. Further, for 85.3% of the neurons, the true identity is within the top three labels.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Annotation framework is generalizable and compatible with different strains and imaging scenarios.</title><p>(<bold>A</bold>) A representative image (max-projection of 3D stack) of head ganglion neurons in NeuroPAL strain OH15495.Â (<bold>B</bold>) (Left) comparison of prediction accuracy for various methods that use different information. CRF_ID framework that combines relative position features along with color information performs best (nÂ =Â 9 animals, *p&lt;0.05, **p&lt;0.01,Â ***p&lt;0.001, Bonferroni paired comparison test). (Right) the best performing method predicts cell identities with high accuracy. OpenWorm static atlas was used for all methods. Color atlas was built using experimental data with test data held out. Ensemble of color atlases that combine two different color matching methods were used for prediction. Accuracy results shown for top predicted labels. Experimental data comes from strain OH15495.Â (<bold>C</bold>) (Left) annotation framework can easily incorporate information from annotated data in the form of data-driven atlas, which improves prediction accuracy (***p&lt;0.001, Bonferroni paired comparison test). Prediction was performed using leave-one-out data-driven atlases for both positional relationship features and color. Accuracy shown for top predicted labels. Ensemble of color atlases that combine two different color matching methods were used for prediction. (Right) accuracy achieved by top, top 3, and top 5 labels. Experimental data comes from strain OH15495. Top, middle, and bottom lines in box plot indicate 75th percentile, median and 25th percentile of data, respectively.Â (<bold>D</bold>) An example image of head ganglion neurons in NeuroPAL strain for rotated animal (nematode lying on DV axis). In contrast, animal lying on the LR axis is shown below. The locations of RMDVL/R, AVEL/R cells in the two images are highlighted for contrasts. Dashed ellipses indicate positions of cells in retrovesicular ganglion, showing that the rotated animal is not rigidly rotated. Experimental data comes from strain OH15495.Â (<bold>E</bold>) Top-label prediction accuracies for non-rigidly rotated animal. nÂ =Â 7 animals. Experimental data comes from strain OH15495 and OH15500. Prediction was performed using leave-one-out data-driven atlases for both positional relationship features and color. Accuracy shown for top predicted labels. Ensemble of color atlases that combine two different color matching methods were used for prediction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6â€”figure supplement 1.</label><caption><title>Additional results on prediction performance of CRF_ID method on NeuroPAL data: comparison against registration method and utility of ensemble of color atlases.</title><p>(<bold>A</bold>) Comparing accuracy of top 3 and top 5 identities predicted by different methods show CRF_ID framework with pairwise positional relationship features performs better than registration method (top three identities case). All methods used the same leave-one-out color atlas (see Appendix 1â€“Extended methods S2.3). Experimental data comes from OH15495 strain. nÂ =Â 9 animals. All comparisons performed with Bonferroni paired comparison test. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data respectively.Â (<bold>B</bold>) Prediction accuracy of CRF_ID framework on experimental datasets across different kinds of data-driven atlases. â€˜Allâ€™ atlas includes positional relationships and color information from all datasets including test dataset. For â€˜All color, leave-one-out pos.â€™ atlas, test dataset is held out from positional relationships atlas only. For â€˜All pos., leave-one-out-colorâ€™ atlas, test dataset is held out from color atlas only. For â€˜Both leave-one-outâ€™ atlas, test dataset is held out from both positional relationship and color atlases. Experimental data comes from OH15495 strain. nÂ =Â 9 animals. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>C</bold>) Effect of different color distribution alignment methods on prediction accuracy. â€˜Both leave-one-outâ€™ case is the baseline case that uses leave-one-out atlases for both positional relationships and color, and color atlases are built by simple aggregation of RGB values across datasets. â€˜normâ€™ indicates normalization of color channels, â€˜histmâ€™ indicates histogram matching of training datasets (images used to build atlas) to test dataset. â€˜colconstâ€™ indicates color invariant transformation applied to images. â€˜norm + histmâ€™ indicates normalization of color channels and then histogram matching of training images to test image. â€˜colconst + histmâ€™ indicates color invariant transformation and then histogram matching of training images to test image. â€˜colconst + norm + histmâ€™ indicates color normalization, subsequent color invariant transformation and finally histogram matching of training images to test image. â€˜hsv + histmâ€™ indicates using hsv color space instead of RGB color space and histogram matching. â€˜contrast and gammaâ€™ indicates contrast and gamma adjustment of image channels. â€˜contrast and gamma + histmâ€™ indicates contrast and gamma adjustment of image channels and subsequent histogram matching. â€˜contrast and gamma + colconst + histmâ€™ indicates contrast and gamma adjust of image channels, subsequent color invariant transformation and finally histogram matching. See Appendix 1â€“Extended methodsÂ S2.4 for more details on methods. Experimental data comes from OH15495 strain. nÂ =Â 9 animals. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>D</bold>) Comparison of prediction accuracy of CRF_ID framework across different kinds of data-driven atlases used for prediction. Test dataset was held out from data-driven atlases. â€˜Pos. atlas onlyâ€™ uses only positional relationship features atlas for prediction (these results are same as <xref ref-type="fig" rid="fig2">Figure 2A</xref>). â€˜Pos. and Col. atlasâ€™ uses positional relationship features and baseline color atlas (Appendix 1â€“Extended methods S2.4) built by simple aggregation of RGB values of cells in training data used to build atlas. â€˜Pos. and Col. ensemble atlasâ€™ uses ensemble of two color atlases for prediction along with positional relationship features atlas. In this case, color distributions in training images were aligned to test data using color invariants and histogram matching prior to building atlas (Appendix 1â€“Extended methods S2.4). (nÂ =Â 9 animals, Bonferroni paired comparison test). Experimental data comes from OH15495 and OH15000 strains. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.Â (<bold>E</bold>) Similar to panel B for animals non-rigidly rotated about AP axis (nÂ =Â 7 animals). Test dataset was held out from data-driven atlases. In this case, Positional relationship and color data-driven atlases were built using data from animals imaged in lateral orientation as well as rotated animals. Comparisons performed with Bonferroni paired comparison test. Experimental data comes from OH15495 and OH15000 strains. Top, middle, and bottom lines in box plot indicate 75th percentile, median, and 25th percentile of data, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6â€”figure supplement 2.</label><caption><title>Example annotations predicted by the CRF_ID framework for animals imaged lying on the LR axis.</title><p>Data comes from OH15495 strain.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6â€”figure supplement 3.</label><caption><title>Example annotations predicted by the CRF_ID framework for animals twisted about the anterior-posterior axis (note the anterior and lateral ganglions show clear left-right separation whereas retrovesicular ganglion instead of being in the middle is more toward one of the left or right sides).</title><p>Data comes from OH15495 and OH15500 strains.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-fig6-figsupp3-v2.tif"/></fig></fig-group><p>Next, to assess true potential of CRF_ID framework, instead of using OpenWorm atlas, we used data-driven positional relationship atlases, so that the predictions are now performed with data-driven atlases for both positional relationships and color. To test the generalizability of the method on unseen datasets, we compared the accuracy of CRF_ID framework across several kinds of data-driven atlases (<xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1B</xref>). These included the following: (1) positional relationship and color atlases, which include information from all datasets including test dataset, (2) color information comesÂ from all datasets and leave-one-out atlases for positional relationships builtÂ with test dataset held out, (3) positional relationship information, which comes from all datasets and leave-one-out color atlases, and (4) leave-one-out atlases for both positional relationships and color. The analysis revealed that accuracy falls more sharply with using color leave-one-out atlases compared to the leave-one-out positional relationship atlases. This implies that in the datasets used, positional relationship features are more consistent compared to color features. Thus, leave-one-out positional relationships atlases can represent positional relationships among cells in test datasets. Further, to assess the contribution of color information to prediction accuracy, we compared the accuracy of the case using both positional relationship and color leave-one-out atlases (<xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1B</xref> last column) to the case where predictions were performed using only leave-one-out positional relationship atlases shown earlier in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. We found that color contributed little to improving accuracy. This is because in the datasets used, the color variability in raw RGB values across animals is far greater than the position variability across animals; hence, the distribution of color features in the training data does not match the distribution of features in the test data. This could be due to inherent variations in fluorophore expressions across animals, or variations in imaging settings (i.e. exposure time of each channel, laser power etc.) across sessions. Thus, a naive approach of building color atlas by directly aggregating RGB values from training images contributed little to improving accuracy. The problem of mismatched feature distributions in test data compared to training data is commonly solved by domain adaptation methods in machine-learning community. We adopted a simple domain adaptation strategy for dealing with color discrepancies and developed a two-step method (Appendix 1â€“Extended methods S2.4). First, we aligned the distributions of RGB values in training datasets to the test dataset by several methods such as simple normalization of color channels, histogram matching color channels in training images to test data set, contrast and gamma adjustment of image channels, and transforming the color space of all images with color invariants (<xref ref-type="bibr" rid="bib18">Finlayson et al., 1998</xref>;Â <xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1C</xref>). Note that this alignment does not rely on cell identity information at all. These color alignment methods by themselves or in combination with other methods improved accuracy for some datasets but not all datasets. Second, we used an ensemble of leave-one-out color atlases for prediction, that is predictions were performed using multiple leave-one-out color atlases each built with a different color alignment technique. The ensemble, in comparison to single atlases, provides a combination of color features from aligned color distributions, thus improving accuracy. The two-step method improves accuracy by 6% over the naÃ¯ve approach (<xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1D</xref>). Overall, a significant improvement in the model accuracy was achieved by using data-driven atlas to account for biological variability in both the positional relationships and color (<xref ref-type="fig" rid="fig6">Figure 6C</xref>; <xref ref-type="fig" rid="fig6s2">Figure 6â€”figure supplement 2</xref>). Using the data-driven atlas, accuracy improved to 81% (top labels); more than 93% of the neurons have their true identities in the top three labels chosen by the model. We expect that more datasets for the atlas will continue to improve the accuracy.</p><p>Lastly, we show that our framework is equipped to work with realistic complex scenarios of animals imaged in different orientations, often not rigid rotations (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Identifying cells in these cases is challenging: manual annotation using the 2D-atlas (<xref ref-type="bibr" rid="bib2">Altun and Hall, 2009</xref>) is not possible since it lacks left-right information; further, due to low-z sampling of image stacks, segmented positions of cells along z-axis are noisier. These challenges can be addressed by using the data-driven atlas. We manually annotated data collected for animals imaged with varying degrees of non-rigid rotations and built data-driven atlases for positional relationships and color. Further, we combined rotated animalsâ€™ atlas with the previous atlas built from animals imaged in lateral orientation to build a super atlas. With the test data held out in atlases, the prediction accuracy of top labels was 48.8%, and the accuracy was 68.7% for top three labels (<xref ref-type="fig" rid="fig6">Figure 6E</xref> ),Â which are reasonable for practical purposes. In this case too, aligning the color distributions in atlas to the test data set and using ensemble of color atlases with different alignment techniques helped to significantly improve accuracy over the naÃ¯ve approach to build color atlases (<xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1E</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Annotating anatomical features and cellular identities in biological images are important tasks for many applications. Here, we demonstrated our CRF_ID framework is suitable for fluorescently labeled cells in 3D images for many applications. Using both ground-truth experimental data of whole-brain image stacks and synthetic data generated from an atlas, we showed that our framework is more accurate compared to existing approaches. We demonstrated using real examples how the pipeline can be used for analysis of gene expression pattern for instance, and for neuron identification from dense multi-cell or whole-brain imaging experiments. Further, our CRF_ID framework significantly speeds up the cell identification compared to manual labeling while reducing bias.</p><p>With the pipeline, we address several challenges. There is ample evidence that anatomy varies from individual to individual, and from condition to condition. This variability, or position noise, is a major source of roadblock in effectively applying previous methods to annotate the whole-brain recording data. Because our framework leverages intrinsic similarity (<xref ref-type="bibr" rid="bib7">Bronstein et al., 2007</xref>), it performs better than registration methods in handling position noise (<xref ref-type="fig" rid="fig2">Figure 2E</xref>; <xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5C</xref>). Further, CRF_ID formulation is more accurate in handling count noise that is cases of missing or undetectable cells in images (<xref ref-type="fig" rid="fig2">Figure 2D</xref>; <xref ref-type="fig" rid="fig4">Figure 4C</xref>; <xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5B</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4â€”figure supplement 1A</xref>), because the missing neurons do not upset the relationships among the detectable neurons in the CRF_ID formulation while missing neurons introduce large uncertainty in registration methods. Lastly, CRF_ID method predicts identities with sufficient accuracy for different postural orientations of the worms often seen in our microfluidic experiments. We expect that this superiority is maintained for any data that have relational information preserved, this is the case virtually in all biological samples where tissues are connected by matrix materials, such as in other whole-brain recordings or for registration of fixed tissues.</p><p>Building and using data-driven atlases in the pipeline is simple and yet highly effective. We expect that data from more animals, different orientations, age, and imaging techniques will further improve the generalizability. Since building such data-driven atlas for our framework requires only cheap mathematical operations (Appendix 1â€“Extended methods S1.7), incorporating more data is quite simple and easily scalable. In contrast, previous registration-based methods may require simultaneous or batch-wise registration of multiple images to one reference image; this would require solving multiple constrained regression problems on increasingly large data sets, thus rendering them computationally unscalable. Further, without systematic methodology of which image should be chosen as reference image, atlas gets biased toward the chosen reference image or by the order in which blocks of images are registered to the reference image. Tackling this challenge is an active field of research (<xref ref-type="bibr" rid="bib82">Wang et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Evangelidis et al., 2014</xref>). In comparison, in CRF method, atlas building is unbiased toward any image set because there is no concept of reference image. Additionally, atlas can be built from all images simultaneously because of the cheapness of mathematical operations.</p><p>Another major advantage of data-driven atlases in our framework is that the atlases can be built incrementally using partially or fully annotated datasets, for exampleÂ using lines that label partial and distinct subsets of cells. In comparison, registration-based methods cannot build atlas from lines that label distinct subset of cells. This is because registration-based methods build probabilistic spatial atlases by first establishing correspondence among cells in images and subsequently registering images. However, this is not possible if the cells in different images do not have any overlapping cells or have very few overlapping cells. In comparison, atlases built in CRF_ID framework store probabilistic positional relationship features among cells observed within each image. Hence, correspondence between images is not required. Thus, in principle, CRF_ID framework can combine manually annotated data across different lines, generated by different researchers (and across labs) in the form of data-driven atlases. Automated annotation using such atlases removes individual biases in annotating cells. Further, it greatly supports researchers with no prior experiences with cell identification. We expect that using our framework, large-scale atlases can be built in the future with community contributed data.</p><p>Finally, a distinction of CRF_ID framework is its ability to build and annotate with complete atlases covering all cells. This is made possible by the efficient utilization of data, even from strains with non-overlapping cells. Annotating against a complete atlas is crucial because commonly in practice, no prior information is available on exactly which cells are missing from the images before annotation. Registration-based or unary potential-based methods are limited in building atlas by the availability of overlapping strains. Thus, in these methods, cells that are missing in the atlas can never be assigned to cells in images; hence these methods do not perform completely unbiased annotation. In comparison, CRF_ID framework uses a complete atlas to assign any possible label in the atlas to cells in the images, thus performing unbiased annotation, resulting in better handling of count noise in images.</p><p>CRF framework offers the flexibility of combining arbitrary unary features with arbitrary pairwise features for cell annotation task. We demonstrate the utility of such flexibility by combining color information of cells in NeuroPAL strain with positional relationship features and show higher accuracy compared to other methods. Our experiments show that in order to be able to utilize color information of cells in NeuroPAL for automatic annotation of cell identities, color consistency across animals needs to be maintained, either experimentally or by <italic>post hoc</italic> corrections. Experimentally, consistent protocol/imaging settings across sessions should be maintained as much as possible. Even with consistent protocol, color variation may exist due to inherent differences across animals in relative expressions of fluorophores that define the chromatic code of cells. This can be tackled by (1) collecting large volume of data to capture each cellsâ€™ full RGB variations and (2) using computational domain adaptation techniques. More advancement in image color transfer and domain adaptation techniques will further improve accuracy in future.</p><p>While we only considered pairwise features in the current formulation, feature functions with arbitrary dependency can be included in the model that may further improve prediction accuracy (<xref ref-type="bibr" rid="bib28">Kohli et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Najafi et al., 2014</xref>). Advances in structured energy minimization field (<xref ref-type="bibr" rid="bib28">Kohli et al., 2009</xref>; <xref ref-type="bibr" rid="bib29">Komodakis and Paragios, 2009</xref>; <xref ref-type="bibr" rid="bib30">KrÃ¤henbÃ¼hl and Koltun, 2011</xref>) will enable tackling the increased complexity of combinatorial optimization in these cases. Our workflow borrows techniques developed in metric object/shape matching literature for annotation in biological images. Log-linear parameterization in our framework makes the model a member of the exponential families (<xref ref-type="bibr" rid="bib80">Wainwright and Jordan, 2007</xref>); thus, the objective function in our framework has interesting connections with max-entropy models and with the entropy-regularized optimal transport objective functions (<xref ref-type="bibr" rid="bib67">Solomon et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Nitzan et al., 2019</xref>). Therefore, improvements in computational speed can be achieved by borrowing fast optimization techniques for quadratic assignment problems developed in optimal transport literature. Advances in these fields will continue to improve the method development in image analysis.</p><p>We anticipate that by using our generalizable formulation, similar pipelines can be set up to annotate more image sets in other organisms and build atlases. Data in many anatomical annotation problems (e.g. brain atlas construction, registering images from different modalities, comparing animals or related species to one another for developmental studies) share a similar property, in that the anatomical features of interest maintain a cohesion from sample to sample. This underlining cohesion lends itself to the CRF framework. As we have shown, the pipeline is extremely flexible in incorporating new information. Thus, framework should be easily modifiable catering to the data demands in other organisms including features besides landmarks and spectral information such as cellular morphology and expected cellular activities (e.g. calcium transients). Because the only inputs to our framework are segmented anatomical regions in images and positional relationships among them, information already available in data across organisms (<xref ref-type="bibr" rid="bib61">Robie et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib12">Chen et al., 2019</xref>; <xref ref-type="bibr" rid="bib62">Ronneberger et al., 2012</xref>), the framework proposed here should be generally useful for many problems in model organisms such as <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib61">Robie et al., 2017</xref>; <xref ref-type="bibr" rid="bib77">Vaadia et al., 2019</xref>), zebrafish (<xref ref-type="bibr" rid="bib62">Ronneberger et al., 2012</xref>), mammalian brains (<xref ref-type="bibr" rid="bib26">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib12">Chen et al., 2019</xref>). Besides fluorescence, the pipeline should also be able to work with data from other modalities including EM, live imaging, and fluorescence imaging from cleared tissues.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Reagents</title><p>For all experiments, animals were cultured using standard techniques (<xref ref-type="bibr" rid="bib69">Stiernagle, 2006</xref>). A detailed list of strains used is provided below.</p><p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Name</th><th valign="top">Genotype</th><th valign="top">Experiments</th><th valign="top">Reference</th></tr></thead><tbody><tr><td valign="top">GT290</td><td valign="top">aEx18[unc-47p::NLS::CyOFP1::egl-13NLS]</td><td valign="top">Strain with nine neuronal landmarks in head</td><td valign="top">This work</td></tr><tr><td valign="top">GT298</td><td valign="top">aEx22[unc-47p::NLS::CyOFP1::egl-13NLS + gcy-32p::NLS::CyOFP1::egl-13NLS]</td><td valign="top">Strain with 12 neuronal landmarks in head</td><td valign="top">This work</td></tr><tr><td valign="top">AML32</td><td valign="top">wtfIs5 [rab-3p::NLS::GCaMP6s + rab-3p::NLS::tagRFP]</td><td valign="top">Strain used to make whole-brain imaging strain with CyOFP labeled landmarks GT296</td><td valign="top"><xref ref-type="bibr" rid="bib49">Nguyen et al., 2016</xref></td></tr><tr><td valign="top">AML70</td><td valign="top">wtfIs5 [rab-3p::NLS::GCaMP6s + rab-3p::NLS::tagRFP]; lite-1(ce314) X</td><td valign="top">Strain used to make whole-brain imaging strain with CyOFP labeled landmarks GT293</td><td valign="top"><xref ref-type="bibr" rid="bib64">Scholz, 2018</xref></td></tr><tr><td valign="top">KG1180</td><td valign="top">lite-1(ce314) X</td><td valign="top">Strain used to make whole-brain imaging strain with CyOFP labeled landmarks GT296</td><td valign="top">CGC</td></tr><tr><td valign="top">GT296</td><td valign="top">wtfIs5 [rab-3p::NLS::GCaMP6s + rab-3p::NLS::tagRFP]; aEx18[unc-47p::NLS::CyOFP1::egl-13NLS]; lite-1(ce314) X</td><td valign="top">Strain used for whole-brain functional imaging experiments (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>) and quantifying cell position variability.</td><td valign="top">This work</td></tr><tr><td valign="top">GT293</td><td valign="top">wtfIs5 [rab-3p::NLS::GCaMP6s + rab-3p::NLS::tagRFP]; lite-1(ce314) X; aEx22[unc-47p::NLS::CyOFP1::egl-13NLS + gcy-32p::NLS::CyOFP1::egl-13NLS]</td><td valign="top">Strain used for quantifying cell position variability.</td><td valign="top">This work</td></tr><tr><td valign="top">AML 5</td><td valign="top">otIs355 [rab-3p(prom1)::2xNLS::TagRFP] IV. kyIs51 [odr-2p::GFP + lin-15(+)]</td><td valign="top">Strain used for mock gene-expression pattern analysis and mock multi-cell calcium imaging experiments</td><td valign="top"><xref ref-type="bibr" rid="bib49">Nguyen et al., 2016</xref></td></tr><tr><td valign="top">OH15495</td><td valign="top">otIs696 [UPN::NLS::TagRFP-T + acr-5::NLS::mTagBFP2::H2B + flp-1::NLS::mTagBFP2::H2B + flp-6::NLS::mTagBFP2::H2B + flp-18::NLS::mTagBFP2::H2B + flp-19::NLS::mTagBFP2::H2B + flp-26::NLS::mTagBFP2::H2B + gcy-18::NLS::mTagBFP2::H2B + ggr-3::NLS::mTagBFP2::H2B + lim-4::NLS::mTagBFP2::H2B + pdfr-1::NLS::mTagBFP2::H2B + srab-20::NLS::mTagBFP2::H2B + unc-25::NLS::mTagBFP2::H2B + cho-1::NLS::CyOFP1::H2B + flp-13::NLS::CyOFP1::H2B + flp-20::NLS::CyOFP1::H2B + gcy-36::NLS::CyOFP1::H2B + gpa-1::NLS::CyOFP1::H2B + nlp-12::NLS::CyOFP1::H2B +nmr-1::NLS::CyOFP1::H2B + ocr-1::NLS::CyOFP1::H2B + osm-9::NLS::CyOFP1::H2B + srh-79::NLS::CyOFP1::H2B + sri-1::NLS::CyOFP1::H2B + srsx-3::NLS::CyOFP1::H2B + unc-8::NLS::CyOFP1::H2B + acr-2::NLS::mNeptune2.5 + ceh-2::NLS::mNeptune2.5 + dat-1::NLS::mNeptune2.5 + dhc-3::NLS::mNeptune2.5 + eat-4::NLS::mNeptune2.5 + flp-3::NLS::mNeptune2.5 + gcy-35::NLS::mNeptune2.5 + glr-1::NLS::mNeptune2.5 + gcy-21::NLS::CyOFP1::H2B::T2A::NLS::mTagBFP2::H2B + klp-6::NLS::mNeptune2.5::T2A::NLS::CyOFP1::H2B + lim-6::NLS::mNeptune2.5::T2A::NLS::CyOFP1::H2B + mbr-1::NLS::mNeptune2.5::T2A::NLS::mTagBFP2::H2B + mec-3::NLS::CyOFP1::H2B::T2A::NLS::mTagBFP2::H2B + odr-1::NLS::mNeptune2.5::T2A::NLS::mTagBFP2::H2B + srab-20::NLS::mNeptune2.5::T2A::NLS::mTagBFP2::H2B]</td><td valign="top">NeuroPAL strain demonstrating the ease of incorporating color information, and thus demonstrating generalizability</td><td valign="top"><xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref></td></tr><tr><td valign="top">OH15500</td><td valign="top">otIs672 [rab-3::NLS::GCaMP6s + arrd-4:NLS:::GCaMP6s]. otIs669 [UPN::NLS::TagRFP-T + acr-5::NLS::mTagBFP2::H2B + flp-1::NLS::mTagBFP2::H2B + flp-6::NLS::mTagBFP2::H2B + flp-18::NLS::mTagBFP2::H2B + flp-19::NLS::mTagBFP2::H2B + flp-26::NLS::mTagBFP2::H2B + gcy-18::NLS::mTagBFP2::H2B + ggr-3::NLS::mTagBFP2::H2B + lim-4::NLS::mTagBFP2::H2B + pdfr-1::NLS::mTagBFP2::H2B + srab-20::NLS::mTagBFP2::H2B + unc-25::NLS::mTagBFP2::H2B + cho-1::NLS::CyOFP1::H2B + flp-13::NLS::CyOFP1::H2B + flp-20::NLS::CyOFP1::H2B + gcy-36::NLS::CyOFP1::H2B + gpa-1::NLS::CyOFP1::H2B + nlp-12::NLS::CyOFP1::H2B + nmr-1::NLS::CyOFP1::H2B + ocr-1::NLS::CyOFP1::H2B + osm-9::NLS::CyOFP1::H2B + srh-79::NLS::CyOFP1::H2B + sri-1::NLS::CyOFP1::H2B + srsx-3::NLS::CyOFP1::H2B + unc-8::NLS::CyOFP1::H2B + acr-2::NLS::mNeptune2.5 + ceh-2::NLS::mNeptune2.5 + dat-1::NLS::mNeptune2.5 + dhc-3::NLS::mNeptune2.5 + eat-4::NLS::mNeptune2.5 + flp-3::NLS::mNeptune2.5 + gcy-35::NLS::mNeptune2.5 + glr-1::NLS::mNeptune2.5 + gcy-21::NLS::CyOFP1::H2B::T2A::NLS::mTagBFP2::H2B + klp-6::NLS::mNeptune2.5::T2A::NLS::CyOFP1::H2B + lim-6::NLS::mNeptune2.5::T2A::NLS::CyOFP1::H2B + mbr-1::NLS::mNeptune2.5::T2A::NLS::mTagBFP2::H2B + mec-3::NLS::CyOFP1::H2B::T2A::NLS::mTagBFP2::H2B + odr-1::NLS::mNeptune2.5::T2A::NLS::mTagBFP2::H2B + srab-20::NLS::mNeptune2.5::T2A::NLS::mTagBFP2::H2B] V</td><td valign="top">NeuroPAL strain demonstrating the ease of incorporating color information, and thus demonstrating generalizability</td><td valign="top"><xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref></td></tr></tbody></table></table-wrap></p></sec><sec id="s4-2"><title>Imaging</title><p>All imagings were performed using either a Perkin Elmer spinning disk confocal microscope (1.3 NA, 40x, oil objective) or Brucker Opterra II Swept field confocal microscope (0.75 NA, 40x, Plan Fluor air objective), with an EMCCD camera.</p><p>To acquire data used for framework validation and comparison against other methods (<xref ref-type="fig" rid="fig2">Figure 2</xref>), gene expression pattern analysis (<xref ref-type="fig" rid="fig3">Figure 3</xref>), multi-cell calcium imaging (<xref ref-type="fig" rid="fig4">Figure 4</xref>), imaging landmark strain (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and NeuroPAL imaging (<xref ref-type="fig" rid="fig6">Figure 6</xref>), animals were synchronized to L4 stage and were imaged in an array microfluidic device (<xref ref-type="bibr" rid="bib34">Lee et al., 2014</xref>). A single 3D stack was acquired with either 0.5 Âµm or 1 Î¼m spacing between z-planes and 10 ms exposure time (except for NeuroPAL strain where exposure times of different channels were chosen based on the guidelines provided in NeuroPAL manuals <xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>).</p><p>Whole-brain functional recording data while providing chemical stimulus were acquired using a microfluidic device designed for applying chemical stimulation (<xref ref-type="bibr" rid="bib13">Cho et al., 2020</xref>) to the nose-tip of the animal. Here, image stacks were acquired with 1 Î¼m spacing between z-planes and 10 ms exposure for each z-plane. This enabled recording videos at 1.1 volumes/s while imaging two channels simultaneously (GCaMP and RFP). Animals were synchronized to Day-1 adult stage.</p></sec><sec id="s4-3"><title>Generating synthetic data for framework tuning and comparison against other methods</title><p>Synthetic data was generated using the freely available 3D atlas at OpenWorm (<xref ref-type="bibr" rid="bib72">Szigeti et al., 2014</xref>). Atlas available at Worm Atlas (<xref ref-type="bibr" rid="bib2">Altun and Hall, 2009</xref>) was not used as it provides only a 2D view. To mimic the conditions encountered in experimental data, two noise perturbations were applied to the 3D atlas (<xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2</xref>). First, due to inherent biological variability, positions of cells observed in images do not exactly match the positions in atlas. Thus, position noise was applied to each cell in the atlas. This noise was sampled from a normal distribution with zero mean and fixed variance <inline-formula><mml:math id="inf59"><mml:mi mathvariant="normal">Î£</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote variances along <inline-formula><mml:math id="inf62"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf63"><mml:mi>y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mi>z</mml:mi></mml:math></inline-formula> image dimensions and <inline-formula><mml:math id="inf65"><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> denotes diagonalizing vector <inline-formula><mml:math id="inf66"><mml:mi mathvariant="normal">x</mml:mi></mml:math></inline-formula>. Hence, the position of <inline-formula><mml:math id="inf67"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>Â cell <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> in synthetic data was defined as <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>Ïµ</mml:mi><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>Ïµ</mml:mi><mml:mo>âˆ¼</mml:mo><mml:mtext>Â </mml:mtext><mml:mrow><mml:mi mathvariant="script">ğ’©</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Î£</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the position of the <inline-formula><mml:math id="inf71"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> cell in the atlas. To determine the variance <inline-formula><mml:math id="inf72"><mml:mi mathvariant="normal">Î£</mml:mi></mml:math></inline-formula>, we quantified the variance of cell positions observed in experimental data (<xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2A, C, E</xref>) using the strains GT293, GT295 with neuronal landmarks. We calculated the 25<sup>th</sup> percentile and 75<sup>th</sup> percentile of the variance across all cells across all animals (n = 31) to define the lower bound and upper bound position noise levels observed in experimental data. However, this variability cannot be directly applied to the atlas due to different spatial scales in images and atlas. Thus, we first normalized the observed variance of cell positions in images with inter-cell distances in images and then scaled it according to the inter-cell distances in atlas (<xref ref-type="fig" rid="fig2s2">Figure 2â€”figure supplement 2B,D,F,G,H</xref>) to define lower bound and upper bound noise to be applied to the atlas. More position noise levels such as those in <xref ref-type="fig" rid="fig2">Figure 2E</xref> and <xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5</xref> were set by varying <inline-formula><mml:math id="inf73"><mml:mi mathvariant="normal">Î£</mml:mi></mml:math></inline-formula> between zero and upper-bound noise level.</p><p>Second, although there are 195â€“200 neurons in head ganglion in <italic>C. elegans,</italic> only 100â€“130 cells were detected in most image stacks. Remaining cells are not detected either due to low-expression levels of fluorophores, variability in expression levels of genetic marker used to label cells (mosaicism, incomplete coverage etc.) or inability of segmentation methods to resolve densely packed cells. This increases the complexity of determining the labels of cells. To illustrateÂ this, matching 195 cells in an image to 195 cells in the atlas is easier as only one or very few possible configurations of label assignments exist that maximally preserves the positional relationships among cells. In contrast, in the case of matching 100 cells in an image to 195 cells in atlas, many possible labeling arrangements may exist that will equally preserve the positional relationships among cells. Thus, to simulate count noise in synthetic data, randomly selected cells in atlas were marked as missing and synthetic data was generated from the atlas with remaining cells. Hence, identities were predicted for remaining cells only in synthetic data using the full atlas. Number of cells set as missing was set by the count noise level parameter, defined as the fraction of total cells in the atlas that were set as missing. Since no prior information was available on which regions of the head ganglion had more cells missing, we selected the missing cells uniformly across brain regions.</p><p>Finally, bounds on prediction accuracy (shown as gray regions in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1</xref>) were obtained as the average prediction accuracy across runs obtained on synthetic data by applying lower bound and upper bound position noise.</p></sec><sec id="s4-4"><title>Generating ground-truth data for framework tuning and comparison against other methods</title><p>NeuroPAL reagents OH15495 and OH15500 were used to generate ground-truth data. 3D image stacks were acquired following the guidelines provided in NeuroPAL manual (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>). Identities were annotated in image stacks using the example annotations provided in NeuroPAL manual. Individual channel image stacks were read in MATLAB, gamma and contrast were adjusted for each channel individually so that the color of cells in the RGB image formed by combining the individual channels matched as much as possible (perceptually) with the colors of cells in NeuroPAL manuals. To annotate identities in the 3D stack, Vaa3D software was used (<xref ref-type="bibr" rid="bib53">Peng et al., 2010</xref>).</p></sec><sec id="s4-5"><title>Model comparison against previous methods</title><p>Detailed description of the methodology used for each method that our CRF_ID framework was compared against is provided in Appendix 1â€“Extended methods S2. Note, for fair comparisons, standard 3D OpenWorm atlas was used by all methods as the reference: either for defining positions of cells (used by registration methods) or for defining positional relationships among cells (used by the CRF_ID framework).</p></sec><sec id="s4-6"><title>Simulations for choosing landmark locations</title><p>Landmarks (cell with known identities) improve prediction accuracy by constraining the optimization problem as it forces the CRF_ID framework to choose optimal labels for all cells such that they preserve their positional relationships with the cells with fixed identities. However, choosing an optimal set of landmarks is difficult. This is because the combinatorial space of choosing landmarks is huge (~10<sup>14</sup> for 10 landmark cells out of 195 in head ganglion). Simulating each such combination and predicting identities is not computationally tractable. Thus, we asked which regions of the brain landmark cells should lie in. We divided the head ganglion region into three groups: anterior group consisting of anterior ganglion, middle group consisting of lateral, dorsal and ventral ganglion, and posterior group consisting of retrovesicular ganglion. Two hundred runs were performed for each group with 15 randomly selected landmarks in each run. We constrained the landmarks cells to lie in a specific group and assessed how well the landmarks in that group perform in predicting the identities of cells in other regions. Overall, landmarks in anterior and posterior groups performed badly in predicting identities of cells in posterior and anterior groups respectively. Landmarks in the middle group and landmarks spatially distributed throughout the head performed equally (<xref ref-type="fig" rid="fig4s2">Figure 4â€”figure supplement 2</xref>). We chose landmarks spatially distributed throughout the head due to practical advantages: spatially distributed landmarks can be easily identified manually in image stacks thus can be used as inputs to the CRF_ID framework. In contrast, cells in middle group are densely packed and may not be identified easily. We tested this using several reporter strains with GFP labeled cells. Further, landmarks should be reliably expressed across animals, should have known and verified expression patterns and should label neither too few cells (not useful) nor too many cells (difficult identification). Thus, we chose <italic>unc-47</italic> and <italic>gcy-32</italic> reporters for labeling landmarks.</p></sec><sec id="s4-7"><title>Construction of landmark strains</title><p>We constructed two different transgenic strains in which nine (GT290) and twelve (GT298) neurons, respectively, were labeled with the fluorescent protein CyOFP1 (<xref ref-type="bibr" rid="bib14">Chu et al., 2016</xref>). Due its long Stokes shift, CyOFP1 can be excited by the same laser line as GCaMP, while emitting red-shifted photons. This conveniently allows us to perform three-color imaging on our two-channel confocal microscope. We designed an optimized version of the CyOFP1 gene using the <italic>C. elegans</italic> Codon Adapter (<xref ref-type="bibr" rid="bib60">Redemann et al., 2011</xref>), which was then synthesized (Integrated DNA Technologies) and sub-cloned into a kanamycin-resistant vector to yield the pDSP11 plasmid. Our CyOFP1 construct contains two different nuclear localization sequences (NLS), SV40 NLS at the N-terminus and EGL-13 NLS at the C-terminus, a strategy which has been shown to more effective at trafficking recombinant proteins to the nucleus of worm cells (<xref ref-type="bibr" rid="bib40">Lyssenko et al., 2007</xref>). The nuclear localization of the CyOFP1 protein allows us to unambiguously identify labeled cells in the densely packed head ganglion of <italic>C. elegans</italic>.</p><p>We tested two different labeling strategies in our study. The first used the promoter of the <italic>unc-47</italic> gene to drive expression CyOFP1 in all 26 GABAergic neurons of the worm (<xref ref-type="bibr" rid="bib42">McIntire et al., 1997</xref>). As our study focused on the head ganglion, only the RMEL, RMER, RMEV, RMED, AVL, RIS, DD1, VD1, and VD2 neurons are labeled by this promoter in this region (Strain GT296, <xref ref-type="fig" rid="fig4">Figure 4D</xref> top panel). Our second strategy used the <italic>unc-47</italic> CyOFP1 construct in combination with a second driven by the promoter of the <italic>gcy-32</italic> gene, which is expressed in the AQR, PQR, and URX neurons (<xref ref-type="bibr" rid="bib85">Yu et al., 1997</xref>), to label twelve cells in the head ganglion (Strain GT293, <xref ref-type="fig" rid="fig4">Figure 4D</xref> bottom panel). The DNA sequence of each promoter was amplified from N2 (wild type) genomic DNA and incorporated into a NotI-digested linear pDSP11 vector backbone using the NEBuilder HiFi DNA Assembly master mix (New England Biolabs) to yield the vectors pSC1 and pSC2. The following primers were used to amplify the promoters: <italic>unc-47</italic> Forward 5â€™- <named-content content-type="sequence">cagttacgctggagtctgaggctcgtcctgaatgatatgcCTGCCAATTTGTCCTGTGAATCGT</named-content>-3â€™ and Reverse 5â€™- <named-content content-type="sequence">gcctctcccttggaaaccatCTGTAATGAAATAAATGTGACGCTGTCGT</named-content>, <italic>gcy-32</italic> Forward 5â€™- <named-content content-type="sequence">cagttacgctggagtctgaggctcgtcctgaatgatatgcTTGTATAGTGGGAAATACTGAAATATGAAACAAAAAATATAGCTG</named-content>-3â€™ and Reverse 5â€™- <named-content content-type="sequence">gcctctcccttggaaaccatTCTATAATACAATCGTGATCTTCGCTTCGG</named-content>-3â€™.</p><p>To make landmark strains pSC1 and pSC2 were injected into N2 strain to make GT290 and GT298. GT290 and GT298 strains can be crossed with any strain where cells need to be identified. Landmarks in these strains help in defining a coordinate system in head and also improve the accuracy of automatic annotation framework by constraining optimization problem. To make strain GT293 for whole-brain imaging experiments, AML70 was crossed with GT298; <italic>lite-1(ce314)</italic> was confirmed by sequencing. To make strain GT296 for whole-brain imaging experiments, AML32 was crossed with GT290 and subsequently crossed with KG1180, <italic>lite-1(ce314)</italic> was confirmed by sequencing.</p></sec><sec id="s4-8"><title>Whole-brain data analysis</title><p>All videos were processed using custom software in MATLAB for automatic segmentation and tracking of nuclei in whole-brain image stacks. Tracks for nuclei with minor tracking errors were corrected in post-processing steps. Tracks with large tracking errors were dropped from the data.</p><sec id="s4-8-1"><title>Segmentation</title><p>Neurons were automatically segmented in image stacks using a Gaussian Mixture model based segmentation technique. Briefly, intensity local maxima are detected in images to initialize the mixture components and subsequently a 3D gaussian mixture model is fitted to the intensity profiles in image stacks using Expectation-Maximization (EM) algorithm. The number of components in the model and the ellipsoidal shape of each component determines the number of nuclei segmented and their shapes.</p></sec><sec id="s4-8-2"><title>Tracking</title><p>Custom software was used for tracking cells. Briefly, segmented nuclei at each timepoint in image stacks are registered to a common reference frame and temporally nearby frames to produce globally and locally consistent matching. Based on these matchings, consistency constraints such as transitivity of matching were imposed in the post-processing step to further improve tracking accuracy. A custom MATLAB GUI was used to quickly manually inspect the accuracy of tracking. Tracks of cells with minor tracking errors were resolved using semi-automated method.</p></sec><sec id="s4-8-3"><title>Cell identification</title><p>Identities were predicted using the CRF_ID framework with positional features (Appendix 1â€“Extended methods S1) using the data-driven atlas. Landmarks cells with known identities were identified in the CyOFP channel and were provided as input to the framework to achieve higher accuracy.</p></sec><sec id="s4-8-4"><title>Identification of stimulus tuned neurons</title><p>To identify stimulus tuned neurons, the power spectrum of activities of all cells within the stimulus application window (100 s â€“ 180 s) was calculated using â€˜fftâ€™ function in MATLAB. Cells that showed significant power (&gt;0.08) at 0.1 Hz (due to 5 s on 5 s off stimulus, <xref ref-type="fig" rid="fig5">Figure 5</xref>) were selected. This criterion identified all cells except two with low response amplitude to the stimulus; however, the response could be manually seen in the video. Thus, these cells were manually selected.</p></sec><sec id="s4-8-5"><title>PCA and Sparse PCA</title><p>Principal Component analysis (PCA) of neuron activity time-series data was performed using in-built functions in MATLAB. Sparse Principal component analysis (SPCA) was performed using freely available MATLAB toolbox (<xref ref-type="bibr" rid="bib66">SjÃ¶strand et al., 2018</xref>).</p></sec><sec id="s4-8-6"><title>Neuron activities correlation to animal motion</title><p>To ascertain that the motion of the worm in device has signatures of wave-propagation in freely moving animals, we looked for phase shift in the velocity of the different regions of the animal in the device (similar to phase shift in curvature of body parts of animals seen in freely moving animals <xref ref-type="bibr" rid="bib68">Stephens et al., 2008</xref>). To calculate the velocity, displacement of randomly selected cells along the anterior-posterior axis of the animal was calculated (<xref ref-type="video" rid="fig5video2">Figure 5â€”video 2</xref>) based on the tracking of cells. Cell displacements were smoothed using Savitzky-Golay filter. Subsequently, velocity of each cell was calculated by differentiating the displacement of each cell.</p><p>Mutual information (MI) of the obtained velocity signal was calculated with (1) stimulus tuned neurons, (2) neurons with significant weights in sparse principal components 1â€“3, and (3) remaining cells. MI analysis requires estimating the joint probability density of velocity and neuron activity. We used the kernel density estimationÂ (KDE) method to do so. KDEÂ method used Gaussian kernel with bandwidth parameters (that specify the variance of gaussian kernel) set to [0.05, 0.05]. Cells grouped in SPC2 always had the largest mutual information with velocity regardless of the choice of the bandwidth parameter.</p></sec></sec><sec id="s4-9"><title>Runtime comparison</title><p>To compare optimization runtimes of CRF and registration-based method CPD (<xref ref-type="bibr" rid="bib47">Myronenko and Song, 2010</xref>), synthetic data was generated using OpenWorm atlas as described previously with randomly selected 10, 20, 50, and 130 cells. Annotation was performed using only positional relationship features. Full head ganglion OpenWorm atlas (206 cells) was used for annotation. Simulations were run on standard desktop computer (Intel Xeon CPU E5-1620 v4 @3.5 GHz, 32 GB RAM).</p></sec><sec id="s4-10"><title>Statistical analysis</title><p>Standard statistical tests were performed using Paired Comparisons App in OriginPro 2020. Details regarding the tests (sample size, significance, method) are reported in figure legends. Following asterisk symbols are used to denote significance level throughout the manuscript - * (p&lt;0.05), ** (p&lt;0.01), *** (p&lt;0.001). Significance level not indicated in figures implies not significantly different (n.s).</p></sec><sec id="s4-11"><title>Code and data availability</title><p>Code and data used in this study can be accessed at <ext-link ext-link-type="uri" xlink:href="https://github.com/shiveshc/CRF_Cell_ID.git">https://github.com/shiveshc/CRF_Cell_ID.git</ext-link>. This repository contains the following (1) All code and individual components necessary for using CRF_ID framework to annotate cells in new data, visualize results, and build new atlases based on annotated data (2) Code to reproduce results for comparison shown against other methods in this study, and (3) all raw datasets used in this study as well as human annotations created for those datasets except whole-brain imaging datasets.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors thank the Hobert Lab for providing NeuroPAL strains. The authors acknowledge the funding support of the U.S. NIH (R21DC015652, R01NS096581, NIH R01GM108962, and R01GM088333) and the U.S. NSF (1764406 and 1707401) to HL. Some nematode strains used in this work were provided by the Caenorhabditis Genetics Center (CGC), which is funded by the NIH (P40 OD010440), National Center for Research Resources and the International <italic>C. elegans</italic> Knockout Consortium. SC and HL designed the algorithm, experiments, and methods. SC collected ground-truth validation data. SC, SL collected whole-brain imaging data. SC, SL, YL, and DSP developed strain with neuronal landmarks; SC, SL, YL, and HL analyzed the data. SC and HL wrote the paper. The authors declare no competing interest. All raw datasets (except whole-brain imaging datasets) used in this study as well as ground-truth human annotations created for those datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/">https://github.com/</ext-link>shiveshc/CRF_Cell_ID/tree/master/Datasets.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-60321-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. Source data files are provided at <ext-link ext-link-type="uri" xlink:href="https://github.com/shiveshc/CRF_Cell_ID.git">https://github.com/shiveshc/CRF_Cell_ID.git</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526/">https://archive.softwareheritage.org/swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526/</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aerni</surname> <given-names>SJ</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Do</surname> <given-names>CB</given-names></name><name><surname>Gross</surname> <given-names>SS</given-names></name><name><surname>Nguyen</surname> <given-names>A</given-names></name><name><surname>Guo</surname> <given-names>SD</given-names></name><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Kim</surname> <given-names>SS</given-names></name><name><surname>Batzoglou</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Automated cellular annotation for high-resolution images of adult <italic>Caenorhabditis elegans</italic></article-title><source>Bioinformatics</source><volume>29</volume><fpage>i18</fpage><lpage>i26</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt223</pub-id><pub-id pub-id-type="pmid">23812982</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altun</surname> <given-names>ZF</given-names></name><name><surname>Hall</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Worm atlas</article-title><source>Wormatlas</source><volume>1</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.3908/wormatlas.1.14</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bakir</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Predicting Structured Data (Neural Information Processing</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname> <given-names>Z</given-names></name><name><surname>Murray</surname> <given-names>JI</given-names></name><name><surname>Boyle</surname> <given-names>T</given-names></name><name><surname>Ooi</surname> <given-names>SL</given-names></name><name><surname>Sandel</surname> <given-names>MJ</given-names></name><name><surname>Waterston</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Automated cell lineage tracing in <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>103</volume><fpage>2707</fpage><lpage>2712</lpage><pub-id pub-id-type="doi">10.1073/pnas.0511111103</pub-id><pub-id pub-id-type="pmid">16477039</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bargmann</surname> <given-names>CI</given-names></name><name><surname>Hartwieg</surname> <given-names>E</given-names></name><name><surname>Horvitz</surname> <given-names>HR</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Odorant-selective genes and neurons mediate olfaction in <italic>C. elegans</italic></article-title><source>Cell</source><volume>74</volume><fpage>515</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/0092-8674(93)80053-H</pub-id><pub-id pub-id-type="pmid">8348618</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentley</surname> <given-names>B</given-names></name><name><surname>Branicky</surname> <given-names>R</given-names></name><name><surname>Barnes</surname> <given-names>CL</given-names></name><name><surname>Chew</surname> <given-names>YL</given-names></name><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Bullmore</surname> <given-names>ET</given-names></name><name><surname>VÃ©rtes</surname> <given-names>PE</given-names></name><name><surname>Schafer</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The multilayer connectome of <italic>Caenorhabditis elegans</italic></article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005283</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005283</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bronstein</surname> <given-names>AM</given-names></name><name><surname>Bronstein</surname> <given-names>MM</given-names></name><name><surname>Kimmel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rock, paper, and scissors: extrinsic vs. intrinsic similarity of non-rigid shapes</article-title><conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.2007.4409076</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bronstein</surname> <given-names>AM</given-names></name><name><surname>Bronstein</surname> <given-names>MM</given-names></name><name><surname>Kimmel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Topology-Invariant similarity of nonrigid shapes</article-title><source>International Journal of Computer Vision</source><volume>81</volume><fpage>281</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1007/s11263-008-0172-2</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caelli</surname> <given-names>T</given-names></name><name><surname>Caetano</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Graphical models for graph matching: approximate models and optimal algorithms</article-title><source>Pattern Recognition Letters</source><volume>26</volume><fpage>339</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2004.10.022</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalasani</surname> <given-names>SH</given-names></name><name><surname>Chronis</surname> <given-names>N</given-names></name><name><surname>Tsunozaki</surname> <given-names>M</given-names></name><name><surname>Gray</surname> <given-names>JM</given-names></name><name><surname>Ramot</surname> <given-names>D</given-names></name><name><surname>Goodman</surname> <given-names>MB</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dissecting a circuit for olfactory behaviour in <italic>Caenorhabditis elegans</italic></article-title><source>Nature</source><volume>450</volume><fpage>63</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/nature06292</pub-id><pub-id pub-id-type="pmid">17972877</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chaudhary</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>CRF_Cell_ID</data-title><source>Software Heritage</source><version designator="swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526">swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f852385572fef1c1df5dd410e7a54fa07378e3af;origin=https://github.com/shiveshc/CRF_Cell_ID.git;visit=swh:1:snp:4cc3097dd2ace051a4a2404108355b9abbc551ed;anchor=swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526/">https://archive.softwareheritage.org/swh:1:dir:f852385572fef1c1df5dd410e7a54fa07378e3af;origin=https://github.com/shiveshc/CRF_Cell_ID.git;visit=swh:1:snp:4cc3097dd2ace051a4a2404108355b9abbc551ed;anchor=swh:1:rev:aeeeb3f98039f4b9100c72d63de25f73354ec526/</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>McElvain</surname> <given-names>LE</given-names></name><name><surname>Tolpygo</surname> <given-names>AS</given-names></name><name><surname>Ferrante</surname> <given-names>D</given-names></name><name><surname>Friedman</surname> <given-names>B</given-names></name><name><surname>Mitra</surname> <given-names>PP</given-names></name><name><surname>Karten</surname> <given-names>HJ</given-names></name><name><surname>Freund</surname> <given-names>Y</given-names></name><name><surname>Kleinfeld</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An active texture-based digital atlas enables automated mapping of structures and markers across brains</article-title><source>Nature Methods</source><volume>16</volume><fpage>341</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0328-8</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname> <given-names>Y</given-names></name><name><surname>Lee</surname> <given-names>SA</given-names></name><name><surname>Chew</surname> <given-names>YL</given-names></name><name><surname>Broderick</surname> <given-names>K</given-names></name><name><surname>Schafer</surname> <given-names>WR</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multimodal Stimulation in a Microfluidic Device Facilitates Studies of Interneurons in Sensory Integration in <italic>C. elegans</italic></article-title><source>Small</source><volume>16</volume><elocation-id>1905852</elocation-id><pub-id pub-id-type="doi">10.1002/smll.201905852</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname> <given-names>J</given-names></name><name><surname>Oh</surname> <given-names>Y</given-names></name><name><surname>Sens</surname> <given-names>A</given-names></name><name><surname>Ataie</surname> <given-names>N</given-names></name><name><surname>Dana</surname> <given-names>H</given-names></name><name><surname>Macklin</surname> <given-names>JJ</given-names></name><name><surname>Laviv</surname> <given-names>T</given-names></name><name><surname>Welf</surname> <given-names>ES</given-names></name><name><surname>Dean</surname> <given-names>KM</given-names></name><name><surname>Zhang</surname> <given-names>F</given-names></name><name><surname>Kim</surname> <given-names>BB</given-names></name><name><surname>Tang</surname> <given-names>CT</given-names></name><name><surname>Hu</surname> <given-names>M</given-names></name><name><surname>Baird</surname> <given-names>MA</given-names></name><name><surname>Davidson</surname> <given-names>MW</given-names></name><name><surname>Kay</surname> <given-names>MA</given-names></name><name><surname>Fiolka</surname> <given-names>R</given-names></name><name><surname>Yasuda</surname> <given-names>R</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name><name><surname>Ng</surname> <given-names>HL</given-names></name><name><surname>Lin</surname> <given-names>MZ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A bright cyan-excitable orange fluorescent protein facilitates dual-emission microscopy and enhances bioluminescence imaging in vivo</article-title><source>Nature Biotechnology</source><volume>34</volume><fpage>760</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1038/nbt.3550</pub-id><pub-id pub-id-type="pmid">27240196</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chui</surname> <given-names>H</given-names></name><name><surname>Rangarajan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A new point matching algorithm for non-rigid registration</article-title><source>Computer Vision and Image Understanding</source><volume>89</volume><fpage>114</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/S1077-3142(03)00009-2</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Danelljan</surname> <given-names>M</given-names></name><name><surname>Meneghetti</surname> <given-names>G</given-names></name><name><surname>Khan</surname> <given-names>FS</given-names></name><name><surname>Felsberg</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Probabilistic framework for Color-Based point set registration</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>1818</fpage><lpage>1826</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.201</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Evangelidis</surname> <given-names>GD</given-names></name><name><surname>Kounades-Bastian</surname> <given-names>D</given-names></name><name><surname>Horaud</surname> <given-names>R</given-names></name><name><surname>Psarakis</surname> <given-names>EZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>A generative model for the joint registration of multiple point sets</chapter-title><person-group person-group-type="editor"><name><surname>Fleet</surname> <given-names>D</given-names></name></person-group><source>Lecture Notes in Computer Science</source><publisher-name>Springer</publisher-name><fpage>109</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10584-0_8</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Finlayson</surname> <given-names>GD</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name><name><surname>Crowley</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>Comprehensive colour image normalization</chapter-title><person-group person-group-type="editor"><name><surname>Burkhardt</surname> <given-names>H</given-names></name></person-group><source>Lecture Notes in Computer Science</source><publisher-name>Springer</publisher-name><fpage>475</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1007/BFb0055685</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ge</surname> <given-names>S</given-names></name><name><surname>Fan</surname> <given-names>G</given-names></name><name><surname>Ding</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Non-rigid point set registration with global-local topology preservation</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</conf-name><fpage>245</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1109/CVPRW.2014.45</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gendrel</surname> <given-names>M</given-names></name><name><surname>Atlas</surname> <given-names>EG</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A cellular and regulatory map of the GABAergic nervous system of <italic>C. elegans</italic></article-title><source>eLife</source><volume>5</volume><elocation-id>e17686</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.17686</pub-id><pub-id pub-id-type="pmid">27740909</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>HyvÃ¤rinen</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Consistency of pseudolikelihood estimation of fully visible boltzmann machines</article-title><source>Neural Computation</source><volume>18</volume><fpage>2283</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.10.2283</pub-id><pub-id pub-id-type="pmid">16907626</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeda</surname> <given-names>S</given-names></name><name><surname>Tanaka</surname> <given-names>T</given-names></name><name><surname>Amari</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Stochastic reasoning, free energy, and information geometry</article-title><source>Neural Computation</source><volume>16</volume><fpage>1779</fpage><lpage>1810</lpage><pub-id pub-id-type="doi">10.1162/0899766041336477</pub-id><pub-id pub-id-type="pmid">15265322</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jian</surname> <given-names>B</given-names></name><name><surname>Vemuri</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A robust algorithm for point set registration using mixture of gaussians</article-title><conf-name>Proceedings IEEE International Conference on Computer Vision</conf-name><fpage>1246</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2005.17</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappes</surname> <given-names>JH</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name><name><surname>SchnÃ¶rr</surname> <given-names>C</given-names></name><name><surname>Nowozin</surname> <given-names>S</given-names></name><name><surname>Batra</surname> <given-names>D</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Kausler</surname> <given-names>BX</given-names></name><name><surname>KrÃ¶ger</surname> <given-names>T</given-names></name><name><surname>Lellmann</surname> <given-names>J</given-names></name><name><surname>Komodakis</surname> <given-names>N</given-names></name><name><surname>Savchynskyy</surname> <given-names>B</given-names></name><name><surname>Rother</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A comparative study of modern inference techniques for structured discrete energy minimization problems</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>155</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0809-x</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname> <given-names>S</given-names></name><name><surname>Kaplan</surname> <given-names>HS</given-names></name><name><surname>SchrÃ¶del</surname> <given-names>T</given-names></name><name><surname>Skora</surname> <given-names>S</given-names></name><name><surname>Lindsay</surname> <given-names>TH</given-names></name><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Lockery</surname> <given-names>S</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Global brain dynamics embed the motor command sequence of <italic>Caenorhabditis elegans</italic></article-title><source>Cell</source><volume>163</volume><fpage>656</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.09.034</pub-id><pub-id pub-id-type="pmid">26478179</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>Y</given-names></name><name><surname>Venkataraju</surname> <given-names>KU</given-names></name><name><surname>Pradhan</surname> <given-names>K</given-names></name><name><surname>Mende</surname> <given-names>C</given-names></name><name><surname>Taranda</surname> <given-names>J</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Arganda-Carreras</surname> <given-names>I</given-names></name><name><surname>Ng</surname> <given-names>L</given-names></name><name><surname>Hawrylycz</surname> <given-names>MJ</given-names></name><name><surname>Rockland</surname> <given-names>KS</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Osten</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping social behavior-induced brain activation at cellular resolution in the mouse</article-title><source>Cell Reports</source><volume>10</volume><fpage>292</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2014.12.014</pub-id><pub-id pub-id-type="pmid">25558063</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname> <given-names>D</given-names></name><name><surname>Brendel</surname> <given-names>W</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Feierstein</surname> <given-names>CE</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name><name><surname>Mainen</surname> <given-names>ZF</given-names></name><name><surname>Qi</surname> <given-names>XL</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Demixed principal component analysis of neural population data</article-title><source>eLife</source><volume>5</volume><elocation-id>e10989</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10989</pub-id><pub-id pub-id-type="pmid">27067378</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohli</surname> <given-names>P</given-names></name><name><surname>LadickÃ½</surname> <given-names>L</given-names></name><name><surname>Torr</surname> <given-names>PHS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Robust higher order potentials for enforcing label consistency</article-title><source>International Journal of Computer Vision</source><volume>82</volume><fpage>302</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1007/s11263-008-0202-0</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Komodakis</surname> <given-names>N</given-names></name><name><surname>Paragios</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Beyond pairwise energies: efficient optimization for higher-order mrfs</article-title><conf-name>2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009</conf-name><fpage>2985</fpage><lpage>2992</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206846</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>KrÃ¤henbÃ¼hl</surname> <given-names>P</given-names></name><name><surname>Koltun</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associate inc</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>The hungarian method for the assignment problem</article-title><source>Naval Research Logistics Quarterly</source><volume>2</volume><fpage>83</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1002/nav.3800020109</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L'Etoile</surname> <given-names>ND</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Olfaction and odor discrimination are mediated by the <italic>C. elegans</italic> guanylyl cyclase ODR-1</article-title><source>Neuron</source><volume>25</volume><fpage>575</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81061-2</pub-id><pub-id pub-id-type="pmid">10774726</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lafferty</surname> <given-names>J</given-names></name><name><surname>McCallum</surname> <given-names>A</given-names></name><name><surname>Pereira</surname> <given-names>FCN</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Conditional random fields: probabilistic models for segmenting and labeling sequence data</article-title><conf-name>ICMLâ€™ 01 Proc. Eighteenth Int. Conf. Mach. Learn</conf-name><fpage>282</fpage><lpage>289</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Kim</surname> <given-names>SA</given-names></name><name><surname>Coakley</surname> <given-names>S</given-names></name><name><surname>Mugno</surname> <given-names>P</given-names></name><name><surname>Hammarlund</surname> <given-names>M</given-names></name><name><surname>Hilliard</surname> <given-names>MA</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A multi-channel device for high-density target-selective stimulation and long-term monitoring of cells and subcellular features in <italic>C. elegans</italic></article-title><source>Lab Chip</source><volume>14</volume><fpage>4513</fpage><lpage>4522</lpage><pub-id pub-id-type="doi">10.1039/C4LC00789A</pub-id><pub-id pub-id-type="pmid">25257026</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Leordeanu</surname> <given-names>M</given-names></name><name><surname>Hebert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A spectral technique for correspondence problems using pairwise constraints</article-title><conf-name>Proceedings of the IEEE International Conference on Computer Vision II</conf-name><fpage>1482</fpage><lpage>1489</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2005.20</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Leordeanu</surname> <given-names>M</given-names></name><name><surname>Hebert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unsupervised learning for graph matching</article-title><conf-name>IEEE Conf. Comput. Vis. Pattern Recognit</conf-name><fpage>864</fpage><lpage>871</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206533</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>H</given-names></name><name><surname>Qin</surname> <given-names>LW</given-names></name><name><surname>Li</surname> <given-names>R</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Al-Sheikh</surname> <given-names>U</given-names></name><name><surname>Wu</surname> <given-names>ZX</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reciprocal modulation of 5-HT and octopamine regulates pumping via feedforward and feedback circuits in <italic>C. elegans</italic></article-title><source>PNAS</source><volume>116</volume><fpage>7107</fpage><lpage>7112</lpage><pub-id pub-id-type="doi">10.1073/pnas.1819261116</pub-id><pub-id pub-id-type="pmid">30872487</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Myers</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><chapter-title>Automatic Recognition of Cells (ARC) for 3D images of <italic>C. elegans</italic></chapter-title><person-group person-group-type="editor"><name><surname>Vingron</surname> <given-names>M</given-names></name><name><surname>Wong</surname> <given-names>L</given-names></name></person-group><source>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</source><publisher-name>Springer</publisher-name><fpage>128</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-78839-3_12</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>SK</given-names></name><name><surname>Myers</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A 3D digital atlas of <italic>C. elegans</italic> and its application to single-cell analyses</article-title><source>Nature Methods</source><volume>6</volume><fpage>667</fpage><lpage>672</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1366</pub-id><pub-id pub-id-type="pmid">19684595</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyssenko</surname> <given-names>NN</given-names></name><name><surname>Hanna-Rose</surname> <given-names>W</given-names></name><name><surname>Schlegel</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cognate putative nuclear localization signal effects strong nuclear localization of a GFP reporter and facilitates gene expression studies in <italic>Caenorhabditis elegans</italic></article-title><source>BioTechniques</source><volume>43</volume><fpage>596</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.2144/000112615</pub-id><pub-id pub-id-type="pmid">18072588</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Robust L2E estimation of transformation for non-rigid registration</article-title><conf-name>IEEE Transactions on Signal Processing : A Publication of the IEEE Signal Processing Society</conf-name><fpage>1115</fpage><lpage>1129</lpage><pub-id pub-id-type="doi">10.1109/TSP.2014.2388434</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McIntire</surname> <given-names>SL</given-names></name><name><surname>Reimer</surname> <given-names>RJ</given-names></name><name><surname>Schuske</surname> <given-names>K</given-names></name><name><surname>Edwards</surname> <given-names>RH</given-names></name><name><surname>Jorgensen</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Identification and characterization of the vesicular GABA transporter</article-title><source>Nature</source><volume>389</volume><fpage>870</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1038/39908</pub-id><pub-id pub-id-type="pmid">9349821</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MÃ©moli</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Gromovâ€“Wasserstein Distances and the Metric Approach to Object Matching</article-title><source>Foundations of Computational Mathematics</source><volume>11</volume><fpage>417</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1007/s10208-011-9093-5</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>KP</given-names></name><name><surname>Weiss</surname> <given-names>Y</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Loopy belief propagation for approximate inference: an empirical study</article-title><conf-name>UAI'99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</conf-name><fpage>467</fpage><lpage>475</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Automated analysis of embryonic gene expression with cellular resolution in <italic>C. elegans</italic></article-title><source>Nat Methods</source><volume>5</volume><fpage>703</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1228</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Myronenko</surname> <given-names>A</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Point-Set registration: coherent point drift</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1009</fpage><lpage>1016</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myronenko</surname> <given-names>A</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Point set registration: coherent point drift</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>32</volume><fpage>2262</fpage><lpage>2275</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.46</pub-id><pub-id pub-id-type="pmid">20975122</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Najafi</surname> <given-names>M</given-names></name><name><surname>Taghavi Namin</surname> <given-names>S</given-names></name><name><surname>Salzmann</surname> <given-names>M</given-names></name><name><surname>Petersson</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Non-associative higher-order markov networks for point cloud classification</chapter-title><person-group person-group-type="editor"><name><surname>Fleet</surname> <given-names>D</given-names></name></person-group><source>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</source><publisher-name>Springer</publisher-name><fpage>500</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10602-1_33</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Shipley</surname> <given-names>FB</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Liu</surname> <given-names>M</given-names></name><name><surname>Setru</surname> <given-names>SU</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whole-brain calcium imaging with cellular resolution in freely behaving <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1074</fpage><lpage>E1081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507110112</pub-id><pub-id pub-id-type="pmid">26712014</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitzan</surname> <given-names>M</given-names></name><name><surname>Karaiskos</surname> <given-names>N</given-names></name><name><surname>Friedman</surname> <given-names>N</given-names></name><name><surname>Rajewsky</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Gene expression cartography</article-title><source>Nature</source><volume>576</volume><fpage>132</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1773-3</pub-id><pub-id pub-id-type="pmid">31748748</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowozin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Structured learning and prediction in computer vision</article-title><source>Foundations and Trends in Computer Graphics and Vision</source><volume>6</volume><fpage>185</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1561/0600000033</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Panaganti</surname> <given-names>V</given-names></name><name><surname>Aravind</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Robust nonrigid point set registration using graph-laplacian regularization</article-title><conf-name>Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015</conf-name><fpage>1137</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1109/WACV.2015.156</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Ruan</surname> <given-names>Z</given-names></name><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Simpson</surname> <given-names>JH</given-names></name><name><surname>Myers</surname> <given-names>EW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets</article-title><source>Nature Biotechnology</source><volume>28</volume><fpage>348</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1038/nbt.1612</pub-id><pub-id pub-id-type="pmid">20231818</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>L</given-names></name><name><surname>Kratsios</surname> <given-names>P</given-names></name><name><surname>Serrano-Saiz</surname> <given-names>E</given-names></name><name><surname>Sheftel</surname> <given-names>H</given-names></name><name><surname>Mayo</surname> <given-names>AE</given-names></name><name><surname>Hall</surname> <given-names>DH</given-names></name><name><surname>White</surname> <given-names>JG</given-names></name><name><surname>LeBoeuf</surname> <given-names>B</given-names></name><name><surname>Garcia</surname> <given-names>LR</given-names></name><name><surname>Alon</surname> <given-names>U</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A cellular and regulatory map of the cholinergic nervous system of <italic>C. elegans</italic></article-title><source>eLife</source><volume>4</volume><elocation-id>e12432</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12432</pub-id><pub-id pub-id-type="pmid">26705699</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Peyre</surname> <given-names>G</given-names></name><name><surname>Cuturi</surname> <given-names>M</given-names></name><name><surname>Solomon</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Gromov-wasserstein averaging of kernel and distance matrices</article-title><conf-name>33rd International Conference on Machine Learning, ICML</conf-name><fpage>3927</fpage><lpage>3935</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piatkevich</surname> <given-names>KD</given-names></name><name><surname>Jung</surname> <given-names>EE</given-names></name><name><surname>Straub</surname> <given-names>C</given-names></name><name><surname>Linghu</surname> <given-names>C</given-names></name><name><surname>Park</surname> <given-names>D</given-names></name><name><surname>Suk</surname> <given-names>HJ</given-names></name><name><surname>Hochbaum</surname> <given-names>DR</given-names></name><name><surname>Goodwin</surname> <given-names>D</given-names></name><name><surname>Pnevmatikakis</surname> <given-names>E</given-names></name><name><surname>Pak</surname> <given-names>N</given-names></name><name><surname>Kawashima</surname> <given-names>T</given-names></name><name><surname>Yang</surname> <given-names>CT</given-names></name><name><surname>Rhoades</surname> <given-names>JL</given-names></name><name><surname>Shemesh</surname> <given-names>O</given-names></name><name><surname>Asano</surname> <given-names>S</given-names></name><name><surname>Yoon</surname> <given-names>YG</given-names></name><name><surname>Freifeld</surname> <given-names>L</given-names></name><name><surname>Saulnier</surname> <given-names>JL</given-names></name><name><surname>Riegler</surname> <given-names>C</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Hughes</surname> <given-names>T</given-names></name><name><surname>Drobizhev</surname> <given-names>M</given-names></name><name><surname>Szabo</surname> <given-names>B</given-names></name><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Flavell</surname> <given-names>SW</given-names></name><name><surname>Sabatini</surname> <given-names>BL</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A robotic multidimensional directed evolution approach applied to fluorescent voltage reporters</article-title><source>Nature Chemical Biology</source><volume>14</volume><fpage>352</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1038/s41589-018-0004-9</pub-id><pub-id pub-id-type="pmid">29483642</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piatkevich</surname> <given-names>KD</given-names></name><name><surname>Bensussen</surname> <given-names>S</given-names></name><name><surname>Tseng</surname> <given-names>HA</given-names></name><name><surname>Shroff</surname> <given-names>SN</given-names></name><name><surname>Lopez-Huerta</surname> <given-names>VG</given-names></name><name><surname>Park</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>EE</given-names></name><name><surname>Shemesh</surname> <given-names>OA</given-names></name><name><surname>Straub</surname> <given-names>C</given-names></name><name><surname>Gritton</surname> <given-names>HJ</given-names></name><name><surname>Romano</surname> <given-names>MF</given-names></name><name><surname>Costa</surname> <given-names>E</given-names></name><name><surname>Sabatini</surname> <given-names>BL</given-names></name><name><surname>Fu</surname> <given-names>Z</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Han</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Population imaging of neural activity in awake behaving mice</article-title><source>Nature</source><volume>574</volume><fpage>413</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1641-1</pub-id><pub-id pub-id-type="pmid">31597963</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qu</surname> <given-names>L</given-names></name><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Myers</surname> <given-names>E</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Simultaneous recognition and segmentation of cells: application in <italic>C. elegans</italic></article-title><source>Bioinformatics</source><volume>27</volume><fpage>2895</fpage><lpage>2902</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btr480</pub-id><pub-id pub-id-type="pmid">21849395</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quattoni</surname> <given-names>A</given-names></name><name><surname>Wang</surname> <given-names>S</given-names></name><name><surname>Morency</surname> <given-names>LP</given-names></name><name><surname>Collins</surname> <given-names>M</given-names></name><name><surname>Darrell</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hidden conditional random fields</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>29</volume><fpage>1848</fpage><lpage>1852</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.1124</pub-id><pub-id pub-id-type="pmid">17699927</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redemann</surname> <given-names>S</given-names></name><name><surname>Schloissnig</surname> <given-names>S</given-names></name><name><surname>Ernst</surname> <given-names>S</given-names></name><name><surname>Pozniakowsky</surname> <given-names>A</given-names></name><name><surname>Ayloo</surname> <given-names>S</given-names></name><name><surname>Hyman</surname> <given-names>AA</given-names></name><name><surname>Bringmann</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Codon adaptation-based control of protein expression in <italic>C. elegans</italic></article-title><source>Nature Methods</source><volume>8</volume><fpage>250</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1565</pub-id><pub-id pub-id-type="pmid">21278743</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robie</surname> <given-names>AA</given-names></name><name><surname>Hirokawa</surname> <given-names>J</given-names></name><name><surname>Edwards</surname> <given-names>AW</given-names></name><name><surname>Umayam</surname> <given-names>LA</given-names></name><name><surname>Lee</surname> <given-names>A</given-names></name><name><surname>Phillips</surname> <given-names>ML</given-names></name><name><surname>Card</surname> <given-names>GM</given-names></name><name><surname>Korff</surname> <given-names>W</given-names></name><name><surname>Rubin</surname> <given-names>GM</given-names></name><name><surname>Simpson</surname> <given-names>JH</given-names></name><name><surname>Reiser</surname> <given-names>MB</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping the neural substrates of behavior</article-title><source>Cell</source><volume>170</volume><fpage>393</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.06.032</pub-id><pub-id pub-id-type="pmid">28709004</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Liu</surname> <given-names>K</given-names></name><name><surname>Rath</surname> <given-names>M</given-names></name><name><surname>RueÎ²</surname> <given-names>D</given-names></name><name><surname>Mueller</surname> <given-names>T</given-names></name><name><surname>Skibbe</surname> <given-names>H</given-names></name><name><surname>Drayer</surname> <given-names>B</given-names></name><name><surname>Schmidt</surname> <given-names>T</given-names></name><name><surname>Filippi</surname> <given-names>A</given-names></name><name><surname>Nitschke</surname> <given-names>R</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name><name><surname>Burkhardt</surname> <given-names>H</given-names></name><name><surname>Driever</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ViBE-Z: a framework for 3D virtual colocalization analysis in zebrafish larval brains</article-title><source>Nature Methods</source><volume>9</volume><fpage>735</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2076</pub-id><pub-id pub-id-type="pmid">22706672</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schmidt</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>UGM: A Matlab Toolbox for Probabilistic Undirected Graphical Models</source><ext-link ext-link-type="uri" xlink:href="http://www.cs.ubc.ca/~schmidtm/Software/UGM.html">http://www.cs.ubc.ca/~schmidtm/Software/UGM.html</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Scholz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predicting natural behavior from whole-brain neural dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/445643</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SchrÃ¶del</surname> <given-names>T</given-names></name><name><surname>Prevedel</surname> <given-names>R</given-names></name><name><surname>Aumayr</surname> <given-names>K</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain-wide 3D imaging of neuronal activity in <italic>Caenorhabditis elegans</italic> with sculpted light</article-title><source>Nature Methods</source><volume>10</volume><fpage>1013</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2637</pub-id><pub-id pub-id-type="pmid">24013820</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SjÃ¶strand</surname> <given-names>K</given-names></name><name><surname>Clemmensen</surname> <given-names>LH</given-names></name><name><surname>Einarsson</surname> <given-names>G</given-names></name><name><surname>Larsen</surname> <given-names>R</given-names></name><name><surname>ErsbÃ¸ll</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>SpaSM: a MATLAB toolbox for sparse statistical modeling</article-title><source>Journal of Statistical Software</source><volume>84</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v084.i10</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname> <given-names>J</given-names></name><name><surname>PeyrÃ©</surname> <given-names>G</given-names></name><name><surname>Kim</surname> <given-names>VG</given-names></name><name><surname>Sra</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Entropic metric alignment for correspondence problems</article-title><source>ACM Transactions on Graphics</source><volume>35</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1145/2897824.2925903</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>GJ</given-names></name><name><surname>Johnson-Kerner</surname> <given-names>B</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Ryu</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dimensionality and dynamics in the behavior of <italic>C. elegans</italic></article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000028</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000028</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stiernagle</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Maintenance of <italic>C. elegans</italic> WormBook: the online review of <italic>C. elegans</italic></article-title><source>Biology</source><volume>1</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>C</given-names></name><name><surname>McCallum</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Piecewise pseudolikelihood for efficient training of conditional random fields</article-title><conf-name>ACM International Conference Proceeding Series</conf-name><fpage>863</fpage><lpage>870</lpage></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>C</given-names></name><name><surname>McCallum</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An introduction to conditional random fields</article-title><source>Machine Learning</source><volume>4</volume><fpage>267</fpage><lpage>373</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szigeti</surname> <given-names>B</given-names></name><name><surname>Gleeson</surname> <given-names>P</given-names></name><name><surname>Vella</surname> <given-names>M</given-names></name><name><surname>Khayrulin</surname> <given-names>S</given-names></name><name><surname>Palyanov</surname> <given-names>A</given-names></name><name><surname>Hokanson</surname> <given-names>J</given-names></name><name><surname>Currie</surname> <given-names>M</given-names></name><name><surname>Cantarelli</surname> <given-names>M</given-names></name><name><surname>Idili</surname> <given-names>G</given-names></name><name><surname>Larson</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>OpenWorm: an open-science approach to modeling <italic>Caenorhabditis elegans</italic></article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>137</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00137</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taskar</surname> <given-names>B</given-names></name><name><surname>Guestrin</surname> <given-names>C</given-names></name><name><surname>Koller</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Max-margin markov networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>25</fpage><lpage>32</lpage></element-citation></ref><ref id="bib74"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taskar</surname> <given-names>B</given-names></name><name><surname>Chatalbashev</surname> <given-names>V</given-names></name><name><surname>Koller</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Learning associative markov networks</article-title><conf-name>Twenty-First International Conference on Machine Learning - ICML</conf-name><fpage>100</fpage><lpage>102</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoshima</surname> <given-names>Y</given-names></name><name><surname>Tokunaga</surname> <given-names>T</given-names></name><name><surname>Hirose</surname> <given-names>O</given-names></name><name><surname>Kanamori</surname> <given-names>M</given-names></name><name><surname>Teramoto</surname> <given-names>T</given-names></name><name><surname>Jang</surname> <given-names>MS</given-names></name><name><surname>Kuge</surname> <given-names>S</given-names></name><name><surname>Ishihara</surname> <given-names>T</given-names></name><name><surname>Yoshida</surname> <given-names>R</given-names></name><name><surname>Iino</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Accurate automatic detection of densely distributed cell nuclei in 3D space</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004970</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004970</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Toyoshima</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An annotation dataset facilitates automatic annotation of whole-brain activity imaging of <italic>C. elegans</italic></article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/698241</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaadia</surname> <given-names>RD</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Voleti</surname> <given-names>V</given-names></name><name><surname>Singhania</surname> <given-names>A</given-names></name><name><surname>Hillman</surname> <given-names>EMC</given-names></name><name><surname>Grueber</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Characterization of proprioceptive system dynamics in behaving <italic>Drosophila larvae</italic> using High-Speed volumetric microscopy</article-title><source>Current Biology</source><volume>29</volume><fpage>935</fpage><lpage>944</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.01.060</pub-id><pub-id pub-id-type="pmid">30853438</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Ji</surname> <given-names>N</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Clark</surname> <given-names>C</given-names></name><name><surname>Mitchell</surname> <given-names>JK</given-names></name><name><surname>Klein</surname> <given-names>M</given-names></name><name><surname>Tabone</surname> <given-names>CJ</given-names></name><name><surname>Florman</surname> <given-names>J</given-names></name><name><surname>Ji</surname> <given-names>H</given-names></name><name><surname>Greenwood</surname> <given-names>J</given-names></name><name><surname>Chisholm</surname> <given-names>AD</given-names></name><name><surname>Srinivasan</surname> <given-names>J</given-names></name><name><surname>Alkema</surname> <given-names>M</given-names></name><name><surname>Zhen</surname> <given-names>M</given-names></name><name><surname>Samuel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pan-neuronal imaging in roaming <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1082</fpage><lpage>E1088</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507109113</pub-id><pub-id pub-id-type="pmid">26711989</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname> <given-names>B</given-names></name><name><surname>Aghayeva</surname> <given-names>U</given-names></name><name><surname>Sun</surname> <given-names>H</given-names></name><name><surname>Wang</surname> <given-names>C</given-names></name><name><surname>Glenwinkel</surname> <given-names>L</given-names></name><name><surname>Bayer</surname> <given-names>EA</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An atlas <italic>of Caenorhabditis elegans</italic> chemoreceptor expression</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004218</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004218</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wainwright</surname> <given-names>MJ</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Graphical Models, Exponential Families, and Variational Inference</source><publisher-name>Foundations and Trends</publisher-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wakabayashi</surname> <given-names>T</given-names></name><name><surname>Kimura</surname> <given-names>Y</given-names></name><name><surname>Ohba</surname> <given-names>Y</given-names></name><name><surname>Adachi</surname> <given-names>R</given-names></name><name><surname>Satoh</surname> <given-names>Y</given-names></name><name><surname>Shingai</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>In vivo calcium imaging of OFF-responding ASK chemosensory neurons in <italic>C. elegans</italic></article-title><source>Biochimica Et Biophysica Acta (BBA) - General Subjects</source><volume>1790</volume><fpage>765</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1016/j.bbagen.2009.03.032</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>F</given-names></name><name><surname>Vemuri</surname> <given-names>BC</given-names></name><name><surname>Rangarajan</surname> <given-names>A</given-names></name><name><surname>Eisenschenk</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Simultaneous nonrigid registration of multiple point sets and atlas construction</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>30</volume><fpage>2011</fpage><lpage>2022</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.70829</pub-id><pub-id pub-id-type="pmid">18787248</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>AH</given-names></name><name><surname>Kim</surname> <given-names>TH</given-names></name><name><surname>Wang</surname> <given-names>F</given-names></name><name><surname>Vyas</surname> <given-names>S</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Schnitzer</surname> <given-names>M</given-names></name><name><surname>Kolda</surname> <given-names>TG</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unsupervised discovery of demixed, Low-Dimensional neural dynamics across multiple timescales through tensor component analysis</article-title><source>Neuron</source><volume>98</volume><fpage>1099</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.015</pub-id><pub-id pub-id-type="pmid">29887338</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Lin</surname> <given-names>A</given-names></name><name><surname>Nejatbakhsh</surname> <given-names>A</given-names></name><name><surname>Varol</surname> <given-names>E</given-names></name><name><surname>Sun</surname> <given-names>R</given-names></name><name><surname>Mena</surname> <given-names>GE</given-names></name><name><surname>Samuel</surname> <given-names>ADT</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>NeuroPAL: a multicolor atlas for Whole-Brain neuronal identification in <italic>C. elegans</italic></article-title><source>Cell</source><volume>184</volume><fpage>272</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.12.012</pub-id><pub-id pub-id-type="pmid">33378642</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>S</given-names></name><name><surname>Avery</surname> <given-names>L</given-names></name><name><surname>Baude</surname> <given-names>E</given-names></name><name><surname>Garbers</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Guanylyl cyclase expression in specific sensory neurons: a new family of chemosensory receptors</article-title><source>PNAS</source><volume>94</volume><fpage>3384</fpage><lpage>3387</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.7.3384</pub-id><pub-id pub-id-type="pmid">9096403</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaslaver</surname> <given-names>A</given-names></name><name><surname>Liani</surname> <given-names>I</given-names></name><name><surname>Shtangel</surname> <given-names>O</given-names></name><name><surname>Ginzburg</surname> <given-names>S</given-names></name><name><surname>Yee</surname> <given-names>L</given-names></name><name><surname>Sternberg</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical sparse coding in the sensory system of <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>112</volume><fpage>1185</fpage><lpage>1189</lpage><pub-id pub-id-type="doi">10.1073/pnas.1423656112</pub-id><pub-id pub-id-type="pmid">25583501</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname> <given-names>H</given-names></name><name><surname>Hastie</surname> <given-names>T</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Sparse principal component analysis</article-title><source>Journal of Computational and Graphical Statistics</source><volume>15</volume><fpage>265</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1198/106186006X113430</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Extended methods</title><boxed-text><sec id="s9"><title>S1: Detailed description of CRF_ID model for cell annotation</title><sec id="s9-1"><title>S1.1 Advantages of CRF_ID framework</title><p>There are several advantages of CRF-based formulation for automatic cell annotation as discussed below.</p><list list-type="order"><list-item><p>The major advantage of CRF framework is that arbitrary feature relationships <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> among non-independent observations <inline-formula><mml:math id="inf75"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula> can be specified (<xref ref-type="bibr" rid="bib71">Sutton and McCallum, 2010</xref>). This enables optimizing user-defined arbitrary features. Depending on the scope of potential functions <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, these features may be cell specific that is unary features (<inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), pairwise features (<inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) or higher order features. For example position of cells in ganglion, position related to a landmark cell with known identity, color etc.</p></list-item><list-item><p>Since we are modeling a probability distribution over labels assigned to neurons, a set of candidate list of names can be generated for each neuron. A straightforward way to do this is by using the marginal probabilities of labels assigned to each cell. Once the normalization constant <inline-formula><mml:math id="inf79"><mml:mi>Z</mml:mi></mml:math></inline-formula> has been calculated, marginal probability of cell <inline-formula><mml:math id="inf80"><mml:mi>i</mml:mi></mml:math></inline-formula> taking a label <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be obtained as <inline-formula><mml:math id="inf82"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo> <mml:mi/><mml:mrow><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>j</mml:mi><mml:mo>â‰ </mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Other computationally efficient methods to estimate the marginal probabilities using eigenvector of potential functions <xref ref-type="bibr" rid="bib35">Leordeanu and Hebert, 2005</xref>; <xref ref-type="bibr" rid="bib36">Leordeanu and Hebert, 2009</xref> have been proposed as well. We propose an alternative method to generate such list (see Appendix 1â€“Extended methods S1.5) taking into account the undetected neurons in image stack.</p></list-item><list-item><p>With log-linear parameterization of feature functions as in (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), CRF models belong to exponential family models (<xref ref-type="bibr" rid="bib80">Wainwright and Jordan, 2007</xref>) or max-entropy models. Thus, the joint probability distribution over labels assigned to neurons that we infer is maximally unbiased (maximum entropy) subject to some empirical constraints (sufficient statistics to define the probability distribution). In our case, these empirical constraints are geometrical relationships among cell positions. Interestingly, the maximum entropy nature of the objective function in our model also makes it very similar to the entropy regularized optimal transport problems (<xref ref-type="bibr" rid="bib67">Solomon et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Nitzan et al., 2019</xref>).</p></list-item><list-item><p>CRF framework is a trainable algorithm (<xref ref-type="bibr" rid="bib74">Taskar et al., 2004</xref>; <xref ref-type="bibr" rid="bib73">Taskar et al., 2003</xref>). Thus, if annotated image stacks are available, the weights <inline-formula><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the feature functions can be optimized directly such that the cell labels in annotated image stacks match the predicted labels. Further, we show that annotated experimental data can be utilized by building a data-driven atlas and feature functions can updated based on data-driven atlas (S1.7). For pairwise feature functions in our model, building such data-driven atlas requires cheap mathematical operations (averaging).</p></list-item></list></sec></sec><sec id="s10"><title>S1.2 Features in CRF_ID-based annotation framework</title><p>During manual annotation of neuron identities in images, researchers use intuitive features such as positions of neurons in image and atlas, positional relationships among neurons, proximity of neurons to one another, neuron with known identities such as neurons expressing specific markers etc. In this section, we describe how such intuitive features are encoded in our model.</p><sec id="s10-1"><title>S1.2.1 Unary potentials â€“ positions along AP axis</title><p>In empirical data, we observed that anterior-posterior (AP) positional relationships among the cells are most stereotypical (<xref ref-type="fig" rid="fig2s6">Figure 2â€”figure supplement 6C,D</xref>) and consistent with the 3D atlas. Further since, x-y sampling of image stacks is much higher than z sampling, AP axisâ€™ sampling is always higher than LR and DV axes. Thus, positions along AP axes detected by segmentation method are less noisy. Thus, we included a feature based on positions of cell along AP axis as unary feature.<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msubsup><mml:mi>Ïƒ</mml:mi><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the position of cell <inline-formula><mml:math id="inf85"><mml:mi>i</mml:mi></mml:math></inline-formula> along AP axes in data and <inline-formula><mml:math id="inf86"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the position of cell with label <inline-formula><mml:math id="inf87"><mml:mi>m</mml:mi></mml:math></inline-formula> in atlas. <inline-formula><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the bandwidth parameter. To account for scale differences between image and atlas, positions along AP axis are normalized first in image and atlas. Low value of <inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> greatly penalizes the deviation of cell position along AP axis in image from atlas position thus favoring that labeling should preserve positions along AP axis. Large value of <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> decreases the effect of deviation of cell position along AP axis. Thus, this feature restricts the assignment of a particular label to certain cells along AP axes making the labeling AP consistent. Low values of <inline-formula><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi>Ïƒ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mi/></mml:math></inline-formula> also increases the influence (weight) of this unary potential compared to the pairwise potentials in labeling. We set this parameter as 1 to set equal influence of unary and pairwise potentials.</p></sec><sec id="s10-2"><title>S1.2.2 Pairwise potentials â€“ binary positional relationships</title><p>These features encode that the labels assigned to neuron in an image should preserve the relative positional relationships among them in the atlas. for example if neuron <inline-formula><mml:math id="inf92"><mml:mi>i</mml:mi></mml:math></inline-formula> is to the left of neuron <inline-formula><mml:math id="inf93"><mml:mi>j</mml:mi></mml:math></inline-formula> in the image stack and these neurons are assigned labels <inline-formula><mml:math id="inf94"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:mi>n</mml:mi></mml:math></inline-formula> respectively, then the neuron with label <inline-formula><mml:math id="inf96"><mml:mi>m</mml:mi></mml:math></inline-formula> in the atlas should be to the left of neuron with label <inline-formula><mml:math id="inf97"><mml:mi>n</mml:mi></mml:math></inline-formula>. Similar constraints can also be applied on anterior-posterior relationship and dorsal-ventral relationship of neurons in the image stack. Let <inline-formula><mml:math id="inf98"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> denote the coordinate of neuron <inline-formula><mml:math id="inf99"><mml:mi>i</mml:mi></mml:math></inline-formula> in the image along anterior-posterior (AP), left-right (LR) and dorsal-ventral (DV) axes, respectively. Similarly, <inline-formula><mml:math id="inf100"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> be the coordinates of neuron with label <inline-formula><mml:math id="inf101"><mml:mi>m</mml:mi></mml:math></inline-formula> in the atlas. The feature functions <inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, this feature implies that if atlas labels <inline-formula><mml:math id="inf103"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:mi>n</mml:mi></mml:math></inline-formula> assigned to neurons <inline-formula><mml:math id="inf105"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:mi>j</mml:mi></mml:math></inline-formula> in image are consistent with the AP positional relationship of cells <inline-formula><mml:math id="inf107"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf108"><mml:mi>j</mml:mi></mml:math></inline-formula>, then the feature value is 1 else 0. Note that the feature values for cells <inline-formula><mml:math id="inf109"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:mi>j</mml:mi></mml:math></inline-formula> is same (1 or 0) irrespective of the labels <inline-formula><mml:math id="inf111"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:mi>n</mml:mi></mml:math></inline-formula> assigned to cells. This is true only if annotation is performed using a static atlas or an atlas built from only one data source. We expand more on this in section S1.7 (Building data-driven atlases) and explain how label dependent feature functions are formed with the availability of empirical hand-annotated datasets.</p><p>Similarly, features are defined for left-right and dorsal-ventral relationships, <inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively.<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are hyperparameters in the model that weigh positional relationship features against other pairwise features in the model. We set these parameters as 1 to give equal weightage to all features.</p></sec><sec id="s10-3"><title>S1.2.3 Pairwise potentials â€“ proximity relationships</title><p>While manually annotating images by comparing positions of neurons to atlas, researchers often use proximity relationship among neurons that is if neuron <inline-formula><mml:math id="inf118"><mml:mi>i</mml:mi></mml:math></inline-formula> is anatomically far from neuron <inline-formula><mml:math id="inf119"><mml:mi>j</mml:mi></mml:math></inline-formula> then the identities to be assigned to these neurons from atlas should not belong to neighboring or nearby neurons. To encode such intuition in the model, we include proximity feature similar to the Gromov-Wasserstein discrepancy used in shape matching (<xref ref-type="bibr" rid="bib67">Solomon et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">MÃ©moli, 2011</xref>)<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf120"><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is any distance measure between neurons <inline-formula><mml:math id="inf121"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf122"><mml:mi>j</mml:mi></mml:math></inline-formula> in the image stack and <inline-formula><mml:math id="inf123"><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is the same distance measure between neurons with labels <inline-formula><mml:math id="inf124"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf125"><mml:mi>n</mml:mi></mml:math></inline-formula> in the atlas. We use geodesic distances between cells. To calculate geodesic distances, graphs were constructed by connecting each neuron to its nearest six neighbors. <inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is hyperparameter that weighs proximity relationship feature function against other features in the model. We set <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as 1.</p><p>We compared geodesic distances rather than Euclidean <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> distances because geodesic distances are invariant to spatial scale in the image and atlas. This is critical since the scale of spatial distribution of neurons in the atlas is very different (much lower) than those in the images. Also, the spatial scale of distribution of cells may vary across images, depending on the size of worm in images; however, geodesic distances among neurons should be preserved.</p></sec><sec id="s10-4"><title>S1.2.4 Pairwise potentials â€“ angular positional relationships</title><p>Relative positional relationship features described in S1.2.2 encode information about positional relationships along axes independently that is each feature contains information about positional relationship along one axis only. For example <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> encodes whether neuron <inline-formula><mml:math id="inf130"><mml:mi>i</mml:mi></mml:math></inline-formula> is anterior to or posterior to neuron <inline-formula><mml:math id="inf131"><mml:mi>j</mml:mi></mml:math></inline-formula> and how the labels should be assigned to these cells. A feature that simultaneously accounts for positional relationships along all axes may additionally help in determining identities of neurons. Such a feature could be formed by multiplying AP, LR, and DV positional relationship features. However, a multiplied feature will still contain binary information only about whether neuron <inline-formula><mml:math id="inf132"><mml:mi>i</mml:mi></mml:math></inline-formula> is anterior, dorsal and to the right of neuron <inline-formula><mml:math id="inf133"><mml:mi>j</mml:mi></mml:math></inline-formula> or not. It would not tell anything about fine scale directional relationships. Thus, we formulated an angular relationship feature. Let <inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>'</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>'</mml:mi></mml:math></inline-formula> be the 3D vectors associated with coordinates of neurons <inline-formula><mml:math id="inf136"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf137"><mml:mi>j</mml:mi></mml:math></inline-formula> in the image stack. Also let <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:math></inline-formula> be the 3D vectors associated with coordinates of neurons with labels <inline-formula><mml:math id="inf140"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:mi>n</mml:mi></mml:math></inline-formula> in the atlas. Then the feature is defined as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac bevelled="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Thus, if the vector <inline-formula><mml:math id="inf142"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi><mml:mi>'</mml:mi><mml:mi>q</mml:mi><mml:mi>'</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> aligns perfectly with the vector <inline-formula><mml:math id="inf143"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi><mml:mi>'</mml:mi><mml:mi>'</mml:mi><mml:mi>q</mml:mi><mml:mi>'</mml:mi><mml:mi>'</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> if the vectors point in completely opposite directions. This feature encodes directional agreement of the labels <inline-formula><mml:math id="inf146"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mi>n</mml:mi></mml:math></inline-formula> assigned to neurons <inline-formula><mml:math id="inf148"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:mi>j</mml:mi></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is hyperparameter that weighs angular relationship feature function against other features in the model. We set <inline-formula><mml:math id="inf151"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as 1.</p></sec></sec><sec id="s11"><title>S1.3 Defining AP, LR, and DV axes</title><p>To compare positional relationships among neurons in image and atlas, it is necessary to define anterior-posterior (AP), left-right (LR), and dorsal-ventral (DV) axes in image as well in atlas. 3D coordinates of neurons along these axes are then used to define features described above. We use two methods to define these axes. In method 1, we use Principal Component Analysis to obtain these axes. Let <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã—</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the centered coordinates (zero mean) of <inline-formula><mml:math id="inf153"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons detected in the image stack or atlas. Then the principal components correspond to the eigenvectors of the matrix <inline-formula><mml:math id="inf154"><mml:mi mathvariant="bold">p</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Since the spatial spread of neurons in image as well as in atlas is maximum along AP axes, the first principal component (eigenvector corresponding to maximum eigenvalue) always corresponds to AP axis. Second and third PCs can be assigned to LR and DV axes depending on the orientation of worm in image as described below. Due to rotation of worm about AP axis, the second and third eigenvectors may not always correspond to LR and DV axes. Thus, we designed two methods for these different scenarios â€“</p><list list-type="order"><list-item><p><bold>Worm lying on LR axis</bold> â€“ In this case LR axis is assigned to the third eigenvector. This is because z-sampling of image stacks is much smaller compared to x-y sampling. Thus, the spread of neurons along LR axis is smallest. Figure below shows axes obtained using PCA.</p></list-item><list-item><p> <bold>Worm rotated about AP axis</bold> â€“ In this case we developed an alternative method. First, we define LR axis. To do so we use left-right pair of neurons that are easily detectable in image stack. We used RMEL-RMER for landmark strain (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and RMDVL-RMDVR for NeuroPAL strain (<xref ref-type="fig" rid="fig6">Figure 6E, F</xref>). Using these neuron-pair coordinates, an LR vector, <inline-formula><mml:math id="inf155"><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> was defined as <inline-formula><mml:math id="inf156"><mml:mfrac bevelled="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula>. Next AP vector, <inline-formula><mml:math id="inf157"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> was determined by solving constrained optimization problem<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover></mml:mrow><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">â†’</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mtext>Â </mml:mtext></mml:mrow><mml:mrow><mml:mtext>Â </mml:mtext></mml:mrow><mml:mtext>Â </mml:mtext><mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover><mml:mo>.</mml:mo><mml:mover><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>Â </mml:mtext><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mtext>Â </mml:mtext><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></list-item></list><p>That is a unit vector which is orthogonal to LR vector and in the direction of first principal component <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">â†’</mml:mo></mml:mover><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Next, <inline-formula><mml:math id="inf159"><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> vector is obtained by defining a vector orthogonal to both <inline-formula><mml:math id="inf160"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> vectors.</p><p>Finally, we check the consistency of <inline-formula><mml:math id="inf162"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="inf163"><mml:mover accent="true"><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula>, <inline-formula><mml:math id="inf164"><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mo>â†’</mml:mo></mml:mover></mml:math></inline-formula> vectors that is these vectors should point to the anterior, right and ventral of the worm and should satisfy cross product rule to constitute a valid coordinate system. This is necessary because PCA axes are determined up to a multiplication factor of -1 that is coordinate system specified by the principal components (PC1, PC2, PC3) is same as the coordinate system specified by (-PC1, -PC2, -PC3). Thus, a user input is taken in the framework while defining axes. Users can easily click on any neuron in the anterior portion and the posterior portion of the worm image, when asked to do so, to specify PC1 direction.</p><fig id="app1fig1" position="float"><label>Appendix 1â€”figure 1.</label><caption><title>Examples of PA (blue), LR (green), and DV (black) axes generated automatically in a whole-brain image stack.</title><p>Here red dots correspond to the segmented nuclei in image stack. Shown are 3D view (a), XY (b), YZ (c), and XZ (d) views of the image stack.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-app1-fig1-v2.tif"/></fig></sec><sec id="s12"><title>S1.4 Inferring neuron identities</title><p>To infer most probable identity of neurons, energy function in (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) is to be maximized. Exact inference techniques for maximizing energy functions over arbitrary graph structures, such as the fully connected graph structure of our model, are not available (<xref ref-type="bibr" rid="bib28">Kohli et al., 2009</xref>). Thus, we use a commonly used approximate inference method that has been used successfully in past for several applications, Loopy Belief Propagation (LBP) (<xref ref-type="bibr" rid="bib44">Murphy et al., 1999</xref>; <xref ref-type="bibr" rid="bib22">Ikeda et al., 2004</xref>) to infer optimal labeling that is the maximum of joint-probability distribution <inline-formula><mml:math id="inf165"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mfrac bevelled="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></inline-formula> as well as the marginal probabilities of labels assigned to each node. We implement our model in MATLAB using an open source package for undirected graphical models (<xref ref-type="bibr" rid="bib63">Schmidt, 2007</xref>).</p></sec><sec id="s13"><title>S1.5 Resolving duplicate assignments with label consistency score</title><p>In general, we define pairwise feature functions in the model such that it penalizes duplicate assignments of any label <inline-formula><mml:math id="inf166"><mml:mi>m</mml:mi></mml:math></inline-formula> to cells <inline-formula><mml:math id="inf167"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf168"><mml:mi>j</mml:mi></mml:math></inline-formula> in image as follows<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>âˆ</mml:mi></mml:math></disp-formula></p><p>Even with this measure, we still see some duplicate assignments when the LBP optimization converges. To resolve such assignments, we mark all cells that were not assigned duplicate labels as assigned. Next, we calculate a label consistency score for each cell that was assigned a duplicate label (examples shown in <xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1C</xref>). This score measures how consistent the current label assigned to the cell is in terms of preserving its positional relationships with other cells that were not assigned any duplicate labels (i.e. cells marked as assigned). Among all the cells that were assigned same duplicate label, the cell with the highest consistency score is assigned the duplicate label and marked as assigned. Remaining cells are marked as unassigned. After resolving all such duplicate assignments for all labels, optimization is run again only for the unassigned cells while keeping the identities of other cells fixed. We calculate the label consistency score for each cell as follows<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>â€²</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>.</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>â€²</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>â€²</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf169"><mml:mi>i</mml:mi></mml:math></inline-formula> is a cell that was assigned a duplicate label and <inline-formula><mml:math id="inf170"><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> denotes the set of all other cells that were not assigned duplicate labels. Intuitively, correctly predicted cells should have higher consistency score. This is because correctly predicted cells will preserve their relationship with other correctly predicted cells and wonâ€™t preserve their relationship with incorrectly predicted cells. In contrast, incorrectly predicted cells will neither preserve their relationship with correctly predicted cells nor with incorrectly predicted cells thus having a lower consistency score. This was observed in simulations as well (<xref ref-type="fig" rid="fig1s1">Figure 1â€”figure supplement 1D</xref>). Thus, label consistency score also serves as a good criterion for sorting candidate list of labels predicted by the framework.</p><p>Label consistency score is combination of features in the model that define the probability distribution <inline-formula><mml:math id="inf171"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> in our model. Thus, another way to look at the label consistency score is that it tries to maximize the pseudo-loglikelihood (<xref ref-type="bibr" rid="bib70">Sutton and McCallum, 2007</xref>; <xref ref-type="bibr" rid="bib21">HyvÃ¤rinen, 2006</xref>) of labels to be assigned to all cells with duplicate labels conditioned on the labels of other cells that were not assigned duplicate labels that is <inline-formula><mml:math id="inf172"><mml:mrow><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>â¡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf173"><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:math></inline-formula> denotes the set of all cells that were assigned duplicate labels.</p></sec><sec id="s14"><title>S1.6 Simulating missing cells and generating candidate name list for each cell</title><p>The 3D atlas of <italic>C. elegans</italic> neurons we used is freely available (<xref ref-type="bibr" rid="bib72">Szigeti et al., 2014</xref>). There are 195 cells in head ganglion in this atlas, that is the label list from which identities are to be assigned to cells in data has 195 elements. However, empirically we detect only ~120â€“130 neurons in whole-brain image stacks. Remaining neurons are undetected due to either no/low expression levels of fluorophore in these cells or false-negatives in automated detection algorithm. Approximately similar number of neurons were detected by other labs as well (<xref ref-type="bibr" rid="bib25">Kato et al., 2015</xref>). Further, which cells are undetected is not known a priori. Thus, to take into account missing cells while annotating identities with the model, we define a hidden variable <inline-formula><mml:math id="inf174"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> that specifies the cells missing in images that is <inline-formula><mml:math id="inf175"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if cell with label <inline-formula><mml:math id="inf176"><mml:mi>k</mml:mi></mml:math></inline-formula> in atlas is missing in the image. Here <inline-formula><mml:math id="inf177"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of labels/cells in atlas. With the hidden variable, we model the joint probability distribution <inline-formula><mml:math id="inf178"><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula> as<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Unary potential functions <inline-formula><mml:math id="inf182"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and pairwise potential functions <inline-formula><mml:math id="inf183"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are same as described in S1 and S1.2. Potential function <inline-formula><mml:math id="inf184"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> captures the dependencies between label <inline-formula><mml:math id="inf185"><mml:mi>m</mml:mi></mml:math></inline-formula> assigned to cell <inline-formula><mml:math id="inf186"><mml:mi>i</mml:mi></mml:math></inline-formula> when the hidden variable is <inline-formula><mml:math id="inf187"><mml:mi mathvariant="bold">h</mml:mi></mml:math></inline-formula>. We set the potential function as<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, the potential specifies that if <inline-formula><mml:math id="inf188"><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> label in atlas is missing in the image then that label cannot be assigned to any cell in the image. Further, if <inline-formula><mml:math id="inf189"><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> label is not missing in the image then each cell has equal potential to be assigned that label which will be updated based on positional relationship potentials. Further, the potential function <inline-formula><mml:math id="inf190"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> captures the dependence between the observed image <inline-formula><mml:math id="inf191"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula> and the hidden variable <inline-formula><mml:math id="inf192"><mml:mi mathvariant="bold">h</mml:mi></mml:math></inline-formula>. Defining this potential function is not trivial for whole-brain images since predicting which cells are missing just based on observed 3D whole-brain image stack is difficult. However, in specific cases confidence values of each label missing in the image may be available, denoted by <inline-formula><mml:math id="inf193"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mn>0</mml:mn><mml:mo>â‰¤</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>â‰¤</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. Such confidence values may be specified based on prior knowledge for example based on expression pattern of fluorophore labeling cells in strain, cells that have low detection probability may have higher parameter <inline-formula><mml:math id="inf194"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Using these confidence values, we define <inline-formula><mml:math id="inf195"><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> as<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Our goal is to calculate <inline-formula><mml:math id="inf196"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula> which can be obtained by marginalizing (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>) over the hidden states <inline-formula><mml:math id="inf197"><mml:mi mathvariant="bold">h</mml:mi></mml:math></inline-formula>. However, since the number of elements in the space of <inline-formula><mml:math id="inf198"><mml:mi mathvariant="bold">h</mml:mi></mml:math></inline-formula> is huge, marginalizing as well as calculating the normalization constant <inline-formula><mml:math id="inf199"><mml:mi>Z</mml:mi></mml:math></inline-formula> is not tractable. But, for specific cases such as we describe below, the calculation can be simplified.</p><p>In the absence of any prior information about which labels are missing in images for whole-brain imaging case, we assigned equal confidence value for each label missing in the image that is <inline-formula><mml:math id="inf200"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Î²</mml:mi></mml:math></inline-formula>. Thus, for a fixed number <inline-formula><mml:math id="inf201"><mml:mi>P</mml:mi></mml:math></inline-formula> of missing cells in image, <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:mi>Î²</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. A consequence of defining potential function this way is that for a given number of missing cells in image, each combination of missing labels in atlas is equally probable as long as labels of missing cells are not assigned to any cell in image<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">â„°</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow/><mml:munder><mml:mo>âˆ</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>âˆˆ</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mo>â€²</mml:mo></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mo>â€²</mml:mo></mml:msup></mml:mrow><mml:mo>âŠ‚</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> such that <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">âˆ€</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula><italic>label assigned to cells in y and</italic> <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>âˆ‘</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Further, <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>â€²</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of elements in <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>â€²</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, we can randomly and uniformly sample <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and keep it fixed while predicting <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Our goal is to calculate <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We do so as<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Therefore, we randomly select <inline-formula><mml:math id="inf211"><mml:mi>P</mml:mi></mml:math></inline-formula> cells in atlas that are considered to be missing in the image data. These <inline-formula><mml:math id="inf212"><mml:mi>P</mml:mi></mml:math></inline-formula> cells are selected uniformly across different regions of the head ganglion following the discussion above. Labels of these <inline-formula><mml:math id="inf213"><mml:mi>P</mml:mi></mml:math></inline-formula> cells are removed from the atlas i.e. the list of possible labels that can be assigned to neurons, and identities of cells are predicted using the remaining atlas. This process is repeated ~1000 times to sample multiple possible combinations of <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, a candidate list of names is generated using (<xref ref-type="disp-formula" rid="equ23">Equation 23</xref>) that is compiling a list of optimum labels predicted for each cell in each run (by maximizing <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in each run) and choosing the top frequent labels for each cell across all runs.</p></sec><sec id="s15"><title>S1.7 Building data-driven consensus atlas</title><p>Here we describe the procedure and the intuition behind building data-driven consensus atlas in our framework. We also describe the intuition behind why it is computationally more efficient than building data-driven atlases for registration methods.</p><p>First, we describe how features in the model are defined using a static atlas such as OpenWorm atlas and then extend the formulation to building and using data-driven atlas. Positional relationships among cells based on the OpenWorm atlas are stored as matrices of size <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã—</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of cells in atlas. Each cell in matrix records the positional relationship between a pair of cells observed in the atlas. For example, for AP positional relationship matrix, if a column cell such as RMEL is anterior to a row cell such as AIZR in atlas, then the corresponding cell in matrix will denote 1 and otherwise 0. Here, 1 implies that according to the prior information available from OpenWorm, cell RMEL is observed to be anterior to cell AIZR with 100% probability. Let <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã—</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the AP positional relationship matrix and <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> be labels in atlas. Then,<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>n</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>n</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Using the matrix <inline-formula><mml:math id="inf221"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, the AP positional relationship feature function <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for cells <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (as described in S1.2.2) can be defined as<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>j</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the AP positional relationship feature for assigning labels <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf227"><mml:mi>m</mml:mi></mml:math></inline-formula> in atlas to cells <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in image. Note that <xref ref-type="disp-formula" rid="equ25">Equation 25</xref> is consistent with <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> as shown below by expanding <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> terms using <xref ref-type="disp-formula" rid="equ24">Equation 24</xref> in <xref ref-type="disp-formula" rid="equ25">Equation 25</xref>.<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>n</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>m</mml:mi><mml:mo>âˆ©</mml:mo><mml:mi>j</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>n</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo>âˆ©</mml:mo><mml:mi>j</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>n</mml:mi><mml:mo>âˆ©</mml:mo><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>n</mml:mi><mml:mo>âˆ©</mml:mo><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, the AP positional relationship matrix <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> stores the prior knowledge available on anterior-posterior relationships among cells. Since, <inline-formula><mml:math id="inf233"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is built using only a single data source that is OpenWorm atlas, all elements in the matrix are either 1 or 0. This implies that for all pairs of cells <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas, the <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix says that cell <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is anterior to cell <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with either 100% probability or 0 probability. Also, note that since <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> consists of only 1â€™s and 0â€™s, the AP positional relationship feature, <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, also consists of only 1â€™s and 0s, thus it is independent of labels <inline-formula><mml:math id="inf241"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf242"><mml:mi>m</mml:mi></mml:math></inline-formula> assigned to cells <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In contrast to using a single data source, if additional information is available in the form of annotated experimental datasets, the prior knowledge on anterior-posterior relationships can be updated. For example it may be possible that cell <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is observed to be anterior to cell <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with 80% probability in annotated datasets (e.g. in 8 out of 10 experimental datasets) and posterior to cell <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with 20% probability. This empirically observed AP positional relationships among cells can be updated in matrix <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mrow><mml:mi>n</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>n</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mn>0</mml:mn><mml:mo>â‰¤</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>â‰¤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the fraction of annotated datasets in which labels <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is annotated to the anterior of label <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, instead of specifying a hard constraint based on a single data source static atlas (as in <xref ref-type="disp-formula" rid="equ24">Equation 24</xref>), <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> specifies a soft constraint based on positional relationships observed in experimental annotated datasets. Formally, <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined as<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>âˆˆ</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes an annotated data set, <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the total number of annotated datasets used to build atlas, <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf257"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>âˆˆ</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula> are the coordinates along AP axis of cells annotated labels <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in dataset <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ26">Equation 26</xref> can be seen as a generalization of <xref ref-type="disp-formula" rid="equ24">Equation 24</xref>. Further, the AP positional relationship feature function <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> can be updated based on data-driven atlas using <xref ref-type="disp-formula" rid="equ25">Equation 25</xref>. The above discussion can be similarly extended to LR, DV relationships, angular relationship, and proximity relationship.</p><p>Similarly, for angular relationship feature, instead of using a fixed vector <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>â€³</mml:mo></mml:msup><mml:msup><mml:mi>q</mml:mi><mml:mo>â€³</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> (refer to <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) between cells <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that is provided in static atlas, we use an average vector obtained from annotated experimental datasets.<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msup><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">â†’</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msubsup><mml:mo>âˆ’</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>â€³</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msubsup><mml:mo>âˆ’</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>â€³</mml:mo></mml:msup></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are position coordinates of cells with labels <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in data <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similarly, for proximity relationship feature (refer to <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), instead of using a fixed distance between cells with labels <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas, we use an average distance obtained from annotated experimental datasets.<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext>Â </mml:mtext></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the distance between cells with labels <inline-formula><mml:math id="inf273"><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in data <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The key difference between registration-based methods and our framework in building data-driven atlases lies in the underlying methodology used by these methods to annotate cells. Registration methods annotate cells by maximizing extrinsic similarity between images and atlas. This requires the pre-alignment of spaces in which the image and the atlas exist. Thus, building data-driven atlas for registration-based methods also requires pre-alignment of all annotated images to the same space. This is typically done by simultaneously registering or block-wise registering all annotated images which requires solving several constrained regression problems. In contrast, our framework annotates cells by maximizing intrinsic similarity between images and atlas which is independent to the pre-alignment of images to same space. Thus, in our framework positional relationship features can be calculated for each annotated dataset in its own space and subsequently the features are aggregated together by simple averaging operations, which is computationally efficient, to build data-driven atlases.</p></sec><sec id="s16"><title>S1.8 Discussion on previous methods using registration to annotate cells</title><p><xref ref-type="bibr" rid="bib58">Qu et al., 2011</xref> and <xref ref-type="bibr" rid="bib39">Long et al., 2009</xref> annotate cells in <italic>C. elegans</italic> images by spatially deforming an atlas with known cell identities and registering it to the image data. Similar method was proposed by <xref ref-type="bibr" rid="bib1">Aerni et al., 2013</xref>. Here, several other features were used along with location feature such as cell shape, cell size, fluorophore expression level etc. In <xref ref-type="bibr" rid="bib38">Long et al., 2008</xref>, <italic>C. elegans</italic> images were annotated by comparing each cell's location in image to cell locations in multiple template images to generate initial matches. Generated matches were then pruned by checking relative position consistency such as anterior-posterior relationship among cells. Thus, although pairwise positions were used, they were not systematically optimized to predict cell labels and were only used for enforcing consistency in post-registration matching. In <xref ref-type="bibr" rid="bib64">Scholz, 2018</xref>, cell identities were determined by registering image data to the 2D atlas (<xref ref-type="bibr" rid="bib2">Altun and Hall, 2009</xref>). Registering 3D data to 2D atlas makes it difficult to disambiguate the identities along LR axis since the LR neurons are not exactly symmetrical along that axis. Similarly, registration-based cell annotation was proposed in <xref ref-type="bibr" rid="bib76">Toyoshima, 2019</xref>; however, in this case, the authors generated their own atlases by registering several partial atlases with subset of cells labeled in each atlas. Registration-based cell identification was proposed in <xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref> as well. Here, additional color information was integrated as a feature for registration along with spatial location of cells.</p></sec><sec id="s17"><title>S1.9 Registration methods do not consider intrinsic similarity features such as relative positional relationships</title><p>One of the major reasons of higher cell annotation accuracy achieved by our framework (<xref ref-type="fig" rid="fig2">Figure 2C,D,E</xref>, <xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig6">Figure 6B</xref>, <xref ref-type="fig" rid="fig2s5">Figure 2â€”figure supplement 5</xref>) is that our model systematically includes and optimizes pairwise positional relationships between cells to determine cell labels. In comparison, registration-based methods that predict cell identities by registering image stack to the atlas maximize the extrinsic similarity between images and atlas (<xref ref-type="bibr" rid="bib7">Bronstein et al., 2007</xref>). Due to inherent biological variability, spatial distribution of cells in worms differ significantly (<xref ref-type="fig" rid="fig2s6">Figure 2â€”figure supplement 6A,B</xref>) from the positions of cells in atlas while the relative positional relationships are still preserved (<xref ref-type="fig" rid="fig2s6">Figure 2â€”figure supplement 6C,D</xref>). Below, we provide a mathematical argument for why registration-based methods do not include pairwise positional relationships. Further, we show that how registration information can be included in our model.</p><p>Following the description in S2.1, the objective function to be optimized in registration methods (<xref ref-type="bibr" rid="bib46">Myronenko and Song, 2009</xref>; <xref ref-type="bibr" rid="bib52">Panaganti and Aravind, 2015</xref>; <xref ref-type="bibr" rid="bib19">Ge et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Ma, 2015</xref>; <xref ref-type="bibr" rid="bib15">Chui and Rangarajan, 2003</xref>) is similar to<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo>âˆ</mml:mo><mml:munder><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the coordinate of <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> cell in the image, <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the registered coordinate of <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> cell with label <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the atlas, <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the posterior probability that <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> cell in image matches to the <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in cell in atlas, and <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">â„›</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the regularization term on the transformation applied to cells to ensure smoothness of transformations for non-rigid registration. If only the first term in the energy function (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) is kept in our model then energy function can be written as<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mi>a</mml:mi></mml:munder><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This model considers only unary (cell specific) features for predicting cell names. Further, if we consider only one unary feature function in the model based on spatial locations of cells in atlas defined as below<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula>and substituted back in (<xref ref-type="disp-formula" rid="equ32">Equation 32</xref>) then the energy function is equivalent to the objective function in registration algorithms.<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mi>a</mml:mi></mml:munder><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, objective function in registration algorithms can be specified using only unary features in CRF_ID framework. This shows that registration methods do not account for relative positional relationship features. Further, it highlights that CRF_ID framework can easily integrate registration information, as unary feature term, with relative positional relationship features for predicting cell identities.</p></sec><sec id="s18"><title>S2: Description of different methods that were compared</title><p>Below, we provide a brief description of the different methods that we compared. Each model uses a different and specific combination of information for predicting cell labels thus helps in dissecting what information is most useful in predicting cell biological names. Some of these methods consist of methods proposed previously for cell annotation in <italic>C. elegans.</italic> Note, for fair comparison, static 3D OpenWorm (<xref ref-type="bibr" rid="bib72">Szigeti et al., 2014</xref>) atlas or 2D atlas available on <ext-link ext-link-type="uri" xlink:href="http://wormatlas.org/">wormatlas.org</ext-link> was used as reference for all methods: absolute positions of cells were used from atlases for registration methods, positional relationships derived from the same atlases were used for CRF_ID framework.</p><sec id="s18-1"><title>S2.1 Registration</title><p>In this case, detected neurons in image stacks or neurons in synthetic data were registered to the atlas using a widely used non-rigid registration algorithm (<xref ref-type="bibr" rid="bib46">Myronenko and Song, 2009</xref>). For synthetic data, the process was repeated ~200 times with new synthetic data generated each time. Here, we provide a brief description of the registration algorithm method. Two point-sets are registered by iteratively applying smooth deformation to one of the point-sets. Let two point-sets be <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow/></mml:msup><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã—</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow/></mml:msup><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã—</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> consisting of <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> points respectively. Here, <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are coordinates of point-sets. In our case, 3D coordinates of nuclei detected in the image form point set <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and cell positions in the atlas form the point-set <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In CPD, each point in point-set <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is considered to be a random sample drawn from a mixture of gaussian distributions. Further the point set <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> specifies the centroids of gaussian components of this distribution. Thus, the probability of observing a point <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mfrac><mml:mi>Ï‰</mml:mi><mml:mi>m</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:mi>Ï‰</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="script">ğ’©</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï‰</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is outlier ratio. A smooth transformation <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">â†’</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is applied to points in point-set <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> given by <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext>Â </mml:mtext><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> so as to maximize the likelihood of point-set <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">Â </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>Ã—</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is gaussian kernel matrix defined as <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the kth column of parameters matrix <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã—</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the transformation <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> lies in a Reproducible Kernel Hilbert Space (RKHS) with gaussian kernel. The aim of parameterizing transformation in this way is to ensure the smoothness of transformation.</p><p>Parameters of transformation matrix <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are estimated by maximizing the joint likelihood of data <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> latent variables corresponding to mixture components. This is done using Expectation-Maximization algorithm. E-step is equivalent to calculating the posterior probability that point <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is generated from component <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, keeping the current parameters <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> fixed.<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:mi>Ï‰</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mi>Ï‰</mml:mi><mml:mi>m</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:mi>Ï‰</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In M-step, component parameters are determined by maximizing the expected value of complete data log-likelihood <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (objective function). Additional regularization term is added to the objective function to minimize norm of <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in RKHS for controlling the complexity (smoothness) of transformation.<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow/><mml:mo>âˆ’</mml:mo><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">G</mml:mi><mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Prior to registration, cells in image stack and in atlas were aligned by defining the position coordinates of cells in AP, LR, and DV coordinate system (as described in S1.3) to improve the registration accuracy. Cell identities were predicted based on the correspondences generated by the registration algorithm. Note that this case does not consider co-dependent features among cells (as discussed in S1.10) as registration algorithms utilize only absolution positions of cells.</p><p>Additionally, we modified the registration method described above to account for missing cells in images, in a manner similar to the CRF_ID framework and for fair comparison with the CRF_ID annotation method. Identities were predicted iteratively with uniformly and randomly selected cells across head-ganglion considered missing thus the labels of those cells were removed from the atlas list. Cells in images were registered to the remaining cells in atlas. This process was repeated ~1000 times to sample multiple combinations of missing cells and identities were predicted by pooling results across each iteration. We did see an improvement in prediction accuracy with this modification compared to the base that does not account for missing cells. Hence all comparisons were performed with this modification and all results shown are for the modified method.</p><p>For registration based matching we used a popular registration method (<xref ref-type="bibr" rid="bib47">Myronenko and Song, 2010</xref>). The parameter settings of the method are below.</p><p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Parameter</th><th valign="top">Value</th><th valign="top">Description</th></tr></thead><tbody><tr><td valign="top">â€ƒopt.method</td><td valign="top">â€˜nonrigidâ€™</td><td valign="top">Non-rigid or rigid registration</td></tr><tr><td valign="top">â€ƒopt.beta</td><td valign="top">1</td><td valign="top">The width of gaussian kernel (smoothness)</td></tr><tr><td valign="top">â€ƒopt.lambda</td><td valign="top">3</td><td valign="top">Regularization weight</td></tr><tr><td valign="top">â€ƒopt.viz</td><td valign="top">0</td><td valign="top">Donâ€™t show any iteration</td></tr><tr><td valign="top">â€ƒopt.outliers</td><td valign="top">0.3</td><td valign="top">Noise weight</td></tr><tr><td valign="top">â€ƒopt.fgt</td><td valign="top">0</td><td valign="top">Do not use FGT (fast gaussian transform)</td></tr><tr><td valign="top">â€ƒopt.normalize</td><td valign="top">1</td><td valign="top">Normalize to unit variance and zero mean before registering</td></tr><tr><td valign="top">â€ƒopt.corresp</td><td valign="top">1</td><td valign="top">Compute correspondence vector at the end of registration</td></tr><tr><td valign="top">â€ƒopt.max_it</td><td valign="top">100</td><td valign="top">Max number of iterations</td></tr><tr><td valign="top">â€ƒopt.tol</td><td valign="top">1e-10</td><td valign="top">tolerance</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="s19"><title>S2.2 Relative position (Rel. Pos)</title><p>In this case, full CRF-based annotation framework as described in Appendix 1â€“Extended methods S1 was used. We considered only co-dependent features (i.e. pairwise relative positional features as described in S1.2) between all cells. Optimal labels of cells were predicted using the optimization procedure described in S1.4. Note that the information used by model in this case is different than the information used in Registration method as no information about absolute positions of cells is used. Thus, comparing prediction accuracy across these cases helps in verifying that co-dependent features are more useful in predicting neuron identities.</p></sec><sec id="s20"><title>S2.3 Registration + Rel position</title><p>In this case, we used both cell-specific features (i.e. unary features) and co-dependent features (i.e. pairwise features) to predict cell labels. Pairwise feature terms were the same as described in S1.2. Unary feature term was modified in this case as<disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msubsup><mml:mi>Ïƒ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the coordinates of cell <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in image stack and cell <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the atlas, respectively (in AP-LR-DV coordinate system). Thus, cell <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in image has a higher potential to take label <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of cell <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas if the distance between registered cell <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in image and cell <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas is small. Optimal transformation <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was inferred using the non-rigid registration method as described in S2.1. Here, parameter <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ïƒ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the weight between registration term and CRF-based matching term. For example if <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ïƒ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is small then the registered cell <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in image strongly prefers matching to the nearest cell in atlas. Thus, the relative position features have little influence on altering the cell label based on relative position consistency criteria. In contrast if <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ïƒ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is big then the registered cell <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> has uniform preference of matching to any cell atlas. This allows more flexibility to CRF_ID method to pick optimal labels enforcing the consistency criteria. We set <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ïƒ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to give equal weightage to the registration term and other features. Optimal labels of cells were predicted using the optimization procedure described in S1.4.</p></sec><sec id="s21"><title>S2.4 Color</title><p>In this case, only color information was used to identify the names of cells without using position information of cells at all. This helps in gauging the prediction accuracy that can be attained by using color information. Further, comparing this case with the cases that combine color information with position information (described below) helps in gauging the contribution of cell position information alone in predicting cell names. We describe below, both the baseline (naive) method for building color atlas and also building ensemble of color atlases with color distributions in training images aligned to test image.</p><sec id="s21-1"><title>Baseline color atlas</title><p>In this case, color atlas was built by aggregating raw RGB values of cells across images used to build atlas leaving the test image out. Feature function in this case was defined as the kernelized Mahalanobis distance between the colors of cells in image stack and colors of cells in atlas. Note that this is a unary feature (as it is cell specific)<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean RGB value of the <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> cell in test image, <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean RGB value of cell with label <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas and <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the covariance matrix of the RGB values of cell with label <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas. Let <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã—</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the observed RGB values of cells with label <inline-formula><mml:math id="inf339"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã—</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> in <inline-formula><mml:math id="inf340"><mml:mi>k</mml:mi></mml:math></inline-formula> training images used to build the color atlas, then <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã—</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector of ones. Thus, this feature specifies that cell <inline-formula><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in image has a high potential of taking label <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in atlas if the Mahalonobis distance between the colors of cell <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and cell <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small.</p></sec><sec id="s21-2"><title>Ensemble atlas with aligned color distributions</title><p>We found that when baseline color data-driven atlas was combined with positional relationship features data-driven atlas to annotate cells, the performance increased marginally over the case when only positional relationship features were used for annotation (<xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1B</xref>). This implies that color information did not contribute to annotation task. We reasoned that this is because distributions of color values vary a lot across images thus color distributions in training data did not reflect the color distribution in test data. This may be due to inherent difference in fluorophore expression across animals, differences in imaging settings across sessions (exposure time, laser power) etc. The problem of different feature distributions in test data compared to training data is solved by domain transfer techniques in machine learning. Here we develop a simple two-step domain transfer method.</p><p>First, we align color distributions in training images used to build the atlas to the color distribution in test image using several methods â€“</p><list list-type="order"><list-item><p>Normalization of each color channel in all images so that pixel values lie between 0 and 1. Let <inline-formula><mml:math id="inf348"><mml:mi>k</mml:mi></mml:math></inline-formula> denote raw intensity value of red color channel (mNeptune channel) at location <inline-formula><mml:math id="inf349"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> in image. Then the normalized values are calculated as<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Similar normalization is performed for CyOFP and BFP channels.</p></list-item><list-item><p>Histogram matching of each color channel in training image to the corresponding channel in test image. Histogram matching transforms the color distribution in an image so that the color histogram matches the color histogram of reference image. This was implemented using â€˜imhistmatchnâ€™ function in MATLAB.</p></list-item><list-item><p>Color invariant transformation of training set images and test image, and subsequent histogram matching of training color invariant images to the test color invariant image. Color invariant transformation transforms the color space of image to remove dependency on lighting geometry and illumination (<xref ref-type="bibr" rid="bib18">Finlayson et al., 1998</xref>). The transformation is performed as follows. Let <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>Ã—</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the matrix that stores RGB values of all voxels in <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> image in the training set. Here <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of voxels in the image. Then the color invariant image is obtained by sequentially and repeatedly normalizing the rows and columns of <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for a fixed number of iterations.</p></list-item></list><p>Note that aligning color distribution of training image to test image does not require cell identity information at all and is performed using all RGB pixel values in the image. After aligning the color distributions, the color atlas is built similar to the baseline color atlas that is by aggregating the RGB values of cells across images. However, in comparison to the baseline case, now RGB values come from the aligned distributions using one of the methods mentioned above.</p><p>Second, an ensemble of data-driven color atlases is used for predicting cell identities that is two data-driven color atlases are used with different color alignment techniques used in each atlas. Feature function in this case was defined as<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mi>l</mml:mi></mml:munder><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean RGB value of the <inline-formula><mml:math id="inf355"><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> cell in test image, <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean RGB value of cell with label <inline-formula><mml:math id="inf357"><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>âˆˆ</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> in <inline-formula><mml:math id="inf358"><mml:mi>k</mml:mi></mml:math></inline-formula> atlas, <inline-formula><mml:math id="inf359"><mml:msup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the covariance matrix of the RGB values of cell with label <inline-formula><mml:math id="inf360"><mml:msub><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in <inline-formula><mml:math id="inf361"><mml:mi>k</mml:mi></mml:math></inline-formula> atlas, and <inline-formula><mml:math id="inf362"><mml:msup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the mixing weight of each atlas, <inline-formula><mml:math id="inf363"><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Note that each atlas is built with using RGB values from all training images.</p><p>In practice we used two atlases in the ensemble atlas. The first atlas used method 2 that is histogram matching of raw RGB distribution in training images to the test image. The second atlas used method 3 that is color invariant transformation was applied to all images (including test image) and subsequently color histogram of training images was matched to the test image. Mixing weights of 0.2 and 0.8 were selected by cross-validation.</p><p>Note that in both cases of color atlas, we do not consider any co-dependent features among cell, thus predicting cell names in this case is equivalent to minimizing the following energy function<disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This is equivalent to maximum weight bipartite graph match problem and thus we used Hungarian algorithm to find the optimal solution (<xref ref-type="bibr" rid="bib31">Kuhn, 1955</xref>).</p></sec></sec><sec id="s22"><title>S2.5 Color + registration</title><p>In this case, we used both color information and position information of cells to predict cell identities. The features used in this case were <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and <inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as described in S2.1 and S2.4. Note that in this case too, we do not use co-dependent cell features. Cell labels were predicted by minimizing the following energy function<disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>âˆˆ</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’´</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here again, we used Hungarian algorithm (<xref ref-type="bibr" rid="bib31">Kuhn, 1955</xref>) to find the optimal solution. By comparing prediction accuracy in this case to the Color only case (S2.4), we can gauge the contribution of position information of cells on prediction accuracy. Here again, we accounted for missing cells in images by predicting the identities iteratively (similar to S2.1). In this case too, we observed improvement in prediction accuracy by accounting for missing cells hence all the comparisons are shown for modified case.</p><p>Below, we briefly show how the objective function in (<xref ref-type="disp-formula" rid="equ44">Equation 44</xref>) naturally arises in registration algorithms that combine multiple features such as color in spatial registration method (<xref ref-type="bibr" rid="bib16">Danelljan et al., 2016</xref>). First, as defined in registration methods (<xref ref-type="bibr" rid="bib46">Myronenko and Song, 2009</xref>; <xref ref-type="bibr" rid="bib23">Jian and Vemuri, 2005</xref>) that use only spatial location feature <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of each cell, the probability of observing each cell is (as described in S2.1)<disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Ï€</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="script">ğ’©</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes a spatial mixture component with mean <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and covariance <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. If color is also available then to maximize complete data log-likelihood using EM method, we need to define the joint probability of observing both spatial location <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and color feature <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of each cell, <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Next, we will discuss two possible cases as discussed in <xref ref-type="bibr" rid="bib16">Danelljan et al., 2016</xref> that differ in the way the joint-probability <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined.</p><p>First case, if the color information is considered to be independent of spatial information then the joint probability factorizes as<disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’©</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Ï€</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï€</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the spatial and color mixture components (latent variables) from which <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are drawn. Also, <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï€</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the mixture weights, parameters and density of observing <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from a mixture density. The independence assumption is akin to assuming that distribution of colors observed in images in different cells is not dependent on spatial location of cells. However, this assumption is not valid since in NeuroPAL each cell is color coded thus observed color depends on spatial location of cells.</p><p>In second case, dependence of color on spatial location of cell is accounted for. Further, it is modeled, that for each spatial component (cell), color is drawn from a location specific mixture density. Thus, the joint probability factorizes as<disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Next, we need to define <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to define complete data likelihood. For NeuroPAL, this is easy since each cell is assigned a unique color (<xref ref-type="bibr" rid="bib84">Yemini et al., 2021</xref>) that is for each spatial component (cell) the color density mixture has only 1 component. Thus, <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> = 1.</p><p>With updated definitions, the complete likelihood of data is defined as<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:munder><mml:mo>âˆ</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>âˆ</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:munder><mml:mo>âˆ</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>âˆ</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msubsup><mml:mi>Ï€</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="script">ğ’©</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Now, if <inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined as <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (as described in S2.4) and expected complete data log-likelihood is maximized using the EM method then the M-step is equivalent to maximizing<disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">â„’</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>âˆ’</mml:mo><mml:munderover><mml:mo movablelimits="false">âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo symmetric="true">â€–</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:mrow><mml:mi mathvariant="script">ğ’¯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">â€–</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">G</mml:mi><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, it can be seen that the first term in <xref ref-type="disp-formula" rid="equ49">Equation 49</xref> is equivalent to <xref ref-type="disp-formula" rid="equ44">Equation 44</xref>.</p></sec><sec id="s23"><title>S2.6 Color + Rel. position</title><p>In this case we combined color information of cells with relative position information of cells. Note that in contrast to S2.5, in this case we use co-dependent features (that is pairwise relative position features) in combination with color information, whereas in S2.5, the feature was dependent on cell-specific information (absolute position of cells). The unary feature in this case is same as the Color only method (S2.4).<disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Pairwise features in this case were the same as described in S1.2. Optimization of the objective function in this case was performed using Loopy Belief Propagation algorithm as described in S1.4. Comparing prediction accuracy in this case to the Color + Registration (S2.5) method helps in verifying that the higher accuracy achieved by this method in predicting cell identities is due to co-dependent cell features thus highlighting the importance of such features.</p></sec><sec id="s24"><title>S2.7 Registration + color + Rel. position</title><p>In this case, we combined all the cell independent and co-dependent features in one model. Thus, the unary features in this case were <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as described in S2.4 and S2.1, and co-dependent features were the same as the described in S1.2. Here again, objective function was optimized using Loopy Belief Propagation algorithm as described in S1.4. We simulated this condition to see if prediction accuracy can be boosted by combining the co-dependent position features with registration algorithm. However, in most cases, we saw a decrease in prediction accuracy. This is because the competition between registration term and relative position features term in objective function decreases accuracy.</p></sec></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.60321.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Linking individual neurons to their anatomical names remains a vexing problem that limits efforts to determine patterns of gene expression and neuronal activity at the single-neuron level in <italic>C. elegans</italic>. This paper presents a computational method for solving this problem and, as such, has the potential to advance neurobiology in <italic>C. elegans</italic>. The approach could, in principle, be adapted to any nervous system from which a computational atlas could be derived following the pipeline laid out in this manuscript. The generality of the approach and its ability to incorporate additional datasets in composing the computational atlas, may lend itself to performance improvements by crowd-sourcing datasets.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Graphical Model Framework for Automated Annotation of Cell Identities in Dense Cellular Images&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Ronald Calabrese as the Senior Editor a Reviewing Editor, and three reviewers. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>Whole brain imaging in <italic>C. elegans</italic> allows monitoring the activities of a large fraction of worm neurons during behaviors. However, the identity of each cell in the nerve ring remains largely unknown, prohibiting further analysis that integrates structural and functional information of the worm circuit. This problem has been partially amended by manual annotation from experts. However, this is a time-consuming and error-prone solution when one is dealing with a large dataset. Thus, the reviewers were enthusiastic about this manuscript because the authors take one important step towards automated annotation of neuronal identities. Methods in this work are innovative and attractive. By minimizing energy functions imposed by absolute coordinate differences (unary potential) between images and atlases, as well as differences of the pairwise relationship between neurons, the authors show that their CRF_ID model outperforms SOTA registration model in accuracy, speed, and robustness.</p><p>Essential revisions:</p><p>There were several concerns that necessitate serious revision and clarification. Ultimate acceptance by <italic>eLife</italic> will depend upon how the concerns can be addressed and whether the methodology will be practically useful to others in the field after further clarification. The original reviews are appended, and all concerns should be addressed. Chief among the concerns that emerged on the reviewer consultation were:</p><p>1) The authors were not clear about whether any data were held out in developing the data-driven atlas and how this choice will impact the ultimate utility of the method. Moreover, they were not clear about how this atlas was built â€“ the motivation for building it.</p><p>2) The authors must be clear when synthetic data is being used and which strain is being used for the biological data.</p><p>3) The reviewers found the text inadequate and essentially reviewed the Appendix. For ultimate publication the authors must make the methodology clear to general readers in the main text.</p><p>4) The prediction accuracies of the model vary from case to case and was quite low in the scenario of missing neurons. The authors must state clearly, under the worst scenario such as whole brain imaging without colors, the prediction accuracy of their methods. The prediction accuracies of the CRF model, contingent upon different datasets and different noise levels, should be in a self-contained main figure, not in the supplements.</p><p>5) A key element of these reagents/strains and the annotation is that, strictly speaking, it annotates nuclei (not neurons). The authors need to explicitly tackle the question of whether the method would work with cytoplasmic markers and to properly annotate the strains they did create as using nuclear localization signals.</p><p>6) Important elements of the metrics presented to evaluate the annotation method are poorly described in the manuscript, and it is unclear from the manuscript or from the materials in the code repo the form of the results. By this we mean not the file format, but rather does the algorithm return a single assignment or a ranked list for each cell or something else?</p><p>7) Concerning the spatial location of the neurons used in the ground truth datasets. From inspecting the authors' GitHub repo, it seems that the neurons in the hand-annotated ground-truth Neuropal datasets are not evenly sampled throughout the head. Rather they are enriched in easy-to-label locations, like the anterior of the head, with less coverage in the more difficult locations (e.g. few neurons are labeled in the lateral ganglia or retrovisicular ganglia). The authors should explicitly recognize this bias in the hand-annotated ground-truth dataset.</p><p><italic>Reviewer #1:</italic></p><p>The application of conditional random fields to neuron registration in <italic>C. elegans</italic> represents a significant advance that will impact the field. I have major concerns about the interpretation of the method's performance: some technical, for example test sets were seemingly not held out from the data-driven atlas; and others more related to confusing or potentially misleading presentation. It was often unclear which dataset, method, reagent or performance metric (for example Top 1 vs Top 5) was being discussed. I also have concerns about giving appropriate credit for work and reagents from other groups.</p><p>1) Test sets used to assess performance do not seem to have been held out from the data-driven atlas, which raises doubts about the interpretation of Figure 2A and Figure 6E and C. This could artificially inflate performance compared to tests with a held-out animal, which would reduce the utility of the method. This could also explain the dramatic difference in performance observed fr om synthetic data with experimentally derived noise (35% accuracy) Figure 2â€”figure supplement 4A, and real experimental data matched to the data-driven atlas (&gt;75% accuracy Figure 2A).</p><p>2) Lack of clarity could falsely give readers the impression of higher performance:</p><p>a) It is often ambiguous as to whether Top 1 or Top 5 performance is being plotted. Example: in Figure 3B the caption makes no mention, so one would assume Top 1 accuracy; but subsection â€œCell annotation in gene expression pattern analysisâ€ suggests Top 5. This is a crucial distinction and needs clarity in every subpanel.</p><p>b) It is also unclear when synthetic data is being used. Example: subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€ states that the algorithm &quot;maintains high accuracy even when up to 75 % of cells in atlas were missing for data&quot; but from the figures I gather this is synthetic data with unnaturally low position noise, so this could give the reader the wrong impression.</p><p>c) Similarly, it is often unclear from the figures which strain is being used. For example the strain in Figure 4A-C is not labeled, but it appears to be the same as Figure 3A,B. But if that's the case its unclear why performance would be so much higher in Figure 4B than Figure 3B. Please label which strain is used in which figure subpanel throughout the manuscript, or provide a table.</p><p>d) Related: Figure 4A prominently displays &quot;calcium imaging,&quot; when this is in fact a GFP strain. This misleads the reader</p><p>3) The work critically relies on reagents from other groups but could do a better job acknowledging those contributions:</p><p>a) The Neuropal strain is crucial to the manuscript and provides the majority of the ground-truth data starting with the Results and Figure 2, but Neuropal is first mentioned only towards the very end of the Results section and even then only in the context of Figure 6.</p><p>b) The strain list should reference publications for strains and alleles used.</p><p>c) Subsection â€œCell annotation in gene expression pattern analysisâ€ misleadingly states that &quot;a reporter strain was crossed with pan neuronal red fluorescent protein&quot; but this cross was not performed by the authors, it was performed by another group, published and is publicly available on CGC. This should be fixed.</p><p>4) More details are needed about the ground truth datasets, including the number of neurons hand-annotated in each of the 9 individuals and their spatial location. (Manual annotation of Neuropal is easiest at the periphery and hardest in center of the ganglia, and so it would be important to know coverage).</p><p><italic>Reviewer #2:</italic></p><p>Whole brain imaging in <italic>C. elegans</italic> allows us to monitor the activities of a large fraction of worm neurons during behaviors. However, the identity of each cell in the nerve ring remains largely unknown, prohibiting further analysis that integrates structural and functional information of the worm circuit. This problem has been partially amended by manual annotation from experts. However, this is a time-consuming and error-prone solution when one is dealing with a large dataset. We are enthusiastic about this manuscript because the authors take one important step towards automated annotation of neuronal identities. Methods in this work are innovative and attractive. By minimizing energy functions imposed by absolute coordinate differences (unary potential) between images and atlases, as well as differences of the pairwise relationship between neurons, the authors show that their CRF_ID model outperforms SOTA registration model in accuracy, speed, and robustness.</p><p>We would like to raise several main issues of this manuscript.</p><p>1) Writing. The main text of the manuscript is brief, dense, technical, and very difficult to understand. I skipped the main text and went directly to the Appendix. This arrangement is not wise for a general audience, especially biologists. In future revisions, please embed the figures in the text, which would save us a lot of time. Here are some specific suggestions:</p><p>a) The CRF framework can be better explained by building upon physical intuition. The mathematical notations sometimes only cause confusion. For example, is it necessary to introduce clique? I thought that every neuron in their graph model is adjacent to every other neuron. If this is not the case, please describe how the graph model is constructed. You find the note on maximum entropy model is out of the blue (subsection â€œStructured prediction frameworkâ€). If you want to make a deep connection with statistical physics, you better explain it well in the text!!</p><p>b) The motivation to build and how to build the data-driven atlases are dismissive in the main text. It is briefly mentioned in subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€. But these sentences are extremely difficult to understand without reading the Appendix. The Appendix -- extended methods S1.7, however, needs to be revised and better explained. For example, the equations are not quite consistent, such as in Equations 6 and 25, and the notations are quite confusing. Linking some examples like Figure 2B to S1.7 will help understand the concept.</p><p>c) The prediction accuracies of the model vary from case to case. It was quite low in the scenario of missing neurons. The authors need to state clearly in their paper, under the worst scenario such as whole brain imaging without colors, the prediction accuracy of their methods. This information is now buried somewhere that is hard to find. The prediction accuracies of the CRF model, contingent upon different datasets and different noise levels, would better be a self-contained main figure, not in the supplements. To follow the logic flow of the text, the figure panels that compared CRF with other registration methods would better be introduced later.</p><p>2) Notations. As mentioned above, the symbol consistency is a problem. In the formulation of the model, Î»'s, trainable parameters in original CRF, have been abused. For the Î» appearing in Equations 2/3/4/6/7/8/9/10/32/34/37/46, the author might consider making a clear statement that which of them are trainable and which are hyperparameters. If they are hyperparameters, the setting and tuning strategies need to be disclosed. Also, the keywords &quot;identification&quot; and &quot;annotation&quot; should be considered unified.</p><p>3) Loop Belief Algorithms, speed, and stability. The authors should provide details on the computational complexity and the stability of their algorithms and compare it to other registration methods. In their current graph model, does each trial converge and do they all converge to the same solution starting from different initial conditions? If not, how do the authors deal with the discrepancies? Likewise, the authors may want to comment on the memory usage for constructing the data-driven atlases to see if it can be another innovation point comparing to the registration methods.</p><p>4) Other missing information and explanations:</p><p>a) The authors should post the exact name and its settings of the registration models used for comparison.</p><p>b) In Appendix S1.2.1 Unary potentials â€“ Positions along AP axis, as x-y is a 2D plane, we do not see why AP's is always better than LR's and DV's.</p><p>c) Following the explanation of generating synthetic data (subsection â€œGenerating synthetic data for framework tuning and comparison 464 against other methodsâ€) is a bit hard. The reason for changing the low bound to zero might need further illustrations. Could you please comment on why the prediction accuracy is much lower in synthetic data (Figure 2A)?</p><p>d) For subsection â€œWhole-brain data analysisâ€, the author might consider linking the paragraph to Figure 5, making 0.1Hz a reasonable choice.</p><p>e) For subsection â€œWhole-brain data analysisâ€, &quot;[0.05, 0.05]&quot; seems to be a mistake.</p><p>f) Subscripts in Equation 13 are wrong.</p><p>g) Providing more annotations and explanations on the meaning of the colors on some worm images (e.g., Figure 4D) would be helpful.</p><p>5) Codes on GitHub may need to be slimmed down.</p><p><italic>Reviewer #3:</italic></p><p>Linking individual neurons to their anatomical names is a vexing problem that limits efforts to determine patterns of gene expression and neuronal activity at the single-neuron level. Given the landmark Mind of the Worm paper (White et al., 1986) and subsequent analyses of its connectome, many imagine that annotating individual neurons in <italic>C. elegans</italic> is a solved problem. Unfortunately, it is not. This paper presents a computational method for solving this problem and, as such, has the potential to advance neurobiology in <italic>C. elegans</italic>. The approach could, in principle, be adapted to any nervous system from which a computational atlas could be derived following the pipeline laid out in this manuscript. There is much to recommend this study and many opportunities for improvement regarding the presentation of the computational method, its limitations and expectations for application, and for considerations of generalization to other nervous systems.</p><p>1) Explanation of the computational method</p><p>The computational strategy is difficult to follow in the main text, unless one reviews the appendices. We suggest elaborating the process using more generic terms in the main text. The following specific elements need clarification or additional support</p><p>- The mixed approach of considering intrinsic and extrinsic similarities between image and atlas is described in a manner that assumes readers are already familiar with this computational strategy. Given the breadth of the <italic>eLife</italic> readership, the authors would increase the accessibility and impact of the paper by providing readers with examples of what features are intrinsic and which are extrinsic in this case.</p><p>- The difference between this approach and previous strategies based on image registration is communicated clearly in the Appendix, but not in the main text.</p><p>- Much is made of the computational efficiency of the CRF_ID approach, but little is provided in the way of data or analysis supporting this claim. The authors could, for example, compare the computational energy/time needed for this approach compared to prior image matching approaches. They could discuss the nature of this computational framework compared to others with respect to computational efficiency.</p><p>Generality of the method</p><p>To better support the authors claim that this method is easily adaptable for any nervous system, please apply the framework to estimate the optimal number (and/or density) of landmarks as a fraction of the total number of nuclei or density in 3D. This would assist others in applying the framework to other simple nervous systems (e.g. <italic>Aplysia</italic>, leech, hydra). Additionally, it is not clear if the image stacks used to derive data-driven atlases are distinct from the data analyzed for prediction or whether the same data is used iteratively to build atlases and improve prediction accuracy.</p><p>Application of the method to <italic>C. elegans</italic></p><p>Specifying the AP/DV/LR axes from raw data is not completely automatic but rather appears to require some user input based on prior knowledge. This is not stated clearly in the text. In addition, it is also not clear how the algorithm handles data from partially twisted worms (non-rigid rotation around the AP-axis). This problem is more likely to be encountered if the images capture the whole worm. Is it possible to allow rotation of the DV and LR axes at different points along the AP axis?</p><p>Specific suggestions for improvement are collected below.</p><p>(1) The method depends on the tidy segmentation of nuclei, but the segmentation strategy is not discussed nor is whether or not nuclear localization of landmarks or unknown markers is essential for accurate annotation.</p><p>(2) The authors need to explicitly inform readers that the markers in the NeuroPal strains (OH15495, OH15500) and the GT strains are localized to neuronal nuclei. Additionally, the authors need to address (1) whether or not nuclear localization is required for the annotation framework to succeed, and (2) discuss whether or not a cytoplasmic marker would be sufficient, and (3) provide more information on the segmentation algorithm used and recommended for use with the computational framework discuss.</p><p>(3) The authors use the AML5 strain as a proxy for GCaMP-labeled strains (subsection â€œCell annotation in multi-cell functional imaging experimentsâ€) to demonstrate the utility of the annotation tool for identifying cells used for calcium imaging. This strain differs from the NeuroPal strain expressing GCaMP is several respects that are not discussed by the authors, leaving readers to wonder about the utility of AML5 as a proxy and to question whether or not the success of cell identification in this strain is truly generalizable. Two main differences exist: (1) In AML5, GFP is expressed in both the nucleus and cytoplasm but in in OH15500 GCaMP6s is tagged with an NLS; (2) The basal fluorescence of GCaMP6s, but not GFP depends on basal calcium concentrations-this fact may introduce additional detection noise/variance into the annotation problem. The authors need to discuss how these differences affect (or don't) the ability to generalize from success with the proxy strain to success with NLS::GCaMP6s expression.</p><p>(4) Please clarify the nature of the output from the code that implements the framework. In other words, a reader should learn from the paper what kind of output file they might receive if they attempted to apply the tool to their own images (annotated image with top prediction candidate, data structure with all predictions and respective probabilities, etc).</p><p>(5) Terminology â€“ There are a number of general computational terms and specific terms relevant to this problem that are not defined for the reader (listed below). Providing readers with implicit or explicit definitions of these terms will improve clarity.</p><p>5(a) &quot;count noises&quot; â€“ used to represent variation in the number of nuclei detected in a given image stack, but not defined at first use. Please provide conditions under which the number of nuclei detected might vary along with defining this term. Additionally, this is a singular concept and should be written as &quot;count noise&quot; not &quot;count noises&quot; (by analogy to &quot;shot noise&quot;).</p><p>5(b) There several other instances in which &quot;noises&quot; is used where &quot;noise&quot; is meant.</p><p>5(c) &quot;intrinsic similarity&quot; â€“ which seems to represent pairwise positions in 3D in the image and the atlas.</p><p>5(d) &quot;extrinsic similarity&quot; â€“ not clear.</p><p>(6) (Subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€) &quot;More importantly, automated annotation is unbiased and several orders of magnitude faster than manual annotation by researchers with no prior experience.&quot;</p><p>(a) Unbiased â€“ It is a common misconception that computational strategies are unbiased. For this claim to be rigorously accurate, the authors would need to demonstrate that each and every nucleus was equally likely to be identified correctly by the automated annotation algorithm. For instance, it is easy to imagine that some nuclei, such as those closer to a landmark, are more likely to identified correctly and with less uncertainty than those further away. If this were true, then automated annotation has a systematic bias in favor of nuclei closer to landmarks and a systematic bias against nuclei further away from landmarks. And would, therefore be biased â€“ even though no human supervision is involved. The authors should back this claim up with data or clarify that what they mean is that automated annotation is an improvement over (biased and subjective) human annotation.</p><p>(b) Faster -The authors are free to speculate about the relative speed of automated vs. manual annotation by an inexperienced researchers in the discussion. By writing this as a result, however, data are needed to back it up.</p><p>(7) (Materials and methods) From the description of how GT290 and GT298 were made, it seems that both constructs include 2x NLS fragments, but the genotype description under reagents lacks that annotation and most, if not all other references to this strain neglect to mention the NLS.</p><p>Presentation concerns that need attention: There are three instances of undisclosed data re-use among the figures that need to be addressed. (1) The image in Figure 4A is identical to the green channel of the image shown in Figure 3A; (2) Some of the data shown in Figure 2â€”figure supplement 4A â€“ Part of the data shown here is also shown in Figure 2C; (3) Figure 4C â€“ Please indicate if the same number of cells were removed in each run. Also, it appears that this data also may have been used in Figure 4â€”figure supplement 1. If so, please disclose data re-use.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Graphical-Model Framework for Automated Annotation of Cell Identities in Dense Cellular Images&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Ronald Calabrese as the Senior Editor, a Reviewing Editor, and two reviewers. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>(1) The authors performed additional computational experiments by holding out some of the data used to create atlases â€“ an important and general strategy for computational solutions that depend on real-world data. The authors present a comparison of this approach for reviewers, but not for explicitly for readers. These analyses and plots must be included in the final version main text.</p><p>(2) The authors should state clearly the computational speed of their algorithms and compare it with the registration methods. The inference and subsampling procedures (~ 1000 times?) on the data-driven atlas are time-consuming and computationally expensive.</p><p><italic>Reviewer #2:</italic></p><p>Main Text</p><p>(1) Thanks for sending us a much better written manuscript. The authors should consider replotting some of their figures, which can be difficult to read.</p><p>(a) The neuron names in Figure 2F and Figure 2â€”figure supplement 6C are difficult to read. Is there a new way to plot? When I wanted to zoom in to have a better understanding, they caused my Preview crashed multiple timesâ€¦</p><p>(b) The meaning of the error bars throughout the manuscript are not well explained.</p><p>(2) The authors should state clearly the computational speed of their algorithms and compare it with the registration methods. The inference and subsampling procedures (~ 1000 times?) on the data-driven atlas are time-consuming and computationally expensive.</p><p>(3) Is there a way to make the data-driven atlas publicly available in addition to the raw annotated datasets (n=9)?</p><p>Rebuttal</p><p>(1) Pipeline (Major Point#6):</p><p>I do not quite understand the necessity of single or multi-run, which can be combined into one inference process. Consider the following scenario in a freely behaving animal. At one time, the region to be annotated consists of N neurons. At a different time, a new neuron (now N+1) is squeezed into the region. It is probably always true that the number of neurons in the data is different from that in the atlas.</p><p>(2) Color and Robustness and Evaluation Metrics (Essential revisions #1, #4, #6):</p><p>It seems that the leverage of color information for annotation is quite limited in the revised manuscript. The procedures for properly normalizing the color distribution are also complicated. I also found that the following sentence was incorrect. &quot;Further, when leave-one-out atlases are used for both positional relationship and color (Author response image 2), the accuracy achieved by top labels is marginally greater than the accuracy achieved by using leave-one-out positional relationship only (Author response image 1)&quot;. It was actually significantly lower if I read the figures correctly. This leaves the question how we shall correctly assemble the data-driven atlas efficiently.</p><p><italic>Reviewer #3:</italic></p><p>The revision is very responsive to the summary and specific reviewers' critiques.</p><p>There remain some awkward phrasing and missing details to address. These are noted in order to make the text accessible to a broader audience and to increase the impact of the study both within and beyond the community of <italic>C. elegans</italic> researchers. These include:</p><p>(1). The use of the plural &quot;noises&quot; when the authors probably mean either &quot;noise&quot; in the singular e.g. â€ Prediction accuracy is bound by position and count 316 noise in dataâ€, replace &quot;amount of various noises&quot; with &quot;amount of noise contributed from various sources&quot;. This latter example combines both the concept of the magnitude of noise and the diverse types of noise.</p><p>(2) Shorthand terms or jargon. For example, when the text reads &quot;the position of cells AIBL&quot; what is meant is &quot;the position of the cell bodies of AIBL&quot;. Similarly, replace &quot;neurons&quot; with &quot;neuronal cell body&quot; or &quot;neuronal somata&quot;.</p><p>(3) <italic>C. elegans</italic> strains â€“ in response to critiques of the initial submission, the authors have clarified the origin of the transgenic strains used in the study. Thank you! Please also correct the reference to AML70 which is not included in the table of strains. Based on context, it looks like the authors are referring to AML32. Please ensure that this is corrected.</p><p>(4) Figure 5, panel E. Please refer to Figure 5â€”figure supplement 1 panel C noting the cells with non-zero weights in SPC1, SPC2, SPC3</p><p>(5) Figure 5 panel H: Indicate the origin of the motion trace shown. It appears to belong to one of the posterior cells shown in Panel F. Please provide a rationale for this choice â€“ since the traces in Panel F certainly indicate that motion signals differ among cells.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.60321.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>There were several concerns that necessitate serious revision and clarification. Ultimate acceptance by eLife will depend upon how the concerns can be addressed and whether the methodology will be practically useful to others in the field after further clarification. The original reviews are appended, and all concerns should be addressed. Chief among the concerns that emerged on the reviewer consultation were:</p><p>1) The authors were not clear about whether any data were held out in developing the data-driven atlas and how this choice will impact the ultimate utility of the method. Moreover, they were not clear about how this atlas was built â€“ the motivation for building it.</p></disp-quote><p>We appreciate the reviewers for pointing these out. These points are indeed very important and were an oversight in the initial submission. We have since tested the model with held out data (detailed below) and added more text to emphasize the motivation for building the atlas main text, Appendix â€“ extended methods S1.7.</p><p>We present new cell identity prediction results with held out test data. In all new results presented below, we built the atlases with all but one data set (one worm image stack) and used the held out set for testing the accuracy; thus, the atlas each time is a leave-one-out atlas. We then report the average and spread of these predictions. In the text below, we explicitly compare what was reported in the initial manuscript and what is reported in the revision. It is important to note that with the new results, all conclusions stand.</p><p>First, for Figure 2A where we build only positional relationship atlases, here we predict identities in experimental whole-brain datasets using data driven atlases. Note in this case, no color information is used for prediction. With leave-one-out-atlases, we see no difference in performance compared to previous results (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>). This implies that leave-one-out atlases are able to represent positional relationships among cells in test data.</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Accuracy comparison on experimental datasets when prediction was done using atlas built with all data vs leaveone-out atlases.</title><p>Prediction was done using positional relationship information only. This figure is update for Figure 2A. Experimental datasets come from NeuroPAL strains.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-resp-fig1-v2.tif"/></fig><p>Second, for Figure 6C, in contrast to Figure 2A, we build both positional relationship atlases and color atlases, <italic>i.e.</italic> we use color information along with positional relationship information to predict identities. We tested accuracy for several kinds of atlases: (1) leave-one-out-positional and allcolor atlas â€“ using only positional relationship leave-one-out atlas and color information comes from all datasets including test data; (2) all-positional and leave-one-out-color atlas â€“ using only color leave-one-out-atlas and positional relationship information comes from all datasets including test data; (3) leave-one-out-positional-and-color atlas â€“ using both positional relationship and color leave-one-out atlases (<xref ref-type="fig" rid="respfig2">Author response image 2</xref>).</p><p>We found that using color leave-one-out atlases decreases accuracy more sharply than using positional relationship leave-one-out atlas. Further, when leave-one-out atlases are used for both positional relationship and color (last bar in <xref ref-type="fig" rid="respfig2">Author response image 2</xref>), the accuracy achieved by top labels is marginally greater than the accuracy achieved by using leave-one-out positional relationship atlas only (second pair of bars in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>, and Figure 2A). With the data sets at hand, using color does not contribute much to improving accuracy; this implies that the color variability among animals is far greater than the positional variability.</p><p>We see that raw RGB intensity distribution in test image data is different from RGB intensity distribution in atlas. This could be due to several reasons, including inherent differences in NeuroPAL fluorophore expression levels across animals, and differences in image acquisition settings across sessions (exposure times, laser power) etc. As a consequence, raw RGB values that represent colors of cells vary across animals. NeuroPAL manuals also suggest manual contrast and Î³ adjustment of color channels in images before performing manual annotation of cell identities. Thus, to be able to use color information for automatic annotation we need to first align distribution of features (RGB values) in atlas to distribution of features in test data, also commonly known as domain adaptation problem in machine learning.</p><p>In order to increase the utility of the color information, either a much larger dataset would need to be acquired to account for the variability, or that the atlas needs to be built based on images with as close to the experimental conditions as the data to be analyzed (which in our estimates is not a trivial task).We have elaborated on these points in the manuscript.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Accuracy achieved on experimental datasets when prediction was done using different combinations of atlases for positional relationships and color.</title><p>In these cases, leave-one-out atlas was used for either positional relationship, color or both. Experimental datasets come from NeuroPAL strains.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-resp-fig2-v2.tif"/></fig><p>To test whether we can improve the utility of the color information, we explored several simple methods to align color values of images used for building leave-one-out color atlas to color values in test data. Note that color matching is performed using full RGB distributions in image <italic>without</italic> using any cell identity information. The methods we tried included simple normalization (norm) of color channels, color constancy/invariant transformation[1] (colconst), histogram matching (histm), contrast and Î³ adjustment (imadj), and a combination of these methods (<xref ref-type="fig" rid="respfig3">Author response image 3</xref>). For each method we found that accuracy was improved for some data sets but not all datasets.</p><fig id="respfig3"><label>Author response image 3.</label><caption><title>Accuracy on experimental datasets when prediction was done using different leave-one-out atlases for color.</title><p>In each method, a different technique was used to match colors of images used to build atlas to colors of test image. Leave-one-out atlas for both positional relationships and color was used for prediction. Experimental datasets come from NeuroPAL strains.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-resp-fig3-v2.tif"/></fig><p>Finally, we found that using an ensemble of two methods for color atlas gave best accuracy results (<xref ref-type="fig" rid="respfig4">Author response image 4</xref>), improving top label accuracy by 6% (with an average of 80.5%) compared to the baseline case of color leave-one-out atlas (average of 74.5%). Here, ensemble of two methods means using a combination of two different leave-one-out color atlases with different color alignment manipulations performed in each for predicting cell identities in test data. Weights of 0.2 and 0.8 were assigned to the two atlases in ensemble (determined by cross validation). The methods included in the best ensemble were (1) histogram matching (i.e. histm), and (2) color invariant manipulation, normalization and subsequent histogram matching (i.e. colconst + norm + histm). We have updated the ensemble methods results in Figure 6C, Figure 6â€”figure supplement 2 and updated the text.</p><fig id="respfig4"><label>Author response image 4.</label><caption><title>Accuracy comparison between base case (when prediction was done using leave-one-out color atlas without any color matching) and ensemble of two leave-one-out color atlases (with different color matching techniques).</title><p>In both cases, same leave-one-out atlases for positional relationships were used. Experimental datasets come from NeuroPAL strain (n = 9). Results in this figure are updated in Fig. 6C.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-resp-fig4-v2.tif"/></fig><p>Using ensemble of leave-one-out-color atlases also led to improvement in accuracy in non-rigidly rotated animal datasets (related to Figure 6E), improving accuracy of top labels from 39.1% (base case) to 48.8% (ensemble atlases with color matching) (<xref ref-type="fig" rid="respfig5">Author response image 5</xref>). We have updated these results in Figure 6E, Figure 6â€”figure supplement 3 and updated the text.</p><fig id="respfig5"><label>Author response image 5.</label><caption><title>Accuracy comparison between base case (when prediction was done using leave-one-out color atlas without any color matching) and ensemble of two leave-one-out color atlases (with different color matching techniques).</title><p>In both cases, same leave-one-out atlases for positional relationships were used. Experimental data comes from NeuroPAL strains with non-rigidly rotated animals (n = 7). Results in this figure are updated in Fig. 6E.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-60321-resp-fig5-v2.tif"/></fig><p>We also updated results in Figure 6B, where we compare accuracies of several methods on experimental whole-brain datasets. In this figure, for fair comparison across methods OpenWorm atlas was used for positional relationships. For, methods that use color information (i.e. â€˜Colorâ€™, â€˜Reg. + Colorâ€™, â€˜Color + Rel. Positionâ€™, and â€˜Reg. + Color + Rel. Positionâ€™ methods), data-driven color atlas was built. Now we perform predictions using ensemble of leave-one-out color atlases for methods that use color information. Note that the conclusion remains same â€“ CRF model with positional relationship features along with color outperforms other methods.</p><p>We have updated Appendix â€“ extended methods section S2.4 with description of building baseline color atlas, building color atlas after matching color distributions in training data to test data and using ensemble of color atlases for prediction.</p><p>Thus, we conclude that in order to be able to utilize color information of cells in NeuroPAL for automatic annotation of cell identities, color consistency across animals needs to be maintained, either experimentally or by post hoc corrections. Experimentally, consistent protocol/imaging settings across sessions should be maintained. Even with consistent protocol, color variation may exist. This can be tackled by (1) collecting large volume of data to capture each cellsâ€™ full RGB variations, and (2) using computational domain adaptation techniques. More advancement in image color transfer and domain adaptation techniques will further improve accuracy in future. We have updated these discussions in the text.</p><p>[1] Finlayson, Schiele and Crowley. 1998.</p><p>To clarify the motivation and methodology of building data-driven atlas, we have now rewritten several sections and included an additional schematic figure â€“</p><p>1) We have text in the Introduction section that talks about the variability of the cell positions in the data which is the main motivation for building such data-driven atlas.</p><p>2) We have re-written the â€œCell annotation using Structured prediction frameworkâ€. Here we describe the motivation and intuition behind building data-driven atlas</p><p>3) We have re-written the section â€œAppendix â€“ extended methods S1.8 Building data-driven atlasâ€. This section further expands on the intuition and methodology of building data driven atlas for positional relationship features.</p><p>4) We have added new details in the section â€œAppendix â€“ extended methods S2.4â€ to describe the methodology of building data driven atlases for color features.</p><p>5) We have added a new figure as Figure 1â€”figure supplement 1 that schematically explains how the positional relationship features in the model are modified when data driven atlas is used compared to when static atlas is used.</p><disp-quote content-type="editor-comment"><p>2) The authors must be clear when synthetic data is being used and which strain is being used for the biological data.</p></disp-quote><p>We have now indicated synthetic data or experimental data in each figure subpanel. Strain being used is mentioned in Figure legends. Following figures and figure legends have been changed: Figure 2, Figure 3, Figure 4, Figure 5, Figure 6, Figure 2â€”figure supplement 1, Figure 2â€”figure supplement 2, Figure 2â€”figure supplement 3, Figure 2â€”figure supplement 4, Figure 2â€”figure supplement 5, Figure 2â€”figure supplement 6, Figure 4â€”figure supplement 1, Figure 5â€”figure supplement 1, Figure 6â€”figure supplement 1, Figure 6â€”figure supplement 2, Figure 6â€”figure supplement 3.</p><disp-quote content-type="editor-comment"><p>3) The reviewers found the text inadequate and essentially reviewed the Appendix. For ultimate publication the authors must make the methodology clear to general readers in the main text.</p></disp-quote><p>We have rewritten the subsection â€œCell annotation formulation using structured prediction frameworkâ€ in main text, mainly by moving some of the prior text from the appendix and streamlining the description.</p><p>1) We describe the methodology to make it more accessible to readers.</p><p>2) We describe the motivation and intuition behind building data-driven atlases (subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€, Appendix â€“ extended methods S1.7)</p><p>3) We have added a new schematic figure as Figure 1â€”figure supplement 1. This figure clearly explains (1) all features in the CRF model, (2) difference between intrinsic similarity and extrinsic similarity features, and (3) how features are updated with data driven atlas.</p><p>We realize that this beginning portion of the text may be math-heavy, but left it as such for the sake of clarity as the reviewers proposed.</p><disp-quote content-type="editor-comment"><p>4) The prediction accuracies of the model vary from case to case and was quite low in the scenario of missing neurons. The authors must state clearly, under the worst scenario such as whole brain imaging without colors, the prediction accuracy of their methods. The prediction accuracies of the CRF model, contingent upon different datasets and different noise levels, should be in a self-contained main figure, not in the supplements.</p></disp-quote><p>We would like to reiterate a few important points that were perhaps obscured in the original manuscript. First, the models we built are data-driven. This means that the accuracy of each model depends on the quality (e.g. signal-to-noise ratio), information content (e.g. availability of labeling of positional information, color information), and quantity of the data. We do not build a single model because the use-case of these models are strongly dependent on the biological questions asked where different reagents are used (e.g. different markers). Second, the accuracy of the models will strongly depend on the information contained in the data that were used to build/train the model. In other words, a model that is built from data of higher quality, more information content, and a larger data set will perform better. Another way to look at this is that models cannot pull information out of thin air (bad data with little information).</p><p>In general, missing neurons in the data strongly erode the performance of the models, regardless of the methods used. How exactly missing neurons influence the model performance depends on the information content of the neurons missing. To give an example, neurons that can anchor the AP, DV, and LR axes are highly â€œvaluableâ€; if they are randomly missing in the simulations, they will have a larger impact than some other neurons. For each scenario that we tested, we ran at least 180-200 random trials to assess the variability.</p><p>As we show in the sections â€œPrediction accuracy is bound by position and count noiseâ€ and â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€, accuracy of all automatic annotation methods will be degraded by the noise in data. We formalize the concept of types of noise encountered in automatic cell annotation task, i.e. position noise and count noise, and study the effect of these noises on accuracy across a range of noise levels.</p><p>Our motivation for testing accuracy across a range of noise levels comes from the fact that the amount of noise that will be encountered practically in experimental data will depend on the use case, and thus affecting the accuracy results. For example, in gene expression pattern analysis, the amount of count noise will be huge since there is no candidate label list available for annotation. In comparison, in multi-cell calcium imaging case, the amount of count noise will be much lower because candidate labels that can be assigned to few cells in image will already be available. Similarly, the effect of position noise on accuracy in whole-brain imaging case (densely packed cells) will be more pronounced than multi-cell calcium imaging case (sparsely labelled and spatially distributed cells). Thus, by systematically varying the noise levels using synthetic data, we show that CRF method outperforms popular registration based methods. The synthetic data illustrate the point about the information content in the data. Again, we emphasize that noise in the data such as missing neurons will affect any method; no one is immune to the problems caused by missing information, and no algorithm can make up information that is not in the data. The important point is whether an algorithm can outperform other algorithms, and get the maximum information from the available data.</p><p>Figure 2A is â€œthe worst scenario such as whole brain imaging without colors (of) the prediction accuracy of (the) methodsâ€. New results with leave-one-out atlas are added for the updated Figure 2A.</p><p>We have moved the figures and figure supplements closer to the text. We compare the accuracies of methods across different noise levels using synthetic data in Figure 2â€”figure supplement 5. Since these comparisons are performed using synthetic data, we felt that it should remain as a supplement. Figure 2C-E does contain the major take-away message of the comparisons, i.e. CRF methods performs better than registration based methods in handling both position and count noise. Further it demonstrated superior performance of experimental datasets.</p><disp-quote content-type="editor-comment"><p>5) A key element of these reagents/strains and the annotation is that, strictly speaking, it annotates nuclei (not neurons). The authors need to explicitly tackle the question of whether the method would work with cytoplasmic markers and to properly annotate the strains they did create as using nuclear localization signals.</p></disp-quote><p>We agree with the reviewers that the algorithm is annotating nuclei of neurons currently for whole-brain data (because the datasets are all labeling nuclei). The algorithm should work with cytoplasmic markers, as long as cells can be demarcated as separate entities (e.g. results shown in Figure 3B, Figure 4B, 4C are cytoplasmic markers). For whole-brain imaging, the cytosolic markers will make a bunch of cells look like one big blob, unless perhaps when cell membranes are labeled, in which case, our algorithm should work.</p><p>We also note that if the cells cannot be distinguished as separate entities (by eye or by segmentation algorithms), then this will contribute to the count noise that we have characterized in the paper; we have shown that CRF performs better than registration method in handling count noise.</p><p>To clarify, our landmark strains contain nuclear localized landmarks for whole-brain imaging application. All nuclei are annotated as shown in Figure 4D.</p><disp-quote content-type="editor-comment"><p>6) Important elements of the metrics presented to evaluate the annotation method are poorly described in the manuscript, and it is unclear from the manuscript or from the materials in the code repo the form of the results. By this we mean not the file format, but rather does the algorithm return a single assignment or a ranked list for each cell or something else?</p></disp-quote><p>We appreciate this comment on clarify. There are two modes of running algorithm: (1) Single run returns top label for each cell; this works when the number of segmented cells/nuclei matches the number of labels available to assign. (2) Accounting for missing neurons returns top candidate for each cell; this requires multiple runs, which we performed in parallel on computing clusters; since neurons missing will dependent on the confidence of knowing which neurons may not show up with the particular reagents (and experimental conditions for that matter), we also report additional top labels (e.g. top 3 and top 5) for users to potentially take advantage of heuristics or prior knowledge that are not hard-coded into the algorithm. We have now described the code output in subsection â€œComputational workflow for cell identificationâ€. We have also now indicated, in each of the corresponding figure legends, which metric is being shown in the figure.</p><disp-quote content-type="editor-comment"><p>7) Concerning the spatial location of the neurons used in the ground truth datasets. From inspecting the authors' GitHub repo, it seems that the neurons in the hand-annotated ground-truth Neuropal datasets are not evenly sampled throughout the head. Rather they are enriched in easy-to-label locations, like the anterior of the head, with less coverage in the more difficult locations (e.g. few neurons are labeled in the lateral ganglia or retrovisicular ganglia). The authors should explicitly recognize this bias in the hand-annotated ground-truth dataset.</p></disp-quote><p>We agree with this comment. NeuroPAL is at the moment the best available data for neuron identity ground truth since it offers more neurons labeled than other strains available. Since we are not experts in neuroPAL, we tried our best at annotating the ground truth and may be biased in the cells identified. Additional details of number of cells in each of the anterior, middle, and posterior regions of head ganglia annotated in datasets set are provided below. We have added these details in the manuscript as Figure 2â€”figure supplement 3. As can be seen, we have good coverage in anterior and lateral ganglions. In our experience, annotating cells in the Retrovesicular ganglion (included in posterior region in our analysis) and the Ventral ganglion (included in middle region in our analysis) is the hardest. In animals with rotated orientations we have lower coverage because manually identifying neurons in this case is more difficult. As the reviewers said, the more data available the better the eventual model for prediction.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The application of conditional random fields to neuron registration in <italic>C. elegans</italic> represents a significant advance that will impact the field. I have major concerns about the interpretation of the method's performance: some technical, for example test sets were seemingly not held out from the data-driven atlas; and others more related to confusing or potentially misleading presentation. It was often unclear which dataset, method, reagent or performance metric (for example Top 1 vs Top 5) was being discussed. I also have concerns about giving appropriate credit for work and reagents from other groups.</p><p>1) Test sets used to assess performance do not seem to have been held out from the data-driven atlas, which raises doubts about the interpretation of Figure 2A and Figure 6E and C. This could artificially inflate performance compared to tests with a held-out animal, which would reduce the utility of the method. This could also explain the dramatic difference in performance observed fr om synthetic data with experimentally derived noise (35% accuracy) Figure 2â€”figure supplement 4A, and real experimental data matched to the data-driven atlas (&gt;75% accuracy Figure 2A).</p></disp-quote><p>Please see our explanation above (#1 in the overall response).</p><disp-quote content-type="editor-comment"><p>2) Lack of clarity could falsely give readers the impression of higher performance:</p><p>a) It is often ambiguous as to whether Top 1 or Top 5 performance is being plotted. Example: in Figure 3B the caption makes no mention, so one would assume Top 1 accuracy; but subsection â€œCell annotation in gene expression pattern analysisâ€ suggests Top 5. This is a crucial distinction and needs clarity in every subpanel.</p></disp-quote><p>Please see our explanation above (#6 in the overall response).</p><disp-quote content-type="editor-comment"><p>b) It is also unclear when synthetic data is being used. Example: subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€ states that the algorithm &quot;maintains high accuracy even when up to 75 % of cells in atlas were missing for data&quot; but from the figures I gather this is synthetic data with unnaturally low position noise, so this could give the reader the wrong impression.</p></disp-quote><p>Please see our explanation above (#2 in the overall response).</p><p>The point about the synthetic data accuracy is not to say that the algorithm has such and such accuracy, but rather to compare to existing methods (particularly registration) that for a fixed amount of position noise CRF maintains higher accuracy in handling even high count noise level (75%). Further we show that this is true across all position noise levels (Figure 2â€”figure supplement 5). We changed the language to say â€œhigher accuracyâ€, rather than high accuracy.</p><p>To demonstrate CRF_ID methodâ€™s advantage over registration methods on experimental data, we show accuracy in several cases (Figure 2A, Figure 3B, Figure 4B, 4C, Figure 6B, 6C, 6E).</p><disp-quote content-type="editor-comment"><p>c) Similarly, it is often unclear from the figures which strain is being used. For example the strain in Figure 4A-C is not labeled, but it appears to be the same as Figure 3A,B. But if that's the case its unclear why performance would be so much higher in Figure 4B than Figure 3B. Please label which strain is used in which figure subpanel throughout the manuscript, or provide a table.</p></disp-quote><p>Figure 4B is demonstration of cell annotation in calcium imaging case. In this case, the candidate list of identities of cells are known a priori in practical experiments (based on promoters used to express GCaMP in cells). Thus, the task is to assign labels from the candidate list to the cells in images hence a smaller atlas is used for automatic annotation. A smaller atlas of candidate identities also implies smaller count noise. In comparison, Figure 3B is demonstration of gene-expression pattern analysis. In this case, no candidate list of cell identities is known a priori. Thus, full brain atlas is used for automatic annotation. Assigning identities to few GFP labeled cells using whole-brain atlas also implies large count noise. Thus, the cell annotation in gene-expression task is more difficult and hence lower accuracy.</p><p>Labeling methods and strains in the figures made figures look very busy, so we have moved the information into the legends instead. We have now added strain information in figure legends. Changes made in Figure 2, Figure 3, Figure 4, Figure 5, Figure 6, Figure 2â€”figure supplement 1, Figure 2â€”figure supplement 2, Figure 2â€”figure supplement 3, Figure 2â€”figure supplement 4, Figure 2â€”figure supplement 5, Figure 2â€”figure supplement 6, Figure 4â€”figure supplement 1, Figure 5â€”figure supplement 1, Figure 6â€”figure supplement 1, Figure 6â€”figure supplement 2, Figure 6â€”figure supplement 3.</p><disp-quote content-type="editor-comment"><p>d) Related: Figure 4A prominently displays &quot;calcium imaging,&quot; when this is in fact a GFP strain. This misleads the reader</p></disp-quote><p>We used GFP strain as a proxy for GCaMP strain to demonstrate cell identification (mentioned in subsection â€œCell annotation in multi-cell functional imaging 603 experimentsâ€). To clarify, we have now modified the Figure 4A title to â€œMock multi-cell calcium imagingâ€.</p><disp-quote content-type="editor-comment"><p>3) The work critically relies on reagents from other groups but could do a better job acknowledging those contributions:</p><p>a) The Neuropal strain is crucial to the manuscript and provides the majority of the ground-truth data starting with the Results and Figure 2, but Neuropal is first mentioned only towards the very end of the Results section and even then only in the context of Figure 6.</p></disp-quote><p>We have cited NeuroPAL strain earlier (Introduction).</p><disp-quote content-type="editor-comment"><p>b) The strain list should reference publications for strains and alleles used.</p></disp-quote><p>We have made the changes (Material and Methods section).</p><disp-quote content-type="editor-comment"><p>c) Subsection â€œCell annotation in gene expression pattern analysisâ€ misleadingly states that &quot;a reporter strain was crossed with pan neuronal red fluorescent protein&quot; but this cross was not performed by the authors, it was performed by another group, published and is publicly available on CGC. This should be fixed.</p></disp-quote><p>We have made the changes.</p><disp-quote content-type="editor-comment"><p>4) More details are needed about the ground truth datasets, including the number of neurons hand-annotated in each of the 9 individuals and their spatial location. (Manual annotation of Neuropal is easiest at the periphery and hardest in center of the ganglia, and so it would be important to know coverage).</p></disp-quote><p>Please see our explanation above (#7 in the overall response).</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Whole brain imaging in <italic>C. elegans</italic> allows us to monitor the activities of a large fraction of worm neurons during behaviors. However, the identity of each cell in the nerve ring remains largely unknown, prohibiting further analysis that integrates structural and functional information of the worm circuit. This problem has been partially amended by manual annotation from experts. However, this is a time-consuming and error-prone solution when one is dealing with a large dataset. We are enthusiastic about this manuscript because the authors take one important step towards automated annotation of neuronal identities. Methods in this work are innovative and attractive. By minimizing energy functions imposed by absolute coordinate differences (unary potential) between images and atlases, as well as differences of the pairwise relationship between neurons, the authors show that their CRF_ID model outperforms SOTA registration model in accuracy, speed, and robustness.</p><p>We would like to raise several main issues of this manuscript.</p><p>1) Writing. The main text of the manuscript is brief, dense, technical, and very difficult to understand. I skipped the main text and went directly to the Appendix. This arrangement is not wise for a general audience, especially biologists. In future revisions, please embed the figures in the text, which would save us a lot of time. Here are some specific suggestions:</p><p>a) The CRF framework can be better explained by building upon physical intuition. The mathematical notations sometimes only cause confusion. For example, is it necessary to introduce clique? I thought that every neuron in their graph model is adjacent to every other neuron. If this is not the case, please describe how the graph model is constructed. You find the note on maximum entropy model is out of the blue (subsection â€œStructured prediction frameworkâ€). If you want to make a deep connection with statistical physics, you better explain it well in the text!!</p></disp-quote><p>We appreciate this comment from the reviewer. We have re-written several sections to make the methodology clear (please see our response to #3 in overall comments). We have moved the figures in the text to facilitate readability, we have moved some supplemental text into the main manuscript, and perhaps most importantly, we have added cartoons to facilitate the explanation of what is done. We now emphasize the intuition-building for potential users while maintaining some terminologies often used in graphical model literature to be consistent with this body of literature.</p><p>To some of the specific points from the reviewer:</p><p>- It is true that every neuron is adjacent to every other neuron in the graph because the underlying graph structure in our model is a fully connected graph. We have clarified this in the text.</p><p>- In mentioning maximum entropy model, we did not intend to make a deep connection with statistical physics. Our aim was to establish an analogy of our data-driven atlases to sufficient statistics in maximum entropy models (exponential family models in general). We realized that this is confusing and not adding too much to the understanding of our model or the use of it, so we have removed this text. We now emphasize in the text, with intuitive explanation, how the statistics can be updated based on available data using simple averaging operations, which makes this approach computationally advantageous over other methods where updating is computationally costly.</p><disp-quote content-type="editor-comment"><p>b) The motivation to build and how to build the data-driven atlases are dismissive in the main text. It is briefly mentioned in subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€. But these sentences are extremely difficult to understand without reading the Appendix. The Appendix -- extended methods S1.7, however, needs to be revised and better explained. For example, the equations are not quite consistent, such as in Equations 6 and 25, and the notations are quite confusing. Linking some examples like Figure 2B to S1.7 will help understand the concept.</p></disp-quote><p>We appreciate the reviewerâ€™s sentiment here. We have changed the text to clarify these points here (Results; Appendix â€“ extended methods S1.7). Briefly, we explain that the current atlases (including the OpenWorm atlas) is too idealistic; this is substantiated by our data as well as other published work. For example, in OpenWorm atlas, left-right cell pairs are exactly symmetric across AP axis whereas this is rarely true for experimental data. Because data-driven atlases can capture the actual observed positional relationships among cells in biological data and they can be updated (or specialized) for new data when they become available, they should prove to be more useful.</p><p>Our paper is motivated by the following important point: that data-driven atlases achieve higher accuracy. One example is to compare accuracy achieved on same datasets using data-driven atlas with test data held out (Figure 2A) and OpenWorm atlas (Figure 2C left panel). Data-driven atlases can be built easily using the code provided for as per the experimental need. For example, for multicell calcium imaging, an atlas can be built only for cells of interest. In contrast, for whole-brain imaging experiments a separate atlas can be built. We echo these points in the Discussion.</p><p>We have revised several sections in the Appendix â€“ extended methods:</p><p>1) Motivation and intuition behind building the data driven atlas is explained clearly in Appendix -extended methods S1.7 and in main text.</p><p>2) Equations and notations are now consistent. Particularly for Equation 6 and 25 we explain how these equations are consistent using Equation 26.</p><p>3) We have added additional figure as Figure 1â€”figure supplement 1 that schematically explains the how the features in the model are updated when data-driven atlas is used compare to static atlas.</p><disp-quote content-type="editor-comment"><p>c) The prediction accuracies of the model vary from case to case. It was quite low in the scenario of missing neurons. The authors need to state clearly in their paper, under the worst scenario such as whole brain imaging without colors, the prediction accuracy of their methods. This information is now buried somewhere that is hard to find. The prediction accuracies of the CRF model, contingent upon different datasets and different noise levels, would better be a self-contained main figure, not in the supplements. To follow the logic flow of the text, the figure panels that compared CRF with other registration methods would better be introduced later.</p></disp-quote><p>Please see our response to the overall point #4. Again, we stress that prediction accuracy depends upon position noise and count noise present in data compared to atlas. The levels of these noise depend on application e.g. different amounts of count and position noise in multi-cell calcium imaging experiments compared to whole-brain imaging. We have characterized several position and count noise cases using synthetic data and shown that CRF model performs better than registration method. In fact, CRF model is very robust in handling count noise compared to registration based methods (Figure 2D, Figure 2â€”figure supplement 5B).</p><p>Also, Figure 2A shows the accuracy using experimental datasets for whole-brain imaging accuracy without colors.</p><disp-quote content-type="editor-comment"><p>2) Notations. As mentioned above, the symbol consistency is a problem. In the formulation of the model, Î»'s, trainable parameters in original CRF, have been abused. For the Î» appearing in Equations 2/3/4/6/7/8/9/10/32/34/37/46, the author might consider making a clear statement that which of them are trainable and which are hyperparameters. If they are hyperparameters, the setting and tuning strategies need to be disclosed. Also, the keywords &quot;identification&quot; and &quot;annotation&quot; should be considered unified.</p></disp-quote><p>We have now clearly described in Appendix â€“ extended methods S1.2, what terms in Equation 6/7/8/9/10 are hyperparameters and methodology for their setting. Explanation for lambdas in Equations 2/3/4 follow from this explanation. Briefly, all lambdas in these Equations are hyperparameters. We also note that (old Equations 37/46 and new Equations 38/49) are not our model and lambdas in these equations are just demonstration of previous methods.</p><p>In our model, the trainable parameters are either feature function values, e.g. positional relationship feature values in Equations 6/7/8, or inputs to the feature functions, e.g. distance between cells in Equation 9 and angle vector between cells in Equation 10. These parameters are trainable parameters and are updated while-building data-driven atlas. This is described in Appendix â€“ extended methods S1.7.</p><disp-quote content-type="editor-comment"><p>3) Loop Belief Algorithms, speed, and stability. The authors should provide details on the computational complexity and the stability of their algorithms and compare it to other registration methods. In their current graph model, does each trial converge and do they all converge to the same solution starting from different initial conditions? If not, how do the authors deal with the discrepancies? Likewise, the authors may want to comment on the memory usage for constructing the data-driven atlases to see if it can be another innovation point comparing to the registration methods.</p></disp-quote><p>We thank the reviewer for pointing out potential confusing points. Each trial with a defined initial condition yields a converged solution; in other words, the solution to the optimization problem is deterministic. If, however, different missing neurons (typically randomly assigned) are used, then the solutions may be different. This is the origin of the ranked list of assigned names for each cell.</p><p>We do want to acknowledge that the CRF method for each round of solution is computationally intensive (because it is a quadratic method, as opposed to linear methods), but updating the model with new data is cheap (while other methods require re-optimization using all data simultaneously).</p><p>Separately as the reviewer pointed out, atlas-building compared to registration methods is indeed a big point. For registration methods, the issue resides in that not only the computational requirements are large, but also that there is no systematic method of building atlas. First, not all images are registered simultaneously due to memory constraints. Second, atlases are built iteratively by blockwise registration of groups of images to a reference image. Without systematic methodology of which image should be chosen as reference image, atlas is biased towards the reference image and the choice of the reference image is often not as â€œprincipledâ€. Another way atlas can be biased is by the order in which blocks of images are registered to the reference frame. Tackling this challenge is an active field of research Wang et al., 2008; Evangelidis et al., 2014. In comparison, in the CRF method, atlas building is unbiased towards a specific image set because there is no concept of reference image and atlas can be built from all images simultaneously because of the cheapness of mathematical operation. This is a major take-away from this work. We echo these points in the Discussion.</p><disp-quote content-type="editor-comment"><p>4) Other missing information and explanations:</p><p>a) The authors should post the exact name and its settings of the registration models used for comparison.</p></disp-quote><p>This information has been added in Appendix-extended methods S2.1.</p><disp-quote content-type="editor-comment"><p>b) In Appendix S1.2.1 Unary potentials â€“ Positions along AP axis, as x-y is a 2D plane, we do not see why AP's is always better than LR's and DV's.</p></disp-quote><p>Here, we wanted to point out that AP is always better because AP axis is always in the xy plane.</p><p>In comparison one of the LR or DV (depending on the orientation of worm) is in the z-direction. Since z-sampling in images is lower in resolution compared to xy-sampling, measurements of neuron positions in z-direction contribute to noise in defining positional relationships.</p><disp-quote content-type="editor-comment"><p>c) Following the explanation of generating synthetic data (subsection â€œGenerating synthetic data for framework tuning and comparison 464 against other methodsâ€) is a bit hard. The reason for changing the low bound to zero might need further illustrations. Could you please comment on why the prediction accuracy is much lower in synthetic data (Figure 2A)?</p></disp-quote><p>To generate synthetic data, we start with OpenWorm atlas and add two types of noise to the atlas to mimic noises present in experimental data. These are position noise (random Gaussian noise that is added to spatial positions of cells) and count noise (random combination of cells that are removed from the data). For both position and count noise, we swept through a range of noise level values for comparison against registration methods. Particularly for position noise we swept from zero (zero noise variance) to upper bound noise (seventy-fifth percentile of the position variance of neurons observed experimentally). I.e. we did not change the low bound to zero, we just expanded the range of parameters swept for comparison against other methods. Results for the different levels of position noise are shown in Figure 2â€”figure supplement 5C.</p><p>Figure 2A is not synthetic data. It is experimental data and predictions were performed using data driven atlas built using experimental data with test dataset held out.</p><p>In comparison, Figure 2C left-panel is experimental data and prediction is performed using OpenWorm atlas. Lower accuracy achieved in Figure 2C left panel compared to Figure 2A highlights the fact that positional relationships of cells in OpenWorm atlas are not representative of experimental data. This is also the main motivation of building data driven atlases and a major point of the work.</p><disp-quote content-type="editor-comment"><p>d) For subsection â€œWhole-brain data analysisâ€, the author might consider linking the paragraph to Figure 5, making 0.1Hz a reasonable choice.</p></disp-quote><p>We have made the changes.</p><disp-quote content-type="editor-comment"><p>e) For subsection â€œWhole-brain data analysisâ€, &quot;[0.05, 0.05]&quot; seems to be a mistake.</p></disp-quote><p>This is the bandwidth parameter used in kerned density estimation method for estimating joint probability distributions of neuron activity and motion speed.</p><disp-quote content-type="editor-comment"><p>f) Subscripts in Equation 13 are wrong.</p></disp-quote><p>Thank you. We have made changes.</p><disp-quote content-type="editor-comment"><p>g) Providing more annotations and explanations on the meaning of the colors on some worm images (e.g., Figure 4D) would be helpful.</p></disp-quote><p>Thank you. We have added color annotations in Figure 4D.</p><disp-quote content-type="editor-comment"><p>5) Codes on GitHub may need to be slimmed down.</p></disp-quote><p>We have moved extra files (e.g. examples and extra functions) to a different folder on GitHub.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>Linking individual neurons to their anatomical names is a vexing problem that limits efforts to determine patterns of gene expression and neuronal activity at the single-neuron level. Given the landmark Mind of the Worm paper (White et al., 1986) and subsequent analyses of its connectome, many imagine that annotating individual neurons in <italic>C. elegans</italic> is a solved problem. Unfortunately, it is not. This paper presents a computational method for solving this problem and, as such, has the potential to advance neurobiology in <italic>C. elegans</italic>. The approach could, in principle, be adapted to any nervous system from which a computational atlas could be derived following the pipeline laid out in this manuscript. There is much to recommend this study and many opportunities for improvement regarding the presentation of the computational method, its limitations and expectations for application, and for considerations of generalization to other nervous systems.</p><p>1) Explanation of the computational method</p><p>The computational strategy is difficult to follow in the main text, unless one reviews the appendices. We suggest elaborating the process using more generic terms in the main text. The following specific elements need clarification or additional support</p><p>- The mixed approach of considering intrinsic and extrinsic similarities between image and atlas is described in a manner that assumes readers are already familiar with this computational strategy. Given the breadth of the eLife readership, the authors would increase the accessibility and impact of the paper by providing readers with examples of what features are intrinsic and which are extrinsic in this case.</p></disp-quote><p>We thank the reviewer for this comment. We agree that it is a balance between details and confusion. We have made changes in many parts of the text to address this issue e.g. we have re-written the subsection â€œCell annotation formulation using structured prediction frameworkâ€ to make methodology clear. To clarify the point about intrinsic and extrinsic similarities, we have also included a supplemental figure to illustrate these relationships (Figure 1â€”figure supplement 1).</p><disp-quote content-type="editor-comment"><p>- The difference between this approach and previous strategies based on image registration is communicated clearly in the supplement (lines 906-910), but not in the main text.</p></disp-quote><p>With new additions, we now describe the conceptual differences between our approach and registration based methods in several places including the Introduction and Discussion, Figure 1â€”figure supplement 1, Figure 2B. Technical details on differences in objective functions used by these methods are present in Appendix â€“ extended methods S1.9.</p><disp-quote content-type="editor-comment"><p>- Much is made of the computational efficiency of the CRF_ID approach, but little is provided in the way of data or analysis supporting this claim. The authors could, for example, compare the computational energy/time needed for this approach compared to prior image matching approaches. They could discuss the nature of this computational framework compared to others with respect to computational efficiency.</p></disp-quote><p>We thank the reviewer for this comment. This indeed can be confusing. We claim the computational efficiency compared to registration methods only in building data-driven atlases. See the Discussion.</p><p>In terms of the run time for optimization, CRF is more computationally expensive because it is a quadratic method as opposed to a linear method. We argue, however, that this is ok because of the gain in accuracy, reduction of bias, and the elimination of the need of choosing references. Please also see responses to related comments (#3) from reviewer 2.</p><disp-quote content-type="editor-comment"><p>Generality of the method</p><p>To better support the authors claim that this method is easily adaptable for any nervous system, please apply the framework to estimate the optimal number (and/or density) of landmarks as a fraction of the total number of nuclei or density in 3D. This would assist others in applying the framework to other simple nervous systems (e.g. Aplysia, leech, hydra).</p></disp-quote><p>Our simulation results show that accuracy increases with more landmarks, i.e. there is no saturation point. Thus, there is no optimal number of landmarks from the computational point of view. Practically, however, there are constraints such as the availability of promoters for genetic control of expression and whether landmarks can be easily distinguished. For instance, if too many densely packed cells are labelled as landmarks such that their identity cannot be easily determined manually, then the automated method cannot use these cells as landmarks. The distinguishability of landmarks depends not only on density in 3D but also on geometry of nervous system. Further it is possible that genetic labelling strategies may be too cumbersome or appropriate promoters may not be available to label all desired landmarks. Therefore, it is not possible to provide an estimate of the optimal number of landmarks in general.</p><p>Importantly, we want to note that even without landmarks, CRF model can achieve high accuracy (Figure 2A) by using data-driven atlases. Thus, in our opinion, data-driven atlases are more critical for achieving high accuracy compared to landmarks.</p><disp-quote content-type="editor-comment"><p>Additionally, it is not clear if the image stacks used to derive data-driven atlases are distinct from the data analyzed for prediction or whether the same data is used iteratively to build atlases and improve prediction accuracy.</p></disp-quote><p>As discussed in the overall response (#1), we performed the hold-out exercise â€“ holding out a test dataset for prediction and building positional relationship and color atlases using the remaining datasets. We do not iteratively build atlases using the same data on which prediction is to be tested.</p><disp-quote content-type="editor-comment"><p>Application of the method to <italic>C. elegans</italic></p><p>Specifying the AP/DV/LR axes from raw data is not completely automatic but rather appears to require some user input based on prior knowledge. This is not stated clearly in the text. In addition, it is also not clear how the algorithm handles data from partially twisted worms (non-rigid rotation around the AP-axis). This problem is more likely to be encountered if the images capture the whole worm. Is it possible to allow rotation of the DV and LR axes at different points along the AP axis?</p></disp-quote><p>AP/DV/LR axes are defined using PCA on positions of cells in images, and thus automatic. The only user input is to determine the polarity â€“ which end of the worm is anterior (dorsal/left); this is because PCA algorithm does not distinguish AP from PA axis (the axes are -1 multiplications of each other); same is true for DV and LR axes. In our framework, this ambiguity is easily resolved by users clicking on any neuron in anterior (dorsal/left) of the worm. No other extra prior knowledge is needed from users. The Materials and methods section has a description of this.</p><p>Our strategy of handling non-rigid rotations about AP axis is to appropriately rotate LR and DV axes plane about AP axis (as described in Appendix â€“ extended methods S1.3). This is done by rotating LR axis using easily identifiable LR neuron pairs in image. In the current framework (dealing with head ganglion cells), the same rotation is applied across all points in AP axis. However, we agree with reviewer that different rotations can be applied across different points along AP axis. This is currently not implemented in current framework.</p><disp-quote content-type="editor-comment"><p>Specific suggestions for improvement are collected below.</p><p>1) The method depends on the tidy segmentation of nuclei, but the segmentation strategy is not discussed nor is whether or not nuclear localization of landmarks or unknown markers is essential for accurate annotation.</p></disp-quote><p>We have added the description of the GMM method in subsection â€œWhole-brain data analysisâ€.</p><p>Nuclear localization is not necessary as long as cells can be distinguished (e.g. results in Figure 3B, Figure 4B, 4C for strains without nuclear localization). If cells are not detected accurately this leads to count noise. We have characterized various count noise levels (Figure 2â€”figure supplement 5).</p><disp-quote content-type="editor-comment"><p>2) The authors need to explicitly inform readers that the markers in the NeuroPal strains (OH15495, OH15500) and the GT strains are localized to neuronal nuclei. Additionally, the authors need to address (1) whether or not nuclear localization is required for the annotation framework to succeed, and (2) discuss whether or not a cytoplasmic marker would be sufficient, and (3) provide more information on the segmentation algorithm used and recommended for use with the computational framework discuss.</p></disp-quote><p>Nuclear localization is not necessary as long as cells can be distinguished (e.g. results in Figure 3B, Figure 4B, 4C for strains without nuclear localization). If cells are not detected accurately this leads to count noise. We have characterized count noise (Figure 2â€”figure supplement 5).</p><p>We use a Gaussian Mixture based segmentation algorithm. We have made changes in the text to clarify this point.</p><disp-quote content-type="editor-comment"><p>3) The authors use the AML5 strain as a proxy for GCaMP-labeled strains (subsection â€œComputational workflow for automatic cell identificationâ€) to demonstrate the utility of the annotation tool for identifying cells used for calcium imaging. This strain differs from the NeuroPal strain expressing GCaMP is several respects that are not discussed by the authors, leaving readers to wonder about the utility of AML5 as a proxy and to question whether or not the success of cell identification in this strain is truly generalizable. Two main differences exist: 1) In AML5, GFP is expressed in both the nucleus and cytoplasm but in in OH15500 GCaMP6s is tagged with an NLS; 2) The basal fluorescence of GCaMP6s, but not GFP depends on basal calcium concentrations-this fact may introduce additional detection noise/variance into the annotation problem. The authors need to discuss how these differences affect (or don't) the ability to generalize from success with the proxy strain to success with NLS::GCaMP6s expression.</p></disp-quote><p>To clarify, according to Nyugen et al., 2016.</p><p>1) Nuclear localization: As discussed earlier as well (response to #5 overall comments), CRF_ID model works regardless of marker localization, as long as the cells can be separated/distinguished. If the cells cannot be separated, this leads to count noise. We have demonstrated superior performance of CRF_ID in handling count noise.</p><p>2) Cells with only basal fluorescence: Basal fluorescence only could lead to cell detection errors, thus leading to count noise. We have characterized for various amounts of count noise levels for both experimental data (Figure 4C) and synthetic data (Figure 2D, Figure 2â€”figure supplement 5B), and shown that CRF model performs better than registration methods. In practice, for GCaMP videos, data from multiple frames can be used to get a superset of cells (thus removing count noise) before annotation with CRF_ID. This would directly address the basal fluorescence issue.</p><p>3) AML5 example (Figure 4B, 4C) is to demonstrate a use case different than whole-brain imaging (fast annotation of cells in multi-cell calcium imaging videos). To demonstrate success in wholebrain imaging, we show prediction for whole-brain datasets (Figure 2A, Figure 6C).</p><disp-quote content-type="editor-comment"><p>4) Please clarify the nature of the output from the code that implements the framework. In other words, a reader should learn from the paper what kind of output file they might receive if they attempted to apply the tool to their own images (annotated image with top prediction candidate, data structure with all predictions and respective probabilities, etc).</p></disp-quote><p>We run the code in two modes:</p><p>Mode 1 â€“ Single run will give a deterministic prediction for each cell.</p><p>Mode 2 â€“ multiple runs taking missing cells into account. Each run will generate predicted identities of cells in that run. The results from various runs are used in subsequent step to generate top candidate list for each cell with label consistency score for each candidate.</p><p>We have clarified the text about these modes of running the algorithm (subsection â€œComputational workflow for automatic cell identificationâ€).</p><disp-quote content-type="editor-comment"><p>(5) Terminology â€“ There are a number of general computational terms and specific terms relevant to this problem that are not defined for the reader (listed below). Providing readers with implicit or explicit definitions of these terms will improve clarity.</p><p>5(a) &quot;count noises&quot; â€“ used to represent variation in the number of nuclei detected in a given image stack, but not defined at first use. Please provide conditions under which the number of nuclei detected might vary along with defining this term. Additionally, this is a singular concept and should be written as &quot;count noise&quot; not &quot;count noises&quot; (by analogy to &quot;shot noise&quot;).</p><p>5(b) There several other instances in which &quot;noises&quot; is used where &quot;noise&quot; is meant.</p></disp-quote><p>Count noise (defined in the Introduction and on) is not variation in the number of nuclei detected. It is the difference in the number of nuclei detected in the image, compared to the number of cells present in atlas to which image is to be matched. For example, if 100 cells are detected in an image whereas 200 cells are present in the atlas (i.e. 200 labels) to be used, count noise would be equal to (200-100)/200 = 50%.</p><p>The practical reasons for count noise is in the mosaicism of the expression of the genetically encoded marker/reporter, the incomplete coverage of the marker/reporter (e.g. â€œpan-neuronalâ€ not covering every neuron), errors in cell segmentation method due to dim cells or tightly clustered cells etc. These reasons are given in the same paragraph.</p><p>We have changed all mentions of â€œcount noisesâ€ to â€œcount noiseâ€.</p><disp-quote content-type="editor-comment"><p>5(c) &quot;intrinsic similarity&quot; â€“ which seems to represent pairwise positions in 3D in the image and the atlas.</p><p>5(d) &quot;extrinsic similarity&quot; â€“ not clear.</p></disp-quote><p>We have included Figure 1â€”figure supplement 1 to clarify these points.</p><disp-quote content-type="editor-comment"><p>(6) (Subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€) &quot;More importantly, automated annotation is unbiased and several orders of magnitude faster than manual annotation by researchers with no prior experience.&quot;</p><p>(a) Unbiased â€“ It is a common misconception that computational strategies are unbiased. For this claim to be rigorously accurate, the authors would need to demonstrate that each and every nucleus was equally likely to be identified correctly by the automated annotation algorithm. For instance, it is easy to imagine that some nuclei, such as those closer to a landmark, are more likely to identified correctly and with less uncertainty than those further away. If this were true, then automated annotation has a systematic bias in favor of nuclei closer to landmarks and a systematic bias against nuclei further away from landmarks. And would, therefore be biased â€“ even though no human supervision is involved. The authors should back this claim up with data or clarify that what they mean is that automated annotation is an improvement over (biased and subjective) human annotation.</p></disp-quote><p>We agree with the reviewer that if there are biases in making the atlas, then those biases will be reflected in the predictions. For example, if no information is present for some cells in the atlas, then it may be expected that prediction accuracy of those cells will be lower, compared to cells for which a lot of information is present in the atlas. However, this is true for any machine learning method, as all machine learning methods are as (un)biased as the training data. (The same perhaps can be said about human curation.)</p><p>Our use of the terminology is with respect to maximum-entropy models. With log-linear parameterization of features, CRF model is similar to maximum-entropy models. Further, maximum-entropy models produce a maximally unbiased (maximum entropy) probability distribution over labels subject to some constraints, which in our case are observed positional relationships in the atlas.</p><p>In addition, the model is unbiased in the sense that it predicts identities using an atlas (that may be built with fully or partially annotated datasets contributed by various researchers). Thus, CRF_ID method combines the knowledge of all researchers in the form of positional relationship atlas and then predicts identities that maximally tries to satisfy the atlas. Thus, it removes individual biases in cell identity annotation. We have clarified this point in text.</p><disp-quote content-type="editor-comment"><p>(b) Faster -The authors are free to speculate about the relative speed of automated vs. manual annotation by an inexperienced researchers in the discussion. By writing this as a result, however, data are needed to back it up.</p></disp-quote><p>We only included discussions on this point in the Discussion section. Our point is that CRF_ID can be run automatically without direct input after the models are trained, so the use is simple.</p><disp-quote content-type="editor-comment"><p>(7) (Materials and methods) From the description of how GT290 and GT298 were made, it seems that both constructs include 2x NLS fragments, but the genotype description under reagents lacks that annotation and most, if not all other references to this strain neglect to mention the NLS.</p></disp-quote><p>We apologize for the confusion; we have clarified the genotypes. The strains are nuclear localized.</p><disp-quote content-type="editor-comment"><p>Presentation concerns that need attention: There are three instances of undisclosed data re-use among the figures that need to be addressed. (1) The image in Figure 4A is identical to the green channel of the image shown in Figure 3A; (2) Some of the data shown in Figure 2â€”figure supplement 4A â€“ Part of the data shown here is also shown in Figure 2C; (3) Figure 4C â€“ Please indicate if the same number of cells were removed in each run. Also, it appears that this data also may have been used in Figure 4â€”figure supplement 1. If so, please disclose data re-use.</p></disp-quote><p>Same strain, AML5, was used for demonstration of cell annotation in gene expression and multicell calcium imaging cases. For gene expression case (Figure 3A) both red and green channels were used for cell identity prediction whereas for multi-cell calcium imaging case (Figure 4A) only green channel was used. We have replaced Figure 4A with a new image.</p><p>Data in Figure 2C (right panel) and Figure 2â€”figure supplement 5A come from the same experiment that is predicting cell identities in experimental datasets using OpenWorm atlas and comparing accuracy across methods. Figure 2C (right panel) shows comparison for only top labels predicted by methods, in comparison Figure 2â€”figure supplement 5A shows comparison for top 3 and top 5 labels as wells.</p><p>Figure 2C is indeed related to Figure 2â€”figure supplement 5A. Figure 2C highlights the most important comparisons. For the sake of easy side-by-side comparisons, we left Figure 2â€”figure supplement 5A as is, and added in the legend that part of data are replotted in Figure 2C for simple comparison.</p><p>For Figure 4C, the same fraction (3 out of 16) but a different combination of randomly selected cells was removed in each run. We have mentioned this in Figure panel now. Figure 4â€”figure supplement 1 shows additional results for different numbers of cells removed (2, 4, and 5). We have now mentioned in the legend of Figure 4â€”figure supplement 1 that part of data are replotted in Figure 4C.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>(1) The authors performed additional computational experiments by holding out some of the data used to create atlases â€“ an important and general strategy for computational solutions that depend on real-world data. The authors present a comparison of this approach for reviewers, but not for explicitly for readers. These analyses and plots must be included in the final version main text.</p></disp-quote><p>We have included the new data in the revised manuscript in the last round (Figure 2A, Figure 6B, C, E, Figure 6â€”figure supplement 1, Figure 6â€”figure supplement 2, Figure 6â€”figure supplement 3).</p><disp-quote content-type="editor-comment"><p>(2) The authors should state clearly the computational speed of their algorithms and compare it with the registration methods. The inference and subsampling procedures (~ 1000 times?) on the data-driven atlas are time-consuming and computationally expensive.</p></disp-quote><p>We have now included a new figure (Figure 2â€”figure supplement 7) comparing the optimization runtimes of CRF and registration methods. A discussion is added.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Main Text</p><p>(1)Thanks for sending us a much better written manuscript. The authors should consider replotting some of their figures, which can be difficult to read.</p><p>(a) The neuron names in Figure 2F and Figure 2â€”figure supplement 6C are difficult to read. Is there a new way to plot? When I wanted to zoom in to have a better understanding, they caused my Preview crashed multiple timesâ€¦</p></disp-quote><p>We have increased the size of Figure 2F and the Figure 2â€”figure supplement 6C. They are now available on github. We enlarged them to page width but perhaps still not clear enough to see details. Here in the figures we mainly want to show the general shape of the data. For readers interested in the exact details, they may download the figure from github.</p><disp-quote content-type="editor-comment"><p>(b) The meaning of the error bars throughout the manuscript are not well explained.</p></disp-quote><p>Description of boxplots and error bars is included now in all figure legends.</p><disp-quote content-type="editor-comment"><p>(2) The authors should state clearly the computational speed of their algorithms and compare it with the registration methods. The inference and subsampling procedures (~ 1000 times?) on the data-driven atlas are time-consuming and computationally expensive.</p></disp-quote><p>We have now compared the runtimes of CRF_ID framework with registration methods. The discussion is added in subsection â€œIdentity assignment using intrinsic features in CRF_ID outperforms other methodsâ€ and on and results are added as Figure 2â€”figure supplement 7.</p><disp-quote content-type="editor-comment"><p>(3) Is there a way to make the data-driven atlas publicly available in addition to the raw annotated datasets (n=9)?</p></disp-quote><p>The datasets are all on our github. Positional relationship atlases are available here https://github.com/shiveshc/CRF_Cell_ID/tree/master/Runs/Data_driven_atlases/PositionalRelationship_ <ext-link ext-link-type="uri" xlink:href="https://github.com/shiveshc/CRF_Cell_ID/tree/master/Runs/Data_driven_atlases/PositionalRelationship_atlases">atlases,</ext-link> and Color atlases are available here https://github.com/shiveshc/CRF_Cell_ID/tree/master/Runs/Data_driven_atlases/Color_atlases.</p><disp-quote content-type="editor-comment"><p>Rebuttal</p><p>(1) Pipeline (Major Point#6):</p><p>I do not quite understand the necessity of single or multi-run, which can be combined into one inference process. Consider the following scenario in a freely behaving animal. At one time, the region to be annotated consists of N neurons. At a different time, a new neuron (now N+1) is squeezed into the region. It is probably always true that the number of neurons in the data is different from that in the atlas.</p></disp-quote><p>We agree with the reviewer that the number of neurons in the dataset is likely to be different from that in the atlas in many experimental conditions. There are, however, use-cases when the count-noise is zero (i.e. the number of cells is exactly the same as in the available labels). For instance, we talk about multicell imaging scenario (Figure 4A/B) â€“ in some situations where multiple cells are labeled and the identity set of the cells is well defined (e.g. well-known well-behaved promoters).</p><disp-quote content-type="editor-comment"><p>(2) Color and Robustness and Evaluation Metrics (Essential revisions #1, #4, #6):</p><p>It seems that the leverage of color information for annotation is quite limited in the revised manuscript. The procedures for properly normalizing the color distribution are also complicated. I also found that the following sentence was incorrect. &quot;Further, when leave-one-out atlases are used for both positional relationship and color (Figure 6â€”figure supplement 1), the accuracy achieved by top labels is marginally greater than the accuracy achieved by using leave-one-out positional relationship only (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>)&quot;. It was actually significantly lower if I read the figures correctly. This leaves the question how we shall correctly assemble the data-driven atlas efficiently.</p></disp-quote><p>We apologize for the confusion. The statement was comparing the yellow bar in Figure 6â€”figure supplement 1 (leave-one-out for both position and color, which is at 74.5%) and the first column orange bar in <xref ref-type="fig" rid="respfig1">Author response image 1</xref> (leave-oneout for position only, which is at 73.4 %). That is why we say including color is only marginally better. The reviewer makes a good point, which we agree, that not all information is additive â€“ either position or color alone can carry quite a bit of information, but in this case, together, they did not provide a whole lot more extra information. With color normalization and using ensemble of color atlases the leave-one-out accuracy is 80.7% for top labels.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The revision is very responsive to the summary and specific reviewers' critiques.</p><p>There remain some awkward phrasing and missing details to address. These are noted in order to make the text accessible to a broader audience and to increase the impact of the study both within and beyond the community of <italic>C. elegans</italic> researchers. These include:</p><p>(1). The use of the plural &quot;noises&quot; when the authors probably mean either &quot;noise&quot; in the singular e.g. â€œPrediction accuracy is bound by position and count 316 noise in dataâ€, replace &quot;amount of various noises&quot; with &quot;amount of noise contributed from various sources&quot;. This latter example combines both the concept of the magnitude of noise and the diverse types of noise.</p></disp-quote><p>Changes are made.</p><disp-quote content-type="editor-comment"><p>(2) Shorthand terms or jargon. For example, when the text reads &quot;the position of cells AIBL&quot; what is meant is &quot;the position of the cell bodies of AIBL&quot;. Similarly, replace &quot;neurons&quot; with &quot;neuronal cell body&quot; or &quot;neuronal somata&quot;.</p></disp-quote><p>Changes are made.</p><disp-quote content-type="editor-comment"><p>(3) <italic>C. elegans</italic> strains â€“ in response to critiques of the initial submission, the authors have clarified the origin of the transgenic strains used in the study. Thank you! Please also correct the reference to AML70 which is not included in the table of strains. Based on context, it looks like the authors are referring to AML32. Please ensure that this is corrected.</p></disp-quote><p>We have updated the strains reference table and description of making strains in the subsection â€œConstruction of landmark strainsâ€.</p><disp-quote content-type="editor-comment"><p>(4) Figure 5, panel E. Please refer to Figure 5â€”figure supplement 1 panel C noting the cells with non-zero weights in SPC1, SPC2, SPC3</p></disp-quote><p>Changes are made.</p><disp-quote content-type="editor-comment"><p>(5) Figure 5 panel H: Indicate the origin of the motion trace shown. It appears to belong to one of the posterior cells shown in Panel F. Please provide a rationale for this choice â€“ since the traces in Panel F certainly indicate that motion signals differ among cells.</p></disp-quote><p>We have now indicated the cell whose motion trace was used for mutual-information (Figure 5G) and cross-correlation analysis (Figure H, I) in Figure 5H. We chose one of the posterior cells because velocity traces of those cell were in phase with neuron activity. We agree with reviewer that motion signal among cells are different in terms of phase shifting and magnitude; this could be because the motion of animal is restricted in the device.</p></body></sub-article></article>