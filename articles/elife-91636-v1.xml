<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91636</article-id><article-id pub-id-type="doi">10.7554/eLife.91636</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91636.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Language experience shapes predictive coding of rhythmic sound sequences</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Morucci</surname><given-names>Piermatteo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4972-0864</contrib-id><email>piermatteomorucci@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Nara</surname><given-names>Sanjeev</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lizarazu</surname><given-names>Mikel</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Clara</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Molinaro</surname><given-names>Nicola</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7549-6042</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>Department of Fundamental Neurosciences, University of Geneva</institution></institution-wrap><addr-line><named-content content-type="city">Geneva</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01a28zg77</institution-id><institution>Basque Center on Cognition, Brain and Language</institution></institution-wrap><addr-line><named-content content-type="city">Donostia-San Sebastian</named-content></addr-line><country>Spain</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Mathematical Institute, Department of Mathematics and Computer Science, Physics, Geography, Liebig-Universität Gießen</institution></institution-wrap><addr-line><named-content content-type="city">Gießen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cc3fy72</institution-id><institution>Ikerbasque, Basque Foundation for Science</institution></institution-wrap><addr-line><named-content content-type="city">Bilbao</named-content></addr-line><country>Spain</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Chait</surname><given-names>Maria</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>13</day><month>09</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP91636</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-21"><day>21</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-03"><day>03</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.28.538247"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-22"><day>22</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91636.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-20"><day>20</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91636.2"/></event></pub-history><permissions><copyright-statement>© 2023, Morucci et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Morucci et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91636-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-91636-figures-v1.pdf"/><abstract><p>Perceptual systems heavily rely on prior knowledge and predictions to make sense of the environment. Predictions can originate from multiple sources of information, including contextual short-term priors, based on isolated temporal situations, and context-independent long-term priors, arising from extended exposure to statistical regularities. While the effects of short-term predictions on auditory perception have been well-documented, how long-term predictions shape early auditory processing is poorly understood. To address this, we recorded magnetoencephalography data from native speakers of two languages with different word orders (Spanish: functor-initial vs Basque: functor-final) listening to simple sequences of binary sounds alternating in duration with occasional omissions. We hypothesized that, together with contextual transition probabilities, the auditory system uses the characteristic prosodic cues (duration) associated with the native language’s word order as an internal model to generate long-term predictions about incoming non-linguistic sounds. Consistent with our hypothesis, we found that the amplitude of the mismatch negativity elicited by sound omissions varied orthogonally depending on the speaker’s linguistic background and was most pronounced in the left auditory cortex. Importantly, listening to binary sounds alternating in pitch instead of duration did not yield group differences, confirming that the above results were driven by the hypothesized long-term ‘duration’ prior. These findings show that experience with a given language can shape a fundamental aspect of human perception – the neural processing of rhythmic sounds – and provides direct evidence for a long-term predictive coding system in the auditory cortex that uses auditory schemes learned over a lifetime to process incoming sound sequences.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>predictive coding</kwd><kwd>mismatch negativity</kwd><kwd>cross-linguistic effects</kwd><kwd>auditory perception</kwd><kwd>prosody</kwd><kwd>magnetoencephalography</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010434</institution-id><institution>'la Caixa' Foundation</institution></institution-wrap></funding-source><award-id>LCF/BQ/IN17/11620019</award-id><principal-award-recipient><name><surname>Morucci</surname><given-names>Piermatteo</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010665</institution-id><institution>H2020 Marie Skłodowska-Curie Actions</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/713673</award-id><principal-award-recipient><name><surname>Morucci</surname><given-names>Piermatteo</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010198</institution-id><institution>Ministerio de Asuntos Económicos y Transformación Digital, Gobierno de España</institution></institution-wrap></funding-source><award-id>PSI2015-65694-P</award-id><principal-award-recipient><name><surname>Molinaro</surname><given-names>Nicola</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010198</institution-id><institution>Ministerio de Asuntos Económicos y Transformación Digital, Gobierno de España</institution></institution-wrap></funding-source><award-id>RTI2018-096311-B-I00</award-id><principal-award-recipient><name><surname>Molinaro</surname><given-names>Nicola</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010198</institution-id><institution>Ministerio de Asuntos Económicos y Transformación Digital, Gobierno de España</institution></institution-wrap></funding-source><award-id>PDC2022-133917-I00</award-id><principal-award-recipient><name><surname>Molinaro</surname><given-names>Nicola</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011033</institution-id><institution>Agencia Estatal de Investigación</institution></institution-wrap></funding-source><award-id>CEX2020-001010-S</award-id><principal-award-recipient><name><surname>Molinaro</surname><given-names>Nicola</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>819093</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Clara</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004837</institution-id><institution>Ministerio de Ciencia e Innovación</institution></institution-wrap></funding-source><award-id>IJC2020-042886-I</award-id><principal-award-recipient><name><surname>Lizarazu</surname><given-names>Mikel</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution>Spanish Ministry of Economy and Competitiveness</institution></institution-wrap></funding-source><award-id>PID2020-113926GB-I00</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Clara</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003086</institution-id><institution>Basque Government</institution></institution-wrap></funding-source><award-id>PIBA18_29</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Clara</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A magnetoencephalography study across different languages shows that life-long listening experience influences the neural mechanisms underlying rhythm perception.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>According to predictive coding theories of perception, sensory processes and perceptual decisions are described as a process of inference, which is strongly shaped by prior knowledge and predictions (<xref ref-type="bibr" rid="bib10">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib16">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib35">Rao and Ballard, 1999</xref>). Predictions can be derived from different sources of information, forming a hierarchical predictive system (<xref ref-type="bibr" rid="bib12">de Lange et al., 2018</xref>). Each level of the predictive hierarchy houses an internal model encoding prior information about the structure of the external environment. When a prediction is violated, the prediction error is computed and used to adjust the corresponding prior and internal model. This results in a constantly evolving system that generates and refines predictions based on incoming sensory input and prior experience.</p><p>In the auditory domain, great progress in the understanding of the predictive capabilities of the auditory system has been made using the oddball design and its variations (see <xref ref-type="bibr" rid="bib19">Heilbron and Chait, 2018</xref>, for a review). In these designs, participants are usually presented with sequences of tones encoding a certain rule that is then violated by a ‘deviant’ event. Such deviants elicit a sharp evoked response in the electroencephalography (EEG) signal which has been defined as ‘mismatch negativity’ (MMN). The MMN peaks at about 0.100–0.250 s from stimulus onset and exhibits enhanced intensity over secondary temporal, central, and frontal areas of topographic scalp maps (<xref ref-type="bibr" rid="bib37">Sams et al., 1985</xref>; <xref ref-type="bibr" rid="bib17">Garrido et al., 2009</xref>). Within the predictive coding framework, the MMN is putatively considered an index of cortical prediction error.</p><p>Functionally, the mechanism underlying the MMN operates over both conscious and preconscious memory representations. MMN responses to auditory violations are observable when the participant is not paying attention to the auditory task and have been reported even in states of sleep (<xref ref-type="bibr" rid="bib36">Sallinen et al., 1994</xref>; <xref ref-type="bibr" rid="bib40">Sculthorpe et al., 2009</xref>; <xref ref-type="bibr" rid="bib42">Strauss et al., 2015</xref>) and coma (<xref ref-type="bibr" rid="bib14">Fischer et al., 2000</xref>). Given its automatic nature, the anticipatory mechanism underlying the MMN has been suggested to reflect a form of ‘<italic>primitive intelligence</italic>’ in the auditory cortex (<xref ref-type="bibr" rid="bib30">Näätänen et al., 2001</xref>). In the present study, we show that life-long experience with a spoken language can shape this automatic anticipatory mechanism.</p><p>Experimental studies using the oddball design and its derivations have been important to unveil the sensitivity of the auditory predictive system to local statistical regularities and transition probabilities (<xref ref-type="bibr" rid="bib19">Heilbron and Chait, 2018</xref>). However, these studies have primarily examined so-called contextual (or short-term) predictive signals. These predictions are usually based on rules acquired in the context of an experimental task – that is, rules linked to short-term memory – and have a short-lived impact on sensory processing. Yet, one core assumption of current predictive coding models is that the brain also deploys predictions based on long-term memory representations (<xref ref-type="bibr" rid="bib41">Seriès and Seitz, 2013</xref>; <xref ref-type="bibr" rid="bib51">Yon et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Teufel and Fletcher, 2020</xref>). Such long-term predictions may emerge via learning of regularities and co-occurring patterns that are relatively stable throughout the lifespan of an organism. Because arising over long timescales, these experiential priors become encoded into the tuning properties of sensory cortices, forming a computational constraint on bottom–up sensory processing (<xref ref-type="bibr" rid="bib44">Teufel and Fletcher, 2020</xref>).</p><p>Long-term priors may have long-lasting effects on perception. One example from the visual domain is the systematic bias in humans toward the perception of cardinal orientations (<xref ref-type="bibr" rid="bib18">Girshick et al., 2011</xref>). This bias has been linked to the presence of a long-term prior that mirrors the statistics of the visual environment, that is, the preponderance of cardinal orientations in visual input (<xref ref-type="bibr" rid="bib18">Girshick et al., 2011</xref>). Monkey studies on visual processing have shown that the visual system employs long-term priors to generate long-term predictions of incoming data (<xref ref-type="bibr" rid="bib27">Meyer and Olson, 2011</xref>). Yet, whether similar predictive coding schemes subserve cortical computation in the human auditory system remains unsettled.</p><p>Here, we take a cross-linguistic approach to test whether the auditory system generates long-term predictions based on life-long exposure to auditory regularities, using rules that extend beyond those acquired in the recent past. Currently, one critical behavioral example of the effect of long-term experience on auditory perception is the influence of language on rhythmic grouping (<xref ref-type="bibr" rid="bib21">Iversen et al., 2008</xref>; <xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>): Sequences of two tones alternating in duration are usually perceived by speakers of functor-initial languages (e.g., Spanish, English) as repetition of short–long groups separated by a pause, while speakers of functor-final languages (e.g., Basque, Japanese) report a bias for the opposite long–short grouping pattern. This perceptual effect has been linked to the co-occurrence statistics underlying the word-order properties of these languages. Specifically, the effect has been proposed to depend on the quasi-periodic alternation of short and long auditory events in the speech signal – reported in previous acoustic analyses (<xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>) – which reflect the linearization of function words (e.g., articles, prepositions) and content words (e.g., nouns, adjectives, verbs). In functor-initial languages, like English or Spanish, short events (i.e., function words; e.g., <italic>un</italic>, a) normally combine with long ones (i.e., content words; e.g., <italic>ordenador</italic>, computer) to form ‘short–long’ auditory chunks (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In contrast, in functor-final languages like Japanese and Basque, short events (i.e., function words; e.g., <italic>bat</italic>, a) normally follow long ones (i.e., content words; e.g., <italic>ordenagailu</italic>, computer), resulting in ‘long–short’ phrasal units (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Regular exposure to such language-specific phrasal structures has been proposed to underlie the automatic grouping biases of non-linguistic sounds (<xref ref-type="bibr" rid="bib21">Iversen et al., 2008</xref>; <xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>), suggesting the presence of an auditory ‘duration prior’ that mirrors the word-order and prosodic properties of a given language.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and rationale of the study.</title><p>Panels <bold>A</bold> and <bold>B</bold> illustrate the contrast between functor-initial and functor-final word order in Spanish and Basque, as well as its consequences on their prosodic structure. Panels <bold>C</bold> and <bold>D</bold> show the design of the experimental and control conditions, respectively. Notes represent individual tones. The structure of the design is the same in both conditions (<italic>ababab</italic>), with 30 s sequences of two tones alternating at fixed inter-stimulus intervals and occasional omissions. In the experimental condition, tones alternate in duration but not frequency, whereas in the control condition tones alternate in frequency but not duration. Panels <bold>E</bold> and <bold>F</bold> depict the hypothesized error responses associated with the different types of omissions for experimental and control conditions, respectively. Round brackets above the tones reflect the grouping bias of the two languages, based on their word-order constraints. Dotted lines reflect short-term predictions based on the transition probabilities of the previous stimuli. Solid lines reflect long-term predictions based on the phrasal chunking scheme of the two languages.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-fig1-v1.tif"/></fig><p>We hypothesize that the auditory system uses the proposed ‘duration prior’ as an internal model to generate long-term predictions about incoming sound sequences. In predictive coding terms, our hypothesis posits that the human auditory system upweights neural activity toward the onset of certain high-level events, based on the statistics of a given language.</p><p>To test this hypothesis, two groups of Basque (<italic>n</italic> = 20) and Spanish (<italic>n</italic> = 20) dominant participants were presented with 30 s rhythmic sequences of two tones alternating in duration at fixed inter-stimulus intervals (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), while magnetoencephalography (MEG) was monitoring their cortical activity. To measure prediction error, random omissions of long and short tones were introduced in each sequence. Omission responses allow to examine the presence of putative error signals decoupled from bottom–up sensory input, offering a critical test for predictive coding (<xref ref-type="bibr" rid="bib49">Walsh et al., 2020</xref>; <xref ref-type="bibr" rid="bib19">Heilbron and Chait, 2018</xref>).</p><p>If, in line with our hypothesis, the human auditory system uses long-term linguistic priors as an internal model to predict incoming sounds, the following predictions ensue. The omission of a long tone should represent the violation of two predictions in the Basque, but not in the Spanish group: a short-term prediction based on the statistics of the previous stimuli (i.e., a prediction about a new tone), and a long-term prediction based on the statistics of the Basque’s phrasal structure (i.e., a prediction about a new phrasal chunk). Consequently, such an omission response should lead to a larger prediction error in the Basque compared to the Spanish group (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). An orthogonally opposite pattern is expected when the deviant event is reflected in the omission of a short tone (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p>The expectation that stronger error responses would be elicited by the omission of the first element rather than the second element of a perceptual chunk (‘long’ for the Basque, ‘short’ for the Spanish group) is primarily based on previous work on rhythm and music perception (e.g., <xref ref-type="bibr" rid="bib23">Ladinig et al., 2009</xref>; <xref ref-type="bibr" rid="bib8">Bouwer et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Brochard et al., 2003</xref>; <xref ref-type="bibr" rid="bib33">Potter et al., 2009</xref>). These studies have shown that the amplitude of evoked responses is larger when deviants occur at the ‘start’ of a perceptual group and decline toward the end of the chunk, suggesting that the auditory system generates predictions about the onset of higher-level, internally formed auditory chunks.</p><p>We tested the predictions above against a control condition having the same alternation design as the experimental condition, but with the two tones alternating in pitch instead of duration (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Here, no difference between groups is expected, as both groups should rely on short-term, but not long-term priors (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Finally, we performed reconstruction of cortical sources to identify the regions supporting long-term auditory priors.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We first examined the responses evoked by the omissions of tones. MEG responses time-locked to the onset of the tones and omissions from the Basque and Spanish dominant groups were pulled together and compared via cluster-based permutation test analysis (<xref ref-type="bibr" rid="bib26">Maris and Oostenveld, 2007</xref>). Cluster analysis was performed over several time points to identify spatiotemporal clusters of neighboring sensors where the two conditions differ (<italic>Methods</italic>). This analysis revealed an early effect of omission responses arising around 0.100 s from deviance onset (p &lt; 0.0001), including several channels over the entire scalp (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>). The latency and topographical distribution of the effect resemble the one elicited by a classical mismatch response, with strong activations over left and right temporal regions (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>). A smaller cluster (p = 0.043) with lower amplitude was also detected in an earlier time window (~0.030–0.050 s post-stimulus/omission onset). This cluster primarily includes left centro-temporal channels. The directionality of the effect is the same as the later cluster, that is, larger responses to omissions compared to tones (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). This finding aligned with previous reports showing that the omission of an expected tone in a regular sequence of sounds generates larger event-related fields (ERFs) than an actual tone (e.g., <xref ref-type="bibr" rid="bib50">Yabe et al., 1997</xref>; <xref ref-type="bibr" rid="bib34">Raij et al., 1997</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Sensor-level topography and time course of neural responses to omitted sounds across groups and conditions.</title><p>(<bold>A</bold>) shows the temporal unfolding and topographical distribution of the overall effect of omission (omissions minus tones). Channels belonging to the significant cluster are highlighted. (<bold>B</bold>) shows the event-related field (ERF) generated by omissions and tones in a representative channel. (<bold>C</bold>) (left) shows the topography of the <italic>t</italic> distribution of the interaction effect between the language background of the participants (Spanish, Basque) and the type of omission mismatch negativity (MMN) (short, long). Channels belonging to the significant interaction cluster are highlighted. The interaction effect was present only in the experimental condition. Panel C (right) shows the averaged magnetoencephalography (MEG) activity over the 0.100–0.250 s time window and channels belonging to the significant cluster for each group and condition separately. (<bold>D–G</bold>) show the effect of language experience in modulating the amplitude of the omission MMN associated with each experimental and control contrast. Topographies (top) show the scalp distribution of the averaged activity over the 0.100–0.250 s time window. Channels belonging to the significant interaction cluster are highlighted. ERFs (middle) show the temporal unfolding of brain activity averaged over the channels belonging to the significant interaction cluster for each contrast and group. The shaded area indicates the time window of interest for the statistical analysis. Boxplots (down) show the mean MEG activity for each participant over the 0.100–0.250 s time window and the channels belonging to the significant interaction cluster. The center of the boxplot indicates the median, and the limits of the box define the interquartile range (IQR = middle 50% of the data). The notches indicate the 95% confidence interval around the median. Dots reflect individual subjects (n = 20 per group). In D–G, asterisks indicate statistical significance for each contrast using a one-sided, independent sample <italic>t</italic>-test with false discovery rate (FDR) correction for multiple comparisons (statistical significance: * signifies p &lt; 0.05, <italic>ns</italic> signifies p &gt; 0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Early neural response to omitted sounds.</title><p>Panel <bold>A</bold> shows the temporal unfolding and topographical distribution of the second cluster that emerged from the comparisons between omissions vs tones. Channels belonging to the significant cluster are highlighted. Panel <bold>B</bold> shows the event-related field (ERF) generated by omissions and tones over all the channels belonging to the significant cluster. Shaded gray area indicates the latency of the cluster. The upper part of the scale has been lowered compared to <xref ref-type="fig" rid="fig2">Figure 2</xref> to match the smaller amplitude of the effect. The temporal scale of the topographies has also been lowered compared to <xref ref-type="fig" rid="fig2">Figure 2</xref>, in order to allow the visualization of the cluster, which has a latency of around 0.020 s.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Effects of omission type and language background.</title><p>Panel <bold>A</bold> shows the event-related fields (ERFs) and topographies reflecting the main effect of omission type, with omissions of long auditory events generating larger omission mismatch negativity (MMN) than short events between groups. Panel <bold>B</bold> shows the ERFs and topographies reflecting the (lack of) main effect of language background, with overall no group differences in the amplitude of the omission MMN. Because no main effect of language background was detected. Panel B uses the same channels of Panel A as representative channels for plotting the ERF. The remaining conventions for the plot are the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-fig2-figsupp2-v1.tif"/></fig></fig-group><p>Our main question was on the presence of long-term predictions induced by the linguistic background of the participants during the processing of simple binary auditory sequences. To assess this, we tested for the presence of an interaction effect between the linguistic background of the participants (Basque, Spanish) and the type of tone omitted (long, short) in modulating the amplitude of the MMN. Omission MMN responses in this analysis were calculated by subtracting the ERF elicited by a given type of tone (i.e., long, short) from its corresponding omission (<xref ref-type="bibr" rid="bib17">Garrido et al., 2009</xref>). A cluster-based permutation test was used to test the interaction effect between omission type (long vs short) and the linguistic background of the participants (Basque vs Spanish). As the cluster-based permutation test is designed to compare two conditions at a time, we tested for an interaction effect by subtracting the MMN elicited by the omission of a long tone from the MMN elicited by the omission of a short tone for each participant, and then compared the resulting differences between groups.</p><p>For this and the following contrasts, we selected a predefined time window of interest between 0.100 and 0.250 s, which covers the typical latency of the MMN (<xref ref-type="bibr" rid="bib31">Näätänen et al., 2007</xref>; <xref ref-type="bibr" rid="bib17">Garrido et al., 2009</xref>). Cluster analysis revealed a significant interaction (p = 0.03), which was particularly pronounced over left frontotemporal channels (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). To unpack the interaction, we averaged data samples for each participant and condition over the channels belonging to the significant cluster and time points of interest. This resulted in two ERFs for each participant, one for each type of omission MMN (long, short). We then compared ERFs for each omission type between the two groups using a one-sided independent sample <italic>t</italic>-test, testing the hypothesis that participants deploy long-term expectations about the onset of abstract language-like grouping units (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Specifically, we compared (1) the omission MMN responses generated by the omission of long tones in the Basque vs the Spanish group, and (2) the MMN responses generated by the omission of short tones in the Basque vs the Spanish group. Consistent with the hypothesis, we found that omissions of long tones generated a larger MMN response in the Basque compared Spanish group (<italic>t</italic>(38) = 2.22; p = 0.03 FDR-corrected; <italic>d</italic> = 0.70), while the omission of short tones generated a larger omission MMN in the Spanish compared to Basque group (<italic>t</italic>(38) = −2; p = 0.03 FDR-corrected; <italic>d</italic> = 0.63) (<xref ref-type="fig" rid="fig2">Figure 2D, E</xref>). Notice that, given the structure of our design, a <italic>between-group</italic> comparison (e.g., comparing the ERF between the Basque and Spanish groups) is more suited to test our hypothesis than a <italic>within-group</italic> comparison (e.g., comparing the ERF evoked by the omission of long vs short tones within the Basque group), as the pre-stimulus baseline activity is virtually identical across conditions only in the <italic>between-group</italic> contrast.</p><p>To further assess that the interaction was driven by the hypothesized long-term ‘duration prior’, the same analysis pipeline was applied to the data from the control condition. Here, no significant omission type × language background interaction was detected (no cluster with p &lt; 0.05). To further check that no interaction was present in the control study, we averaged the data samples over the channels and time points in which we detected a significant interaction in the test condition and ran an independent sample <italic>t</italic>-test by comparing MMN responses elicited by the omission of high- and low-frequency tones in both groups (<italic>Methods</italic>). Even within this subset of channels, no between-group difference was detected between MMN responses evoked by omissions of high- (<italic>t</italic>(38) = −1.6; p = 0.12 FDR-corrected; <italic>d</italic> = −0.51), and low-frequency tones (<italic>t</italic>(38) = −1.1; p = 0.55 FDR-corrected; <italic>d</italic> = −0.04) (<xref ref-type="fig" rid="fig2">Figure 2F, G</xref>).</p><p>A linearly constrained minimum variance (LCMV) beamformer approach (<xref ref-type="bibr" rid="bib45">Van Veen et al., 1997</xref>) was used to reconstruct the cortical sources of the MEG signal. We first focused on the source activity underlying the effect of omission (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>). Source activity was calculated for the epochs averaged in the 0.100–0.250 s interval for both tones and omissions. In order to isolate regions underlying the effect of omission, whole-brain maps for the ratio of the source activity associated with omission responses and tones were created (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). A large network of regions showed a stronger response to omissions compared to auditory tones, including the bilateral inferior frontal gyri, premotor cortices, angular gyri, as well as the superior temporal gyri (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Source activity underlying the omission response network and long-term predictions.</title><p>Panel <bold>A</bold> shows brain maps representing the ratio of source activity of tone to the omission (neural activity index, NAI = <italic>S</italic><sub>Omission</sub>/<italic>S</italic><sub>Tone</sub>) over the 0.100–0.250 s time window. Panel <bold>B</bold> shows the source activity peaks from the two clusters in the left superior temporal gyrus (STG) for each group and condition separately. Panels <bold>C</bold> and <bold>D</bold> show the time course of source activity associated with the omission mismatch negativity (MMN) over distinct regions of interest (ROIs) of the left STG and inferior frontal gyrus (IFG). Dashed rectangles indicate the two temporal clusters within the 0.100–0.250 s time window. Error bars reflect standard errors (n = 20 per group). In Panels C and D, asterisks indicate statistical significance for each contrast using a one-sided, independent sample <italic>t</italic>-test (statistical significance: * signifies p &lt; 0.05, <italic>ns</italic> signifies p &gt; 0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-fig3-v1.tif"/></fig><p>We then examined the cortical origin of the interaction effect that emerged at the sensor level (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). In line with the MMN analysis at the sensor level, we focused on the source activity of the difference between omissions and tones for each omission type (short, long) and language group (Basque, Spanish). This analysis was aimed at identifying the cortical origin of the hypothesized long-term predictions. We hypothesized that such long-term priors might be linked to long-term experience with the rhythmic properties of the two languages. As such, they would be expected to arise around early auditory areas, such as the superior temporal gyrus (STG). An alternative hypothesis is that these priors are linked to the abstract syntactic structure of the two languages. Under this account, long-term predictions would be generated via long-range feedback from regions associated with syntactic processing, such as the left inferior frontal gyrus (IFG) (<xref ref-type="bibr" rid="bib4">Ben Shachar et al., 2003</xref>). To disentangle these possibilities, we performed an analysis on three regions of interest (ROIs, based on the Brainnetome atlas, <xref ref-type="bibr" rid="bib13">Fan et al., 2016</xref>) within the left STG, (Brodmann areas (BAs) 41/42 of the auditory cortex, rostral portion of BA 22, caudal portion of BA 22), and three ROIs within the left IFG (dorsal, ventral, and opercular portions of BA 44). We restricted our analysis to the left hemisphere only, which is where the significant interaction effect emerged at the sensor level (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We first performed a cluster-based permutation analysis in the whole left STG and the left IFG in the 0.100–0.250 s time interval, testing for the presence of an interaction between omission type and language group. This strategy is similar to the one performed at the sensor level, but is more time-sensitive as individual time points in the 0.100–0.250 s time interval are considered. No significant interaction was detected in the left IFG (no cluster with p &lt; 0.05), while two temporally distinct effects emerged in the left STG: an early effect arising in the 0.110–0.145 s time interval (p = 0.01) and a later effect in the 0.200–0.220 s (p = 0.04) (<xref ref-type="fig" rid="fig3">Figure 3B–D</xref>). It is possible that these two clusters reflect different temporal responses of the two groups to long and short omissions. To better understand the nature of the interaction effect within the left STG, pairwise comparisons were performed on each ROI and cluster using one-sided independent sample <italic>t</italic>-tests, following the same contrasts that were performed at the sensor level. Despite the high redundancy of activity across neighboring brain regions due to MEG source-reconstruction limitations (<xref ref-type="bibr" rid="bib7">Bourguignon et al., 2018</xref>), we were interested in verifying whether the more robust interaction effect, both in magnitude and reliability, was emerging in primary (BA 41/42) or associative (BAs 22) auditory regions. Given the limitations of the cluster-based statistical approach in the definition of the timing of significant effects (<xref ref-type="bibr" rid="bib39">Sassenhagen and Draschkow, 2019</xref>), peak activity within each temporal cluster in the STG was selected and used for the pairwise comparisons on each ROI (early peak: 0.120 s; late peak: 0.210 s). Pairwise comparisons showed that the omission of long tones generated larger responses on the latency of the early peak (0.120 s) in the Basque compared to the Spanish dominant group over BA 41/42 and BA 22c BA 41/42 (<italic>t</italic>(38) = 1.88, p = 0.03, <italic>d</italic> = 0.59); BA 22c (<italic>t</italic>(38) = 2.04, p = 0.02, <italic>d</italic> = 0.64), while no difference emerged for later peak responses (0.210 s) (BA 41/42: <italic>t</italic>(38) = 0.59, p = 0.27, <italic>d</italic> = 0.32; BA 22c: <italic>t</italic>(38) = −1.14, p = 0.87, <italic>d</italic> = −0.36) (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). On the contrary, the omission of short tones led to stronger responses over the later peak latency (0.210 s) in the Spanish compared to the Basque group in BA 41/42 (<italic>t</italic>(38) = −1.98, p = 0.02, <italic>d</italic> = −0.62), while no difference was observed on the latency of the earlier peak (BA 41/42: <italic>t</italic>(38) = 0.32, p = 0.62, <italic>d</italic> = 0.10) (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). No significant effect emerged in BA22r. Overall, these findings suggest that segments of the left STG, in particular BA41/42, exhibit distinct sensitivity to omission responses in the two groups. The Basque group shows larger responses to the omission of long tones at an earlier time interval, whereas the Spanish group displays increased responses to the omission of short tones at a later time interval (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>Besides documenting an interaction, which was the analysis of interest of our study, we also searched for a main effect of omission type. A cluster-based permutation test was used to compare the MMN responses elicited by omissions of long tones and omissions of short tones averaged across the two groups. The results showed that long-tone omissions generated a larger omission MMN response than short-tone omissions (p = 0.03), with the cluster including several frontal channels (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). This effect was consistent in the Basque (p = 0.003), but not in the Spanish group (no clusters with p &lt; 0.05). No main effect of language background was detected (no clusters with p &lt; 0.05) (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). In the control condition, neither a main effect of omission type nor an effect of language background was detected (no clusters with p &lt; 0.05).</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>By comparing MEG data from native speakers of functor-initial (i.e., Spanish) and functor-final languages (i.e., Basque) listening to simple binary sequences of tones with occasional violations, we show that experience with a given language can shape a very simple aspect of human perception, such as the neural processing of a binary rhythmic sound. This finding suggests that the human auditory system uses structural patterns of their native language to generate predictive models of non-linguistic sound sequences. This result highlights the presence of an active predictive system that relies on natural sound statistics learned over a lifetime to process incoming auditory input.</p><p>We first looked at the responses generated by sound omissions. In line with previous reports, we found that omissions of expected tones in an auditory sequence generate a sharp response in the ERF. Such omission responses have been suggested to reflect pure error signals decoupled from bottom–up sensory input (<xref ref-type="bibr" rid="bib20">Hughes et al., 2001</xref>; <xref ref-type="bibr" rid="bib48">Wacongne et al., 2011</xref>). On the other hand, other studies have proposed that omission responses could reflect pure predictions (<xref ref-type="bibr" rid="bib3">Bendixen et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">SanMiguel et al., 2013</xref>). While the exact nature of such responses is currently debated and likely dependent on factors such as task and relevance, the latency and topography of the omission response in our data resemble those evoked by a classical mismatch response (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>). Analysis of cortical sources also supports this interpretation. Indeed, source activity associated with omissions leads to stronger responses compared to tones over a distributed network of regions, including the bilateral inferior frontal gyri, premotor areas, angular gyri, and right STG. Since sensory predictive signals primarily arise in the same regions as the actual input, the activation of a broader network of regions in omission responses compared to tones suggests that omission responses reflect, at least in part, prediction error signals.</p><p>Importantly, we showed that when an unexpected omission disrupts a binary sequence of sounds, the amplitude of the omission MMN varies orthogonally depending on the speaker’s linguistic background. Omissions of long auditory events generate a larger omission MMN in the Basque compared to the Spanish group, while omissions of short sounds lead to a larger omission MMN responses in the Spanish compared to the Basque group. We hypothesized that this effect is linked to a long-term ‘duration prior’ originating from the acoustic properties of the two languages, specifically from the alternation of short and long auditory events in their prosody. Importantly, no difference between groups was detected in a control task in which tones alternate in frequency instead of duration, suggesting that the reported effect was driven by the hypothesized long-term linguistic priors instead of uncontrolled group differences.</p><p>It is important to note that both Spanish and Basque speakers are part of the same cultural community in Northern Spain. These languages share almost the same phonology and orthography. However, Basque is a non-Indo-European language (an isolated language) with no typological relationship with Spanish. It is thus very unlikely that the current findings are driven by cultural factors that are not language-specific (e.g., exposure to different musical traditions or educational and writing systems).</p><p>How would such long-term priors arise? One possible interpretation is that long-term statistical learning of the duration prosodic pattern of native language shapes the tuning properties of early auditory regions, affecting predictive coding at early stages. Such language-driven tuning is arguably important for reducing the prediction error during the segmentation of speech material into phrasal units, as it allows the auditory system to generate a functional coding scheme, or auditory template, against which the incoming speech input can be parsed. Such an auditory template is likely to be recycled by the auditory system to build top–down predictive models of non-linguistic auditory sequences.</p><p>The idea that the auditory system implements long-term predictions based on the prosodic structure of the native language could explain the previously reported behavioral influence of language experience on rhythmic grouping (<xref ref-type="bibr" rid="bib21">Iversen et al., 2008</xref>; <xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>): when listening to sequences of two tones alternating in duration, like those used in the present study, speakers of functor-initial languages report to perceive the rhythmic sequences as a repetition of ‘short–long’ units, while speakers of functor-final languages have the opposite ‘long–short’ grouping bias (<xref ref-type="bibr" rid="bib21">Iversen et al., 2008</xref>; <xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>). Despite lacking a direct behavioral assessment (but see <xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>, for related behavioral evidence), our results indicate that this perceptual grouping effect can be explained within a predictive coding framework that incorporates long-term prior knowledge into perceptual decisions. Under such an account, the auditory system internalizes the statistics underlying the prosodic structure of language and uses this knowledge to make long-term predictions of incoming sound sequences. Such long-term predictions would bias auditory processing at early, rather than later decision-making stages, affecting how rhythmic sounds are experienced.</p><p>Our work capitalized on a specific aspect of natural sound acoustic – the duration pattern in Basque and Spanish prosody – as a testbed to assess the presence of long-term priors in the auditory system. Despite our work being restricted to this specific feature, it is likely that the auditory system forms several other types of long-term priors using the spectrotemporal features that dominate the auditory environment. Support for this claim comes from (1) studies showing that the human auditory system uses the statistics underlying the acoustic structure of speech and music to form perceptual grouping decisions (<xref ref-type="bibr" rid="bib28">Młynarski and McDermott, 2019</xref>); and (2) behavioral experiments reporting off-line effects of language experience on auditory perception based on different acoustic features (<xref ref-type="bibr" rid="bib24">Liu et al., 2023</xref>). For instance, native speakers of languages in which pitch carries phonemically meaningful information (i.e., tone languages, e.g., Mandarin Chinese) benefit from a behavioral advantage in non-linguistic pitch discrimination tasks as compared to speakers of non-tone languages like English (<xref ref-type="bibr" rid="bib6">Bidelman et al., 2013</xref>). Similarly, speakers of languages that use duration to differentiate between phonemes (e.g., Finnish, Japanese) manifest an enhanced ability to discriminate the duration of non-linguistic sounds (<xref ref-type="bibr" rid="bib43">Tervaniemi et al., 2006</xref>). Our results, in conjunction with these studies, suggest that the auditory system forms long-term priors and predictions over development, using the co-occurrences that dominate the natural stimulus statistics. Yet, our results leave open the question of whether these long-term priors can be updated during adulthood, following extensive exposure to new statistical dependencies. This can be tested by exposing adult speakers to natural sounds encoding rules that ‘violate’ the long-term prior (e.g., a language with opposite prosodic structure) and exploring the effects of such short-term exposure to behavioral and neural performance.</p><p>One potential alternative to the conjecture that the ‘duration prior’ is linked to the spectro-temporal features of a language is that the prior depends on abstract syntactic/word-order rules. This latter account would predict that violations of long-term predictions in our study would lead to larger error responses in regions sensitive to syntactic variables, such as the left IFG (<xref ref-type="bibr" rid="bib4">Ben Shachar et al., 2003</xref>). Instead, the former account would predict that violations of long-term predictions elicit stronger responses in early left-lateralized auditory regions, which are putatively associated with early speech processing (<xref ref-type="bibr" rid="bib5">Bhaya-Grossman and Chang, 2022</xref>). The reconstruction of cortical sources associated with the omission of short and long tones in the two groups showed that an interaction effect mirroring the one at the sensor level was present in the left STG, but not in the left IFG (<xref ref-type="fig" rid="fig3">Figure 3B–D</xref>). Pairwise comparisons within different ROIs of the left STG indicated that the interaction effect was stronger over primary (BA 41/42) rather than associative (BAs 22) portions of the auditory cortex. Overall, these results suggest that the ‘duration prior’ is linked to the acoustic properties of a given language rather than its syntactic configurations.</p><p>Our results are in line with predictive coding models stating that predictions are organized hierarchically. When two predictive signals, one short-term and one long-term are violated, the amplitude of the prediction error is larger compared to a scenario in which only one short-term prediction is violated. This result complements previous studies using the local–global design showing that the same deviancy presented in different contexts gives rise to different error signals, such as the MMN and the P3 (<xref ref-type="bibr" rid="bib2">Bekinschtein et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Wacongne et al., 2011</xref>). These studies provide empirical evidence that predictive coding of auditory sequences is organized at different functional levels, with early sensory regions using transition probabilities to generate expectations about the present, and frontal and associative regions inferring the global structure of an auditory event. Our results extend this work by providing direct evidence for the presence of a system in the auditory cortex that uses long-term natural sound statistics to generate long-term predictions. This interpretation is also supported by the reconstruction of cortical sources. Indeed, while the overall omission effect is larger in the right hemisphere (<xref ref-type="fig" rid="fig2">Figures 2A</xref> and <xref ref-type="fig" rid="fig3">3A</xref>), the interaction effect arises in the left hemisphere (<xref ref-type="fig" rid="fig2">Figures 2C</xref> and <xref ref-type="fig" rid="fig3">3B, C</xref>). This finding further suggests that distinct cortical systems, supporting different predictive models, underlie the generation of the omission MMN.</p><p>Our findings are also consistent with more recent predictive coding models incorporating the idea of ‘stubborn’ predictive signals – that is, predictions resilient to model updates. Unlike short-term expectations, long-term predictions are usually implemented as a computational constraint on input data, thus being largely unaffected by short-term experience (<xref ref-type="bibr" rid="bib44">Teufel and Fletcher, 2020</xref>). In our study, the deployment of long-term predictions does not represent an effective coding strategy to perform the task. Yet, listeners still seem to assign different weights to incoming data, using a ‘default’ predictive coding scheme that resembles the segmentation strategy used to parse speech material. Why should a neural system rely on such stubborn priors even when irrelevant to solving a given perceptual task? One possibility is that implementing stable priors as a constraint on perception is computationally less expensive in terms of metabolic costs than recalibrating cortical internal models anew based on any type of novel experience. Another possibility is that relying on unchanging predictive schemes helps the system to form coherent models in front of environmental contingencies, thus reflecting an effective computational strategy for the reduction of the long-term prediction error. Defining how stubborn predictions emerge during learning and what their computational role is represents an important challenge to understanding the role of prior experience in perceptual inference.</p><p>We also reported a main effect of omission type, indicating that the MMN generated by the omission of a long tone was generally larger compared to that generated by the omission of a short one. Because such group effect was consistent only in the Basque group, it is possible that it merely reflects a larger sensitivity of the auditory system of this group to the omission of long events, in line with the interaction reported above. Alternatively, this effect could be driven by the fact that, during language processing, major predictive resources are invested in predicting the onset of long events, compared to short ones, as the formers usually refer to content words that is, semantically relevant events. Consequently, the auditory system may apply a similar predictive scheme also during the processing of non-linguistic sound sequences, independently of language background. Independently on the interpretation, the lack of a main effect of omission type in the control condition suggests that the long omission effect is driven by experience with the native language.</p><p>Our results also refine previous studies showing modulatory effects of (long-term) musical expertise on the MMN (e.g., <xref ref-type="bibr" rid="bib46">Vuust et al., 2005</xref>; <xref ref-type="bibr" rid="bib47">Vuust et al., 2009</xref>). These studies indicate that responses to violation during auditory rhythm perception are larger when the listener is an expert musician compared to a non-musician, pointing to the role of long-term auditory experience in shaping early predictive mechanisms. In our study, we manipulated long-term prediction orthogonally, with clear-cut predictions about the effect of language experience on early auditory predictive processing. Our results thus provide direct evidence for the presence of an active system in the auditory cortex that uses long-term priors to constrain information processing of incoming auditory stimuli.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>In total, 20 native speakers of Spanish (mean age: 25.6 years, range: 20–33, 13 females) and 20 native speakers of Basque (mean age: 27.11 years, range: 22–40, 17 females) took part in the experiment. It must be noted that in the original <xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref> experiment, a sample size of 16 subjects per group was sufficient to detect a behavioral perceptual grouping effect (under the request of a reviewer, we report a post hoc power analysis indicating an achieved power of 46% for medium effect sizes (<italic>d</italic> = 0.5, and alpha = 0.05, one-sided test) in a between-groups design with 20 subjects per group; while a sensitivity analysis indicates that the experiment possesses 80% power for effect sizes of <italic>d</italic> = 0.8 and above). Members of the two groups were selected based on self-reported scores for exposure (percentage of time exposed to a given language at the time of testing) and speaking (percentage of time speaking a given language at the time of testing). Participants from the Basque group were living in a Basque-speaking region of the Basque Country. They all reported having learned Basque as a first language, being primarily exposed to Basque during daily life (mean exposure: 69%; standard deviation [SD]: 13.28; range: 50–90%) and using it as a main language for communication (mean speaking: 77%; SD: 10.56; range: 60–90%). All native speakers of Basque reported having learned Spanish as a second language. However, they had overall low exposure (mean exposure: 22%; SD: 10.31; range: 10–40%) and speaking scores for Spanish (mean speaking: 17%; SD: 7.33; range: 10–30%). In this respect, it is important to notice that previous behavioral studies on perceptual grouping in Basque bilinguals showed that language dominance is the main factor driving non-linguistic rhythmic grouping (<xref ref-type="bibr" rid="bib29">Molnar et al., 2016</xref>). Therefore, despite limited exposure to the Spanish language, the formation of the hypothesized ‘duration prior’ in the Basque group should be primarily linked to experience with the dominant language (i.e., Basque), with no or only minimal influence from Spanish. Participants from the Spanish dominant group were coming from different regions of Spain. All of them learned Spanish as their first language, and had high self-reported scores for Spanish exposure (mean exposure: 79%; SD: 9.67; range: 60–100%) and speaking (mean speaking: 88.5%; SD: 6.7; range: 80–100%). Spanish participants reported having learned a second or third language after childhood (e.g., Basque, English, Italian, and Catalan).</p><p>Participants were recruited through the participant recruitment system of the Basque Center on Cognition, Brain and Language. The experiment and methods received approval from both the ethical committee and scientific committee of the Basque Center on Cognition, Brain and Language (Ethics approval number: 18072018M), following with the principles of the Declaration of Helsinki. Written informed consent was obtained from all participants in line with the guidelines of the Research Committees of the BCBL.</p></sec><sec id="s4-2"><title>Stimuli and experimental design</title><p>Stimuli were created using Matlab Psychtoolbox and presented binaurally via MEG-compatible headphones. Experimental stimuli consisted of 60 sequences of two tones alternating in duration (short tones: 0.250 s; long tones: 0.437 s) with fixed inter-stimulus intervals (0.020 s). Both long and short tones had a frequency of 500 Hz. The beginning and end of each tone were faded in and out of 0.015 s. Overall, each sequence consisted of 40 short- to long-tone pairs, for a total of 80 unique tones per sequence, and lasted around 30 s. Half of the sequences started with a long tone and half with a short tone. The beginning and the end of each sequence were faded in and faded out of 2.5 s to mask possible grouping biases. In each sequence, two to six tones were omitted and substituted with a 0.6-s silence gap. The larger gap was introduced to avoid that activity related to the onset of the tone following the omission overlaps with the activity generated by the omitted tone. Tone omissions occurred pseudorandomly, for a total of 240 omissions (120 short and 120 long). The pseudorandomization of the omissions consisted in separating the omissions within each sequence of at least seven tones. In the control condition, sequences consisted of tones alternating in frequency at fixed inter-stimulus intervals (0.020 s). High-frequency tones had a frequency of 700 Hz, while low-frequency tones had a frequency of 300 Hz. Both high- and low-frequency tones had an overall duration of 0.343 s. This duration was selected to keep the overall length of the sequences equal to that of the test condition, by keeping the total number of 80 tones per sequence. As in the test condition, tones and sequences were faded in and out of 0.015 and 2.5 s, respectively. In each sequence, two to six tones were omitted and substituted with a 0.600-s silence gap.</p><p>Overall, the experiment was divided into two main blocks: test and control. The order in which the blocks were presented was counterbalanced across participants. Each block consisted of 60 sequences and lasted around 35 min. Each sequence was separated by an 8-s silence gap. Every 20 sequences, a short pause was introduced. The end of each block was followed by a longer pause.</p><p>Participants were requested to minimize movement throughout the experiment, except during pauses. Subjects were asked to keep their eyes open, to avoid eye movements by fixating on a cross on the screen. Similar to previous studies, the only task that was asked to subjects was to count how many omissions were present in each sequence (e.g., <xref ref-type="bibr" rid="bib2">Bekinschtein et al., 2009</xref>) – and report it at the end of the sequence during the 8-s silence gap. Participants only received instructions at the very beginning of the task, and no verbal or written instructions were introduced during the task.</p></sec><sec id="s4-3"><title>MEG recordings</title><p>Measurements were carried out with the Elekta Neuromag VectorView system (Elekta Neuromag) of the Basque Center on Cognition Brain and Language, which comprises 204 planar gradiometers and 102 magnetometers in a helmet-shaped array. Electrocardiogram (ECG) and electrooculogram (EOG) (horizontal and vertical) were recorded simultaneously as auxiliary channels. MEG and auxiliary channels were low-pass filtered at 330 Hz, high-pass filtered at 0.03 Hz, and sampled at 1 kHz. The head position with respect to the sensor array was determined by five head-position indicator coils attached to the scalp. The locations of the coils were digitized with respect to three anatomical landmarks (nasion and preauricular points) with a 3D digitizer (Polhemus Isotrak system). Then, the head position with respect to the device origin was acquired before each block.</p></sec><sec id="s4-4"><title>Preprocessing</title><p>Signal space separation correction, head movement compensation, and bad channels correction were applied using the MaxFilter Software 2.2 (Elekta Neuromag). After that, data were analyzed using the FieldTrip toolbox (<xref ref-type="bibr" rid="bib32">Oostenveld et al., 2011</xref>) in Matlab (MathWorks). Trials were initially epoched from 1.200 s before to 1.200 s after the onset of each tone or omitted tone. Epochs time-locked to the onset of short and long tones were undersampled to match approximately the number of their corresponding omissions. Trials containing muscle artifacts and jumps in the MEG signal were detected using an automatic procedure and removed after visual inspection. Subsequently, independent component analysis (<xref ref-type="bibr" rid="bib25">Makeig et al., 1995</xref>) was performed to partially remove artifacts attributable to eye blinks and heartbeat artifacts (<xref ref-type="bibr" rid="bib22">Jung et al., 2000</xref>). To facilitate the detection of components reflecting eye blinks and heartbeat artifacts, the coherence between all components and the ECG/EOG electrodes was computed. Components were inspected visually before rejection. On average, we removed 14.28% (SD = 5.71) of the trials and 2.45 (SD = 0.55) components per subject. After artifact rejection, trials were low-pass filtered at 40 Hz and averaged per condition and per subject. ERFs were baseline corrected using the 0.050 s preceding trial onset and resampled to 256 Hz. The latitudinal and longitudinal gradiometers were combined by computing the root mean square of the signals at each sensor position to facilitate the interpretation of the sensor-level data.</p></sec><sec id="s4-5"><title>ERF analysis</title><p>Statistical analyses were performed using FieldTrip (<xref ref-type="bibr" rid="bib32">Oostenveld et al., 2011</xref>) in Matlab 2014 (MathWorks) and R studio for post hoc analysis. For data visualization, we used Matlab or FieldTrip plotting functions, R studio and the RainCloud plots tool (<xref ref-type="bibr" rid="bib1">Allen et al., 2019</xref>). Plots were then arranged as cohesive images using Inkscape (<ext-link ext-link-type="uri" xlink:href="https://inkscape.org/">https://inkscape.org/</ext-link>). All comparisons were performed on combined gradiometer data. For statistical analyses, we used a univariate approach in combination with cluster-based permutations (<xref ref-type="bibr" rid="bib26">Maris and Oostenveld, 2007</xref>) for family-wise error correction. This type of test controls the type I error rate in the context of multiple comparisons by identifying clusters of significant differences over space and time, instead of performing a separate test on each sensor and sample pair. Two-sided paired- and independent-samples <italic>t</italic>-tests were used for within- and between-subjects contrasts, respectively. The minimum number of neighboring channels required for a sample to be included in the clustering algorithm was set at 3. The cluster-forming alpha level was set at 0.05. The cluster-level statistic was the maximum sum of <italic>t</italic>-values (maxsum) and the number of permutations was set to 100,000. To control for the false alarm rate, we selected the standard <italic>α</italic> = 0.05. For the first analysis only, in which we compared ERF generated by pure tones vs omitted tones, we used a time window between −0.050 and 0.350 s and considered both the spatial and temporal dimensions in the cluster-based permutation test. This explorative analysis was performed to assess the effect of unexpected omission, as well as its temporal unfolding. In all the remaining analyses, MMN responses were calculated by subtracting the ERFs of each type of tone (i.e., long, short) from the ERFs of the corresponding omission. Moreover, all the contrasts were conducted using the average activity in the latency range between 0.100 and 0.250 s, which covers the typical latency of the MMN (<xref ref-type="bibr" rid="bib31">Näätänen et al., 2007</xref>; <xref ref-type="bibr" rid="bib17">Garrido et al., 2009</xref>). This approach uses spatial clusters to test for the difference between conditions. When multiple clusters emerged from a comparison, only the most significant cluster was reported. When an interaction effect was detected, post hoc analyses were performed to assess its directionality. This was done by averaging data for each participant and condition over the preselected latency and channels belonging to the significant clusters (see <italic>Results</italic>). The ERFs generated by this averaging were compared using one-sided independent sample <italic>t</italic>-tests. All p-values resulting from the post hoc <italic>t</italic>-test comparisons were FDR-corrected for multiple comparisons. All effect sizes reported are Cohen’s <italic>d</italic> (<xref ref-type="bibr" rid="bib11">Cohen, 2013</xref>).</p></sec><sec id="s4-6"><title>Source reconstruction</title><p>Source reconstruction mainly focused on the statistically significant effects observed at the sensor-level ERF analysis. Individual T1-weighted magnetic resonance imaging (MRI) images were first segmented into scalp, skull, and brain components using the segmentation algorithm implemented in Fieldtrip (<xref ref-type="bibr" rid="bib32">Oostenveld et al., 2011</xref>). A standard Montreal Neurological Institute (MNI) brain template available in SPM toolbox was used for the participants whose MRI was not available. Co-registration of the anatomical MRI images with MEG signal was performed using the manual co-registration tool available in Fieldtrip. The source space was defined as a regular 3D grid with a 5-mm resolution, and the lead fields were computed using a single-sphere head model for three orthogonal source orientations. The whole-brain cortical sources of the MEG signals were estimated using an LCMV beamformer approach (<xref ref-type="bibr" rid="bib45">Van Veen et al., 1997</xref>). Only planar gradiometers were used for modeling source activity. First, we compared the average source activity associated with pure omissions and standard tones in the time range of 0.100–0.250 s after the stimulus onset, extracted from the individual epochs. The covariance matrix used to derive LCMV beamformer weights was calculated from the 0.2 s preceding and 0.4 s following the onset of events (i.e., tones and omissions). Neural activity index (NAI) was computed as the ratio of source activity of tone to the omission (NAI = <italic>S</italic><sub>Omission</sub>/<italic>S</italic><sub>Tone</sub>). A non-linear transformation using the spatial-normalization algorithm (implemented in SPM8; <xref ref-type="bibr" rid="bib15">Friston et al., 1994</xref>) was employed to transform individual MRIs to the standard MNI brain. The source maps were plotted using the Surf Ice tool (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/surfice/">https://www.nitrc.org/projects/surfice/</ext-link>).</p><p>Furthermore, we selected predefined ROIs for the subsequent analysis using the Brainnetome atlas (<xref ref-type="bibr" rid="bib13">Fan et al., 2016</xref>). The ROIs included three areas within the left STG (BA 41/42 of the auditory cortex, rostral portion of BA 22, and caudal portion of BA 22) and left IFG (dorsal, ventral, and opercular portions of BA 44). We created an LCMV filter using the same parameters used for whole-brain source reconstruction. The virtual electrode in the source space corresponding to each ROI was generated. Singular vector decomposition was applied to select the component with maximum variance. Later, the differential omission MMN was computed by subtracting the power of the long tones from the power of the long omissions, and the power of the short tones from the power of the short omissions. This procedure was applied to each group separately (i.e., Spanish and Basque natives).</p><p>To examine the presence of an interaction effect at the source level, we first averaged the source activity time-series across ROIs for both the STG and IFG. A cluster-based permutation test was then applied on source activity data to identify temporal clusters over the 0.100–0.250 s time interval, following the same contrasts that were performed at the sensor level. This was done separately for the STG and IFG. This approach is similar to the one performed at the sensor level, but provides higher temporal sensitivity by considering individual time points within the 0.100–0.250 s interval. All the remaining parameters of the cluster-based permutation test are the same as the test used for the sensor-level analysis. When a cluster associated with a significant p-value was detected, the highest peak within the cluster was selected and used for the subsequent pairwise comparisons. Between-group comparisons were performed on each ROI and peak using a one-sided independent sample <italic>t</italic>-test, following the same contrasts employed at the sensor level.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Visualization, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Participants were recruited through the participant recruitment system of the Basque Center on Cognition, Brain and Language. The experiment and methods received approval from both the ethical committee and scientific committee of the Basque Center on Cognition, Brain and Language (Ethics approval number: 18072018M), following with the principles of the Declaration of Helsinki. Written informed consent was obtained from all participants in line with the guidelines of the Research Committees of the BCBL.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91636-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and codes for reproducing the analysis and figures are publicly available via the Open Science Framework (OSF): <ext-link ext-link-type="uri" xlink:href="https://osf.io/6jep8/">https://osf.io/6jep8/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Morucci</surname><given-names>P</given-names></name><name><surname>Molinaro</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Language experience shapes predictive coding of rhythmic sound sequences</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/6jep8/">osf.io/6jep8</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the Basque Government through the BERC 2022–2025 program and by the Spanish State Research Agency through BCBL Severo Ochoa excellence accreditation CEX2020-001010-S. Work by PM received support from 'la Caixa' Foundation (ID 100010434) through the fellowship LCF/BQ/IN17/11620019, and the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement no. 713673. CM received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant Agreement No: 819093), the Spanish Ministry of Economy and Competitiveness (PID2020-113926GB-I00), and the Basque Government (PIBA18_29). NM was supported by the Spanish Ministry of Economy and Competitiveness (PSI2015-65694-P, RTI2018-096311-B-I00, PDC2022-133917-I00). Work by ML received support from Juan de la Cierva IJC2020-042886-I. SN acknowledges the support from 'The Adaptive Mind', funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art. We wish to express our gratitude to the BCBL lab staff and the research assistants who helped to recruit the participants and collect the data. We thank Ram Frost for providing helpful comments on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>M</given-names></name><name><surname>Poggiali</surname><given-names>D</given-names></name><name><surname>Whitaker</surname><given-names>K</given-names></name><name><surname>Marshall</surname><given-names>TR</given-names></name><name><surname>van Langen</surname><given-names>J</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Raincloud plots: a multi-platform tool for robust data visualization</article-title><source>Wellcome Open Research</source><volume>4</volume><elocation-id>63</elocation-id><pub-id pub-id-type="doi">10.12688/wellcomeopenres.15191.2</pub-id><pub-id pub-id-type="pmid">31069261</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bekinschtein</surname><given-names>TA</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Rohaut</surname><given-names>B</given-names></name><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural signature of the conscious processing of auditory regularities</article-title><source>PNAS</source><volume>106</volume><fpage>1672</fpage><lpage>1677</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809667106</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bendixen</surname><given-names>A</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name><name><surname>Winkler</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>I heard that coming: event-related potential evidence for stimulus-driven prediction in the auditory system</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>8447</fpage><lpage>8451</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1493-09.2009</pub-id><pub-id pub-id-type="pmid">19571135</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben Shachar</surname><given-names>M</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Kahn</surname><given-names>I</given-names></name><name><surname>Ben Bashat</surname><given-names>D</given-names></name><name><surname>Grodzinsky</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The neural reality of syntactic transformations: evidence from functional magnetic resonance imaging</article-title><source>Psychological Science</source><volume>14</volume><fpage>433</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.01459</pub-id><pub-id pub-id-type="pmid">12930473</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhaya-Grossman</surname><given-names>I</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Speech computations of the human superior temporal gyrus</article-title><source>Annual Review of Psychology</source><volume>73</volume><fpage>79</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-022321-035256</pub-id><pub-id pub-id-type="pmid">34672685</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname><given-names>GM</given-names></name><name><surname>Hutka</surname><given-names>S</given-names></name><name><surname>Moreno</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tone language speakers and musicians share enhanced perceptual and cognitive abilities for musical pitch: evidence for bidirectionality between the domains of language and music</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e60676</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0060676</pub-id><pub-id pub-id-type="pmid">23565267</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname><given-names>M</given-names></name><name><surname>Molinaro</surname><given-names>N</given-names></name><name><surname>Wens</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Contrasting functional imaging parametric maps: The mislocation problem and alternative solutions</article-title><source>NeuroImage</source><volume>169</volume><fpage>200</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.033</pub-id><pub-id pub-id-type="pmid">29247806</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwer</surname><given-names>FL</given-names></name><name><surname>Werner</surname><given-names>CM</given-names></name><name><surname>Knetemann</surname><given-names>M</given-names></name><name><surname>Honing</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling beat perception from sequential learning and examining the influence of attention and musical abilities on ERP responses to rhythm</article-title><source>Neuropsychologia</source><volume>85</volume><fpage>80</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.02.018</pub-id><pub-id pub-id-type="pmid">26972966</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brochard</surname><given-names>R</given-names></name><name><surname>Abecasis</surname><given-names>D</given-names></name><name><surname>Potter</surname><given-names>D</given-names></name><name><surname>Ragot</surname><given-names>R</given-names></name><name><surname>Drake</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The “ticktock” of our internal clock: direct brain evidence of subjective accents in isochronous sequences</article-title><source>Psychological Science</source><volume>14</volume><fpage>362</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.24441</pub-id><pub-id pub-id-type="pmid">12807411</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Statistical Power Analysis for the Behavioral Sciences</source><publisher-name>Routledge</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Zhuo</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Chu</surname><given-names>C</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Jiang</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human brainnetome atlas: A new brain atlas based on connectional architecture</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3508</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw157</pub-id><pub-id pub-id-type="pmid">27230218</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>C</given-names></name><name><surname>Morlet</surname><given-names>D</given-names></name><name><surname>Giard</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mismatch negativity and N100 in comatose patients</article-title><source>Audiology &amp; Neuro-Otology</source><volume>5</volume><fpage>192</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1159/000013880</pub-id><pub-id pub-id-type="pmid">10859413</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Worsley</surname><given-names>KJ</given-names></name><name><surname>Poline</surname><given-names>JP</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistical parametric maps in functional imaging: A general linear approach</article-title><source>Human Brain Mapping</source><volume>2</volume><fpage>189</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido</surname><given-names>MI</given-names></name><name><surname>Kilner</surname><given-names>JM</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The mismatch negativity: a review of underlying mechanisms</article-title><source>Clinical Neurophysiology</source><volume>120</volume><fpage>453</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2008.11.029</pub-id><pub-id pub-id-type="pmid">19181570</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>AR</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Great expectations: Is there evidence for predictive coding in auditory cortex?</article-title><source>Neuroscience</source><volume>389</volume><fpage>54</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.07.061</pub-id><pub-id pub-id-type="pmid">28782642</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>HC</given-names></name><name><surname>Darcey</surname><given-names>TM</given-names></name><name><surname>Barkan</surname><given-names>HI</given-names></name><name><surname>Williamson</surname><given-names>PD</given-names></name><name><surname>Roberts</surname><given-names>DW</given-names></name><name><surname>Aslin</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Responses of human auditory association cortex to the omission of an expected acoustic event</article-title><source>NeuroImage</source><volume>13</volume><fpage>1073</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0766</pub-id><pub-id pub-id-type="pmid">11352613</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iversen</surname><given-names>JR</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Ohgushi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perception of rhythmic grouping depends on auditory experience</article-title><source>The Journal of the Acoustical Society of America</source><volume>124</volume><fpage>2263</fpage><lpage>2271</lpage><pub-id pub-id-type="doi">10.1121/1.2973189</pub-id><pub-id pub-id-type="pmid">19062864</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>TP</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name><name><surname>Westerfield</surname><given-names>M</given-names></name><name><surname>Townsend</surname><given-names>J</given-names></name><name><surname>Courchesne</surname><given-names>E</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects</article-title><source>Clinical Neurophysiology</source><volume>111</volume><fpage>1745</fpage><lpage>1758</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(00)00386-2</pub-id><pub-id pub-id-type="pmid">11018488</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ladinig</surname><given-names>O</given-names></name><name><surname>Honing</surname><given-names>H</given-names></name><name><surname>Háden</surname><given-names>G</given-names></name><name><surname>Winkler</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Probing attentive and preattentive emergent meter in adult listeners without extensive music training</article-title><source>Music Perception</source><volume>26</volume><fpage>377</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1525/mp.2009.26.4.377</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Hilton</surname><given-names>CB</given-names></name><name><surname>Bergelson</surname><given-names>E</given-names></name><name><surname>Mehr</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Language experience predicts music processing in a half-million speakers of fifty-four languages</article-title><source>Current Biology</source><volume>33</volume><fpage>1916</fpage><lpage>1925</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.03.067</pub-id><pub-id pub-id-type="pmid">37105166</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Makeig</surname><given-names>S</given-names></name><name><surname>Bell</surname><given-names>A</given-names></name><name><surname>Jung</surname><given-names>TP</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Independent component analysis of electroencephalographic data</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Statistical learning of visual transitions in monkey inferotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>19401</fpage><lpage>19406</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112895108</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname><given-names>W</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ecological origins of perceptual grouping principles in the auditory system</article-title><source>PNAS</source><volume>116</volume><fpage>25355</fpage><lpage>25364</lpage><pub-id pub-id-type="doi">10.1073/pnas.1903887116</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molnar</surname><given-names>M</given-names></name><name><surname>Carreiras</surname><given-names>M</given-names></name><name><surname>Gervain</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Language dominance shapes non-linguistic rhythmic grouping in bilinguals</article-title><source>Cognition</source><volume>152</volume><fpage>150</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.03.023</pub-id><pub-id pub-id-type="pmid">27062227</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Sussman</surname><given-names>E</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Winkler</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>“Primitive intelligence” in the auditory cortex</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>283</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01790-2</pub-id><pub-id pub-id-type="pmid">11311381</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The mismatch negativity (MMN) in basic research of central auditory processing: a review</article-title><source>Clinical Neurophysiology</source><volume>118</volume><fpage>2544</fpage><lpage>2590</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.026</pub-id><pub-id pub-id-type="pmid">17931964</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname><given-names>DD</given-names></name><name><surname>Fenwick</surname><given-names>M</given-names></name><name><surname>Abecasis</surname><given-names>D</given-names></name><name><surname>Brochard</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perceiving rhythm where none exists: event-related potential (ERP) correlates of subjective accenting</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>45</volume><fpage>103</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2008.01.004</pub-id><pub-id pub-id-type="pmid">19027894</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raij</surname><given-names>T</given-names></name><name><surname>McEvoy</surname><given-names>L</given-names></name><name><surname>Mäkelä</surname><given-names>JP</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Human auditory cortex is activated by omissions of auditory stimuli</article-title><source>Brain Research</source><volume>745</volume><fpage>134</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/s0006-8993(96)01140-7</pub-id><pub-id pub-id-type="pmid">9037402</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sallinen</surname><given-names>M</given-names></name><name><surname>Kaartinen</surname><given-names>J</given-names></name><name><surname>Lyytinen</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Is the appearance of mismatch negativity during stage 2 sleep related to the elicitation of K-complex?</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>91</volume><fpage>140</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(94)90035-3</pub-id><pub-id pub-id-type="pmid">7519143</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Auditory frequency discrimination and event-related potentials</article-title><source>Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section</source><volume>62</volume><fpage>437</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1016/0168-5597(85)90054-1</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SanMiguel</surname><given-names>I</given-names></name><name><surname>Widmann</surname><given-names>A</given-names></name><name><surname>Bendixen</surname><given-names>A</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hearing silences: human auditory processing relies on preactivation of sound-specific brain activity patterns</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>8633</fpage><lpage>8639</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5821-12.2013</pub-id><pub-id pub-id-type="pmid">23678108</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sassenhagen</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cluster-based permutation tests of MEG/EEG data do not establish significance of effect latency or location</article-title><source>Psychophysiology</source><volume>56</volume><elocation-id>e13335</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13335</pub-id><pub-id pub-id-type="pmid">30657176</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sculthorpe</surname><given-names>LD</given-names></name><name><surname>Ouellet</surname><given-names>DR</given-names></name><name><surname>Campbell</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>MMN elicitation during natural sleep to violations of an auditory pattern</article-title><source>Brain Research</source><volume>1290</volume><fpage>52</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2009.06.013</pub-id><pub-id pub-id-type="pmid">19527697</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname><given-names>P</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning what to expect (in visual perception)</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>668</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00668</pub-id><pub-id pub-id-type="pmid">24187536</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strauss</surname><given-names>M</given-names></name><name><surname>Sitt</surname><given-names>JD</given-names></name><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Elbaz</surname><given-names>M</given-names></name><name><surname>Azizi</surname><given-names>L</given-names></name><name><surname>Buiatti</surname><given-names>M</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Disruption of hierarchical predictive coding during sleep</article-title><source>PNAS</source><volume>112</volume><fpage>E1353</fpage><lpage>E1362</lpage><pub-id pub-id-type="doi">10.1073/pnas.1501026112</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Jacobsen</surname><given-names>T</given-names></name><name><surname>Röttger</surname><given-names>S</given-names></name><name><surname>Kujala</surname><given-names>T</given-names></name><name><surname>Widmann</surname><given-names>A</given-names></name><name><surname>Vainio</surname><given-names>M</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Selective tuning of cortical sound-feature processing by language experience</article-title><source>The European Journal of Neuroscience</source><volume>23</volume><fpage>2538</fpage><lpage>2541</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.04752.x</pub-id><pub-id pub-id-type="pmid">16706861</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teufel</surname><given-names>C</given-names></name><name><surname>Fletcher</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Forms of prediction in the nervous system</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>231</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0275-5</pub-id><pub-id pub-id-type="pmid">32157237</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname><given-names>BD</given-names></name><name><surname>van Drongelen</surname><given-names>W</given-names></name><name><surname>Yuchtman</surname><given-names>M</given-names></name><name><surname>Suzuki</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Pallesen</surname><given-names>KJ</given-names></name><name><surname>Bailey</surname><given-names>C</given-names></name><name><surname>van Zuijen</surname><given-names>TL</given-names></name><name><surname>Gjedde</surname><given-names>A</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Østergaard</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>To musicians, the message is in the meter pre-attentive neuronal responses to incongruent rhythm are left-lateralized in musicians</article-title><source>NeuroImage</source><volume>24</volume><fpage>560</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.08.039</pub-id><pub-id pub-id-type="pmid">15627598</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Ostergaard</surname><given-names>L</given-names></name><name><surname>Pallesen</surname><given-names>KJ</given-names></name><name><surname>Bailey</surname><given-names>C</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Predictive coding of music--brain responses to rhythmic incongruity</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>45</volume><fpage>80</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2008.05.014</pub-id><pub-id pub-id-type="pmid">19054506</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Labyt</surname><given-names>E</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evidence for a hierarchy of predictions and prediction errors in human cortex</article-title><source>PNAS</source><volume>108</volume><fpage>20754</fpage><lpage>20759</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117807108</pub-id><pub-id pub-id-type="pmid">22147913</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>KS</given-names></name><name><surname>McGovern</surname><given-names>DP</given-names></name><name><surname>Clark</surname><given-names>A</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Evaluating the neurophysiological evidence for predictive processing as a model of perception</article-title><source>Annals of the New York Academy of Sciences</source><volume>1464</volume><fpage>242</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1111/nyas.14321</pub-id><pub-id pub-id-type="pmid">32147856</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yabe</surname><given-names>H</given-names></name><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Reinikainen</surname><given-names>K</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Temporal window of integration revealed by MMN to sound omission</article-title><source>Neuroreport</source><volume>8</volume><fpage>1971</fpage><lpage>1974</lpage><pub-id pub-id-type="doi">10.1097/00001756-199705260-00035</pub-id><pub-id pub-id-type="pmid">9223087</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yon</surname><given-names>D</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Press</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The predictive brain as a stubborn scientist</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>6</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.10.003</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91636.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chait</surname><given-names>Maria</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This study presents <bold>important</bold> observations about how the human brain uses long-term priors (acquired during our lifetime of listening) to make predictions about expected sounds - an open question in the field of predictive processing. The evidence presented is <bold>solid</bold> and based on state-of-the-art statistical analysis, but limited by a relatively low N and low magnitude for the interaction effect.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91636.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work, the authors study whether the human brain uses long term priors (acquired during our lifetime) regarding the statistics of auditory stimuli to make predictions respecting auditory stimuli. This is an important open question in the field of predictive processing.</p><p>To address this question, the authors cleverly profit from the naturally existing differences in two linguistic groups. While speakers of Spanish use phrases in which function-words (short words like, articles and prepositions) are followed by content-words (longer words like nouns, adjectives and verbs), speakers of Basque use phrases with the opposite order. Because of this, speakers of Spanish usually hear phrases in which short words are followed by longer words, and speakers of Basque experience the opposite. This difference in the order of short and longer words is hypothesized to result in a long term duration prior that is used to make predictions regarding the likely durations of incoming sounds, even if they are not linguistic in nature.</p><p>To test this, the authors used MEG to measure the mismatch responses (MMN) elicited by the omission of short and long tones that were presented in alternation. The authors report an interaction between the language background of the participants (Spanish, Basque) and the type of omission MMN (short, long), which goes in line with their predictions. They supplement these results with a source level analysis.</p><p>Strengths:</p><p>This work has many strengths. To test the main question, the authors profit from naturally occurring differences in the everyday auditory experiences of two linguistic groups, which allows to test the effect of putative auditory priors consolidated over the years. This is a direct way of testing the effect of long term priors.</p><p>The fact that the priors in question are linguistic and that the experiment was conducted using non-linguistic stimuli (i.e. simple tones), allows to test if these long term priors generalize across auditory domains.</p><p>The experimental design is elegant and the analysis pipeline appropriate. This work is very well written. In particular the introduction and discussion sections are clear and engaging. The literature review is complete.</p><p>Weaknesses:</p><p>The authors report a widespread omission response, which resembles the classical mismatch response (in MEG planar gradiometers) with strong activations in sensors over temporal regions. However the interaction reported is circumscribed to four sensors that do not overlap with the peaks of activation of the omission response.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91636.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Morucci et al. tested the influence of linguistic prosody long-term priors in forming predictions about simple acoustic rhythmic tone sequences composed of alternating tone duration, by violating context-dependent short-term priors formed during sequence listening. Spanish and Basque participants were selected due to the different rhythmic prosody of the two languages (functor-initial vs. Functor final, respectively), despite a common cultural background. The authors found that neuromagnetic responses to casual tone omissions reflected the linguistic prosody pattern of the participant's dominant language: in Spanish speakers, omission responses were larger to short tones, whereas in Basque speakers, omission responses were larger to long tones. Source localization of these responses revealed this interaction pattern in the left auditory cortex, which the authors interpret as reflecting a perceptual bias due to acoustic cues (inherent linguistic rhythms, rather than linguistic content). Importantly, this pattern was not found when the rhythmic sequence entailed pitch, rather than duration, cues. To my knowledge, this is the first study providing neural signatures of a known behavioral effect linking ambiguous rhythmic tone sequence perceptual organization to linguistic experience.</p><p>The conclusions of the study are well supported by the data. The hypotheses, albeit allowing alternative perspectives, are well justified according to the existing literature. Albeit with inconclusive results, additional analyses to test entrained oscillatory activity to the perceived rhythms have been performed, which adds explanatory power to the study.</p><p>Strengths:</p><p>(1) The choice of participants. The bilingual population of the Basque country is perfect for performing studies which need to control for cultural and socio-economic background while having profound linguistic differences. In this sense, having dominant Basque speakers as a sample equates that in Molnar et al. (2016), and thus overcomes the lack of direct behavioral evidence for a difference in rhythmic grouping across linguistic groups. Molnar et al. (2016)'s evidence on the behavioral effect is compelling, and the evidence on neural signatures provided by the present study aligns with it.</p><p>(2) The experimental paradigm. It is a well designed acoustic sequence, which considers aspects such as gap length insertion, to be able to analyze omission responses free from subsequent stimulus-driven responses, and which includes a control sequence which uses pitch instead of duration as a cue to rhythmic grouping, which provides a stronger case for the differences found between groups to be due to prosodic duration cues.</p><p>(3) Data analyses. Sound, state-of-the-art methodology in the event-related field analyses at the sensor and source levels.</p><p>Weaknesses:</p><p>(1) The main conclusion of the study reflects a known behavioral effect on rhythmic sequence perceptual organization driven by linguistic background (Molnar et al. 2016, particularly) and, thus, the novelty of the findings is restricted to neural activity evidence.</p><p>(2) Although the paradigm is well designed, there are alternative views in formulating the hypotheses. For instance, one could argue that, according to predictive coding views, omission responses should be larger when the gap occurs at the end of the pattern, as that would be where stronger expectations are placed. However, the authors provide good justification based on previous literature for the expectation of larger omission responses at the downbeat of a rhythmic pattern.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91636.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The paper investigates the effects of long-term linguistic experience on early auditory processing, a subject that has been relatively less studied compared to short-term influences. Using MEG, the study examines brain responses to auditory stimuli in speakers of Spanish and Basque, whose syntactic rules provide different degrees of exposure to durational patterns (long-short vs short-long). The findings suggest that both long-term language experience as well as short-term transitional probabilities can shape auditory predictive coding for non-linguistic sound sequences, evidenced by differences in mismatch negativity amplitudes localised to left auditory cortex.</p><p>Strengths:</p><p>The study integrates linguistics and auditory neuroscience in an interesting interdisciplinary way that may interest linguists as well as neuroscientists. The fact that long-term language experience affects early auditory predictive coding is important for understanding group and individual differences in domain-general auditory perception. It has importance for neurocognitive models of auditory perception (e.g. inclusion of long-term priors), and will be of interest to researchers in linguistics, auditory neuroscience, and the relationship between language and perception. The inclusion of a control condition based on pitch is also a strength.</p><p>Weaknesses:</p><p>The main weaknesses are the strength of the effects and generalisability. Only two languages were examined, Spanish and Basque. The sample size is also relatively small by today's standards, with N=20 in each group. Furthermore, the crucial effects are all mostly in the .01&gt;P&lt;.05 range, such as the crucial interaction P=.03, although I note the methods used to derive the results are sound and state-of-the-art. It would be nice to see it replicated in the future, with more participants and other languages. It would also have been nice to see behavioural data that could be correlated with neural data to better understand the real-world consequences of the effect.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91636.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Morucci</surname><given-names>Piermatteo</given-names></name><role specific-use="author">Author</role><aff><institution>University of Geneva</institution><addr-line><named-content content-type="city">Geneva</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Nara</surname><given-names>Sanjeev</given-names></name><role specific-use="author">Author</role><aff><institution>University of Giessen</institution><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Lizarazu</surname><given-names>Mikel</given-names></name><role specific-use="author">Author</role><aff><institution>Basque Center on Cognition Brain and Language</institution><addr-line><named-content content-type="city">Donostia-San Sebastian, Gipuzkoa</named-content></addr-line><country>Spain</country></aff></contrib><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Clara</given-names></name><role specific-use="author">Author</role><aff><institution>Basque Center on Cognition Brain and Language</institution><addr-line><named-content content-type="city">Donostia-San Sebastian, Gipuzkoa</named-content></addr-line><country>Spain</country></aff></contrib><contrib contrib-type="author"><name><surname>Molinaro</surname><given-names>Nicola</given-names></name><role specific-use="author">Author</role><aff><institution>BCBL</institution><addr-line><named-content content-type="city">Donostia/San Sebastian</named-content></addr-line><country>Spain</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the current reviews.</p><p>We thank the Reviewers and Editors for the constructive comments, which we believe have significantly improved the quality of our manuscript.</p><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>(1) With respect to the predictions, the authors propose that the subjects, depending on their linguistic background and the length of the tone in a trial, can put forward one or two predictions. The first is a short-term prediction based on the statistics of the previous stimuli and identical for both groups (i.e. short tones are expected after long tones and vice versa). The second is a long-term prediction based on their linguistic background. According to the authors, after a short tone, Basque speakers will predict the beginning of a new phrasal chunk, and Spanish speakers will predict it after a long tone.</p><p>In this way, when a short tone is omitted, Basque speakers would experience the violation of only one prediction (i.e. the short-term prediction), but Spanish speakers will experience the violation of two predictions (i.e. the short-term and long-term predictions), resulting in a higher amplitude MMN. The opposite would occur when a long tone is omitted. So, to recap, the authors propose that subjects will predict the alternation of tone durations (short-term predictions) and the beginning of new phrasal chunks (long-term predictions).</p><p>The problem with this is that subjects are also likely to predict the completion of the current phrasal chunk. In speech, phrases are seldom left incomplete. In Spanish is very unlikely to hear a function-word that is not followed by a content-word (and the opposite happens in Basque). On the contrary, after the completion of a phrasal chunk, a speaker might stop talking and a silence might follow, instead of the beginning of a new phrasal chunk.</p><p>Considering that the completion of a phrasal chunk is more likely than the beginning of a new one, the prior endowed to the participants by their linguistic background should make us expect a pattern of results actually opposite to the one reported here.</p></disp-quote><p>We thank the Reviewer #1 for this pertinent comment and the opportunity to address this issue. A very similar concern was also raised by Reviewer #2. Below we try to clarify the motivations that led us to predict that the hypothesized long-term predictions should manifest at the onset (and not within or the end) of a perceptual chunk.</p><p>Reviewers #1 and #2 contest a critical assumption of our study i.e., the fact that longterm predictions should occur at the beginning of a rhythmic chunk as opposed to its completion. They also contest the prediction deriving from this view i.e., omitting the first sound in a perceptual chunk (short for Spanish, long for Basque) would lead to larger error responses than omitting a later element. They suggest an alternative view: the omission of tones at the end of a perceptual rhythmic chunk would evoke larger error responses than omissions at its onset, as subjects are more likely to predict the completion of the chunk than its beginning. This view predicts an interaction effect in the opposite direction of our findings.</p><p>While we acknowledge this as a plausible hypothesis, we believe that the current literature provides strong support for our view. Indeed, many studies in the rhythm and music perception literature have investigated the ERP responses to deviant sounds and omissions placed at different positions within rhythmic patterns (e.g., Ladinig et al., 2009; Bouwer et al., 2016; Brochard et al., 2003; Potter et al., 2009; Yabe et al., 2001). For instance, Lading et al., 2009 presented participants with metrical rhythmical sound sequences composed of eight tones. In some deviant sequences, the first or a later tone was omitted. They found that earlier omissions elicited earlier and higher-amplitude MMN responses than later omissions (irrespective of attention). Overall, this and other studies showed that the amplitude of ERP responses are larger when deviants occur at positions that are expected to be the “start” of a perceptual group - “on the beat” in musical terms - and decline toward the end of the chunk. According to some of these studies, the first element of a chunk is particularly important to track the boundaries of temporal sequences, which is why more predictive resources are invested at that position. We believe that this body of evidence provides robust bases for our hypotheses and the directionality of our predictions.</p><p>An additional point that should be considered concerns the amplitude of the prediction error response elicited by the omission. From a predictive coding perspective, the omission of the onset of a chunk should elicit larger error responses because the system is expecting the whole chunk (i.e., two tones/more acoustic information). On the other hand, the omission of the second tone - in the transition between two tones within the chunk - should elicit a smaller error response because the system is expecting only the missing tone (i.e. less acoustic information).</p><p>Given the importance of these points, we have now included them in the updated version of the paper, in which we try to better clarify the rationale behind our hypothesis (see Introduction section, around the 10th paragraph).</p><disp-quote content-type="editor-comment"><p>(2) The authors report an interaction effect that modulates the amplitude of the omission response, but caveats make the interpretation of this effect somewhat uncertain. The authors report a widespread omission response, which resembles the classical mismatch response (in MEG) with strong activations in sensors over temporal regions. Instead, the interaction found is circumscribed to four sensors that do not overlap with the peaks of activation of the omission response.</p></disp-quote><p>We thank the Reviewer for this comment. As mentioned in the provisional response, the approach employed to identify the presence of an interaction effect was conservative: We utilized a non-parametric test on combined gradiometers data, without making a priori assumptions about the location of the effect, and employed small cluster thresholds (cfg.clusteralpha = 0.05) to increase the chances of detecting highly localized clusters with large effect sizes. The fact that the interaction effect arises in a relatively small cluster of sensors does not alter its statistical robustness. It should be also considered that in the present analyses we focused on planar gradiometer data that, compared to magnetometers and axial gradiometers, present more fine-grained spatial resolution and are more suited for picking up relatively small effects.</p><p>The partial overlap of the cluster with the activation peaks may simply reflect the fact that different sources contribute to the generation of the omission-MMN, which has been reported in several studies (e.g., Zhang et al., 2018; Ross &amp; Hamm, 2020). We value the Reviewer’s input and are grateful for the opportunity to address these considerations.</p><disp-quote content-type="editor-comment"><p>Furthermore, the boxplot in Figure 2E suggests that part of the interaction effect might be due to the presence of two outliers (if removed, the effect is no longer significant). Overall, it is possible that the reported interaction is driven by a main effect of omission type which the authors report, and find consistently only in the Basque group (showing a higher amplitude omission response for long tones than for short tones). Because of these points, it is difficult to interpret this interaction as a modulation of the omission response.</p></disp-quote><p>We thank the Reviewer for the comment and appreciate the opportunity to address these concerns. We have re-evaluated the boxplot in Figure 2E and want to clarify that the two participants mentioned by Reviewer #1, despite being somewhat distant from the rest of the group, are not outliers according to the standard Tukey’s rule. As shown in the figure below, no participant fell outside the upper (Q3+1.5xIQR) and lower whiskers (Q1-1.5xIQR) of the boxplot.</p><p>Moreover, we believe that the presence of a main effect of omission type does not impact the interpretation of the interaction, especially considering that these effects emerge over distinct clusters of channels (see Fig. 1 C; Supplementary Fig. 2 A).</p><p>Based on these considerations - and along with the evidence collected in the control study and the source reconstruction data reported in the new version of the manuscript - we find it unlikely that the interaction effect is driven by outliers or by a main effect of omission type. We appreciate the opportunity provided by the Reviewer to address these concerns, as we believe they strengthen the claim that the observed effect is driven by the hypothesized long-term linguistic priors rather than uncontrolled group differences.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-sa4-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>It should also be noted that in the source analysis, the interaction only showed a trend in the left auditory cortex, but in its current version the manuscript does not report the statistics of such a trend.</p></disp-quote><p>We appreciate the Reviewer’s suggestion to incorporate more comprehensive source analyses. In the new version of the paper, we perform new analyses on the source data using a new Atlas with more fine-grained parcellations of the regions of interests (ROIs) (Brainnetome atlas; Fan et al., 2016) and focusing on peak activity to increase response’s sensitivity in space and time. We therefore invite the Reviewer to read the updated part on source reconstruction included in the Results and Methods sections of the paper.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>While I have described my biggest concerns with respect to this work in the public review, here I list more specific points that I hope will help to improve the manuscript. Some of these are very minor, but I hope you will still find them constructive.</p><p>(1) I understand the difficulties implied in recruiting subjects from two different linguistic groups, but with 20 subjects per group and a between-groups design, the current study is somewhat underpowered. A post-hoc power analysis shows an achieved power of 46% for medium effect sizes (d = 0.5, and alpha = 0.05, one-sided test). A sensitivity analysis shows that the experiment only has 80% power for effect sizes of d = 0.8 and above. It would be important to acknowledge this limitation in the manuscript.</p></disp-quote><p>We thank the Reviewer for reporting these analyses. It must be noted that our effect of interest was based on Molnar et al.’s (2016) behavioral experiment, in which a sample size of 16 subjects per group was sufficient to detect the perceptual grouping effect. In Yoshida et al., (2010), the perceptual grouping effect emerged with two groups of 20 7–8-month-old Japanese and English-learning infants. Based on these previous findings, we believe that a sample size of 20 participants per group can be considered appropriate for the current MEG study. We clarified these aspects in the Participants section of the manuscript, in which we specified that previous behavioral studies detected the perceptual grouping with similar sample sizes. Moreover, to acknowledge the limitation highlighted by the Reviewer, we also include the power and sensitivity analysis in a note in the same section (see note 2 in the Participants section).</p><disp-quote content-type="editor-comment"><p>(2) All the line plots in the manuscript could be made much more informative by adding 95% CI bars. For example, in Figure 4A, the omission response for the long tone departs from the one for the short tone very early. Adding CIs would help to assess the magnitude of that early difference. Error bars are present in Figure 3, but it is not specified what these bars represent.</p></disp-quote><p>Thanks for the comments. We added the explanation of the error bars in the new version of Figure 3. For the remaining figures, we prefer maintaining the current version of the ERF, as the box-plots accompanying them provide information about the distribution of the effect across participants.</p><disp-quote content-type="editor-comment"><p>(3) In the source analysis, there is only mention of an interaction trend in the left auditory cortex, but no statistics are presented. If the authors prefer to mention such a trend, I think it would be important to provide its stats to allow the reader to assess its relevance.</p></disp-quote><p>We performed new analysis on the source data, all reported in the updated version of the manuscript.</p><disp-quote content-type="editor-comment"><p>(4) In the discussion section, the authors refer to the source analysis and state that &quot;the interaction is evident in the left&quot;. But if only a statistical trend was observed, this statement would be misleading.</p></disp-quote><p>We agree with this comment. We invite the Reviewer to check the new part on source reconstruction, in which contrasts going in the same direction of the sensor level data are performed.</p><disp-quote content-type="editor-comment"><p>(5) In the discussion the authors argue that &quot;This result highlights the presence of two distinct systems for the generation of auditory&quot; that operate at different temporal scales, but the current work doesn't offer evidence for the existence of two different systems. The effects of long-term priors and short-term priors presented here are not dissociated and instead sum up. It remains possible that a single system is in place, collecting statistics of stimuli over a lifetime, including the statistics experienced during the experiment.</p></disp-quote><p>Thanks for pointing that out. We changed the sentence above as follows: “This result highlights the presence of an active predictive system that relies on natural sound statistics learned over a lifetime to process incoming auditory input”.</p><disp-quote content-type="editor-comment"><p>(6) In the discussion, the authors acknowledge that the omission response has been interpreted both as pure prediction and as pure prediction error. Then they declare that &quot;Overall, these findings are consistent with the idea that omission responses reflect, at least in part, prediction error signals.&quot;. However an argument for this statement is not provided.</p></disp-quote><p>Thanks for pointing out this lack of argument. In the new version of the manuscript, we explained our rationale as follows: “Since sensory predictive signals primarily arise in the same regions as the actual input, the activation of a broader network of regions in omission responses compared to tones suggests that omission responses reflect, at least in part, prediction error signals”.</p><disp-quote content-type="editor-comment"><p>(7) In the discussion the authors present an alternative explanation in which both groups might devote more resources to the processing of long events, because these are relevant content words. Following this, they argue that &quot;Independently on the interpretation, the lack of a main effect of omission type in the control condition suggests that the long omission effect is driven by experience with the native language.&quot; However as there was no manipulation of duration in the control experiment, a lack of the main effect of omission type there does not rule out the alternative explanation that the authors put forward.</p></disp-quote><p>This is correct; thanks for noticing it. We removed the sentence above to avoid ambiguities.</p><disp-quote content-type="editor-comment"><p>Minor points:</p><p>(8) The scale of the y-axis in Figure 2C might be wrong, as it goes from 9 to 11 and then to 12. If the scale is linear, the top value should be 13, or the bottom value should be 10.</p></disp-quote><p>Figure 2C has been modified accordingly, thanks for noticing the error.</p><disp-quote content-type="editor-comment"><p>(9) There is a very long paragraph starting on page 7 and ending on page 8. Toward the end of the paragraph, the analysis of the control condition is presented. That could start a new paragraph.</p></disp-quote><p>Thanks for the suggestion. We modified the manuscript as suggested.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>(1) Despite the evidence provided on neural responses, the main conclusion of the study reflects a known behavioral effect on rhythmic sequence perceptual organization driven by linguistic background (Molnar et al. 2016, particularly). Also, the authors themselves provide a good review of the literature that evidences the influence of longterm priors in neural responses related to predictive activity. Thus, in my opinion, the strength of the statements the authors make on the novelty of the findings may be a bit far-fetched in some instances.</p></disp-quote><p>Thanks for the suggestion. A similar point was also advanced by Reviewer 1. In general, we believe our work speaks about the predictive nature of such experiencedependent effects, and show that these linguistic priors shape sensory processes at very early stages. This is discussed in the sixth and seventh paragraphs of the Discussion section. In the new version of the article, we modified some statements and tried to make them more coherent with the scope of the present work. For instance, we changed &quot;This result highlights the presence of two distinct systems for the generation of auditory predictive models, one relying on the transition probabilities governing the recent past, and another relying on natural sound statistics learned over a lifetime“ with “This result highlights the presence of an active predictive system that relies on natural sound statistics learned over a lifetime to process incoming auditory input”.</p><disp-quote content-type="editor-comment"><p>(2) Albeit the paradigm is well designed, I fail to see the grounding of the hypotheses laid by the authors as framed under the predictive coding perspective. The study assumes that responses to an omission at the beginning of a perceptual rhythmic pattern will be stronger than at the end. I feel this is unjustified. If anything, omission responses should be larger when the gap occurs at the end of the pattern, as that would be where stronger expectations are placed: if in my language a short sound occurs after a long one, and I perceptually group tone sequences of alternating tone duration accordingly, when I hear a short sound I will expect a long one following; but after a long one, I don't necessarily need to expect a short one, as something else might occur.</p></disp-quote><p>A similar point was advanced by Reviewer #1. We tried to clarify the rationale behind our hypothesis. Please refer to the response provided to the first comment of Reviewer #1 above.</p><disp-quote content-type="editor-comment"><p>(3) In this regard, it is my opinion that what is reflected in the data may be better accounted for (or at least, additionally) by a different neural response to an omission depending on the phase of an underlying attentional rhythm (in terms of Large and Jones rhythmic attention theory, for instance) and putative underlying entrained oscillatory neural activity (in terms of Lakatos' studies, for instance). Certainly, the fact that the aligned phase may differ depending on linguistic background is very interesting and would reflect the known behavioral effect.</p></disp-quote><p>We thank the Reviewer for this comment. We explored in more detail the possibility that the aligned phase may differ depending on linguistic background, which is indeed a very interesting hypothesis. In the phase analyses reported below we focused on the instantaneous phase angle time locked to the onset of short and long tones presented in the experiment.</p><p>In short, we extracted time intervals of two seconds centered on the onset of the tones for each participant (~200 trials per condition) and using a wavelet transform (implemented in Fieldtrip ft_freqanalysis) we targeted the 0.92 Hz frequency that corresponds to the rhythm of presentation of our pairs of tones. We extracted the phase angle for each time point and using the circular statistics toolbox implemented in Matlab we computed the Raleigh z scores across all the sensor space for each tone (long and short tone) and group (Spanish (Spa) dominants and Basque (Eus) dominants). This method evaluates the instantaneous phase clustering at a specific time point, thus evaluating the presence of a specific oscillatory pattern at the onset of the specific tone.</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-sa4-fig2-v1.tif"/></fig><p>Here we observe that the phase clustering was stronger in the right sensors for both groups. The critical point is to evaluate the phase angle (estimated in phase radians) for the two groups and the two tones and see if there are statistical differences. We focused first on the sensor with higher clustering (right temporal MEG1323) and observed very similar phase angles for the two groups both for long and short tones (see image below). We then focused on the four left fronto-temporal sensor pairs who showed the significant interaction: here we observed one sensor (MEG0412) with different effects for the two groups (interaction group by tone was significant, p=0.02): for short tones the “Watson (1961) approximation U2 test” showed a p-value of 0.11, while for long tones the p-value was 0.03 (after correction for multiple comparisons).</p><p>Overall, the present findings suggest the tendency to phase aligning differently in the two groups to long and short tones in the left fronto-temporal hemisphere. However, the effect could be detected only in one gradiometer sensor and it was not statistically robust. The effect in the right hemisphere was statistically more robust, but it was not sensitive to group language dominance.</p><p>Due to the inconclusive nature of these analyses regarding the role of language experience in shaping the phase alignment to rhythmic sound sequences, we prefer to keep these results in the public review rather than incorporating them in the article. Nonetheless, we believe that this decision does not undermine the main finding that the group differences in the MMN amplitude are driven by long-term predictions – especially in light of the many studies indicating the MMN as a putative index of prediction error (e.g., Bendixen et al., 2012; Heilbron and Chait, 2018). Moreover, as suggested in the preliminary reply, despite evoked responses and oscillations are often considered distinct electrophysiological phenomena, current evidence suggests that these phenomena are interconnected (e.g., Studenova et al., 2023). In our view, the hypotheses that the MMN reflects differences in phase alignment and long-term prediction errors are not mutually exclusive.</p><fig id="sa4fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-sa4-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(4) Source localization is performed on sensor-level significant data. The lack of sourcelevel statistics weakens the conclusions that can be extracted. Furthermore, only the source reflecting the interaction pattern is taken into account in detail as supporting their hypotheses, overlooking other sources. Also, the right IFG source activity is not depicted, but looking at whole brain maps seems even stronger than the left. To sum up, source localization data, as informative as it could be, does not strongly support the author's claims in its current state.</p></disp-quote><p>A similar comment was also advanced by Reviewer #1 (comment 2). We appreciate the suggestion to incorporate more comprehensive source analyses. In the new version of the paper, we perform new analyses on the source data using a new Atlas with more fine-grained parcellations of the ROIs, and focusing on peak activity to increase response’s sensitivity in space and time. We therefore invite the Reviewer to read the updated part on source reconstruction included in the Results and Methods sections of the paper.</p><p>In the article, we report only the source reconstruction data from ROIs in the left hemisphere, because it is there that the interaction effect arises at the sensor level. However, we also explored the homologous regions in the right hemisphere, as requested by the Reviewer. A cluster-based permutation test focusing on the interaction between language group and omission type was performed on both the right STG and IFG data. No significant interaction emerged in any of these regions. Below a plot of the source activity time series over ROIs in the right STG and IFG.</p><fig id="sa4fig4" position="float"><label>Author response image 4.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91636-sa4-fig4-v1.tif"/></fig><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>In this set of private recommendations for the authors, I will outline a couple of minor comments and try to encourage additional data analyses that, in my opinion, would strengthen the evidence provided by the study.</p><p>(1) As I noted in the public review, I believe an oscillatory analysis of the data would, on one hand, provide stronger support for the behavioral effect of rhythmic perceptual organization given the lack of behavioral direct evidence; and, on the other hand, provide evidence (to be discussed if so) for a role of entrained oscillation phase in explaining the different pattern of omission responses. One analysis the authors could try is to measure the phase angle of an oscillation, the frequency of which relates to the length of the binary pattern, at the onset of short and long tones, separately, and compare it across groups. Also, single trials of omission responses could be sorted according to that phase.</p></disp-quote><p>Thanks for the suggestion. Please see phase analyses reported above.</p><disp-quote content-type="editor-comment"><p>(2) I wonder why source activity for the right IFG was not shown. I urge the authors to provide and discuss a more complete picture of the source activity found. Given the lack of source statistics (which could be performed), I find it a must to give an overall view. I find it so because I believe the distinction between perceptual grouping effects due to inherent acoustic differences across languages or semantic differences is so interesting.</p></disp-quote><p>Thanks again for the invitation to provide a more complete picture of the source activity data. As mentioned in the response above, we invite the Reviewer to read the new related part included in the Results and Methods sections of the paper. In our updated source reconstruction analysis, we find that some regions around the left STG show a pattern that resembles the one found at the sensor-level, providing further support for the “acoustic” (rather than syntactic/semantic) nature of the effect.</p><p>We did not report ROI analysis on the right hemisphere because the interaction effect at sensor level emerged on the left hemisphere. Yet, we included a summary of this analysis in the public response above.</p><disp-quote content-type="editor-comment"><p>(3) Related to this, I have to acknowledge I had to read the whole Molnar et al. (2016) study to find the only evidence so far that, acoustically, in terms of sound duration, Basque and Spanish differ. This was hypothesized before but only at Molnar, an acoustic analysis is performed. I think this is key, and the authors should give it a deeper account in their manuscript. I spend my review of this study thinking, well, but when we speak we actually bind together different words and the syllabic structure does not need to reflect the written one, so maybe the effect is due to a high-level statistical prior related to the content of the words... but Molnar showed me that actually, acoustically, there's a difference in accent and duration: &quot;Taken together, Experiments 1a and 1b show that Basque and Spanish exhibit the predicted differences in terms of the position of prosodic prominence in their phonological phrases (Basque: trochaic, Spanish: iambic), even though the acoustic realization of this prominence involves not only intensity in Basque but duration, as well. Spanish, as predicted, only uses duration as a cue to mark phrasal prosody.&quot;</p></disp-quote><p>Thanks for the suggestion, the distinction in terms of sound duration in Spanish and Basque reported by Molnar is indeed very relevant for the current study.</p><p>We add a few sentences to highlight the acoustic analysis by Molnar and the consequent acoustic nature of the reported effect.</p><p>In the introduction: “Specifically, the effect has been proposed to depend on the quasiperiodic alternation of short and long auditory events in the speech signal – reported in previous acoustic analyses (Molnar et al., 2016) – which reflect the linearization of function words (e.g., articles, prepositions) and content words (e.g., nouns, adjectives, verbs).”</p><p>In the discussion, paragraph 3, we changed “We hypothesized that this effect is linked to a long-term “duration prior” originating from the syntactic function-content word order of language, and specifically, from its acoustic consequences on the prosodic structure” with “We hypothesized that this effect is linked to a long-term “duration prior” originating from the acoustic properties of the two languages, specifically from the alternation of short and long auditory events in their prosody”.</p><p>In the discussion, end of paragraph eight: “The reconstruction of cortical sources associated with the omission of short and long tones in the two groups showed that an interaction effect mirroring the one at the sensor level was present in the left STG, but not in the left IFG (fig. 3, B, C, D). Pairwise comparisons within different ROIs of the left STG indicated that the interaction effect was stronger over primary (BA 41/42) rather than associative (BAs 22) portions of the auditory cortex. Overall, these results suggest that the “duration prior” is linked to the acoustic properties of a given language rather than its syntactic configurations”.</p><disp-quote content-type="editor-comment"><p>Now, some minor comments:</p><p>(1) Where did the experiments take place? Were they in accordance with the Declaration of Helsinki? Did participants give informed consent?</p></disp-quote><p>All the requested information has been added to the updated version of the manuscript. Thanks for pointing out this.</p><disp-quote content-type="editor-comment"><p>(2) The fixed interval should be called inter-stimulus interval.</p></disp-quote><p>Thanks for pointing this out. We changed the wording as suggested.</p><disp-quote content-type="editor-comment"><p>(3) The authors state that &quot;Omission responses allow to examine the presence of putative error signals decoupled from bottom-up sensory input, offering a critical test for predictive coding (Walsh et al 2020, Heilbron and Chait, 2018).&quot;. However the way omission responses are computed in their study is by subtracting the activity from the previous tone. This necessarily means that in the omission activity analyzed, there's bottom-up sensory input activity. As performing another experiment with a control condition in which a sequence of randomly presented tones with different durations to compare directly the omission activity in both sequences (experimental and control) is possibly too demanding, I at least urge the authors to incorporate the fact that their omission responses do reflect also tone activity. And consider, for future experiments, the inclusion of further control conditions.</p></disp-quote><p>Thanks for the opportunity to clarify this aspect. Actually, the way we computed the omission MMN is not by subtracting the activity of the previous tone from the omission, but by subtracting the activity of randomly selected tones across the whole experiment. That is, we randomly selected around 120 long and short tones (i.e., about the same number as the omissions); we computed the ERF for the long and short tones; we subtracted these ERF from the ERF of the corresponding short and long omissions. We clarified these aspects in both the Materials and Methods (ERF analysis paragraph) and Results section.</p><p>Moreover, the subtraction strategy - which is the standard approach to calculate the MMN - allows to handle possible neural carryover effects arising from the perception of the tone preceding the omission.</p><p>The sentence &quot;Omission responses allow to examine the presence of putative error signals decoupled from bottom-up sensory input, offering a critical test for predictive coding (Walsh et al 2020, Heilbron and Chait, 2018).&quot; simply refer to the fact that the error responses resulting from an omission are purely endogenous, as omissions are just absence of an expected input (i.e., silence). On the other hand, when a predicted sequence of tones is disrupted by an auditory deviants (e.g., a tone with a different pitch or duration than the expected one), the resulting error response is not purely endogenous, but it partially includes the response to the acoustic properties of the deviant.</p><disp-quote content-type="editor-comment"><p>(4) When multiple clusters emerged from a comparison, only the most significant cluster was reported. Why?</p></disp-quote><p>We found more than one significant cluster only in the comparison between pure omissions vs tones (figure 2 A, B). The additional significant cluster from this comparison is associated with a P-value of 0.04, emerges slightly earlier in time, and goes in the same direction as the cluster reported in the paper i.e., larger ERF responses for omission vs tones. We added a note specifying the presence of this second cluster, along with a figure on the supplementary material (Supplementary Fig. 1 A, B).</p><disp-quote content-type="editor-comment"><p>(5) Fig 2, if ERFs are baseline corrected -50 to 0ms, why do the plots show pre-stimulus amplitudes not centered at 0?</p></disp-quote><p>This is because we combined the latitudinal and longitudinal gradiometers on the ERF obtained after baseline correction, by computing the root mean square of the signals at each sensor position (see also <ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/example/combineplanar_pipelineorder/">https://www.fieldtriptoolbox.org/example/combineplanar_pipelineorder/</ext-link>). This information is reported in the methods part of the article.</p><disp-quote content-type="editor-comment"><p>(6) Fig 2, add units to color bars.</p></disp-quote><p>Sure.</p><disp-quote content-type="editor-comment"><p>(7) Fig 2 F and G, put colorbar scale the same for all topographies.</p></disp-quote><p>Sure, thanks for pointing this out.</p><disp-quote content-type="editor-comment"><p>(8) The interaction effect language (Spanish; Basque) X omission type (short; long) appears only in a small cluster of 4 sensors not located at the locations with larger amplitudes to omissions. Authors report it as left frontotemporal, but it seems to me frontocentral with a slight left lateralization.</p></disp-quote><p>(1) the fact that the cluster reflecting the interaction effect does not overlap with the peaks of activity is not surprising in our view. Many sources contribute to the generation of the MMN. The goal of our work was to establish whether there is also evidence for a long-term system (among the many) contributing to this. That is why we perform a first analysis on the whole omission response network (likely including many sources and predictive/attentional systems), and then we zoom in and focus on our hypothesized interaction. We never claim that the main source underlying the omissionMMM is the long-term predictive system.</p><p>(2) The exact location of those sensors is at the periphery of the left-hemisphere omission response, which mainly reflects activity from the left temporal regions. The sensor location of this cluster could be influenced by multiple factors, including (i) the direction of the source dipoles determining an effect; (ii) the combination of multiple sources contributing to the activity measured at a specific sensor location, whose unmixing could be solved only with a beamforming source approach. Based on the whole evidence we collected also in the source analyzes we concluded that the major contributors to the sensor-level interaction are emerging from both frontal and temporal regions.</p><p><bold>Reviewer #3 (Public Review):</bold></p><disp-quote content-type="editor-comment"><p>(1) The main weaknesses are the strength of the effects and generalisability. The sample size is also relatively small by today's standards, with N=20 in each group. Furthermore, the crucial effects are all mostly in the .01&gt;P&lt;.05 range, such as the crucial interaction P=.03. It would be nice to see it replicated in the future, with more participants and other languages. It would also have been nice to see behavioural data that could be correlated with neural data to better understand the real-world consequences of the effect.</p></disp-quote><p>We appreciate the positive feedback from Reviewer #3. We agree that it would be nice to see this study replicated in the future with larger sample sizes and a behavioral counterpart. Below are a few comments concerning the weakness highlighted:</p><p>(i) Concerning the sample size: a similar point was raised by Reviewer #1. We report our reply as presented above: “Despite a sample size of 20 participants per group can be considered relatively small for detecting an effect in a between-group design, it must be noted that our effect of interest was based on Molnar et al.’s (2016) experiment, where a sample size of 16 subjects per group was sufficient to detect the perceptual grouping effect. In Yoshida et al., 2010, the perceptual grouping effect arose with two groups of 20 7–8-month-old Japanese and English-learning infants. Based on these findings, we believe that a sample size of 20 participants per group can be considered appropriate for the current study”. We clarified these aspects in the new version of the manuscript.</p><p>(ii) We believe that the lack of behavioral data does not undermine the main findings of this study, given the careful selection of the participants and the well-known robustness of the perceptual grouping effect (e.g., Iversen 2008; Yoshida et al., 2010; Molnar et al. 2014; Molnar et al. 2016). As highlighted by Reviewer #2, having Spanish and Basque dominant “speakers as a sample equates that in Molnar et al. (2016), and thus overcomes the lack of direct behavioral evidence for a difference in rhythmic grouping across linguistic groups. Molnar et al. (2016)'s evidence on the behavioral effect is compelling, and the evidence on neural signatures provided by the present study aligns with it”. (iii) Regarding the fact that the “crucial effects are all mostly in the .01&gt;P&lt;.05 range”: we want to stress that the approach we used to detect the interaction effect was conservative, using a cluster-based permutation approach with no a priori assumptions about the location of the effect. The robustness of our approach has also been highlighted by Reviewer 2: “Data analyses. Sound, state-of-the-art methodology in the event-related field analyses at the sensor level.” In sum, despite some crucial effects being in the .01&gt;P&lt;.05 range, we believe that the statistical soundness of our analysis, combined with the lack of effect in the control condition, provides compelling evidence for our H1.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>Figures - Recommend converting all diagrams and plots to vector images to ensure they remain clear when zoomed in the PDF format.</p></disp-quote><p>Sure, thanks.</p><disp-quote content-type="editor-comment"><p>Figure 1: To improve clarity, the representation of sound durations in panels C and D should be revisited. The use of quavers/eighth notes can be confusing for those familiar with musical notation, as they imply isochrony. If printed in black and white, colour distinctions may be lost, making it difficult to discern the different durations. A more universal representation, such as spectrograms, might be more effective.</p></disp-quote><p>Thanks for the suggestion. It’s true that the quavers/eighth notes might be confusing in that respect. However, we find this notation as a relatively standard approach to define paradigms in auditory neuroscience, see for instance the two papers below. In the new version of the manuscript, we specified in the captions under the figure that the notes refer to individual tones, in order to avoid ambiguities.</p><p>- Wacongne, C., Labyt, E., Van Wassenhove, V., Bekinschtein, T., Naccache, L., &amp; Dehaene, S. (2011). Evidence for a hierarchy of predictions and prediction errors in human cortex. Proceedings of the National Academy of Sciences, 108(51), 20754-20759.</p><p>- Dehaene, S., Meyniel, F., Wacongne, C., Wang, L., &amp; Pallier, C. (2015). The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees. Neuron, 88(1), 2-19.</p><disp-quote content-type="editor-comment"><p>Figure 2 : In panel C of Figure 2, please include the exact p-value for the interaction observed. Refrain from using asterisks or &quot;n.s.&quot; and opt for exact p-values throughout for the sake of clarity.</p></disp-quote><p>Thank you for your suggestion. We have included the exact p-value for the interaction in panel C of Figure 2. However, for the remaining figures, we have chosen to maintain the use of asterisks and &quot;n.s.&quot;. We would like our pictures to convey the key findings concisely, while the numerical details can be found in the article text. The caption below the image also provides guidance on the interpretation of the p-values: (statistical significance: **p &lt; 0.01, *p &lt; 0.05, and ns p &gt; 0.05).</p><disp-quote content-type="editor-comment"><p>Figure 3 Note typo &quot;Omission reponse&quot;</p></disp-quote><p>Fixed. Thanks for noticing the typo.</p><p>A note: we moved the figure reflecting the main effect of long tone omission and the lack of main effect of language background (Figure 4 in the previous manuscript) in the supplementary material (Supplementary Figure 2).</p><p>References</p><p>Bendixen, A., SanMiguel, I., &amp; Schröger, E. (2012). Early electrophysiological indicators for predictive processing in audition: a review. International Journal of Psychophysiology, 83(2), 120-131.</p><p>Heilbron, M., &amp; Chait, M. (2018). Great expectations: is there evidence for predictive coding in auditory cortex?. Neuroscience, 389, 54-73.</p><p>Iversen, J. R., Patel, A. D., &amp; Ohgushi, K. (2008). Perception of rhythmic grouping depends on auditory experience. The Journal of the Acoustical Society of America, 124(4), 22632271.</p><p>Molnar, M., Lallier, M., &amp; Carreiras, M. (2014). The amount of language exposure determines nonlinguistic tone grouping biases in infants from a bilingual environment. Language Learning, 64(s2), 45-64.</p><p>Molnar, M., Carreiras, M., &amp; Gervain, J. (2016). Language dominance shapes non-linguistic rhythmic grouping in bilinguals. Cognition, 152, 150-159.</p><p>Ross, J. M., &amp; Hamm, J. P. (2020). Cortical microcircuit mechanisms of mismatch negativity and its underlying subcomponents. Frontiers in Neural Circuits, 14, 13.</p><p>Simon, J., Balla, V., &amp; Winkler, I. (2019). Temporal boundary of auditory event formation: An electrophysiological marker. International Journal of Psychophysiology, 140, 53-61.</p><p>Studenova, A. A., Forster, C., Engemann, D. A., Hensch, T., Sander, C., Mauche, N., ... &amp; Nikulin, V. V. (2023). Event-related modulation of alpha rhythm explains the auditory P300 evoked response in EEG. bioRxiv, 2023-02.</p><p>Yoshida, K. A., Iversen, J. R., Patel, A. D., Mazuka, R., Nito, H., Gervain, J., &amp; Werker, J. F. (2010). The development of perceptual grouping biases in infancy: A Japanese-English cross-linguistic study. Cognition, 115(2), 356-361.</p><p>Zhang, Y., Yan, F., Wang, L., Wang, Y., Wang, C., Wang, Q., &amp; Huang, L. (2018). Cortical areas associated with mismatch negativity: A connectivity study using propofol anesthesia. Frontiers in Human Neuroscience, 12, 392.</p><p>Ladinig, O., Honing, H., Háden, G., &amp; Winkler, I. (2009). Probing attentive and preattentive emergent meter in adult listeners without extensive music training. <italic>Music Perception</italic>, <italic>26</italic>(4), 377-386.</p><p>Brochard, R., Abecasis, D., Potter, D., Ragot, R., &amp; Drake, C. (2003). The “ticktock” of our internal clock: Direct brain evidence of subjective accents in isochronous sequences. <italic>Psychological Science</italic>, <italic>14</italic>(4), 362-366.</p><p>Potter, D. D., Fenwick, M., Abecasis, D., &amp; Brochard, R. (2009). Perceiving rhythm where none exists: Event-related potential (ERP) correlates of subjective accenting. <italic>Cortex</italic>, <italic>45</italic>(1), 103-109.</p><p>Bouwer, F. L., Werner, C. M., Knetemann, M., &amp; Honing, H. (2016). Disentangling beat perception from sequential learning and examining the influence of attention and musical abilities on ERP responses to rhythm. <italic>Neuropsychologia</italic>, <italic>85</italic>, 80-90.</p></body></sub-article></article>