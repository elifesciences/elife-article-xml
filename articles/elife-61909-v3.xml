<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">61909</article-id><article-id pub-id-type="doi">10.7554/eLife.61909</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-141550"><name><surname>Kane</surname><given-names>Gary A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7703-5055</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-201764"><name><surname>Lopes</surname><given-names>Gonçalo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0731-4945</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-201765"><name><surname>Saunders</surname><given-names>Jonny L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0545-5066</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-200232"><name><surname>Mathis</surname><given-names>Alexander</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3777-2202</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-187946"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7368-4456</contrib-id><email>mackenzie.mathis@epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>The Rowland Institute at Harvard, Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>NeuroGEARS Ltd</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Institute of Neuroscience, Department of Psychology, University of Oregon</institution><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Center for Neuroprosthetics, Center for Intelligent Systems, &amp; Brain Mind Institute, School of Life Sciences, Swiss Federal Institute of Technology (EPFL)</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>08</day><month>12</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e61909</elocation-id><history><date date-type="received" iso-8601-date="2020-08-08"><day>08</day><month>08</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-12-06"><day>06</day><month>12</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Kane et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Kane et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-61909-v3.pdf"/><abstract><p>The ability to control a behavioral task or stimulate neural activity based on animal behavior in real-time is an important tool for experimental neuroscientists. Ideally, such tools are noninvasive, low-latency, and provide interfaces to trigger external hardware based on posture. Recent advances in pose estimation with deep learning allows researchers to train deep neural networks to accurately quantify a wide variety of animal behaviors. Here, we provide a new <monospace>DeepLabCut-Live!</monospace> package that achieves low-latency real-time pose estimation (within 15 ms, &gt;100 FPS), with an additional forward-prediction module that achieves zero-latency feedback, and a dynamic-cropping mode that allows for higher inference speeds. We also provide three options for using this tool with ease: (1) a stand-alone GUI (called <monospace>DLC-Live! GUI</monospace>), and integration into (2) <monospace>Bonsai,</monospace> and (3) <monospace>AutoPilot</monospace>. Lastly, we benchmarked performance on a wide range of systems so that experimentalists can easily decide what hardware is required for their needs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>pose-estimation</kwd><kwd>DeepLabCut</kwd><kwd>real-time tracking</kwd><kwd>any animal</kwd><kwd>low-latency</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Chan Zuckerberg Initiative</institution></institution-wrap></funding-source><award-id>EOSS</award-id><principal-award-recipient><name><surname>Mathis</surname><given-names>Alexander</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1309047</award-id><principal-award-recipient><name><surname>Saunders</surname><given-names>Jonny L</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009835</institution-id><institution>Rowland Institute at Harvard</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kane</surname><given-names>Gary A</given-names></name><name><surname>Mathis</surname><given-names>Alexander</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>Harvard Brain Science Initiative</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kane</surname><given-names>Gary A</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>DeepLabCut-Live allows for real-time, ultra-low latency posture tracking of animals.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In recent years, advances in deep learning have fueled sophisticated behavioral analysis tools (<xref ref-type="bibr" rid="bib15">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Newell et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Cao et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>; <xref ref-type="bibr" rid="bib38">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Graving et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Zuffi et al., 2019</xref>). Specifically, advances in animal pose estimation–the ability to measure the geometric configuration of user-specified keypoints–have ushered in an era of high-throughput quantitative analysis of movements (<xref ref-type="bibr" rid="bib31">Mathis and Mathis, 2020</xref>). One such state-of-the-art animal pose estimation package, DeepLabCut (DLC; <xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>), provides tailored networks that predict the posture of animals of interest based on video frames, and can run swiftly in offline batch processing modes (up to 2500 FPS on standard GPUs; <xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>; <xref ref-type="bibr" rid="bib32">Mathis and Warren, 2018</xref>). This high-throughput analysis has proven to be an invaluable tool to probe the neural mechanisms of behavior (<xref ref-type="bibr" rid="bib31">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="bib48">von Ziegler et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Mathis et al., 2020b</xref>). The ability to apply these behavioral analysis tools to provide feedback to animals in real time is crucial for causally testing the behavioral functions of specific neural circuits.</p><p>Here, we describe a series of new software tools that can achieve low-latency closed-loop feedback based on animal pose estimation with DeepLabCut (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Additionally, these tools make it easier for experimentalists to design and conduct experiments with little to no additional programming, to integrate real-time pose estimation with DeepLabCut into custom software applications, and to share previously trained DeepLabCut neural networks with other users. First, we provide a marked speed and latency improvement from existing real-time pose estimation software (<xref ref-type="bibr" rid="bib12">Forys et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Štih et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Schweihoff et al., 2019</xref>). Although these tools are also built on top of DeepLabCut, our software optimizes inference code and uses lightweight DeepLabCut models that perform well not only on GPUs, but also on CPUs and affordable embedded systems such as the NVIDIA Jetson platform. Second, we introduce a module to export trained neural network models and load them into other platforms with ease, improving the ability to transfer trained models between machines, to share trained models with other users, and to load trained models in other software packages. Easy loading of trained models into other software packages enabled integration of DeepLabCut into another popular systems neuroscience software, Bonsai (<xref ref-type="bibr" rid="bib24">Lopes et al., 2015</xref>). Third, we provide a new lightweight <monospace>DeepLabCut-Live!</monospace> package to run DeepLabCut inference online (or offline). This package has minimal software dependencies and can easily be installed on integrated systems, such as the NVIDIA Jetson platform. Furthermore, it is designed to enable easy integration of real-time pose estimation using DeepLabCut into custom software applications. We demonstrate this ability via integration of <monospace>DeepLabCut-Live!</monospace> into the new AutoPilot framework (<xref ref-type="bibr" rid="bib41">Saunders and Wehr, 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of using DLC networks in real-time experiments within Bonsai, the DLC-Live! GUI, and AutoPilot.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig1-v3.tif"/></fig><p>Using these new software tools, we achieve low latency real-time pose estimation, with delays as low as 10 ms using GPUs and 30 ms using CPUs. Furthermore, we introduce a forward-prediction module that counteracts these delays by predicting the animal’s future pose. Using this forward-prediction module, we were able to provide ultra-low latency feedback to an animal (even down to sub-zero ms delay). Such short latencies have only been approachable in marked animals (<xref ref-type="bibr" rid="bib43">Sehara et al., 2019</xref>), but have not been achieved to the best of our knowledge previously with markerless pose estimation (<xref ref-type="bibr" rid="bib53">Zhao et al., 2019</xref>; <xref ref-type="bibr" rid="bib12">Forys et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Štih et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Schweihoff et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Privitera et al., 2020</xref>).</p><p>Lastly, we developed a benchmarking suite to test the performance of these tools on multiple hardware and software platforms. We provide performance metrics for ten different GPUs, two integrated systems and five CPUs across different operation systems. We openly share this benchmarking suite at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut-live">https://github.com/DeepLabCut/DeepLabCut-live</ext-link>; <xref ref-type="bibr" rid="bib18">Kane, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:703f20f0f4b80417f8277826efdef699213216d5;origin=https://github.com/DeepLabCut/DeepLabCut-live;visit=swh:1:snp:662794ebc2eed5e6c60e7becf6bbd43ea0ea4ba2;anchor=swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f/">swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f</ext-link> so that users can look up expected inference speeds and run the benchmark on their system. We believe that with more user contributions this will allow the community to comprehensively summarize system performance for different hardware options and can thus guide users in choosing GPUs, integrated systems, and other options for their particular use case.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Exporting DeepLabCut models</title><p>DeepLabCut enables the creation of tailored neuronal networks for pose estimation of user-defined bodyparts (<xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>; <xref ref-type="bibr" rid="bib35">Nath et al., 2019</xref>). We sought to make these neural networks, which are TensorFlow graphs, easily deployable by developing a model-export functionality. These customized DeepLabCut models can be created from standard trained DeepLabCut models by running the <monospace>export_model</monospace> function within DeepLabCut (see Materials and methods), or models can be downloaded from the new <ext-link ext-link-type="uri" xlink:href="http://www.mousemotorlab.org/dlc-modelzoo">DeepLabCut Model Zoo</ext-link>.</p><p>The model export module utilizes the protocol buffer format (.pb file). Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data (<ext-link ext-link-type="uri" xlink:href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</ext-link>), which makes sharing models simple. Sharing a whole (DeepLabCut) project is not necessary, and an end-user can simply point to the protocol buffer file of a model to run inference on novel videos (online or offline). The flexibility offered by the protocol buffer format allowed us to integrate DeepLabCut into applications written in different languages: a new python package <monospace>DeepLabCut-Live!</monospace>, which facilitates loading DeepLabCut networks to run inference; and into Bonsai, which is written in C# and runs DeepLabCut inference using TensorFlowSharp (<ext-link ext-link-type="uri" xlink:href="https://github.com/migueldeicaza/TensorFlowSharp">https://github.com/migueldeicaza/TensorFlowSharp</ext-link>).</p></sec><sec id="s2-2"><title>A new python package to develop real-time pose estimation applications</title><p>The <monospace>DeepLabCut-Live!</monospace> package provides a simple programming interface to load exported DeepLabCut models and perform pose estimation on single images (i.e., from a camera feed). By design, this package has minimal dependencies and can be easily installed even on integrated systems.</p><p>To use the <monospace>DeepLabCut-Live!</monospace> package to perform pose estimation, experimenters must simply start with a trained DeepLabCut model in the exported protocol buffer format (.pb file) and instantiate a <monospace>DLCLive</monospace> object. This object can be used to load the DeepLabCut network and perform pose estimation on single images:<code xml:space="preserve">from dlclive import DLCLive
my_live_ob ject = DLCLive(”/exportedmodel/directory”)
my_live_object.init_inference(my_image)
pose = my_live_object.get_pose(my_image)</code></p><p>On its own, the <monospace>DLCLive</monospace> class only enables experimenters to perform real-time pose estimation. To use poses estimated by DeepLabCut to provide closed-loop feedback, the <monospace>DeepLabCut-Live!</monospace> package uses a <monospace>Processor</monospace> class. A <monospace>Processor</monospace> class must contain two methods: process and save. The process method takes a pose as an argument, performs some operation, such as giving a command to control external hardware (e.g. to give reward or to turn on a laser for optogenetic stimulation), and returns a <italic>processed</italic> pose. The save method allows the user to save data recorded by the <monospace>Processor</monospace> in any format the user desires. By imposing few constraints on the <monospace>Processor</monospace> object, this tool is very flexible; for example, it can be used to read and write from a variety of commonly used data acquisition and input/output devices, including National Instruments devices, Arduino and Teensy micro-controllers, as well as Raspberry Pis and similar embedded systems. An example <monospace>Processor</monospace> object that uses a Teensy micro-controller to control a laser for optogenetics is provided in the <monospace>DeepLabCut-Live!</monospace> package.</p><p>We also provide functionality within this package to test inference speed of DeepLabCut networks. This serves to find the bounds for inference speeds an end user can expect given their hardware and pose estimation requirements. Furthermore, there is a method to display the DeepLabCut estimated pose on top of images to visually inspect the accuracy of DeeplabCut networks.</p><p>Ultimately, this package is meant to serve as a software development kit (SDK): to be used to easily integrate real-time pose estimation and closed-loop feedback into other software, either that we provide (described below), or integrated into other existing camera capture packages.</p></sec><sec id="s2-3"><title>Inference speed using the DeepLabCut-Live! package</title><p>Maximizing inference speed is of utmost importance to experiments that require real-time pose estimation. Some known factors that influence inference speed of DeepLabCut networks include (i) the size of the network (<xref ref-type="bibr" rid="bib30">Mathis et al., 2020b</xref>; <xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>), (ii) the size of images (<xref ref-type="bibr" rid="bib32">Mathis and Warren, 2018</xref>), and (iii) the computational power of the hardware (<xref ref-type="bibr" rid="bib32">Mathis and Warren, 2018</xref>).</p><p>The <monospace>DeepLabCut-Live!</monospace> package offers three convenient methods to increase inference speed by reducing the size of images: static image cropping, dynamic cropping around keypoints, and downsizing images (see Materials and methods). These methods are especially important tools, as they enable experimenters to capture a higher resolution, larger ‘full frame’ view, but increase inference speed by either performing inference only on the portion of the image in which the animal is present (i.e., dynamically crop the image around the animal), or if the entire image is needed, by performing inference on an image with reduced resolution (i.e., a smaller image). To demonstrate the effect of these factors on inference speeds using the <monospace>DeepLabCut-Live!</monospace> package, we measured inference speeds for two different architectures: DLC-ResNet-50v1 (<xref ref-type="bibr" rid="bib15">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>; <xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>) and DLC-MobileNetV2-0.35 (<xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>) and across a range of image sizes using the downsizing method. These tests were performed on a variety of hardware configurations, ranging from NVIDIA GPUs to Intel CPUs on Linux, Windows, and MacOS computers, as well as NVIDIA Jetson developer kits–inexpensive embedded systems with on-board GPUs, and using two different sets of DeepLabCut networks: a dog tracking model with 20 keypoints and a mouse pupil and licking tracking model with eight keypoints.</p><p>As expected, inference speeds were faster for the smaller DLC-MobileNetV2-0.35 networks than the larger DLC-ResNet-50v1 networks (<xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>), faster with smaller images, and faster when using more powerful NVIDIA GPUs compared to smaller GPUs or CPUs (<xref ref-type="fig" rid="fig2">Figure 2</xref>). For example, with the NVIDIA Titan RTX GPU (24 GB) and the NVIDIA GeForce GTX 1080 GPU (8 GB), we achieved inference speeds of 152 ± 15 and 134 ± 9 frames per second on medium sized images (459 × 349 pixels) using the MobileNetV2-0.35 DeepLabCut network. Full results are presented in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, and <xref ref-type="table" rid="table1">Table 1</xref>.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Inference speed of different networks with different image sizes on different system configurations.</title><p>Solid lines are tests using DLC-MobileNetV2-0.35 networks and dashed lines using DLC-ResNet-50 networks. Shaded regions represent 10th-90th percentile of the distributions. N = 1000–10,000 observations per point.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Inference speed of different networks with different image sizes with different system configurations, using CPUs for inference.</title><p>Inference speed of different networks with different image sizes with different system configurations, using CPUs for inference. Solid lines are tests using DLC-MobileNetV2-0.35 networks and dashed lines using DLC-ResNet-50 networks. Shaded regions represent 10th-90th percentile of the distributions. N = 1000–10,000 observations per point.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig2-figsupp1-v3.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Accuracy of DLC networks on resized images.</title><p>Accuracy of DLC networks on resized images. Accuracy of DLC networks was tested on a range of image sizes (test image scale = 0.125–1.5, different panels). Lines indicate the root mean square error (RMSE, y-axis) between DLC predicted keypoints, using DLC networks trained with different scale parameters (x-axis), and human labels. Different colors are for errors on images in the training dataset (blue) vs. test dataset (red). Note, the RMSE is not reported as on the scaled image, but as would be in the original image (640 × 480) for comparability. .</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig2-figsupp2-v3.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Dynamic cropping increased inference speed with little sacrifice to accuracy.</title><p>(Left) Inference speeds (in fps) for the full size images compared to dynamic cropping with 50, 25, or 10 pixel margin around the animals last position. Boxplots represent the 0.1, 0.3, 0.5, 0.7, and 0.9% quantiles of each distribution. (Right) The cumulative distribution of errors (i.e. the proportion of images with a smaller error) for dynamic tracking relative to full size images.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig2-figsupp3-v3.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>The number of keypionts in a DeepLabCut network does not affect inference speed.</title><p>Inference speeds were not affected by the number of keypoints in a DeepLabCut network. Inference speeds were tested on a series of dog tracking networks with 1 (only the nose), 7 (the face), 13 (the upper body), or 20 (the entire body) keypoints. Inference speeds were tested on different image sizes (x-axis) and on different DeepLabCut architectures (different panels). Lines represent the median and the ribbon represents the 0.1–0.9 quantiles of the distribution of inference speeds.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig2-figsupp4-v3.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Inference speed using the DeepLabCut-live package.</title><p>All values are mean ± standard deviation; Frames Per Second (FPS). See also: <ext-link ext-link-type="uri" xlink:href="https://deeplabcut.github.io/DLC-inferencespeed-benchmark/">https://deeplabcut.github.io/DLC-inferencespeed-benchmark/</ext-link>.</p></caption><table frame="hsides" rules="groups"><tbody><tr><th>Dog video</th><th colspan="7">Image size (pixels, w*h)</th></tr><tr><th/><th>GPU type</th><th>DLC model</th><th>75 × 133</th><th>150 × 267</th><th>300 × 533</th><th>424 × 754</th><th>600 × 1067</th></tr><tr><td>Linux</td><td>Intel Xeon CPU</td><td>MobileNetV2-0.35</td><td>62 ± 5</td><td>31 ± 1</td><td>14 ± 0</td><td>8 ± 0</td><td>4 ± 0</td></tr><tr><td/><td>(3.1 GHz)</td><td>ResNet-50</td><td>24 ± 1</td><td>11 ± 0</td><td>3 ± 0</td><td>2 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>GeForce GTX 1080</td><td>MobileNetV2-0.35</td><td>256 ± 51</td><td>208 ± 46</td><td>124 ± 19</td><td>80 ± 9</td><td>43 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>121 ± 15</td><td>95 ± 13</td><td>52 ± 3</td><td>33 ± 1</td><td>19 ± 0</td></tr><tr><td/><td>TITAN RTX</td><td>MobileNetV2-0.35</td><td>168 ± 29</td><td>144 ± 20</td><td>133 ± 14</td><td>115 ± 9</td><td>71 ± 3</td></tr><tr><td/><td/><td>ResNet-50</td><td>132 ± 14</td><td>113 ± 11</td><td>82 ± 4</td><td>56 ± 2</td><td>33 ± 1</td></tr><tr><td/><td>Tesla V100-SXM2-16GB</td><td>MobileNetV2-0.35</td><td>229 ± 13</td><td>189 ± 10</td><td>138 ± 11</td><td>105 ± 6</td><td>64 ± 3</td></tr><tr><td/><td/><td>ResNet-50</td><td>169 ± 7</td><td>133 ± 4</td><td>90 ± 4</td><td>65 ± 2</td><td>42 ± 1</td></tr><tr><td/><td>Tesla P100-PCIE-16GB</td><td>MobileNetV2-0.35</td><td>220 ± 12</td><td>179 ± 9</td><td>114 ± 7</td><td>77 ± 3</td><td>44 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>115 ± 3</td><td>91 ± 2</td><td>59 ± 2</td><td>45 ± 1</td><td>26 ± 1</td></tr><tr><td/><td>Tesla K80</td><td>MobileNetV2-0.35</td><td>118 ± 4</td><td>105 ± 3</td><td>64 ± 4</td><td>47 ± 2</td><td>26 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>58 ± 2</td><td>43 ± 1</td><td>21 ± 1</td><td>13 ± 0</td><td>7 ± 0</td></tr><tr><td/><td>Tesla T4</td><td>MobileNetV2-0.35</td><td>200 ± 17</td><td>166 ± 13</td><td>117 ± 10</td><td>86 ± 5</td><td>49 ± 2</td></tr><tr><td/><td/><td>ResNet-50</td><td>134 ± 8</td><td>99 ± 5</td><td>51 ± 3</td><td>33 ± 1</td><td>18 ± 0</td></tr><tr><td/><td>Quadro P400</td><td>MobileNetV2-0.35</td><td>105 ± 4</td><td>76 ± 2</td><td>31 ± 1</td><td>18 ± 0</td><td>10 ± 0</td></tr><tr><td/><td/><td>ResNet-50</td><td>24 ± 0</td><td>16 ± 0</td><td>6 ± 0</td><td>4 ± 0</td><td>2 ± 0</td></tr><tr><td>Windows</td><td>Intel Xeon Silver CPU</td><td>MobileNetV2-0.35</td><td>28 ± 1</td><td>13 ± 0</td><td>6 ± 0</td><td>3 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>(2.1 GHz)</td><td>ResNet-50</td><td>22 ± 1</td><td>9 ± 0</td><td>3 ± 0</td><td>2 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>GeForce GTX 1080</td><td>MobileNetV2-0.35</td><td>142 ± 17</td><td>132 ± 14</td><td>98 ± 5</td><td>69 ± 3</td><td>41 ± 1</td></tr><tr><td/><td>with Max-Q Design</td><td>ResNet-50</td><td>87 ± 3</td><td>77 ± 3</td><td>48 ± 1</td><td>31 ± 1</td><td>18 ± 0</td></tr><tr><td/><td>GeForce GTX 1080</td><td>MobileNetV2-0.35</td><td>128 ± 11</td><td>115 ± 10</td><td>94 ± 7</td><td>72 ± 3</td><td>41 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>101 ± 5</td><td>86 ± 4</td><td>49 ± 1</td><td>32 ± 1</td><td>18 ± 0</td></tr><tr><td/><td>Quadro P1000</td><td>MobileNetV2-0.35</td><td>120 ± 11</td><td>108 ± 10</td><td>58 ± 2</td><td>39 ± 1</td><td>20 ± 0</td></tr><tr><td/><td/><td>ResNet-50</td><td>54 ± 2</td><td>38 ± 1</td><td>17 ± 0</td><td>10 ± 0</td><td>5 ± 0</td></tr><tr><td>MacOS</td><td>Intel Core i5 CPU</td><td>MobileNetV2-0.35</td><td>39 ± 5</td><td>20 ± 2</td><td>11 ± 1</td><td>7 ± 1</td><td>4 ± 0</td></tr><tr><td/><td>(2.4 GHz)</td><td>ResNet-50</td><td>8 ± 1</td><td>4 ± 0</td><td>1 ± 0</td><td>1 ± 0</td><td>0 ± 0</td></tr><tr><td/><td>Intel Core i7 CPU</td><td>MobileNetV2-0.35</td><td>117 ± 8</td><td>47 ± 3</td><td>15 ± 1</td><td>8 ± 0</td><td>4 ± 0</td></tr><tr><td/><td>(3.5 GHz)</td><td>ResNet-50</td><td>29 ± 2</td><td>11 ± 1</td><td>3 ± 0</td><td>2 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>Intel Core i9 CPU</td><td>MobileNetV2-0.35</td><td>126 ± 25</td><td>66 ± 13</td><td>19 ± 3</td><td>11 ± 1</td><td>6 ± 0</td></tr><tr><td/><td>(2.4 GHz)</td><td>ResNet-50</td><td>31 ± 6</td><td>16 ± 2</td><td>6 ± 1</td><td>4 ± 0</td><td>2 ± 0</td></tr><tr><td>Jetson</td><td>Xavier</td><td>MobileNetV2-0.35</td><td>68 ± 8</td><td>60 ± 7</td><td>54 ± 4</td><td>41 ± 1</td><td>25 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>46 ± 1</td><td>34 ± 1</td><td>17 ± 0</td><td>12 ± 0</td><td>6 ± 0</td></tr><tr><td/><td>Tegra X2</td><td>MobileNetV2-0.35</td><td>37 ± 2</td><td>32 ± 2</td><td>13 ± 0</td><td>7 ± 0</td><td>4 ± 0</td></tr><tr><td/><td/><td>ResNet-50</td><td>18 ± 1</td><td>12 ± 0</td><td>5 ± 0</td><td>3 ± 0</td><td>2 ± 0</td></tr><tr><th>Mouse Video</th><th colspan="7">image size (pixels, w*h)</th></tr><tr><td/><th>GPU Type</th><th>DLC Model</th><th>115 × 87</th><th>229 × 174</th><th>459 × 349</th><th>649 × 493</th><th>917 × 698</th></tr><tr><td>Linux</td><td>Intel Xeon CPU</td><td>MobileNetV2-0.35</td><td>61 ± 4</td><td>32 ± 1</td><td>15 ± 0</td><td>8 ± 0</td><td>4 ± 0</td></tr><tr><td/><td>(3.1 GHz)</td><td>ResNet-50</td><td>28 ± 1</td><td>11 ± 0</td><td>4 ± 0</td><td>2 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>GeForce GTX 1080</td><td>MobileNetV2-0.35</td><td>285 ± 24</td><td>209 ± 23</td><td>134 ± 9</td><td>86 ± 2</td><td>44 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>136 ± 8</td><td>106 ± 3</td><td>60 ± 1</td><td>35 ± 0</td><td>19 ± 0</td></tr><tr><td/><td>TITAN RTX</td><td>MobileNetV2-0.35</td><td>169 ± 28</td><td>145 ± 19</td><td>152 ± 15</td><td>124 ± 9</td><td>78 ± 3</td></tr><tr><td/><td/><td>ResNet-50</td><td>140 ± 16</td><td>119 ± 11</td><td>92 ± 3</td><td>58 ± 2</td><td>35 ± 1</td></tr><tr><td/><td>Tesla V100-SXM2-16GB</td><td>MobileNetV2-0.35</td><td>260 ± 12</td><td>218 ± 9</td><td>180 ± 18</td><td>121 ± 8</td><td>68 ± 3</td></tr><tr><td/><td/><td>ResNet-50</td><td>184 ± 6</td><td>151 ± 5</td><td>111 ± 6</td><td>75 ± 3</td><td>47 ± 2</td></tr><tr><td/><td>Tesla P100-PCIE-16GB</td><td>MobileNetV2-0.35</td><td>246 ± 12</td><td>198 ± 7</td><td>138 ± 8</td><td>79 ± 3</td><td>46 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>128 ± 3</td><td>103 ± 2</td><td>70 ± 3</td><td>46 ± 1</td><td>28 ± 1</td></tr><tr><td/><td>Tesla K80</td><td>MobileNetV2-0.35</td><td>127 ± 6</td><td>119 ± 5</td><td>79 ± 4</td><td>52 ± 2</td><td>28 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>60 ± 2</td><td>45 ± 2</td><td>23 ± 1</td><td>13 ± 0</td><td>7 ± 0</td></tr><tr><td/><td>Tesla T4</td><td>MobileNetV2-0.35</td><td>242 ± 21</td><td>197 ± 16</td><td>156 ± 14</td><td>101 ± 6</td><td>56 ± 2</td></tr><tr><td/><td/><td>ResNet-50</td><td>141 ± 7</td><td>102 ± 5</td><td>57 ± 3</td><td>34 ± 1</td><td>20 ± 0</td></tr><tr><td/><td>Quadro P400</td><td>MobileNetV2-0.35</td><td>114 ± 5</td><td>84 ± 3</td><td>37 ± 1</td><td>20 ± 0</td><td>10 ± 0</td></tr><tr><td/><td/><td>ResNet-50</td><td>25 ± 0</td><td>16 ± 0</td><td>7 ± 0</td><td>4 ± 0</td><td>2 ± 0</td></tr><tr><td>Windows</td><td>Intel Xeon Silver CPU</td><td>MobileNetV2-0.35</td><td>28 ± 1</td><td>14 ± 0</td><td>6 ± 0</td><td>3 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>(2.1 GHz)</td><td>ResNet-50</td><td>23 ± 1</td><td>9 ± 0</td><td>3 ± 0</td><td>2 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>GeForce GTX 1080</td><td>MobileNetV2-0.35</td><td>147 ± 17</td><td>136 ± 15</td><td>108 ± 6</td><td>73 ± 3</td><td>46 ± 1</td></tr><tr><td/><td>with Max-Q Design</td><td>ResNet-50</td><td>107 ± 5</td><td>93 ± 4</td><td>54 ± 2</td><td>32 ± 1</td><td>18 ± 0</td></tr><tr><td/><td>GeForce GTX 1080</td><td>MobileNetV2-0.35</td><td>133 ± 15</td><td>119 ± 14</td><td>100 ± 9</td><td>77 ± 3</td><td>43 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>110 ± 5</td><td>94 ± 3</td><td>55 ± 1</td><td>34 ± 1</td><td>19 ± 0</td></tr><tr><td/><td>Quadro P1000</td><td>MobileNetV2-0.35</td><td>129 ± 13</td><td>115 ± 11</td><td>72 ± 3</td><td>43 ± 1</td><td>22 ± 0</td></tr><tr><td/><td/><td>ResNet-50</td><td>57 ± 2</td><td>41 ± 1</td><td>19 ± 0</td><td>10 ± 0</td><td>6 ± 0</td></tr><tr><td>MacOS</td><td>Intel Core i5 CPU</td><td>MobileNetV2-0.35</td><td>42 ± 5</td><td>22 ± 2</td><td>12 ± 1</td><td>7 ± 1</td><td>4 ± 0</td></tr><tr><td/><td>(2.4 GHz)</td><td>ResNet-50</td><td>9 ± 1</td><td>4 ± 0</td><td>2 ± 0</td><td>1 ± 0</td><td>0 ± 0</td></tr><tr><td/><td>Intel Core i7 CPU</td><td>MobileNetV2-0.35</td><td>127 ± 10</td><td>48 ± 3</td><td>16 ± 1</td><td>9 ± 1</td><td>4 ± 0</td></tr><tr><td/><td>(3.5 GHz)</td><td>ResNet-50</td><td>30 ± 3</td><td>10 ± 2</td><td>3 ± 0</td><td>2 ± 0</td><td>1 ± 0</td></tr><tr><td/><td>Intel Core i9 CPU</td><td>MobileNetV2-0.35</td><td>178 ± 15</td><td>74 ± 16</td><td>19 ± 3</td><td>10 ± 1</td><td>6 ± 0</td></tr><tr><td/><td>(2.4 GHz)</td><td>ResNet-50</td><td>35 ± 8</td><td>14 ± 2</td><td>6 ± 1</td><td>4 ± 0</td><td>2 ± 0</td></tr><tr><td>Jetson</td><td>Xavier</td><td>MobileNetV2-0.35</td><td>79 ± 9</td><td>68 ± 9</td><td>59 ± 5</td><td>44 ± 2</td><td>27 ± 1</td></tr><tr><td/><td/><td>ResNet-50</td><td>46 ± 2</td><td>36 ± 1</td><td>19 ± 0</td><td>12 ± 0</td><td>7 ± 0</td></tr><tr><td/><td>TX2</td><td>MobileNetV2-0.35</td><td>39 ± 2</td><td>30 ± 2</td><td>18 ± 1</td><td>9 ± 0</td><td>5 ± 0</td></tr><tr><td/><td/><td>ResNet-50</td><td>19 ± 1</td><td>11 ± 0</td><td>6 ± 0</td><td>4 ± 0</td><td>2 ± 0</td></tr></tbody></table></table-wrap><sec id="s2-3-1"><title>Inference speed, size vs. accuracy</title><p>Although reducing the size of images increases inference speed, it may result in reduced accuracy in tracking if the resolution of the downsized image is too small to make accurate predictions, or if the network is not trained to perform inference on smaller images. To test the accuracy of DeepLabCut tracking on downsized images, we trained three DeepLabCut networks on a mouse open-field dataset that has been previously used for DLC accuracy benchmarking (640 × 480 pixels; <xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>). Each network was trained on a different range of image sizes: the default (50–125% of the raw image size), a wider range of image sizes (10–125%), or only on smaller images (10–50%). This was achieved by altering the training scale parameters (‘scale_jitter_lo’ and ‘scale_jitter_hi’) in the DeepLabCut training configuration file. We then tested the accuracy of predictions of each network on test images with scale of 12.5–150%, and calculated the root mean square error (RMSE) of predictions compared to human labels (n = 571 images; 50% for train/test) for each test image scale on each network (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Note, that pixel units is the natural metric for reporting accuracy, but in this dataset the width of length of the mouse was 115.7 ± 0.6 (mean ± s.e.m., n = 571) to give the reader a sense of scale.</p><p>Naturally, the best accuracy was achieved on the largest images using networks trained on large images (RMSE = 2.90 pixels at image scale = 150% and training scales of 50–125% or 10–125%). However, there was only minimal loss in accuracy when downsizing images to a scale of 25% of the image size, and these errors were mitigated by using a network trained specifically on smaller images (RMSE = 7.59 pixels at 25% scale with training scale = 10–50%; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). However, for the smallest images tested (test scale = 12.5%), accuracy suffered on all networks (RMSE = 26.99–70.92; all RMSE reported as would be in the original image (640 × 480) for comparability), likely because the mouse is only about 10 pixels long. Overall, this highlights that downsizing does not strongly impact the accuracy (assuming the animal remains a reasonable size) and that this is an easy way to increase inference speeds.</p></sec><sec id="s2-3-2"><title>Dynamic cropping: increased speed without effects on accuracy</title><p>Another alternative is to not to downsize, but rather to process only the portion of the image that contains the animal. Our approach, which we call ‘dynamic cropping’, can provide a speed boost (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Thereby the relevant part of the image is selected by taking advantage of predictions from the previous frame, together with the intuition that the animal cannot move arbitrarily fast. Specifically, this method analyzes the portion of the full-size image by calculating a bounding box based on all the body parts from the previous image as well as a margin (note that if the animal is ‘lost’ in the next frame, the full frame is automatically analyzed). This method captures the animal at it’s full resolution (the animal covers the same number of pixels as in the full image), but still runs inference on a smaller image, increasing inference speed.</p><p>To examine the effect of dynamic cropping on both inference speed and tracking accuracy, we performed benchmarking on the open source open-field dataset (n = 2330 images <xref ref-type="bibr" rid="bib27">Mathis et al., 2018a</xref>; <xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>). We recorded the DeepLabCut estimated keypoints and inference speed when analyzing the full image (640 × 480 pixels) and when dynamically cropping with a margin of 50, 25, or 10 pixels around the bounding box enclosing all bodyparts from the animal. Dynamic cropping increased inference speed by 75% (full image: 59.6 ± 2.08; dynamic-50: 103.7 ± 19.8; dynamic-25: 110.8 ± 17.6; dynamic-10: 108 ± 25.2; mean ± standard deviation), and resulted in only a small change in tracking performance, with RMSEs of 4.4, 5.5, and 20.6 for dynamic cropping with 50, 25, and 10 pixel margins, respectively (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Qualitatively all predictions apart from the margin 10, looked comparable.</p></sec><sec id="s2-3-3"><title>Number of keypoints does not affect inference speed</title><p>Lastly, we tested whether the number of keypoints in the DeepLabCut network affected inference speeds. We modified the dog network to track only the dog’s nose (one keypoint), the face (seven keypoints), the upper body (13 keypoints), or the entire body (20 keypoints). Similar to the benchmarking experiments above, we trained a series of networks with both DLC-ResNet-50v1 and DLC-MobileNetV2-0.35 networks and tested inference speeds on a range of image sizes. The number of keypoints in the model had no effect on inference speed (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p><p>Moreover, we created a website that we aim to continuously update with user input: one can simply export the results of these tests (which capture information about the hardware automatically), and submit the results on GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DLC-inferencespeed-benchmark">https://github.com/DeepLabCut/DLC-inferencespeed-benchmark</ext-link>. These results, in addition to the extensive testing we provide below, then become a community resource for considerations with regard to GPUs and experimental design <ext-link ext-link-type="uri" xlink:href="https://deeplabcut.github.io/DLC-inferencespeed-benchmark/">https://deeplabcut.github.io/DLC-inferencespeed-benchmark/</ext-link>.</p></sec></sec><sec id="s2-4"><title>User-interfaces for real-time feedback</title><p>In addition to the DeepLabCut-live package that serves as a SDK for experimenters to write custom real-time pose estimation applications, we provide three methods for conducting experiments that use DeepLabCut to provide closed-loop feedback that do not require users to write any additional code: a standalone user interface called the DLC-Live! GUI (DLG), and by integrating DeepLabCut into popular existing experimental control softwares Autopilot (<xref ref-type="bibr" rid="bib41">Saunders and Wehr, 2019</xref>) and Bonsai (<xref ref-type="bibr" rid="bib24">Lopes et al., 2015</xref>).</p></sec><sec id="s2-5"><title>DLC-Live! GUI</title><p>The DLG provides a graphical user interface that simultaneously controls capturing data from a camera (many camera types are supported, see Materials and methods), recording videos, and performing pose estimation and closed-loop feedback using the <monospace>DeepLabCut-Live!</monospace> package. To allow users to both record video and perform pose estimation at the fastest possible rates, these processes run in parallel. Thus, video data can be acquired from a camera without delays imposed by pose estimation, and pose estimation will not be delayed by the time spent acquiring images and saving video data to the hard drive. However, if pose estimation is slower than the frame rate, which will occur if acquiring images at a high frame rate with large images, or if inference is run on less powerful GPUs users can see <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="table" rid="table1">Table 1</xref> as a guide, image acquisition and pose estimation will not be synchronized. If these processes are not synchronized (i.e. pose estimation is not run as soon as the image is captured), then the delay from image acquisition to obtain the pose consists not only of the time it takes DeepLabCut to perform pose estimation, but also the time from when the image was captured until pose estimation begins. Thus, running image acquisition and pose estimation asynchronously allows users to run pose estimation at the fastest possible rate, but it does not minimize the time from when an image was captured until the pose is measured. If users prefer to minimize the delay from image acquisition to pose estimation, the pose estimation process can wait for the next image to be acquired. In this case, each time a pose is estimated, the pose estimation process will choose to skip a frame to get back in sync with the image acquisition process. Waiting to get back in sync with the pose estimation process will result in a slower rate of pose estimation, and, over the course of an entire experiment, fewer estimated poses. DLG offers users a choice of which mode they prefer: an ‘Optimize Rate’ mode, in which pose estimation is performed at the maximum possible rate, but there may be delays from the time an image was captured to the time pose estimation begins, and an ‘Optimize Latency’ mode, in which the pose estimation process waits for a new image to be acquired, minimizing the delay from the time an image was acquired to when the pose becomes available.</p><p>To measure the performance of DLG in both modes, we used a video of a head-fixed mouse performing a task that required licking to receive a liquid reward. To simulate a camera feed from an animal in real-time, single frames from the video were loaded (i.e. acquired) at the rate that the video was initially recorded–100 frames per second. We measured three latency periods: (i) the delay from image acquisition to obtaining the pose for each measured pose; (ii) the delay from one measured pose to the next measured pose; and (iii) for each pose in which the tongue was detected, the delay from detecting an action (the mouse’s tongue was detected) to turn on an LED (if the tongue was not detected, the LED was not turned on). The presence or absence of the tongue in any image was determined using the likelihood of the tongue keypoint provided by DeepLabCut; if the likelihood was greater than 0.5, the tongue was considered detected. To measure the time from lick detection to turning on an LED, we used a <monospace>Processor</monospace> object that, when the likelihood of the tongue was greater than 0.5, sent a command to a Teensy micro-controller to turn on an infrared LED. To determine that the LED had been activated, the status of the LED was read using an infrared photodetector. When the photodetector was activated, the Teensy reported this back to the <monospace>Processor</monospace>. The delay from image acquisition to turning on the LED was measured as the difference between the time the frame was acquired and the time that the photodetector had been activated.</p><p>This procedure was run under four configurations: pose estimation performed on full-size images (352 × 274 pixels) and images downsized by 50% in both width and height (176 × 137 pixels); both image sizes were run in ‘Optimize Rate’ and ‘Optimize Latency’ modes. These four configurations were run on four different computers to span a range of options (to see how they generally perform, please see <xref ref-type="table" rid="table1">Table 1</xref>): a Windows desktop with NVIDIA GeForce 1080 GPU, a Linux desktop with NVIDIA Quadro P400 GPU, a NVIDIA Jetson Xavier, and a MacBook Pro laptop with Intel Core-i7 CPU.</p><p>On a Windows system with GeForce GTX 1080 GPU, DLG achieved delays from frame acquisition to obtaining the pose as fast as 10 ± 1 ms (mean ± sd) in the ‘Optimize Latency’ mode. Compared to the ‘Optimize Latency’ mode, this delay was, on average, 4.38 ms longer (95% CI: 4.32–4.44 ms) with smaller images and 4.6 ms longer (95% CI: 4.51–4.63 ms) with larger images in the ‘Optimize Rate’ mode. As suggested above, the longer delay from frame acquisition to pose in the ‘Optimize Rate’ mode can be attributed to delays from when images are acquired until pose estimation begins. With a frame acquisition rate of 100 FPS, this delay would be expected to be 5 ms with a range from 0 to 10 ms, as observed.</p><p>Running DLG In the ‘Optimize Rate’ mode on this Windows system, the delay from obtaining one pose to the next was 11 ± 2 ms (rate of 91 ± 11 poses per second) for smaller images and 12 ± 1 ms (rate of 84 ± 9 poses per second) for larger images. Compared to the ‘Optimize Rate’ mode, the “Optimize Latency“ mode was 7.7 ms (95% CI: 7.6–7.8 ms) slower for smaller images and 9.1 ms (95% CI: 9.0–9.1 ms) for larger images. This increased delay from one pose to another can be attributed to time waiting for acquisition of the next image in the ‘Optimize Latency’ mode.</p><p>Lastly, the delay from acquiring an image in which the tongue was detected until the LED could turned on/off includes the time needed to obtain the pose, plus additional time to determine if the tongue is present and to execute the control signal (send a TTL pulse to the LED). To determine the additional delay caused by detection of the tongue and sending a TTL signal to the LED, we compared the delay from image acquisition to turning on the LED with the delay from image acquisition to obtaining a pose in which the LED was not triggered. Detecting the tongue and sending a TTL pulse only took an additional 0.4 ms (95% CI: 0.3–0.6 ms). Thus, the delay from image acquisition to turn on the LED can be almost entirely attributed to pose estimation. Full results from all four tested systems can be found in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="table" rid="table2">Table 2</xref>.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Pose estimation latency using the DLC-Live!-GUI.</title><p>Top: A lick sequence from the test video with eight keypoints measured using the DLC-Live!-GUI (using Windows/GeForce GTX 1080 GPU). Image timestamps are presented in the bottom-left corner of each image. Bottom: Latency from image acquisition to obtaining the pose, from the last pose to the current pose, and from image acquisition when the mouse’s tongue was detected to turning on an LED. The width of the violin plots indicate the probability density – the likelihood of observing a given value on the y-axis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig3-v3.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Performance of the DeepLabCut-live-GUI (DLG).</title><p>F-P = delay from image acquisition to pose estimation; F-L = delay from image acquisition to turning on the LED; FPS (DLG) = Rate of pose estimation (in frames per second) in the DeepLabCut-live-GUI; FPS (DLCLive) = Rate of pose estimation for the same exact configuration directly tested using the DeepLabCut-live benchmarking tool. All values are mean ± STD.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3"/><th colspan="4">176 × 137 pixels</th><th colspan="4">352 × 274 pixels</th></tr><tr><th/><th rowspan="2">GPU type</th><th rowspan="2">Mode</th><th>F-P</th><th>F-L</th><th>FPS</th><th>FPS</th><th>F-P</th><th>F-L</th><th>FPS</th><th>FPS</th></tr></thead><tbody><tr><td/><td>(ms)</td><td>(ms)</td><td>(DLG)</td><td>(DLCLive)</td><td>(ms)</td><td>(ms)</td><td>(DLG)</td><td>(DLCLive)</td></tr><tr><td>Windows</td><td>GeForce GTX 1080</td><td>Latency</td><td>10 ± 1</td><td>11 ± 1</td><td>56 ± 16</td><td rowspan="2">123 ± 16</td><td>12 ± 2</td><td>12 ± 2</td><td>48 ± 6</td><td rowspan="2">112 ± 12</td></tr><tr><td/><td/><td>Rate</td><td>15 ± 3</td><td>16 ± 3</td><td>91 ± 11</td><td>16 ± 3</td><td>17 ± 3</td><td>84 ± 9</td></tr><tr><td>Linux</td><td>Quadro P400</td><td>Latency</td><td>11 ± 1</td><td>11 ± 1</td><td>63 ± 19</td><td rowspan="2">105 ± 4</td><td>20 ± 1</td><td>21 ± 1</td><td>42 ± 7</td><td rowspan="2">52 ± 1</td></tr><tr><td/><td/><td>Rate</td><td>15 ± 3</td><td>16 ± 3</td><td>93 ± 9</td><td>27 ± 4</td><td>27 ± 4</td><td>47 ± 4</td></tr><tr><td>Jetson</td><td>Xavier</td><td>Latency</td><td>13 ± 1</td><td>14 ± 1</td><td>48 ± 3</td><td rowspan="2">84 ± 7</td><td>13 ± 1</td><td>14 ± 1</td><td>48 ± 3</td><td rowspan="2">73 ± 9</td></tr><tr><td/><td/><td>Rate</td><td>18 ± 3</td><td>18 ± 3</td><td>75 ± 5</td><td>18 ± 3</td><td>18 ± 3</td><td>74 ± 5</td></tr><tr><td>MacOS</td><td>CPU</td><td>Latency</td><td>29 ± 5</td><td>29 ± 4</td><td>29 ± 5</td><td rowspan="2">62 ± 6</td><td>79 ± 19</td><td>79 ± 22</td><td>12 ± 2</td><td rowspan="2">21 ± 3</td></tr><tr><td/><td/><td>Rate</td><td>34 ± 7</td><td>35 ± 6</td><td>35 ± 4</td><td>91 ± 19</td><td>92 ± 22</td><td>12 ± 2</td></tr></tbody></table></table-wrap></sec><sec id="s2-6"><title>DeepLabCut models in Bonsai</title><p>Bonsai is a widely used visual language for reactive programming, real-time behavior tracking, synchronization of multiple data streams and closed-loop experiments (<xref ref-type="bibr" rid="bib24">Lopes et al., 2015</xref>). It is written in C#, thus provides an alternative environment for running real-time DeepLabCut and also test the performance of native TensorFlow inference outside of a Python environment. We developed equivalent performance benchmarks for testing our newly developed Bonsai-DLC plugin <ext-link ext-link-type="uri" xlink:href="https://github.com/bonsai-rx/deeplabcut">https://github.com/bonsai-rx/deeplabcut</ext-link>. This plugin allows loading of the DeepLabCut exported .pb files directly in Bonsai.</p><p>We compared the performance of Bonsai-DLC and <monospace>DeepLabCut-Live!</monospace> on a Windows 10 computer with GeForce GTX 1080 with Max-Q design GPU and found that the performance of running inference through Bonsai-DLC was comparable to <monospace>DeepLabCut-Live!</monospace> inference (<xref ref-type="fig" rid="fig4">Figure 4</xref>), suggesting that, as expected, inference speed is limited primarily by available CPU/GPU computational resources rather than by any native language interface optimizations. Moreover, we found the latency to be 34 ms ± 9.5 ms (median, IQR, n = 500) tested at 30 Hz with 384 × 307 pixels, which is equivalent to what was found with DLG above.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Inference speed using the Bonsai-DLC Plugin.</title><p>Dashed Lines are ResNet-50, solid lines are MobileNetV2-0.35 (<bold>A</bold>) Overview of using DeepLabCut within Bonsai. Both capture-to-write or capture to detect for real-time feedback is possible. (<bold>B</bold>) Top: Direct comparison of the inference speeds using Bonsai-DLC plugin vs. the <monospace>DeepLabCut-Live!</monospace> package across image sizes from the same computer (OS: Windows 10, GPU: NVIDIA Ge-Force 1080 with Max-Q Design). Bottom: Inference speeds using the Bonsai-DLC plugin while the GPU was engaged in a particle simulation. More particles indicates greater competition for GPU resources.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig4-v3.tif"/></fig><p>We then took advantage of the built-in OpenGL shader support in Bonsai to assess how external load on the GPU would impact DLC inference performance, as would happen when running closed-loop virtual reality simulations in parallel with video inference. To do this, we implemented a simulation of N-body particle interactions using OpenGL compute shaders in Bonsai, where we were able to vary the load on the GPU by changing the number of particles in the simulation, from 5120 up to 51,200 particles. This is a quadratic problem as each particle interacts with every other particle, so it allows us to easily probe the limits of GPU load and its effects on competing processes.</p><p>Overall, we found that as the number of particle interactions increased the GPU load, there was a corresponding drop in DLC inference speeds (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The effects of load on inference were non-linear and mostly negligible until the load on the GPU approached 50%, then started to drop as more and more compute banks were scheduled and used, likely due to on-chip memory bottlenecks in the GPU compute units (<xref ref-type="bibr" rid="bib14">Hill et al., 2017</xref>). Nevertheless, as long as GPU load remained balanced, there was no obvious effect on inference speeds, suggesting that in many cases, it would be possible to combine closed-loop accelerated DeepLabCut-live inference with real-time visual environments running on the same GPU (<xref ref-type="bibr" rid="bib25">Lopes et al., 2020</xref>).</p></sec><sec id="s2-7"><title>Distributed DeepLabCut with Autopilot</title><p><ext-link ext-link-type="uri" xlink:href="https://auto-pi-lot.com">Autopilot</ext-link> is a Python framework designed to overcome problems of simultaneously collecting multiple streams of data by distributing different data streams over a swarm of networked computers (<xref ref-type="bibr" rid="bib41">Saunders and Wehr, 2019</xref>). Its distributed design could be highly advantageous for naturalistic experiments that require large numbers of cameras and GPUs operating in parallel.</p><p>Thus, we integrated <monospace>DeepLabCut-Live!</monospace> into Autopilot in a new module of composable data transformation objects. As a proof of concept, we implemented the minimal distributed case of two computers: one Raspberry Pi capturing images and one NVIDIA Jetson TX2, an affordable embedded system with an onboard GPU, processing them (see Materials and methods, <xref ref-type="table" rid="table3">Table 3</xref>).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Materials for Autopilot tests.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Tool</th><th>Version</th></tr></thead><tbody><tr><td>Raspberry Pi</td><td>4, 2 GB</td></tr><tr><td>Autopilot</td><td>0.3.0-2f31e78</td></tr><tr><td>Jetson</td><td>TX2 Developer Kit</td></tr><tr><td>Camera</td><td>FLIR CM3-U3-13Y3M-CS</td></tr><tr><td>Spinnaker SDK</td><td>2.0.0.147</td></tr><tr><td>Oscilloscope</td><td>Tektronix TDS 2004B</td></tr></tbody></table></table-wrap><p>We tested the performance of this system by measuring the end-to-end latency of a simple light detection task (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The Raspberry Pi lit an LED while capturing and streaming frames to the Jetson. Autopilot’s networking modules stream arrays by compressing them on-the-fly with blosc (<xref ref-type="bibr" rid="bib2">Alted et al., 2020</xref>) and routing them through a series of ‘nodes’– in this case, each frame passed through four networking nodes in each direction. The Jetson then processed the frames in a chain of <monospace>Transform</monospace>s that extracted poses from frames using <monospace>DLC-Live!</monospace> (DLC-MobileNetV2-0.35) and returned a Boolean flag indicating whether the LED was illuminated. <monospace>True</monospace> triggers were sent back to the Raspberry Pi which emitted a TTL voltage pulse to the LED on receipt.</p><p>Experiments were performed with differing acquisition frame rates (30, 60, 90 FPS), and image sizes (128 × 128, 256 × 256, 512 × 416 pixels; <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Frame rate had a little effect with smaller images, but at 512 × 416, latencies at 30 FPS (median = 161.3 ms, IQR = [145.6–164.4], n = 500) were 38.6 and 52.6 ms longer than at 60 FPS (median = 122.7 ms, IQR = [109.4–159.3], n = 500) and 90 FPS (median = 113.7 ms, IQR = [106.7–118.5], n = 500), respectively.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Latency of DeepLabCut-Live with Autopilot.</title><p>(<bold>A</bold>) Images of an LED were captured on a Raspberry Pi 4 (shaded blue) with Autopilot, sent to an Nvidia Jetson TX2 (shaded red) to process with DeepLabCut-Live and Autopilot Transforms, which triggered a TTL pulse from the Pi when the LED was lit. (<bold>B</bold>) Latencies from LED illumination to TTL pulse (software timestamps shown, points and densities) varied by resolution (color groups) and acquisition frame rate (shaded FPS bar). Processing time at each stage of the chain in (<bold>A</bold>) contributes to latency, but pose inference on the relatively slow TX2 (shaded color areas, from <xref ref-type="fig" rid="fig2">Figure 2</xref>) had the largest effect. Individual measurements (n = 500 each condition) cluster around multiples of the inter-frame interval (eg. 1/30 FPS = 33.3 ms).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig5-v3.tif"/></fig><p>Frame rate imposes intrinsic latency due to asynchrony between acquired events and the camera’s acquisition interval: for example if an event happens at the beginning of a 30 FPS exposure, the frame will not be available to process until 1/30 s = 33.3 ms later. If this asynchrony is distributed uniformly then a latency of half the inter-frame interval is expected for a given frame rate. This quantization of frame rate latency can be seen in the multimodal distributions in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, with peaks separated by multiples of their inter-frame interval. The inter-frame interval of inference with <monospace>DeepLabCut-Live!</monospace> imposes similar intrinsic latency. The combination of these two sources of periodic latency and occasional false-negatives in inference gives a parsimonious, though untested, account of the latency distribution for the 512 × 416 experiments.</p><p>Latency at different image sizes were primarily influenced by the relatively slow frame processing of the Jetson TX2 (See <xref ref-type="fig" rid="fig2">Figure 2</xref>). WIth smaller images (128 × 128 and 256 × 256), inference time (shaded areas in <xref ref-type="fig" rid="fig5">Figure 5B</xref>) was the source of roughly half of the total latency (inference/median total latency, n = 1500 each, pooled across frame rates. 128 × 128: 32.2/64.6 ms = 49.7%. 256 x 256: 34.5/65.1 ms = 53.0%). At 512 × 416, inference time accounted for between 50% and 70% of total latency (512 × 416, 30 FPS: 79.9/161.3ms = 49.5%, 90 FPS: 79.9/113.7 ms = 70.3%). Minimizing instrument cost for experimental reproducibility and scientific equity is a primary design principle of Autopilot, so while noting that it would be trivial to reduce latency by using a faster GPU or the lower-latency Jetson Xavier (i.e. see above sections), we emphasize that <monospace>DeepLabCut-Live!</monospace> in Autopilot is very usable with $350 of computing power (TX2 with education discount: $299, Jetson TX2 with NVIDIA Exclusive Education Discount: <ext-link ext-link-type="uri" xlink:href="https://web.archive.org/web/20200723081533/">https://web.archive.org/web/20200723081533/</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://www.nvidia.com/en-us/autonomous-machines/jetson-tx2-education-discount/">https://www.nvidia.com/en-us/autonomous-machines/jetson-tx2-education-discount/</ext-link>; Raspberry Pi 4 2 GB RAM: $35, Raspberry Pi from Adafruit: <ext-link ext-link-type="uri" xlink:href="https://web.archive.org/web/20200426114340/">https://web.archive.org/web/20200426114340/</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://www.adafruit.com/product/4292">https://www.adafruit.com/product/4292</ext-link>).</p><p>Autopilot, of course, imposes its own latency in distributing frame capture and processing across multiple computers. Subtracting latency that is intrinsic to the experiment (frame acquisition asynchrony and GPU speed), Autopilot has approximately 25 ms of overhead (median of latency - mean DeepLabCut inference time - 1/2 inter-frame interval = 25.4 ms, IQR = [14.7–37.9], n = 4500). Autopilot’s networking modules take 6.9 ms on average to route a message one-way (<xref ref-type="bibr" rid="bib41">Saunders and Wehr, 2019</xref>) and have been designed for high-throughput rather than low-latency (message batching, on-the-fly compression).</p></sec><sec id="s2-8"><title>Real-time feedback based on posture</title><p>Lastly, to demonstrate practical usability of triggering a TTL signal based on posture, we performed an experiment using DLG on a Jetson Xavier in which an LED was turned on when a dog performed a ‘rearing’ movement (raised forelimbs in the air, standing on only hindlimbs; <xref ref-type="fig" rid="fig6">Figure 6</xref>). First, a DeepLabCut network based on the ResNet-50 architecture (DLC-ResNet-50v1) was trained to track 20 keypoints on the face, body, forelimbs, and hindlimbs of a dog (see Materials and methods). Next, the Jetson Xavier running DLG was used to record the dog as she performed a series of ‘rearing’ movements in response to verbal commands, with treats given periodically by an experimenter. Video was recorded using a Logitech C270 webcam, with 640 × 480 pixel images at 30 FPS. Inference was run on images downsized by 50% (320 × 240 pixels), using the ‘Optimize Rate’ mode in DLG.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Real-time feedback based on posture.</title><p>(<bold>A</bold>) Diagram of the workflow for the dog feedback experiment. Image acquisition and pose estimation were controlled by the DLC-Live! GUI. A DeepLabCut-live Processor was used to detect ‘rearing’ postures based on the likelihood and y-coordinate of the withers and elbows and turn an LED on or off accordingly. (<bold>B</bold>) An example jump sequence with LED status, labeled with keypoints measured offline using the DeepLabCut-live benchmarking tool. The images are cropped for visibility. (<bold>C</bold>) Example trajectories of the withers and elbows, locked to one jump sequence. Larger, transparent points represent the true trajectories – trajectories measured offline, from each image in the video. The smaller opaque points represent trajectories measured in real-time, in which the time of each point reflects the delay from image acquisition to pose estimation, with and without the Kalman filter forward prediction. Without forward prediction, estimated trajectories are somewhat delayed from the true trajectories. With the Kalman filter forward prediction, trajectories are less accurate but less delayed when keypoints exhibit rapid changes in position, such as during a rearing movement. (<bold>D</bold>) The delay from the time the dog first exhibited a rearing posture (from postures measured offline) to the time the LED was turned on or off. Each point represents a single instance of the detection of a transition to a rearing posture or out of a rearing posture.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig6-v3.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>The Kalman filter predictor reduces errors when predicting up to three frames into the future in a mouse reaching task.</title><p>(Top) Images at different stages of the mouse reach, with the blue circle indicating the DeepLabCut predicted coordinate of the back of the hand. (Bottom, Left) The trajectory of the x-coordinate of the back of the mouse’s hand during the reach (pictured above). The thicker gray line indicates the trajectory measured by DLC. the blue line indicates the DLC trajectory as if it was delayed by 1–7 frames (indicated by the panel label on the right). The red line indicates the trajectory as if it were predicted 1–7 frames into the future by the Kalman filter predictor. (Bottom, Right) The cumulative distribution of errors for the back of the hand coordinate as if it were delayed 1–7 frames (blue) or as if it were predicted 1–7 frames into the future using the Kalman filter predictor (red) compared to the DLC estimated back of the hand coordinate. The dotted vertical line indicates the error at which the KF distribution intersects with the delayed pose distribution (i.e. the point at which there are more frames with this pixel error or smaller for the Kalman filter distribution than the delayed pose distribution). .</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig6-figsupp1-v3.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>The Kalman filter predictor reduces errors when predicting up to three frames into the future in a mouse open-field task.</title><p>The Kalman filter predictor reduces errors when predicting up to three frames into the future in a mouse open-field task. (Top) Images at different stages of the mouse open field video, with the blue circle indicating the DeepLabCut predicted coordinate of the snout. (Bottom Left) The trajectory of the x-coordinate of the mouse’s snout. The thicker gray line indicates the trajectory measured by DLC. The blue line indicates the DLC trajectory as if it was delayed by 1–7 frames (indicated by the panel label on the right). The red line indicates the trajectory as if it were predicted 1–7 frames into the future by the Kalman filter predictor. (Bottom Right) The cumulative distribution of errors for the back of the hand coordinate as if it were delayed 1–7 frames (blue) or as if it were predicted 1–7 frames into the future using the Kalman filter predictor (red) compared to the DLC estimated snout coordinate. The dotted vertical line indicates the error at which the two distributions intersect (i.e. the point at which there are more frames with this pixel error or smaller for either the delayed pose or the Kalman filter distribution).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61909-fig6-figsupp2-v3.tif"/></fig></fig-group><p>The dog was considered to be in a ‘rearing’ posture if the vertical position of at least one of the elbows was above the vertical position of the withers (between the shoulder blades). Similar to the mouse experiment, a <monospace>Processor</monospace> was used to detect ‘rearing’ postures and control an LED via communication with a Teensy micro-controller (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The LED was turned on upon the first image in which a ‘rearing’ posture was detected, and subsequently turned off upon the first image in which the dog was not in a ‘rearing’ posture (for a fully closed-loop stimulus) (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p>This setup achieved a rate of pose estimation of 22.417 ± 0.928 frames per second, with an image to pose latency of 61 ± 10 ms (N = 1848 frames) and, on images for which the LED was turned on or off, an image to LED latency of 59 ± 11 ms (N = 9 ‘rearing’ movements). However, using DLG, if the rate of pose estimation is slower than video acquisition, not all images will be used for pose estimation (N = 2433 total frames, 1848 poses recorded). To accurately calculate the delay from the ideal time to turn the LED on or off, we must compare the time of the first frame in which a ‘rearing’ posture was detected from all images recorded, not only from images used for pose estimation. To do so, we estimated the pose on all frames recorded offline using the same exported DeepLabCut model and calculated the ideal times that the LED would have been turned on or off from all available images. According to this analysis, there was a delay of 70 ± 23 ms to turn the LED on or off (consistent with estimates on Jetson systems shown above, see <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>As shown above, there are two methods that could reduce these delays: (i) training a DeepLabCut model based on the MobileNetV2 architecture (DLC-MobileNetV2-0.35 vs DLC-ResNet-50v1) or (ii) using more computationally powerful GPU-accelerated hardware for pose estimation (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). However, no matter how fast the hardware system, there will be some delay from acquiring images, estimating pose, and providing feedback. To overcome these delays, we developed another method to reduce latency for highly sensitive applications–to perform a forward prediction, or predict the animal’s future pose before the next image is acquired and processed. Depending on the forward prediction model, this could potentially reach zero-latency feedback levels (or below)– a dream for experimentalists who aim to study timing of causal manipulations in biological systems.</p><p>To reduce the delay to turn on the LED when the dog exhibited a ‘rearing’ movement, we implemented a Kalman filter that estimated the position, velocity, and acceleration of each keypoint, and used this information to predict the position of each keypoint at the current point in time (i.e., not the time at which the image was acquired, but after time had been taken to process the image). For example, if the image was acquired 60 ms ago, the Kalman filter ‘Processor’ predicted keypoints 60 ms into the future.</p><p>Thus, in another set of experiments, we recorded as the dog performed a series of ‘rearing’ movements, this time using a <monospace>Processor</monospace> that first forward-predicted the dog’s pose, then detected ‘rearing’ movements and controlled an LED accordingly. Using the <monospace>Processor</monospace> with a Kalman filter reduced inference speed compared to the <monospace>Processor</monospace> without the Kalman filter, due to the time required to compute Kalman filter updates (15.616 ± 0.332 frames per second). The image to pose latency was 81 ± 10 ms (N = 1187 frames), and image to LED latency of 82 ± 11 ms (N = 9 ‘rearing’ movements). However, compared to the ideal times to turn the LED on or off calculated from pose estimation performed on all available images, the Kalman filter ‘Processor’ achieved a delay of −13 ± 61 ms. In this case, the Kalman filter turned the LED on or off 13 ms <italic>prior</italic> to the point at which the first rearing posture was detected (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). These results indicate the potential to perform zero or negative latency feedback based on posture.</p><p>To better understand the utility and limitations of the Kalman filter predictor, we tested the Kalman filter <monospace>Processor</monospace> offline on two additional datasets: a mouse reaching dataset that exhibited movements on a similar timescale to dog ‘rearing’ movements (on the order of a few hundred milliseconds), but that was recorded with a higher frame rate (150 FPS), and a mouse open-field dataset that was recorded at the same frame rate as the dog ‘rearing’ experiment (30 FPS), but exhibited slower, smoother movements. To simplify these analyses, we focused on a single point in both datasets: the ‘back of the hand’ coordinate involved in the reaching movement, and the ‘snout’ in the open-field dataset. We examined the accuracy of the Kalman filter when predicting 1–7 frames into the future ( 7–47 ms for the mouse reaching or 33–233 ms for the open field) into the future compared to the accuracy of using the DLC pose that was obtained after a delay of 1–7 frames.</p><p>For the mouse reaching dataset, similar to the dog ‘rearing’ experiment, the Kalman filter predictions eliminated the lag that is present in the delayed pose, but the Kalman filter became more noisy and less accurate as it predicted the position of the back of the hand further into the future (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). To quantitatively examine the tracking errors when using the DLC estimated pose delayed by 1–7 frames vs. the Kalman filter predicted pose, we calculated the error as the euclidean distance between the delayed pose or Kalman filter predicted pose and the true DLC estimated pose for each frame throughout an entire experimental session (n = 153,279 frames). We then compared the cumulative distribution function (CDF) of errors (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). The CDF indicates the percentage of frames that have errors less than or equal to a particular error value, where a greater percentage of frames at a given error level indicates better performance (or more frames with errors that are smaller than this value). When predicting 4 frames/27.6 ms or further into the future, the delayed pose distribution had a greater percentage of frames with smaller errors at all error values, indicating that the Kalman filter produced larger errors than using the delayed pose when predicting 4 or more frames into the future. However, when predicting 1 frame/6.7 ms, 2 frames/13.3 ms, or 3 frames/20 ms into the future, the Kalman filter predicted back of the hand coordinate had more frames with errors smaller than 0.18, 0.85, and 2.94 pixels, respectively. This finding indicates that the Kalman filter predictor reduced the number of frames with larger errors, improving tracking performance when predicting up to 20 ms into the future.</p><p>The same analysis of the cumulative distribution of errors was performed on the open field dataset (n = 2330 frames), with similar results (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). When predicting 4 frames/133 ms or more into the future, the delayed pose distribution had a greater percentage of frames with smaller errors at nearly all error values. But when predicting 2 frames/67 ms or 3 frames/100 ms into the future, the Kalman filter predicted snout coordinate had more frames with errors smaller than 28.61 and 18.34 pixels, respectively, and when predicting 1 frame/33 ms into the future, the Kalman filter predictions had a greater percentage of frames with smaller errors across the entire distribution. These data indicate that the Kalman filter is effective at predicting further into the future on tracking problems that involve slower, more gradual movements.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Providing event-triggered feedback in real-time is one of the strongest tools in neuroscience. From closed-loop optogenetic feedback to behaviorally-triggered virtual reality, some of the largest insights from systems neuroscience have come through causally testing the relationship of behavior to the brain (<xref ref-type="bibr" rid="bib20">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Chettih and Harvey, 2019</xref>; <xref ref-type="bibr" rid="bib16">Jazayeri and Afraz, 2017</xref>). Although tools for probing both the brain and for measuring behavior have become more advanced, there is still the need for such tools to be able to seamlessly interact. Here, we aimed to provide a system that can provide real-time feedback based on advances in deep learning-based pose estimation. We provide new computational tools to do so with high speeds and low latency, as well as a a full benchmarking test suite (and related website: <ext-link ext-link-type="uri" xlink:href="https://deeplabcut.github.io/DLC-inferencespeed-benchmark/">https://deeplabcut.github.io/DLC-inferencespeed-benchmark/</ext-link>), which we hope enables ever more sophisticated experimental science.</p><sec id="s3-1"><title>Related work</title><p>DeepLabCut and related animal pose estimation tools reviewed in <xref ref-type="bibr" rid="bib31">Mathis and Mathis, 2020</xref> have become available starting in early 2018, and two groups have built tools around real-time applications with DeepLabCut. However, the reported speed and latencies are slower than what we were able to achieve here: <xref ref-type="bibr" rid="bib12">Forys et al., 2020</xref> achieved latencies of 30 ms using top-end GPUs, and this delay increases if the frame acquisition rate is increased beyond 100 frames per second. <xref ref-type="bibr" rid="bib42">Schweihoff et al., 2019</xref> also achieve latencies of 30 ms from frame acquisition to detecting a behavior of interest (round-trip frame to LED equivalent was not reported). We report a 2–3x reduction in latency (11 ms/16 ms from frame to LED in the ‘Optimize Latency’/‘Optimize Rate’ mode of DLG) on a system that uses a less powerful GPU (Windows/GeForce GTX 1080) compared to these studies, and equivalent performance (29 ms/35 ms from frame to LED in the ‘Optimize Latency’/‘Optimize Rate’ mode of DLG) on a conventional laptop (MacBook Pro with Intel Core-i7 CPU). Although we believe such tools can use the advances presented in this work to achieve higher frame rates and lower latencies, our new real-time approach provides an improvement in portability, speed, and latency.</p><p>Animal pose estimation toolboxes, like DeepLabCut, have all benefited from advances in human pose estimation research. Although the goals do diverge (reviewed in <xref ref-type="bibr" rid="bib31">Mathis and Mathis, 2020</xref>) in terms of required speed, the ability to create tailored networks, and accuracy requirements, competitions on human pose estimation benchmarks such as PoseTrack (<xref ref-type="bibr" rid="bib3">Andriluka et al., 2018</xref>) and COCO <xref ref-type="bibr" rid="bib23">Lin et al., 2014</xref> have advanced computer vision. Several human pose estimation systems have real-time options: OpenPose (<xref ref-type="bibr" rid="bib7">Cao et al., 2017</xref>) has a real-time hand/face pose tracker available, and PifPaf (<xref ref-type="bibr" rid="bib22">Kreiss et al., 2019</xref>) reaches about 10 Hz on COCO (depending on the backbone; <xref ref-type="bibr" rid="bib23">Lin et al., 2014</xref>). On the challenging multi-human PoseTrack benchmark (<xref ref-type="bibr" rid="bib3">Andriluka et al., 2018</xref>), LightTrack (<xref ref-type="bibr" rid="bib37">Ning et al., 2020</xref>) reaches less than 1 Hz. However, recent work achieves 3D multi-human pose estimation at remarkable frame rates (<xref ref-type="bibr" rid="bib8">Chen et al., 2020</xref>), in particular they report an astonishing 154 FPS for 12 cameras with four people in the frame. State of the art face detection frameworks, based on optimized architectures such as BlazeFace can achieve remarkable speeds of &gt;500 FPS on GPUs of cell phones (<xref ref-type="bibr" rid="bib5">Bazarevsky et al., 2019</xref>). The novel (currently unpublished) multi-animal version of DeepLabCut can also be used for feedback, and depending on the situation, tens of FPS for real-time applications should be possible. Inference speed can also be improved by various techniques such as network pruning, layer decomposition, weight discretization or feed-forward efficient convolutions (<xref ref-type="bibr" rid="bib52">Zhang et al., 2019</xref>). Plus, the ability to forward predict postures, as we show here, can be used to compensate for hardware delays.</p></sec><sec id="s3-2"><title>Scalability, affordability, and integration into existing pipelines</title><p>If neuroscience’s embrace of studying the brain in its natural context of complex, contingent, and open-ended behavior (<xref ref-type="bibr" rid="bib21">Krakauer et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="bib10">Datta et al., 2019</xref>) is smashing the champagne on a long-delayed voyage, the technical complexity of the experiments is the grim spectre of the sea. Markerless tracking has already enabled a qualitatively new class of data-dense behavioral experiments, but the heroism required to simultaneously record natural behavior from 62 cameras (<xref ref-type="bibr" rid="bib4">Bala et al., 2020</xref>) or electrophysiology from 65,000 electrodes (<xref ref-type="bibr" rid="bib40">Sahasrabuddhe et al., 2020</xref>), or integrate dozens of heterogeneous custom-built components (<xref ref-type="bibr" rid="bib11">Findley et al., 2020</xref>) hints that a central challenge facing neuroscience is <italic>scale</italic>.</p><p>Hardware-intensive experiments typically come at a significant cost, even if the pose estimation tools are ‘free’ (developed in laboratories at a non-significant expense, but provided open source). Current commercial systems are expensive–up to $10,000–and they have limited functionality; these systems track the location of animals but not postures or movements of experimenter-defined points of the animal, and few to none offer any advanced deep learning-based solutions. Thus, being able to track posture with state-of-the-art computer vision at scale is a highly attractive goal.</p><p><monospace>DeepLabCut-Live!</monospace> experiments are a many-to-many computing problem: many cameras to many GPUs (and coordinated with many other hardware components). The Autopilot experiment we described is the simplest 2-computer case of distributed applications of <monospace>DeepLabCut-Live!</monospace>, but Autopilot provides a framework for its use with arbitrary numbers of cameras and GPUs in parallel. Autopilot is not prescriptive about hardware configuration, so for example if lower latencies were needed users could capture frames on the same computer that processes them, use more expensive GPUs, or use the forward-prediction mode. Along with the rest of its hardware, experimental design, and data management infrastructure, integration of DeepLabCut in Autopilot makes complex experiments scalable and affordable.</p><p>Thus, here we presented options that span ultra-high performance (at GPU cost) to usable, affordable solutions that will work very well for most all applications (i.e. up to 90 FPS with zero to no latency if using our forward-prediction mode). Indeed, the Jetson experiments that we performed used simple hardware (inexpensive webcam, simple LED circuit) and either the open source DLC-Live! GUI or AutoPilot.</p><p>In addition to integration with Autopilot, we introduce integration of real-time DeepLabCut into Bonsai, a popular framework that is already integrated into many popular neuroscience tools such as OpenEphys (<xref ref-type="bibr" rid="bib44">Siegle et al., 2017</xref>), BonVision (<xref ref-type="bibr" rid="bib25">Lopes et al., 2020</xref>), and BpodDeveloped by Sanworks: <ext-link ext-link-type="uri" xlink:href="https://www.sanworks.io/index.php">https://www.sanworks.io/index.php</ext-link>. The merger of DeepLabCut and Bonsai could therefore allow for real-time posture tracking with sophisticated neural feedback with hardware such as NeuroPixels, Miniscopes, and beyond. For example, Bonsai and the newly released BonVision toolkit (<xref ref-type="bibr" rid="bib25">Lopes et al., 2020</xref>) are tools for providing real-time virtual reality (VR) feedback to animals. Here, we tested the capacity for a single GPU laptop system to run Bonsai-DLC with another computational load akin to what is needed for VR, making this an accessible tool for systems neuroscientists wanting to drive stimuli based on potentially sophisticated postures or movements. Furthermore, in our real-time dog-feedback utilizing the forward-prediction mode we utilized both posture and kinematics (velocity) to be able to achieve sub-zero latency.</p></sec><sec id="s3-3"><title>Sharing DLC models</title><p>With this paper we also introduce three new features within the core DeepLabCut ecosystem. One, the ability to easily export trained models without the need to share project folders (as previously); two, the ability to load these models into other frameworks aside from DLC-specific tools; and three, we modified the code-base to allow for frozen-networks. These three features are not only useful for real-time applications, but if users want to share models more globally (as we are doing with the DeepLabCut Model Zoo Project <xref ref-type="bibr" rid="bib30">Mathis et al., 2020b</xref>), or have a easy-install lightweight DeepLabCut package on dedicated machines for running inference, this is an attractive option. For example, the protocol buffer files are system and framework agnostic: they are easy to load into TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>) wrappers based on C++, Python, etc. This is exactly the path we pursued for Bonsai’s plugin via a C#-TensorFlow wrapperTensorFlowSharp: (<ext-link ext-link-type="uri" xlink:href="https://github.com/migueldeicaza/TensorFlowSharp">https://github.com/migueldeicaza/TensorFlowSharp</ext-link>). Moreover, this package can be utilized even in offline modes where batch processing is desirable for very large speed gains (<xref ref-type="bibr" rid="bib32">Mathis and Warren, 2018</xref>; <xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>).</p></sec><sec id="s3-4"><title>Benchmarking <monospace>DeepLabCut-Live!</monospace> GUI performance</title><p>Importantly, the DLG benchmarking data described above was collected by loading images from a pre-recorded video at the same rate that these images were originally acquired, effectively simulating the process of streaming video from a camera in real-time. This method was chosen for a few reasons. First, using a pre-recorded video standardized the benchmarking procedure across different platforms – it would have been extremely difficult to record nearly identical videos across different machines. Second, reading images from the video in the same exact manner that a video would be recorded from a physical camera in real-time exposed the same delays in DLG as we would observe when recording from a camera in real-time. The delays that we report all occur after images are acquired. Thus, if the desired frame rate is achieved, these benchmarking results are exactly equivalent to the delays that would be observed when recording from a physical camera. The possible frame rates that can be achieved by different cameras are documented by camera manufacturers and the software used to record video from cameras relies on libraries provided by camera manufacturers (e.g. The Imaging Source, The Imaging Source Libraries on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/TheImagingSource">https://github.com/TheImagingSource</ext-link>) or well-documented open-source projects (e.g. OpenCV,<ext-link ext-link-type="uri" xlink:href="https://github.com/opencv/opencv">https://github.com/opencv/opencv</ext-link> and Aravis, <ext-link ext-link-type="uri" xlink:href="https://github.com/AravisProject/aravis">https://github.com/AravisProject/aravis</ext-link>; <xref ref-type="bibr" rid="bib6">Bradski, 2000</xref>). Additional delays caused by recording from a physical camera are therefore specific to each individual type of camera. Finally, by making the videos used for benchmarking available, DLG users can replicate our benchmarking results, which will help in diagnosing the cause of performance issues.</p></sec><sec id="s3-5"><title>Choosing to optimize pose estimation rate vs. latency in the <monospace>DeepLabCut-Live!</monospace> GUI</title><p>When using DLG, if the rate of pose estimation is faster than the rate of image acquisition, pose estimation will run in synchrony with image acquisition (in parallel processes). However, if the pose estimation rate is slower than image acquisition, users have the choice of one of two modes: ‘Optimize Rate’ or ‘Optimize Latency.’ In the ‘Optimize Rate’ mode, pose estimation and image acquisition are run asynchronously, such that pose estimation is run continuously, but pose estimation for a given image does not necessarily start at the time the image was acquired. Thus, the delay from image acquisition to pose estimation is the delay from image acquisition until pose estimation begins plus the time it takes DeepLabCut to estimate the pose. In the ‘Optimize Latency’ mode, after the pose estimation process finishes estimating the pose on one frame, it will wait for the next frame to be acquired to get back in sync with the image acquisition process. This minimizes the latency from image acquisition to pose estimation, but results in a slower rate of pose estimation, and over course of an experiment, fewer estimated poses. Because the ‘Optimize Latency’ mode results in fewer estimated poses, we recommend using the ‘Optimize Rate’ mode for most applications. However, the ‘Optimize Latency’ mode may be particularly useful for applications in which it is critical to minimize delays to the greatest extent possible and it is not critical to sometimes miss the behavior of interest. One example in which a user may consider the ‘Optimize Latency’ mode could be to stimulate neural activity upon detecting the tongue exiting the mouth on a subset of trials in a licking task. In this example, the ‘Optimize Latency’ mode will provide a lower latency from when the image that detects the tongue was acquired to stimulating neural activity. However, the slower pose estimation rate will make it more likely that, on a given trial, the first image in which the tongue is present will not be analyzed. Thus, the ‘Optimize Latency’ mode will provide the shortest delays on trials in which this image is analyzed, and users can choose to not stimulate on trials in which the tongue is too far outside of the mouth when it is first detected by DeepLabCut.</p></sec><sec id="s3-6"><title>Feedback on posture and beyond</title><p>To demonstrate the feedback capabilities of <monospace>DeepLabCut-Live!</monospace> we performed a set of experiments where an LED was triggered based on the confidence of the DeepLabCut network and the posture of the animal (here a dog, but as is DeepLabCut, this package is animal and object agnostic). We also provide a forward-prediction mode that utilizes temporal information via kinematics to predict future postural states. In this demonstration, we used a Kalman filter to obtain filtered estimates of the position, velocity and acceleration at each point in time, and then predicted the future pose via quadratic approximation. We chose this approach for a few reasons: (i) it requires no prior training and (ii) with simple modifications to 2–3 parameters, it can be tailored to suit a wide variety of applications. Since this approach relies on a quadratic approximation, it can be successfully applied to any application for which it is possible to obtain accurate measurements of the position, velocity, and acceleration using a Kalman filter. The performance of the Kalman filter predictor will critically depend on how far into the future one wishes to predict and how quickly the velocity and acceleration of the keypoints change. If there are substantial changes in the velocity or acceleration of the keypoint within the time frame of the forward prediction, or if a keypoint is not present in most images but suddenly appears (e.g. detecting the appearance of the tongue during a lick sequence), the forward prediction will be inaccurate. This was evident in the dog feedback experiment where we were predicting <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> ms into the future during a rapid movement. Using a mouse reaching dataset with movements on a similar timescale, but with video recorded at a much higher rate (150 FPS for mouse reaching vs. 30 FPS for dog), and a mouse open field dataset with video recorded at the same rate as the dog ‘rearing’ experiment, but with slower movements, we demonstrated that the Kalman filter works extremely well in predicting the future pose at a shorter timescale, and demonstrated the way in which it’s predictions become inaccurate as it predicts the pose further into the future. However, despite the inaccurate predictions when the time to predict is longer than the timescale at which velocity and acceleration are changing, the Kalman filter still provides great promise to improve the time to detect the onset of rapid movements.</p><p>Additionally, we would like to emphasize that the Kalman filter is only one possible approach to predict the future pose. In addition to demonstrating its utility for forward prediction, the source code for the Kalman filter <monospace>Processor</monospace> provides a blueprint for implementing different methods of forward prediction using the DeepLabCut-Live! framework tailored for the specific tracking problem. For instance, one can imagine applications to rhythmic movements, where one predicts future behavior from many past cycles. Other time series models such as LSTMs, or neural networks can also be integrated in the predictor class. Furthermore, the simple comparison of the position of 2–3 keypoints is only one possible strategy for detecting the time to trigger peripheral devices. For example, one can build <monospace>Processor</monospace> objects to trigger on joint angles, or more abstract targets such as being in a particular high dimensional state space. We believe the flexibility of this feedback tool, plus the ability to record long-time scale videos for ‘standard’ DeepLabCut analysis makes this broadly applicable to many applications.</p></sec><sec id="s3-7"><title>Conclusions</title><p>We report the development of a new light-weight Python pose estimation package based on DeepLabCut, which can be integrated with behavioral control systems (such as Bonsai and AutoPilot) or used within a new DLC-Live! GUI. This toolkit allows users to do real-time, low-latency tracking of animals (or objects) on high-performance GPU cards or on low cost, affordable and scalable systems. We envision this being useful for precise behavioral feedback in a myriad of paradigms.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Alongside this publication we developed several software packages that are available on GitHub. Links are listed in n <xref ref-type="table" rid="table4">Table 4</xref> and <xref ref-type="table" rid="table5">Table 5</xref> and details provided throughout the paper.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Software packages presented with this paper.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>URL</th></tr></thead><tbody><tr><td><monospace>DeepLabCut-Live!</monospace> SDK</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut-live">GitHub Link</ext-link></td></tr><tr><td>Benchmarking Submission</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DLC-inferencespeed-benchmark">GitHub Link</ext-link></td></tr><tr><td>Benchmarking Results</td><td><ext-link ext-link-type="uri" xlink:href="https://deeplabcut.github.io/DLC-inferencespeed-benchmark/">Website Link</ext-link></td></tr><tr><td>DLC-Live! GUI</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut-live-GUI">GitHub Link</ext-link></td></tr><tr><td>Bonsai - DLC Plugin</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/bonsai-rx/deeplabcut">GitHub Link</ext-link></td></tr><tr><td>AutoPilot - DLC</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/wehr-lab/autopilot">GitHub Link</ext-link></td></tr></tbody></table></table-wrap><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Relevant DLC updates.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Feature</th><th>DLC version</th><th>Pub. link</th></tr></thead><tbody><tr><td>DLC-ResNets</td><td>1, 2.0+</td><td><xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>; <xref ref-type="bibr" rid="bib35">Nath et al., 2019</xref></td></tr><tr><td>DLC-MobileNetV2s</td><td>2.1+</td><td><xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref></td></tr><tr><td>Model Export Fxn</td><td>2.1.8+</td><td>this paper</td></tr><tr><td>DeepLabCut-live</td><td>new package</td><td>this paper</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Animals</title><p>All mouse work were carried out under the permission of the IACUC at Harvard University (#17-07-309). Dog videos and feedback was exempt from IACUC approval (with conformation from IACUC).</p><p>Mice were surgically implanted with a headplate as in <xref ref-type="bibr" rid="bib26">Mathis et al., 2017</xref>. In brief, using aseptic technique mice were anesthetized to the surgical plane, a small incision in the skin was made, the skull was cleaned and dried and a titanium headplate was placed with Metabond. Mice were allowed 7 days to recover and given burphrenorphine for 48 hr post-operatively. Mice used in the licking task were trained to lick at fixed intervals (details will be published elsewhere). Mice used in the reaching task were trained as in <xref ref-type="bibr" rid="bib26">Mathis et al., 2017</xref>.</p><p>The dog used in this paper was previously trained to perform rearing actions for positive reinforcement, and therefore no direct behavioral manipulation was done for this study.</p></sec><sec id="s4-2"><title>DeepLabCut</title><p>The mouse DeepLabCut model was trained according to the protocol in <xref ref-type="bibr" rid="bib35">Nath et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Mathis et al., 2020a</xref>. Briefly, the DeepLabCut toolbox (version 2.1.6.4) was used to (i) extract frames from selected videos, (ii) manually annotate keypoints on selected frames, (iii) create a training dataset to train the convolutional neural network, (iv) train the neural network, (v) evaluate the performance of the network, and (vi) refine the network. This network was trained on a total of 120 labeled frames.</p><p>The dog model was initially created based on the ‘full_dog’ model available from the DeepLabCut Model Zoo (ResNet-50, with ADAM optimization and imgaug augmentation <xref ref-type="bibr" rid="bib17">Jung et al., 2020</xref>; currently unpublished, more details will be provided elsewhere). Prior to running the DLG feedback experiments, initial training videos were taken, frames from these videos were extracted and labeled, and the model was retrained using imgaug with the built in scaling set to 0.1–0.5 to optimize network accuracy on smaller images. This network was re-trained with 157 labeled frames.</p><p>After training, DeepLabCut models were exported to a protocol buffer format (.pb) using the new export model feature in the main DeepLabCut package (2.1.8). This can be performed using the command line: <code xml:space="preserve">dlc model-export /path/to/config.yaml
        or in python:
import deeplabcut as dlc
dlc .export_model(”/path/to/config.yaml’)</code></p></sec><sec id="s4-3"><title><monospace>DeepLabCut-Live!</monospace> package</title><p>The DeepLabCut-Live code was written in Python 3 (<ext-link ext-link-type="uri" xlink:href="http://www.python.org">http://www.python.org</ext-link>), and distributed as open source code on GitHub and on PyPi. It utilizes TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>), numpy (<xref ref-type="bibr" rid="bib46">Svd et al., 2011</xref>), scipy (<xref ref-type="bibr" rid="bib47">Virtanen et al., 2020</xref>), OpenCV (<xref ref-type="bibr" rid="bib6">Bradski, 2000</xref>), and others. Please see GitHub for complete, platform-specific installation instructions and description of the package.</p><p>The DeepLabCut-live package provides a DLCLive class that facilitates loading DeepLabCut models and performing inference on single images. The DLC-Live class also has built in image pre-processing methods to reduce the size of images for faster inference: image cropping, dynamic image cropping around detected keypoints, and image downsizing. DLC-Live objects can be instantiated in the following manner:<code xml:space="preserve">from dlclive import DLCLive
my_live_object = DLCLive(”/path/to/exported/model/directory’)

# base instantiation
my_live_object = DLCLive(”/path/to/exported/model/directory’)

# use only the first 200 pixels in both width and height dimensions of image
my_live_object = DLCLive(”/path/to/exported/model/directory’,
                             cropping=[0, 200, 0, 200])

# dynamically crop image around detected keypoints, with 20 pixel buffer
my_live_object = DLCLive(”/path/to/exported/model/directory’,
                             dynamic=(True, 0.5, 20))

# resize height and width of image to 1/2 its original size
my_live_object = DLCLive(”/path/to/exported/model/directory’,
                          resize    = 0.5)</code></p><p>Inference speed tests were run using a benchmarking tool built into the DeepLabCut-Live package. Different image sizes were tested by adjusting the <italic>pixels</italic> parameter, which specifies the total number of pixels in the image while maintaining the aspect ratio of the full-size image. For all options of the benchmarking tool, please see GitHub or function documentation. Briefly, this tool can be used from the command line:<code xml:space="preserve">dlc—live—benchmark /path/to/model/directory/ path/to/video/file -o /path/to/output/directory</code> </p><p>or from python:<code xml:space="preserve">from dlclive import benchmark_videos
benchmark_videos (”/path/to/model/directory”,
                                                     ”/path/to/video”,
                                 output=”/path/to/output/directory”)</code></p><p>When using dynamic tracking within the <monospace>DeepLabCut-Live!</monospace> package, if the DeepLabCut reported likelihood of a keypoint is less than the user-defined likelihood threshold (i.e. it is likely that the keypoint was ‘lost’ from the image), the bounding box around the following image may not include that keypoint. Thus, for the dynamic cropping accuracy analysis, we only analyzed the accuracy of tracking for keypoints that had a likelihood greater than 0.5 on the previous image. This resulted in the exclusion of 2.9%, 7.6%, and 27.6% of all individual keypoints across all images, respective to the bound box size (50, 25, 10).</p></sec><sec id="s4-4"><title>DLC-Live! GUI software</title><p>The DLC-Live! GUI (DLG) code was also written in Python 3 (<ext-link ext-link-type="uri" xlink:href="http://www.python.org">http://www.python.org</ext-link>), and distributed as open source code on GitHub and on PyPi. DLG utilizes Tkinter for the graphical user interface. Please see GitHub for complete installation instructions and a detailed description of the package.</p><p>DLG currently supports a wide variety of cameras across platforms. On Windows, DLG supports The Imaging Source USB cameras and OpenCV compatible webcams. On MacOS, DLG supports OpenCV webcams, PlayStation Eye cameras (<ext-link ext-link-type="uri" xlink:href="https://github.com/bensondaled/pseyepy">https://github.com/bensondaled/pseyepy</ext-link>) and USB3 Vision and GigE Vision cameras (<ext-link ext-link-type="uri" xlink:href="https://github.com/AravisProject/aravis">https://github.com/AravisProject/aravis</ext-link>). On Linux, DLG supports any device compatible with Video4Linux drivers using OpenCV, and USB3 Vision and GigE Vision devices using the Aravis Project.</p><p>DLG uses the multiprocess package (<xref ref-type="bibr" rid="bib33">McKerns et al., 2012</xref>) to run image acquisition, writing images to disk and pose estimation in separate processes from the main user-interface. Running these processes in parallel enables users to record higher frame rate videos with minimal sacrifice to pose-estimation speed. However, there are still some delays when running image acquisition and pose-estimation asynchronously: if these processes are run completely independently, the image may not have been acquired immediately before pose-estimation begins. For example, if images are acquired at 100 frames per second, the image will have been acquired with a range of 0–10 ms prior to running pose-estimation on the image. If the pose-estimation process waits for a new image to be acquired then there will be a delay between completing pose-estimation on one image and beginning pose-estimation on the next one. Accordingly, DLG allows users to choose between two modes: (i) the Latency mode, in which the pose-estimation process waits for an a new image to reduce the latency between image acquisition and pose-estimation and (ii) the Rate mode, in which the pose-estimation process runs independently of image acquisition. In this mode, there will be longer latencies from image acquisition to pose-estimation but the rate of pose-estimation will be faster than in the Latency mode.</p><p>To test the performance of DLG, we used a video of a mouse performing a task that required licking to receive reward in the form of a drop of water. Video was collected at 100 frames per second using a The Imaging Source USB3 camera (model number: DMK 37AUX287) and Camera Control software (<xref ref-type="bibr" rid="bib19">Kane and Mathis, 2019</xref>).</p><p>We tested the performance of DLG under four conditions– on full-size images (352 × 274 pixels) and downsized images (176 × 137 pixels), both image sizes in Latency mode and Rate mode. All four conditions were tested on four different computers (see <xref ref-type="table" rid="table2">Table 2</xref> for specifications). The mouse licking video used for this test was different from the mouse licking video used for the inference speed benchmarking of the <monospace>DeepLabCut-Live!</monospace> package (a video with larger images was used in the <monospace>DeepLabCut-Live!</monospace> benchmark). Across all DLG benchmarking experiments, a single outlier frame-to-LED data point was removed from the Windows OS, GeForce 1080 GPU, 352 × 274 pixel size configuration. This one point had a time frame-to-LED delay of 3 s, nearly 10 times that of the next longest delay under any configuration, likely due to misalignment of the infrared LED and photodetector.</p></sec><sec id="s4-5"><title>Inference speed, size vs. accuracy and dynamic cropping</title><p>For the analyses regarding the size and accuracy dependency (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), we used the 640 × 480 pixel images available at Zenodo (<xref ref-type="bibr" rid="bib27">Mathis et al., 2018a</xref>) from <xref ref-type="bibr" rid="bib28">Mathis et al., 2018b</xref>.</p></sec><sec id="s4-6"><title>Postural feedback experiment</title><p>Prior to conducting the dog feedback experiment, the dog was extensively trained to ‘rear’ upon the verbal command ‘jump’ and the visual cue of a human’s arm raised at shoulder height. ‘Rearing’ was reinforced by manually providing treats for successful execution. Prior to recording videos for tracking and feedback, the dog routinely participated in daily training sessions with her owners. There was no change to the dog’s routine when implementing sessions in which feedback was provided.</p><p>To conduct the feedback experiment, we used the DLG software on a Jetson Xavier developer kit. Pose estimation was performed using an exported DeepLabCut model (details regarding training provided above). Feedback was provided using a custom <monospace>DeepLabCut-Live!</monospace> <monospace>Processor</monospace> that detected ‘rearing’ movements and controlled an LED via serial communication to a Teensy microcontroller.</p><p>The dog was considered to be in a ‘rearing’ posture if (a) the likelihood of the withers and at least one elbow was greater than 0.5 and (b) the vertical position of at least one elbow, whose likelihood was greater than 0.5, was above the vertical position of the withers (i.e., <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mtext>elbow</mml:mtext></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mtext>withers</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>, with the top of the image as <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and bottom of image as <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). For each pose, the <monospace>Processor</monospace> determined whether the dog was in a ‘rearing’ posture, queried the current status of the LED from the Teensy microcontroller, and if the current status did not match the dog’s posture (i.e. if the LED was off and the dog was in a ‘rearing’ posture), sent a command to the Teensy to turn the LED on or off.</p><p>In this experiment, we recorded the time at which images were accessed by DLG, the time at which poses were obtained by DLG after processing, and the times that the LED was turned on or off by the <monospace>Processor</monospace>. We calculated the pose estimation rate as the inverse of the delay from obtaining one pose to the next, the latency from image acquisition to obtaining the pose from that image, and, for poses in which the <monospace>Processor</monospace> turned the LED on or off, the latency from image acquisition to sending the command to turn the LED on or off.</p><p>As not all images will be run through pose estimation using DLG, to assess the delay from behavior to feedback, we performed offline analyses to determine the ideal time to turn the LED on or off given all the acquired images. Using the <monospace>DeepLabCut-Live!</monospace> benchmarking tool, we obtained the pose for all frames in the acquired videos by setting the <monospace>save_poses</monospace> flag from the command line:<code xml:space="preserve">dlc -live-benchmark /path/to/model/directory /path/to/video/file –save-poses -n 0</code> </p><p>This command can also be run from python:<code xml:space="preserve">from dlclive import benchmark_videos
benchmark_videos (”/path/to/model/directory”,
                                                     ”/path/to/video/file”,
                                                     n_frames = 0,
                                                     save_poses = True)</code></p><p>We then ran this full sequence of poses through the ‘rearing’ detection <monospace>Processor</monospace>, and compared these times – for each ‘rearing’ movement, the time at which the first frame that showed a transition to a ‘rearing’ posture and out of a ‘rearing’ posture was acquired – with the times that the LED was turned on or off during real-time feedback.</p><p>The videos analyzed in this experiment were different from the dog videos used in the <monospace>DeepLabCut-Live!</monospace> benchmarking experiment. For that experiment, a video with longer duration and different aspect ratio was used.</p><sec id="s4-6-1"><title>Forward prediction using a Kalman filter</title><p>To implement the forward-predicting Kalman filter, we used a <monospace>Processor</monospace> object that first used a Kalman filter to estimate the position, velocity, and acceleration of each keypoint; then used the position, velocity, and acceleration to predict the position of the limb into the future. The Kalman filter was defined by the state vector <italic>X</italic>, consisting of the x and y position, x and y velocity, and x and y acceleration of each keypoint; the forward transition matrix <italic>F</italic>; and the measurement matrix <italic>H</italic>. An example of the full state vector with <italic>n</italic> keypoints is:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&quot;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&quot;</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&quot;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&quot;</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p><p>For simplicity, we will consider a Kalman filter for a DeepLabCut network with one keypoint:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&quot;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&quot;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Importantly, performance of the Kalman filter depended on three user-defined scalar variance parameters: the initial variance in state estimates <inline-formula><mml:math id="inf5"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, process noise <italic>Q</italic>, and measurement noise <italic>R</italic>. The initial covariance matrix in state estimates was defined as <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>At each time step, we calculated the estimated pose <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> and estimated covariance <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> based on the previous state:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We noticed that predictions tended to be inaccurate when the predicted velocity and acceleration were very high. To encourage lower estimates of velocity and acceleration in the estimated state vector <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>, we introduced a priori assumption that, at any given timestep, velocity and acceleration will have a zero mean with a user-defined variance <italic>B</italic>. We incorporated this prior assumption using Bayes rule, such that the velocity and acceleration components of <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> were the weighted mean of 0 and <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>. The weight depended on the ratio of the variance in the estimate of <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> (the diagonals of the covariance matrix <italic>P</italic>, referred to as <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>) and the variance of the prior given by <italic>B</italic>:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mi>X</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The observation vector <italic>y</italic> consisted of the observed position, velocity, and acceleration. The observed position was taken as the pose returned by the DeepLabCut network, the observed velocity was calculated as the change in position from the observed position and position components of the latest state vector <italic>X</italic>, and the acceleration was calculated as the change between the observed velocity and the velocity components of the latest state vector <italic>X</italic>. Next, we updated the state vector <italic>X</italic> and covariance matrix <italic>P</italic> according to the Kalman gain <italic>K</italic>:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>∙</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Given this estimate of the current position, velocity and acceleration (the state vector <italic>X</italic>), we used the forward transition matrix <italic>F</italic> to calculate the predicted future state <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>X</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula>. In the dog feedback experiment, the amount of time we predicted into the future depended on the delay for that image (<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">q</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>):<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To obtain the future pose, we extracted the position elements from <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>X</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula>, and discarded the velocity and acceleration components. Lastly, the <monospace>Processor</monospace> checked if the dog was in a ‘rearing’ posture and controlled the LED accordingly.</p><p>Source code for the base <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut-live/tree/master/dlclive/processor/kalmanfilter.py">Kalman filter Processor</ext-link> and the <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut-live/tree/master/example_processors/DogJumpLED/izzy_jump.py">dog rearing Processor</ext-link> can be found on Github. Additionally, the Kalman filter predicting <monospace>Processor</monospace> is in the main <monospace>DeepLabCut-Live!</monospace> package, and can be used as follows:<code xml:space="preserve">from dlclive .processor import KalmanFilterPredictor</code></p></sec></sec><sec id="s4-7"><title>Details of AutoPilot setup</title><p>Latencies were measured using software timestamps and confirmed by oscilloscope. Software measurements could be gathered in greater quantity but were reliably longer than the oscilloscope latency measurements by 2.8 ms (median of difference, n = 75, IQR=[2.4–3.4]), thus we use the software measurements noting they are a slightly conservative estimate of functional latency.</p><p>Autopilot experiments were performed using the <ext-link ext-link-type="uri" xlink:href="https://github.com/wehr-lab/autopilot/blob/2f31e7897e1a45195db83c1db7cde9006f8239cf/autopilot/tasks/test.py#L20">DLC_Latency</ext-link> Task and the <ext-link ext-link-type="uri" xlink:href="https://github.com/wehr-lab/autopilot/blob/2f31e7897e1a45195db83c1db7cde9006f8239cf/autopilot/tasks/children.py#L168">Transformer</ext-link> ‘Child’ (see <xref ref-type="bibr" rid="bib41">Saunders and Wehr, 2019</xref> for terminology).</p><p>Separate DLC-MobileNetV2-0.35 models tracking a single point were trained for each capture resolution (128 × 128, 256 × 256, 512 × 416 pixels). Training data was labeled such that the point was touching the LED when it was on, and in the corner of the frame when the LED was off. Frames were processed with a chain of Autopilot <monospace>Transform</monospace> objects like:<code xml:space="preserve">from autopilot import transform as t

# create transformation object
tfm =t.image.DLC(”/model/path’) + \
              t.selection
              t.logical.Condition(
                      minimum = [min_x, min_y],
                      maximum = [max_x, max_y] 
              )
# process a frame, yielding a bool
# true/false == LED on/off
led_on =tfm.process(frame)</code></p><p>where <monospace>min_x, min_y</monospace>, etc. defined a bounding box around the LED.</p></sec><sec id="s4-8"><title>Data analysis and visualization</title><p>Autopilot data were analyzed with Pandas (1.0.5; <xref ref-type="bibr" rid="bib34">McKinney et al., 2010</xref>) in Python and Tidyverse (1.3.0; <xref ref-type="bibr" rid="bib50">Wickham et al., 2019</xref>) in R. Data were visualized with ggplot2 (3.3.0; <xref ref-type="bibr" rid="bib49">Wickham, 2016</xref>) and ggridges (0.5.2; <xref ref-type="bibr" rid="bib51">Wilke, 2020</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Sébastien Hausmann for testing the <monospace>DLC-Live! GUI</monospace>, and to Jessy Lauer for optimization assistance. We thank Jessica Schwab and Izzy Schwane for assistance with the dog feedback experiment. We greatly thank Michael Wehr for support of AutoPilot-DLC, and the Mathis Lab for comments. Funding was provided by the Rowland Institute at Harvard University and the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundation to AM and MWM; a Harvard Mind, Brain, Behavior Award to GK, MWM; and NSF Graduate Research Fellowship No. 1309047 to JLS. The authors declare no conflicts of interest. Contributions GK, AM, MWM conceptualized the project. GK, JS, GL, AM, MWM designed experiments. GK and JS developed Jetson integration. GK developed and performed experiments for DLC-Live, and the DLC-Live GUI. JS developed and performed experiments for AutoPilot-DLC. GL developed and performed experiments for Bonsai-DLC. AM contributed to Bonsai-DLC and DLC-Live. All authors performed benchmarking tests. GK and MWM original draft, all authors contributed to writing. MWM supervised the project.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Gonçalo Lopes is director at NeuroGEARS Ltd.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Investigation, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Formal analysis, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All mouse work was carried out under the permission of the IACUC at Harvard University (#17-07-309). Dog videos and feedback was exempt from IACUC approval (with conformation with IACUC).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-61909-transrepform-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All models, data, test scripts and software is released and made freely available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut-live">https://github.com/DeepLabCut/DeepLabCut-live</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f/">https://archive.softwareheritage.org/swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f/</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name><name><surname>Ghemawat</surname> <given-names>S</given-names></name><name><surname>Irving</surname> <given-names>G</given-names></name><name><surname>Isard</surname> <given-names>M</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: a system for large-scale machine learning</article-title><conf-name>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Alted</surname> <given-names>F</given-names></name><name><surname>Haenel</surname> <given-names>V</given-names></name><name><surname>Team</surname> <given-names>BD</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Python-blosc</data-title><source>Github</source><version designator="d08c8a1">d08c8a1</version><ext-link ext-link-type="uri" xlink:href="http://github.com/blosc/python-blosc">http://github.com/blosc/python-blosc</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Iqbal</surname> <given-names>U</given-names></name><name><surname>Insafutdinov</surname> <given-names>E</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Milan</surname> <given-names>A</given-names></name><name><surname>Gall</surname> <given-names>J</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posetrack: a benchmark for human pose estimation and tracking</article-title><conf-name> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>5167</fpage><lpage>5176</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00542</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bala</surname> <given-names>PC</given-names></name><name><surname>Eisenreich</surname> <given-names>BR</given-names></name><name><surname>Yoo</surname> <given-names>SBM</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Park</surname> <given-names>HS</given-names></name><name><surname>Zimmermann</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4560</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18441-5</pub-id><pub-id pub-id-type="pmid">32917899</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bazarevsky</surname> <given-names>V</given-names></name><name><surname>Kartynnik</surname> <given-names>Y</given-names></name><name><surname>Vakunov</surname> <given-names>A</given-names></name><name><surname>Raveendran</surname> <given-names>K</given-names></name><name><surname>Grundmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Blazeface: sub-millisecond neural face detection on mobile gpus</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.05047">https://arxiv.org/abs/1907.05047</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bradski</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><data-title>The OpenCV Library</data-title><source>Dr. Dobb’s Journal of Software Tools</source></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cao</surname> <given-names>Z</given-names></name><name><surname>Simon</surname> <given-names>T</given-names></name><name><surname>Wei</surname> <given-names>S</given-names></name><name><surname>Sheikh</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Realtime multi-person 2d pose estimation using part affinity fields</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>1302</fpage><lpage>1310</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.143</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Ai</surname> <given-names>H</given-names></name><name><surname>Chen</surname> <given-names>R</given-names></name><name><surname>Zhuang</surname> <given-names>Z</given-names></name><name><surname>Liu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cross-view tracking for multi-human 3d pose estimation at over 100 fps</article-title><conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>3276</fpage><lpage>3285</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00334</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chettih</surname> <given-names>SN</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-neuron perturbations reveal feature-specific competition in V1</article-title><source>Nature</source><volume>567</volume><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0997-6</pub-id><pub-id pub-id-type="pmid">30842660</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname> <given-names>SR</given-names></name><name><surname>Anderson</surname> <given-names>DJ</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name><name><surname>Leifer</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational neuroethology: a call to action</article-title><source>Neuron</source><volume>104</volume><fpage>11</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id><pub-id pub-id-type="pmid">31600508</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Findley</surname> <given-names>TM</given-names></name><name><surname>Wyrick</surname> <given-names>DG</given-names></name><name><surname>Cramer</surname> <given-names>JL</given-names></name><name><surname>Brown</surname> <given-names>MA</given-names></name><name><surname>Holcomb</surname> <given-names>B</given-names></name><name><surname>Attey</surname> <given-names>R</given-names></name><name><surname>Yeh</surname> <given-names>D</given-names></name><name><surname>Monasevitch</surname> <given-names>E</given-names></name><name><surname>Nouboussi</surname> <given-names>N</given-names></name><name><surname>Cullen</surname> <given-names>I</given-names></name><name><surname>Songco</surname> <given-names>J</given-names></name><name><surname>King</surname> <given-names>JF</given-names></name><name><surname>Ahmadian</surname> <given-names>Y</given-names></name><name><surname>Smear</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sniff-synchronized. gradient-guided olfactory search by freely moving mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.29.069252</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forys</surname> <given-names>BJ</given-names></name><name><surname>Xiao</surname> <given-names>D</given-names></name><name><surname>Gupta</surname> <given-names>P</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Real-Time selective markerless tracking of forepaws of head fixed mice using deep neural networks</article-title><source>Eneuro</source><volume>7</volume><elocation-id>ENEURO.0096-20.2020</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0096-20.2020</pub-id><pub-id pub-id-type="pmid">32409507</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname> <given-names>JM</given-names></name><name><surname>Chae</surname> <given-names>D</given-names></name><name><surname>Naik</surname> <given-names>H</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Koger</surname> <given-names>B</given-names></name><name><surname>Costelloe</surname> <given-names>BR</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hill</surname> <given-names>P</given-names></name><name><surname>Jain</surname> <given-names>A</given-names></name><name><surname>Hill</surname> <given-names>M</given-names></name><name><surname>Zamirai</surname> <given-names>B</given-names></name><name><surname>Hsu</surname> <given-names>CH</given-names></name><name><surname>Laurenzano</surname> <given-names>M</given-names></name><name><surname>Mahlke</surname> <given-names>SA</given-names></name><name><surname>Tang</surname> <given-names>L</given-names></name><name><surname>Mars</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DeftNN: addressing bottlenecks for dnn execution on gpus via synapse vector elimination and near-compute data fission</article-title><conf-name>2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</conf-name><fpage>786</fpage><lpage>799</lpage><pub-id pub-id-type="doi">10.1145/3123939.3123970</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Insafutdinov</surname> <given-names>E</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>DeeperCut: a deeper, stronger, and faster multi-person pose estimation model</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>34</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46466-4_3</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname> <given-names>M</given-names></name><name><surname>Afraz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Navigating the neural space in search of the neural code</article-title><source>Neuron</source><volume>93</volume><fpage>1003</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.019</pub-id><pub-id pub-id-type="pmid">28279349</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jung</surname> <given-names>AB</given-names></name><name><surname>Wada</surname> <given-names>K</given-names></name><name><surname>Crall</surname> <given-names>J</given-names></name><name><surname>Tanaka</surname> <given-names>S</given-names></name><name><surname>Graving</surname> <given-names>J</given-names></name><name><surname>Reinders</surname> <given-names>C</given-names></name><name><surname>Yadav</surname> <given-names>S</given-names></name><name><surname>Banerjee</surname> <given-names>J</given-names></name><name><surname>Vecsei</surname> <given-names>G</given-names></name><name><surname>Kraft</surname> <given-names>A</given-names></name><name><surname>Rui</surname> <given-names>Z</given-names></name><name><surname>Borovec</surname> <given-names>J</given-names></name><name><surname>Vallentin</surname> <given-names>C</given-names></name><name><surname>Zhydenko</surname> <given-names>S</given-names></name><name><surname>Pfeiffer</surname> <given-names>K</given-names></name><name><surname>Cook</surname> <given-names>B</given-names></name><name><surname>Fernández</surname> <given-names>I</given-names></name><name><surname>De Rainville</surname> <given-names>FM</given-names></name><name><surname>Weng</surname> <given-names>CH</given-names></name><name><surname>Ayala-Acevedo</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Imgaug</data-title><source>Github</source><version designator="0101108">0101108</version><ext-link ext-link-type="uri" xlink:href="https://github.com/aleju/imgaug">https://github.com/aleju/imgaug</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kane</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>DeepLabCut-live</data-title><source>Software Heritage</source><version designator="swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f">swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:703f20f0f4b80417f8277826efdef699213216d5;origin=https://github.com/DeepLabCut/DeepLabCut-live;visit=swh:1:snp:662794ebc2eed5e6c60e7becf6bbd43ea0ea4ba2;anchor=swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f/">https://archive.softwareheritage.org/swh:1:dir:703f20f0f4b80417f8277826efdef699213216d5;origin=https://github.com/DeepLabCut/DeepLabCut-live;visit=swh:1:snp:662794ebc2eed5e6c60e7becf6bbd43ea0ea4ba2;anchor=swh:1:rev:02cd95312ec6673414bdc4ca4c8d9b6c263e7e2f/</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kane</surname> <given-names>G</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Camera Control: record video and system timestamps from Imaging Source USB3 cameras</data-title><source>Zenodo</source><version designator="1.0.0">v1.0.0</version><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3360725">https://doi.org/10.5281/zenodo.3360725</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>CK</given-names></name><name><surname>Adhikari</surname> <given-names>A</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integration of optogenetics with complementary methodologies in systems neuroscience</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>222</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.15</pub-id><pub-id pub-id-type="pmid">28303019</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname> <given-names>JW</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Gomez-Marin</surname> <given-names>A</given-names></name><name><surname>MacIver</surname> <given-names>MA</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist Bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kreiss</surname> <given-names>S</given-names></name><name><surname>Bertoni</surname> <given-names>L</given-names></name><name><surname>Alahi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pifpaf: composite fields for human pose estimation</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>11977</fpage><lpage>11986</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.01225</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname> <given-names>TY</given-names></name><name><surname>Maire</surname> <given-names>M</given-names></name><name><surname>Belongie</surname> <given-names>S</given-names></name><name><surname>Hays</surname> <given-names>J</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name><name><surname>Ramanan</surname> <given-names>D</given-names></name><name><surname>Dollár</surname> <given-names>P</given-names></name><name><surname>Zitnick</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Microsoft coco: common objects in context</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>740</fpage><lpage>755</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname> <given-names>G</given-names></name><name><surname>Bonacchi</surname> <given-names>N</given-names></name><name><surname>Frazão</surname> <given-names>J</given-names></name><name><surname>Neto</surname> <given-names>JP</given-names></name><name><surname>Atallah</surname> <given-names>BV</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Moreira</surname> <given-names>L</given-names></name><name><surname>Matias</surname> <given-names>S</given-names></name><name><surname>Itskov</surname> <given-names>PM</given-names></name><name><surname>Correia</surname> <given-names>PA</given-names></name><name><surname>Medina</surname> <given-names>RE</given-names></name><name><surname>Calcaterra</surname> <given-names>L</given-names></name><name><surname>Dreosti</surname> <given-names>E</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lopes</surname> <given-names>G</given-names></name><name><surname>Farrell</surname> <given-names>K</given-names></name><name><surname>Horrocks</surname> <given-names>EAB</given-names></name><name><surname>Lee</surname> <given-names>CY</given-names></name><name><surname>Morimoto</surname> <given-names>MM</given-names></name><name><surname>Muzzu</surname> <given-names>T</given-names></name><name><surname>Papanikolaou</surname> <given-names>A</given-names></name><name><surname>Rodrigues</surname> <given-names>FR</given-names></name><name><surname>Wheatcroft</surname> <given-names>T</given-names></name><name><surname>Zucca</surname> <given-names>S</given-names></name><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Saleem</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>BonVision – an open-source software to create and control visual environments</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.09.983775</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Somatosensory cortex plays an essential role in forelimb motor adaptation in mice</article-title><source>Neuron</source><volume>93</volume><fpage>1493</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.049</pub-id><pub-id pub-id-type="pmid">28334611</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018a</year><data-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</data-title><source>Zenodo</source><version designator="1.0">1.0</version><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4008504">https://doi.org/10.5281/zenodo.4008504</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Biasi</surname> <given-names>T</given-names></name><name><surname>Schneider</surname> <given-names>S</given-names></name><name><surname>Yüksekgönül</surname> <given-names>M</given-names></name><name><surname>Rogers</surname> <given-names>B</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Pretraining boosts out-of-domain robustness for pose estimation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1909.11229.pdf">https://arxiv.org/pdf/1909.11229.pdf</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Schneider</surname> <given-names>S</given-names></name><name><surname>Lauer</surname> <given-names>J</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>A primer on motion capture with deep learning: principles, pitfalls, and perspectives</article-title><source>Neuron</source><volume>108</volume><fpage>44</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.017</pub-id><pub-id pub-id-type="pmid">33058765</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>60</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.10.008</pub-id><pub-id pub-id-type="pmid">31791006</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Warren</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the inference speed and video-compression robustness of deeplabcut</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/457242</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McKerns</surname> <given-names>MM</given-names></name><name><surname>Strand</surname> <given-names>L</given-names></name><name><surname>Sullivan</surname> <given-names>T</given-names></name><name><surname>Fang</surname> <given-names>A</given-names></name><name><surname>Aivazis</surname> <given-names>MAG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Building a framework for predictive science, CoRR</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1202.1056">http://arxiv.org/abs/1202.1056</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKinney</surname> <given-names>W</given-names></name><name><surname>van der Walt</surname> <given-names>S</given-names></name><name><surname>Millman</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Data structures for statistical computing in Python</article-title><conf-name>Proceedings of the 9th Python in Science Conference</conf-name><fpage>56</fpage><lpage>61</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>T</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>A</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>Nature Protocols</source><volume>14</volume><fpage>2152</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><pub-id pub-id-type="pmid">31227823</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Newell</surname> <given-names>A</given-names></name><name><surname>Yang</surname> <given-names>K</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stacked hourglass networks for human pose estimation</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>483</fpage><lpage>499</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ning</surname> <given-names>G</given-names></name><name><surname>Pei</surname> <given-names>J</given-names></name><name><surname>Huang</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Lighttrack: a generic framework for online top-down human pose tracking</article-title><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</conf-name><fpage>1034</fpage><lpage>1035</lpage><pub-id pub-id-type="doi">10.1109/CVPRW50498.2020.00525</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Aldarondo</surname> <given-names>DE</given-names></name><name><surname>Willmore</surname> <given-names>L</given-names></name><name><surname>Kislin</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>SS</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Privitera</surname> <given-names>M</given-names></name><name><surname>Ferrari</surname> <given-names>KD</given-names></name><name><surname>von Ziegler</surname> <given-names>LM</given-names></name><name><surname>Sturman</surname> <given-names>O</given-names></name><name><surname>Duss</surname> <given-names>SN</given-names></name><name><surname>Floriou-Servou</surname> <given-names>A</given-names></name><name><surname>Germain</surname> <given-names>PL</given-names></name><name><surname>Vermeiren</surname> <given-names>Y</given-names></name><name><surname>Wyss</surname> <given-names>MT</given-names></name><name><surname>De Deyn</surname> <given-names>PP</given-names></name><name><surname>Weber</surname> <given-names>B</given-names></name><name><surname>Bohacek</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A complete pupillometry toolbox for real-time monitoring of locus coeruleus activity in rodents</article-title><source>Nature Protocols</source><volume>15</volume><fpage>2301</fpage><lpage>2320</lpage><pub-id pub-id-type="doi">10.1038/s41596-020-0324-6</pub-id><pub-id pub-id-type="pmid">32632319</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sahasrabuddhe</surname> <given-names>K</given-names></name><name><surname>Khan</surname> <given-names>AA</given-names></name><name><surname>Singh</surname> <given-names>AP</given-names></name><name><surname>Stern</surname> <given-names>TM</given-names></name><name><surname>Ng</surname> <given-names>Y</given-names></name><name><surname>Tadić</surname> <given-names>A</given-names></name><name><surname>Orel</surname> <given-names>P</given-names></name><name><surname>LaReau</surname> <given-names>C</given-names></name><name><surname>Pouzzner</surname> <given-names>D</given-names></name><name><surname>Nishimura</surname> <given-names>K</given-names></name><name><surname>Boergens</surname> <given-names>KM</given-names></name><name><surname>Shivakumar</surname> <given-names>S</given-names></name><name><surname>Hopper</surname> <given-names>MS</given-names></name><name><surname>Kerr</surname> <given-names>B</given-names></name><name><surname>Hanna</surname> <given-names>MES</given-names></name><name><surname>Edgington</surname> <given-names>RJ</given-names></name><name><surname>McNamara</surname> <given-names>I</given-names></name><name><surname>Fell</surname> <given-names>D</given-names></name><name><surname>Gao</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The argo: a 65,536 channel recording system for high density neural recording in vivo</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.17.209403</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Saunders</surname> <given-names>JL</given-names></name><name><surname>Wehr</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Autopilot: automating behavioral experiments with lots of raspberry pis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/807693</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schweihoff</surname> <given-names>JF</given-names></name><name><surname>Loshakov</surname> <given-names>M</given-names></name><name><surname>Pavlova</surname> <given-names>I</given-names></name><name><surname>Kück</surname> <given-names>L</given-names></name><name><surname>Ewell</surname> <given-names>LA</given-names></name><name><surname>Schwarz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepLabStream: closing the loop using deep learning-based markerless</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2019.12.20.884478</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sehara</surname> <given-names>K</given-names></name><name><surname>Bahr</surname> <given-names>V</given-names></name><name><surname>Mitchinson</surname> <given-names>B</given-names></name><name><surname>Pearson</surname> <given-names>MJ</given-names></name><name><surname>Larkum</surname> <given-names>ME</given-names></name><name><surname>Sachdev</surname> <given-names>RNS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast, flexible Closed-Loop feedback: tracking movement in &quot;Real-Millisecond-Time&quot;</article-title><source>Eneuro</source><volume>6</volume><elocation-id>ENEURO.0147-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0147-19.2019</pub-id><pub-id pub-id-type="pmid">31611334</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>López</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>YA</given-names></name><name><surname>Abramov</surname> <given-names>K</given-names></name><name><surname>Ohayon</surname> <given-names>S</given-names></name><name><surname>Voigts</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Open ephys: an open-source, plugin-based platform for multichannel electrophysiology</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>045003</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa5eea</pub-id><pub-id pub-id-type="pmid">28169219</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Štih</surname> <given-names>V</given-names></name><name><surname>Petrucco</surname> <given-names>L</given-names></name><name><surname>Kist</surname> <given-names>AM</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stytra: an open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006699</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006699</pub-id><pub-id pub-id-type="pmid">30958870</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Svd</surname> <given-names>W</given-names></name><name><surname>Colbert</surname> <given-names>SC</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The numpy array: a structure for efficient numerical computation</article-title><source>Computing in Science &amp; Engineering</source><volume>13</volume><fpage>22</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2011.37</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname> <given-names>P</given-names></name><name><surname>Gommers</surname> <given-names>R</given-names></name><name><surname>Oliphant</surname> <given-names>TE</given-names></name><name><surname>Haberland</surname> <given-names>M</given-names></name><name><surname>Reddy</surname> <given-names>T</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Burovski</surname> <given-names>E</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name><name><surname>Weckesser</surname> <given-names>W</given-names></name><name><surname>Bright</surname> <given-names>J</given-names></name><name><surname>van der Walt</surname> <given-names>SJ</given-names></name><name><surname>Brett</surname> <given-names>M</given-names></name><name><surname>Wilson</surname> <given-names>J</given-names></name><name><surname>Millman</surname> <given-names>KJ</given-names></name><name><surname>Mayorov</surname> <given-names>N</given-names></name><name><surname>Nelson</surname> <given-names>ARJ</given-names></name><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Kern</surname> <given-names>R</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Carey</surname> <given-names>CJ</given-names></name><name><surname>Polat</surname> <given-names>İ</given-names></name><name><surname>Feng</surname> <given-names>Y</given-names></name><name><surname>Moore</surname> <given-names>EW</given-names></name><name><surname>VanderPlas</surname> <given-names>J</given-names></name><name><surname>Laxalde</surname> <given-names>D</given-names></name><name><surname>Perktold</surname> <given-names>J</given-names></name><name><surname>Cimrman</surname> <given-names>R</given-names></name><name><surname>Henriksen</surname> <given-names>I</given-names></name><name><surname>Quintero</surname> <given-names>EA</given-names></name><name><surname>Harris</surname> <given-names>CR</given-names></name><name><surname>Archibald</surname> <given-names>AM</given-names></name><name><surname>Ribeiro</surname> <given-names>AH</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>van Mulbregt</surname> <given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Ziegler</surname> <given-names>L</given-names></name><name><surname>Sturman</surname> <given-names>O</given-names></name><name><surname>Bohacek</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Big behavior: challenges and opportunities in a new era of deep behavior profiling</article-title><source>Neuropsychopharmacology</source><volume>46</volume><fpage>33</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1038/s41386-020-0751-7</pub-id><pub-id pub-id-type="pmid">32599604</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Ggplot2: Elegant Graphics for Data Analysis</source><publisher-name>Springer-Verlag</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-98141-3</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name><name><surname>Averick</surname> <given-names>M</given-names></name><name><surname>Bryan</surname> <given-names>J</given-names></name><name><surname>Chang</surname> <given-names>W</given-names></name><name><surname>McGowan</surname> <given-names>L</given-names></name><name><surname>François</surname> <given-names>R</given-names></name><name><surname>Grolemund</surname> <given-names>G</given-names></name><name><surname>Hayes</surname> <given-names>A</given-names></name><name><surname>Henry</surname> <given-names>L</given-names></name><name><surname>Hester</surname> <given-names>J</given-names></name><name><surname>Kuhn</surname> <given-names>M</given-names></name><name><surname>Pedersen</surname> <given-names>T</given-names></name><name><surname>Miller</surname> <given-names>E</given-names></name><name><surname>Bache</surname> <given-names>S</given-names></name><name><surname>Müller</surname> <given-names>K</given-names></name><name><surname>Ooms</surname> <given-names>J</given-names></name><name><surname>Robinson</surname> <given-names>D</given-names></name><name><surname>Seidel</surname> <given-names>D</given-names></name><name><surname>Spinu</surname> <given-names>V</given-names></name><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Vaughan</surname> <given-names>D</given-names></name><name><surname>Wilke</surname> <given-names>C</given-names></name><name><surname>Woo</surname> <given-names>K</given-names></name><name><surname>Yutani</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Welcome to the tidyverse</article-title><source>Journal of Open Source Software</source><volume>4</volume><elocation-id>1686</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01686</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wilke</surname> <given-names>CO</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Ggridges</data-title><source>Ridgeline Plots in ’ggplot2’</source><version designator="0.1">0.1</version><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=ggridges">https://CRAN.R-project.org/package=ggridges</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Q</given-names></name><name><surname>Zhang</surname> <given-names>M</given-names></name><name><surname>Chen</surname> <given-names>T</given-names></name><name><surname>Sun</surname> <given-names>Z</given-names></name><name><surname>Ma</surname> <given-names>Y</given-names></name><name><surname>Yu</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recent advances in convolutional neural network acceleration</article-title><source>Neurocomputing</source><volume>323</volume><fpage>37</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2018.09.038</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>Y</given-names></name><name><surname>Ye</surname> <given-names>W</given-names></name><name><surname>Vela</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Low-latency visual slam with appearance-enhanced local map building</article-title><conf-name>2019 International Conference on Robotics and Automation (ICRA)</conf-name><fpage>8213</fpage><lpage>8219</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zuffi</surname> <given-names>S</given-names></name><name><surname>Kanazawa</surname> <given-names>A</given-names></name><name><surname>Berger-Wolf</surname> <given-names>T</given-names></name><name><surname>Black</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Three-d safari: learning to estimate zebra pose, shape, and texture from images ”in the wild”</article-title><conf-name>ICCV IEEE Computer Society</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.2019.00546</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61909.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewer</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Branco</surname><given-names>Tiago</given-names> </name><role>Reviewer</role><aff><institution>UCL</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This submission introduces a new set of software tools for implementing real-time, marker-less pose tracking. The manuscript describes these tools, presents a series of benchmarks, and demonstrates their use in several experimental settings: including deploying low-latency closed-loop events triggered on an animal's pose. The first key development presented is a new python package – DeepLabCut-Live! – which optimizes pose inference to increase its speed, a key step for real-time application of DLC. The authors then present a new method for exporting trained DLC networks in a language-independent format and demonstrate how these can be used in three different environments to deploy experiments. Importantly, in addition to developing their own GUI, the authors have developed plugins for Bonsai and AutoPilot, two software packages already widely used by the systems neuroscience community to run experiments.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Real-time, low-latency closed-loop feedback using markerless posture tracking&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Gordon Berman as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Tiago Branco (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another, were largely positive about the article, and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>This submission introduces a new set of software tools for implementing real-time, marker-less pose tracking. The manuscript describes these tools, presents a series of benchmarks and demonstrates their use in several experimental settings, which include deploying very low-latency closed-loop events triggered on pose detection. The software core is based on DeepLabCut (DLC), previously developed by the senior authors. The first key development presented is a new python package – DeepLabCut-Live! – which optimizes pose inference to increase its speed, a key step for real-time application of DLC. The authors then present a new method for exporting trained DLC networks in a language-independent format and demonstrate how these can be used in three different environments to deploy experiments. Importantly, in addition to developing their own GUI, the authors have developed plugins for Bonsai and AutoPilot, two software packages already widely used by the systems neuroscience community to run experiments.</p><p>All three reviewers agreed that this work is exciting, carefully done, and would be of interest to a wide community of researchers. The consensus was the this was a strong submission. There were, however, four points that the reviewers felt could be addressed to increase the scope and the influence of the work (enumerated below).</p><p>Essential revisions:</p><p>1) The fundamental trade-off in tracking isn't image size vs. speed, but rather accuracy vs. speed. Thus, the reviewers felt that providing a measure of how the real (i.e., not pixel space) accuracy of the tracking was affected by changing the image resolution would be very helpful to researchers wishing to design experiments that utilize this software.</p><p>2) The manuscript would also benefit from including additional details about the Kalman filtering approach used here (as well as, potentially, further discussion about how it might be improved in future work). For instance, while the use of Kalman Filters to achieve sub-zero latencies is very exciting, it is unclear how robust this approach is. This applies not only to the parameters of the filter itself, but also on the types of behavior that this approach can work with successfully. Presumably, this requires a high degree of stereotypy and reproducibility of the actions being tracked and the reviewers felt that some discussion on this point would be valuable.</p><p>3) A general question that the reviewers had was how the number of key (tracked) points affects the latency. For example, the “light detection task” using AutoPilot uses a single key-point, how would the additional of more key-points affect performance in this particular configuration? More fully understanding this relationship would be very helpful in guiding future experimental design using the system.</p><p>4) The DLG values appear to have been benchmarked using an existing video as opposed to a live camera feed. It is conceivable that a live camera feed would experience different kinds of hardware-based bottlenecks that are not present when streaming in a video (e.g., USB3 vs. ethernet vs. wireless). Although this point is partially addressed with the demonstration of real-time feedback based on posture later in the manuscript, a replication of the DLG benchmark with a live stream from a camera at 100 FPS would be helpful to demonstrate frame rates and latency given the hardware bottlenecks introduced by cameras. If this is impossible to do at the moment, however, at minimum, adding a discussion stating that this type of demonstration is currently missing and outlining these potential challenges would be important.</p><p>Again, though, the reviewers were very enthusiastic about this Tools and Resources submission, and the above points primarily point towards ways in which the authors could increase the manuscript's impact, rather than fundamental concerns about its validity or overall value to the community.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61909.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The fundamental trade-off in tracking isn't image size vs. speed, but rather accuracy vs. speed. Thus, the reviewers felt that providing a measure of how the real (i.e., not pixel space) accuracy of the tracking was affected by changing the image resolution would be very helpful to researchers wishing to design experiments that utilize this software.</p></disp-quote><p>We thank the reviewers for this important point! Indeed, accuracy can be affected when images are downsampled, if this is not accounted for during network training. Accuracy could be affected for two reasons: (i) downsampling the image lowers the resolution significantly, and this loss of information may lead to inaccurate predictions, or (ii) the network may not have been trained on smaller images. Mathis and Warren, 2018, has shown that DeepLabCut is mostly robust to loss of information due to video compression (see Mathis and Warren, 2018, Figure 3). However, when images are compressed to a great degree, accuracy is reduced. Similarly, we expect that DeepLabCut will be robust to downsizing images within reason, but that accuracy will be reduced if images are downsized to a great degree. However, reduced accuracy may be mitigated by training networks specifically on downsized images. If someone knows they want to use the network in a more downsampled setting, they can train with a different range. Within DeepLabCut, this can easily be set in the `pose_cfg.yaml` file by modifying the default parameters:</p><p>`scale_jitter_lo: 0.5` and/or `scale_jitter_up: 1.25` before training (see more here: https://github.com/DeepLabCut/DeepLabCut/blob/8115ee24750b573bce4eb987a27010a5b 6cf2557/deeplabcut/pose_cfg.yaml#L44)​. Note, in this work, we already account for users to downsample to ~0.5 and up to ~1.25 as this is the default for training all DLC networks.</p><p>However, to better understand how downsizing images affects tracking accuracy, and to demonstrate this relationship to users, we directly tested accuracy across a range of downsized images on networks that were trained using the default scale parameters (scale = 0.5-1.25), a wider range of image sizes (scale = 0.1-1.25), and a network trained only on smaller images (scale = 0.1-0.5). A new subsection of the Results “Inference speed: size vs. accuracy” has been added (and quoted below), and we added a new supplementary figure, Figure 2—figure supplement 2:</p><p>“Although reducing the size of images increases inference speed, it may result in reduced accuracy in tracking if the resolution of the downsized image is too small to make accurate predictions, or if the network is not trained to perform inference on smaller images. […] Overall, this highlights for one example that downsizing might not strongly impact the accuracy and that this might be the easiest way to increase inference speeds”.</p><p>Relatedly, we want to highlight a feature that allows the user to still run “full size” frames as they wish, but only run online analysis in a dynamically cropped way. I.e., if you wish to track the pupil, or a hand reaching, for example, you can set up a box around just these smaller areas, and even if the animal moves, DLC will “dynamically crop” to that size and find the pupil or hand. Thus, small frame sizes can be very attractive for many applications – the benefit of full resolution (higher accuracy), with the speed of a smaller area.</p><p>We now include a new subsection in the results “Dynamic cropping: increased speed without effects on accuracy”, along with a new supplementary figure, Figure 2—figure supplement 3, on this feature.</p><disp-quote content-type="editor-comment"><p>2) The manuscript would also benefit from including additional details about the Kalman filtering approach used here (as well as, potentially, further discussion about how it might be improved in future work). For instance, while the use of Kalman Filters to achieve sub-zero latencies is very exciting, it is unclear how robust this approach is. This applies not only to the parameters of the filter itself, but also on the types of behavior that this approach can work with successfully. Presumably, this requires a high degree of stereotypy and reproducibility of the actions being tracked and the reviewers felt that some discussion on this point would be valuable.</p></disp-quote><p>We agree that the manuscript could benefit from both additional details about the Kalman filter prediction and a further discussion about its applications and limitations. We added complete mathematical details about the Kalman filter estimation of the position, velocity, and acceleration of each keypoint and how they are used to predict the future position of each keypoint. This can be found in the Materials and methods subsection “Postural feedback experiment”, “Forward prediction using the Kalman Filters”.</p><p>Regarding limitations of the Kalman filter, we do want to clarify this point: “Presumably, this​ requires a high degree of stereotypy and reproducibility of the actions being tracked.” ​In fact, there is no formal requirement for the behavior to be stereotypical. The forward model is not trained directly on the data beforehand -- the forward prediction is simply a quadratic approximation using the position, velocity and acceleration. We encourage the user to try this on the behavior beforehand, but there is no inherent limitation, provided that the time the user wishes to predict in the future is not significantly greater than the timescale at which velocity and acceleration of the keypoints are changing. We edited our discussion about the postural feedback experiment to include a more detailed discussion regarding the use-cases and limitations of the Kalman filter:</p><p>“To demonstrate the capabilities of DeepLabCut-Live! we performed a set of experiments where an LED was triggered based on the confidence of the DeepLabCut network and the posture of the animal (here a dog, but as is DeepLabCut, this package is animal and object agnostic). […] We believe the flexibility of this feedback tool, plus the ability to record long-time scale videos for “standard” DeepLabCut analysis makes this broadly applicable to many applications.“</p><p>We also further demonstrate the utility of the Kalman filter on two additional datasets: a mouse reaching dataset with rapid movements on the order of a few hundred milliseconds, and a mouse open-field dataset with slower, more gradual movements.</p><disp-quote content-type="editor-comment"><p>3) A general question that the reviewers had was how the number of key (tracked) points affects the latency. For example, the “light detection task” using AutoPilot uses a single key-point, how would the additional of more key-points affect performance in this particular configuration? More fully understanding this relationship would be very helpful in guiding future experimental design using the system.</p></disp-quote><p>Thank you for raising this concern. To address this point, we directly tested how the number of keypoints affects inference speed using the DeepLabCut-Live! Package, as this would directly affect any downstream packages, like AutoPilot. These results are reported in a new subsection of the Results, “Number of keypoints does not affect inference speed”, and in a supplementary figure, Figure 2—figure supplement 4:</p><p>“We also tested whether the number of keypoints in the DeepLabCut network affected inference speeds. […] The number of keypoints in the model had no effect on inference speed for either backbone (Figure 2—figure supplement 4).”</p><disp-quote content-type="editor-comment"><p>4) The DLG values appear to have been benchmarked using an existing video as opposed to a live camera feed. It is conceivable that a live camera feed would experience different kinds of hardware-based bottlenecks that are not present when streaming in a video (e.g., USB3 vs. ethernet vs. wireless). Although this point is partially addressed with the demonstration of real-time feedback based on posture later in the manuscript, a replication of the DLG benchmark with a live stream from a camera at 100 FPS would be helpful to demonstrate frame rates and latency given the hardware bottlenecks introduced by cameras. If this is impossible to do at the moment, however, at minimum, adding a discussion stating that this type of demonstration is currently missing and outlining these potential challenges would be important.</p></disp-quote><p>Thank you for raising this point: ​ “It is conceivable that a live camera feed would experience​ different kinds of hardware-based bottlenecks that are not present when streaming in a video (e.g., USB3 vs. ethernet vs. wireless).”​ We have addressed this concern in a new section of the Discussion, “Benchmarking DeepLabCut-Live! GUI Performance,”:</p><p>“Importantly, the DLG benchmarking data described above was collected by loading images from a pre-recorded video at the same rate that these images were acquired, effectively simulating the process of streaming video from a camera in real-time. […] Finally, by making the videos used for benchmarking available, DLG users can replicate our benchmarking results, which will help in diagnosing the cause of performance issues.”</p></body></sub-article></article>