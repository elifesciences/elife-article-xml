<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">96217</article-id><article-id pub-id-type="doi">10.7554/eLife.96217</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.96217.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Convolutional networks can model the functional modulation of the MEG responses associated with feed-forward processes during visual word recognition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>van Vliet</surname><given-names>Marijn</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6537-6899</contrib-id><email>w.m.vanvliet@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rinkinen</surname><given-names>Oona</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Shimizu</surname><given-names>Takao</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Niskanen</surname><given-names>Anni-Mari</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Devereux</surname><given-names>Barry</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Salmelin</surname><given-names>Riitta</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2499-193X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/020hwjq30</institution-id><institution>Department of Neuroscience and Biomedical Engineering, Aalto University</institution></institution-wrap><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hswnk62</institution-id><institution>School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast</institution></institution-wrap><addr-line><named-content content-type="city">Belfast</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/020hwjq30</institution-id><institution>Aalto NeuroImaging, Aalto University</institution></institution-wrap><addr-line><named-content content-type="city">Espo</named-content></addr-line><country>Finland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>13</day><month>05</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP96217</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-02-06"><day>06</day><month>02</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-19"><day>19</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.02.08.479654"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-30"><day>30</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96217.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-03-13"><day>13</day><month>03</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96217.2"/></event></pub-history><permissions><copyright-statement>© 2024, van Vliet et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>van Vliet et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-96217-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-96217-figures-v1.pdf"/><abstract><p>Traditional models of reading lack a realistic simulation of the early visual processing stages, taking input in the form of letter banks and predefined line segments, making them unsuitable for modeling early brain responses. We used variations of the VGG-11 convolutional neural network (CNN) to create models of visual word recognition that starts from the pixel-level and performs the macro-scale computations needed for the detection and segmentation of letter shapes to word-form identification of large vocabulary of 10k Finnish words, regardless of letter size, shape, or rotation. The models were evaluated based on an existing magnetoencephalography (MEG) study where participants viewed regular words, pseudowords, noise-embedded words, symbol strings, and consonant strings. The original images used in the study were presented to the models and the activity in the layers was compared to MEG evoked response amplitudes. Through a few alterations to make the network more biologically plausible, we found an CNN architecture that can correctly simulate the behavior of three prominent responses, namely the type I (early visual response), type II (the ‘letter string’ response), and the N400m. In conclusion, starting a model of reading with convolution-and-pooling steps enables the flexibility and realism crucial for a direct model-to-brain comparison.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational modeling</kwd><kwd>magnetoencephalography</kwd><kwd>reading</kwd><kwd>evoked potentials</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002341</institution-id><institution>Research Council of Finland</institution></institution-wrap></funding-source><award-id>#310988</award-id><principal-award-recipient><name><surname>van Vliet</surname><given-names>Marijn</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002341</institution-id><institution>Research Council of Finland</institution></institution-wrap></funding-source><award-id>#315553</award-id><principal-award-recipient><name><surname>Salmelin</surname><given-names>Riitta</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002341</institution-id><institution>Research Council of Finland</institution></institution-wrap></funding-source><award-id>#343385</award-id><principal-award-recipient><name><surname>van Vliet</surname><given-names>Marijn</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002341</institution-id><institution>Research Council of Finland</institution></institution-wrap></funding-source><award-id>#355407</award-id><principal-award-recipient><name><surname>Salmelin</surname><given-names>Riitta</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006306</institution-id><institution>Sigrid Juséliuksen Säätiö</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Salmelin</surname><given-names>Riitta</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Convolutional neural networks can model the early feed-forward components of the MEG evoked response during visual word recognition.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>What computational operations is the brain performing when it recognizes some splotches of ink on a piece of paper as a meaningful word? This question has been the focus of a large number of neuroimaging studies that examine brain activity during reading. Noninvasive measurement techniques such as electroencephalography (EEG; <xref ref-type="bibr" rid="bib25">Grainger and Holcomb, 2009</xref>), magnetoencephalography (MEG; <xref ref-type="bibr" rid="bib74">Salmelin, 2007</xref>), and functional magnetic resonance imaging (fMRI; <xref ref-type="bibr" rid="bib67">Price, 2012</xref>) have provided a wealth of information about when and where changes in activity might be expected during various tasks involving orthographic processing (<xref ref-type="bibr" rid="bib8">Carreiras et al., 2014</xref>).</p><p>In the case of visual word recognition, a lot of these results come in the form of changes in the amplitude of specific components of the neural response evoked by stimuli that are designed to create interesting experimental contrasts (<xref ref-type="bibr" rid="bib51">Luck, 2014</xref>). Such evoked components reflect macro-level computations—that is, the net result of thousands of individual biological neurons working together. Taken together, the results indicate the presence of a processing pipeline, starting with the extraction of low-level visual features (e.g., edges, line segments), which are subsequently refined into more complex features (e.g., letter shapes) and further into lexical features (e.g., bigrams, words) (<xref ref-type="bibr" rid="bib19">Dehaene et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Grainger and Holcomb, 2009</xref>). While neuroimaging studies provide us with information about what processing steps are performed where and when, the observed data alone yield little information as to what kind of computations are performed during these steps (<xref ref-type="bibr" rid="bib66">Poeppel, 2012</xref>). To develop such an understanding, we need to make these computations explicit, model them, and test and refine the model against the data provided by imaging studies (<xref ref-type="bibr" rid="bib2">Barber and Kutas, 2007</xref>; <xref ref-type="bibr" rid="bib68">Price, 2018</xref>; <xref ref-type="bibr" rid="bib22">Doerig et al., 2023</xref>).</p><p>In this study, we aimed to computationally reproduce the results of <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>, which is a representative MEG study that employed multiple experimental contrasts designed to study key processing steps throughout the first 500 ms of the visual word recognition pipeline. The authors catalogued the effects of the experimental contrasts on the amplitudes of all major evoked MEG responses found in the data, and concluded that the significant effects could be attributed to three components that dominate the early MEG time course during visual word recognition (<xref ref-type="bibr" rid="bib74">Salmelin, 2007</xref>; <xref ref-type="bibr" rid="bib38">Jobard et al., 2003</xref>), namely:</p><p>Type I: This component peaks occipitally around 100 ms after stimulus onset, is modulated by the visual complexity of the stimulus and hence thought to reflect the processing of low-level visual features (<xref ref-type="bibr" rid="bib84">Tarkiainen et al., 1999</xref>). <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> used a contrast between stimuli with and without added visual noise to highlight this processing stage.</p><p>Type II: This component peaks occipitotemporally around 150 ms after stimulus onset, and is modulated by whether the stimulus contains letters of the participant’s native alphabet (<xref ref-type="bibr" rid="bib84">Tarkiainen et al., 1999</xref>; <xref ref-type="bibr" rid="bib63">Parviainen et al., 2006</xref>). Hence, this component is colloquially referred to as the ‘letter string’ response and thought to reflect letter shape detection. <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> used a contrast between letter strings and symbol strings (visually similar to letters but not part of the Finnish alphabet) to highlight this processing stage.</p><p>N400m: This component peaks at around 400 ms after stimulus onset, and is often studied in the context of priming (<xref ref-type="bibr" rid="bib44">Kutas and Federmeier, 2011</xref>), but experimental effects can also be observed when using isolated stimuli. For example, this component is modulated by the lexicality of the stimulus, hence associated with more lexical processing (<xref ref-type="bibr" rid="bib28">Halgren et al., 2002</xref>; <xref ref-type="bibr" rid="bib33">Helenius, 1998</xref>; <xref ref-type="bibr" rid="bib81">Service et al., 2007</xref>; <xref ref-type="bibr" rid="bib74">Salmelin, 2007</xref>). <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> used a contrast between consonant strings, valid Finnish words and pseudowords to highlight this processing stage.</p><p>These three components are also observed in EEG studies (referred to as the P1, N1/N170, and N400 components, respectively; <xref ref-type="bibr" rid="bib6">Brem et al., 2009</xref>). Together, they are indicative of three feed-forward processing stages during visual word recognition: basic visual analysis, orthographic analysis, and lexical analysis. To explore possible cognitive computations during these stages, we sought to create a computational model that performs the macro-level computations needed to achieve visual word recognition in such a manner as to reproduce the behavior of the three evoked components when presented with the same stimuli as a human participant.</p><p>In past decades, several computational models have been created that successfully capture some aspects of visual word recognition. For the purposes of this study, we draw a distinction between the older ‘traditional-style’ models (<xref ref-type="bibr" rid="bib60">Norris, 2013</xref>) and the large artificial neural networks trained through deep learning that have been gaining popularity as models of brain function (<xref ref-type="bibr" rid="bib71">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">LeCun et al., 2015</xref>). We will show that a model will need to combine elements from both approaches if it is to be able to reproduce the stimulus-dependent modulation of all evoked components mentioned above.</p><p>Among the first models of visual word recognition, the interactive activation and competition (IAC) model of letter perception by <xref ref-type="bibr" rid="bib53">McClelland and Rumelhart, 1981</xref>; <xref ref-type="bibr" rid="bib72">Rumelhart and McClelland, 1982</xref>, showed how the interplay of feed-forward and feed-back connections results in a system capable of ‘filling in the blanks’ when faced with a partially obscured word. This model was later extended to model semantics as well, showing how the activation of some semantic features (‘is alive’, ‘is a bird’) leads to the subsequent activation of more semantic features (‘can fly’, ‘lays eggs’), in what became known as the parallel distributed processing (PDP) framework (<xref ref-type="bibr" rid="bib54">McClelland and Rogers, 2003</xref>). Note that while models such as these consist of many interconnected units, those units and their connections do not aim to mimic a biological connectome, but are an abstract representation of macro-level computations performed by one. <xref ref-type="bibr" rid="bib12">Coltheart et al., 2001</xref> pointed out the benefits of explicitly defined connections in their dual-route cascaded (DRC) model of visual word recognition and reading out loud, as they grant the researcher exact control over the macro-level computations. However, as the scope of the models increases, ‘hand-wiring’ the connections in the model becomes increasingly difficult. Therefore, most current models employ back-propagation to learn the connection weights between units based on a large training dataset. Together, IAC, PDP, and DRC models have been shown to account for many behavioral findings, such as reaction times and recognition errors, in both healthy volunteers and patients (<xref ref-type="bibr" rid="bib55">McLeod et al., 2000</xref>; <xref ref-type="bibr" rid="bib54">McClelland and Rogers, 2003</xref>; <xref ref-type="bibr" rid="bib65">Perry et al., 2007</xref>). Furthermore, <xref ref-type="bibr" rid="bib46">Laszlo and Plaut, 2012</xref> demonstrated how a PDP-style model can produce an N400m-like signal by summing the activity of the computational units in specific layers of the model.</p><p>However, none of the aforementioned models can produce detailed quantitative data that can be directly compared with neuroimaging data, leaving researchers to focus on finding indirect evidence that the same macro-level computations occur in both the model and the brain, with varying levels of success (<xref ref-type="bibr" rid="bib38">Jobard et al., 2003</xref>; <xref ref-type="bibr" rid="bib69">Protopapas et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Barber and Kutas, 2007</xref>). The simulated environments in these models are extremely simplified, partly due to computational limitations and partly due to the complex interaction of feed-forward and feed-back connectivity that causes problems with convergence when the model grows too large. Consequently, these models have primarily focused on feed-back lexico-semantic effects while oversimplifying the initial feed-forward processing of the visual input. The models make use of ‘letter banks’, where each letter of a written word is encoded in as a separate group of inputs. The letters themselves are encoded as either a collection of 16 predefined line segments (<xref ref-type="bibr" rid="bib53">McClelland and Rumelhart, 1981</xref>; <xref ref-type="bibr" rid="bib12">Coltheart et al., 2001</xref>) or a binary code indicating the letter (<xref ref-type="bibr" rid="bib46">Laszlo and Plaut, 2012</xref>; <xref ref-type="bibr" rid="bib47">Laszlo and Armstrong, 2014</xref>). This rather high level of visual representation sidesteps having to deal with issues such as visual noise, letters with different scales, rotations and fonts, segmentation of the individual letters, and so on. More importantly, it makes it impossible to create the visual noise and symbol string conditions used in the MEG study to modulate the type I and II components. In order to model the process of visual word recognition to the extent where one may reproduce neuroimaging studies such as <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>, we need to start with a model of vision that is able to directly operate on the pixels of a stimulus. We sought to construct a model that is able to recognize words regardless of length, size, typeface, and rotation with very high accuracy, while producing activity that mimics the type I, type II, and N400m components which serve as snapshots of this process unfolding in the brain. For this model, we chose to focus on the early feed-forward processing occurring during visual word recognition, as the experimental setup in the MEG study was designed to demonstrate, rather than feed-back effects.</p><p>Models of vision seem to have converged on a sequence of convolution-and-pooling operations to simulate the macro-level behavior of the visual cortex (<xref ref-type="bibr" rid="bib80">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Lindsay, 2021</xref>). Such models have become much more powerful as advances in deep learning and its software ecosystem are rapidly changing our notion of what is computationally tractable to model (<xref ref-type="bibr" rid="bib71">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">LeCun et al., 2015</xref>). Convolutional neural networks (CNNs) have emerged that perform scale- and rotation-invariant visual object recognition at very high levels of accuracy (<xref ref-type="bibr" rid="bib42">Krizhevsky et al., 2017</xref>; <xref ref-type="bibr" rid="bib82">Simonyan and Zisserman, 2015</xref>; <xref ref-type="bibr" rid="bib13">Dai et al., 2021</xref>). Furthermore, they model some functions of the visual cortex well enough that a direct comparison between network state and neuroimaging data has become possible (<xref ref-type="bibr" rid="bib78">Schrimpf et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Devereux et al., 2018</xref>; <xref ref-type="bibr" rid="bib92">Yamins and DiCarlo, 2016</xref>).</p><p>Visual word recognition can be seen as a specialized form of object recognition (<xref ref-type="bibr" rid="bib26">Grainger, 2018</xref>). Developmental studies suggest that orthographic-specific neuroimaging findings such as the visual word form area (VWFA) and the type II evoked components emerge after part of our established vision system is reconfigured as we learn to read (<xref ref-type="bibr" rid="bib63">Parviainen et al., 2006</xref>; <xref ref-type="bibr" rid="bib11">Cohen and Dehaene, 2004</xref>; <xref ref-type="bibr" rid="bib20">Dehaene-Lambertz et al., 2018</xref>). Studies on the visual cortex of non-human primates have shown that visual systems even without extensive exposure to written language have neural representations that carry enough information to distinguish letter and word shapes (<xref ref-type="bibr" rid="bib70">Rajalingham et al., 2020</xref>), even accounting for distortions (<xref ref-type="bibr" rid="bib40">Katti and Arun, 2022</xref>), and this finding was reproduced in CNNs. Therefore, CNNs may very well be suitable tools for increasing the scale of traditional connectionist models of reading, thus allowing for a much more realistic modeling of the early orthographic processing steps than what has been possible so far. Indeed, earlier work has established that training a CNN to classify written words can coerce it to form a VWFA-like region on top of an existing visual system (<xref ref-type="bibr" rid="bib30">Hannagan et al., 2021</xref>), and cause it to form high-level orthographic representations (<xref ref-type="bibr" rid="bib85">Testolin et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Katti and Arun, 2022</xref>) which can be used by a linear regression algorithm to predict human MEG activity in the visual cortex (<xref ref-type="bibr" rid="bib9">Caucheteux and King, 2022</xref>).</p><p>Whereas traditional-style models are typically designed to replicate a specific experimental effect in a biologically feasible manner, deep learning models are designed to perform in a more naturalistic setting, regardless of biological plausibility. Consequently, traditional-style models have been evaluated by comparing the timing and amount of the simulated activity to the amplitude of an evoked component (<xref ref-type="bibr" rid="bib46">Laszlo and Plaut, 2012</xref>; <xref ref-type="bibr" rid="bib47">Laszlo and Armstrong, 2014</xref>; <xref ref-type="bibr" rid="bib61">Nour Eddine et al., 2024</xref>) or human reaction times (<xref ref-type="bibr" rid="bib1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="bib59">Norris and Kinoshita, 2012</xref>) in a tightly controlled experimental setting, and deep learning models have been evaluated using data from participants who are operating in a more challenging setting, such as watching movies (<xref ref-type="bibr" rid="bib35">Huth et al., 2012</xref>), reading sentences (<xref ref-type="bibr" rid="bib9">Caucheteux and King, 2022</xref>), or listening to stories (<xref ref-type="bibr" rid="bib36">Huth et al., 2016</xref>). For the latter type of comparisons, information-based metrics are used. For example, representational similarity analysis (RSA; <xref ref-type="bibr" rid="bib41">Kriegeskorte et al., 2008</xref>) quantifies the extent to which the inner state of a model discriminates between the same types of stimuli as a neural response, and BrainScore (<xref ref-type="bibr" rid="bib78">Schrimpf et al., 2020</xref>) quantifies the extent to which a (usually linear) regressor or classifier can use the inner state of a model to predict brain activity. However, information-based metrics by themselves provide at best only a rough indication, and it is questionable whether better correlation scores truly indicates how closely a model mimics the computations performed by the brain (<xref ref-type="bibr" rid="bib76">Schaeffer et al., 2024</xref>). Furthermore, if the score is not satisfactory, the metric does not provide a clear pathway to understand what the model is lacking and what could be done to improve it. Hence, it would be desirable to establish a more direct link between a model and the experimental results obtained in neuroscience studies (<xref ref-type="bibr" rid="bib5">Bowers et al., 2022</xref>).</p><p>Therefore, in the current study, we evaluated the performance of a CNN in the spirit of the traditional models, namely by its ability to accurately reproduce the behavior of the type I, type II, and N400m responses in the tightly controlled experimental setting of <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>. By doing so, we restrict ourselves to an investigation of how well the stimulus-dependent modulation of the three evoked components can be explained by a feed-forward CNN in an experimental setting designed to demonstrate feed-forward effects. As such, the goal is not to present a complete model of all aspects of reading, which should include feed-back effects, but rather to demonstrate the effectiveness of using a model that has a realistic form of input when the aim is to align the model with the evoked responses observed during visual word recognition.</p><p>We started with a basic CNN architecture, namely VGG-11 (<xref ref-type="bibr" rid="bib82">Simonyan and Zisserman, 2015</xref>), that had been pretrained to classify images and evolved it into a model of visual word recognition by retraining it to classify written words. The performance of the model was evaluated by presenting it with the same set of stimuli that had been used by <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> and comparing the amount of activity within its layers with the amplitude of the type I, type II, and N400m components measured from the human participants. Different versions of the model were created by introducing slight modifications to the base architecture and using different training regimens. These variations were first evaluated on their ability to replicate the experimental effects in that study, namely that the type I response is larger for noise-embedded words than all other stimuli, the type II response is larger for all letter strings than symbols, and that the N400m is larger for real and pseudowords than consonant strings. Once a variation was found that could reproduce these effects satisfactorily, it was further evaluated based on the correlation between the amount of activation of the units in the model and MEG response amplitude. The final model (1) has the ability of deep learning models to operate directly on the pixels of a stimulus, (2) can qualitatively replicate the experimental results of <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> and some beyond, (3) has a large vocabulary of 10,000 words, and (4) yields a good quantitative correlation between the amount of activation in various layers and the amplitude of the type I, type II, and N400m responses.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The three MEG responses have unique response profiles indicative of different feed-forward processing steps</title><p>The computational models in this study were evaluated by their success in replicating the functional effects observed in the evoked MEG components as recorded by <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> during the presentation of written words, pseudowords, consonant strings, symbol strings, and noise-embedded words (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) to 15 participants. Performing distributed source analysis (<xref ref-type="bibr" rid="bib15">Dale et al., 2000</xref>) yielded a comprehensive map of where activity can be found (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), which is used in this study mostly to provide context for a deeper investigation into three peaks of activity that can be observed along the ventral stream. Through guided equivalent current dipole (ECD) modeling (<xref ref-type="bibr" rid="bib75">Salmelin, 2010</xref>), the high-dimensional data was summarized as a sparse set of ECDs, each one capturing an isolated spatiotemporal component, allowing us to study selected components in more detail. The original study <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> found three ECD groups along the ventral stream that correspond to the type I, type II, and N400m responses, based on their location (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), timing (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), and responses to the different stimulus types (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Similar ECD groups were identified in the right hemisphere (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), but the effects were clearer in the left. Therefore, in this study, we re-used the three ECD groups on the left hemisphere to serve as ground-truth for our modeling efforts.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Summary of the magnetoencephalography (MEG) results obtained by <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>.</title><p>(<bold>A</bold>) Examples of stimuli used in the magnetoencephalography (MEG) experiment. Each stimulus contained seven to eight letters or symbols. (<bold>B</bold>) Source estimate of the evoked MEG activity, using MNE-dSPM. The grand-average activity to word stimuli, averaged for three time intervals, is shown in orange hues. For each time interval, white circles indicate the location of the most representative left-hemisphere equivalent current dipoile (ECD) for each participant, as determined by <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>. (<bold>C</bold>) Grand-average time course of signal strength for each group of ECDs in response to the different stimulus types. The traces are color-coded to indicate the stimulus type as shown in (<bold>A</bold>). Shaded regions indicate time periods over which statistical analysis was performed. (<bold>D</bold>) For each group of ECDs shown in (<bold>B</bold>), and separately for each stimulus type (different colors, see <bold>A</bold>), the distribution (and mean) of the grand-average response amplitudes to the different stimulus types, obtained by integrating the ECD signal strength over the time intervals highlighted in (<bold>C</bold>). Whenever there is a significant difference (linear mixed effects [LME] model, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Summary of the magnetoencephalography (MEG) results on the right hemisphere.</title><p>This figure mimics <xref ref-type="fig" rid="fig1">Figure 1</xref> in the paper, but contains informa tion about the right hemisphere. (<bold>A</bold>) Examples of stimuli used in the magnetoencephalography (MEG) experiment. Each stimulus contained 7–8 letters or symbols. (<bold>B</bold>) Source estimate of the evoked MEG activity, using MNE-dSPM. The grand-average activity to word stimuli, averaged for three time intervals, is shown in orange hues. For each time interval, white circles indicate the location of the most representative left-hemisphere equivalent current dipole (ECD) for each participant, as determined by <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>. (<bold>C</bold>) Grand-average time course of signal strength for each group of ECDs in response to the different stimulus types. The traces are color-coded to indicate the stimulus type as shown in (<bold>A</bold>). Shaded regions indicate time periods over which statistical analysis was performed. (<bold>D</bold>) For each group of ECDs shown in (<bold>B</bold>), and separately for each stimulus type (different colors, see <bold>A</bold>), the distribution (and mean) of the grand-average response amplitudes to the different stimulus types, obtained by integrating the ECD signal strength over the time intervals highlighted in (<bold>C</bold>). Whenever there is a significant difference (LME model, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig1-figsupp1-v1.tif"/></fig></fig-group><p>To compare the evoked MEG responses and model layer activity, we collected summary statistics of both, referred to as ‘response profiles’. For the ECD time courses, the response profiles were formed by computing for each stimulus the average signal amplitude in the time intervals indicated in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, z-transformed across the stimuli, and averaged across participants to obtain grand-average response profiles (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The response profile for the type I response demonstrates how this occipital component is driven by the visual complexity of the stimulus and is characterized by a large response to noise-embedded words relative to all other stimulus types. The type II response, located further along the fusiform gyrus, exhibits sensitivity to whether the stimulus contains letters that are part of the participant’s native alphabet and, in contrast to the type I, has a reduced response to the noise-embedded words. The N400m response is located in the temporal cortex and is modulated by the lexical content of the stimulus, manifested in this study as a larger response to the word-like (i.e., words and pseudowords) versus the non-word-like stimuli. Taken together, these three response profiles are indicative of different phases during feed-forward processing, namely analysis of low-level visual features such as line segments and edges, followed by the analysis of letter shapes, and finally lexical analysis (<xref ref-type="bibr" rid="bib74">Salmelin, 2007</xref>).</p></sec><sec id="s2-2"><title>CNNs can produce responses that are qualitatively and quantitatively similar to those of the three MEG evoked components</title><p>As a model of the computations underlying the brain activity observed during the MEG experiment, we started with a VGG-11 (<xref ref-type="bibr" rid="bib83">Szegedy et al., 2015</xref>) network architecture, pretrained on ImageNet (<xref ref-type="bibr" rid="bib73">Russakovsky et al., 2015</xref>). This architecture consists of five convolution layers (three of which perform convolution twice), followed by two densely connected layers, terminating in an output layer (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Variations of the model were trained to perform visual word recognition using training sets consisting of images of written Finnish words, rendered in varying fonts, sizes and rotations (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In each case, training lasted for 20 epochs, which was enough for each model to obtain a near perfect accuracy (&gt;99%) on an independent validation set, as well as being able to correctly identify the original 118 word stimuli used in the MEG experiment. During training, the models were never exposed to any of the other stimulus types used in the MEG experiment (noisy words, symbols, consonant strings, and pseudowords) nor to the exact word stimuli used in that experiment. After training was complete, response profiles for all layers were created by computing the <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> norm of the ReLU activations of the units in each layer in response to each stimulus, followed by a z-transformation (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Finally, the response profiles of the CNN models were compared to those of the three MEG evoked components by judging whether similar effects of the experimental contrasts could be observed. This allowed for an iterative design approach, where the network architecture and training diet of the model were manipulated to address shortcomings in the response profiles, until a model was reached with the response profiles of the contained layers matching those of the type I, type II, and N400m evoked MEG components.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Overview of the proposed computation model of feed-forward processing during visual word recognition.</title><p>(<bold>A</bold>) The VGG-11 model architecture, consisting of five convolution layers, two fully connected layers, and one output layer. (<bold>B</bold>) Examples of the images used to train the model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig2-v1.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Building a model that can simulate the type I, type II, and N400m responses.</title><p>Starting from a VGG-11 model, we made adjustments to the base architecture and training diet of the model to produce variations which simulated activity better matches the response profiles of the three magnetoencephalography (MEG) evoked components. (<bold>A</bold>) For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the MEG experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. Layers for which the response pattern was qualitatively similar to that of the type I, type II, or N400m component are outlined with a box of the appropriate color. (<bold>B</bold>) Correlation between the layers of each model (horizontal axis) and the three MEG evoked components (different curves). Layers for which the response profile (<bold>A</bold>) was judged to qualitatively correspond to one of the MEG components (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) are indicated as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Impact of batch normalization and noisy activations.</title><p>To accurately model the type I response profile, the model needs to filter out visual noise in its later layers.This figure explores the effect of batch normalization and having noisy unit activations on the response profiles of the model. For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Impact of (pre-)training.</title><p>This figure shows how the response profiles of the model changes, starting from randomly initialized weights, to training with ImageNet, to training on words (10k vocabulary, frequency scaled), to pretraining on ImageNet and then training onwords. For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Random initializations.</title><p>The weights of the model were initialized from a model that was trained on ImageNet, so are the same every time the model is trained. However, the order of the words shown during training are random. This figure shows the variance in response profiles as the model is retrained 10 times, each time with a new random shuffling of the training words (10k vocabulary, frequency scaled). For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig3-figsupp3-v1.tif"/></fig></fig-group><p>Since both model layer activation and MEG evoked response amplitudes are described by a single number per stimulus in the response profiles, quantitative comparison between response profiles can be performed through straightforward correlation analysis (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). While the models will consistently produce the exact same response to a given stimulus, evoked MEG response amplitudes can vary substantially between repetitions of the same stimulus, either when presenting it multiple times to the same participant or presenting it to multiple participants. The naturally occurring variability places a boundary on the maximal obtainable correlation score between model and brain responses, which we will refer to as the noise ceiling. The estimate for the single-participant noise ceiling, computed as the average correlation between the response amplitudes of a single participant versus those averaged across all remaining participants (<xref ref-type="bibr" rid="bib41">Kriegeskorte et al., 2008</xref>), is very low for all three MEG components (type I: 0.236, type II: 0.112, N400m: 0.087), indicating a large amount of inter-participant variability. Therefore, we chose not to compare the models to single-participant data, but rather to the average across participants. The estimated noise ceilings for these average responses, as estimated through a procedure proposed by <xref ref-type="bibr" rid="bib45">Lage-Castellanos et al., 2019</xref>, are much higher (type I: 0.765, type II: 0.618, N400m: 0.567), and hence the comparison will be more informative.</p><p>The design process started with an evaluation of the standard VGG-11 model, trained only on ImageNet. The layer response profiles of this model showed a distinctive pattern in response to the different types of stimuli used in <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top row), where the stimuli containing a large amount of noise evoke a very high level of activation in the first convolution layer, which carries over into all other layers. Thus, this model simulated the response profile of the type I component well, and correlation with the type I component was at the noise ceiling (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, top), but it failed to simulate the response profiles of the type II and N400m components and correlated negatively with them. This is not surprising given that this model is illiterate.</p><p>Next, we trained the model on a training set containing a million examples of 250 possible written words (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, second row). After the model had learned to recognize written words (regardless of size, font family, and some rotation), the convolution layers still showed a response profile similar to the type I. However, the later linear layers now had a less extreme response to the noisy stimuli and also showed a dissociation between symbols and letters, making their response profiles more similar to that of the type II and N400m components of the evoked response. Nonetheless, in this version of the model, the response to noisy stimuli is still too large to be a good fit with the response profiles of the type II and N400m.</p><p>In vision research, having unit activations be noisy has been shown to make a system more robust against image perturbations, both in the brain and in CNNs (<xref ref-type="bibr" rid="bib17">Dapello et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Carandini et al., 1997</xref>). In our case, the addition of a small amount of Gaussian noise to all unit activations in the model reduced markedly the response of the later layers to the noisy stimuli, resulting in less activity to the noisy stimuli than the other stimulus types (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, third row), while the recognition accuracy of the model was not degraded. We found that in order for the model to display this behavior, both noisy unit activations and batch normalization were necessary (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). For this model, the response profiles of the first four convolution layers match that of the type I component, and the response profile of the last convolution layer is similar to that of the type II component in that both the noisy word stimuli and symbol stimuli evoked smaller responses than stimuli containing letters and all noiseless letter stimuli evoked comparable responses. However, this model does not contain any layers with response profiles matching that of the N400m. The N400m component of the evoked MEG response is modulated by the lexicality of the stimulus, as demonstrated through the contrast between consonant strings, proper words, and pseudowords. Whereas consonant strings evoked a smaller N400m response than proper words, pseudowords produced a response that was equal to (or even larger than) that to proper words (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). In this version of the model however, the response to pseudowords is less than that of proper words. This can be attributed to the relatively small vocabulary (250 words) of this model. The pseudowords used in the MEG experiment were selected to be orthographically similar to words in the extended vocabulary of a native Finnish speaker, but not necessarily similar to any of the 250 words in the vocabulary of the model. However, despite the failure to capture the N400m response profile, the fully connected layers of the model correlated much better with that component than the previous two models (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, third row).</p><p>Increasing the vocabulary size to 1000 words was enough to solve the problem regarding pseudowords and yielded a model where the response profiles of the early convolution layers matched that of the type I, those of the two fully connected layers matched that of the type II, and that of the output layer matched that of the N400m (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, fourth row). This did not cause a big increase in the correlation between the layers and the N400m (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, fourth row), showing that a better qualitative fit of response profiles does not necessarily translate into a better correlation score.</p><p>Further increasing the vocabulary size to 10,000 words initially had a detrimental effect on the model’s ability to simulate the N400m component, as the response to consonant strings was nearly as strong as that to words and pseudowords (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, fifth row). In order to reproduce the response profile of the N400m during all experimental conditions of the MEG experiment, a model needs to hit a sweet spot regarding specificity of activation to in-vocabulary versus out-of-vocabulary letter strings. We found that this specificity can be controlled by manipulating the frequency in which words are presented to the model during training. By scaling the number of examples of a word in the training set in accordance with the frequency of occurrence of the word in a large text corpus (<xref ref-type="bibr" rid="bib39">Kanerva et al., 2014</xref>), a model was obtained with a large vocabulary of 10,000 and the response profiles of the convolution layers matching that of the type I, response profiles of the fully connected layers matching that of the type II, and the response profiles of the output layer matching that of the N400m (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom row). These response profiles were stable across different random initializations when training the model (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). This manipulation of word frequency also led to a modest improvement in the correlation between the model and the type II and N400m responses. However, the main reason to include this variation was to achieve the desired response profiles while increasing vocabulary size. Compared to the model with a vocabulary of 1000 (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, third row), the model with a vocabulary of 10,000 (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, bottom row) achieved a substantially better correlation with the type II component, as well as a modest improvement in correlation with the N400m.</p></sec><sec id="s2-3"><title>The response profiles are dependent on the number of layers</title><p>Various variations in model architecture and training procedure were evaluated. We found that the number of layers had a large impact on the response patterns produced by the model (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The original VGG-11 architecture defines five convolution layers and three fully connected layers (including the output layer). Removing a convolution layer (<xref ref-type="fig" rid="fig4">Figure 4</xref>, top row), or removing one of the fully connected layers (<xref ref-type="fig" rid="fig4">Figure 4</xref>, second row), resulted in a model that did exhibit an enlarged response to noisy stimuli in the early layers that mimics the type I response. However, such models failed to show a sufficiently diminished response to noisy stimuli in the later layers, hence failing to produce responses that mimic the type II or N400m, a failure which also showed as low correlation scores.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Exploring changes in model architecture.</title><p>Model variations were constructed with a different number of convolution and fully connected layers to see what architecture produces activity that is the most like the three magnetoencephalography (MEG) components. (<bold>A</bold>) For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the MEG experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. Layers for which the response pattern was qualitatively similar to that of the type I, type II, or N400m component are outlined with a box of the appropriate color. A black line separates convolution layers from fully connected layers. (<bold>B</bold>) Correlation between the layers of each model (horizontal axis) and the three MEG evoked components (different curves). Layers for which the response profile (<bold>A</bold>) was judged to qualitatively correspond to one of the MEG components (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) are indicated as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig4-v1.tif"/></fig><p>Adding an additional convolution layer (<xref ref-type="fig" rid="fig4">Figure 4</xref>, third row) resulted in a model where none of the layer response profiles mimics that of the type II response. The type II response is characterized by a reduced response to both noise and symbols, but an equally large response to consonant strings, real and pseudo words. However, in the model with an additional convolution layer, the consonant strings evoked a reduced response already in the first fully connected layer, which is a feature of the N400m rather than the type II. These kind of subtleties in the response pattern, which are important for the qualitative analysis, generally did not show quantitatively in the correlation scores, as the fully connected layers in this model correlate as well with the type II response as models that did show a response pattern that mimics the type II.</p><p>Adding an additional fully connected layer (<xref ref-type="fig" rid="fig4">Figure 4</xref>, fourth row) resulted in a model with similar response profiles and correlation with the MEG components as the original VGG-11 architecture (<xref ref-type="fig" rid="fig4">Figure 4</xref>, bottom row) The N400m-like response profile is now observed in the third fully connected layer rather than the output layer. However, the decrease in response to consonant strings versus real and pseudo words, which is typical of the N400m, is less distinct than in the original VGG-11 architecture.</p><p>Some experimentation was done with the initialization of the model parameters. A model with either randomly initialized weights, or trained only on ImageNet, have response profiles in which each layer resembled that of the type I response (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, top rows). Training on word stimuli was required to simulate the type II and N400m components. While starting from a pretrained model on ImageNet reduced the number of epochs necessary for the model to reach &gt;99% accuracy, pretraining was not found to be strictly necessary in order to arrive at a good model that can simulate all three components (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, bottom rows).</p><p>Based on our qualitative and quantitative analysis, the model variant that performed best overall was the one that had the original VGG-11 architecture and was pre-initialized from earlier training on ImageNet, as depicted in the bottom rows of <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>.</p><p>Various other aspects of the model architecture were evaluated which ultimately did not lead to any improvements of the model. The correlations between the models and the MEG components are presented in <xref ref-type="fig" rid="fig5">Figure 5</xref> and the corresponding response profiles can be found in the supplemantary figures of <xref ref-type="fig" rid="fig5">Figure 5</xref>. The vocabulary of the final model (10,000) exceeds the number of units in its fully connected layers, which means that a bottleneck is created in which a sub-lexical representation is formed. The number of units in the fully connected layers, that is the width of the bottleneck, has some effect on the correlation between model and brain (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), and the amount of noise added to the unit activations less so (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). We already saw that the size of the vocabulary, that is the number of word-forms in the training data and number of units in the output layer of the model, had a large effect on the response profiles (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Having a large vocabulary is of course desirable from a functional point of view, but also modestly improves correlation between model and brain (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). For large vocabularies, we found it beneficial to apply frequency balancing of the training data, meaning that the number of times a word-form appears in the training data is scaled according to its frequency in a large text corpus. However, this cannot be a one-to-one scaling, since the most frequent words occur so much more often than other words that the training data would consist of mostly the top-10 most common words, with less common words only occurring once or not at all. Therefore, we decided to scale not by the frequency <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> directly, but by <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where 0 &lt; <italic>s</italic> &lt; 1, opting for <italic>s</italic> = 0.2 for the final model (<xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Impact of several hyperparameters on the correlation between model and brain.</title><p>This figure shows the correlation between the response profiles of the type I, type II, and N400m evoked magnetoencephalography (MEG) components and the three layers in the models whose response profiles best match each of the MEG components. Estimated noise ceilings for each of the MEG components are shown as vertical lines. Each panel shows the impact of tweaking a hyperparameter and has an illustration to indicate the property of the model affected by the hyperparameter. The settings of the hyperparameters chosen for the final modal are highlighted in gray. (<bold>A</bold>) The number of units in the two fully-connected layers, the bottleneck, is being modulated. The impact of the number of units in the two fully-connected layers, the bottleneck, is being modulated. (<bold>B</bold>) The impact of the amount of noise (σ<sub>noise</sub>) added to the activation of the units. (<bold>C</bold>) The impact of the number of words in the vocabulary of the model. (<bold>D</bold>) The impact of the amount of frequency balancing (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) in the training data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Impact of fully connected layer width on model response profiles.</title><p>Impact of fully connected layer width on model response profiles. As the vocabulary of the model (10,000) exceeds the number of units in the fully connected layers, a bottleneck is created in which a sub-lexical representation is formed. This figure shows how the response profiles of the model change as a function of the width of this bottleneck. For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn ashorizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Impact of amount of noise in the unit activations on model response profiles.</title><p>To accurately model the type I response profile, the model needs to filter out visual noise in its later layers. This figure explores the effect of the amount of noise added to the activation of the units on the response profiles of the model. For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Impact of vocabulary size on model response profiles.</title><p>This figures explores the impact of vocabulary size, that is the number of word-forms in the training data and number of units in the output layer of the model. For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig5-figsupp3-v1.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Impact of amount of frequency balancing on model response profiles.</title><p>For large vocabularies, we found it beneficial to apply frequency balancing of the training data, meaning that the number of times a word-form appears in the training data is scaled according to its frequency in a large text corpus. However, this cannot be a one-to-one scaling, since the most frequent words occur so much than other words that the training data would consist of mostly the top-10 most common words, with less common words only occurring once or not at all. Therefore, we decided to scale not by the frequency <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> directly, but by <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where 0 &lt; <italic>s</italic> &lt; 1. This figure explores the effect of the <italic>s</italic> parameter. For each layer, the response profile, that is the <italic>z</italic>-scored magnitude of ReLU activations in response to the same stimuli as used in the magnetoencephalography (MEG) experiment, is shown. Whenever there is a significant difference (<italic>t</italic>-test, p &lt; 0.05, FDR corrected) between two adjacent distributions, the corresponding difference in means is shown. On the right side of the figure, correlation is shown between the layers of each model and the MEG evoked components. Layers which were judged to correspond to one of the MEG components are shown as filled squares. Noise ceilings for the MEG components are drawn as horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig5-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Model–brain correlation is mostly driven by experimental conditions</title><p>Upon closer examination of the relationship between the response profiles of the final model and the MEG evoked components (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), we see that the correlation is mostly driven by differences in response strength between the experimental conditions. Within each stimulus type, MEG evoked component amplitude and model layer activation are not significantly uncorrelated (p &gt; 0.05 for all components), which is not surprising as the stimuli were designed to minimize within-condition variation. To verify that the three ECD groups capture the most important correlations between the brain activity and the model, we compared the model to cortex-wide MNE-dSPM source estimates (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The areas with maximum correlation appear around the locations of the ECDs and at a time that is close to the peak activation of the ECDs.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>A closer look at the relationship between the final model and magnetoencephalography (MEG) responses.</title><p>(<bold>A</bold>) A closer look at the relationship between the response profiles of the MEG responses and three layers of the model that qualitatively best capture those MEG responses. Kernel density distributions are shown at the borders. (<bold>B</bold>) Correlation between the MNE-dSPM source estimate and the model. Grand-average source estimates were obtained in response to each stimulus. The correlation map was obtained by correlating the activity at each source point with that for the chosen three layers of the model. The correlation map is shown at the time of peak correlation (within the time windows indicated in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Only positive correlations are shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Brainscore.</title><p>A visualization of the brainscores between two models and the single-participant, single-trial evoked responses, source localized using MNE-dSPM (MNE-Python). The first model shown (<bold>A</bold>) is an illiterate model, trained only on ImagetNet, which is the model shown in the first row of <xref ref-type="fig" rid="fig3">Figure 3</xref> in the main manuscript. The second model (<bold>B</bold>) is the final model, as shown in the last row of <xref ref-type="fig" rid="fig3">Figure 3</xref> in the main manuscript. Brainscores were computed by training a ridge regression model on the average evoked responses of 14 participants and computing the correlation between the predicted and true responses for the 1 left-out participant (SciKit-Learn). The shrinkage parameter of the regresion model was determined using inner leave-one-out cross validation. At the top are maps of the brainscores for three layers of the model at three time points and below is the time course of maximum brainscores (across all source points) for each layer of the model. Cluster permutation tests were used to determine clusters of brainscores that were significantly different from zero (clustering <italic>t</italic>-value threshold of 4, α threshold of 0.05 for determining significant clusters). The temporal extent of the significant clusters is indicated with horizontal lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig6-figsupp1-v1.tif"/></fig></fig-group><p>To test whether a multivariate analysis would yield additional insights, we trained a linear ridge regression model to predict brain activity given the activity within the layers of a model, what <xref ref-type="bibr" rid="bib78">Schrimpf et al., 2020</xref> refer to as BrainScore. BrainScores where computed for both the illiterate model, trained only on ImageNet (<xref ref-type="fig" rid="fig3">Figure 3</xref>, top row), and the final model (<xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom row). We found that for both models, every layer could be used to predict the brain activity along key locations along the ventral stream (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Furthermore, there was very little difference in BrainScore between the illiterate and the final model. Hence, this information-based metric was in this case not helpful for evaluating model–brain fit.</p></sec><sec id="s2-5"><title>CNNs can reproduce experimental results beyond the original MEG study</title><p>The experimental contrasts employed in the MEG experiment were designed to distinguish the type I, type II, and N400m components from one another. However, many more experimental contrasts have been used to study these components in the past. We explored the effect of some of these on our final model (<xref ref-type="fig" rid="fig7">Figure 7</xref>) in order to ascertain the extent to which the model can simulate the behavior of the three components under conditions extending beyond the original MEG data that guided its design.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Post hoc exploration of various experimental contrasts.</title><p>For each contrast, four sample stimuli are shown to demonstrate the effect of the manipulated stimulus property and below are the correlation between the manipulation and the amount of activity in each layer of the final model. For the experimental contrasts indicated with a number, one or more confounding factors were corrected for (partial correlation). Different colors indicate convolution layers (blue), fully connected layers (orange), and the output layer (green).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96217-fig7-v1.tif"/></fig><p>In <xref ref-type="bibr" rid="bib84">Tarkiainen et al., 1999</xref>, the difference between the type I and II component was highlighted by contrasting stimuli that contained only noise with stimuli that contained both noise and letters. Our model successfully replicates the finding that the amplitude of both the type I and II components increase with the amount of visual noise, except when there are letters present, in which case the amplitude of the type II component decreases with the amount of noise while that of the type I component is unaffected. This striking change in behavior occurs in the model when the layer type switches from convolution to fully connected linear. Furthermore, our model reproduces the finding by <xref ref-type="bibr" rid="bib84">Tarkiainen et al., 1999</xref> that the amplitudes of both the type I and II component increases when a stimulus contains more letters or symbols, however given two strings of equal length, the amplitude of the type II component is larger for the string containing letters versus a string containing symbols, while the amplitude of the type I component is similar for both strings.</p><p>Basic visual features such as font family and font size have a large effect on the activity in the initial convolution layers of the model, and less so on the activity in the later layers. This roughly corresponds to findings in the masked repetition priming EEG literature, where they find that early components such as the N/P150 are affected by such properties, whereas later components are not (<xref ref-type="bibr" rid="bib10">Chauncey et al., 2008</xref>; <xref ref-type="bibr" rid="bib25">Grainger and Holcomb, 2009</xref>; <xref ref-type="bibr" rid="bib31">Hauk and Pulvermüller, 2004</xref>). However, our results may not be directly comparable to that literature, because our model does not simulate repetition priming. For example, the fully connected linear layers in the model show a decrease in activity with increasing font size, which is an effect not found during repetition priming (<xref ref-type="bibr" rid="bib10">Chauncey et al., 2008</xref>), but was found in studies that manipulated the font size of words presented in isolation (<xref ref-type="bibr" rid="bib4">Bayer et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Schindler et al., 2018</xref>).</p><p>Using invasive recordings, <xref ref-type="bibr" rid="bib91">Woolnough et al., 2021</xref> found effects of letter and bigram frequency in the occipitotemporal cortex, where we find the type II component in MEG, but not in occipital cortex, where we find the type I component. This result has been shown in EEG studies as well (<xref ref-type="bibr" rid="bib48">Laszlo and Federmeier, 2014</xref>; <xref ref-type="bibr" rid="bib26">Grainger, 2018</xref>). Our model shows similar sensitivity to letter and bigram frequency in the later convolution and fully connected layers. However, we found that our model did not show any sensitivity to word frequency or orthographic neighborhood size, even though these are known to affect later components in EEG studies (<xref ref-type="bibr" rid="bib34">Holcomb et al., 2002</xref>; <xref ref-type="bibr" rid="bib16">Dambacher et al., 2006</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we demonstrated how training a CNN for visual word classification results in a model that can explain key feed-forward functional effects of the evoked MEG response during visual word recognition, provided some conditions to enhance biological realism are met. Necessary attributes of the CNN and its training diet were found to be that unit activations should be noisy, the vocabulary should be large enough, and for very large vocabularies, the frequency of occurrence of words in the training set should follow their frequency in the language environment. Following a neuroconnectionist approach (<xref ref-type="bibr" rid="bib22">Doerig et al., 2023</xref>), we started with an illiterate CNN, trained to perform image classification, and iteratively made changes to its architecture and training diet, until it could replicate the effects of manipulating low-level visual properties such as noise levels, fonts and sizes, to mid-level orthographic properties such as symbols versus letters, and to high-level lexical properties such as consonants strings versus pseudowords versus real words. Some of these contrasts explore what we consider to be ‘edge cases’ of visual word processing that explore what happens when the brain tries to recognize a word and fails. Symbol strings, consonant strings, and pseudowords do not occur often in our natural environment and we do not explicitly train ourselves to distinguish them from real words when we learn to read. Following this logic, we included only real words in the training set for the model, so that responses to the non-word stimuli were simulations of a computational process failing to properly recognize a word.</p><sec id="s3-1"><title>A computational process exhibiting type I, type II, and N400m-like effects</title><p>In this study, the computational process of visual word recognition was taken to be a transformation from a bitmap image containing a written word (with some variation in letter shapes, size, and rotation) to the identity of the word. Many network architectures can be found in the deep-learning literature that could by used to implement such a process. For this study, we chose the vgg architecture (<xref ref-type="fig" rid="fig2">Figure 2</xref>), where high accuracy is achieved by performing the same few operations many times, rather than many different operations or clever tricks. The operations performed by the network are: convolution, pooling, batch normalization, and linear transforms. Our rationale was to start with these operations, which already have links to the neuroscience of vision (<xref ref-type="bibr" rid="bib7">Carandini et al., 1997</xref>; <xref ref-type="bibr" rid="bib80">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib92">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib50">Lindsay, 2021</xref>) and only when they prove insufficient to explain a desired experimental effect, introduce others. This turned out to be unnecessary for explaining the results of <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>, but one can imagine the necessity of introducing recurrence (<xref ref-type="bibr" rid="bib43">Kubilius et al., 2019</xref>) or attention mechanisms (<xref ref-type="bibr" rid="bib88">Vaswani et al., 2017</xref>) in future extensions of the model into for example semantic processing.</p><p>The visual word recognition task itself did not prove particularly difficult for a CNN, as all variations of the network architecture and training diet that were tried resulted in a model that achieved near perfect accuracy. However, not all variations produced response profiles matching those of the evoked components (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This validates a criticism sometimes heard concerning whether deep learning models are brain-like, namely that the ability of a model to perform a task that once could only be done by a brain is not enough evidence that a model computes like a brain (<xref ref-type="bibr" rid="bib5">Bowers et al., 2022</xref>). Indeed, we found that task performance alone, that is the identification of the correct stimulus word, was a poor indicator for judging a model’s suitability as computational hypothesis for the brain. More compelling evidence was obtained by comparing the model’s internal state to measurements of brain activity. Two variations of the model, with respective vocabulary sizes of 1k and 10k words, implement the computational process in such a way that they produce amounts of activity within their layers that resemble the amplitudes of the type I, type II, and N400m components of the MEG evoked response. The model with the larger vocabulary was chosen to be the final iteration of the model and subjected to a more extensive range of experimental contrasts (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><p>The computational process implemented by the final model starts with a series of convolution-and-pooling operations. Individual convolution units have a restricted receptive field, which is gradually widened through pooling operations. The units in the first layer see a patch of <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> pixels and those in the final fifth convolution layer combine information from patches of <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>18</mml:mn><mml:mo>×</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> pixels. This setup enforces a hierarchical representation forming in those layers, from edges to line segments and corners to (parts of) letter shapes (<xref ref-type="bibr" rid="bib19">Dehaene et al., 2005</xref>, <xref ref-type="bibr" rid="bib20">Dehaene-Lambertz et al., 2018</xref>). Hence, increasing the visual complexity, that is adding more edges to an image by for example introducing visual noise or adding more symbols, will increase the activity of the convolution units. This behavior corresponds closely to that of the type I component, which is in line with the growing body of evidence of convolution-and-pooling being performed in visual cortex (<xref ref-type="bibr" rid="bib50">Lindsay, 2021</xref>; <xref ref-type="bibr" rid="bib80">Serre et al., 2007</xref>).</p><p>The kind of information selected for in the later layers of the model changes through the introduction of non-relevant input signals during training, here done through noisy unit activations. Without noisy activations, the task of the computational units is limited to finding features to distinguish one letter from another. With noisy activations, their task also entails isolating letter-shape information from non-relevant information. The property of not merely having a preference for, but actively seeking to isolate certain types of information is key for producing the response profiles of the type II and N400m components. When receptive field sizes were <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> pixels and larger (fourth and fifth layers), the convolution units showed a preference for letters over symbols and a preference for more frequently used letters. This orthographic tuning is also the hallmark of the type II component. However, while the amplitude of this component increases with increasing visual complexity through the addition of more symbols or letters to a stimulus (<xref ref-type="bibr" rid="bib84">Tarkiainen et al., 1999</xref>), an important aspect of the type II is that manipulations that reduce readability, such as the addition of visual noise, decrease its amplitude, even though the overall visual complexity of the stimulus is increased. It is in this aspect that the convolution units in our models are not necessarily a good fit for the type II response, since their tight receptive field strongly ties their activation to that of their counterparts in the previous layer. Increased activation in the early convolution layers tends to carry over, with the later convolution layers having only limited capabilities for filtering out noise. With small vocabularies, we did observe noise-filtering behavior in the fifth convolution layer, but less so when vocabulary size was increased from 250 to 1k and the effect was gone with a vocabulary of 10k.</p><p>In the model, convolution units are followed by pooling units, which serve the purpose of stratifying the response across changes in position, size, and rotation within the receptive field of the pooling unit. Hence, the effect of small differences in letter shape, such as the usage of different fonts, was only present in the early convolution layers, in line with findings in the EEG literature (<xref ref-type="bibr" rid="bib10">Chauncey et al., 2008</xref>; <xref ref-type="bibr" rid="bib25">Grainger and Holcomb, 2009</xref>; <xref ref-type="bibr" rid="bib31">Hauk and Pulvermüller, 2004</xref>). However, the ability of pooling units to stratify such differences depends on the size of their receptive field, which is determined by the number of convolution-and-pooling layers. As a consequence, the response profiles of the subsequent fully connected layers was also very sensitive to the number of convolution-and-pooling layers. The optimal number of such layers is likely dependent on the input size and pooling strategy. Given the VGG-11 design of doubling the receptive field after each layer, combined with an input size of <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>225</mml:mn><mml:mo>×</mml:mo><mml:mn>225</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> pixels, the optimal number of convolution-and-pooling layers for our model was five, or the model would struggle to produce response profiles mimicking those of the type II component in the subsequent fully connected layers (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>The fully connected layers pool information across the entire input image. Their large receptive fields make them more effective in isolating orthographic information from noise than the convolution layers (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and their response profile matches that of the type II component well, also when the model has a large vocabulary. A minimum of two fully connected layers was needed to achieve this in our case, and adding more fully connected layers would make them behave more like the N400m component (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Moreover, units in the fully connected layers can see multiple letters, making them sensitive not only to letter frequency, but also bigram frequency, allowing them to replicate these effects on the type II component (<xref ref-type="bibr" rid="bib91">Woolnough et al., 2021</xref>).</p><p>The fully connected linear layers in the model show a negative correlation with font size. While the N400 has been shown to be unaffected by font size during repetition priming (<xref ref-type="bibr" rid="bib10">Chauncey et al., 2008</xref>), it has been shown that in the absence of priming, larger font sizes decrease the evoked activity in the 300–500 ms window (<xref ref-type="bibr" rid="bib4">Bayer et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Schindler et al., 2018</xref>). Those studies refer to the activity within this time window, which seems to encompass the N400, as early posterior negativity (EPN). What possibly happens in the model is that an increase in font size causes an initial stronger activation in the first layers, due to more convolution units receiving input. This leads to a better signal-to-noise ratio (SNR) later on, as the noise added to the activation of the units remains constant while the amplitude of the input signal increases. A better SNR translates ultimately in less co-activation of units corresponding to orthographic neighbors in the final layers, hence to a decrease in overall layer activity.</p><p>The final fully connected layer, the output layer, represents the model’s notion of a mental lexicon: a collection of known word-forms, not yet linked to their semantic meaning (which we did not model at all). Since orthographic word-form and semantic meaning are generally only very loosely coupled (<xref ref-type="bibr" rid="bib56">Monaghan et al., 2014</xref>), the lexicon often appears as a separate unit in computational models (<xref ref-type="bibr" rid="bib90">Woollams, 2015</xref>). At face value, since we employed cross-entropy as loss function during training, the output layer simulates the lexicon following a localist structural approach, which harks back to the earliest models of visual word recognition (<xref ref-type="bibr" rid="bib57">Morton, 1969</xref>; <xref ref-type="bibr" rid="bib53">McClelland and Rumelhart, 1981</xref>). In the localist approach, each word is represented by a dedicated unit that should only be active when the sensory version of that word (in this study, a bitmap image containing the word as rendered text) is provided as input to the model, to the exclusion of all other words in the vocabulary. This is known in the machine learning literature as ‘one-hot’ encoding. The localist approach is often contrasted with the distributed approach, wherein each word is represented by a unique combination of multiple output units that respond to subword parts (e.g., n-grams, pairings of letters in the first and final position, etc.), which is a more efficient encoding scheme (<xref ref-type="bibr" rid="bib79">Seidenberg and McClelland, 1989</xref>). Both the localist and distributed approaches have their strengths and weaknesses when it comes to explaining experimental results, and the debate on how exactly the human lexicon is encoded in the brain is far from settled (<xref ref-type="bibr" rid="bib90">Woollams, 2015</xref>). Interestingly, because our model covers the entire process from pixel to word-form, its simulation of the lexicon is not strictly localist, but combines both localist and distributed approaches.</p><p>When the input image contains a word that is present in the model’s vocabulary, it is true that the corresponding word-specific unit in the output layer will have the highest amount of activation of all units in the output layer. However, given the connectionist nature of our model, all units tuned to orthographically similar words will co-activate to some degree. This inaccuracy is important when it comes to simulating the amplitude of the N400m component in response to pseudowords, which is as large as (and sometimes even larger than) the response to real words. In the model, a pseudoword does not have a dedicated output unit, but given a large enough vocabulary, there will be enough units that respond to orthographically similar words to produce a large amount of activity nonetheless. Crucially, letter strings that are not word-like, such as consonant strings, will not activate many units in the output layer, which is an important difference between the response profiles of the N400m and the type II. Furthermore, in our final model, the first two fully connected layers have fewer units (4096) than the number of words in the vocabulary (10,000), which produces a distributed representation, where one unit is involved in the encoding of many words. The response profiles of both the second fully connected layer (distributed representation) and the output layer (localist representation) resemble that of the N400m response. Hence, our model does not provide direct cause for preferring a localist or distributed approach when modeling the lexicon, but shows how both approaches interact when creating a model of the entire process of sensory input to lexicon: the localist approach can function as training objective and a distributed representation emerges in the hidden layers.</p></sec><sec id="s3-2"><title>On the importance of experimental contrasts and qualitative analysis of the model</title><p>When the same stimuli can be presented to both human participants and model, model–brain correspondence can be evaluated quantitatively through metrics such as correlation, linear regression (brainscore), and RSA. Although stimuli need not be designed to fall into a priori defined categories, it still pays off to have experimental contrasts in the experiment. For one, quantitative metrics are more meaningful when the variation in the data is due to multiple causes and no single cause (e.g., word length or visual complexity) dominates the variation. More importantly, as demonstrated by our results, quantitative metrics alone can give a distorted view of model–brain correspondence. For example, it is clear that the amount of visual complexity in a stimulus (e.g., visual noise, number of letters, font size) accounts for a large portion of the variation in the evoked responses (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) as well as all layers of the model (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Since even a CNN with randomly initialized weights will mimic the variation due to visual complexity (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, top row), one can expect a high correlation between model and brain whenever there are large differences in visual complexity between stimuli. In our case, the inclusion of stimuli that occur less frequently in our natural environment, such as symbol strings, consonant strings and pseudowords, matched in length with real words, were very informative causes of signal-variation beyond visual complexity, and were essential for the evaluation of the models’ simulation of the neurofunctionally relevant type II and N400m components. The contrast between noise-embedded words and noise-free stimuli sometimes caused a negative correlation between model and brain, which is a clear sign that the model was a poor fit. One should keep this in mind when using quantitative metrics that are insensitive to the directionality of the correlation, as they can hide such a clear deficit (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>Overall, we found that a qualitative evaluation of the response profiles was more helpful than correlation scores. Often, a deficit in the response profile of a layer that would cause a decrease in correlation on one condition would be masked by an increased correlation in another condition. A notable example is the necessity for frequency balancing the training data when building models with a vocabulary of 10,000. Going by correlation score alone, there does not seem to be much difference between the model trained with and without frequency balancing (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, fifth row versus bottom row). However, without frequency balancing, we found that the model did not show a response profile where consonant strings were distinguished from words and pseudowords (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, fifth row), which is an important behavioral trait that sets the N400m component apart from the type II component (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). This underlines the importance of the qualitative evaluation in this study, which was only possible because of a straightforward link between the activity simulated within a model to measurements obtained from the brain, combined with the presence of clear experimental conditions.</p><p>Since evoked activity is produced by large amounts of neurons firing at the same time (<xref ref-type="bibr" rid="bib29">Hämäläinen et al., 1993</xref>; <xref ref-type="bibr" rid="bib58">Murakami and Okada, 2006</xref>), we chose in this study to use the total amount of activity within the layers of the model as direct simulations of the amplitudes of the various components of the MEG response. This simulation could perhaps be improved through the use of more biologically realistic methods, such as neural mass models (<xref ref-type="bibr" rid="bib18">David et al., 2005</xref>) and dynamic causal modeling (<xref ref-type="bibr" rid="bib23">Friston et al., 2003</xref>).</p></sec><sec id="s3-3"><title>Limitations of the current model and the path forward</title><p>The VGG-11 architecture was originally designed to achieve high image classification accuracy on the ImageNet challenge (<xref ref-type="bibr" rid="bib82">Simonyan and Zisserman, 2015</xref>). Although we have introduced some modifications that make the model more biologically plausible, the final model is still incomplete in many ways as a complete model of brain function during reading.</p><p>An important limitation of the model is the lack of feed-back activity, which is an important hallmark of the reading process and is likely to already have an influence on early brain activity in visual cortex (<xref ref-type="bibr" rid="bib32">Heilbron et al., 2020</xref>). For one, this means that our model is not able to capture the effect of the previous stimulus on the current one, such as priming effects (<xref ref-type="bibr" rid="bib25">Grainger and Holcomb, 2009</xref>; <xref ref-type="bibr" rid="bib44">Kutas and Federmeier, 2011</xref>). But even when considering stimuli in isolation, there are feed-back effects that the model cannot simulate. While the design process of the model has been guided by the experimental contrasts of <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>, we further evaluated the final modal on several more contrasts (<xref ref-type="fig" rid="fig7">Figure 7</xref>). For some contrasts, it did not succeed in reproducing the experimental effects as typically found in EEG/MEG studies. The fact that our feed-forward model failed to simulate the effects of word frequency on the N400m, even after frequency balancing of the training data, suggests that this effect may be largely driven by feed-back activity, as for example modeled by <xref ref-type="bibr" rid="bib61">Nour Eddine et al., 2024</xref>.</p><p>Another limitation of the current model is the lack of an explicit mapping from the units inside its layers to specific locations in the brain at specific times. The temporal ordering of the components is simulated correctly, with the response profile matching that of the type I occurring in layers that precede those matching the type II, followed by the layers corresponding to the N400m. Furthermore, every component is best modeled by a different type of layer, with the type I best described by convolution-and-pooling, the type II by fully connected linear layers, and the N400m by a one-hot encoded layer. However, there is no clear relationship between the number of layers the signal needs to traverse in the model to the processing time in the brain. Even if one considers that the operations performed by the initial two convolution layers happen in the retina rather than the brain, the signal needs to propagate through three more convolution layers to reach the point where it matches the type II component at 140–200 ms, but only through one more additional layer to reach the point where it starts to match the N400m component at 300–500 ms. Still, cutting down on the number of times convolution is performed in the model seems to make it unable to achieve the desired suppression of noise (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This raises the question what the brain is doing during the time between the type II and N400m component that seems to take so long. It is possible that the timings of the MEG components are not indicative solely of when the feed-forward signal first reaches a certain location, but are rather dictated by the resolution of feed-forward and feed-back signals (<xref ref-type="bibr" rid="bib61">Nour Eddine et al., 2024</xref>).</p><p>In this paper we have restricted our simulations to feed-forward processes. Now, the way is open to incorporate convolution-and-pooling principles in models of reading that simulate feed-back processes as well, which should allow the model to capture more nuance in the type II and N400m components, as well as extend the simulation to encompass a realistic semantic representation. A promising way forward may be to use a network architecture like cornet (<xref ref-type="bibr" rid="bib43">Kubilius et al., 2019</xref>) that performs convolution multiple times in a recurrent fashion, yet simultaneously propagates activity forward after each pass. The introduction of recursion into the model will furthermore align it better with traditional-style models, since it can cause a model to exhibit attractor behavior (<xref ref-type="bibr" rid="bib55">McLeod et al., 2000</xref>), which will be especially important when extending the model into the semantic domain. Furthermore, convolution-and-pooling has recently been explored in the domain of predictive coding models (<xref ref-type="bibr" rid="bib62">Ororbia and Mali, 2023</xref>), a type of model that seems particularly well suited to model feed-back processes during reading (<xref ref-type="bibr" rid="bib24">Gagl et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Heilbron et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Nour Eddine et al., 2024</xref>).</p><p>Despite its limitations, our model is an important milestone for computational models of reading that leverages deep learning techniques to encompass the entire computational process starting from raw pixels values to representations of word-forms in the mental lexicon. The overall goal is to work towards models that can reproduce the dynamics observed in brain activity observed in the large number of neuroimaging experiments with human volunteers that have been performed over the last few decades. To achieve this, models need to be able to operate on more realistic inputs than a collection of predefined lines or letter banks (<xref ref-type="bibr" rid="bib53">McClelland and Rumelhart, 1981</xref>; <xref ref-type="bibr" rid="bib12">Coltheart et al., 2001</xref>; <xref ref-type="bibr" rid="bib47">Laszlo and Armstrong, 2014</xref>; <xref ref-type="bibr" rid="bib32">Heilbron et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Nour Eddine et al., 2024</xref>). We have shown that even without feed-back connections, a CNN can simulate the behavior of three important MEG evoked components across a range of experimental conditions, but only if unit activations are noisy and the frequency of occurrence of words in the training dataset mimics their frequency of use in actual language.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>MEG study design and data analysis</title><p>The MEG data that had been collected by <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> was re-analyzed for the current study using MNE-python (<xref ref-type="bibr" rid="bib27">Gramfort et al., 2013</xref>) and FreeSurfer (<xref ref-type="bibr" rid="bib14">Dale et al., 1999</xref>). The study received ethical approval from the Ethics Committee of the Helsinki and Uusimaa Hospital District and informed written consent, and consent to publish, was obtained from each participant. We refer to the original publication for additional details on the study protocol and data collection process.</p><p>Simultaneous MEG and EEG was recorded from 15 Finnish participants (7 females; age 20–49 years, mean 27 years, all included in the analysis). The stimuli consisted of visually presented Finnish words, pseudowords (pronounceable but meaningless), consonant strings (random consonants), symbol strings (randomly formed from 10 possible shapes), and words embedded in high-frequency visual noise (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each stimulus category contained 112 different stimuli and each stimulus contained 7–8 letters or symbols. The stimuli were presented sequentially in a block design. Each block started with a random period of rest (0–600 ms), followed by seven stimuli of the same category. Each stimulus was shown for 300 ms with an inter-stimulus interval of 1500 ms of gray background. In addition to the five stimulus conditions (16 blocks each), there were 16 blocks of rest (12 s). An additional five target blocks were added, in which one stimulus appeared twice in a row, that the participants were asked to detect and respond to with a button press. The target blocks were not included in the analysis. The same paradigm was applied in an fMRI–EEG recording of the same participants as well (data not used here).</p><p>The data were recorded with a Vector View device (Elekta Neuromag, now MEGIN Oy, Finland) in a magnetically shielded room. In addition, vertical and horizontal electro-oculography (EOG) were recorded. All MEG data analysis was performed using the MNE-Python software (<xref ref-type="bibr" rid="bib27">Gramfort et al., 2013</xref>). The signals were bandpass filtered at 0.03–200 Hz and digitized at 600 Hz. For analysis, the data were further processed using the maxfilter software (MEGIN Oy, Finland) and bandpass filtered at 0.1–40 Hz. Eye-movement and heart-beat artifacts were reduced using independant component analysis (ICA): the signal was high-passed at 1 Hz, and components with correlations larger than 5 standard deviations with any of the EOG channels or an estimated electro-cardiogram (based on the magnetometers without the maxfilter applied) were removed (3–7 out of 86–108 components). Epochs were cut −200 to 1000 ms relative to the onset of the stimulus and baseline-corrected using the pre-stimulus interval. Participant-specific noise covariance matrices of the MEG sensors were estimated using the pre-stimulus interval.</p><p>Source estimation of the sensor-level signals was performed using two methods. The first, MNE-dSPM (<xref ref-type="bibr" rid="bib15">Dale et al., 2000</xref>), yields a comprehensive map of where activity can be found, which is used in this study mostly to provide context for a deeper investigation into three peaks of activity that can be observed along the ventral stream. The second, guided ECD modeling (<xref ref-type="bibr" rid="bib75">Salmelin, 2010</xref>), summarizes the high-dimensional data as a sparse set of ECDs, each one capturing an isolated spatiotemporal component, allowing us to study selected components in more detail. The advantage of this sparse method over distributed source estimation techniques such as the MNE or beamformers, is that in cases like <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>, a single ECD may conveniently capture an isolated component of the evoked response. Furthermore, this sparsity makes it straightforward to take individual differences regarding the cortical origin of the evoked component into account, since the location and orientation of the ECDs were fitted to optimally explain the data of each individual. For each individual, the matching between ECDs and the type I, type II, and N400m responses was performed based on their approximate location, timing of peak activity, and behavior across the different stimulus types. The groups do not necessarily contain an ECD for each of the 15 participants, as some participants did not have an ECD to contribute that matched well enough in position and/or timing. The number of participants contributing an ECD to each group were: type I: 14, type II: 14, and N400m: 15.</p><p>For both ECD and MNE-dSPM source estimation, three-layer boundary element method (BEM) forward models were used. These models were based on T1-weighted anatomical images that were acquired using a 3T Signa excite MRI scanner. FreeSurfer (<xref ref-type="bibr" rid="bib14">Dale et al., 1999</xref>) was used to obtain the surface reconstructions required to compute the BEM models. <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> defines three groups of ECDs in the left-hemisphere, associated with the type I, type II, and N400m responses. The locations and orientations of the ECDs in these groups were re-used in the current study. For each individual epoch, the signal at each of the three ECDs and a cortex-wide MNE-dSPM source estimate was computed using the noise covariance matrices and BEM forward models. Finally, the locations of the ECDs and MNE-dSPM source points were morphed to FreeSurfer’s template brain.</p></sec><sec id="s4-2"><title>Computational models</title><p>The models of the computational process underlying the brain activity observed during the MEG experiment used a VGG-11 (<xref ref-type="bibr" rid="bib83">Szegedy et al., 2015</xref>) network architecture, pretrained on ImageNet (<xref ref-type="bibr" rid="bib73">Russakovsky et al., 2015</xref>), as defined in the TorchVision (<xref ref-type="bibr" rid="bib52">Marcel and Rodriguez, 2010</xref>) module of the PyTorch (<xref ref-type="bibr" rid="bib64">Paszke et al., 2019</xref>) package. This architecture consists of five convolution layers (with the final three performing convolution twice), followed by two fully connected linear layers, terminating in a fully connected linear output layer (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Each individual convolution step is followed by batch normalization (<xref ref-type="bibr" rid="bib37">Ioffe and Szegedy, 2015</xref>) and max-pooling. Every unit, including the output units, used the following non-linear activation function:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>noisyReLU</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mtext>noise</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>N</italic> denotes Gaussian distributed random noise with zero mean and standard deviation <italic>σ</italic><sub>noise</sub>. For version of the model with noisy activations <italic>σ</italic><sub>noise</sub> = 0.1, and for versions without <italic>σ</italic><sub>noise</sub> = 0.</p><p>The model was trained to perform visual word recognition using a training set of approximately 1,000,000 images, where each image depicted a word, rendered in varying fonts, sizes, and rotations (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). An independent test set containing 100,000 images was used to track the performance of the model during training. None of the images in the training or test set were an exact duplicate of the images used as stimuli in the MEG experiment. Therefore, the stimulus set can be treated as an independent validation set. The task for the model was to identify the correct word (regardless of the font, size, and rotation used to render the text) by setting the corresponding unit in the output layer to a high value.</p><p>Different versions of the model were created using vocabulary sizes of 250, 1000, and 10,000 Finnish words. The vocabulary was compiled using the 112 words used in the MEG, extended with the most frequently used Finnish words, as determined by <xref ref-type="bibr" rid="bib39">Kanerva et al., 2014</xref>, having a length of 3–9, excluding proper names and using their lemma form. No symbol strings, consonant strings, nor pseudowords were present in the training set.</p><p>For the training sets that do not take word frequency into account, each word was rendered 100 times in various fonts, sizes, and rotations. When word frequency was taken into account, the number of times each word occurred in the training set c was scaled by its frequency <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in a large Finnish text corpus (<xref ref-type="bibr" rid="bib39">Kanerva et al., 2014</xref>) in the following manner:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mi>f</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mfrac><mml:mi>r</mml:mi><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">r</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the frequency of the least commonly occurring word in the training vocabulary, 0 ≤ <italic>s</italic> ≤ 1 is the amount of frequency balancing to apply, <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the frequency ratio, <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">r</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the total frequency ratio of all words in the training vocabulary, and <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the desired number of items in the training set, in our case 1,000,000. The actual number of items in the training set will deviate a little from <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, as the floor operation causes <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">c</mml:mtext></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The number of times a word occurred in the test set was always 10.</p><p>Images were assembled by overlaying the rendered text onto an image of visual noise. Visual noise was rendered by randomly selecting a gray-scale value (10–100%) for each pixel. Next, if the training image contained text, the word was rendered in uppercase letters, using a randomly chosen font (15 possible fonts), font size (14 pt to 32 pt), and rotation (–20° to 20°). The list of fonts consisted of: Ubuntu Mono, Courier, Luxi Mono Regular, Lucida Console, Lekton, Dejavu Sans Mono, Times New Roman, Arial, Arial Black, Verdana, Comic Sans MS, Georgia, Liberation Serif, Impact, and Roboto Condensed.</p><p>For every variation of the model, the training procedure was the same. The training objective was to minimize the cross-entropy loss between the model output and a one-hot encoded vector of a length equal to the vocabulary size, indicating the target word. Optimization of the model’s parameters was performed using stochastic gradient descend with an initial learning rate of 0.01, which was reduced to 0.001 after 10 epochs (an ‘epoch’ in this context refers to one sequence of all 1,000,000 training images), and a momentum of 0.9. During training, the noisy-ReLU activation function was temporarily disabled for the output units, and re-activated during evaluation and normal operation of the model. In total, training was performed for 20 epochs, by which point the performance on the test set, for all variations of the model, had plateaued at around 99.6%.</p></sec><sec id="s4-3"><title>Comparison between model and MEG data</title><p>To compare the model with the MEG responses, statistical summaries were made of the high-dimensional data, that we refer to as ‘response profiles’, that could be compared either qualitatively by examining the pattern of activation across stimulus types, and quantitatively by computing Pearson correlation between them.</p><p>To construct the response profiles for a model, it was applied to the bitmap images that were used during the MEG experiment (padded to be <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> pixels with 50% gray-scale pixels). The activity at each layer of the model was quantified by recording the ReLU activation right before the maxpool operation, and computing the <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm across the units. This yielded, for each stimulus image, a single number for each layer in the model representing the amount of activity within that layer. For each layer, the response profile was z-scored across the stimulus images.</p><p>To construct the response profiles for the type I, type II, and N400m components, the neural activity represented by the corresponding ECD was quantified by taking the mean of its timecourse during an appropriate time window (indicated in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, 64–115 ms for the type I, 140–200 ms for the type II, and 300–400 ms for the N400m, after stimulus onset). This yielded for each stimulus image a single number that indicates the amount of signal at the ECD. Collected across all stimuli, these numbers form the response pattern of each ECD. The response patterns were first <italic>z</italic>-scored across the stimulus images independently for each participant and later averaged across participants for the grand-average analysis.</p><p>The MNE-dSPM source estimates used 4000 source points, distributed evenly across the cortex. The response profile at each source point was computed by taking the maximum signal value of the source point’s timecourse within the same time window as the ECD analysis. This yielded for each stimulus image, a single number for each source point for each participant, indicating the amount of signal at the source point. The response patterns were first <italic>z</italic>-scored across the stimulus images independently for each participant and later averaged across participants during the grand-average analysis.</p></sec><sec id="s4-4"><title>Statistics</title><p>The comparisons between response profiles obtained from the models and the MEG data were performed through Pearson correlation (<xref ref-type="fig" rid="fig3">Figures 3B</xref>, <xref ref-type="fig" rid="fig4">4B</xref>). In cases where distributions were tested for a significant difference in mean (<xref ref-type="fig" rid="fig3">Figures 3A</xref> and <xref ref-type="fig" rid="fig4">4A</xref>), this was done through two-tailed paired <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>-tests, as implemented in SciPy (<xref ref-type="bibr" rid="bib89">Virtanen et al., 2020</xref>). In <xref ref-type="fig" rid="fig1">Figure 1D</xref>, multiple participants contribute to each data point in the distributions, hence a linear mixed effects model was used to compare distributions, with participants modeled as random effect (both slope and intercept), as implemented in the Julia MixedModels package (<xref ref-type="bibr" rid="bib3">Bates et al., 2023</xref>).</p><p>The noise ceilings, indicating the maximum obtainable correlation between the response profile of a model and those of the evoked components (<xref ref-type="fig" rid="fig3">Figures 3B</xref> and <xref ref-type="fig" rid="fig4">4B</xref>), were estimated following a method proposed by <xref ref-type="bibr" rid="bib45">Lage-Castellanos et al., 2019</xref>, which is based on the between-participant variation:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the standard deviation (across the stimuli) of the grand-average response profile, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean (across the stimuli) variation across the participant-specific response profiles, and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of participants.</p></sec><sec id="s4-5"><title>Post hoc experimental contrasts</title><p>The final model was not only tested with the stimuli of <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>, but also sets of stimuli designed with additional experimental contrasts (<xref ref-type="fig" rid="fig7">Figure 7</xref>). For each contrast, a different set of stimuli was generated. Unless mentioned otherwise, stimuli were text strings rendered on a gray background in the Arial font at a fontsize of 23 pixels.</p><sec id="s4-5-1"><title>Pure noise</title><p>1000 images were generated containing a gray background, which was blended with Gaussian noise with varying levels between 0% and 100%, with no rendered text.</p></sec><sec id="s4-5-2"><title>Noisy word</title><p>All words of length 8 were selected from the original 10k vocabulary and rendered. They were blended with Gaussian noise at random levels between 0% and 50%.</p></sec><sec id="s4-5-3"><title>Num. letters</title><p>All words in the 10k vocabulary were rendered.</p></sec><sec id="s4-5-4"><title>Num. symbols</title><p>For each word in the 10k vocabulary, a symbol string of matching length was generated using the unicode characters ‘square’, ‘circle’, ‘triangle up’, ‘triangle down’, ‘diamond’, and rendered.</p></sec><sec id="s4-5-5"><title>Word versus symbols</title><p>The stimulus lists used for the ‘Num. letters’ and ‘Num. symbols’ contrasts were combined. For this contrast, the model response profile was correlated with a vector containing zeros indicating symbols and ones indicating words.</p></sec><sec id="s4-5-6"><title>Font family</title><p>All words in the 10k vocabulary were rendered in both the Impact and Comic Sans fonts. For this contrast, the model response profile was correlated with a vector containing zeros indicating stimuli rendered in the Impact font, and ones indicating stimuli rendered in the Comic Sans font.</p></sec><sec id="s4-5-7"><title>Font size</title><p>All words of length 8 were selected from the original 10k vocabulary. Each selected word was rendered at 8 fontsizes ranging from 14 to 32 pixels.</p></sec><sec id="s4-5-8"><title>Letter freq</title><p>10,000 random letter strings of length 8 were generated and rendered. In this contrast, the model response profile was correlated with the total letter frequency of each random letter string in the 10k vocabulary.</p></sec><sec id="s4-5-9"><title>Bigram freq</title><p>10,000 strings consisting of 4 bigrams were generated and rendered. Since there is generally a strong correlation between letter and bigram frequency, the random bigrams were selected in a fashion designed to minimize this confounding factor. Bigrams were randomly selected from a set of 50 common bigrams (freq ≥ 30,000) with the lowest letter frequency, and a set of 50 uncommon bigrams (freq &lt; 10,000) with the highest letter frequency. Bigram and letter frequencies were computed on the 10k vocabulary.</p></sec><sec id="s4-5-10"><title>Word freq</title><p>All words in the 10k vocabulary were rendered. The word frequency was computed as the square root of the number of times the word occured in a large corpus of Finnish text (<xref ref-type="bibr" rid="bib39">Kanerva et al., 2014</xref>).</p></sec><sec id="s4-5-11"><title>Orth. neighb</title><p>All words in the 10k vocabulary were rendered. The orthographic neighborhood size was computed as the number of words in the 10k vocabulary with a Levenshtein distance of 1 to the target word.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Visualization</p></fn><fn fn-type="con" id="con4"><p>Investigation, Visualization</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The MEG data that had been collected by Vartiainen et al. (2011) was re-analyzed for the current study. The original study received ethical approval from the Ethics Committee of the Helsinki and Uusimaa Hospital District and informed written consent, and consent to publish, was obtained from each participant.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-96217-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and materials created during this study are publically available. The programming code for training the models and reproducing the results of this study and generating all the figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/wmvanvliet/viswordrec-baseline">https://github.com/wmvanvliet/viswordrec-baseline</ext-link> (copy archived at <xref ref-type="bibr" rid="bib86">van Vliet, 2025</xref>; license: BSD-3-clause). A public data package containing all models, training sets, and MEG derivatives necessary for performing the model–brain comparison and is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/nu2ep">https://osf.io/nu2ep</ext-link>. For the comparison between model and brain, MEG data was re-used from a previous study (<xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref>) and the informed consent obtained from the participants in that study prohibits sharing of personal data, even after removal of direct identifiers, hence only MEG data that has been aggregated across participants is made publically available. This does not have a big impact regarding the reproducibility of the present study, as the only parts relying on an individual's data are the computation of the noise ceilings, the results of which are included in the public data package, and the locations of the dipoles on the cortex, which are solely used for illustrative purposes in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The reuse of the personal data collected in <xref ref-type="bibr" rid="bib87">Vartiainen et al., 2011</xref> for other research purposes requires a new ethical review, and such requests should be addressed to Riitta Salmelin (riitta.salmelin@aalto.fi).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>van Vliet</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Convolutional networks can model the functional modulation of MEG responses during reading</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/NU2EP</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We acknowledge the computational resources provided by the Aalto Science-IT project. The funders had no role in study design, data collection, and analysis, decision to publish, or preparation of the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title><source>eLife</source><volume>9</volume><elocation-id>e54846</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54846</pub-id><pub-id pub-id-type="pmid">32369017</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Interplay between computational models and cognitive electrophysiology in visual word recognition</article-title><source>Brain Research Reviews</source><volume>53</volume><fpage>98</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.brainresrev.2006.07.002</pub-id><pub-id pub-id-type="pmid">16905196</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Alday</surname><given-names>P</given-names></name><name><surname>Kleinschmidt</surname><given-names>D</given-names></name><name><surname>José Bayoán Santiago Calderón</surname><given-names>P</given-names></name><name><surname>Zhan</surname><given-names>L</given-names></name><name><surname>Noack</surname><given-names>A</given-names></name><name><surname>Bouchet-Valat</surname><given-names>M</given-names></name><name><surname>Arslan</surname><given-names>A</given-names></name><name><surname>Kelman</surname><given-names>T</given-names></name><name><surname>Baldassari</surname><given-names>A</given-names></name><name><surname>Ehinger</surname><given-names>B</given-names></name><name><surname>Karrasch</surname><given-names>D</given-names></name><name><surname>Saba</surname><given-names>E</given-names></name><name><surname>Quinn</surname><given-names>J</given-names></name><name><surname>Hatherly</surname><given-names>M</given-names></name><name><surname>Piibeleht</surname><given-names>M</given-names></name><name><surname>Mogensen</surname><given-names>PK</given-names></name><name><surname>Babayan</surname><given-names>S</given-names></name><name><surname>Holy</surname><given-names>T</given-names></name><name><surname>Gagnon</surname><given-names>YL</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>JuliaStats/mixedmodels.jl</data-title><version designator="v4.14.1">v4.14.1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7979497">https://doi.org/10.5281/zenodo.7979497</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayer</surname><given-names>M</given-names></name><name><surname>Sommer</surname><given-names>W</given-names></name><name><surname>Schacht</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Font size matters--emotion and attention in cortical responses to written words</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e36042</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0036042</pub-id><pub-id pub-id-type="pmid">22590518</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowers</surname><given-names>JS</given-names></name><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>Dujmović</surname><given-names>M</given-names></name><name><surname>Llera Montero</surname><given-names>M</given-names></name><name><surname>Tsvetkov</surname><given-names>C</given-names></name><name><surname>Biscione</surname><given-names>V</given-names></name><name><surname>Puebla</surname><given-names>G</given-names></name><name><surname>Adolfi</surname><given-names>F</given-names></name><name><surname>Hummel</surname><given-names>JE</given-names></name><name><surname>Heaton</surname><given-names>RF</given-names></name><name><surname>Evans</surname><given-names>BD</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name><name><surname>Blything</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep problems with neural network models of human vision</article-title><source>The Behavioral and Brain Sciences</source><volume>46</volume><fpage>1</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1017/S0140525X22002813</pub-id><pub-id pub-id-type="pmid">36453586</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brem</surname><given-names>S</given-names></name><name><surname>Halder</surname><given-names>P</given-names></name><name><surname>Bucher</surname><given-names>K</given-names></name><name><surname>Summers</surname><given-names>P</given-names></name><name><surname>Martin</surname><given-names>E</given-names></name><name><surname>Brandeis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Tuning of the visual word processing system: Distinct developmental ERP and fMRI effects</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>1833</fpage><lpage>1844</lpage><pub-id pub-id-type="doi">10.1002/hbm.20751</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Linearity and normalization in simple cells of the macaque primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>8621</fpage><lpage>8644</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-21-08621.1997</pub-id><pub-id pub-id-type="pmid">9334433</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carreiras</surname><given-names>M</given-names></name><name><surname>Armstrong</surname><given-names>BC</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Frost</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The what, when, where, and how of visual word recognition</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>90</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.11.005</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Brains and algorithms partially converge in natural language processing</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>30361</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-03036-1</pub-id><pub-id pub-id-type="pmid">35173264</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chauncey</surname><given-names>K</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of stimulus font and size on masked repetition priming: An event-related potentials (ERP) investigation</article-title><source>Language and Cognitive Processes</source><volume>23</volume><fpage>183</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1080/01690960701579839</pub-id><pub-id pub-id-type="pmid">19590754</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Specialization within the ventral stream: the case for the visual word form area</article-title><source>NeuroImage</source><volume>22</volume><fpage>466</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.12.049</pub-id><pub-id pub-id-type="pmid">15110040</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coltheart</surname><given-names>M</given-names></name><name><surname>Rastle</surname><given-names>K</given-names></name><name><surname>Perry</surname><given-names>C</given-names></name><name><surname>Langdon</surname><given-names>R</given-names></name><name><surname>Ziegler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>DRC: A dual route cascaded model of visual word recognition and reading aloud</article-title><source>Psychological Review</source><volume>108</volume><fpage>204</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.108.1.204</pub-id><pub-id pub-id-type="pmid">11212628</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Le</surname><given-names>QA</given-names></name><name><surname>Tan</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>CoAtNet: Marrying convolution and attention for all data sizes</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2106.04803</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Liu</surname><given-names>AK</given-names></name><name><surname>Fischl</surname><given-names>BR</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Lewine</surname><given-names>JD</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title><source>Neuron</source><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81138-1</pub-id><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dambacher</surname><given-names>M</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Hofmann</surname><given-names>M</given-names></name><name><surname>Jacobs</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Frequency and predictability effects on event-related potentials during reading</article-title><source>Brain Research</source><volume>1084</volume><fpage>89</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.02.010</pub-id><pub-id pub-id-type="pmid">16545344</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dapello</surname><given-names>J</given-names></name><name><surname>Marques</surname><given-names>T</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simulating a primary visual cortex at the front of CNNs improves robustness to image perturbations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.16.154542</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>O</given-names></name><name><surname>Harrison</surname><given-names>L</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Modelling event-related responses in the brain</article-title><source>NeuroImage</source><volume>25</volume><fpage>756</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.030</pub-id><pub-id pub-id-type="pmid">15808977</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Vinckier</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The neural code for written words: A proposal</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>335</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.05.004</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Monzalvo</surname><given-names>K</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The emergence of the visual word form: Longitudinal evolution of category-specific ventral visual areas during reading acquisition</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004103</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004103</pub-id><pub-id pub-id-type="pmid">29509766</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integrated deep visual and semantic attractor neural networks predict fMRI pattern-information along the ventral object processing pathway</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>10636</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-28865-1</pub-id><pub-id pub-id-type="pmid">30006530</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>RP</given-names></name><name><surname>Seeliger</surname><given-names>K</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Ismael</surname><given-names>J</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The neuroconnectionist research programme</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>431</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00705-w</pub-id><pub-id pub-id-type="pmid">37253949</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Harrison</surname><given-names>L</given-names></name><name><surname>Penny</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dynamic causal modelling</article-title><source>NeuroImage</source><volume>19</volume><fpage>1273</fpage><lpage>1302</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(03)00202-7</pub-id><pub-id pub-id-type="pmid">12948688</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagl</surname><given-names>B</given-names></name><name><surname>Sassenhagen</surname><given-names>J</given-names></name><name><surname>Haan</surname><given-names>S</given-names></name><name><surname>Gregorova</surname><given-names>K</given-names></name><name><surname>Richlan</surname><given-names>F</given-names></name><name><surname>Fiebach</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An orthographic prediction error as the basis for efficient visual word recognition</article-title><source>NeuroImage</source><volume>214</volume><elocation-id>116727</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116727</pub-id><pub-id pub-id-type="pmid">32173410</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Watching the word go by: on the time-course of component processes in visual word recognition</article-title><source>Language and Linguistics Compass</source><volume>3</volume><fpage>128</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1111/j.1749-818X.2008.00121.x</pub-id><pub-id pub-id-type="pmid">19750025</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Orthographic processing: A ‘mid-level’ vision of reading: The 44th Sir Frederic Bartlett lecture</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>71</volume><fpage>335</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1080/17470218.2017.1314515</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halgren</surname><given-names>E</given-names></name><name><surname>Dhond</surname><given-names>RP</given-names></name><name><surname>Christensen</surname><given-names>N</given-names></name><name><surname>Van Petten</surname><given-names>C</given-names></name><name><surname>Marinkovic</surname><given-names>K</given-names></name><name><surname>Lewine</surname><given-names>JD</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>N400-like magnetoencephalography responses modulated by semantic context, word frequency, and lexical class in sentences</article-title><source>NeuroImage</source><volume>17</volume><fpage>1101</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1268</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hämäläinen</surname><given-names>M</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name><name><surname>Knuutila</surname><given-names>J</given-names></name><name><surname>Lounasmaa</surname><given-names>OV</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Magnetoencephalography—theory, instrumentation, and applications to noninvasive studies of the working human brain</article-title><source>Reviews of Modern Physics</source><volume>65</volume><fpage>413</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1103/RevModPhys.65.413</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannagan</surname><given-names>T</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Emergence of a compositional neural code for written words: Recycling of a convolutional neural network for reading</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2104779118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2104779118</pub-id><pub-id pub-id-type="pmid">34750255</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Effects of word length and frequency on the human event-related potential</article-title><source>Clinical Neurophysiology</source><volume>115</volume><fpage>1090</fpage><lpage>1103</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2003.12.020</pub-id><pub-id pub-id-type="pmid">15066535</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Richter</surname><given-names>D</given-names></name><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Word contexts enhance the neural representation of individual letters in early visual cortex</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>321</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13996-4</pub-id><pub-id pub-id-type="pmid">31949153</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helenius</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Distinct time courses of word and context comprehension in the left temporal cortex</article-title><source>Brain</source><volume>121</volume><fpage>1133</fpage><lpage>1142</lpage><pub-id pub-id-type="doi">10.1093/brain/121.6.1133</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holcomb</surname><given-names>PJ</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>O’Rourke</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>An Electrophysiological study of the effects of orthographic neighborhood size on printed word perception</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>938</fpage><lpage>950</lpage><pub-id pub-id-type="doi">10.1162/089892902760191153</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id><pub-id pub-id-type="pmid">23259955</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>de Heer</surname><given-names>WA</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><volume>532</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title><conf-name>Proceedings of the 32nd International Conference on International Conference on Machine Learning</conf-name><fpage>448</fpage><lpage>456</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jobard</surname><given-names>G</given-names></name><name><surname>Crivello</surname><given-names>F</given-names></name><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Evaluation of the dual route theory of reading: a metanalysis of 35 neuroimaging studies</article-title><source>NeuroImage</source><volume>20</volume><fpage>693</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00343-4</pub-id><pub-id pub-id-type="pmid">14568445</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanerva</surname><given-names>J</given-names></name><name><surname>Luotolahti</surname><given-names>J</given-names></name><name><surname>Laippala</surname><given-names>V</given-names></name><name><surname>Ginter</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Syntactic N-gram collection from a large-scale corpus of internet Finnish</article-title><source>Frontiers in Artificial Intelligence and Applications</source><volume>268</volume><fpage>184</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3233/978-1-61499442-8-184</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katti</surname><given-names>H</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A separable neural code in monkey IT enables perfect CAPTCHA decoding</article-title><source>Journal of Neurophysiology</source><volume>127</volume><fpage>869</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.1152/jn.00160.2021</pub-id><pub-id pub-id-type="pmid">35196158</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Communications of the ACM</source><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>N</given-names></name><name><surname>Issa</surname><given-names>E</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain-like object recognition with high-performing shallow recurrent ANNs</article-title><conf-name>In: Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lage-Castellanos</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Methods for computing the maximum performance of computational models of fMRI responses</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006397</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006397</pub-id><pub-id pub-id-type="pmid">30849071</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laszlo</surname><given-names>S</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A neurally plausible parallel distributed processing model of event-related potential word reading data</article-title><source>Brain and Language</source><volume>120</volume><fpage>271</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2011.09.001</pub-id><pub-id pub-id-type="pmid">21945392</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laszlo</surname><given-names>S</given-names></name><name><surname>Armstrong</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>PSPs and ERPs: Applying the dynamics of post-synaptic potentials to individual units in simulation of temporally extended Event-Related Potential reading data</article-title><source>Brain and Language</source><volume>132</volume><fpage>22</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2014.03.002</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laszlo</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Never seem to find the time: evaluating the physiological time course of visual word recognition with regression analysis of single item ERPs</article-title><source>Language and Cognitive Processes</source><volume>29</volume><fpage>642</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1080/01690965.2013.866259</pub-id><pub-id pub-id-type="pmid">24954966</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Convolutional neural networks as a model of the visual system: past, present, and future</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2017</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01544</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>An Introduction to the Event-Related Potential Technique</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Marcel</surname><given-names>S</given-names></name><name><surname>Rodriguez</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Torchvision the machine-vision package of torch</article-title><conf-name>MM ’10: Proceedings of the 18th ACM international conference on Multimedia</conf-name><fpage>1485</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1145/1873951.1874254</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Rumelhart</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An interactive activation model of context effects in letter perception: I. An account of basic findings</article-title><source>Psychological Review</source><volume>88</volume><fpage>580</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1016/B978-1-4832-1446-7.50048-0</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The parallel distributed processing approach to semantic cognition</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>310</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1038/nrn1076</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLeod</surname><given-names>P</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attractor dynamics in word recognition: converging evidence from errors by normal subjects, dyslexic patients and a connectionist model</article-title><source>Cognition</source><volume>74</volume><fpage>91</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(99)00067-0</pub-id><pub-id pub-id-type="pmid">10594311</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monaghan</surname><given-names>P</given-names></name><name><surname>Shillcock</surname><given-names>RC</given-names></name><name><surname>Christiansen</surname><given-names>MH</given-names></name><name><surname>Kirby</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How arbitrary is language?</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>369</volume><elocation-id>20130299</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0299</pub-id><pub-id pub-id-type="pmid">25092667</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morton</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Interaction of information in word recognition</article-title><source>Psychological Review</source><volume>76</volume><fpage>165</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1037/h0027366</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murakami</surname><given-names>S</given-names></name><name><surname>Okada</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contributions of principal neocortical neurons to magnetoencephalography and electroencephalography signals</article-title><source>The Journal of Physiology</source><volume>575</volume><fpage>925</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2006.105379</pub-id><pub-id pub-id-type="pmid">16613883</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>Kinoshita</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reading through a noisy channel: why there’s nothing special about the perception of orthography</article-title><source>Psychological Review</source><volume>119</volume><fpage>517</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1037/a0028450</pub-id><pub-id pub-id-type="pmid">22663560</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Models of visual word recognition</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>517</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.08.003</pub-id><pub-id pub-id-type="pmid">24012145</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nour Eddine</surname><given-names>S</given-names></name><name><surname>Brothers</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Spratling</surname><given-names>M</given-names></name><name><surname>Kuperberg</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A predictive coding model of the N400</article-title><source>Cognition</source><volume>246</volume><elocation-id>105755</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2024.105755</pub-id><pub-id pub-id-type="pmid">38428168</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ororbia</surname><given-names>A</given-names></name><name><surname>Mali</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Convolutional neural generative coding: scaling predictive coding to natural images</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2211.12047</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parviainen</surname><given-names>T</given-names></name><name><surname>Helenius</surname><given-names>P</given-names></name><name><surname>Poskiparta</surname><given-names>E</given-names></name><name><surname>Niemi</surname><given-names>P</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical sequence of word perception in beginning readers</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>6052</fpage><lpage>6061</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0673-06.2006</pub-id><pub-id pub-id-type="pmid">16738248</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title><conf-name>Advances in Neural Information Processing Systems 32</conf-name><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>C</given-names></name><name><surname>Ziegler</surname><given-names>JC</given-names></name><name><surname>Zorzi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nested incremental modeling in the development of computational theories: the CDP+ model of reading aloud</article-title><source>Psychological Review</source><volume>114</volume><fpage>273</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.2.273</pub-id><pub-id pub-id-type="pmid">17500628</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The maps problem and the mapping problem: two challenges for a cognitive neuroscience of speech and language</article-title><source>Cognitive Neuropsychology</source><volume>29</volume><fpage>34</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1080/02643294.2012.710600</pub-id><pub-id pub-id-type="pmid">23017085</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading</article-title><source>NeuroImage</source><volume>62</volume><fpage>816</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.062</pub-id><pub-id pub-id-type="pmid">22584224</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The evolution of cognitive models: From neuropsychology to neuroimaging and back</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>107</volume><fpage>37</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.12.020</pub-id><pub-id pub-id-type="pmid">29373117</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Protopapas</surname><given-names>A</given-names></name><name><surname>Orfanidou</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>JSH</given-names></name><name><surname>Karavasilis</surname><given-names>E</given-names></name><name><surname>Kapnoula</surname><given-names>EC</given-names></name><name><surname>Panagiotaropoulou</surname><given-names>G</given-names></name><name><surname>Velonakis</surname><given-names>G</given-names></name><name><surname>Poulou</surname><given-names>LS</given-names></name><name><surname>Smyrnis</surname><given-names>N</given-names></name><name><surname>Kelekis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evaluating cognitive models of visual word recognition using fMRI: Effects of lexical and sublexical variables</article-title><source>NeuroImage</source><volume>128</volume><fpage>328</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.01.013</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Sanghavi</surname><given-names>S</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The inferior temporal cortex is a potential cortical precursor of orthographic processing in untrained monkeys</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3886</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17714-3</pub-id><pub-id pub-id-type="pmid">32753603</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>An interactive activation model of context effects in letter perception: Part 2. The contextual enhancement effect and some tests and extensions of the model</article-title><source>Psychological Review</source><volume>89</volume><fpage>60</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">7058229</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Clinical neurophysiology of language: The MEG approach</article-title><source>Clinical Neurophysiology</source><volume>118</volume><fpage>237</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2006.07.316</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Multi-dipole modeling in MEG</chapter-title><person-group person-group-type="editor"><name><surname>Hansen</surname><given-names>P</given-names></name><name><surname>Kringelbach</surname><given-names>M</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><source>MEG: An Introduction to Methods</source><publisher-name>Oxford University Press</publisher-name><fpage>124</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195307238.003.0006</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schaeffer</surname><given-names>R</given-names></name><name><surname>Khona</surname><given-names>M</given-names></name><name><surname>Chandra</surname><given-names>S</given-names></name><name><surname>Ostrow</surname><given-names>M</given-names></name><name><surname>Miranda</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Position: maximizing neural regression scores may not identify good models of the brain</article-title><conf-name>UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models</conf-name></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindler</surname><given-names>S</given-names></name><name><surname>Schettino</surname><given-names>A</given-names></name><name><surname>Pourtois</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Electrophysiological correlates of the interplay between low-level visual features and emotional content during word reading</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>12228</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-30701-5</pub-id><pub-id pub-id-type="pmid">30111849</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Brain-score: Which artificial neural network for object recognition is most brain-like?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seidenberg</surname><given-names>MS</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>A distributed, developmental model of word recognition and naming</article-title><source>Psychological Review</source><volume>96</volume><fpage>523</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.96.4.523</pub-id><pub-id pub-id-type="pmid">2798649</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Wolf</surname><given-names>L</given-names></name><name><surname>Bileschi</surname><given-names>S</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Robust object recognition with cortex-like mechanisms</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>29</volume><fpage>411</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.56</pub-id><pub-id pub-id-type="pmid">17224612</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Service</surname><given-names>E</given-names></name><name><surname>Helenius</surname><given-names>P</given-names></name><name><surname>Maury</surname><given-names>S</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Localization of syntactic and semantic brain responses using magnetoencephalography</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1193</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.7.1193</pub-id><pub-id pub-id-type="pmid">17583994</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Rabinovich</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Going deeper with convolutions</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarkiainen</surname><given-names>A</given-names></name><name><surname>Helenius</surname><given-names>P</given-names></name><name><surname>Hansen</surname><given-names>PC</given-names></name><name><surname>Cornelissen</surname><given-names>PL</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Dynamics of letter string perception in the human occipitotemporal cortex</article-title><source>Brain</source><volume>122</volume><fpage>2119</fpage><lpage>2132</lpage><pub-id pub-id-type="doi">10.1093/brain/122.11.2119</pub-id><pub-id pub-id-type="pmid">10545397</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Testolin</surname><given-names>A</given-names></name><name><surname>Stoianov</surname><given-names>I</given-names></name><name><surname>Zorzi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Letter perception emerges from unsupervised deep learning and recycling of natural image features</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>657</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0186-2</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>van Vliet</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>viswordrec-baseline</data-title><version designator="swh:1:rev:9cf26caf6fdaa230c83177c88e8a476ede4d1dd2">swh:1:rev:9cf26caf6fdaa230c83177c88e8a476ede4d1dd2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:4cf5cee0d31681e2d57c7c5e5f530a63663ab99b;origin=https://github.com/wmvanvliet/viswordrec-baseline;visit=swh:1:snp:7e9ac736b977bb482dd4aeeae078b7caf07a83e5;anchor=swh:1:rev:9cf26caf6fdaa230c83177c88e8a476ede4d1dd2">https://archive.softwareheritage.org/swh:1:dir:4cf5cee0d31681e2d57c7c5e5f530a63663ab99b;origin=https://github.com/wmvanvliet/viswordrec-baseline;visit=swh:1:snp:7e9ac736b977bb482dd4aeeae078b7caf07a83e5;anchor=swh:1:rev:9cf26caf6fdaa230c83177c88e8a476ede4d1dd2</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vartiainen</surname><given-names>J</given-names></name><name><surname>Liljeström</surname><given-names>M</given-names></name><name><surname>Koskinen</surname><given-names>M</given-names></name><name><surname>Renvall</surname><given-names>H</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional magnetic resonance imaging blood oxygenation level-dependent signal and magnetoencephalography evoked responses yield different neural functionality in reading</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>1048</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3113-10.2011</pub-id><pub-id pub-id-type="pmid">21248130</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention Is All You Need</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: Fundamental algorithms for scientific computing in python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woollams</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Lexical is as lexical does: computational approaches to lexical representation</article-title><source>Language, Cognition and Neuroscience</source><volume>30</volume><fpage>395</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1005637</pub-id><pub-id pub-id-type="pmid">25893204</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolnough</surname><given-names>O</given-names></name><name><surname>Donos</surname><given-names>C</given-names></name><name><surname>Rollo</surname><given-names>PS</given-names></name><name><surname>Forseth</surname><given-names>KJ</given-names></name><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Crone</surname><given-names>NE</given-names></name><name><surname>Fischer-Baum</surname><given-names>S</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Tandon</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spatiotemporal dynamics of orthographic and lexical processing in the ventral visual pathway</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>389</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00982-w</pub-id><pub-id pub-id-type="pmid">33257877</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96217.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Max Planck Institute for Psycholinguistics</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Useful</kwd></kwd-group></front-stub><body><p>van Vliet and colleagues show a <bold>useful</bold> correlation between internal states of a convolutional neural network (CNN) trained on visual word stimuli with three specific components of evoked MEG potentials during reading in humans. The findings are <bold>solid</bold>, though quantitative evidence that model can produce any of the phenomena that the human visual system is known to have (e.g., feedback connections, sensitivity to word frequency), or that it has comparable performance to human behaviour (i.e., similar task accuracy with a comparable pattern of mistakes) would make the conclusions much stronger.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96217.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>van Vliet and colleagues present results of a study correlating internal states of a convolutional neural network trained on visual word stimuli with evoked MEG potentials during reading.</p><p>In this study, a standard deep learning image recognition model (VGG-11) trained on a large natural image set (ImageNet) that begins illiterate but is then further trained on visual word stimuli, is used on a set of predefined stimulus images to extract strings of characters from &quot;noisy&quot; words, pseudowords and real words. This methodology is used in hopes of creating a model which learns to apply the same nonlinear transforms that could be happening in different regions of the brain - which would be validated by studying the correlations between the weights of this model and neural responses. Specifically, the aim is that the model learns some vector embedding space, as quantified by the spread of activations across a layer's weights (L2 Norm prior to ReLu Activation Function), for the different kinds of stimuli, that creates a parameterized decision boundary that is similar to amplitude changes at different times for a MEG signal. More importantly, the way that the stimuli are ordered or ranked in that space should be separable to the degree we see separation in neural activity. This study does show that the layer weights corresponding to five different broad classes of stimuli do statistically correlate with three specific components in the ERP. However, I believe there are fundamental theoretical issues that limit the implications of the results of this study.</p><p>As has been shown over many decades, there are many potential computational algorithms, with varied model architectures, that can perform the task of text recognition from an image. However, there is no evidence presented here that this particular algorithm has comparable performance to human behavior (i.e. similar accuracy with a comparable pattern of mistakes). This is a fundamental prerequisite before attempting to meaningfully correlate these layer activations to human neural activations. Therefore, it is unlikely that correlating these derived layer weights to neural activity provides meaningful novel insights into neural computation beyond what is seen using traditional experimental methods.</p><p>One example of a substantial discrepancy between this model and neural activations is that, while incorporating frequency weighting into the training data is shown to slightly increase neural correlation with the model, Figure 7 shows that no layer of the model appears directly sensitive to word frequency. This is in stark contrast to the strong neural sensitivity to word frequency seen in EEG (e.g. Dambacher et al 2006 Brain Research), fMRI (e.g. Kronbichler et al 2004 NeuroImage), MEG (e.g. Huizeling et al 2021 Neurobio. Lang.), and intracranial (e.g. Woolnough et al 2022 J. Neurosci.) recordings. Figure 7 also demonstrates that late stages of the model show a strong negative correlation with font size, whereas later stages of neural visual word processing are typically insensitive to differences in visual features, instead showing sensitivity to lexical factors.</p><p>Another example of the mismatch between this model and visual cortex is the lack of feedback connections in the model. Within visual cortex there are extensive feedback connections, with later processing stages providing recursive feedback to earlier stages. This is especially evident in reading, where feedback from lexical level processes feeds back to letter level processes (e.g. Heilbron et al 2020 Nature Comms.). This feedback is especially relevant for reading of words in noisy conditions, as tested in the current manuscript, as lexical knowledge enhances letter representation in visual cortex (the word superiority effect). This results in neural activity in multiple cortical areas varying over time, changing selectivity within a region at different measured time points (e.g. Woolnough et al 2021 Nature Human Behav.), which in the current study is simplified down to three discrete time windows, each attributed to different spatial locations.</p><p>The presented model needs substantial further development to be able to replicate, both behaviorally and neurally, many of the well-characterized phenomena seen in human behavior and neural recordings that are fundamental hallmarks of human visual word processing. Until that point it is unclear what novel contributions can be gleaned from correlating low dimensional model weights from these computational models with human neural data.</p><p>The revised version of this manuscript has not addressed these concerns.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96217.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors investigate the extent to which the responses of different layers of a vision model (VGG-11) can be linked to the cascade of responses (namely, type-I, type-II and N400) in the human brain when reading words. To achieve maximal consistency between, they add noisy-activations to VGG and finetune it on a character recognition task. In this setup, they observe various similarities between the behavior of VGG and the brain when presented with various transformations of the words (added noise, font modification etc).</p><p>Strengths:</p><p>- The paper is well written and well presented</p><p>- The topic studied is interesting.</p><p>- The fact that the response of the CNN on unseen experimental contrasts such as adding noise correlated with previous results on the brain is compelling.</p><p>Weaknesses:</p><p>- The paper is rather qualitative in nature. In particular, the authors show that some resemblance exists between the behavior of some layers and some parts of the brain, but it is hard to quantitively understand how strong the resemblences are in each layer, and the exact impact of experimental settings such as the frequency balancing (which seems to only have a very moderate effect according to figure 5)</p><p>- The experiments only consider a rather outdated vision model (VGG)</p><p>Comments on revisions:</p><p>After rebuttal, the authors significantly strengthened their results. I now find the paper much more convincing, and thank the author for their careful consideration of the reviewers' suggestions.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96217.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>van Vliet</surname><given-names>Marijn</given-names></name><role specific-use="author">Author</role><aff><institution>Aalto University</institution><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff></contrib><contrib contrib-type="author"><name><surname>Rinkinen</surname><given-names>Oona</given-names></name><role specific-use="author">Author</role><aff><institution>Aalto University</institution><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff></contrib><contrib contrib-type="author"><name><surname>Shimizu</surname><given-names>Takao</given-names></name><role specific-use="author">Author</role><aff><institution>Aalto University</institution><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff></contrib><contrib contrib-type="author"><name><surname>Niskanen</surname><given-names>Anni-Mari</given-names></name><role specific-use="author">Author</role><aff><institution>Aalto University</institution><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff></contrib><contrib contrib-type="author"><name><surname>Devereux</surname><given-names>Barry</given-names></name><role specific-use="author">Author</role><aff><institution>Queen's University Belfast</institution><addr-line><named-content content-type="city">Belfast</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Salmelin</surname><given-names>Riitta</given-names></name><role specific-use="author">Author</role><aff><institution>Aalto University</institution><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>We thank the reviewers for their efforts. They have pointed out several shortcomings and made very helpful suggestions. Based on their feedback, we have substantially revised the manuscript and feel the paper has been much improved because of it.</p><p>Notable changes are:</p><p>(1) As our model does not contain feed-back connections, the focus of the study is now more clearly communicated to be on feed-forward processes only, with appropriate justifications for this choice added to the Introduction and Discussion sections. Accordingly, the title has been changed to include the term “feed-forward”.</p><p>(2) The old Figure 5 has been removed in favor of reporting correlation scores to the right of the response profiles in other figures.</p><p>(3) We now discuss changes to the network architecture (new Figure 5) and fine-tuning of the hyperparameters (new Figure 6) in the main text instead of only the Supplementary Information.</p><p>(4) The discussion on qualitative versus quantitative analysis has been extended and given its own subsection entitled “On the importance of experimental contrasts and qualitative analysis of the model”.</p><p>Below, we address each point that the reviewers brought up in detail and outline what improvements we have made in the revision to address them.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>This study trained a CNN for visual word classification and supported a model that can explain key functional effects of the evoked MEG response during visual word recognition, providing an explicit computational account from detection and segmentation of letter shapes to final word-form identification.</p><p>Strengths:</p><p>This paper not only bridges an important gap in modeling visual word recognition, by establishing a direct link between computational processes and key findings in experimental neuroimaging studies, but also provides some conditions to enhance biological realism.</p><p>Weaknesses:</p><p>The interpretation of CNN results, especially the number of layers in the final model and its relationship with the processing of visual words in the human brain, needs to be further strengthened.</p></disp-quote><p>We have experimented with the number of layers and the number of units in each layer. In the previous version of the manuscript, these results could be found in the supplementary information. For the revised version, we have brought some of these results into the main text and discuss them more thoroughly.</p><p>We have added a figure (Figure 5 in the revised manuscript) showing the impact of the number of convolution and fully-connected layers on the response profiles of the layers, as well as the correlation with the three MEG components.</p><p>We discuss the figure in the Results section as follows:</p><p>“Various variations in model architecture and training procedure were evaluated. We found that the number of layers had a large impact on the response patterns produced by the model (Figure 5). The original VGG-11 architecture defines 5 convolution layers and 3 fully connected layers (including the output layer). Removing a convolution layer (Figure 5, top row), or removing one of the fully connected layers (Figure 5, second row), resulted in a model that did exhibit an enlarged response to noisy stimuli in the early layers that mimics the Type-I response. However, such models failed to show a sufficiently diminished response to noisy stimuli in the later layers, hence failing to produce responses that mimic the Type-II or N400m, a failure which also showed as low correlation scores.</p><p>Adding an additional convolution layer (Figure 5, third row) resulted in a model where none of the layer response profiles mimics that of the Type-II response. The Type-II response is characterized by a reduced response to both noise and symbols, but an equally large response to consonant strings, real and pseudo words. However, in the model with an additional convolution layer, the consonant strings evoked a reduced response already in the first fully connected layer, which is a feature of the N400m rather than the Type-II. These kind of subtleties in the response pattern, which are important for the qualitative analysis, generally did not show quantitatively in the correlation scores, as the fully connected layers in this model correlate as well with the Type-II response as models that did show a response pattern that mimics the Type-II.</p><p>Adding an additional fully connected layer (Figure 5, fourth row) resulted in a model with similar response profiles and correlation with the MEG components as the original VGG-11 architecture (Figure 5, bottom row) The N400m-like response profile is now observed in the third fully connected layer rather than the output layer. However, the decrease in response to consonant strings versus real and pseudo words, which is typical of the N400m, is less distinct than in the original VGG-11 architecture.”</p><p>And in the Discussion section:</p><p>“In the model, convolution units are followed by pooling units, which serve the purpose of stratifying the response across changes in position, size and rotation within the receptive field of the pooling unit. Hence, the effect of small differences in letter shape, such as the usage of different fonts, was only present in the early convolution layers, in line with findings in the EEG literature (Chauncey et al., 2008; Grainger &amp; Holcomb, 2009; Hauk &amp; Pulvermüller, 2004). However, the ability of pooling units to stratify such differences depends on the size of their receptive field, which is determined by the number of convolution-and-pooling layers. As a consequence, the response profiles of the subsequent fully connected layers was also very sensitive to the number of convolution-and-pooling layers. The optimal number of such layers is likely dependent on the input size and pooling strategy. Given the VGG-11 design of doubling the receptive field after each layer, combined with an input size of 225×225 pixels, the optimal number of convolution-andpooling layers for our model was five, or the model would struggle to produce response profiles mimicking those of the Type-II component in the subsequent fully connected layers (Figure 5).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) The similarity between CNNs and human MEG responses, including type-I (100ms), type-II (150ms), and N400 (400ms) components, looks like separately, lacking the sequential properties among these three components. Is the recurrent neural network (RNN), which can be trained to process and convert a sequential data input into a specific sequential data output, a better choice?</p></disp-quote><p>When modeling sequential effects, meaning that the processing of the current word is influenced by the word that came before it, such as priming and top-down modulations, we agree that such a model would indeed require recurrency in its architecture. However, we feel that the focus of modeling efforts in reading has been overwhelmingly on the N400 and such priming effects, usually skipping over the pixel-to-letter process. So, for this paper, we were keen on exploring more basic effects such as noise and symbols versus letters on the type-I and type-II responses. And for these effects, a feed-forward model turns out to be sufficient, so we can keep the focus of this particular paper on bottom-up processes during single word reading, on which there is already a lot to say.</p><p>To clarify our focus on feed-forward process, we have modified the title of the paper to be:</p><p>“Convolutional networks can model the functional modulation of the MEG responses associated with feed-forward processes during visual word recognition” furthermore, we have revised the Introduction to highlight this choice, noting:</p><p>“Another limitation is that these models have primarily focused on feed-back lexicosemantic effects while oversimplifying the initial feed-forward processing of the visual input.</p><p>[…]</p><p>For this study, we chose to focus on modeling the early feed-forward processing occurring during visual word recognition, as the experimental setup in Vartiainen et al. (2011) was designed to demonstrate.</p><p>[…]</p><p>By doing so, we restrict ourselves to an investigation of how well the three evoked components can be explained by a feed-forward CNN in an experimental setting designed to demonstrate feed-forward effects. As such, the goal is not to present a complete model of all aspects of reading, which should include feed-back effects, but rather to demonstrate the effectiveness of using a model that has a realistic form of input when the aim is to align the model with the evoked responses observed during visual word recognition.”</p><p>And in the Discussion section:</p><p>“In this paper we have restricted our simulations to feed-forward processes. Now, the way is open to incorporate convolution-and-pooling principles in models of reading that simulate feed-back processes as well, which should allow the model to capture more nuance in the Type-II and N400m components, as well as extend the simulation to encompass a realistic semantic representation.”</p><p>(2) There is no clear relationship between the layers that signal needs to traverse in the model and the relative duration of the three components in the brain.</p><p>While some models offer a tentative mapping between layers and locations in the brain, none of the models we are aware of actually simulate time accurately and our model is no exception.</p><p>While we provide some evidence that the three MEG components are best modeled with different types of layers, and the type-I becomes somewhere before type-II and N400m is last in our model, the lack of timing information is a weakness of our model we have not been able to address. In our previous version, this already was the main topic of our “Limitations of the model” section, but since this weakness was pointed out by all reviewers, we have decided to widen our discussion of it:</p><p>“One important limitation of the current model is the lack of an explicit mapping from the units inside its layers to specific locations in the brain at specific times. The temporal ordering of the components is simulated correctly, with the response profile matching that of the type-I occurring the layers before those matching the type-II, followed by the N400m. Furthermore, every component is best modeled by a different type of layer, with the type-I best described by convolution-and-pooling, the type-II by fully-connected linear layers and the N400m by a one-hot encoded layer. However, there is no clear relationship between the number of layers the signal needs to traverse in the model to the processing time in the brain. Even if one considers that the operations performed by the initial two convolution layers happen in the retina rather than the brain, the signal needs to propagate through three more convolution layers to reach the point where it matches the type-II component at 140-200 ms, but only through one more additional layer to reach the point where it starts to match the N400m component at 300-500 ms. Still, cutting down on the number of times convolution is performed in the model seems to make it unable to achieve the desired suppression of noise (Figure 5). It also raises the question what the brain is doing during the time between the type-II and N400m component that seems to take so long. It is possible that the timings of the MEG components are not indicative solely of when the feed-forward signal first reaches a certain location, but are rather dictated by the resolution of feed-forward and feedback signals (Nour Eddine et al., 2024).”</p><p>See also our response to the next comment of the Reviewer, in which we dive more into the effect of the number of layers, which could be seen as a manipulation of time.</p><disp-quote content-type="editor-comment"><p>(3) I am impressed by the CNN that authors modified to match the human brain pattern for the visual word recognition process, by the increase and decrease of the number of layers. The result of this part was a little different from the author’s expectation; however, the author didn’t explain or address this issue.</p></disp-quote><p>We are glad to hear that the reviewer found these results interesting. Accordingly, we now discuss these results more thoroughly in the main text.</p><p>We have moved the figure from the supplementary information to the main text (Figure 5 in the revised manuscript). And describe the results in the Results section:</p><p>“Various variations in model architecture and training procedure were evaluated. We found that the number of layers had a large impact on the response patterns produced by the model (Figure 5). The original VGG-11 architecture defines 5 convolution layers and 3 fully connected layers (including the output layer). Removing a convolution layer (Figure 5, top row), or removing one of the fully connected layers (Figure 5, second row), resulted in a model that did exhibit an enlarged response to noisy stimuli in the early layers that mimics the Type-I response. However, such models failed to show a sufficiently diminished response to noisy stimuli in the later layers, hence failing to produce responses that mimic the Type-II or N400m, a failure which also showed as low correlation scores.</p><p>Adding an additional convolution layer (Figure 5, third row) resulted in a model where none of the layer response profiles mimics that of the Type-II response. The Type-II response is characterized by a reduced response to both noise and symbols, but an equally large response to consonant strings, real and pseudo words. However, in the model with an additional convolution layer, the consonant strings evoked a reduced response already in the first fully connected layer, which is a feature of the N400m rather than the Type-II. These kind of subtleties in the response pattern, which are important for the qualitative analysis, generally did not show quantitatively in the correlation scores, as the fully connected layers in this model correlate as well with the Type-II response as models that did show a response pattern that mimics the Type-II.</p><p>Adding an additional fully connected layer (Figure 5, fourth row) resulted in a model with similar response profiles and correlation with the MEG components as the original VGG-11 architecture (Figure 5, bottom row) The N400m-like response profile is now observed in the third fully connected layer rather than the output layer. However, the decrease in response to consonant strings versus real and pseudo words, which is typical of the N400m, is less distinct than in the original VGG-11 architecture.”</p><p>We also incorporated these results in the Discussion:</p><p>“However, the ability of pooling units to stratify such differences depends on the size of their receptive field, which is determined by the number of convolution-andpooling layers. This might also explain why, in later layers, we observed a decreased response to stimuli where text was rendered with a font size exceeding the receptive field of the pooling units (Figure 8). Hence, the response profiles of the subsequent fully connected layers was very sensitive to the number of convolution-and-pooling layers. This number is probably dependent on the input size and pooling strategy. Given the VGG11 design of doubling the receptive field after each layer, combined with an input size of 225x225 pixels, the optimal number of convolution-and-pooling layers for our model was five, or the model would struggle to produce response profiles mimicking those of the type-II component in the subsequent fully connected layers (Figure 5).</p><p>[…]</p><p>A minimum of two fully connected layers was needed to achieve this in our case, and adding more fully connected layers would make them behave more like the component (Figure 5).”</p><disp-quote content-type="editor-comment"><p>(4) Can the author explain why the number of layers in the final model is optimal by benchmarking the brain hierarchy?</p></disp-quote><p>We have incorporated the figure describing the correlation between each model and the MEG components (previously Figure 5) with the figures describing the response profiles (Figures 4 and 5 in the revised manuscript and Supplementary Figures 2-6). This way, we (and the reader) can now benchmark every model qualitatively and quantitatively.</p><p>As we stated in our response to the previous comment, we have added a more thorough discussion on the number of layers, which includes the justification for our choice for the final model. The benchmark we used was primarily whether the model shows the same response patterns as the Type I, Type II and N400 responses, which disqualifies all models with fewer than 5 convolution and 3 fully connected layers. Models with more layers also show the proper response patterns, however we see that there is actually very little difference in the correlation scores between different models. Hence, our justification for sticking with the original VGG11 architecture is that it produces the qualitative best response profiles, while having roughly the same (decently high) correlation with the MEG components. Furthermore, by sticking to the standard architecture, we make it slightly easier to replicate our results as one can use readily available pre-trained ImageNet weights.</p><p>As well as always discussing the correlation scores in tandem with the qualitative analysis, we have added the following statement to the Results:</p><p>“Based on our qualitative and quantitative analysis, the model variant that performed best overall was the model that had the original VGG11 architecture and was preinitialized from earlier training on ImageNet, as depicted in the bottom rows of Figure 4 and Figure 5.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>As has been shown over many decades, many potential computational algorithms, with varied model architectures, can perform the task of text recognition from an image. However, there is no evidence presented here that this particular algorithm has comparable performance to human behavior (i.e. similar accuracy with a comparable pattern of mistakes). This is a fundamental prerequisite before attempting to meaningfully correlate these layer activations to human neural activations. Therefore, it is unlikely that correlating these derived layer weights to neural activity provides meaningful novel insights into neural computation beyond what is seen using traditional experimental methods.</p></disp-quote><p>We very much agree with the reviewer that a qualitative analysis of whether the model can explain experimental effects needs to happen before a quantitative analysis, such as evaluating model-brain correlation scores. In fact, this is one of the intended key points we wished to make.</p><p>As we discuss at length in the Introduction, “traditional” models of reading (those that do not rely on deep learning) are not able to recognize a word regardless of exact letter shape, size, and (up to a point) rotation. In this study, our focus is on these low-level visual tasks rather than high-level tasks concerning semantics. As the Reviewer correctly states, there are many potential computational algorithms able to perform these visual task at a human level and so we need to evaluate the model not only on its ability to mimic human accuracy but also on generating a comparable pattern of mistakes. In our case, we need a pattern of behavior that is indicative of the visual processes at the beginning of the reading pipeline. Hence, rather than relying on behavioral responses that are produced at the very end, we chose the evaluate the model based on three MEG components that provide “snapshots” of the reading process at various stages. These components are known to manifest a distinct pattern of “behavior” in the way they respond to different experimental conditions (Figure 2), akin to what to Reviewer refers to as a “pattern of mistakes”. The model was first evaluated on its ability to replicate the behavior of the MEG components in a qualitative manner (Figure 4). Only then do we move on to a quantitative correlation analysis. In this manner, we feel we are in agreement with the approach advocated by the Reviewer.</p><p>In the Introduction, we now clarify:</p><p>“Another limitation is that these models have primarily focused on feed-back lexicosemantic effects while oversimplifying the initial feed-forward processing of the visual input.</p><p>[…]</p><p>We sought to construct a model that is able to recognize words regardless of length, size, typeface and rotation, as well as humans can, so essentially perfectly, whilst producing activity that mimics the type-I, type-II, and N400m components which serve as snapshots of this process unfolding in the brain.</p><p>[…]</p><p>These variations were first evaluated on their ability to replicate the experimental effects in that study, namely that the type-I response is larger for noise embedded words than all other stimuli, the type-II response is larger for all letter strings than symbols, and that the N400m is larger for real and pseudowords than consonant strings. Once a variation was found that could reproduce these effects satisfactorily, it was further evaluated based on the correlation between the amount of activation of the units in the model and MEG response amplitude.”</p><p>To make this prerequisite more clear, we have removed what was previously Figure 5, which showed the correlation between the various models the MEG components out of the context of their response patterns. Instead, these correlation values are now always presented next to the response patterns (Figures 4 and 5, and Supplementary Figures 2-6 in the revised manuscript). This invites the reader to always consider these metrics in relation to one another.</p><disp-quote content-type="editor-comment"><p>One example of a substantial discrepancy between this model and neural activations is that, while incorporating frequency weighting into the training data is shown to slightly increase neural correlation with the model, Figure 7 shows that no layer of the model appears directly sensitive to word frequency. This is in stark contrast to the strong neural sensitivity to word frequency seen in EEG (e.g. Dambacher et al 2006 Brain Research), fMRI (e.g. Kronbichler et al 2004 NeuroImage), MEG (e.g. Huizeling et al 2021 Neurobio. Lang.), and intracranial (e.g. Woolnough et al 2022 J. Neurosci.) recordings. Figure 7 also demonstrates that the late stages of the model show a strong negative correlation with font size, whereas later stages of neural visual word processing are typically insensitive to differences in visual features, instead showing sensitivity to lexical factors.</p></disp-quote><p>We are glad the reviewer brought up the topic of frequency balancing, as it is a good example of the importance of the qualitative analysis. Frequency balancing during training only had a moderate impact on correlation scores and from that point of view does not seem impactful. However, when we look at the qualitative evaluation, we see that with a large vocabulary, a model without frequency balancing fails to properly distinguish between consonant strings and (pseudo)words (Figure 4, 5th row). Hence, from the point of view of being able to reproduce experimental effects, frequency balancing had a large impact. We now discuss this more explicitly in the revised Discussion section:</p><p>“Overall, we found that a qualitative evaluation of the response profiles was more helpful than correlation scores. Often, a deficit in the response profile of a layer that would cause a decrease in correlation on one condition would be masked by an increased correlation in another condition. A notable example is the necessity for frequency-balancing the training data when building models with a vocabulary of 10 000. Going by correlation score alone, there does not seem to be much difference between the model trained with and without frequency balancing (Figure 4A, fifth row versus bottom row). However, without frequency balancing, we found that the model did not show a response profile where consonant strings were distinguished from words and pseudowords (Figure 4A, fifth row), which is an important behavioral trait that sets the N400m component apart from the Type-II component (Figure 2D). This underlines the importance of the qualitative evaluation in this study, which was only possible because of a straightforward link between the activity simulated within a model to measurements obtained from the brain, combined with the presence of clear experimental conditions.”</p><p>It is true that the model, even with frequency balancing, only captures letter- and bigramfrequency effects and not the word-frequency effects that we know the N400m is sensitive to. Since our model is restricted to feed-forward processes, this finding adds to the evidence that frequency-modulated effects are driven by feed-back effects as modeled by Nour Eddine et al. (2024, doi:10.1016/j.cognition.2024.105755). See also our response to the next comment by the Reviewer where we discuss feed-back connections. We have added the following to the section about model limitations in the revised Discussion:</p><p>“The fact that the model failed to simulate the effects of word-frequency on the N400m (Figure 8), even after frequency-balancing of the training data, is additional evidence that this effect may be driven by feed-back activity, as for example modeled by Nour Eddine et al. (2024).”</p><p>Like the Reviewer, we initially thought that later stages of neural visual word processing would be insensitive to differences in font size. When diving into the literature to find support for this claim, we found only a few works directly studying the effect of font size on evoked responses, but, surprisingly, what we did find seemed to align with our model. We have added the following to our revised Discussion:</p><p>“The fully connected linear layers in the model show a negative correlation with font size. While the N400 has been shown to be unaffected by font size during repetition priming (Chauncey et al., 2008), it has been shown that in the absence of priming, larger font sizes decrease the evoked activity in the 300–500 ms window (Bayer et al., 2012; Schindler et al., 2018). Those studies refer to the activity within this time window, which seems to encompass the N400, as early posterior negativity (EPN). What possibly happens in the model is that an increase in font size causes an initial stronger activation in the first layers, due to more convolution units receiving input. This leads to a better signal-to-noise ratio (SNR) later on, as the noise added to the activation of the units remains constant whilst the amplitude of the input signal increases. A better SNR translates ultimately in less co-activation of units corresponding to orthographic neighbours in the final layers, hence to a decrease in overall layer activity.”</p><disp-quote content-type="editor-comment"><p>Another example of the mismatch between this model and the visual cortex is the lack of feedback connections in the model. Within the visual cortex, there are extensive feedback connections, with later processing stages providing recursive feedback to earlier stages. This is especially evident in reading, where feedback from lexical-level processes feeds back to letter-level processes (e.g. Heilbron et al 2020 Nature Comms.). This feedback is especially relevant for the reading of words in noisy conditions, as tested in the current manuscript, as lexical knowledge enhances letter representation in the visual cortex (the word superiority effect). This results in neural activity in multiple cortical areas varying over time, changing selectivity within a region at different measured time points (e.g. Woolnough et al 2021 Nature Human Behav.), which in the current study is simplified down to three discrete time windows, each attributed to different spatial locations.</p></disp-quote><p>We agree with the Reviewer that a full model of reading in the brain must include feed-back connections and share their sentiment that these feed-back processes play an important role and are a fascinating topic to study. The intent for the model presented in our study is very much to be a stepping stone towards extending the capabilities of models that do include such connections.</p><p>However, there is a problem of scale that cannot be ignored.</p><p>Current models of reading that do include feedback connections fall into the category we refer to in the paper as “traditional models” and all only a few layers deep and operate on very simplified inputs, such as pre-defined line segments, a few pixels, or even a list of prerecognized letters. The Heilbron et al. 2020 study that the Reviewer refers to is a good example of such a model. (This excellent and relevant work was somehow overlooked in our literature discussion in the Introduction. We thank the Reviewer for pointing it out to us.) Models incorporating realistic feed-back activity need these simplifications, because they have a tendency to no longer converge when there are too many layers and units. However, in order for models of reading to be able to simulate cognitive behavior such as resolving variations in font size or typeface, or distinguish text from non-text, they need to operate on something close to the pixel-level data, which means they need many layers and units.</p><p>Hence, as a stepping stone, it is reasonable to evaluate a model that has the necessary scale, but lacks the feed-back connections that would be problematic at this scale, to see what it can and cannot do in terms of explaining experimental effects in neuroimaging studies. This was the intended scope of our study. For the revision, we have attempted to make this more clear.</p><p>We have changed the title to be:</p><p>“Convolutional networks can model the functional modulation of the MEG responses associated with feed-forward processes during visual word recognition” and added the following to the Introduction:</p><p>“The simulated environments in these models are extremely simplified, partly due to computational limitations and partly due to the complex interaction of feed-forward and feed-back connectivity that causes problems with convergence when the model grows too large. Consequently, these models have primarily focused on feed-back lexico-semantic effects while oversimplifying the initial feed-forward processing of the visual input.</p><p>[…]</p><p>This rather high level of visual representation sidesteps having to deal with issues such as visual noise, letters with different scales, rotations and fonts, segmentation of the individual letters, and so on. More importantly, it makes it impossible to create the visual noise and symbol string conditions used in the MEG study to modulate the type-I and type-II components. In order to model the process of visual word recognition to the extent where one may reproduce neuroimaging studies such as Vartiainen et al. (2011), we need to start with a model of vision that is able to directly operate on the pixels of a stimulus. We sought to construct a model that is able to recognize words regardless of length, size, typeface and rotation with very high accuracy, whilst producing activity that mimics the type-I, type-II, and N400m components which serve as snapshots of this process unfolding in the brain. For this model, we chose to focus on the early feed-forward processing occurring during visual word recognition, as the experimental setup in the MEG study was designed to demonstrate, rather than feed-back effects</p><p>[…]</p><p>By doing so, we restrict ourselves to an investigation of how well the three evoked components can be explained by a feed-forward CNN in an experimental setting designed to demonstrate feed-forward effects. &gt; As such, the goal is not to present a complete model of all aspects of reading, which should include feed-back effects, but rather to demonstrate the effectiveness of using a model that has a realistic form of input when the aim is to align the model with the evoked responses observed during visual word recognition.”</p><p>And we have added the following to the Discussion section:</p><p>“In this paper we have restricted our simulations to feed-forward processes. Now, the way is open to incorporate convolution-and-pooling principles in models of reading that simulate feed-back processes as well, which should allow the model to capture more nuance in the Type-II and N400m components, as well as extend the simulation to encompass a realistic semantic representation. A promising way forward may be to use a network architecture like CORNet (Kubilius et al., 2019), that performs convolution multiple times in a recurrent fashion, yet simultaneously propagates activity forward after each pass. The introduction of recursion into the model will furthermore align it better with traditional-style models, since it can cause a model to exhibit attractor behavior (McLeod et al., 2000), which will be especially important when extending the model into the semantic domain.</p><p>Furthermore, convolution-and-pooling has recently been explored in the domain of predictive coding models (Ororbia &amp; Mali, 2023), a type of model that seems particularly well suited to model feed-back processes during reading (Gagl et al., 2020; Heilbron et al., 2020; Nour Eddine et al., 2024).”</p><p>We also would like to point out to the Reviewer that we did in fact perform a correlation between the model and the MNE-dSPM source estimate of all cortical locations and timepoints (Figure 7B). Such a brain-wide correlation map confirms that the three dipole groups are excellent summaries of when and where interesting effects occur within this dataset.</p><disp-quote content-type="editor-comment"><p>The presented model needs substantial further development to be able to replicate, both behaviorally and neurally, many of the well-characterized phenomena seen in human behavior and neural recordings that are fundamental hallmarks of human visual word processing. Until that point, it is unclear what novel contributions can be gleaned from correlating low-dimensional model weights from these computational models with human neural data.</p></disp-quote><p>We hope that our revisions have clarified the goals and scope of this study. The CNN model we present in this study is a small but, we feel, essential piece in a bigger effort to employ deep learning techniques to further enhance already existing models of reading. In our revision, we have extended our discussion where to go from here and outline our vision on how these techniques could help us better model the phenomena the reviewer speaks of. We agree with the reviewer that there is a long way to go, and we are excited to be a part of it.</p><p>In addition to the changes described above, we now end the Discussion section as follows:</p><p>“Despite its limitations, our model is an important milestone for computational models of reading that leverages deep learning techniques to encompass the entire computational process starting from raw pixels values to representations of wordforms in the mental lexicon. The overall goal is to work towards models that can reproduce the dynamics observed in brain activity observed during the large number of neuroimaging experiments performed with human volunteers that have been performed over the last few decades. To achieve this, models need to be able to operate on more realistic inputs than a collection of predefined lines or letter banks (for example: Coltheart et al., 2001; Heilbron et al., 2020; Laszlo &amp; Armstrong, 2014; McClelland &amp; Rumelhart, 1981; Nour Eddine et al., 2024). We have shown that even without feed-back connections, a CNN can simulate the behavior of three important MEG evoked components across a range of experimental conditions, but only if unit activations are noisy and the frequency of occurrence of words in the training dataset mimics their frequency of use in actual language.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>The paper is rather qualitative in nature. In particular, the authors show that some resemblance exists between the behavior of some layers and some parts of the brain, but it is hard to quantitively understand how strong the resemblances are in each layer, and the exact impact of experimental settings such as the frequency balancing (which seems to only have a very moderate effect according to Figure 5).</p></disp-quote><p>The large focus on a qualitative evaluation of the model is intentional. The ability of the model to reproduce experimental effects (Figure 4) is a pre-requisite for any subsequent quantitative metrics (such as correlation) to be valid. The introduction of frequency balancing is a good example of this. As the reviewer points out, frequency balancing during training has only a moderate impact on correlation scores and from that point of view does not seem impactful. However, when we look at the qualitative evaluation, we see that with a large vocabulary, a model without frequency balancing fails to properly distinguish between consonant strings and (pseudo)words (Figure 4, 5th row). Hence, from the point of view of being able to reproduce experimental effects, frequency balancing has a large impact.</p><p>That said, the reviewer is right to highlight the value of quantitative analysis. An important limitation of the “traditional” models of reading that do not employ deep learning is that they operate in unrealistically simplified environments (e.g. input as predefined line segments, words of a fixed length), which makes a quantitative comparison with brain data problematic. The main benefit that deep learning brings may very well be the increase in scale that makes more direct comparisons with brain data possible. In our revision we attempt to capitalize on this benefit more. The reviewer has provided some helpful suggestions for doing so in their recommendations, which we discuss in detail below.</p><p>We have added the following discussion on the topic of qualitative versus quantitative analysis to the Introduction:</p><p>“We sought to construct a model that is able to recognize words regardless of length, size, typeface and rotation, as well as humans can, so essentially perfectly, whilst producing activity that mimics the type-I, type-II, and N400m components which serve as snapshots of this process unfolding in the brain.</p><p>[…]</p><p>These variations were first evaluated on their ability to replicate the experimental effects in that study, namely that the type-I response is larger for noise embedded words than all other stimuli, the type-II response is larger for all letter strings than symbols, and that the N400m is larger for real and pseudowords than consonant strings. Once a variation was found that could reproduce these effects satisfactorily, it was further evaluated based on the correlation between the amount of activation of the units in the model and MEG response amplitude.”</p><p>And follow this up in the Discussion with a new sub-section entitled “On the importance of experimental contrasts and qualitative analysis of the model”</p><disp-quote content-type="editor-comment"><p>The experiments only consider a rather outdated vision model (VGG).</p></disp-quote><p>VGG was designed to use a minimal number of operations (convolution-and-pooling, fullyconnected linear steps, ReLU activations, and batch normalization) and rely mostly on scale to solve the classification task. This makes VGG a good place to start our explorations and see how far a basic CNN can take us in terms of explaining experimental MEG effects in visual word recognition. However, we agree with the reviewer that it is easy to envision more advanced models that could potentially explain more. In our revision, we expand on the question of where to go from here and outline our vision on what types of models would be worth investigating and how one may go about doing that in a way that provides insights beyond higher correlation values.</p><p>We have included the following in our Discussion sub-sections on “Limitations of the current model and the path forward”:</p><p>“The VGG-11 architecture was originally designed to achieve high image classification accuracy on the ImageNet challenge (Simonyan &amp; Zisserman, 2015). Although we have introduced some modifications that make the model more biologically plausible, the final model is still incomplete in many ways as a complete model of brain function during reading.</p><p>[…]</p><p>In this paper we have restricted our simulations to feed-forward processes. Now, the way is open to incorporate convolution-and-pooling principles in models of reading that simulate feed-back processes as well, which should allow the model to capture more nuance in the Type-II and N400m components, as well as extend the simulation to encompass a realistic semantic representation. A promising way forward may be to use a network architecture like CORNet (Kubilius et al., 2019), that performs convolution multiple times in a recurrent fashion, yet simultaneously propagates activity forward after each pass. The introduction of recursion into the model will furthermore align it better with traditional-style models, since it can cause a model to exhibit attractor behavior (McLeod et al., 2000), which will be especially important when extending the model into the semantic domain. Furthermore, convolution-and-pooling has recently been explored in the domain of predictive coding models (Ororbia &amp; Mali, 2023), a type of model that seems particularly well suited to model feed-back processes during reading (Gagl et al., 2020; Heilbron et al., 2020; Nour Eddine et al., 2024).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>(1) The method used to select the experimental conditions under which the behavior of the CNN is the most brain-like is rather qualitative (Figure 4). It would have been nice to have a plot where the noisyness of the activations, the vocab size and the amount of frequency balancing are varied continuously, and show how these three parameters impact the correlation of the model layers with the MEG responses.</p></disp-quote><p>We now include this analysis (Figure 6 in the revised manuscript, Supplementary Figures 47) and discuss these factors in the revised Results section:</p><p>“Various other aspects of the model architecture were evaluated which ultimately did not lead to any improvements of the model. The response profiles can be found in the supplementary information (Supplementary Figures 4–7) and the correlations between the models and the MEG components are presented in Figure 6. The vocabulary of the final model (10 000) exceeds the number of units in its fullyconnected layers, which means that a bottleneck is created in which a sub-lexical representation is formed. The number of units in the fully-connected layers, i.e. the width of the bottleneck, has some effect on the correlation between model and brain (Figure 6A), and the amount of noise added to the unit activations less so (Figure 6B). We already saw that the size of the vocabulary, i.e. the number of wordforms in the training data and number of units in the output layer of the model, had a large effect on the response profiles (Figure 4). Having a large vocabulary is of course desirable from a functional point of view, but also modestly improves correlation between model and brain (Figure 6C). For large vocabularies, we found it beneficial to apply frequency-balancing of the training data, meaning that the number of times a word-form appears in the training data is scaled according to its frequency in a large text corpus. However, this cannot be a one-to-one scaling, since the most frequent words occur so much more often than other words that the training data would consist of mostly the top-ten most common words, with less common words only occurring once or not at all. Therefore, we decided to scale not by the frequency 𝑓 directly, but by 𝑓𝑠, where 0 &lt; 𝑠 &lt; 1, opting for 𝑠 = 0.2 for the final model (Figure 6D).”</p><disp-quote content-type="editor-comment"><p>(2) It is not clear which layers exactly correspond to which of the three response components. For this to be clearer, it would have been nice to have a plot with all the layers of VGG on the x-axis and three curves corresponding to the correlation of each layer with each of the three response components.</p></disp-quote><p>This is a great suggestion that we were happy to incorporate in the revised version of the manuscript. Every figure comparing the response patterns of the model and brain now includes a panel depicting the correlation between each layer of the model and each of the three MEG components (Figures 4 &amp; 5, Supplementary Figures 2-5). This has given us (and now also the reader) the ability to better benchmark the different models quantitatively, adding to our discussion on qualitative to quantitative analysis.</p><disp-quote content-type="editor-comment"><p>(3) It is not clear to me why the authors report the correlation of all layers with the MEG responses in Figure 5: why not only report the correlation of the final layers for N400, and that of the first layers for type-I?</p></disp-quote><p>We agree with the reviewer that it would have been better to compare the correlation scores for those layers which response profile matches the MEG component. While the old Figure 5 has been merged with Figure 4, and now provides the correlations between all the layers and all MEG components, we have taken the Reviewer’s advice and marked the layers which qualitatively best correspond to each MEG component, so the reader can take that into account when interpreting the correlation scores.</p><disp-quote content-type="editor-comment"><p>(4) The authors mention that the reason that they did not reproduce the protocol with more advanced vision models is that they needed the minimal setup capable of yielding the desired experiment effect. I am not fully convinced by this and think the paper could be significantly strengthened by reporting results for a vision transformer, in particular to study the role of attention layers which are expected to play an important role in processing higher-level features.</p></disp-quote><p>We appreciate and share the Reviewer’s enthusiasm in seeing how other model architectures would fare when it comes to modeling MEG components. However, we regard modifying the core model architecture (i.e., a series of convolution-and-pooling followed by fully-connected layers) to be out of scope for the current paper.</p><p>One of the key points of our study is to create a model that reproduces the experimental effects of an existing MEG study, which necessitates modeling the initial feed-forward processing from pixel to word-form. For this purpose, a convolution-and-pooling model was the obvious choice, because these operations play a big role in cognitive models of vision in general. In order to properly capture all experimental contrasts in the MEG study, many variations of the CNN were trained and evaluated. This iterative design process concluded when all experimental contrasts could be faithfully reproduced.</p><p>If we were to explore different model architectures, such as a transformer architecture, reproducing the experimental contrasts of the MEG study would no longer be the end goal, and it would be unclear what the end goal should be. Maximizing correlation scores has no end, and there are a nearly endless number of model architectures one could try. We could bring in a second MEG study with experimental contrasts that the CNN cannot explain and a transformer architecture potentially could and set the end goal to explain all experimental effects in both MEG studies. But even if we had access to such a dataset, this would almost double the length of the paper, which is already too long.</p></body></sub-article></article>