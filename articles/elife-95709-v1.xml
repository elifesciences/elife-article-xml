<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">95709</article-id><article-id pub-id-type="doi">10.7554/eLife.95709</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95709.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Anti-drift pose tracker (ADPT), a transformer-based network for robust animal pose estimation cross-species</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Tang</surname><given-names>Guoling</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0008-2318-2624</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Han</surname><given-names>Yaning</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1650-2262</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Xing</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ruonan</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Han</surname><given-names>Ming-Hu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Liu</surname><given-names>Quanying</given-names></name><email>liuqy@sustech.edu.cn</email><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wei</surname><given-names>Pengfei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1845-8856</contrib-id><email>pf.wei@siat.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02c9qn167</institution-id><institution>Guangxi University of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Liuzhou</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03hz5th67</institution-id><institution>Shenzhen University of Advanced Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/049tv2d57</institution-id><institution>Department of Biomedical Engineering, Southern University of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>05</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP95709</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-02-06"><day>06</day><month>02</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-02-08"><day>08</day><month>02</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.06.579164"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-14"><day>14</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95709.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-03-24"><day>24</day><month>03</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95709.2"/></event></pub-history><permissions><copyright-statement>© 2024, Tang, Han et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Tang, Han et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-95709-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-95709-figures-v1.pdf"/><abstract><p>Deep learning-based methods have advanced animal pose estimation, enhancing accuracy, and efficiency in quantifying animal behavior. However, these methods frequently experience tracking drift, where noise-induced jumps in body point estimates compromise reliability. Here, we present the anti-drift pose tracker (ADPT), a transformer-based tool that mitigates tracking drift in behavioral analysis. Extensive experiments across cross-species datasets—including proprietary mouse and monkey recordings and public <italic>Drosophila</italic> and macaque datasets—demonstrate that ADPT significantly reduces drift and surpasses existing models like DeepLabCut and SLEAP in accuracy. Moreover, ADPT achieved 93.16% identification accuracy for 10 unmarked mice and 90.36% accuracy for freely interacting unmarked mice, which can be further refined to 99.72%, enhancing both anti-drift performance and pose estimation accuracy in social interactions. With its end-to-end design, ADPT is computationally efficient and suitable for real-time analysis, offering a robust solution for reproducible animal behavior studies. The ADPT code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT">https://github.com/tangguoling/ADPT</ext-link>.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multi-animal pose estimation</kwd><kwd>deep learning</kwd><kwd>animal behavior analysis</kwd><kwd>social behavior analysis</kwd><kwd>animal pose estimation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Rhesus macaque</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32222036</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Pengfei</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Research Fund for International Senior Scientists</institution></institution-wrap></funding-source><award-id>T2250710685</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Pengfei</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>STI2030-Major Projects</institution></institution-wrap></funding-source><award-id>2021ZD0203900</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Pengfei</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100017610</institution-id><institution>Shenzhen Science and Technology Innovation Program</institution></institution-wrap></funding-source><award-id>2022410129</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Quanying</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>ADPT enhances the robustness of animal pose estimation by minimizing keypoint tracking drift, enabling more accurate multi-animal tracking and facilitating refined behavioral analysis across species.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animal behavior is a complex and dynamic phenomenon that is shaped by a wide range of factors, including environment, genetics, diseases, cognitive states, and social interactions (<xref ref-type="bibr" rid="bib40">Robinson et al., 2008</xref>). Understanding the underlying mechanisms and neural correlates of animal behaviors requires accurate and detailed pose tracking as they move freely (<xref ref-type="bibr" rid="bib38">Pereira et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Krakauer et al., 2017</xref>). Recently, deep learning-based tools such as DeepLabCut, SLEAP, and DeepPoseKit have offered the feasibility of automatically quantifying complex freely moving animal behaviors from videos recorded by contactless cameras (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib11">Graving et al., 2019</xref>). Nevertheless, these deep learning methods are susceptible to uncertainty and noise interference, leading to tracking drift due to errors in the detection of one or more keypoints, in the estimated keypoint dynamics (<xref ref-type="bibr" rid="bib58">Weinreb et al., 2024</xref>; <xref ref-type="bibr" rid="bib16">Hsu and Yttri, 2021</xref>; <xref ref-type="bibr" rid="bib28">Lonini et al., 2022</xref>). Such drift in keypoints estimates can broadly affect subsequent animal behavior statistics and downstream tasks, such as behavior classification, individual identification, and social behavior clustering (<xref ref-type="bibr" rid="bib45">Sheppard et al., 2022</xref>; <xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>). It severely jeopardizes the reliability and repeatability of ethological studies. Thus, there is an urgent need for an anti-drift pose tracking tool for animal behavior analysis.</p><p>Tracking drift of pose estimation, occurring at the upstream behavioral analysis, generally hinders all downstream behavior-related studies. For example, animal gait analysis relies on accurate tracking of limbs and paws (<xref ref-type="bibr" rid="bib45">Sheppard et al., 2022</xref>), and behavioral classification relies on the dynamics of body keypoints (<xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Han et al., 2022</xref>). So far, deep learning pose estimation has not achieved the reliability of classical kinematic analysis, which often involves post-processing in real-world applications (<xref ref-type="bibr" rid="bib36">Niknejad et al., 2023</xref>; <xref ref-type="bibr" rid="bib2">Aljovic et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Weinreb et al., 2024</xref>). One major reason is tracking drift. The drifted keypoints may be unsystematically distributed within each predefined behavior class, misleading the decision boundaries of the behavior class, thereby reducing the performance of supervised behavior classification <xref ref-type="bibr" rid="bib10">Gabriel et al., 2022</xref> or unsupervised behavior representation (<xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>). Even the state-of-the-art (SOTA) deep learning methods such as DeepLabCut and SLEAP have no effective strategies to avoid the tracking drift (<xref ref-type="bibr" rid="bib58">Weinreb et al., 2024</xref>; <xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib11">Graving et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Lauer et al., 2022</xref>). Inherited from the tracking drifts, the inaccuracy of pose estimation, gait analysis, and behavioral classification may result in wrong behavioral discoveries, such as those investigating behavioral correlates of genes, neural circuits, and neuropsychiatric diseases (<xref ref-type="bibr" rid="bib45">Sheppard et al., 2022</xref>; <xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Han et al., 2022</xref>). These concerns, along with issues related to the safety of deep learning tools, have slowed the widespread application of deep learning-based methods in behavioral analysis and limited the development of ethology.</p><p>There are three strategies to eliminate tracking drift in current SOTA methods of animal pose estimation. The first strategy is human refinement or human in the loop (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>). DeepLabCut (DLC) and SLEAP both embed a user interface to allow humans to exclude and rectify outliers frame by frame (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>). Although it would be the golden criterion to reduce the tracking drift, this strategy restricts the efficiency of the biological experiment when the human faces millions of drifted frames. The second strategy is signal processing filters such as median filter and low pass filter (<xref ref-type="bibr" rid="bib46">Stenum et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Luxem et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Weinreb et al., 2024</xref>; <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>; <xref ref-type="bibr" rid="bib23">Li and Lee, 2021</xref>). They can efficiently remove most of the drifted points without human intervention, but they will also remove the subtle behaviors with high-frequency features such as self-grooming in autism mouse models (<xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>) or tremor in animal models of Parkinson’s disease (<xref ref-type="bibr" rid="bib3">Baker et al., 2022</xref>). The third strategy is fitting the drifted frames using linear dynamic models such as Keypoint-Moseq (<xref ref-type="bibr" rid="bib58">Weinreb et al., 2024</xref>) and adaptive Kalman filter (<xref ref-type="bibr" rid="bib18">Huang et al., 2022</xref>). They can reduce the drift and maintain the high-frequency behaviors at the same time. Nevertheless, the performance of these models would drop sharply when processing continuous and long-duration drifted frames. These three strategies are only expedient to reduce tracking drift after pose estimation, whose performances are also restricted by the tracking accuracy of raw frames. Therefore, the elimination of tracking drift should be tackled from the beginning of the deep learning pose estimation step.</p><p>The structure design of the artificial neural network (ANN) is the first step to correct tracking drift. DeepLabCut, SLEAP, and DeepPoseKit all take the convolutional neural network (CNN) as the main component of pose estimation ANNs, which is the core problem causing tracking drift (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib11">Graving et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Lauer et al., 2022</xref>). The limited working memory of the CNN makes it easy to be influenced by the content-independent parameters to predict the wrong locations of keypoints and finally cause tracking drift (<xref ref-type="bibr" rid="bib63">Yang et al., 2021</xref>). To avoid this drawback, the Transformer becomes a better option to construct pose estimation ANNs because it is more efficient to capture global dependent features of images (<xref ref-type="bibr" rid="bib63">Yang et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Stoffl et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Xu and Zhang, 2022</xref>). Although Transformer-based ANNs have achieved new SOTA in lots of human pose estimation datasets, it is rarely applied in animal pose estimation. Different from human poses, animal poses have more indistinct body structures because they are covered by furs (<xref ref-type="bibr" rid="bib56">Vidal et al., 2021</xref>). In addition, the well-annotated animal pose datasets are not abundant enough to cover various experiment settings. Experimenters always need to make customized datasets for their specific applications (<xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>). Therefore, the application of the Transformer to reduce tracking drift in the animal pose estimation task still needs an elaborate design of ANN structures.</p><p>To import the Transformer to overcome the tracking drift of animals, we designed an ADPT following the characteristics of animal behavior data. CNN and Transformer are cascaded with skip connections to capture subtle animal appearance features from only hundreds of labeled frames (<xref ref-type="bibr" rid="bib15">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">LeCun et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Vaswani et al., 2017</xref>). This structure design makes ADPT show significantly fewer tracking drifts than (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>). The effect of anti-drift of ADPT is universally validated in the public datasets and our customized datasets including <italic>Drosophilas</italic>, mice, and macaques, which demonstrates that ADPT is robust in broad application scenarios cross-species (<xref ref-type="bibr" rid="bib37">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>). ADPT also achieves robust pose estimation and identity recognition of free-interactive mice combined with a mix-up dataset generation strategy. The results of markerless identity recognition show that the feature extraction of ADPT is reliable enough to cover both multi-animal pose estimation and identity recognition tasks, which are more difficult than the single-animal pose estimation task (<xref ref-type="bibr" rid="bib1">Agezo and Berman, 2022</xref>; <xref ref-type="bibr" rid="bib21">Lauer et al., 2022</xref>; <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>). It reduces the computational time cost and increases the throughput of behavioral data processing because ADPT does not need a multi-stage neural network such as (SIPEC <xref ref-type="bibr" rid="bib31">Marks et al., 2022</xref> or Social Behavior Atlas <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>). Together, ADPT would be an accessible tool to reduce the pose tracking shift across species from the upstream of behavior analysis. ADPT has the potential to improve the reliability of computational ethology-based biological studies.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Anti-drift pose tracker</title><p>Existing deep learning-based methods often produce some unreliable pose estimation, such as interference caused by similar objects, keypoint drift, and failures of body part detection (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To clarify these errors, we use ‘track’ or ‘tracking’ to refer to the tracking of all body points or poses of an individual, and ‘detect’ or ‘detection’ when referring to specific keypoints. These estimation errors largely compromise the robustness of pose estimation in freely behaving animals, which can affect the statistical results of behavioral analyses and sometimes even lead to erroneous scientific findings. In this study, we present a reliable animal behavioral analysis tool, called the ADPT. ADPT can effectively eliminate estimated drifts (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). ADPT is a heatmap-based pose estimation network that inferences input images to confidence heatmap, location refinement, and low-resolution semantic segmentation (LRSS) (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In the network architecture of ADPT (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), we utilize the convolutional structure to extract local information on the one hand, and the transformer attention mechanism to learn the long-term global dependencies on the other hand. Compared with purely attention-based network structures (such as ViT <xref ref-type="bibr" rid="bib63">Yang et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Stoffl et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Xu and Zhang, 2022</xref>), our CNN-transformer structure can significantly reduces the number of model parameters and, therefore, requires fewer training data samples. It is particularly suitable for data-limited applications such as animal behavior analysis.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Anti-drift pose tracker (ADPT).</title><p>(<bold>A</bold>) Three examples of drifts in deep learning-based animal behavioral analysis. Similar object disturbance means that the object similar to a specific body part misleads the deep learning-based methods. Inexplicable keypoint drift is caused by the high confidence score predicted on the wrong place by the network. Failure to detect the keypoint is probably caused by the predicted low confidence score. (<bold>B</bold>) The anti-drift effects of ADPT. (<bold>C</bold>) The general workflow of ADPT. The network is trained to predict confidence heatmap, low-resolution semantic segmentation (LRSS), and location refinement. (<bold>D</bold>) The network architecture of ADPT.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Customized behavioral videos for testing ADPT</title><p>The identification of drifting keypoints relies heavily on videos generated during inference or visualized coordinates. Yet there is no publicly available video dataset specifically designed for anti-drift evaluation. To fill this gap, we collected behavioral data from mice and monkeys (see <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>, <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>). For single animal pose estimation, we recorded videos from free-moving mice and monkeys with four cameras and then hand-labeled randomly extracted frames. For mice, we labeled these frames with 16 keypoints, including nose, eyes, ears, front limbs, front claws, back, hind limbs, hind claws, root tail, mid tail, and tip tail. For monkeys, we labeled these frames with 17 keypoints, including nose, eyes, ears, shoulders, elbows, hands, hips, knees, and ankle. Given the popularity of mouse behavioral study, mice served as our primary subjects for evaluation, with videos obtained from four different perspectives involving four distinct individuals. Each mouse video spanned 15 min. The training dataset comprised 440 randomly extracted images from these videos and other collected videos (training:validation = 95%:5%). Monkey videos, on the other hand, encompassed eight different viewpoints, featuring multiple individuals, from which a 30 min video was used for performance evaluation. The training set consisted of 3488 randomly sampled images (training:validation = 95%:5%). For social or multi-animal pose estimation, we recorded 10 min videos of freely socializing mice in a homecage from three different perspectives. We manually labeled 1200 images for training and validation (training = 95%:5%) and also labeled the back locations of two mice during the first minute to evaluate tracking accuracy. Using our dataset, we trained ADPT, DeepLabCut, and SLEAP models, separately, to detect body keypoints from behavioral videos. The behavioral data is available via <ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT/blob/main/data/link.md">https://github.com/tangguoling/ADPT/blob/main/data/link.md</ext-link>.</p></sec><sec id="s2-3"><title>ADPT demonstrates the remarkable anti-drift performance</title><p>Firstly, we visualized the time course of seventeen estimated key body parts from a 1 min segment of mouse videos (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), demonstrating the anti-drift effects of ADPT. In contrast, the other two deep learning-based methods suffer from drift and misses of body parts. Then, we zoomed into the frames of failures in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. The quantitative results of 240 min videos from two mice were shown in <xref ref-type="fig" rid="fig2">Figure 2C and D</xref>. Interestingly, DeepLabCut has almost the same probability of generating drift and misses, while SLEAP was more prone to misses. As presented in <xref ref-type="fig" rid="fig2">Figure 2D</xref>, the tip tail was the most challenging part of the body for both drifts and missed due to the long distance from the tip tail to the rest of the body. For CNN-based methods such as DeepLabCut and SLEAP, learning such long-range tail-body relationships is particularly difficult, while the attention mechanism of ADPT allows it to learn long-range dependencies. Due to frequent occlusion in the video, the left and right claws could be easily missed or drifted. Our model evaluations show that ADPT has significantly lower root mean squared errors compared to SLEAP and achieves comparable or improved accuracy compared to DeepLabCut (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), suggesting that ADPT can reliably detect the hind claws, offering a potential tool for gait analysis and tail-related behavior paradigms.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Analysis of anti-drift pose trackers (ADPT’s) anti-drift performance in a mouse dataset collected by our lab.</title><p>(<bold>A</bold>) The time course of the y-axis position of sixteen body parts extracted from a 1 min video using ADPT, DeepLabCut, and SLEAP tools. It showed that ADPT successfully detected all 16 body parts of a mouse, whereas DeepLabCut and SLEAP encountered inexplicable tracking drifts. (<bold>B</bold>) Two anti-drift examples from ADPT, where the tail was drifted by DeepLabCut and the hind claw failed to detect by SLEAP. (<bold>C</bold>) Overall percentage of tracking drift and failing to detect (miss) frames from three methods. ADPT demonstrated a significantly lower drift percentage than other methods. (<bold>D</bold>) The percentage of frames with tracking drift (left) and failing to detect (right). Drifts were mainly from the top four body parts, including the tip tail, the left and the right hind claws, and the middle tail. (<bold>E</bold>) The averaged RMSE across all body parts (left) and RMSE of the top four body parts with drifts (right). ADPT achieved the smallest RMSE than other two tools when thresholded at 0.2. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001. RMSE: root mean square error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig2-v1.tif"/></fig></sec><sec id="s2-4"><title>Anti-drift performance remains consistent irrespective of the video background and individual animals</title><p>Any measuring tool that exhibits biased measurement errors towards specific subjects introduces inaccuracies in its assessments. For example, if the model accurately estimates the posture of mouse A but experiences greater posture drift in estimating mouse B, this discrepancy leads to measurement errors, impacting subsequent behavioral analyses. Hence, to evaluate the independence of posture estimation’s anti-drift effect concerning individual animals or background factors, we conducted one-way ANOVA on the tracking results. We trained ADPT, DeepLabCut, and SLEAP five times each and applied these models to infer behavioral videos across different individuals and video backgrounds. Firstly, we compared ADPT’s anti-drift performance across different individuals and backgrounds. The results showed that ADPT exhibited significantly lower drift percentages than the other two methods across different individuals and video backgrounds (<xref ref-type="fig" rid="fig3">Figure 3A and C</xref>). Then, the inference results were grouped based on individual animals and video backgrounds, respectively, for five individual one-way ANOVA analyses. The results of these five ANOVA analyses are presented in <xref ref-type="fig" rid="fig3">Figure 3B and D</xref>. Our analyses revealed that drift occurrences were more significantly affected by backgrounds in DeepLabCut, while individual variations had a more significant impact on SLEAP. However, ADPT showed slight susceptibility to background influence. Consequently, we assert that in comparison to DeepLabCut and SLEAP, ADPT only demonstrates a lesser susceptibility to the influence of individual animals and background factors. This resilience significantly mitigates biases in tracking results. ADPT’s ability to generate fewer biases due to individual or background factors during inference holds promise for achieving better consistency in downstream behavioral analyses. This analysis also underscores the importance, when using ADPT, of minimizing background variations, ideally maintaining consistent backgrounds.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Anti-drift performance cross background and individual, where the percentage of frames includes two types of drift phenomena: drift and miss.</title><p>(<bold>A</bold>) The overall cross-individual anti-drift performance of anti-drift pose tracker (ADPT) and the other methods. The drift percentage of ADPT is significant lower than other methods. (<bold>B</bold>) After training the model 5 times on the dataset shuffle, the cross-individual drift percentage for each shuffle was analysed using one-way ANOVA. The ANOVA results revealed that there are differences in the inference results of the SLEAP model among individual, and there were no differences for ADPT or DeepLabCut. (<bold>C</bold>) The overall cross-background anti-drift performance of ADPT and the other methods. The drift percentage of ADPT is significant lower than other methods. (<bold>D</bold>) The cross-background drift percentage for each shuffle was analysed using one-way ANOVA. The ANOVA results revealed that there are slight differences in the inference results of the DeepLabCut model among individual, and there were no differences for ADPT or SLEAP. ns.: no significant, *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig3-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-95709-fig3-video1.mp4" id="fig3video1"><label>Figure 3—video 1.</label><caption><title>Video file containing clips of mouse behavior videos.</title></caption></media></fig-group></sec><sec id="s2-5"><title>Cross-species anti-drift capability of ADPT is reliable</title><p>While ADPT has demonstrated exceptional anti-drift abilities in mice, numerous other animal models are employed in behavioral studies. To validate the robustness of ADPT in tracking different species, particularly those posing significant tracking challenges, we selected cynomolgus monkey as a species known for its complexities in tracking. We utilized the models to track a video in which both humans and monkeys appeared simultaneously, presenting similar objects in the scene. Visualizing the keypoint tracking results from 1 min time course featuring both entities allowed us to showcase the anti-drift efficacy of ADPT (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In contrast, the other two methods exhibited tracking failures when humans were present, as illustrated in the zoomed-in frames of failure in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. When humans were present, both DeepLabCut and SLEAP exhibited instances of tracking drift, whereas ADPT remained unaffected by the presence of similar objects. Similarly, we evaluated the performance of ‘drift’ and ‘miss’ for various body parts in this scenario. We observed that ADPT consistently outperformed the other two methods overall (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). However, given the more complex experimental setup and animal movements, ADPT exhibited slight instances of drift and ‘fail to detect’ effects.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Analysis of anti-drift pose trackers (ADPT’s) anti-drift performance on monkey data, showing the cross species anti-drift ability.</title><p>(<bold>A</bold>) The time course of the <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>-axis position of sixteen body parts extracted from a 1 min video using ADPT, DeepLabCut, and SLEAP tools. It showed that ADPT successfully detected all 17 body parts of a monkey, while the other two methods encountered tracking drift because of the appearance of humans. (<bold>B</bold>) DeepLabCut and SLEAP both mistakenly located the monkey’s eyes on humans when they appeared, while ADPT can achieve robust tracking. (<bold>C</bold>, <bold>D</bold>) The percentage of frames with tracking drift and failing to detect (miss). The occurrence of drift was mainly concentrated in the limbs, because the appearance of humans.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig4-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-95709-fig4-video1.mp4" id="fig4video1"><label>Figure 4—video 1.</label><caption><title>Video file containing a clip of monkey behavior video.</title></caption></media></fig-group><p>Consequently, our findings suggest that our approach demonstrates remarkable anti-drift performance, cross-individual and cross-view capabilities. Notably, our anti-drift performance was more pronounced in consistent experimental scenarios. Our experiments with monkeys substantiated our method’s profound cross-species anti-drift capability, emphasizing its significance in behavioral studies involving diverse animal models.</p></sec><sec id="s2-6"><title>Public datasets confirm the outperformance of ADPT in precision and practicality</title><p>In adddition to evaluating ADPT’s performance on behavioral study videos, we recognized the significance of image datasets as benchmarks for assessing pose estimation effectiveness. Thus, to comprehensively evaluate the generalizability of ADTP performance to animals in skeletal complexity and body size, and the background complexity of videos, we used two public datasets, a single fly dataset (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; <xref ref-type="bibr" rid="bib37">Pereira et al., 2019</xref>), and a macaque OMS_Datase (<xref ref-type="bibr" rid="bib4">Bala et al., 2020</xref>). The single fly dataset contains 1500 annotated frames of 32-node skeleton fly. To ensure a fair comparison, we followed the same dataset split strategy and data augmentation strategy described in <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>. The evaluation metric used was mean Average Precision (mAP), which measures the accuracy of keypoint localization for all body parts, following the protocol established in <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>. On the other hand, The OMS_Dataset (<xref ref-type="bibr" rid="bib4">Bala et al., 2020</xref>) is a large database of annotated macaque images (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). To evaluate the performance of our methods, we randomly selected 5000 images out of 195,228 images from this dataset and resized them to 368×368 resolution. We split the dataset into 40% training data and 60% validation data. We employed the same strategy used in the default configuration of DeepLabCut toolbox to augment the data. The average distance (root square mean errors, RMSE) between the ground truth and predicted keypoints and the mAP were used as evaluation metrics. <xref ref-type="fig" rid="fig5">Figure 5A and F</xref> presented several examples annotated by ADPT on these two datasets, respectively. Furthermore, to verify the practicality of ADPT, we also evaluated the amount of required training data and the inference speed of the model. Finally, we evaluated the scalability of ADPT on the StanfordExtra dataset (<xref ref-type="bibr" rid="bib5">Biggs et al., 2020</xref>). Our results demonstrated the capability of ADPT on non-laboratory dogs (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>). These evaluations underscore ADPT’s versatility, showcasing its robustness and accuracy in diverse animal contexts, thereby affirming its potential as a highly adaptable tool for comprehensive behavioral studies.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Results of public datasets evaluation.</title><p>(<bold>A</bold>) Samples of prediction on single fly dataset. (<bold>B</bold>) Mean average precision (mAP) on fly dataset, where anti-drift pose tracker (ADPT) achieved average of 92.8% accuracy (the best model achieved 93.27%). (<bold>C</bold>) Low-resolution semantic segmentation (LRSS) improved the average accuracy by 0.3% on a single fly dataset. (<bold>D</bold>) Relationship between annotated image and accuracy of ADPT on fly dataset where ADPT achieved acceptable performance with only 350 annotated images in a simple laboratory environment. Points indicate the validation accuracy of model training on specific number of labels dataset. (<bold>E</bold>) Transformer improved the average accuracy by 0.4% on a single fly dataset. (<bold>F</bold>) Samples of prediction on OMS_Dataset. (<bold>G</bold>) Root mean square error (RMSE) on OMS_Dataset, where ADPT achieved smaller root square mean error (RMSE) than SLEAP when threshold = 0.2, and smaller than DeepLabCut when threshold = 0.6. p-value, **: 0.001862, ns.: 0.243472, ***8.700e-06. (<bold>H</bold>) RMSE comparison on hip and tail of OMS_Dataset. p-value, ***0.000561, Hip ns.:0.023766, Tail ns.:0.336642, *: 0.035782.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Picture examples of dog pose estimation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig5-figsupp1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-95709-fig5-video1.mp4" id="fig5video1"><label>Figure 5—video 1.</label><caption><title>Video examples of dog pose estimation.</title></caption></media></fig-group></sec><sec id="s2-7"><title>ADPT offers higher tracking accuracy than existing SOTA methods</title><p>The tracking performance of ADPT was compared with the existing SOTA methods, such as DeepLabCut and SLEAP (<xref ref-type="fig" rid="fig5">Figure 5B, G and H</xref>). On the single-fly dataset, ADPT exceled with an average mAP of 92.83%, surpassing both DeepLabCut and SLEAP (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). On the OMS Dataset, ADPT exhibited significant advantages in terms of mAP, RMSE (threshold = 0.2), RMSE (threshold = 0.6), achieving values of 30.9%, 8.32, and 6.25, which were significantly superior to SLEAP, and slightly outperforming DeepLabCut when the threshold set as 0.6 (<xref ref-type="fig" rid="fig5">Figure 5G</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Moreover, we further examined the detection of macaque hip and tail on OMS_Dataset (<xref ref-type="fig" rid="fig5">Figure 5H</xref>). We found that ADPT’s tracking performance of tail is better than DeepLabCut and SLEAP, while the hip tracking is equivalent to DeepLabCut and better than SLEAP. This further demonstrates the superiority of ADPT in tail-related behavior paradigms. By conducting evaluations on these diverse datasets, we aimed to assess the robustness and generalizability of our methods across more different animal species, pose complexities, and environmental conditions. The results obtained from these evaluations provide solid proof of the performance and potential of our methods for single-animal pose estimation.</p><p>Since annotating behavioral data is tedious, a deep learning-based method that does not require large amounts of annotated data is crucial. Here, we studied how the accuracy of ADPT changes with the amount of annotated data. Notably, ADPT achieved acceptable performance using only 350 annotated images (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), indicating that ADPT is data efficient.</p></sec><sec id="s2-8"><title>ADPT’s fast inference enables real-time applications</title><p>Here, we evaluate the inference speed of ADPT. We compared it with DeepLabCut and SLEAP on mouse videos at 1288x964 resolution. Our method exhibited an impressive prediction speed of 90±4 frames per second (fps), faster than DeepLabCut (44±2 fps) and equivalent to SLEAP (106±4 fps). These results highlighted the efficient inference capabilities of ADPT, which is crucial for real-time applications and the analysis of large-scale behavioral data.</p></sec><sec id="s2-9"><title>LRSS and transformer help improve tracking accuracy</title><p>To examine the contribution of the low-resolution semantic segmentation (LRSS) and the transformer architecture to ADPT, we conducted two ablation studies using the fly dataset. We compared multiple variants to uncover the impacts of the LRSS module and the transformer module on pose estimation performance. First, we explored the influence of LRSS by comparing the performance of the complete ADPT with the one removed LRSS. As shown in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, LRSS module can improve the average accuracy by 0.2%. Moreover, to assess the role of transformer architecture, we conducted a comparative analysis between the complete ADPT with the transformer and a variant of the model where the transformer was removed. As shown in <xref ref-type="fig" rid="fig5">Figure 5E</xref>, the transformer improved the average accuracy by 0.4%, suggesting the benefits of the transformer architecture in pose estimation.</p></sec><sec id="s2-10"><title>ADPT can accurately track the non-laboratory dog</title><p>To test the generalizability of our approach beyond laboratory-behavior animals, we applied ADPT to the keypoint detection task for the non-laboratory dog. The dataset is from <xref ref-type="bibr" rid="bib5">Biggs et al., 2020</xref>. We randomly divided the dataset into 85% and 15% training and validation data. ADPT was instantiated with the same network architecture for laboratory animal pose estimation, showcasing the versatility of ADPT. We followed <xref ref-type="bibr" rid="bib5">Biggs et al., 2020</xref> and used Percentage of Correct Keypoints (PCK) metric to evaluate the accuracy of keypoint detection. The results showed that ADPT achieved an average 86.54% PCK score (legs: 85.54%, tail: 79.89%, ears: 88.61%, face: 95%). Examples of identified keypoints of dogs were shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>; <xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>. These results supported the flexibility of ADPT in different animal species and potentially more challenging real-world scenarios.</p></sec><sec id="s2-11"><title>ADPT can be adapted for end-to-end pose estimation and identification of freely social animals</title><p>We adapted ADPT to end-to-end tracking of the social interacting mice with similar appearances. To this end, we added additional heads after feature concatenation and utilized LRSS to confirm the identities of the mice. We generated a multi-animal dataset for social tracking by mixing up two labeled frames from single mouse videos (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The evaluation of our social tracking capability was performed by visualizing the predicted video data (see <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref> and <xref ref-type="video" rid="fig7video2">Figure 7—video 2</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Illustration for mix-up social animal dataset generation.</title><p>(<bold>A</bold>) Frames originating from different videos and corresponding background. (<bold>B</bold>) Mix-up image. (<bold>C</bold>) Represents schematic diagrams illustrating the keypoint generated from single animal pose estimation of anti-drift pose tracker (ADPT). (<bold>D</bold>) Represents an augmented mix-up image. (<bold>E</bold>) Represents schematic diagrams of augmented annotation. (<bold>F</bold>) Represents augmented keypoints. (<bold>G</bold>) Represents augmented low-resolution semantic segmentation (LRSS). (<bold>H</bold>) Represents schematic diagrams of augmented Body Affinity Fields (BAF), inspired by Part Affinity Fileds (<xref ref-type="bibr" rid="bib7">Cao et al., 2021</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig6-v1.tif"/></fig><p>Prior to social tracking, we evaluated identity-tracking accuracy using a dataset consisting of 10 mouse videos of different individuals. The overall workflow of these extended applications is depicted in <xref ref-type="fig" rid="fig7">Figure 7</xref>. Initially, we utilized a variant of ADPT (empowering LRSS with identity information) for simultaneous animal pose estimation and identity synchronized tracking. For each frame, identity recognition was based on the LRSS output by ADPT (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Although the appearance of the mice is very similar, our experimental results showcased a remarkable 93.16% accuracy in identity recognition (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). This approach demonstrates LRSS’s capability to record individual identities like semantic segmentation masks. The outcomes, showcased in <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>, manifested synchronized tracking of identity and pose estimation.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Applications of anti-drift pose tracker (ADPT) for multi-animal pose tracking.</title><p>(<bold>A</bold>) Left: The pipeline for the multi-animal identity-pose tracking task. (<bold>B</bold>) Confusion matrix of the 10 mice classification (accuracy = 93.16%). (<bold>C</bold>) Social mice tracking pipeline with identification accuracy of 99.72%.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig7-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-95709-fig7-video1.mp4" id="fig7video1"><label>Figure 7—video 1.</label><caption><title>Video file demonstrating single animal pose estimation and identity synchronized tracking.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-95709-fig7-video2.mp4" id="fig7video2"><label>Figure 7—video 2.</label><caption><title>Video file demonstrating social animal pose estimation and identity synchronized tracking.</title></caption></media></fig-group><p>Subsequently, we tested the tracking performance with free-social animals. Inspired by Part Affinity Fileds (<xref ref-type="bibr" rid="bib7">Cao et al., 2021</xref>), we created Body Affinity Fileds (BAF) to help distinguish different individuals. BAF and LRSS were used together to identify individuals. In the first scenario, We trained ADPT on the Mix-up social animal dataset and employed it to predict 1 min free-social video of mice with similar appearance. Without additional temporal post-processing, ADPT achieved a 90.36% accuracy in identity recognition, as referenced in <xref ref-type="video" rid="fig7video2">Figure 7—video 2</xref>. Following temporal identity correction, ADPT remarkably achieved a 99.72% accuracy in identity recognition (<xref ref-type="fig" rid="fig7">Figure 7C</xref>), as shown in <xref ref-type="video" rid="fig7video2">Figure 7—video 2</xref>. The pose estimation accuracy was acceptable, but we recognized that there are detection errors of tail or tracking errors when animals are very closed sometimes which may be due to the lack of real-world training data.</p><p>In the second scenario, we trained ADPT on manual labeled homecage social mice dataset, a set of real-world training data (<xref ref-type="fig" rid="fig8">Figure 8A</xref>.) and used it to predict a 1 min free-social video. We evaluated anti-drift performance and found that ADPT outperformed Deeplabcut and SLEAP, achieving 15% improvement of pose estimation accuracy (<xref ref-type="fig" rid="fig8">Figure 8D and E</xref>) and almost 5–10 times improvement of tracking accuracy (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). <xref ref-type="fig" rid="fig8">Figure 8B</xref> illustrates ADPT’s prediction of the x-coordinates of different mice, demonstrating less keypoint drift. In <xref ref-type="fig" rid="fig8">Figure 8C</xref>, we compare the anti-drift performance of the raw predictions from the three methods, highlighting ADPT’s superior tracking performance compared to DeepLabCut and SLEAP. Furthermore, we assessed pose estimation accuracy between ADPT and DeepLabCut/SLEAP, showing that ADPT has better pose estimation accuracy than both SLEAP and DLC (<xref ref-type="fig" rid="fig8">Figure 8D and E</xref>). Lastly, an ablation study confirmed that BAF improves pose estimation accuracy (<xref ref-type="fig" rid="fig8">Figure 8F</xref>). The tracking result of this scenario was shown in <xref ref-type="video" rid="fig8video1">Figure 8—video 1</xref>.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Evaluation of anti-drift pose tracker (ADPT) for homecage social mice scenario.</title><p>(<bold>A</bold>) Illustration of homecage social mice dataset. (<bold>B</bold>) Filtered predicted back locations of different mice by ADPT. (<bold>C</bold>) Comparison of different methods and manual labels. We trained each model three times, and this figure presents the results from one of those training sessions. We calculated the average root square mean error (RMSE) between predictions and manual labels, demonstrating that ADPT achieved an average RMSE of 15.8±0.59 pixels, while DeepLabCut (DLC) and SLEAP recorded RMSEs of 113.19±42.75 pixels and 94.76±1.95 pixels, respectively. (<bold>D</bold>) Pose estimation accuracy comparison between ADPT and DLC based on the DLC evaluation metric. ADPT achieved an accuracy of 6.35±0.14 pixels across all body parts of the mice, while DLC reached 7.49±0.2 pixels. (<bold>E</bold>) Pose estimation accuracy comparison between ADPT and SLEAP using the SLEAP evaluation metric. ADPT achieved 8.33±0.19 pixels across all body parts of the mice, compared to SLEAP’s 9.82±0.57 pixels. (<bold>F</bold>) Body affinity fields (BAF) improved pose estimation accuracy by 0.4 pixels under the SLEAP evaluation metric.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95709-fig8-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-95709-fig8-video1.mp4" id="fig8video1"><label>Figure 8—video 1.</label><caption><title>Video file demonstrating homecage social mice pose estimation and identity synchronized tracking.</title></caption></media></fig-group><p>In addition to mice, we evaluated the pose estimation accuracy of ADPT on the marmoset dataset, a publicly available resource (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>). We adhered to the default marmoset configuration of DeepLabCut, randomly dividing the dataset into training and validation sets while employing the same data augmentation strategy. Under the evaluation metrics used by DeepLabCut, ADPT achieved an average accuracy of 6.14±0.19 pixels, whereas DeepLabCut reached 6.63±0.09 pixels. Additionally, when assessed using SLEAP evaluation metrics, ADPT achieved an average accuracy of 7.02±0.11 pixels, compared to SLEAP’s 11.39±0.31 pixels.</p><p>Together, these different applications demonstrate the versatility of ADPT, ranging from single animal pose estimation to complex situations involving social interactions. ADPT’s versatility and adaptability paves the way for comprehensive behavioral studies.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we have presented ADPT, a transformer-based pose tracker, to address the pose drift problem in animal pose estimation. The core of ADPT is the elaborate combination of the convolutional network <xref ref-type="bibr" rid="bib22">LeCun et al., 2015</xref> and transformer layers (<xref ref-type="bibr" rid="bib55">Vaswani et al., 2017</xref>), with the goal of capturing both local details and global context. This architecture helps ADPT achieve a more reliable feature extraction on animal objects, resulting in higher accuracy in tracking the poses frame by frame with less drifts or misses, compared to <xref ref-type="bibr" rid="bib21">Lauer et al., 2022</xref>; <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>. In addition, we presented the procedure for the data generation of Mix-up social animals, which is convenient and effective for exponentially synthesizing new data to improve the performance of ADPT. We showed that ADPT can be used for multi-animal pose estimation and identification. These two tasks were considered much more difficult than single-animal pose estimation (<xref ref-type="bibr" rid="bib21">Lauer et al., 2022</xref>). The end-to-end network structure of ADPT only needs to calculate one model loss so it is more computationally efficient than the multi-stage methods such as SIPEC and Social Behavior Atlas (<xref ref-type="bibr" rid="bib31">Marks et al., 2022</xref>; <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>). These advances show that ADPT is an accurate, universal, and efficient method, suggesting broader application scenarios in neuroscience, genetics, and drug discovery. Now, the toolbox of ADPT has also been released at (<xref ref-type="bibr" rid="bib51">Tang and Sun, 2025</xref>).</p><p>As the higher resolution of microscopy promotes the discovery of biological microstructures, the higher precision of animal pose estimation helps to detect subtle behavior structures and patterns, advancing ethology research. Behavior structures have been proven to be the signatures, fingerprints, and biomarkers to indicate disease developments (<xref ref-type="bibr" rid="bib6">Bohic et al., 2023</xref>; <xref ref-type="bibr" rid="bib12">Gschwind et al., 2023</xref>), genetic mutations (<xref ref-type="bibr" rid="bib27">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>; <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>), and drug effects (<xref ref-type="bibr" rid="bib60">Wiltschko et al., 2020</xref>; <xref ref-type="bibr" rid="bib14">Han et al., 2024</xref>). Although these studies refine the behavior to module level (<xref ref-type="bibr" rid="bib59">Wiltschko et al., 2015</xref>), this spatiotemporal scale of behavior structures is not sufficient to support finer animal studies such as decoding millisecond neural recordings with un-drifted poses (<xref ref-type="bibr" rid="bib44">Schneider et al., 2023</xref>). Therefore, improving the accuracy and reliability of animal pose estimation is of high need for behavioral studies. ADPT provides such a tool for animal pose estimation.</p><p>ADPT enables a wide range of downstream applications, for instance, aligning behavioral manifold from keypoint dynamics with the neural manifold from large-scale neural recordings (<xref ref-type="bibr" rid="bib54">Urai et al., 2022</xref>). Recent advances in neural decoding of speech <xref ref-type="bibr" rid="bib25">Li et al., 2023</xref>; <xref ref-type="bibr" rid="bib33">Metzger et al., 2023</xref> and vision <xref ref-type="bibr" rid="bib44">Schneider et al., 2023</xref>; <xref ref-type="bibr" rid="bib49">Takagi and Nishimoto, 2023</xref> have achieved incredible performance, but the accurate neural decoding of poses is still an existing problem. ADPT can quantify the poses of animals to reach a high resolution like the microphone for speech acquisition or visual pixels, which is an improvement from the aspect of behavior data acquisition. The second application is the gait analysis for 3D movements. Non-human primates are not restricted to moving on the ground, and the 3D gait would reflect their abnormal state after modeling treatment (<xref ref-type="bibr" rid="bib26">Liang et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Thota and Alberts, 2013</xref>). ADPT decreases the pose drift caused by body occlusion of single-view frames, which would reduce the error of 3D gait reconstruction. It also reduces the number of cameras for view angle compensation except for the profound understanding of 3D gait-related disorders (<xref ref-type="bibr" rid="bib4">Bala et al., 2020</xref>). The third application is behavior-based drug screening (<xref ref-type="bibr" rid="bib60">Wiltschko et al., 2020</xref>). Although MoSeq has built up the relationship between behavior syllables and psychoactive drugs (<xref ref-type="bibr" rid="bib59">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Wiltschko et al., 2020</xref>), the resolution of behavior only exists at the syllable level. It is predictable for ADPT to improve the behavior resolution of MoSeq even Keypoint-MoSeq to a finer level to be not limited to the screen of psychoactive drugs (<xref ref-type="bibr" rid="bib59">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Weinreb et al., 2024</xref>). In summary, solving the anti-drift problem from the very beginning of ADPT determines that it has widespread applications.</p><p>One potential improvement of ADPT is the design of positional encoding. With the increase in image size, the positional encoding would occupy more memory of the graphics processing unit. The process of high-resolution videos has to resize the frame to avoid being out of memory, in which the pixel-level information could be missed. Conditional positional encoding would be a possible solution to improve ADPT to face high-resolution frames (<xref ref-type="bibr" rid="bib9">Chu et al., 2021</xref>). Another improvement of ADPT is using a more powerful backbone neural network. To facilitate the comparison between ADPT with other methods, the ResNet50 is used in all of the validation (<xref ref-type="bibr" rid="bib15">He et al., 2016</xref>). Recent advances in the backbone such as (<xref ref-type="bibr" rid="bib61">Xu et al., 2022</xref>) could be the better choice to replace ResNet and improve the performance of ADPT.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>In this section, we first present ADPT method, then introduce the datasets used in each experiment, and finally describe the details of multi-animal experiments.</p><sec id="s4-1"><title>The details of ADPT</title><p>Here, we present the key components and details of ADPT. We also provide the code for ADPT at <ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT/tree/main/code">https://github.com/tangguoling/ADPT/tree/main/code</ext-link> (<xref ref-type="bibr" rid="bib50">Tang, 2025</xref>).</p></sec><sec id="s4-2"><title>The network architecture</title><p>Applying transformer in freely behaving animal pose estimation can help us alleviate keypoint tracking drift. Thus, we created a heatmap-based pose estimation model, called ADPT. The overall structure of the method and network is illustrated in <xref ref-type="fig" rid="fig1">Figure 1C and D</xref>. Initially, ADPT will resize image to a scale (a hyper parameter, the same as global_scale in DLC). Then, the network employs the stack1-2 of the ResNet50 model to extract shallow-level features from the input images. At this stage, the images are extracted into features with a size of one-fourth of their original dimensions. Subsequently, network separately process these features in three branches, compute features at scale of one-fourth, one-eight and one-sixteenth, and generate one-eight scale features using convolution layer or deconvolution layer. Of particular significance is the utilization of the one-sixteenth scale feature, which is input into a transformer module for computation. This large-scale feature’s involvement in the multi-head attention mechanism substantially enhances the model’s ability to capture global relationships within the data. Finally, model concatenates these features by skip connections and compute them using convolution layers to generate output, including keypoint position confidence heatmaps, location refinement maps, low-resolution semantic segmentation map, and body affinity fields map.</p></sec><sec id="s4-3"><title>Low resolution semantic segmentation</title><p>In addition to generating the animal’s skeletal keypoints, we also create a low-resolution semantic segmentation map (LRSS) of the animal. This segmentation map captures the coarse-level information about the different body parts or regions of the animal. By connecting the skeletal keypoints, the model can infer the boundaries, shapes, and identities of these regions. According to keypoints set <italic>kps</italic> of all individuals in frame, the pixel p-value at segmentation map is defined as,<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mtext> on </mml:mtext><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The low-resolution map plays a crucial role in training our model. It allows the model to learn the correlation between the skeletal keypoints and the semantic information of the animal’s body. By incorporating the segmentation map into the training process, the model can better understand the spatial relationships between different keypoints and improve the accuracy and robustness of pose estimation.</p></sec><sec id="s4-4"><title>Network training details</title><p>In our animal pose estimation tasks, we employed specific training configurations to optimize the performance of our models. The following training details were utilized. We trained the models for a total of 190 epochs. Additionally, we included 10 warm-up epochs at the beginning of the training process. The batch size used during training was set to 8. We utilized the <italic>AdamW</italic> optimizer, and the weight decay rate was set to 1e-4. We employed a warmup cosine decay schedule for the learning rate. Initially, the learning rate was warmed up from 1e-5 to 1e-3 over the warm-up epochs. Subsequently, the learning rate gradually decayed to 1e-5 using a cosine decay function. For optimizing the keypoint confidence heatmaps and location refinement maps, we utilized root square error (RMSE) as the loss function. RMSE measures the average squared difference between the predicted and ground truth key points, providing a measure of the accuracy of the model’s predictions. Additionally, for training the low-resolution semantic segmentation map, we used sparse categorical cross-entropy loss, which is suitable for multi-class segmentation tasks. We early stop the training procedure when it reaches a plateau for 30 epochs according to validation loss. These training details were carefully chosen to ensure effective training and optimization of our models for single animal pose estimation. For data augmentation, we followed DeepLabCut augmentation strategy <xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref> in training ADPT, and followed (<xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>) specifically for single fly dataset. The image inputs of ADPT were resized to a size that can be trained on the computer which was defined as ‘global_scale’ in configuration file. For mouse images, it was reduced to half of the original size. For monkey images, it was reduced to 0.8 of the original size. For macaque and fruit flies, there were no resizing, while for dogs, it was resized to 224×224 resolution. For homecage social mice images and marmoset images, there were no resizing.</p><p>The specific values and configurations may vary depending on the dataset, network architecture, and specific requirements of the task.</p></sec><sec id="s4-5"><title>Network implementation</title><p>We implementated ADPT in the Python programming language(python 3.9). We used tensorflow 2.9.1 for all deep learning models. We used imgaug for image and annotation augmentation. We used OpenCV for video reading/writing and matplotlib for image reading. The hardware condition includes RTX4090 GPU, Intel 12,900 K CPU, Samsung 980 Pro hard disk, and 128 GB DDR5 memory. For comparison, we used DeepLabCut 2.2.1 with default configuration during training, in which ‘global_scale’ parameter was adjusted to match with ADPT resizing configuration. Similarly, SLEAP 1.2.9 was used with the baseline_medium_rf.single configuration, adjusting the ‘input scaling’ to align with ADPT’s resizing configuration.</p></sec><sec id="s4-6"><title>Datasets</title><p>To comprehensively evaluate the robust performance of ADPT, we selected datasets consider factors such as skeletal complexity, body size, and background complexity. However, there exists no publicly available video dataset specifically designed for anti-drift evaluation. Therefore, we also collected behavioral video data involving mice and monkeys. We also provide code to transfer DeepLabCut format labeled dataset to our ADPT format dataset, which may allow users to make deeper study toward the past behavioral data. Code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT/blob/main/data/dlc2adpt.py">https://github.com/tangguoling/ADPT/blob/main/data/dlc2adpt.py</ext-link> (<xref ref-type="bibr" rid="bib50">Tang, 2025</xref>). Source data files have also been provided for <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig6">6</xref>—<xref ref-type="fig" rid="fig8">8</xref>, details for accessing which are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT/blob/main/data/link.md">https://github.com/tangguoling/ADPT/blob/main/data/link.md</ext-link>.</p></sec><sec id="s4-7"><title>Mouse dataset</title><p>The mouse dataset is a customized single animal dataset collected by ourselves. We recorded a C57BL/6 mouse freely behaving in an open field from four different views. The dataset contained 440 labeled image in 1288×964 resolution across four different backgrounds and 11 individuals, 16 single mouse videos with the same resolution across 4 different individuals and four backgrounds. Each video spans 15 min.</p></sec><sec id="s4-8"><title>Monkey dataset</title><p>The monkey dataset is a customized single animal dataset collected by ourselves. We recorded a cynomolgus monkey freely behaving in behavioral cage. The dataset contained 3488 labeled image in 640×360 resolution across 8 different backgrounds and multiple individuals, and one specific 30 minutes video in which a monkey and people appeared simultaneously.</p></sec><sec id="s4-9"><title>Single fly dataset</title><p>The single fly dataset is a benchmark dataset used in animal pose estimation (<xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>). It contained 1500 manual labeled frames which was split into 1200 training, 150 validation, and 150 test frames. The fly in the dataset was annotated with 32-node skeleton. Source data files are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/DeepPoseKit-Data/tree/master/datasets/fly">https://github.com/jgraving/DeepPoseKit-Data/tree/master/datasets/fly</ext-link>.</p></sec><sec id="s4-10"><title>OpenMonkeyStudio Dataset</title><p>The OpenMonkeyStudio dataset is a macaque pose estimation dataset, containing 195,228 labeled frames with 13-node skeletons (<xref ref-type="bibr" rid="bib4">Bala et al., 2020</xref>). we randomly selected 5000 images and resized them to 368×368 resolution to evaluate the performance of our methods. We randomly divided this selected dataset into a 40–60% training and validation split. Source data files are available at <ext-link ext-link-type="uri" xlink:href="https://z.umn.edu/OMS_data_link">https://z.umn.edu/OMS_data_link</ext-link>.</p></sec><sec id="s4-11"><title>StanfordExtradataset</title><p>StanfordExtradataset is a large-scale dog dataset with 2D keypoint and silhouette annotations, containing 12,000 images of dogs with 24-node skeletons (<xref ref-type="bibr" rid="bib5">Biggs et al., 2020</xref>). We randomly split the dataset into 85% training and 15% validation. Source data files are available at <ext-link ext-link-type="uri" xlink:href="https://paperswithcode.com/dataset/stanfordextra">https://paperswithcode.com/dataset/stanfordextra</ext-link>.</p></sec><sec id="s4-12"><title>Mouse videos of different individuals</title><p>Video 1288×964 resolution across four different backgrounds and 10 individuals. Each video spans 15 min during which the first 12 min was used for training identity lrss while the rest was used for validation.</p></sec><sec id="s4-13"><title>Free-social mice video</title><p>A 1 min video in 1288×964 resolution of free-social mice.</p></sec><sec id="s4-14"><title>Homecage social mice dataset</title><p>The homecage social mice dataset is a customized animal dataset collected by ourselves. We recorded two markerless C57BL/6 mice freely behaving in a homecage from three different view. The dataset contained 1200 labeled images in 960×540 resolution across 3 different backgrounds and two paired individuals. Each video spans 10 min during which the first 1 min video was use for anti-drift performance evaluation. We manually annotated the position location in the 1 min video every 30 frames.</p></sec><sec id="s4-15"><title>Marmoset</title><p>Marmoset is a dataset released by multi-animal DeepLabCut for marmosets pose estimation, containing 5316 images of marmoset with 15-node skeletons <xref ref-type="bibr" rid="bib21">Lauer et al., 2022</xref>. We resized the images to 368×368 resolution to evaluate the performance of our methods.</p></sec><sec id="s4-16"><title>Mix-up social animal dataset generation</title><p>To address the challenge of acquiring labeled datasets for multi-animal pose estimation, we introduce a novel data augmentation strategy. This strategy involves mixing up a background picture and two labeled frames from single animal videos predicted by single animal model, generating synthetic data with multiple animals. The process is illustrated in <xref ref-type="fig" rid="fig6">Figure 6</xref>, and the algorithm is detailed in Algorithm 1. Initially, we employ the ADPT model to predict keypoint position for two images originating from different videos, resulting in two frame annotation sets of keypoints. Using these frames and the corresponding background image (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), we create a mix-up image, as shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. We utilize two frame annotations to generate Mix-up annotation heatmaps. These heatmaps associate each keypoint with its corresponding location on the mix-up image, as shown in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. For the augmented image as shown in <xref ref-type="fig" rid="fig6">Figure 6D</xref>, we generated augmented annotations as shown in <xref ref-type="fig" rid="fig6">Figure 6E, F</xref> represents augmented keypoints. Importantly, we leverage LRSS to distinguish between animals’ identities, as indicated in the <xref ref-type="fig" rid="fig6">Figure 6G</xref>. Finally, we leverage body affinity fields (BAF) to match the body parts and identity, as indicated in the <xref ref-type="fig" rid="fig6">Figure 6H</xref> in which we set back as the center point.</p></sec><sec id="s4-17"><title>Body affinity fields</title><p>Inspired by PAF, we create Body Affinity Fields for associating body part to instance identity. Considering all individuals in frame, the pixel p-value at BAF map is defined as,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mi>p</mml:mi><mml:mtext> on </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext>for </mml:mtext><mml:mi>i</mml:mi><mml:mtext> in </mml:mtext><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents pixel <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>’s location (x and y coordination), and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents the center location. Combining BAF and LRSS, we can infer pixels identities. We only used this map in social animal tracking.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups" id="AL1"><thead><tr><th align="left" valign="bottom">Algorithm 1. Generation of Mix-up Social Animal Data</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Data</bold>: video1, video2, backgrounds<break/><break/><bold>Result</bold>: Mix-up frame, Mix-up annotation<break/>1 <bold>Tool</bold>: ADPT; .<break/>2 Select randomly;<break/>3 <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>∈</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>;<break/>4 <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>∈</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>;<break/>5 <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>;<break/>6 Label frame;<break/>7 <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">_</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>;<break/>8 <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">_</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>;<break/>9 <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mtext>-</mml:mtext><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mtext/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">_</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">_</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>;<break/>10 Mix up image;<break/>11 <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;=</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> ;<break/>12 <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;=</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>==</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>;<break/>13 <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mtext>-</mml:mtext><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mtext/><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>==</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>;</td></tr></tbody></table></table-wrap></sec><sec id="s4-18"><title>Experiments for 10 mice identity tracking</title><p>In this experiment, we used videos featuring different identified mice, allocating 80% of the data for model training and the remaining 20% for accuracy validation. We configured the output channels of the model’s LRSS to be 11 (1 background channel +10 identity channels). Finally, we determined the identity of mice in the image by analyzing the proportion of each category within the LRSS image. For data augmentation, random rotation (±30°), random pixel translation (x:[–100,100], y:[–30,15]) and random scale (0.9,1.1) were used in training ADPT.</p><p>Following metrics was used for identity determination:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>∑</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents pixel <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> value at LRSS.</p></sec><sec id="s4-19"><title>Experiments for social mice tracking</title><p>In this experiment, we randomly selected two mice. We created a Mix-up Social Keypoint Dataset using individual videos of these mice and randomly captured background. We computed the BAF centered on the back of the mice. For the social interaction task, the LRSS channels of the model were set to 3 (one background channel and two identity channels), while two channels were introduced for the newly incorporated BAF (representing a two-dimensional vector). Random pixel translation (x:[–100,100], y:[–30,15]) was the only augmentation method used in training ADPT.</p><p>We trained the model on this mix-up dataset and used it to predict real social interaction videos of mice spanning 1 min. In practical application, we employed a bidirectional approach both bottom-up and top-down to ascertain mouse identities. Specifically, we utilized the BAF image to confirm the center position pointed by each pixel. Then, based on the identity information from LRSS corresponding to the center positions, we determined the identity information of each pixel (body pixels) to generate an identity map. Finally, by matching the location heatmap with the identity map, we calculated the posture information of the interacting animals.</p><p>Both manual verification and following metrics was used for evaluating identity exchange rate:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>F</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:munderover><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents center location of each individual, and <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> represent drift distance threshold which was set as 75 pixels.</p><p>In our ten mice identity tracking and social mice tracking task, we trained the model for a total of 300 epochs with 10 warm-up epochs. We early stop the training procedure when it reaches a plateau for 30 epochs according to training loss. The batch size used during training was set to 8. Each epoch has 250 iterations for the first task and 50 iterations for the social task. To optimize the BAF maps, we utilized RMSE as the loss function.</p></sec><sec id="s4-20"><title>Evaluation metrics</title><p>To evaluate keypoint tracking drift, we use following metrics: for each keypoint,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>F</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:munderover><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>F</mml:mi></mml:mstyle></mml:math></inline-formula> represents the total number of frames, <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represent a predicted keypoint position, <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> represent the drift distance threshold which was set as 50 pixels on mice, and 30 pixels on monkey, and <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula> is an indicator function that equals 1 when <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mi>α</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, and 0 otherwise.<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>F</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:munderover><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents the confidence score of the predicted heatmap of i-th frames.</p><p>We used the following metrics for single animal pose estimation: PCK@0.15, RMSE, mAP<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mn>0.15</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mn>0.15</mml:mn><mml:mo>⋅</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> represents the total number of keypoints, <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the Euclidean distance for the i-th keypoint, <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the normalized limb scale associated with the i-th keypoint.<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>O</mml:mi><mml:mi>K</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mn>2</mml:mn><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>s</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> is the bounding box area occupied by the GT instance, <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a visibility flag for the i-th keypoint, and <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> is the uncertainty factor(set to 0.025 for all measurements, the same as SLEAP)<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mi>K</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> represent the accuracy threshold.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mn>0.55</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mn>0.6</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>@</mml:mo></mml:mrow><mml:mn>0.95</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mtext>true</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represent a predicted keypoint position and <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is its’ ground truth.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Software, Validation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Methodology</p></fn><fn fn-type="con" id="con5"><p>Resources, Supervision</p></fn><fn fn-type="con" id="con6"><p>Resources, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures of mice in this study were approved by Animal Care and Use Committees at the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences. And all experimental procedures of monkey adhered to the Guidelines for the Care and Use of Laboratory Animals established by Jinan University.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Comparison among three methods on single fly dataset and OMS_Dataset.</title></caption><media xlink:href="elife-95709-supp1-v1.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-95709-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files; source data files have been provided for Figures 1, 2, 3, 4, 6, 7 and 8.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Anti-drift pose tracker (ADPT): a transformer-based network for robust animal pose estimation cross-species (Part 1)</data-title> <source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10473121</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Anti-drift pose tracker (ADPT): A transformer-based network for robust animal pose estimation cross-species (Part 2)</data-title> <source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10473280</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Anti-drift pose tracker (ADPT): A transformer-based network for robust animal pose estimation cross-species (Part 3)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.14218678</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset4"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>ADPT-TOOLBOX Demonstration Mouse Video</data-title> <source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.14566416</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset5"><person-group person-group-type="author"><name><surname>Biggs</surname><given-names>B</given-names></name><name><surname>Boyne</surname><given-names>O</given-names></name><name><surname>Charles</surname><given-names>J</given-names></name><name><surname>Fitzgibbon</surname><given-names>A</given-names></name><name><surname>Cipolla</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>StanfordExtra</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/benjiebob/StanfordExtra">StanfordExtra</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset6"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>J</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>DeepPoseKit Data: example datasets for DeepPoseKit - Single fly dataset</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/jgraving/DeepPoseKit-Data/tree/master/datasets/fly">DeepPoseKit-Data/tree/master/datasets/fly</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset7"><person-group person-group-type="author"><name><surname>Bala</surname><given-names>PC</given-names></name><name><surname>Eisenreich</surname><given-names>BR</given-names></name><name><surname>SBM</surname><given-names>Yoo</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>OMS_Dataset</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/OpenMonkeyStudio/OMS_Data">OpenMonkeyStudio/OMS_Data</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We acknowledge the effort from Wenhao Liu who recorded the mouse behavioral data and Professor Sen Yan’s laboratory who recorded the monkey behavioral data. This work was supported in part by the National Natural Science Foundation of China (32222036 to PF W), Research Fund for International Senior Scientists (T2250710685 to PF W), STI2030-Major Projects (2021ZD0203900 to PFW), and Shenzhen Science and Technology Innovation Committee (2022410129 to QL). We thank ChatGPT for the English language editing of this paper.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agezo</surname><given-names>S</given-names></name><name><surname>Berman</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Tracking together: estimating social poses</article-title><source>Nature Methods</source><volume>19</volume><fpage>410</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01452-z</pub-id><pub-id pub-id-type="pmid">35414127</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aljovic</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Chahin</surname><given-names>M</given-names></name><name><surname>de la Rosa</surname><given-names>C</given-names></name><name><surname>Van Steenbergen</surname><given-names>V</given-names></name><name><surname>Kerschensteiner</surname><given-names>M</given-names></name><name><surname>Bareyre</surname><given-names>FM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A deep learning-based toolbox for Automated Limb Motion Analysis (ALMA) in murine models of neurological disorders</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>131</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-03077-6</pub-id><pub-id pub-id-type="pmid">35169263</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>S</given-names></name><name><surname>Tekriwal</surname><given-names>A</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name><name><surname>Christensen</surname><given-names>E</given-names></name><name><surname>Hirt</surname><given-names>L</given-names></name><name><surname>Ojemann</surname><given-names>SG</given-names></name><name><surname>Kramer</surname><given-names>DR</given-names></name><name><surname>Kern</surname><given-names>DS</given-names></name><name><surname>Thompson</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Automatic extraction of upper-limb kinematic activity using deep learning-based markerless tracking during deep brain stimulation implantation for Parkinson’s disease: A proof of concept study</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0275490</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0275490</pub-id><pub-id pub-id-type="pmid">36264986</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bala</surname><given-names>PC</given-names></name><name><surname>Eisenreich</surname><given-names>BR</given-names></name><name><surname>Yoo</surname><given-names>SBM</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4560</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18441-5</pub-id><pub-id pub-id-type="pmid">32917899</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Biggs</surname><given-names>B</given-names></name><name><surname>Boyne</surname><given-names>O</given-names></name><name><surname>Charles</surname><given-names>J</given-names></name><name><surname>Fitzgibbon</surname><given-names>A</given-names></name><name><surname>Cipolla</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Who left the dogs out? 3D animal reconstruction with expectation maximization in the loop</article-title><conf-name>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16</conf-name><fpage>195</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58621-8_12</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bohic</surname><given-names>M</given-names></name><name><surname>Pattison</surname><given-names>LA</given-names></name><name><surname>Jhumka</surname><given-names>ZA</given-names></name><name><surname>Rossi</surname><given-names>H</given-names></name><name><surname>Thackray</surname><given-names>JK</given-names></name><name><surname>Ricci</surname><given-names>M</given-names></name><name><surname>Mossazghi</surname><given-names>N</given-names></name><name><surname>Foster</surname><given-names>W</given-names></name><name><surname>Ogundare</surname><given-names>S</given-names></name><name><surname>Twomey</surname><given-names>CR</given-names></name><name><surname>Hilton</surname><given-names>H</given-names></name><name><surname>Arnold</surname><given-names>J</given-names></name><name><surname>Tischfield</surname><given-names>MA</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name><name><surname>St John Smith</surname><given-names>E</given-names></name><name><surname>Abdus-Saboor</surname><given-names>I</given-names></name><name><surname>Abraira</surname><given-names>VE</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mapping the neuroethological signatures of pain, analgesia, and recovery in mice</article-title><source>Neuron</source><volume>111</volume><fpage>2811</fpage><lpage>2830</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.06.008</pub-id><pub-id pub-id-type="pmid">37442132</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Hidalgo</surname><given-names>G</given-names></name><name><surname>Simon</surname><given-names>T</given-names></name><name><surname>Wei</surname><given-names>SE</given-names></name><name><surname>Sheikh</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>OpenPose: realtime multi-person 2D pose estimation using part affinity fields</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>43</volume><fpage>172</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2929257</pub-id><pub-id pub-id-type="pmid">31331883</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Peng</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>G</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cascaded pyramid network for multi-person pose estimation</article-title><conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>7103</fpage><lpage>7112</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00742</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>X</given-names></name><name><surname>Tian</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Conditional positional encodings for vision transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2102.10882">https://arxiv.org/abs/2102.10882</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabriel</surname><given-names>CJ</given-names></name><name><surname>Zeidler</surname><given-names>Z</given-names></name><name><surname>Jin</surname><given-names>B</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Goodpaster</surname><given-names>CM</given-names></name><name><surname>Kashay</surname><given-names>AQ</given-names></name><name><surname>Wu</surname><given-names>A</given-names></name><name><surname>Delaney</surname><given-names>M</given-names></name><name><surname>Cheung</surname><given-names>J</given-names></name><name><surname>DiFazio</surname><given-names>LE</given-names></name><name><surname>Sharpe</surname><given-names>MJ</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Wilke</surname><given-names>SA</given-names></name><name><surname>DeNardo</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>BehaviorDEPOT is a simple, flexible tool for automated behavioral detection based on markerless pose tracking</article-title><source>eLife</source><volume>11</volume><elocation-id>e74314</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.74314</pub-id><pub-id pub-id-type="pmid">35997072</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gschwind</surname><given-names>T</given-names></name><name><surname>Zeine</surname><given-names>A</given-names></name><name><surname>Raikov</surname><given-names>I</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Felong</surname><given-names>S</given-names></name><name><surname>Isom</surname><given-names>LL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Soltesz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Hidden behavioral fingerprints in epilepsy</article-title><source>Neuron</source><volume>111</volume><fpage>1440</fpage><lpage>1452</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.02.003</pub-id><pub-id pub-id-type="pmid">36841241</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Pan</surname><given-names>H</given-names></name><name><surname>Ju</surname><given-names>F</given-names></name><name><surname>Long</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>MouseVenue3D: a markerless three-dimension behavioral tracking system for matching two-photon brain imaging in free-moving mice</article-title><source>Neuroscience Bulletin</source><volume>38</volume><fpage>303</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1007/s12264-021-00778-6</pub-id><pub-id pub-id-type="pmid">34637091</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Han</surname><given-names>C</given-names></name><name><surname>Liao</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Cai</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>GD</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Multi-animal 3D social pose estimation, identification and behaviour embedding with a few-shot learning framework</article-title><source>Nature Machine Intelligence</source><volume>6</volume><fpage>48</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1038/s42256-023-00776-5</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5188</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id><pub-id pub-id-type="pmid">34465784</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Pan</surname><given-names>H</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Yi</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A hierarchical 3D-motion learning framework for animal spontaneous behavior mapping</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2784</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22970-y</pub-id><pub-id pub-id-type="pmid">33986265</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>An easily compatible eye-tracking system for freely-moving small animals</article-title><source>Neuroscience Bulletin</source><volume>38</volume><fpage>661</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1007/s12264-022-00834-9</pub-id><pub-id pub-id-type="pmid">35325370</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Insafutdinov</surname><given-names>E</given-names></name><name><surname>Pishchulin</surname><given-names>L</given-names></name><name><surname>Andres</surname><given-names>B</given-names></name><name><surname>Andriluka</surname><given-names>M</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deepercut: a deeper, stronger, and faster multi-person pose estimation model</article-title><conf-name>Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14</conf-name><fpage>34</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46466-4_3</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>S</given-names></name><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Schneider</surname><given-names>S</given-names></name><name><surname>Nath</surname><given-names>T</given-names></name><name><surname>Rahman</surname><given-names>MM</given-names></name><name><surname>Di Santo</surname><given-names>V</given-names></name><name><surname>Soberanes</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Lauder</surname><given-names>G</given-names></name><name><surname>Dulac</surname><given-names>C</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title><source>Nature Methods</source><volume>19</volume><fpage>496</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01443-0</pub-id><pub-id pub-id-type="pmid">35414125</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>From synthetic to real: unsupervised domain adaptation for animal pose estimation</article-title><conf-name>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>1482</fpage><lpage>1491</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00153</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pose recognition with cascade transformers[C]</article-title><conf-name>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</conf-name><fpage>1944</fpage><lpage>1953</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00198</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Anumanchipalli</surname><given-names>GK</given-names></name><name><surname>Mohamed</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>P</given-names></name><name><surname>Carney</surname><given-names>LH</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dissecting neural computations in the human auditory pathway using deep neural networks for speech</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>2213</fpage><lpage>2225</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01468-4</pub-id><pub-id pub-id-type="pmid">37904043</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>F</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Pang</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Jie</surname><given-names>J</given-names></name><name><surname>Gao</surname><given-names>F</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Liao</surname><given-names>W-H</given-names></name><name><surname>Yin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Non-human primate models and systems for gait and neurophysiological analysis</article-title><source>Frontiers in Neuroscience</source><volume>17</volume><elocation-id>1141567</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2023.1141567</pub-id><pub-id pub-id-type="pmid">37188006</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>N</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Ding</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Objective and comprehensive re-evaluation of anxiety-like behaviors in mice using the Behavior Atlas</article-title><source>Biochemical and Biophysical Research Communications</source><volume>559</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.bbrc.2021.03.125</pub-id><pub-id pub-id-type="pmid">33932895</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lonini</surname><given-names>L</given-names></name><name><surname>Moon</surname><given-names>Y</given-names></name><name><surname>Embry</surname><given-names>K</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>McKenzie</surname><given-names>K</given-names></name><name><surname>Jenz</surname><given-names>S</given-names></name><name><surname>Jayaraman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Video-based pose estimation for gait analysis in stroke survivors during clinical assessments: a proof-of-concept study</article-title><source>Digital Biomarkers</source><volume>6</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1159/000520732</pub-id><pub-id pub-id-type="pmid">35224426</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luxem</surname><given-names>K</given-names></name><name><surname>Mocellin</surname><given-names>P</given-names></name><name><surname>Fuhrmann</surname><given-names>F</given-names></name><name><surname>Kürsch</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>SR</given-names></name><name><surname>Palop</surname><given-names>JJ</given-names></name><name><surname>Remy</surname><given-names>S</given-names></name><name><surname>Bauer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Identifying behavioral structure from deep variational embeddings of animal motion</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>1267</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-04080-7</pub-id><pub-id pub-id-type="pmid">36400882</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>W</given-names></name><name><surname>Ge</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Tian</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Tfpose: direct human pose estimation with transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.15320">https://arxiv.org/abs/2103.15320</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>M</given-names></name><name><surname>Qiuhan</surname><given-names>J</given-names></name><name><surname>Sturman</surname><given-names>O</given-names></name><name><surname>von Ziegler</surname><given-names>L</given-names></name><name><surname>Kollmorgen</surname><given-names>S</given-names></name><name><surname>von der Behrens</surname><given-names>W</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Bohacek</surname><given-names>J</given-names></name><name><surname>Yanik</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep-learning based identification, tracking, pose estimation, and behavior classification of interacting primates and mice in complex environments</article-title><source>Nature Machine Intelligence</source><volume>4</volume><fpage>331</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00477-5</pub-id><pub-id pub-id-type="pmid">35465076</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzger</surname><given-names>SL</given-names></name><name><surname>Littlejohn</surname><given-names>KT</given-names></name><name><surname>Silva</surname><given-names>AB</given-names></name><name><surname>Moses</surname><given-names>DA</given-names></name><name><surname>Seaton</surname><given-names>MP</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Dougherty</surname><given-names>ME</given-names></name><name><surname>Liu</surname><given-names>JR</given-names></name><name><surname>Wu</surname><given-names>P</given-names></name><name><surname>Berger</surname><given-names>MA</given-names></name><name><surname>Zhuravleva</surname><given-names>I</given-names></name><name><surname>Tu-Chan</surname><given-names>A</given-names></name><name><surname>Ganguly</surname><given-names>K</given-names></name><name><surname>Anumanchipalli</surname><given-names>GK</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A high-performance neuroprosthesis for speech decoding and avatar control</article-title><source>Nature</source><volume>620</volume><fpage>1037</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06443-4</pub-id><pub-id pub-id-type="pmid">37612505</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monsees</surname><given-names>A</given-names></name><name><surname>Voit</surname><given-names>K-M</given-names></name><name><surname>Wallace</surname><given-names>DJ</given-names></name><name><surname>Sawinski</surname><given-names>J</given-names></name><name><surname>Charyasz</surname><given-names>E</given-names></name><name><surname>Scheffler</surname><given-names>K</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Kerr</surname><given-names>JND</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Estimation of skeletal kinematics in freely moving rodents</article-title><source>Nature Methods</source><volume>19</volume><fpage>1500</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01634-9</pub-id><pub-id pub-id-type="pmid">36253644</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stacked hourglass networks for human pose estimation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1603.06937">https://doi.org/10.48550/arXiv.1603.06937</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niknejad</surname><given-names>N</given-names></name><name><surname>Caro</surname><given-names>JL</given-names></name><name><surname>Bidese-Puhl</surname><given-names>R</given-names></name><name><surname>Bao</surname><given-names>Y</given-names></name><name><surname>Staiger</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Stride length and stance duration estimation</article-title><source>Journal of the ASABE</source><volume>1</volume><elocation-id>66</elocation-id><pub-id pub-id-type="doi">10.13031/ja.15386</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Willmore</surname><given-names>L</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Tabris</surname><given-names>N</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>DM</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Ravindranath</surname><given-names>S</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Normand</surname><given-names>E</given-names></name><name><surname>Deutsch</surname><given-names>DS</given-names></name><name><surname>Wang</surname><given-names>ZY</given-names></name><name><surname>McKenzie-Smith</surname><given-names>GC</given-names></name><name><surname>Mitelut</surname><given-names>CC</given-names></name><name><surname>Castro</surname><given-names>MD</given-names></name><name><surname>D’Uva</surname><given-names>J</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name><name><surname>Kocher</surname><given-names>SD</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title><source>Nature Methods</source><volume>19</volume><fpage>486</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id><pub-id pub-id-type="pmid">35379947</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>GE</given-names></name><name><surname>Fernald</surname><given-names>RD</given-names></name><name><surname>Clayton</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Genes and Social Behavior</article-title><source>Science</source><volume>322</volume><fpage>896</fpage><lpage>900</lpage><pub-id pub-id-type="doi">10.1126/science.1159277</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>U-Net: Convolutional Networks for Biomedical Image Segmentation</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Imagenet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Howard</surname><given-names>A</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Zhmoginov</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</article-title><conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>4510</fpage><lpage>4520</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00474</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learnable latent embeddings for joint behavioural and neural analysis</article-title><source>Nature</source><volume>617</volume><fpage>360</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06031-6</pub-id><pub-id pub-id-type="pmid">37138088</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Gardin</surname><given-names>J</given-names></name><name><surname>Sabnis</surname><given-names>GS</given-names></name><name><surname>Peer</surname><given-names>A</given-names></name><name><surname>Darrell</surname><given-names>M</given-names></name><name><surname>Deats</surname><given-names>S</given-names></name><name><surname>Geuther</surname><given-names>B</given-names></name><name><surname>Lutz</surname><given-names>CM</given-names></name><name><surname>Kumar</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Stride-level analysis of mouse open field behavior using deep-learning-based pose estimation</article-title><source>Cell Reports</source><volume>38</volume><elocation-id>110231</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.110231</pub-id><pub-id pub-id-type="pmid">35021077</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stenum</surname><given-names>J</given-names></name><name><surname>Rossi</surname><given-names>C</given-names></name><name><surname>Roemmich</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Two-dimensional video-based analysis of human gait using pose estimation</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008935</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008935</pub-id><pub-id pub-id-type="pmid">33891585</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stoffl</surname><given-names>L</given-names></name><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>End-to-End Trainable Multi-Instance Pose Estimation with Transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.12115">https://arxiv.org/abs/2103.12115</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>K</given-names></name><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep high-resolution representation learning for human pose estimation</article-title><conf-name>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>5693</fpage><lpage>5703</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00584</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Takagi</surname><given-names>Y</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>High-resolution image reconstruction with latent diffusion models from human brain activity</article-title><conf-name>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>14453</fpage><lpage>14463</lpage><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.01389</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Anti-drift pose tracker (ADPT): A transformer-based network for robust animal pose estimation cross-species</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT">https://github.com/tangguoling/ADPT</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>G</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Anti-drift pose tracker (ADPT): A transformer-based network for robust animal pose estimation cross-species</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT-TOOLBOX">https://github.com/tangguoling/ADPT-TOOLBOX</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Thota</surname><given-names>AK</given-names></name><name><surname>Alberts</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Novel use of retro-reflective paint to capture 3D kinematic gait data in non-human primates</article-title><conf-name>2013 29th Southern Biomedical Engineering Conference (SBEC 2013)</conf-name><fpage>113</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1109/SBEC.2013.65</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Toshev</surname><given-names>A</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>DeepPose: human pose estimation via deep neural networks</article-title><conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>1653</fpage><lpage>1660</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.214</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Leifer</surname><given-names>AM</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>11</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00980-9</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Wolf</surname><given-names>N</given-names></name><name><surname>Rosenberg</surname><given-names>B</given-names></name><name><surname>Harris</surname><given-names>BP</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Perspectives on individual animal identification from biology and computer vision</article-title><source>Integrative and Comparative Biology</source><volume>61</volume><fpage>900</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1093/icb/icab107</pub-id><pub-id pub-id-type="pmid">34050741</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>SE</given-names></name><name><surname>Ramakrishna</surname><given-names>V</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Sheikh</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Convolutional pose machines</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>4724</fpage><lpage>4732</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.511</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinreb</surname><given-names>C</given-names></name><name><surname>Pearl</surname><given-names>JE</given-names></name><name><surname>Lin</surname><given-names>S</given-names></name><name><surname>Osman</surname><given-names>MAM</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Annapragada</surname><given-names>S</given-names></name><name><surname>Conlin</surname><given-names>E</given-names></name><name><surname>Hoffmann</surname><given-names>R</given-names></name><name><surname>Makowska</surname><given-names>S</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Jay</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>S</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Pereira</surname><given-names>T</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics</article-title><source>Nature Methods</source><volume>21</volume><fpage>1329</fpage><lpage>1339</lpage><pub-id pub-id-type="doi">10.1038/s41592-024-02318-2</pub-id><pub-id pub-id-type="pmid">38997595</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>JM</given-names></name><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Abraira</surname><given-names>VE</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping sub-second structure in mouse behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Tsukahara</surname><given-names>T</given-names></name><name><surname>Zeine</surname><given-names>A</given-names></name><name><surname>Anyoha</surname><given-names>R</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1433</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00706-3</pub-id><pub-id pub-id-type="pmid">32958923</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Pan</surname><given-names>X</given-names></name><name><surname>Hoi</surname><given-names>S</given-names></name><name><surname>Yi</surname><given-names>Z</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>RegNet: self-regulated network for image classification</article-title><source>IEEE Transactions on Neural Networks and Learning Systems</source><volume>34</volume><fpage>9562</fpage><lpage>9567</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2022.3158966</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Vitpose: simple vision transformer baselines for human pose estimation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2204.12484">https://arxiv.org/abs/2204.12484</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Quan</surname><given-names>Z</given-names></name><name><surname>Nie</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>TransPose: keypoint localization via transformer</article-title><conf-name>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name><fpage>11802</fpage><lpage>11812</lpage><pub-id pub-id-type="doi">10.1109/ICCV48922.2021.01159</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Deep learning pose estimation</title><p>Pose estimation is a well-established computer vision task that has achieved significant advancements in human pose estimation. Traditional CNN-based algorithms for human pose estimation <xref ref-type="bibr" rid="bib35">Newell et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Cao et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Toshev and Szegedy, 2014</xref>; <xref ref-type="bibr" rid="bib8">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Wei et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib48">Sun et al., 2019</xref> have been widely applied and have shown promising results. With the recent rise of transformer-based models, researchers have explored the use of transformers for human pose estimation (<xref ref-type="bibr" rid="bib63">Yang et al., 2021</xref>; <xref ref-type="bibr" rid="bib24">Li et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Xu and Zhang, 2022</xref>; <xref ref-type="bibr" rid="bib30">Mao et al., 2021</xref>), leading to improved accuracy and performance. At the same time, some of these works (<xref ref-type="bibr" rid="bib35">Newell et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Wei et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Xu and Zhang, 2022</xref>) has also been extended to the field of animal pose estimation. Notably, keypoint detection methods typically employ two main approaches: heatmap-based and regression-based methods. Heatmap-based methods generate keypoint heatmaps, calculate the index of the maximum confidence score within these heatmaps, and obtain keypoint coordinates. Heatmap-based methods have the advantage of providing confidence scores, allowing researchers to gauge the reliability of each keypoint’s estimate. However, they can be computationally intensive due to the generation of multiple heatmaps. Conversely, regression-based methods directly output keypoint coordinates from the model. Regression-based methods are often computationally efficient and can provide accurate results. However, they may lack the ability to express the confidence or uncertainty associated with each keypoint prediction, which heatmap-based methods can provide. The choice between these methods depends on the specific requirements of the pose estimation task.</p><p>In the domain of behavioral studies, specific estimation methods have been developed and widely used. Notable examples include DeepLabCut (<xref ref-type="bibr" rid="bib32">Mathis et al., 2018</xref>), SLEAP <xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>, and DeepPoseKit (<xref ref-type="bibr" rid="bib11">Graving et al., 2019</xref>). These methods have found extensive application in experimental animal pose estimation, where the estimated poses are used for quantifying and analyzing animal behavior. They are heatmap-based methods. DeepLabCut is a popular toolbox utilized for animal pose estimation, employing CNNs such as ResNets (<xref ref-type="bibr" rid="bib15">He et al., 2016</xref>) or MobileNets (<xref ref-type="bibr" rid="bib43">Sandler et al., 2018</xref>) that initial pretrained on ImageNet (<xref ref-type="bibr" rid="bib42">Russakovsky et al., 2015</xref>) to accurately estimate animal poses. It has been widely adopted in various experimental settings, enabling researchers to track and analyze animal behavior with high precision. Similarly, SLEAP is another widely used tool for multi-animal pose estimation, leveraging U-NET (<xref ref-type="bibr" rid="bib41">Ronneberger et al., 2015</xref>) liked CNN architectures to estimate poses and facilitate behavior analysis in animals. Additionally, DeepPoseKit is another notable software toolkit using Stacked DenseNet for behavioral animal pose estimation. The results of pose estimation serve as a critical component in quantifying and analyzing animal behavior. By accurately estimating animal poses, researchers can extract valuable insights into the kinematics (<xref ref-type="bibr" rid="bib34">Monsees et al., 2022</xref>), dynamics (<xref ref-type="bibr" rid="bib29">Luxem et al., 2022</xref>), and patterns of animal movements (<xref ref-type="bibr" rid="bib17">Huang et al., 2021</xref>). This information further contributes to a better understanding of animal behavior, cognition, and underlying neural mechanisms.</p><p>According to literature report (<xref ref-type="bibr" rid="bib39">Pereira et al., 2022</xref>), SLEAP and DeepLabCut have similar accuracy on a benchmark single-fly datasets (<xref ref-type="bibr" rid="bib37">Pereira et al., 2019</xref>), with mean average precision scores(mAP) of 92.7% and 92.8%, respectively. Their accuracies are significantly higher than that of DeepPoseKit(86.4%). Additionally, SLEAP demonstrates the highest inference speed among the three tools. Therefore, currently, SLEAP and DeepLabCut are considered to have the best performance in freely behaving animal pose estimation. However, these methods are still limited by their robustness, which refers to the presence of uncertainty or noise interference in the estimated positions of keypoints due to the inherent limitations of the algorithms or noise in the image. For instance, the limited receptive fields of convolutional kernels may hinder their ability to capture the global dependencies within an image. This constraint can be particularly relevant in tasks that require modeling complex spatial relationships or long-range interactions. ADPT primarily aims to compare and improve upon these two methods.</p><p>In summary, various pose estimation methods, including DeepLabCut, SLEAP, and DeepPoseKit, have been developed and extensively employed in the field of experimental animal pose estimation. These methods leverage CNN-based models to estimate animal poses, enabling researchers to conduct detailed behavior quantification and analysis.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95709.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Useful</kwd></kwd-group></front-stub><body><p>This <bold>useful</bold> study introduces a deep learning-based algorithm that tracks animal postures with reduced drift by incorporating transformers for more robust keypoint detection. The efficacy of this new algorithm for single-animal pose estimation was demonstrated through comparisons with two popular algorithms. The strength of evidence is <bold>solid</bold> but would benefit from consideration of issues in multi-animal tracking. This work will be of interest to those interested in animal behavior tracking.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95709.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors present a new model for animal pose estimation. The core feature they highlight is the model's stability compared to existing models in terms of keypoint drift. The authors test this model across a range of new and existing datasets. The authors also test the model with two mice in the same arena. For the single animal datasets the authors show a decrease in sudden jumps in keypoint detection and the number of undetected keypoints compared with DeepLabCut and SLEAP. Overall average accuracy, as measured by root mean squared error, generally shows generally similar but sometimes superior performance to DeepLabCut and better performance compared to SLEAP. The authors confusingly don't quantify the performance of pose estimation in the multi (two) animal case instead focusing on detecting individual identity. This multi-animal model is not compared with the model performance of the multi-animal mode of DeepLabCut or SLEAP.</p><p>Strengths:</p><p>The major strength of the paper is successfully demonstrating a model that is less likely to have incorrect large keypoint jumps compared to existing methods. As noted in the paper, this should lead to easier-to-interpret descriptions of pose and behavior to use in the context of a range of biological experimental workflows.</p><p>Weaknesses:</p><p>There are two main types of weaknesses in this paper. The first is a tendency to make unsubstantiated claims that suggest either model performance that is untested or misrepresents the presented data, or suggest excessively large gaps in current SOTA capabilities. One obvious example is in the abstract when the authors state ADPT &quot;significantly outperforms the existing deep-learning methods, such as DeepLabCut, SLEAP, and DeepPoseKit.&quot; All tests in the rest of the paper, however, only discuss performance with DeepLabCut and SLEAP, not DeepPoseKit. At this point, there are many animal pose estimation models so it's fine they didn't compare against DeepPoseKit, but they shouldn't act like they did. Similar odd presentation of results are statements like &quot;Our method exhibited an impressive prediction speed of 90{plus minus}4 frames per second (fps), faster than DeepLabCut (44{plus minus}2 fps) and equivalent to SLEAP (106{plus minus}4 fps).&quot; Why is 90{plus minus}4 fps considered &quot;equivalent to SLEAP (106{plus minus}4 fps)&quot; and not slower? I agree they are similar but they are not the same. The paper's point of view of what is &quot;equivalent&quot; changes when describing how &quot;On the single-fly dataset, ADPT excelled with an average mAP of 92.83%, surpassing both DeepLabCut and SLEAP (Figure 5B)&quot; When one looks at Figure 5B, however, ADPT and DeepLabCut look identical. Beyond this, oddly only ADPT has uncertainty bars (no mention of what uncertainty is being quantified) and in fact, the bars overlap with the values corresponding to SLEAP and DeepPoseKit. In terms of making claims that seem to stretch the gaps in the current state of the field, the paper makes some seemingly odd and uncited statements like &quot;Concerns about the safety of deep learning have largely limited the application of deep learning-based tools in behavioral analysis and slowed down the development of ethology&quot; and &quot;So far, deep learning pose estimation has not achieved the reliability of classical kinematic gait analysis&quot; without specifying which classical gait analysis is being referred to. Certainly, existing tools like DeepLabCut and SLEAP are already widely cited and used for research.</p><p>The other main weakness in the paper is the validation of the multi-animal pose estimation. The core point of the paper is pose estimation and anti-drift performance and yet there is no validation of either of these things relating to multi-animal video. All that is quantified is the ability to track individual identity with a relatively limited dataset of 10 mice IDs with only two in the same arena (and see note about train and validation splits below). While individual tracking is an important task, that literature is not engaged with (i.e. papers like Walter and Couzin, eLife, 2021: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.64000">https://doi.org/10.7554/eLife.64000</ext-link>) and the results in this paper aren't novel compared to that field's state of the art. On the other hand, while multi-animal pose estimation is also an important problem the paper doesn't engage with those results either. The two methods already used for comparison in the paper, SLEAP and DeepPoseKit, already have multi-animal modes and multi-animal annotated datasets but none of that is tested or engaged with in the paper. The paper notes many existing approaches are two-step methods, but, for practitioners, the difference is not enough to warrant a lack of comparison. The authors state that &quot;The evaluation of our social tracking capability was performed by visualizing the predicted video data (see supplement Videos 3 and 4).&quot; While the authors report success maintaining mouse ID, when one actually watches the key points in the video of the two mice (only a single minute was used for validation) the pose estimation is relatively poor with tails rarely being detected and many pose issues when the mice get close to each other.</p><p>Finally, particularly in the methods section, there were a number of places where what was actually done wasn't clear. For example in describing the network architecture, the authors say &quot;Subsequently, network separately process these features in three branches, compute features at scale of one-fourth, one-eight and one-sixteenth, and generate one-eight scale features using convolution layer or deconvolution layer.&quot; Does only the one-eight branch have deconvolution or do the other branches also? Similarly, for the speed test, the authors say &quot;Here we evaluate the inference speed of ADPT. We compared it with DeepLabCut and SLEAP on mouse videos at 1288 x 964 resolution&quot;, but in the methods section they say &quot;The image inputs of ADPT were resized to a size that can be trained on the computer. For mouse images, it was reduced to half of the original size.&quot; Were different image sizes used for training and validation? Or Did ADPT not use 1288 x 964 resolution images as input which would obviously have major implications for the speed comparison? Similarly, for the individual ID experiments, the authors say &quot;In this experiment, we used videos featuring different identified mice, allocating 80% of the data for model training and the remaining 20% for accuracy validation.&quot; Were frames from each video randomly assigned to the training or validation sets? Frames from the same video are very correlated (two frames could be just 1/30th of a second different from each other), and so if training and validation frames are interspersed with each other validation performance doesn't indicate much about performance on more realistic use cases (i.e. using models trained during the first part of an experiment to maintain ids throughout the rest of it.)</p><p>Editors' note: None of the original reviewers responded to our request to re-review the manuscript. The attached assessment statement is the editor's best attempt at assessing the extent to which the authors addressed the outstanding concerns from the previous round of revisions.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95709.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Guoling</given-names></name><role specific-use="author">Author</role><aff><institution>Shenzhen Institutes of Advanced Technology</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Han</surname><given-names>Yaning</given-names></name><role specific-use="author">Author</role><aff><institution>Shenzhen Institutes of Advanced Technology</institution><addr-line><named-content content-type="city">shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Xing</given-names></name><role specific-use="author">Author</role><aff><institution>Shenzhen Institutes of Advanced Technology</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ruonan</given-names></name><role specific-use="author">Author</role><aff><institution>Guangxi University of Science and Technology</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Han</surname><given-names>Ming-Hu</given-names></name><role specific-use="author">Author</role><aff><institution>Shenzhen University of Advanced Technology</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Quanying</given-names></name><role specific-use="author">Author</role><aff><institution>Southern University of Science and Technology</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Wei</surname><given-names>Pengfei</given-names></name><role specific-use="author">Author</role><aff><institution>Shenzhen Institutes of Advanced Technology</institution><addr-line><named-content content-type="city">shenzhen</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife Assessment</bold></p><p>This study introduces a useful deep learning-based algorithm that tracks animal postures with reduced drift by incorporating transformers for more robust keypoint detection. The efficacy of this new algorithm for single-animal pose estimation was demonstrated through comparisons with two popular algorithms. However, the analysis is incomplete and would benefit from comparisons with other state-of-the-art methods and consideration of multi-animal tracking.</p></disp-quote><p>First, we would like to express our gratitude to the eLife editors and reviewers for their thorough evaluation of our manuscript. ADPT aims to improve the accuracy of body point detection and tracking in animal behavior, facilitating more refined behavioral analyses. The insights provided by the reviewers have greatly enhanced the quality of our work, and we have addressed their comments point-by-point.</p><p>In this revision, we have included additional quantitative comparisons of multi-animal tracking capabilities between ADPT and other state-of-the-art methods. Specifically, we have added evaluations involving homecage social mice and marmosets to comprehensively showcase ADPT’s advantages from various perspectives. This additional analysis will help readers better understand how ADPT effectively overcomes point drift and expands its applicability in the field.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>In this paper, the authors introduce a new deep learning-based algorithm for tracking animal poses, especially in minimizing drift effects. The algorithm's performance was validated by comparing it with two other popular algorithms, DeepLabCut and LEAP.The accessibility of this tool for biological research is not clearly addressed, despite its potential usefulness. Researchers in biology often have limited expertise in deep learning training, deployment, and prediction. A detailed, step-by-step user guide is crucial, especially for applications in biological studies.</p></disp-quote><p>We appreciate the reviewers' acknowledgment of our work. While ADPT demonstrates superior performance compared to DeepLabCut and SLEAP, we recognize that the absence of a user-friendly interface may hinder its broader application, particularly for users with a background solely in biology. In this revision, we have enhanced the command-line version of the user tutorial to provide a clear, step-by-step guide. Additionally, we have developed a simple graphical user interface (GUI) to further support users who may not have expertise in deep learning, thereby making ADPT more accessible for biological research.</p><disp-quote content-type="editor-comment"><p>The proposed algorithm focuses on tracking and is compared with DLC and LEAP, which are more adept at detection rather than tracking.</p></disp-quote><p>In the field of animal pose estimation, the distinction between detection and tracking is often blurred. For instance, the title of the paper &quot;SLEAP: A deep learning system for multi-animal pose tracking&quot; refers to &quot;tracking,&quot; while &quot;detection&quot; is characterized as &quot;pose estimation&quot; in the body text. Similarly, &quot;Multi-animal pose estimation, identification, and tracking with DeepLabCut&quot; uses &quot;tracking&quot; in the title, yet &quot;detection&quot; is also mentioned in the pose estimation section. We acknowledge that referencing these articles may have contributed to potential confusion.</p><p>To address this, we have clarified the distinction between &quot;tracking&quot; and &quot;detection&quot; Results section under &quot; Anti-drift pose tracker.&quot; (see lines 118-119). In this paper, we now explicitly use “track” to refer to the tracking of all body points or poses of an individual, and “detect” for specific keypoints.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 recommendations:</bold></p><p>(1) DLC and LEAP are mainly good in detection, not tracking. The authors should compare their ADPT algorithm with idtracker.ai, ByteTrack, and other advanced tracking algorithms, including recent track-anything algorithms.</p><p>(2) DeepPoseKit is outdated and no longer maintained; a comparison with the T-REX algorithm would be more appropriate.</p></disp-quote><p>We appreciate the reviewer's suggestion for a more comprehensive comparison and acknowledge the importance of including these advanced tracking algorithms. However, we have not yet found suitable publicly available datasets for such comparative testing. We appreciate this insight and will consider incorporating T-REX into future comparisons.</p><disp-quote content-type="editor-comment"><p>(3) The authors primarily compared their performance using custom data. A systematic comparison with published data, such as the dataset reported in the paper &quot;Multi-animal pose estimation, identification, and tracking with DeepLabCut,&quot; is necessary. A detailed comparison of the performances between ADPT and DLC is required.</p></disp-quote><p>In the previous version of our manuscript, we included the SLEAP single-fly public dataset and the OMS_dataset from OpenMonkeyStudio for performance comparisons. We recognize that these datasets were not comprehensive. In this revision, we have added the marmoset dataset from &quot;Multi-animal pose estimation, identification, and tracking with DeepLabCut&quot; and a customized homecage social mice dataset to enhance our comparative analysis of multi-animal pose estimation performance. Our comprehensive comparison reveals that ADPT outperforms both DLC and SLEAP, as discussed in the Results section under &quot;ADPT can be adapted for end-to-end pose estimation and identification of freely social animals.&quot;. (Figure 1, see lines 303-332)</p><disp-quote content-type="editor-comment"><p>(4) Given the focus on biological studies, an easy-to-use interface and introduction are essential.</p></disp-quote><p>In this revision, we have not only developed a GUI for ADPT but also included a more detailed tutorial. This can be accessed at <ext-link ext-link-type="uri" xlink:href="https://github.com/tangguoling/ADPT-TOOLBOX">https://github.com/tangguoling/ADPT-TOOLBOX</ext-link></p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>The authors present a new model for animal pose estimation. The core feature they highlight is the model's stability compared to existing models in terms of keypoint drift. The authors test this model across a range of new and existing datasets. The authors also test the model with two mice in the same arena. For the single animal datasets the authors show a decrease in sudden jumps in keypoint detection and the number of undetected keypoints compared with DeepLabCut and SLEAP. Overall average accuracy, as measured by root mean squared error, generally shows similar but sometimes superior performance to DeepLabCut and better performance compared to SLEAP. The authors confusingly don't quantify the performance of pose estimation in the multi (two) animal case instead focusing on detecting individual identity. This multi-animal model is not compared with the model performance of the multi-animal mode of DeepLabCut or SLEAP.</p></disp-quote><p>We appreciate the reviewer's thoughtful assessment of our manuscript. Our study focuses on addressing the issue of keypoint drift prevalent in animal pose estimation methods like DeepLabCut and SLEAP. During the model design process, we discovered that the structure of our model also enhances performance in identifying multiple animals. Consequently, we included some results related to multi-animal identity recognition in our manuscript.</p><p>In recent developments, we are working to broaden the applicability of ADPT for multi-animal pose estimation and identity recognition. Given that our manuscript emphasizes pose estimation, we have added a comparison of anti-drift performance in multi-animal scenarios in this revision. This quantifies ADPT's capability to mitigate drift in multi-animal pose estimation.</p><p>Using our custom Homecage social mice dataset, we compared ADPT with DeepLabCut and SLEAP. The results indicate that ADPT achieves more accurate anti-drift pose estimation for two mice, with superior keypoint detection accuracy. Furthermore, we also evaluated pose estimation accuracy on the publicly available marmoset dataset, where ADPT outperformed both DeepLabCut and SLEAP. These findings are discussed in the Results section under &quot;ADPT can be adapted for end-to-end pose estimation and identification of freely social animals.&quot;</p><disp-quote content-type="editor-comment"><p>The first is a tendency to make unsubstantiated claims that suggest either model performance that is untested or misrepresents the presented data, or suggest excessively large gaps in current SOTA capabilities. One obvious example is in the abstract when the authors state ADPT &quot;significantly outperforms the existing deep-learning methods, such as DeepLabCut, SLEAP, and DeepPoseKit.&quot; All tests in the rest of the paper, however, only discuss performance with DeepLabCut and SLEAP, not DeepPoseKit. At this point, there are many animal pose estimation models so it's fine they didn't compare against DeepPoseKit, but they shouldn't act like they did.</p></disp-quote><p>We appreciate the reviewer's feedback regarding unsubstantiated claims in our manuscript. Upon careful review, we acknowledge that our previous revisions inadvertently included statements that may misrepresent our model's performance. In particular, we have revised the abstract to eliminate the mention of DeepPoseKit, as our comparisons focused exclusively on DeepLabCut and SLEAP.</p><p>In addition to this correction, we have thoroughly reviewed the entire manuscript to address other instances of ambiguity and ensure that our claims are well-supported by the data presented. Thank you for bringing this to our attention; we are committed to maintaining the integrity of our claims throughout the paper.</p><disp-quote content-type="editor-comment"><p>In terms of making claims that seem to stretch the gaps in the current state of the field, the paper makes some seemingly odd and uncited statements like &quot;Concerns about the safety of deep learning have largely limited the application of deep learning-based tools in behavioral analysis and slowed down the development of ethology&quot; and &quot;So far, deep learning pose estimation has not achieved the reliability of classical kinematic gait analysis&quot; without specifying which classical gait analysis is being referred to. Certainly, existing tools like DeepLabCut and SLEAP are already widely cited and used for research.</p></disp-quote><p>In this revision, we have carefully reviewed the entire manuscript and addressed the instances of seemingly odd and unsubstantiated claims. Specifically, we have revised the statements &quot;largely limited&quot; to &quot;limited&quot; to ensure accuracy and clarity. Additionally, we thoroughly reviewed the citation list to ensure proper attribution, incorporating references such as &quot;A deep learning-based toolbox for Automated Limb Motion Analysis (ALMA) in murine models of neurological disorders&quot; to better substantiate our claims and provide a clearer context.</p><p>We have also added an additional section to comprehensively discuss the applications of widely-used tools like DeepLabCut and SLEAP in behavioral research. This new section elaborates on the challenges and limitations researchers encounter when applying these methods, highlighting both their significant contributions and the areas where improvements are still needed.</p><disp-quote content-type="editor-comment"><p>The other main weakness in the paper is the validation of the multi-animal pose estimation. The core point of the paper is pose estimation and anti-drift performance and yet there is no validation of either of these things relating to multi-animal video. All that is quantified is the ability to track individual identity with a relatively limited dataset of 10 mice IDs with only two in the same arena (and see note about train and validation splits below). While individual tracking is an important task, that literature is not engaged with (i.e. papers like Walter and Couzin, eLife, 2021: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.64000">https://doi.org/10.7554/eLife.64000</ext-link>) and the results in this paper aren't novel compared to that field's state of the art. On the other hand, while multi-animal pose estimation is also an important problem the paper doesn't engage with those results either. The two methods already used for comparison in the paper, SLEAP and DeepPoseKit, already have multi-animal models and multi-animal annotated datasets but none of that is tested or engaged with in the paper. The paper notes many existing approaches are two-step methods, but, for practitioners, the difference is not enough to warrant a lack of comparison.</p></disp-quote><p>We appreciate the reviewer's insights regarding the validation of multi-animal pose estimation in our paper. While our primary focus has been on pose estimation and anti-drift performance, we recognize the importance of validating these aspects within the context of multi-animal videos.</p><p>In this revision, we have included a comparison of ADPT's anti-drift performance in multi-animal pose estimation, utilizing our custom Homecage social mouse dataset (Figure 1A). Our findings indicate that ADPT achieves more accurate pose estimation for two mice while significantly reducing keypoint drift, outperforming both DeepLabCut and SLEAP. (see lines 311-322). We trained each model three times, and this figure presents the results from one of those training sessions. We calculated the average RMSE between predictions and manual labels, demonstrating that ADPT achieved an average RMSE of 15.8 ± 0.59 pixels, while DeepLabCut (DLC) and SLEAP recorded RMSEs of 113.19 ± 42.75 pixels and 94.76 ± 1.95 pixels, respectively (Figure 1C). ADPT achieved an accuracy of 6.35 ± 0.14 pixels based on the DLC evaluation metric across all body parts of the mice, while DLC reached 7.49 ± 0.2 pixels (Figure 1D). ADPT achieved 8.33 ± 0.19 pixels using the SLEAP evaluation Metric across all body parts of the mice, compared to SLEAP’s 9.82 ± 0.57 pixels (Figure 1E).</p><p>Furthermore, we have conducted pose estimation accuracy evaluations on the publicly available marmoset dataset from DeepLabCut, where ADPT also demonstrated superior performance compared to DeepLabCut and SLEAP. These results can be found in the &quot;ADPT can be adapted for end-to-end pose estimation and identification of freely social animals&quot; section of the Results. (see lines 323-329)</p><p>We acknowledge the existing literature on multi-animal tracking, such as the work by Walter and Couzin (2021). While individual tracking is crucial, our primary focus lies in the effective tracking of animal poses and minimizing drift during this process. This dual emphasis on pose tracking and anti-drift performance distinguishes our work and aligns with ongoing advancements in the field. Engaging with relevant literature, highlights the importance of contextualizing our results within the broader tracking literature, demonstrating that while our findings may overlap with existing methods, the unique focus on improving tracking stability and reducing drift presents valuable contributions to the field. Thank you for your valuable feedback, which has helped us improve the robustness of our manuscript.</p><disp-quote content-type="editor-comment"><p>The authors state that &quot;The evaluation of our social tracking capability was performed by visualizing the predicted video data (see supplement Videos 3 and 4).&quot; While the authors report success maintaining mouse ID, when one actually watches the key points in the video of the two mice (only a single minute was used for validation) the pose estimation is relatively poor with tails rarely being detected and many pose issues when the mice get close to each other.</p></disp-quote><p>We acknowledge that there are indeed challenges in pose estimation, particularly when the two mice get close to each other, leading to tracking failures and infrequent detection of tails in the predicted videos. The reasons for these issues can be summarized as follows:</p><p>Lack of Training Data from Real Social Scenarios: The training data used for the social tracking assessment were primarily derived from the Mix-up Social Animal Dataset, which does not fully capture the complexities of real social interactions. In future work, we plan to incorporate a blend of real social data and the Mix-up data for model training. Specifically, we aim to annotate images where two animals are in close proximity or interacting to enhance the model's understanding of genuine social behaviors.</p><p>Challenges in Tail Tracking in Social Contexts: Tracking the tails of mice in social situations remains a significant challenge. To validate this, we have added an assessment of tracking performance in real social settings using homecage data. Our findings indicate that using annotated data from real environments significantly improves tail tracking accuracy, as demonstrated in the supplementary video.</p><p>We appreciate your feedback, which highlights critical areas for improvement in our model.</p><disp-quote content-type="editor-comment"><p>Finally, particularly in the methods section, there were a number of places where what was actually done wasn't clear.</p></disp-quote><p>We have carefully reviewed and revised the corresponding parts to clarify the previously incomprehensible statements. Thank you for your valuable feedback, which has helped enhance the clarity of our methods.</p><disp-quote content-type="editor-comment"><p>For example in describing the network architecture, the authors say &quot;Subsequently, network separately process these features in three branches, compute features at scale of one-fourth, one-eight and one-sixteenth, and generate one-eight scale features using convolution layer or deconvolution layer.&quot; Does only the one-eight branch have deconvolution or do the other branches also?</p></disp-quote><p>We apologize for the confusion this has caused. Upon reviewing our manuscript, we identified an error in the diagram. In the revised version, we have clarified that the model samples feature maps at multiple resolutions and ultimately integrates them at the 1/8 resolution for feature fusion. Specifically, the 1/4 feature map from ResNet50's stack 2 is processed through max-pooling and convolution to generate a 1/8 feature map. Additionally, the 1/4 feature map from ResNet50's stack 2 is also transformed into a 1/8 feature map using a convolution operation with a stride of 2. Finally, both the input and output of the transformer are at the 1/16 resolution, which can be trained on a 2080Ti GPU. The 1/16 feature map is then upsampled to produce the final 1/8 feature map. We have updated the manuscript to reflect these changes, and we also modified the model architecture diagram for better clarity.</p><disp-quote content-type="editor-comment"><p>Similarly, for the speed test, the authors say &quot;Here we evaluate the inference speed of ADPT. We compared it with DeepLabCut and SLEAP on mouse videos at 1288 x 964 resolution&quot;, but in the methods section they say &quot;The image inputs of ADPT were resized to a size that can be trained on the computer. For mouse images, it was reduced to half of the original size.&quot; Were different image sizes used for training and validation? Or Did ADPT not use 1288 x 964 resolution images as input which would obviously have major implications for the speed comparison?</p></disp-quote><p>For our inference speed evaluation, all models, including ADPT, used images with a resolution of 1288 x 964. In ADPT's processing pipeline, the first layer is a resizing layer designed to compress the images to a scale determined by the global scale parameter. For the mouse images, we set the global scale to 0.5, allowing our GPU to handle the data at that resolution during transformer training.</p><p>We recorded the time taken by ADPT to process the entire 15-minute mouse video, which included the time taken for the resizing operation, and subsequently calculated the frames per second (FPS). We have clarified this process in the manuscript, particularly in the &quot;Network Architecture&quot; section, where we specify: &quot;Initially, ADPT will resize the images to a390 scale (a hyperparameter, consistent with the global scale in the DLC configuration).&quot;</p><disp-quote content-type="editor-comment"><p>Similarly, for the individual ID experiments, the authors say &quot;In this experiment, we used videos featuring different identified mice, allocating 80% of the data for model training and the remaining 20% for accuracy validation.&quot; Were frames from each video randomly assigned to the training or validation sets? Frames from the same video are very correlated (two frames could be just 1/30th of a second different from each other), and so if training and validation frames are interspersed with each other validation performance doesn't indicate much about performance on more realistic use cases (i.e. using models trained during the first part of an experiment to maintain ids throughout the rest of it.)</p></disp-quote><p>In our study, we actually utilized the first 80% of frames from each video for model training and the remaining 20% for testing the model's ID tracking accuracy. We have revised the relevant description in the manuscript to clarify this process. The updated description can be found in the &quot;Datasets&quot; section under &quot;Mouse Videos of Different Individuals.&quot;</p></body></sub-article></article>