<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">42870</article-id><article-id pub-id-type="doi">10.7554/eLife.42870</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural dynamics at successive stages of the ventral visual stream are consistent with hierarchical error signals</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-105215"><name><surname>Issa</surname><given-names>Elias B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5387-7207</contrib-id><email>elias.issa@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" id="author-111115"><name><surname>Cadieu</surname><given-names>Charles F</given-names></name><email>c.cadieu@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa2">‡</xref></contrib><contrib contrib-type="author" id="author-12392"><name><surname>DiCarlo</surname><given-names>James J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Brain and Cognitive Sciences, McGovern Institute for Brain Research</institution><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Connor</surname><given-names>Ed</given-names></name><role>Reviewing Editor</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Marder</surname><given-names>Eve</given-names></name><role>Senior Editor</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Department of Neuroscience, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Bay Labs, Inc, San Francisco, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>28</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e42870</elocation-id><history><date date-type="received" iso-8601-date="2018-10-15"><day>15</day><month>10</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-11-27"><day>27</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Issa et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Issa et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-42870-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.42870.001</object-id><p>Ventral visual stream neural responses are dynamic, even for static image presentations. However, dynamical neural models of visual cortex are lacking as most progress has been made modeling static, time-averaged responses. Here, we studied population neural dynamics during face detection across three cortical processing stages. Remarkably,~30 milliseconds after the initially evoked response, we found that neurons in intermediate level areas decreased their responses to typical configurations of their preferred face parts relative to their response for atypical configurations even while neurons in higher areas achieved and maintained a preference for typical configurations. These hierarchical neural dynamics were inconsistent with standard feedforward circuits. Rather, recurrent models computing prediction errors between stages captured the observed temporal signatures. This model of neural dynamics, which simply augments the standard feedforward model of online vision, suggests that neural responses to static images may encode top-down prediction errors in addition to bottom-up feature estimates.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual cortex</kwd><kwd>face recognition</kwd><kwd>neural dynamics</kwd><kwd>neurophysiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY014970</award-id><principal-award-recipient><name><surname>DiCarlo</surname><given-names>James J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K99-EY022671</award-id><principal-award-recipient><name><surname>Issa</surname><given-names>Elias B</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32-EY019609</award-id><principal-award-recipient><name><surname>Issa</surname><given-names>Elias B</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32-EY022845</award-id><principal-award-recipient><name><surname>Cadieu</surname><given-names>Charles F</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>MURI-114407</award-id><principal-award-recipient><name><surname>DiCarlo</surname><given-names>James J</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>MIT McGovern Institute for Brain Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>DiCarlo</surname><given-names>James J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural spiking responses in the visual cortex carry both an explicit representation of the presence of a face and a late-arriving, explicit encoding of errors in face estimation.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The primate ventral visual stream is a hierarchically organized set of cortical areas beginning with the primary visual cortex (V1) and culminating with distributed patterns of neural firing across the inferior temporal cortex (IT) that explicitly encode objects (i.e. linearly decodable object identity) (<xref ref-type="bibr" rid="bib17">Hung et al., 2005</xref>) and quantitatively account for core invariant object discrimination behavior in primates (<xref ref-type="bibr" rid="bib25">Majaj et al., 2015</xref>). Formalizing object recognition as the result of a series of feedforward computations yields models that achieve impressive performance in object categorization (<xref ref-type="bibr" rid="bib21">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Zeiler and Fergus, 2013</xref>) similar to the absolute level of performance achieved by IT neural populations, and these models are the current best predictors of neural responses in IT cortex and its primary input layer, V4 (<xref ref-type="bibr" rid="bib5">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Yamins et al., 2014</xref>). Thus, the feedforward inference perspective provides a simple but powerful, first-order framework for the ventral stream and core invariant object recognition.</p><p>However, visual object recognition behavior may not be executed via a single feedforward neural processing pass (a.k.a. feedforward inference) because IT neural responses are well-known to be dynamic even in response to images without dynamic content (<xref ref-type="bibr" rid="bib4">Brincat and Connor, 2006</xref>; <xref ref-type="bibr" rid="bib41">Sugase et al., 1999</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Meyer et al., 2014</xref>), raising the question of what computations those neural activity dynamics might reflect. Prior work has proposed that such neuronal response dynamics could be the result of different types of circuits executing different types of computation such as: (1) recurrent circuits within each ventral stream processing stage implementing local normalization of the feedforward information as it passes through that stage (<xref ref-type="bibr" rid="bib6">Carandini et al., 1997</xref>; <xref ref-type="bibr" rid="bib38">Schwartz and Simoncelli, 2001</xref>; <xref ref-type="bibr" rid="bib7">Carandini and Heeger, 2011</xref>), (2) feedback circuits between each pair of ventral stream stages implementing the integration of top-down with bottom-up information to improve the current (online) inference (<xref ref-type="bibr" rid="bib40">Seung, 1997</xref>; <xref ref-type="bibr" rid="bib22">Lee et al., 2002</xref>; <xref ref-type="bibr" rid="bib47">Zhang and von der Heydt, 2010</xref>; <xref ref-type="bibr" rid="bib13">Epshtein et al., 2008</xref>), or (3) feedback circuits between each pair of stages comparing top-down and bottom-up information to compute prediction errors that guide changes in synaptic weights so that neurons are better tuned to features useful for future feedforward behavior (learning) (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>). Thus, neural dynamics may reflect the various adaptive computations (within-stage normalization, top-down Bayesian inference) or reflect the underlying error intermediates that could be generated during those processes (e.g. predictive coding).</p><p>These computationally motivated ideas can each be formalized in neural circuits that contain feedforward, lateral (normalization), or feedback (hierarchical Bayesian inference) connections to ask which connection motif best predicts response dynamics across the visual hierarchy. Here, our main goal was to look beyond the initial, feedforward response edge to see if we could disambiguate among dynamics that might result from stacked feedforward, lateral, and feedback operations. Rather than record from a single processing level, we measured the dynamics of neural signals across three hierarchical levels (pIT, cIT, aIT) within macaque IT. We focused on face processing subregions within each of these levels for three reasons. First, prior evidence argues that these three face processing subregions are tightly anatomically and functionally connected and that the subregion in pIT is the dominant input to the higher subregions (<xref ref-type="bibr" rid="bib16">Grimaldi et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Moeller et al., 2008</xref>). Second, because prior work argues that a key behavioral function of these three subregions is to distinguish faces from non-faces, this allowed us to focus our testing on a relatively small number of images targeted to engage that processing function. Third, prior knowledge of pIT neural tuning properties (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>) allowed us to design images that were quantitatively matched in their ability to drive neurons in the pIT input subregion but that should ultimately be processed into two separate groups (face vs non-face). We reasoned that these images would force important computations for disambiguation to occur somewhere between the pIT subregion and the higher level (cIT, aIT) subregions. With this setup, our aim was to observe the dynamics at all three levels of the hierarchy in response to that image processing challenge so that we might discover – or at least constrain -- which type of computation is at work.</p><p>Consistent with the idea that the overall system performs – among other things -- face vs non-face discrimination (i.e. face detection), we found that in the highest face processing stage (aIT), neurons rapidly developed and maintained a response preference for the typical frontal configuration of the face parts even though our images were designed to be challenging for frontal face detection. However, we found that many neurons in the early (pIT) and intermediate (cIT) processing levels of IT paradoxically showed an overall stronger response for atypical face-part configurations relative to typical face-part configurations over time. That is, these neurons evolved an apparent preference for images of misarranged face parts within 30 milliseconds of their feedforward response. We found that standard feedforward models that employ local recurrences such as adaptation, lateral inhibition, and normalization could not capture this stage-wise pattern of image selectivity despite our best attempts. However, we found that a <italic>decreasing</italic> -- rather than increasing – relative preference for typical face-part configurations in early and intermediate processing stages is a natural dynamical signature of previously suggested ‘error coding’ models (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>) in which the neural spiking activity at each processing stage carries both an explicit representation of the variables of interest (e.g. Is an eye present? And is a whole face present?) and an explicit encoding of errors computed between each pair of stages in the hierarchy (e.g. a face was present, but the eye was not present at the correct location).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We leveraged the hierarchically arranged face processing system in macaque ventral visual cortex to study the dynamics of neural processing across a hierarchy (<xref ref-type="bibr" rid="bib42">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib43">Tsao et al., 2008</xref>) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The serially arranged posterior, central, and anterior face-selective subregions of IT (pIT, cIT, and aIT) can be conceptualized as building increasing selectivity for faces culminating in aIT representations (<xref ref-type="bibr" rid="bib15">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib8">Chang and Tsao, 2017</xref>). Using serial, single electrode recording, we sampled neural sites across the posterior to anterior extent of the IT hierarchy in the left hemispheres of two monkeys to generate neurophysiological maps (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; example neurophysiological map in one monkey using a faces versus non-face objects screen set) (<xref ref-type="bibr" rid="bib20">Issa et al., 2013</xref>). We localized the recording locations in vivo and co-registered across all penetrations using a stereo microfocal x-ray system (~400 micron in vivo resolution) (<xref ref-type="bibr" rid="bib11">Cox et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Issa et al., 2010</xref>) allowing accurate assignment of sites to different face processing stages (n = 633 out of 1891 total sites recorded were assigned as belonging to a face-selective subregion based on their spatial location; see Materials and methods). Results are reported here for sites that were spatially located in a face-selective subregion, that showed visual drive to any category in the screen set (see Materials and methods), and that were subsequently tested with our face versus non-face challenge set (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left panel) (n = 115 pIT, 70 cIT, and 40 aIT sites).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.42870.002</object-id><label>Figure 1.</label><caption><title>Neural recordings and experimental design in face-selective subregions of the ventral visual stream.</title><p>(<bold>A</bold>) Neurons were recorded along the lateral convexity of the inferior temporal lobe spanning the posterior to anterior extent of IT (+0 to+20 mm AP, Horsely-Clarke coordinates) in two monkeys (data from monkey one are shown). Based on prior work, face-selective sites (red) were operationally defined as those with a response preference for images of frontal faces versus images of non-face objects (see Materials and methods). While these neurons were found throughout IT, they tended to be found in clusters that mapped to previously identified subdivisions of IT (posterior, central, and anterior IT) and corresponded to face-selective areas identified under fMRI in the same subjects (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="bib20">Issa et al., 2013</xref>) (STS = superior temporal sulcus, IOS = inferior occipital sulcus, OTS = occipitotemporal sulcus). (<bold>B</bold>) (top diagram) The three visual processing stages in IT lie downstream of early visual areas V1, V2, and V4 in the ventral visual stream. (left) We designed our stimuli to focus on the intermediate stage pIT by seeking images of faces and images of non-faces that would, on average, drive equally strong initial responses in pIT. Novel images were generated from an exemplar monkey face by positioning the face parts in different positions within the face outline. This procedure generated both frontal face and non-face arrangements of the face parts, and we identified 21 images (red and black boxes) that drove the mean, early (60–100 ms) pIT population response to <underline>&gt; </underline>90% of its response to the intact face (first image in red box is synthesized whole face; compare to the second image which is the original whole face), and of these 21 images, 13 images contained atypical, non-face arrangements of the face parts. For example, images with an eye centered in the outline (black box, 3<sup>rd</sup> and 4<sup>th</sup> rows) as opposed to the lateralized position of the eye in a frontal face (red box) have a global interpretation (‘cyclops’) that is not consistent with a frontal face but still evoked strong pIT responses. Selectivity of neural sites (see <xref ref-type="fig" rid="fig3">Figure 3 and 4</xref>) for typical versus atypical face-part configuration images was quantified using a d’ measure. (middle) Computational hypotheses of cortical dynamics make differing predictions about how neural selectivity in pIT may evolve following images with similar local face features matched in their ability to evoke initial response but with different spatial context (typical vs atypical part configuration of the face). (right) Predictions of how aIT would behave as an output stage building selectivity for images of with face parts configured in the typical frontal face configuration through multiple stages of processing. (<bold>C</bold>) A population decoder, trained on average firing rates (60–200 ms post image onset, linear SVM classifier) for typical frontal face versus atypical part configurations of the face parts in this image subset, performed poorly in pIT on held-out trials of the same images (trial splits used so that the same images were shown in classifier training (90% of trials) and testing (10% of trials)). However, the particular configuration (typical vs atypical) could be determined at above chance levels when reading the cIT and aIT population responses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42870-fig1-v2.tif"/></fig><p>Our experimental design was intended to test previously proposed computational hypotheses of hierarchical neural dynamics during visual face processing (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Briefly, these hypotheses predict how stimulus preference (in this instance, for typical versus atypical configurations of the face parts) might change over time in a neural population (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, middle panel): (1) simple spike-rate adaptation predicts that initial rank-order selectivity (i.e. relative stimulus preference) will be largely preserved (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, dashed line) while neurons adapt their absolute response strength over time, (2) local normalization predicts that stronger responses are in some cases normalized to match weaker responses based on population activity to specific dimensions (<xref ref-type="bibr" rid="bib6">Carandini et al., 1997</xref>); importantly, normalization is strongest for nuisance (non-coding) dimensions (e.g. low versus high stimulus contrast) and in its idealized form would not alter selectivity along coding dimensions (e.g. typical versus scrambled feature configurations) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, dashed line), (3) evidence accumulation through temporal integration, winner-take-all through recurrent inhibition, or Bayesian inference through top-down feedback mechanisms all qualitatively predict a similar or stronger response over time for preferred features presented in the learned, typical face-part configuration versus presentation in an unexperienced atypical face-part configuration (<xref ref-type="bibr" rid="bib23">Lee and Mumford, 2003</xref>) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, light gray line), and (4) predictive coding posits that, for neurons that are coding error, their responses would show the opposite trend being greater for stimuli containing their preferred features but presented in configurations inconsistent with predictions of a typical frontal face (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, black line). Note, that error signaling is a qualitatively different computation than normalization, as error coding predicts a decreased response along the coding dimension (typical vs atypical configuration of features) whereas normalization would ideally not affect selectivity for typical versus atypical face-part configurations and only affect variation along orthogonal, nuisance dimensions. Properly testing these predictions (no change, increased, or decreased response over time to preferred face parts presented in typical versus atypical configurations) requires measurements from the intermediate stages of the hierarchy as all of these models operate under the premise that the system builds and maintains a preference for typically configured face parts at the top of the hierarchy (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, right, and see Introduction). Thus, the intermediate stages (here pIT, see <xref ref-type="fig" rid="fig1">Figure 1B</xref>) are most likely to be susceptible to confusions from typical/atypical face-part configurations and thus be influenced by, for example, the top-down mechanisms posited in Bayesian inference and predictive coding where higher areas encoding faces generate predictions of the face features and their locations that directly influence the responses of lower areas encoding those local face features (<xref ref-type="bibr" rid="bib23">Lee and Mumford, 2003</xref>; <xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>).</p><p>To ensure that we are observing purely visual predictions and not signals from other downstream top–down mechanisms, it is important to consider the effects of potential arousal and attention signals. To limit the effect of arousal to surprising novel face-part configurations, we presented atypical stimuli rapidly (100 ms on, 100 ms off) and in a randomly interleaved fashion with typical stimuli. Given that endogenous attention mechanisms operate on timescales of hundreds of milliseconds (<xref ref-type="bibr" rid="bib44">Ward et al., 1996</xref>; <xref ref-type="bibr" rid="bib30">Müller et al., 1998</xref>; <xref ref-type="bibr" rid="bib12">Egeth and Yantis, 1997</xref>) and that the preceding stimulus is not predictive of the next, dynamics observed during the first hundred milliseconds of the response are likely not the result of neural mechanisms that are hypothesized to be at work in endogenous attention.</p><sec id="s2-1"><title>Typical and atypical configurations of face parts driving similar initial responses in pIT</title><p>Here, we chose to focus our key, controlled tests on pIT – an intermediate stage in the ventral stream hierarchy, but the first stage within IT where neural specialization for face detection (i.e. face vs non-face) has been reported (<xref ref-type="bibr" rid="bib16">Grimaldi et al., 2016</xref>). Consistent with its intermediate position in the ventral visual system, we had previously found that pIT face-selective neurons are not truly selective for whole faces but respond to local face features, specifically those in the eye region (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>). Taking advantage of this prior result, we created face stimuli and similar but non-facelike stimuli that were customized to challenge the face processing system in that each would strongly drive pIT responses, thus forcing the higher IT stages to complete the discrimination between face and challenging non-face images based on higher-level information. To generate these challenging images, we systematically varied the positions of parts, in particular the eye, within the face (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>) (see Materials and methods). This set included images that contained face parts in positions consistent with a frontal view of a face or images that only differed in the relative spatial configuration of the face parts within the face outline (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left). Of the 82 images screened, we identified 21 part configurations that each drove the pIT population response to <underline>&gt;</underline>90% of its response to a correctly configured whole face. Of those 21 images, 13 images were inconsistent with the face-part configuration of a frontal face (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, black box). For the majority of the results that follow, we focus on comparing the neural responses to these 13 pIT-matched images that could <italic>not</italic> have arisen from frontal faces (referred to hereafter as ‘atypical face-part configurations’) with the 8 images that could have arisen from frontal faces (referred to hereafter as ‘typical face-part configurations’). Again, we stress that these two groups of images were selected to evoke closely matched initial pIT population activity.</p><p>Importantly, the pIT-matched images used here presented a more stringent test of face vs non-face discrimination than prior work. Specifically, most prior work used images of faces and non-face objects (‘classic images’) that contain differences across multiple dimensions including local contrast, spatial frequency, and types of features (<xref ref-type="bibr" rid="bib42">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib2">Afraz et al., 2006</xref>; <xref ref-type="bibr" rid="bib29">Moeller et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Sadagopan et al., 2017</xref>). Consistent with this, we found that the population decoding classification accuracy of our recorded neural populations using these classic images (faces versus non-face objects) is near perfect (&gt;99% in pIT, cIT, and aIT, n = 30 sites per region). However, we found that population decoding classification accuracy for the pIT-matched typical vs atypical face-part configurations we used here was near chance level (50%) in pIT (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, blue bar; by comparison, classification accuracy for face versus non-face objects classification was 99.6% using the same pIT sites). Further downstream in regions cIT and aIT, we found that the linear population decoding classification of these early pIT response-matched typical vs atypical face-part configurations was well above chance, suggesting that our pIT-matched face-part configuration detection challenge is largely solved somewhere between pIT and aIT (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p></sec><sec id="s2-2"><title>Time course of responses in pIT for images with typical versus atypical arrangements of the face parts</title><p>We next closely examined the pIT neural response dynamics. To do this, we defined a face-part configuration preference value (d’; see Materials and methods) that measured each site’s average selectivity for the typical face-part configurations relative to the atypical face-part configurations, and we asked how a given site’s preference evolved over time (see alternative hypotheses in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). First, we present three example sites which were chosen based on having the largest selectivity (absolute d’) in the late phase (100–130 ms post image onset). In particular, most standard interpretations of face processing would predict a late phase preference for typical face-part configurations, if any preference were to develop (d’ &gt; 0). However, all three sites with the largest absolute d’ preference had evolved a strong late phase preference for the atypical face-part configurations (d’&lt;0) despite having had very similar, robust rising edge responses to both stimulus classes (response in early phase from 60 to 90 ms, shows that we were able to achieve relatively well matched stimuli from a feedforward perspective) (<xref ref-type="fig" rid="fig2">Figure 2</xref>, left column). Thus, these sites, which responded strongly overall to both stimulus classes derived from faces consistent with the overall face preference of sites in face-selective IT cortex (i.e. stronger responses to faces than to non-face objects; <xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="bib42">Tsao et al., 2006</xref>), nonetheless demonstrated an additional (smaller) late modulation related to the configuration of the face parts. A late response modulation for atypical over typical face-part configurations was not restricted to the example sites as a majority of pIT sites (66%) responded more strongly to atypical over typical face-part configurations in the late response phase (prefer typical frontal face-part arrangement: 60–90 ms = 66% vs 100–130 ms = 34%; p = 0.000, n = 115) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, blue bars) even though almost all sites preferred faces over non-face objects throughout this time-course (60–90 ms = 98% vs 100–130 ms = 90%; p = 0.009, n = 115).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.42870.003</object-id><label>Figure 2.</label><caption><title>Responses in example sites to face-like images with typical and atypical face-part configurations.</title><p>The three sites with the highest selectivity in the late response phase in each region are shown (pIT, cIT, and aIT; left, middle, and right columns, respectively) (d’ selectivity measured in a 100–130 ms window, gray shaded region shown in bottom, left panel). While the three aIT sites (right column) demonstrated a late phase signal for the matched typical face context, the three pIT sites demonstrated the opposite preference in their late phase (100–130 ms) responses (red line = mean response of 8 images shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref> red box, and black line = mean response of 13 images shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref> black box).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42870-fig2-v2.tif"/></fig><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.42870.004</object-id><label>Figure 3.</label><caption><title>Time course of neural response preferences in pIT, cIT, and aIT for images with typical versus atypical face-part configurations.</title><p>(<bold>A</bold>) Preferences for typical vs atypical part arrangements for each site are plotted in both early (60–90 ms post image onset) and late (100–130 ms) time windows. Sites are grouped based on region (pIT, cIT, aIT) and whether they showed a significant change in selectivity from early to late time windows (light gray = increased preference, black = decreased preference, and dark gray = no change in preference for typical versus atypical face-part configurations, significance tested at p &lt; 0.01 level; example sites from <xref ref-type="fig" rid="fig2">Figure 2</xref> are plotted using thicker, darker lines). Many sites in pIT and cIT showed a decreasing signal for the typical face-part configuration context versus atypical configuration contexts over time (black lines, middle row, left and center panels). In contrast, no sites in aIT had this dynamic (middle row, right panel). (<bold>B</bold>) The fraction of sites whose responses showed a preference for images of typical, face-like arrangements of the face parts in pIT (blue), cIT (green) and aIT (red) in the early (60–90 ms) and late (100–130 ms) phase of the response. Note that, in the late phase of the response, most pIT neurons responded more strongly to atypical arrangements of face parts. (<bold>C</bold>) Selectivity measured for images driving similar responses within a site. This procedure ensured matched initial responses on a site-by-site basis rather than using a fixed set of images based on the overall population response (i.e. the fixed image set of <xref ref-type="fig" rid="fig1">Figure 1B</xref>; here, the initial d’ for 60–90 ms is close to zero when images are selected site by site). Although initial response differences were near zero when using site based image selection, a late phase signal that was stronger for atypical face-part configurations still emerged in pIT and cIT but not in aIT similar to the decreasing selectivity profile observed when using a fixed image set for all sites.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42870-fig3-v2.tif"/></fig><p>Next, we focused on the small but significant modulation of responses encoding the face-part configuration. Though this modulation can be relatively small compared to the absolute face response, the dynamics of face-part configuration selectivity (no change, increasing, decreasing) across the pIT population could provide insights into competing models of how additional, recurrent operations might modulate face processing (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In the adaptation and normalization models, we would expect no change in the average population selectivity, and the evidence accumulation, winner-take-all, or Bayesian inference models predict an increase in face configuration selectivity over the population over time. Instead, we found that many sites significantly decreased their typical face-part configuration preference over time similar to the three example sites. Of the 51 sites in our pIT sample that showed a significantly changing preference for typical face-part configurations over time (p &lt; 0.01 criterion for significant change in d’), 84% of these sites showed a decreasing preference (n = 43 of 51 sites, p &lt; 10^−6, binomial test, n = 115 total sites) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, left column, light gray vs black lines). This surprising trend -- decreasing relative response for typical face-part configurations versus atypical face-part configurations -- was observed in both monkeys when analyzed separately (p<sub>M1</sub> = 0.000, p<sub>M2</sub> = 0.002, n<sub>M1</sub> = 43, n<sub>M2</sub> = 72 sites; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Specifically, sites still behaved like classic face-selective sites (responded more to faces than non-face objects) even in the late phase of the response (median d’, 100–130 ms, faces vs non-face objects = 0.96 ± 0.06, n = 115 sites); however, we observed an additional, comparatively smaller response modulation encoding configuration information whereby typical face-part configurations drove weaker responses relative to atypical face-part configurations across the population in the late response phase (median d’, 100–130 ms, typical vs atypical face-part configurations = −0.12 ± 0.03, n = 115 sites). The observed trend for decreasing relative selectivity for typical face-part configurations over time over the population was driven by decreasing firing rates to the face images containing normally arranged face parts. Responses to these images were weaker by 18% on average in the late phase of the response compared to the early phase (Δrate (60–90 vs 100–130 ms) = −18% ± 4%, p = 0.000; n = 7 images) while responses to the images with atypical spatial arrangements of face parts -- also capable of driving high early phase responses -- did not experience any firing rate reduction in the late phase of the response (Δrate (60–90 vs 100–130 ms) = 2 ± 1%, p = 0.467; n = 13 images). The relative speed of this decreasing preference for typical face-part configurations – starting 30 milliseconds after response onset – argues against arousal or attention mechanisms as those phenomena occur over long timescales, and our stimuli were randomized to avoid top-down priming effects for typical or atypical images.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.42870.005</object-id><label>Figure 4.</label><caption><title>Individual monkey comparison and image controls for the decreasing selectivity profile in pIT.</title><p>(<bold>A</bold>) Preference for images with the typical face-part configuration analyzed separately for each monkey. Median d’ of pIT sites in both early and late time windows is shown. (<bold>B</bold>) Preference for images with typical versus atypical arrangements of the parts was re-computed using image subsets containing the same number of parts in the outline (the five 1-part and the three 4-part image subsets shown at top; the larger 2-part subset contained 30 images and is not shown). (<bold>C</bold>) The 1-part image subset was further tested at three different sizes (3<sup>o</sup>, 6<sup>o</sup>, and 12<sup>o</sup>). In all cases, pIT responses showed a decreasing preference over time for typically-arranged face parts leading to a preference for atypically arranged face parts in the later time window (100–130 ms).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42870-fig4-v2.tif"/></fig><p>The above observation of decreasing preference for typical face-part configurations over the pIT population seemed most consistent with predictions of error coding models (i.e. a conflict between the features represented locally and mismatched late-arriving predictions of those features from the face context), but one potential confound was that initial responses to typical and challenging, atypical configurations containing similar local features were not perfectly matched across the population (recall that we only required typical and atypical face-part configuration images to drive a response <underline>&gt;</underline>90% of the whole face response). As a result, initial selectivity was non-zero (d’=0.11, n = 115 sites). This residual preference for the typical face-part configuration images may be small, but if this residual face selectivity is driven by nuisance dimensions, for example excess stimulus contrast in the typical face-part configuration class relative to the atypical face-part configuration class, then the typical configuration class may have experienced stronger activity dependent adaptation or normalization resulting in a decreasing typical face-part configuration preference over time. To more adequately limit general activity dependent mechanisms that could lead to decreasing responses to typical face-part configurations, we performed control analyses where initial activity was tightly matched per site or where the number of parts was matched across images.</p></sec><sec id="s2-3"><title>Controls in pIT for firing rate and low-level image variation</title><p>To strictly control for the possibility that simple initial firing rate differences could predict the observed phenomenon, we re-computed selectivity after first matching initial responses site-by-site. For this analysis, images were selected on a per site basis to evoke similar initial firing rates (images driving initial response within 25% of synthetic whole face response for that site, at least 5 images required per class). This image selection procedure virtually eliminated any differences in initial responses between the images of typical and atypical face-part configurations and hence any firing rate difference driven by potential differences in nuisance parameters when rearranging the face parts (<xref ref-type="fig" rid="fig3">Figures 3C</xref> and 60–90 ms), yet we still observed a significant drop in preference for images with typical frontal face-part arrangements versus atypical face-part arrangements in pIT (Δ<sub>d’ </sub>= −0.10 ± 0.03, p = 0.001, n = 77) (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, blue line). Thus, the remaining dependence of firing rate dynamics on the image class and not on initial response strength argued against an exclusively activity based explanation to account for decreasing neural responses to typically configured face parts over time. Further arguing against this activity-dependent hypothesis, we found that the pattern of late phase population firing rates in pIT across images could not be significantly predicted from early phase pIT firing rates for each image (ρ<sub>pIT early, pIT late</sub> = 0.07 ± 0.17, p = 0.347; n = 20 images).</p><p>Thus far, we have performed analyses where images from the typical and atypical face-part configuration classes were similar in their initially evoked response which equated images at the level of neural activity but produced images varying in the number of parts. An alternative is to match the number of face parts between the typical and atypical configuration classes as another means of limiting the differences in nuisance dimensions such as the contrast, spatial frequency and retinal position of energy across images (see examples in <xref ref-type="fig" rid="fig4">Figure 4B</xref>). When we recomputed selectivity across subsets of images containing a matched number of one, two, or four parts (n = 5, 30, and 3 images, respectively), we still observed that pIT face selectivity decreased. For all three image subsets controlling the number of face parts, d’ of the pIT population began positive on average in the sampled pIT population (i.e. preferred frontal face-part arrangements in 60–90 ms post-image onset) (median d’ for 60–90 ms = 0.13 ± 0.05, 0.05 ± 0.02, 0.33 ± 0.09 for one, two, and four parts) and significantly decreased in the next phase of the response (100–130 ms post-image onset) becoming negative on average (median d’ for 100–130 ms: −0.27 ± 0.06,–0.14 ± 0.02,–0.04 ± 0.12; one, two, four parts: p = 0.000, 0.000, 0.004, for d’ comparisons between 60–90 ms and 100–130 ms, n = 115, 115, 76 sites) (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). A similar decreasing face-part configuration selectivity profile was observed when we re-tested single part images at smaller (3<sup>o</sup>) and larger (12<sup>o</sup>) image sizes suggesting a dependence on the relative configuration of the parts and not on their absolute retinal location or absolute retinal size (median d’ for 60–90 ms vs 100–130 ms: three degrees = 0.51 ± 0.09 vs −0.29 ± 0.14, twelve degrees = 0.07 ± 0.14 vs −0.11 ± 0.14; n = 15; p = 0.000, 0.025, 0.07) (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Thus, we suggest that the dynamic in pIT of a decreasing population selectivity for typical face-part configurations is a fairly robust phenomenon specific to the face versus non-face configuration dimension as this dynamic persisted even when limiting potential variation across nuisance dimensions.</p></sec><sec id="s2-4"><title>Time course of responses in aIT and cIT for images with typical versus atypical arrangements of face parts</title><p>While previous studies have suggested the presence of putative error-like signals in the ventral visual cortex broadly agreeing with our present observations, none of these studies have recorded under the same experimental conditions using the same stimuli from areas that may provide the necessary prediction signals for computing the errors, leaving open the question of whether these signals are generated within the same area or could arrive from higher cortical areas (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib39">Schwiedrzik and Freiwald, 2017</xref>). Here, we recorded from the anterior face-selective regions of IT which are furthest downstream of pIT and reflect additional stages of feedforward processing that could build selectivity for typical face-part configurations, a prerequisite for generating face predictions (see block diagram in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Indeed, in our aIT sample, the three sites with the greatest selectivity (absolute d’) in the late response phase (100–130 ms) all displayed a preference for typical frontal face-part configurations (d’ &gt; 0) (<xref ref-type="fig" rid="fig2">Figure 2</xref>, right column). Also, in contrast to the dynamic selectivity profiles observed in many pIT sites, 98% of aIT sites (39 of 40) did not significantly change their relative preference for typical versus atypical configurations of the face parts from their initial feedforward response (p &lt; 0.01 criterion for significant change at the site level) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right column, bottom row, dark gray sites). Rather, we observed a stable selectivity profile over time in aIT (median d’: 60–90 ms = 0.13 ± 0.03 vs 100–130 ms = 0.17 ± 0.03, p = 0.34, n = 40 sites). As a result, the majority of anterior sites preferred images with typical frontal configurations of the face parts in the late phase of the response (prefer typical face-part configuration: 60–90 ms = 78% of sites vs 100–130 ms = 78% of sites; p = 0.451, n = 40 sites; <xref ref-type="fig" rid="fig3">Figure 3B</xref>, red bars) despite only a minority (34%) of upstream sites in pIT preferring these images in their late response. Thus, spiking responses of individual aIT sites were as expected from a computational system whose purpose is to detect faces, as previously suggested (<xref ref-type="bibr" rid="bib15">Freiwald and Tsao, 2010</xref>). Furthermore, the responses of aIT sites in this relatively early response window (60–130 ms post image onset) were too rapid and in the opposite direction (prefer typical face-part configurations) to be accounted for by late-arriving arousal or attention signals to the novel, atypical face-part configuration stimuli. In cIT whose anatomical location is intermediate to pIT and aIT, we observed many sites with decreasing selectivity (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig3">3A</xref>, middle columns), a dynamic that persisted even when we tightly matched initial responses on a site by site basis (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, green line). The overall stimulus preference in cIT was intermediate to that of pIT and aIT (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) consistent with the intermediate position of cIT in the IT hierarchy.</p><p>To further test whether downstream areas cIT and aIT could be candidates for the putative prediction signals underlying face part prediction errors in pIT, we examined whether early response patterns in cIT and aIT were correlated to the later response in pIT. Interestingly, we found that the turning profiles across images in the early response phases of cIT and aIT were significant predictors of late phase activity in pIT (ρ<sub>cIT early, pIT late</sub> = -0.52 ± 0.11, p = 0.000; ρ<sub>aIT early, pIT late</sub> = -0.36 ± 0.14, p = 0.012; n<sub>pIT</sub> = 115, n<sub>cIT</sub> = 70, n<sub>aIT</sub> = 40 sites; n = 20 images), even better predictors than early phase activity in pIT itself (ρ<sub>pIT early, pIT late</sub> = 0.07 ± 0.17, p = 0.347). That is, for images that produced high early phase responses in cIT and aIT, the following later phase responses of units in the lower level area (pIT) tended to be low, consistent with error coding models which posit that feedback from higher areas (in the form of predictions of the face features) would contribute to the decreasing activity observed in lower areas encoding those face features.</p></sec><sec id="s2-5"><title>Computational models of neural dynamics in IT</title><p>We next proceeded to formalize the conceptual ideas introduced in <xref ref-type="fig" rid="fig1">Figure 1B</xref> and build neurally mechanistic, dynamical models of gradually increasing complexity to determine the minimal set of assumptions that could capture our empirical findings of non-trivial, dynamic selectivity changes during face detection across face-selective subregions in IT. This modeling effort is only intended to present at least one formal, working model of the observed population dynamics in IT which could complement previously reported phenomena in the literature that lacked a quantitative modeling framework. We submit that our model will inherently be underconstrained given the present, limited data. Much further circuit dissection work would need to be done to identify the sources of dynamics in pIT as these could arrive from downstream areas (as suggested by our correlative data between cIT/aIT early responses and pIT late responses) or could be shaped by lateral recurrences in pIT or both. Nonetheless, we leverage normative modeling principles for feedforward hierarchal processing and top-down hierarchical prediction (e.g. predictive coding, hierarchical Bayesian inference) to define at least one model class that can account for our data.</p><p>Previous functional and anatomical data show that the face-selective subregions in IT are connected forming an anterior to posterior hierarchy and show that pIT serves as the primary input into this hierarchy (<xref ref-type="bibr" rid="bib28">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib15">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib16">Grimaldi et al., 2016</xref>). Thus, we evaluated dynamics in different hierarchical architectures using a linear dynamical systems modeling framework where pIT, cIT, and aIT act as sequential stages of processing (network diagrams in <xref ref-type="fig" rid="fig5">Figure 5</xref> and see Materials and methods). A core principle of feedforward ventral stream models is that object selectivity is built by stage-wise feature integration in a manner that leads to relatively low dimensional representations at the top of the hierarchy abstracted from the high-dimensional input layer. We were interested in how signals temporally evolve across a similar architectural layout. We used the simplest feature integration architecture where a unit in a downstream area linearly sums the input from units in an upstream area, and we stacked this computation to form three layer networks (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This simple, generic feedforward encoding model conceptualizes the idea that different types of evidence, local and global (i.e. information about the parts and the relative spatial arrangement of parts), have to converge and be integrated to separate typical from atypical face-part configurations in our image set. We used linear networks as monotonic nonlinearities can be readily accommodated in our framework (<xref ref-type="bibr" rid="bib40">Seung, 1997</xref>; <xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>; also see <xref ref-type="fig" rid="fig6">Figure 6</xref>). Importantly, we used a simple encoding scheme as our goal was not to build full-scale deep neural network encoding models of image representation (<xref ref-type="bibr" rid="bib45">Yamins et al., 2014</xref>) but to bring focus to an important biological property that is often not considered in deep nets, neural dynamics.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.42870.006</object-id><label>Figure 5.</label><caption><title>Computational modeling of neural dynamics in IT.</title><p>(<bold>A</bold>) Three stage neural networks with recurrent dynamics were constructed to model neural signals measured in pIT, cIT, and aIT corresponding to the first (blue), second (green), and third (red) model processing stages (top row; see Materials and methods). Models received four inputs (gray) into four hidden stage units (blue) which sent feedforward projections that converged onto two units in the next layer (green) (self-connections reflecting leak currents are not shown here for clarity). State (feature) coding models generally showed increasing selectivity over time from hidden to output layers as exemplified by the feedforward model (left) and did not demonstrate the strong decrease of stimulus preference in their hidden processing stage as observed in the pIT and cIT neural population (blue and green lines, feedforward model shown). However, the neurons coding errors in a feedback-based hierarchical model did show a strong decrease of stimulus preference in the hidden processing stage (second column; reconstruction errors instead of the states were fit directly to the data). This model which codes the error signals (filled circles) also codes the states (open circles). Far right, population averaged neural selectivity profile for difference between typical, frontal versus atypical face-part arrangements (normalized by the mean population response to the whole face) used in model fitting (best fitting feedforward and error coding models are shown at left). (<bold>B</bold>) Goodness of fit of all three stage models tested to population averaged selectivity profiles (dashed lines represent mean and standard error of reliability of neural data as estimated by bootstrap resampling). Besides the base feedforward architecture, additional excitatory feedback (Bayesian inference) or lateral inhibitory (lateral inhibition or normalization) connections between units were implemented to produce recurrent dynamics. The goodness of fit to the population averaged neural data (far right in (<bold>A</bold>)) of the state coding models (first five bars) and of the reconstruction error coding model (last bar) are shown.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42870-fig5-v2.tif"/></fig><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.42870.007</object-id><label>Figure 6.</label><caption><title>Comparison of variants of error coding hierarchical models that use different algorithms for online inference.</title><p>(<bold>A</bold>) Additional varieties of error computing networks can be generated by varying the online inference algorithm that they use. In one case, inference does not utilize top-down information between stages (classic error backpropagation; between-stage feedback connections shown are not used in these networks during runtime). On the other hand, between-stage feedback can be used to optimize online estimates such as in more general forms of error backpropagation and predictive coding. We approximated these two extremes by including a parameter (<italic>k<sub>td</sub></italic>, see Materials and methods) controlling the relative weighting of bottom-up (feedforward) and top-down (feedback) evidence during online inference (first and second panels). We found that top-down inference between stages was not necessary to produce the appropriate error signal dynamics, and <italic>k<sub>td</sub></italic> was equal to zero (similar to the lack of inference in classic error backpropagation) in our best fitting two-layer (first column) and three-layer (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, second column) models although models with <italic>k<sub>td</sub> ~ 1</italic> also performed well (second column). Models can also differ in their goal (cost function) which directly impacts the error signals computed (equations in top row). Under a nonlinear reconstruction goal (emulating the nonlinear nature of spiking output), the resulting error signals were still consistent with our data (third column). A simple sigmoidal nonlinearity, however, did lead to additional details present in our neural data such as a rapid return of stimulus preference to zero in the hidden layer. When we tested a discriminative, construction goal more consistent with a supervised learning setting where high-dimensional bottom-up responses simply have to match a low-dimensional downstream target signal as in classification tasks, we found that the errors of construction did not match the data as well as reconstruction errors (compare fourth column to first three columns). (<bold>B</bold>) Goodness of fit to population neural data for all two stage models including two layer versions of state (feature) coding controls (same format as <xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42870-fig6-v2.tif"/></fig><p>We implemented a range of ideas previously proposed in the literature. The functional implications of these ideas were highlighted in <xref ref-type="fig" rid="fig1">Figure 1B</xref>; at a mechanistic level, these functional properties can be directly realized via different recurrent processing motifs between neurons (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, base feedforward model was augmented with recurrent connections to form new models). We focus on error encoding models since the observed neural phenomena in pIT and their relationship to responses in cIT and aIT suggested the generation of a prediction error of pIT preferred local features in late phase pIT responses. Here, we asked whether a dynamical systems implementation of error coding in a hierarchical prediction network could account for the temporal response patterns observed neurally. To constrain our choice of an error coding model, we took a normative approach minimizing a quadratic reconstruction cost between stages (top stages predict their input stages) as the classical reconstruction cost is at the core of an array of hierarchical generative models including hierarchical Bayesian inference (<xref ref-type="bibr" rid="bib23">Lee and Mumford, 2003</xref>), Boltzmann machines (<xref ref-type="bibr" rid="bib1">Ackley et al., 1985</xref>), analysis-by-synthesis networks (<xref ref-type="bibr" rid="bib40">Seung, 1997</xref>), sparse coding (<xref ref-type="bibr" rid="bib33">Olshausen and Field, 1996</xref>), predictive coding (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>), and autoencoders in general (<xref ref-type="bibr" rid="bib35">Rifai et al., 2011</xref>). Optimizing a quadratic loss results in feedforward and feedback connections that are symmetric -- reducing the number of free parameters -- such that inference on the represented variables at any intermediate stage is influenced by both bottom-up sensory evidence and current top-down interpretations. Critically, a common feature of this large model family is the computation of between-stage error signals via feedback, which is distinct from state-estimating model classes (i.e. feedforward models) that do not compute or propagate errors. A dynamical implementation of such a network uses leaky integration of error signals which, as shared computational intermediates, guide gradient descent of the values of the represented variables to a previously learned target value (∆activity of each neuron =&gt; online inference) or descend the connection weights to values that give the best future behavior (∆synaptic strengths =&gt; offline learning), here defined as an unsupervised reconstruction goal (similar results were found using other goals and networks such as supervised discriminative networks; see <xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>We found that the dynamics of error signals in our hierarchical model naturally displayed a strong decrease of selectivity in a sub-component of its first processing stage -- qualitatively similar behavior to the selectivity decrease that we observed in many pIT and cIT neural sites (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, second column, blue and green curves). These error signals integrate converging state signals from two stages -- one above (prediction) and one below (sensory evidence). The term ‘error’ is thus meaningful in the hidden processing stages where state signals from two stages can converge. The top nodes of a hierarchy receive little descending input and hence do not carry additional errors with respect to the desired computation; rather, top nodes convey the face predictions that influence errors in bottom nodes. This behavior in the higher processing stages is consistent with our observation of explicit representation of faces in aIT in all phases of the response (<xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig3">3</xref>, aIT data) and with similar observations of decodable identity signals by others in all phases of aIT responses for faces (<xref ref-type="bibr" rid="bib27">Meyers et al., 2015</xref>) and objects (<xref ref-type="bibr" rid="bib17">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Majaj et al., 2015</xref>). We also found similar error dynamics when using a simpler two-layer network as opposed to three layers suggesting that these error signal dynamics along with prediction signals emerge even in the simplest cascaded architecture (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>For control comparisons, we also implemented a range of feature coding models beginning with a basic feedforward model and augmenting it with lateral connections (winner-take-all lateral inhibition and normalization models) or feedback connections (hierarchical Bayesian inference) (<xref ref-type="bibr" rid="bib6">Carandini et al., 1997</xref>; <xref ref-type="bibr" rid="bib40">Seung, 1997</xref>). However, all of these state coding control models failed to reproduce the observed neural dynamics across the ventral visual hierarchy. Rather, the selectivity of these models simply increased to a saturation level set by the leak term (shunting inhibition) in the system as in the strictly feedforward model (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, first column). That adding normalization proved insufficient to generate the observed neural dynamics can be explained by the fact that the normalized response to a stimulus cannot easily fall below the response to a stimulus that was initially similar in strength. Thus, a decreasing average preference for a stimulus across a population of cells (i.e. <xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig4">4</xref>, pIT data) for similar levels of average input is difficult when only using a basic normalization model mediated by surround (within-stage) suppression.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have measured neural responses during a difficult discrimination between images with typical and atypical face-part configurations across the IT hierarchy and demonstrated that the population preference for normally configured face parts in the intermediate (a.k.a hidden) processing stages decreases over time – that is population responses at lower levels of the hierarchy (pIT and cIT) signal deviations of their preferred features from their expected configuration whereas the top level (aIT) rapidly developed and then maintained a preference for natural, frontal face-part configurations. The relative speed of selectivity changes in pIT makes high-level explanations based on fixational eye movements or shifts in attention (e.g. from behavioral surprise to unnatural arrangements of face parts) unlikely as saccades and attention shifts occur on slower timescales (hundreds of milliseconds) (<xref ref-type="bibr" rid="bib12">Egeth and Yantis, 1997</xref>; <xref ref-type="bibr" rid="bib30">Müller et al., 1998</xref>; <xref ref-type="bibr" rid="bib44">Ward et al., 1996</xref>) than the ~30 ms dynamical phenomena we observed. The presence of stronger responses to typical than to atypical face-part configuration images in aIT further argues against general arousal effects which would have predicted stronger not weaker responses to surprising, atypical images in aIT. Rather, the rapid propagation of neural signals over tens of milliseconds suggested intracortical processing within the ventral visual stream in a manner that was not entirely consistent with a pure feedforward model, even when we included strong nonlinearities in these models such as normalization and even when we stacked these operations to form more complex three stage models. However, augmenting the feedforward model so that it represented the prediction errors generated during hierarchical processing of atypical configurations produced the observed neural dynamics and hierarchical pattern of signal propagation (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6</xref>). This view argues that many IT neurons code error signals. However, the exact mechanism for producing prediction errors remains to be determined. While we showed that a recurrent model could recapitulate the observed signals, how this model maps to the IT network is unclear. Recurrence could be implemented by a circuit within pIT which computes the predictions that lead to prediction errors within the same region. Whether the error computation is done internally in pIT or depends on downstream sources such as cIT or aIT can be directly tested by causal knock-outs of cIT or aIT.</p><sec id="s3-1"><title>Comparison to previous neurophysiology studies in IT</title><p>Multiple visual neurophysiology studies have shown evidence of neural responses consistent with error signals. This includes the seminal predictive coding study on end-stopping in V1 (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>). More recently, studies in IT have used pairing of images over time to create sequences with predictable temporal structure and found evidence of putative error signals when those statistically exposed temporal predictions were violated (<xref ref-type="bibr" rid="bib26">Meyer et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Schwiedrzik and Freiwald, 2017</xref>). Our work expands those IT findings in three directions. First, we revealed error coding dynamics that are naturally present in the system without using statistical exposure or behavioral training to induce signals. Second, errors depended on the spatial statistics of the features -- rather than depending on temporal statistics -- which may be more directly related to native, spatial form processing in IT. Third and perhaps most importantly, we identified a putative source of face prediction signals in downstream IT by recording from multiple areas in the same experiment and showing that early signals in anterior areas were (negatively) correlated with late signals in pIT. Together, these advances suggest a more definitive role of error signaling in natural, online vision. Formalizing this claim, we found that the pattern of observed dynamics in pIT, cIT, and aIT were indeed difficult to account for quantitatively when using feature coding models but could be computationally modeled at a population level using a simple error coding model (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Extensions of our dynamical modeling framework to more realistic large-scale networks could be useful for future studies of IT response dynamics.</p><p>Our suggestion that many IT neurons code errors is consistent with the observation of strong responses to extremes in face space (<xref ref-type="bibr" rid="bib24">Leopold et al., 2006</xref>) providing an alternative interpretation to the prior suggestion that cIT neurons are not tuned for typical faces but are instead tuned for atypical face features (i.e. extreme feature tuning) (<xref ref-type="bibr" rid="bib14">Freiwald et al., 2009</xref>). In that prior work, the response preference of each neuron was determined by averaging over a long time window (~200 ms). By looking more closely at the fine time scale dynamics of the IT response, we suggest that this same extreme coding phenomenon can instead be interpreted as a natural consequence of networks that have an actual tuning preference for face features in typical configurations (as evidenced by an initial, feedforward response preference for typical frontal faces in pIT, cIT, and aIT; <xref ref-type="fig" rid="fig3">Figure 3B</xref>) but that also compute error signals with respect to that preference. Under the present hypothesis, some IT neurons are preferentially tuned to typical spatial arrangements of face features, and other IT neurons are involved in coding errors with respect to those typical arrangements. We speculate that these intermixed state estimating and error coding neuron populations are both sampled in standard neural recordings of IT, even though only state estimating neurons are truly reflective of the tuning preferences of that IT processing stage. The precise fractional contribution of errors to total neural activity is difficult to estimate from our data. Under the primary image condition tested, not all sites significantly decreased their selectivity (~60% did not change their selectivity). We currently interpret these sites as coding state (feature) estimates (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, light and dark gray lines in top and bottom rows, respectively). Alternatively, at least some of the non-reversing sites might be found to code errors under other image conditions than the one that we tested. Furthermore, in our primary image condition, selectivity decreases only accounted for ~15% of the overall spiking. However, at a computational level, the absolute contribution of error signals to spiking may not be the critical factor as even a small relative contribution may have important consequences in the network.</p></sec><sec id="s3-2"><title>Comparison across dynamical models of neural processing</title><p>Our goal was to test a range of existing recurrent models by recording neural dynamics across multiple cortical stages which provided stronger constraints on computational models than fitting neural responses from only one area as in prior work (<xref ref-type="bibr" rid="bib6">Carandini et al., 1997</xref>; <xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>). Crucially, we found that the multi-stage neural dynamics observed in our data could not be adequately fit by only using lateral recurrences such as adaptation, lateral inhibition, and standard forms of normalization (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). These results did not change when we made our simple networks more complex by adding more stages (compare <xref ref-type="fig" rid="fig5">Figure 5</xref> versus <xref ref-type="fig" rid="fig6">Figure 6</xref>) or by using more realistic model units with monotonic nonlinearities similar to a spiking nonlinearity (data not shown). Indeed, we specifically chose our stimuli to evoke similar levels of within stage neural activity to limit the effects of known mechanisms that depend on activity levels through lateral interactions (e.g. adaptation, normalization), and we fully expect that these activity dependent mechanisms would operate in parallel to top-down, recurrent processes during general visual processing. We emphasize that we only tested the standard form of normalization as originally proposed, using within stage pooling and divisive mechanisms (<xref ref-type="bibr" rid="bib6">Carandini et al., 1997</xref>). Since that original mechanistic formulation, normalization has evolved to become a term that broadly encapsulates many forms of suppression phenomena and can include both lateral interactions within an area and feedback interactions from other areas (<xref ref-type="bibr" rid="bib31">Nassi et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Coen-Cagli et al., 2015</xref>). Thus, while our results do not follow from the original mechanistic form of normalization, they may yet fall under normalization more broadly construed as a term for suppression phenomena (error coding would require a similar suppressive component). Here, we have provided a normative model for how top-down suppression would follow from the well-defined computational goals of many hierarchical neural network models. Finally, we clarify that any top-down interactions instantiated in coding errors need not originate in other areas but could happen within the same area (e.g. layer 2/3 predictions interacting with layer 4) which could be viewed as a local feedback interaction with respect to the whole network, and this is a testable mechanistic hypothesis that is not ruled out by the present work.</p></sec><sec id="s3-3"><title>Computational utility of coding errors in addition to states</title><p>The present study provides evidence that errors are not only computed, but that they might be explicitly encoded in spiking rates. We emphasize that this result at the level of population neural dynamics was robust across choices of cost function; we tested models with different unsupervised and supervised performance errors (reconstruction, nonlinear reconstruction, and discriminative) and found similar population level error signals across these networks (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Thus, errors as generally instantiated in the state-error coding hierarchical model family provide a good approximation to IT population neural dynamics. In error-computing networks, errors provide control signals for guiding learning giving these networks additional adaptive power over basic feature estimation networks. This property helps augment the classical, feature coding view of neurons which, with only feature activations and Hebbian operations, does not lead to efficient learning in the manner produced by gradient descent using error backpropagation (<xref ref-type="bibr" rid="bib36">Rumelhart et al., 1986</xref>). Observation of error signals may provide insight into how more intelligent unsupervised and supervised learning algorithms such as backpropagation could be plausibly implemented in the brain. A potentially important contribution of this work is the suggestion that gradient descent algorithms are facilitated by using an error code so that efficient learning is reduced to a simple Hebbian operation at synapses and efficient inference is simply integration of inputs at the cell body (see <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> and text in Materials and methods). This representational choice, to code the computational primitives of gradient descent in spiking activity, would simply leverage the existing biophysical machinery of neurons for inference and learning.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals and surgery</title><p>All surgery, behavioral training, imaging, and neurophysiological techniques are identical to those described in detail in previous work (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>). Two rhesus macaque monkeys (<italic>Macaca mulatta</italic>) weighing 6 kg (Monkey 1, female) and 7 kg (Monkey 2, male) were used. A surgery using sterile technique was performed to implant a plastic fMRI compatible headpost prior to behavioral training and scanning. Following scanning, a second surgery was performed to implant a plastic chamber positioned to allow targeting of physiological recordings to posterior, middle, and anterior face patches in both animals. All procedures were performed in compliance with National Institutes of Health guidelines and the standards of the MIT Committee on Animal Care and the American Physiological Society.</p></sec><sec id="s4-2"><title>Behavioral training and image presentation</title><p>Subjects were trained to fixate a central white fixation dot during serial visual presentation of images at a natural saccade-driven rate (one image every 200 ms). Although a 4<sup>o</sup> fixation window was enforced, subjects generally fixated a much smaller region of the image (&lt;1<sup>o</sup>) (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>). Images were presented at a size of 6<sup>o</sup> except for control tests at 3<sup>o</sup> and 12<sup>o</sup> sizes (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), and all images were presented for 100 ms duration with 100 ms gap (background gray screen) between each image. Images were presented in a randomly interleaved fashion at this rate of 5 images per second, so subjects could not predict the image class (e.g. face vs non-face or typical vs atypical face-part configuration) and were more likely to engage automatic processing of the visual stimuli. Up to 15 images were presented during a single fixation trial, and the first image presentation in each trial was discarded from later analyses. Five repetitions of each image in the general screen set were presented, and ten repetitions of each image were collected for all other image sets. The screen set consisted of a total of 40 images drawn from four categories (faces, bodies, objects, and places; 10 exemplars each) which was used to derive a measure of face versus non-face object selectivity (faces versus bodies, objects, and places grouped together).</p><p>Following the screen set testing, some sites were tested using an image set containing images of face parts presented in different combinations and positions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left panel). We first segmented the face parts (eye, nose, mouth) from a monkey face image. These parts were then blended using a Gaussian window, and the face outline was filled with pink noise to create a continuous background texture. A face part could appear on the outline at any one of nine positions on an evenly spaced 3 × 3 grid. Although the number of possible images is large (4<sup>9</sup> = 262,144 images), we chose a subset of these images for testing neural sites (n = 82 images). Specifically, we tested the following images: the original whole face image, the noise-filled outline, the whole face reconstructed by blending the four face parts with the outline, all possible single part images where the eye, nose, or mouth could be at one of nine positions on the outline (n = 3 × 9 = 27 images), all two part images containing a nose, mouth, left eye, or right eye at the correct outline-centered position and an eye tested at all remaining positions (n = 4*8–1 = 31 images), all two part images containing a correctly positioned contralateral eye while placing the nose or mouth at all other positions (n = 2*8–2 = 14 images), and all correctly configured faces but with one or two parts missing besides those already counted above (n = 4 + 3 = 7 images). The particular two-part combinations tested were motivated by prior work demonstrating the importance of the eye in early face processing (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>), and we sought to determine how the position of the eye relative to the outline and other face parts was encoded in neural responses. The three and four part combinations were designed to manipulate the presence or absence of a face part for testing the integration of face parts, and in these images, we did not vary the positions of the parts from those in a naturally occurring face. In a follow-up test on a subset of sites, we permuted the position of the four face parts under the constraint that they still formed the configuration of a naturally occurring face (i.e. preserve the ‘T’ configuration, n = 10 images; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). We tested single part images at 3<sup>o</sup> and 12<sup>o</sup> sizes in a subset of sites (n = 27 images at each size; <xref ref-type="fig" rid="fig4">Figure 4C</xref>).</p></sec><sec id="s4-3"><title>MR imaging and neurophysiological recordings</title><p>Both structural and functional MRI scans were collected in each monkey. Putative face patches were identified in fMRI maps of face versus non-face object selectivity in each subject. A stereo microfocal x-ray system (<xref ref-type="bibr" rid="bib11">Cox et al., 2008</xref>) was used to guide electrode penetrations in and around the fMRI defined face-selective subregions of IT. X-ray based electrode localization was critical for making laminar assignments since electrode penetrations are often not perpendicular to the cortical lamina when taking a dorsal-ventral approach to IT face patches. Laminar assignments of recordings were made by co-registering x-ray determined electrode coordinates to MRI where the pial-to-gray matter border and the gray-to-white matter border were defined. Based on our prior work estimating sources of error (e.g. error from electrode tip localization and brain movement), registration of electrode tip locations to MRI brain volumes has a total of &lt;400 micron error which is sufficient to distinguish deep from superficial layers (<xref ref-type="bibr" rid="bib20">Issa et al., 2013</xref>). Multi-unit activity (MUA) was systematically recorded at 300 micron intervals starting from penetration of the superior temporal sulcus such that all sites at these regular intervals were tested with a screen set containing both faces and non-face objects, and a subset of sites that were visually driven were further tested with our main image set manipulating the position of face parts. Although we did not record single-unit activity, our previous work showed similar responses between single-units and multi-units on images of the type presented here (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>), and our results are consistent with observations in previous single-unit work in IT (<xref ref-type="bibr" rid="bib14">Freiwald et al., 2009</xref>). Recordings were made from PL, ML, and AM in the left hemisphere of monkeys 1 and 2 and additionally from AL in monkey 2. AM and AL are pooled together in our analyses forming the aIT sample while PL and ML correspond to the pIT and cIT samples, respectively.</p></sec><sec id="s4-4"><title>Neural data analysis</title><p>The face patches were physiologically defined in the same manner as in our previous study (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>). Briefly, we fit a graded 3D sphere model (linear profile of selectivity that rises from a baseline value toward the maximum at the center of the sphere) to the spatial profile of face versus non-face object selectivity across our sites. We tested spherical regions with radii from 1.5 to 10 mm and center positions within a 5 mm radius of the fMRI-based centers of the face patches. The resulting physiologically defined regions were 1.5 to 3 mm in diameter. Sites which passed a visual response screen (mean response in a 60–160 ms window &gt;2*SEM above baseline for at least one of the four categories in the screen set) were included in further analysis. All firing rates were baseline subtracted using the activity in a 25–50 ms window following image onset averaged across all repetitions of an image. Finally, given that the visual response latencies in monkey two were on average 13 ms slower than those in monkey one for corresponding face-selective regions, we applied a single latency correction (13 ms shift to align monkey 1 and monkey 2’s data) prior to averaging across monkeys. This was done so as not to wash out any fine timescale dynamics by averaging. Similar results were obtained without using this latency correction as dynamics occurred at longer timescales (~30 ms). This single absolute adjustment was more straightforward than the site-by-site adjustment used in our previous work (<xref ref-type="bibr" rid="bib19">Issa and DiCarlo, 2012</xref>) (though similar results were obtained using this alternative latency correction). Even when each monkey was analyzed separately, we still observed pIT selectivity dynamics (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Furthermore, there was &lt;10 ms average latency difference between pIT, cIT, and aIT so that a common 30 ms wide analysis window for early (60–90 ms) and late (100–130 ms) firing rates was sufficient across IT stages. Images that produced an average population response <underline>&gt;</underline>0.9 of the initial response (60–100 ms) to a face image with all face parts arranged in their typical positions in a frontal face were analyzed further (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). Stimulus selection was intended to limit potentially confounding differences in visual drive between image classes. In a control test, we also repeated our analysis by selecting images on a site-by-site basis where images with typical frontal and atypical arrangements of face parts were chosen to be within 0.75x to 1.25x of the initial response to the complete face image (minimum of five typical and five atypical images in this response range for inclusion of site in analysis). In follow-up analyses of population responses, we specifically limited comparison to images with the same number of parts (<xref ref-type="fig" rid="fig4">Figure 4B,C</xref>). For example, for single part images, we used the image with the eye in the upper, contralateral region of the outline as a reference requiring a response <underline>&gt;</underline>0.9 of the initial population response to this reference for inclusion of the images in this analysis. We found that four other images of the 27 single-part images elicited a response at least as large as 90% of the response to this standard image. For images containing all four face parts, we used the complete, frontal face as the standard and found atypical face-part arrangements of the four face parts that drove at least 90% of the early response to the whole face (2 images out of 10 tested). To measure decoding performance for typical versus atypical face-part configurations (or face versus non-face objects from our screen set), we used a linear-SVM classifier trained on responses (60–200 ms post image onset) of resampled subsets of 30 sites from pIT, cIT, or aIT. Trials splits were used so that all images were used in training and tested but on separate, held-out trials (90% train, 10% test). To compute individual site d’ for each stimulus partition (e.g. typical versus atypical arrangements of 1 face part), we combined all presentations of images with frontal face-part arrangements and compared these responses to responses from all presentations of images with atypical face-part arrangements using <italic>d’ =</italic> (<italic>u<sub>1</sub>- u<sub>2</sub></italic>)/((<italic>var<sub>1</sub> +var<sub>2</sub></italic>)/2)<sup>1/2</sup> where variance was computed across all trials for that image class (e.g. all presentations of all typical face-part configuration images); this was identical to the d’ measure used for face versus non-face object selectivity in <xref ref-type="fig" rid="fig1">Figure 1</xref> and Results and to that used in previous work for computing selectivity for faces versus non-face objects (<xref ref-type="bibr" rid="bib3">Aparicio et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Ohayon et al., 2012</xref>). For example, for the main image set (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), we compared all presentations of typical face-part configuations (8 images x 10 presentations/image = 80 total presentations) to all presentations of atypical face-part arrangements (13 images x 10 presentations/image = 130 total presentations) to compute the d’ values for each site in two time windows (60–90 ms and 100–130 ms) as shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. A positive d’ implies a stronger response to more naturally occurring typical frontal arrangements of face parts while a negative d’ indicates a preference for atypical arrangements of the face parts.</p></sec><sec id="s4-5"><title>Dynamical models</title><sec id="s4-5-1"><title>Modeling framework and equations</title><p>To model the dynamics of neural response rates in a hierarchy, we start with the simplest possible model that might capture those dynamics: a model architecture consisting of a hidden stage of processing containing two units that linearly converge onto a single output unit. We use this two-stage cascade for illustration of the basic concepts which can be easily extended to longer cascades with additional stages, and we ultimately used a three-stage version of the model to fit our neural data collected from three cortical stages (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>An external input is applied separately to each hidden stage unit, which can be viewed as representing different features for downstream integration. We vary the connections between the two hidden units within the hidden processing stage (lateral connections) or between hidden and output stage units (feedforward and feedback connections) to instantiate different model families. The details of the different architectures specified by each model class can be visualized by their equivalent neural network diagrams (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Here, we provide a basic description for each model tested using the two stage example for simplicity. All two stage models utilized a 2 × 2 feedforward identity matrix <italic>A</italic> that simply transfers inputs <bold><italic>u</italic></bold> (2 × 1) to hidden layer units <bold><italic>x</italic></bold> (2 × 1) and a 1 × 2 feedforward vector <italic>B</italic> that integrates hidden layer activations <bold><italic>x</italic></bold> into a single output unit <italic>y</italic>.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>By simply substituting in the appropriate unit vector and weight matrix transforming inputs from one layer to the next for the desired network architecture, this simple two-stage architecture can be extended to larger networks (e.g. see three-stage network diagrams in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). To generate dynamics in the simple networks below, we assumed that neurons act as leaky integrators of their total synaptic input, a standard rate-based model of a neuron used in previous work (<xref ref-type="bibr" rid="bib40">Seung, 1997</xref>),(<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>).</p></sec><sec id="s4-5-2"><title>Pure feedforward</title><p>In the purely feedforward family, connections are exclusively from hidden to output stages through feedforward matrices <italic>A</italic> and <italic>B</italic>.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>τ</italic> is the time constant of the leak current which can be seen as reflecting the biophysical limitations of neurons (a perfect integrator with large <italic>τ</italic> would have almost no leak and hence infinite memory).</p></sec><sec id="s4-5-3"><title>Lateral inhibition</title><p>Lateral connections (matrix with off-diagonal terms) are included and are inhibitory. The scalar <italic>k<sub>l</sub></italic> sets the relative strength of lateral inhibition versus bottom-up input.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5-4"><title>Normalization</title><p>An inhibitory term that scales with the summed activity of units within a stage is included. The scalar <italic>k<sub>s</sub></italic> sets the relative strength of normalization versus bottom-up input.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:mi>x</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>y</mml:mi><mml:mo>⋅</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5-5"><title>Normalization (nonlinear) (<xref ref-type="bibr" rid="bib6">Carandini et al., 1997</xref>)</title><p>The summed activity of units within a stage is used to nonlinearly scale shunting inhibition.<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:mi>x</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>y</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that this is technically a nonlinear dynamical system, and since the normalization term in <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref> is not continuously differentiable, we used the fourth-order Taylor approximation around zero in the simulations of <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>.</p></sec><sec id="s4-5-6"><title>Feedback (linear reconstruction)</title><p>The feedback-based model is derived using a normative framework that performs optimal inference in the linear case (<xref ref-type="bibr" rid="bib40">Seung, 1997</xref>) (unlike the networks in <xref ref-type="disp-formula" rid="equ2 equ3 equ4 equ5">Equations (2)-(5)</xref> which are motivated from a mechanistic perspective but do not directly optimize a squared error performance loss). The feedback network minimizes the cost <italic>C</italic> of reconstructing the inputs of each stage (i.e. mean squared error of layer <italic>n</italic> predicting layer <italic>n-1</italic>).<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Differentiating this coding cost with respect to the encoding variables in each layer <bold><italic>x</italic></bold>, <italic>y</italic> yields:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The cost function <italic>C</italic> can be minimized by descending these gradients over time to optimize the values of <bold><italic>x</italic></bold> and <italic>y</italic>:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The above dynamical equations are equivalent to a linear network with a connection matrix containing symmetric feedforward (<italic>B</italic>) and feedback (<italic>B<sup>T</sup></italic>) weights between stages <bold><italic>x</italic></bold> and <italic>y</italic> as well as within-stage pooling followed by recurrent inhibition (-<italic>AA<sup>T</sup><bold>x</bold></italic> and <italic>-BB<sup>T</sup>y</italic>) that resembles normalization. The property that symmetric connections minimize the cost function <italic>C</italic> generalizes to a feedforward network of any size or number of hidden processing stages (i.e. holds for arbitrary lower triangular network connection matrices). The final activation states (<bold><italic>x</italic></bold>,<italic>y</italic>) of the hierarchical generative network are optimal in the sense that the bottom-up activations (implemented through feedforward connections) are balanced by the top-down expectations (implemented by feedback connections) which is equivalent to a Bayesian network combining bottom-up likelihoods with top-down priors to compute the maximum <italic>a posteriori</italic> (MAP) estimate. Here, the priors are embedded in the weight structure of the network. In simulations, we include an additional scalar <italic>k<sub>td</sub></italic> that sets the relative weighting of bottom-up versus top-down signals.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5-7"><title>Error signals computed in the feedback model</title><p>In <xref ref-type="disp-formula" rid="equ9">Equation (9)</xref>, inference can be thought of as proceeding through integration of inputs on the dendrites of neuron population <bold><italic>x</italic></bold>. In this scenario, all computations are implicit in dendritic integration. Alternatively, the computations in <xref ref-type="disp-formula" rid="equ9">Equation (9)</xref> can be done in two steps where, in the first step, reconstruction errors are computed (i.e. <bold><italic>e<sub>0</sub></italic></bold> <italic>= <bold>u-</bold>A<sup>T</sup><bold>x</bold>, <bold>e<sub>1</sub></bold> = <bold>x-</bold>B<sup>T</sup>y</italic>) and explicitly represented in a separate error coding population. These error signals can then be integrated by their downstream target population to generate the requisite update to the state signal of neuron population <bold><italic>x</italic></bold>.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>An advantage of this strategy is that the a state unit now directly receives errors as inputs, and those inputs allow implementation of an efficient Hebbian rule for learning weight matrices (<xref ref-type="bibr" rid="bib34">Rao and Ballard, 1999</xref>) -- the gradient rule for learning is simply a product of the state activation and the input error activation (weight updates obtained by differentiating <xref ref-type="disp-formula" rid="equ6">Equation (6)</xref> with respect to weight matrices <italic>A</italic> and <italic>B</italic>: ΔA = <bold>x</bold>•<bold><italic>e<sub>0</sub></italic></bold><italic><sup>T</sup></italic>, ΔA<italic><sup>T</sup></italic> = <bold><italic>e</italic><italic><sub>0</sub></italic></bold>•<bold>x<italic><sup>T</sup></italic></bold>, ΔB = y•<bold><italic>e<sub>1</sub></italic></bold><italic><sup>T</sup></italic>, and ΔB<italic><sup>T</sup></italic> = <bold><italic>e</italic><italic><sub>1</sub></italic></bold>•y). Thus, the reconstruction errors serve as computational intermediates for both the gradients of online inference mediated by dendritic integration (dynamics in state space, <xref ref-type="disp-formula" rid="equ10">Equation (10)</xref>) and gradients for offline learning mediated by Hebbian plasticity (dynamics in weight space).</p><p>In order for the reconstruction errors at each layer to be scaled appropriately in the feedback model, we invoke an additional downstream variable <italic>z</italic> to predict activity at the top stage such that, instead of <bold><italic>e<sub>2</sub></italic></bold> <italic>= y</italic> which scales as a state variable, we have <bold><italic>e<sub>2</sub></italic></bold> <italic>= y-C<sup>T</sup>z</italic> (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). This overall model reflects a state and error coding model as opposed to a state only model.</p></sec><sec id="s4-5-8"><title>Feedback (three-stage)</title><p>For the simulations in <xref ref-type="fig" rid="fig5">Figure 5</xref>, three-stage versions of the above equations were used. These deeper networks were also wider such that they began with four input units (<bold><italic>u</italic></bold>) instead of only two inputs in the two-stage models. These inputs converged through successive processing stages (<bold><italic>w,x,y</italic></bold>) to one unit at the top node (<bold><italic>z</italic></bold>) (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></sec><sec id="s4-5-9"><title>Feedback (nonlinear reconstruction)</title><p>To test the generality of our findings beyond a linear reconstruction cost, we simulated feedback-based models which optimized different candidate cost functions proposed for the ventral stream (<xref ref-type="fig" rid="fig6">Figure 6</xref>). In nonlinear hierarchical inference, reconstruction is performed using a monotonic nonlinearity with a threshold (<italic>th</italic>) and bias (<italic>bi</italic>):<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5-10"><title>Feedback (linear construction)</title><p>Instead of a reconstruction cost where responses match the input (i.e. generative model) as in unsupervised learning, we additionally simulated the states and errors in a feedback network minimizing a linear construction cost where the network is producing responses to match a given output (i.e. discriminative model) similar to supervised learning:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5-11"><title>Model simulation</title><p>To simulate the dynamical systems in <xref ref-type="disp-formula" rid="equ2 equ3 equ4 equ5 equ6 equ7 equ8 equ9 equ10 equ11 equ12 equ13 equ14">Equations (2)-(14)</xref>, a step input <bold><italic>u</italic></bold> was applied. This input was smoothed using a Gaussian kernel to approximate the lowpass nature of signal propagation in the series of processing stages from the retina to pIT:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mfrac></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:msup><mml:mo>∗</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">⇒</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mfrac></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where the elements of <bold><italic>h</italic></bold> are scaled Heaviside step functions. The input is thus a sigmoidal ramp whose latency to half height is set by <italic>t<sub>0</sub></italic> and rise time is set by σ. For simulation of two-stage models, there were ten basic parameters: latency of the input <italic>t<sub>0</sub></italic>, standard deviation of the Gaussian ramp <italic>σ</italic>, system time constant <italic>τ</italic>, input connection strength <italic>a</italic>, feedforward connection strength <italic>b</italic>, the four input values across two stimulus conditions (i.e. <italic>h<sub>11</sub></italic>, <italic>h<sub>12</sub></italic>, <italic>h<sub>21</sub></italic>, <italic>h<sub>22</sub></italic>), and a factor <italic>sc</italic> for scaling the final output to the neural activity. In the deeper three-stage network, there were a total of fifteen parameters which included an additional feedforward connection strength <italic>c</italic> and additional input values since the three-stage model had four inputs instead of two. The lateral inhibition model class required one additional parameter <italic>k<sub>l</sub></italic> as did the normalization model family <italic>k<sub>s</sub></italic>, and for feedback model simulations, there was an additional feedback weight <italic>k<sub>td</sub></italic> to scale the relative contribution of the top-down errors in driving online inference. For the error coding variants of the feedback model, gain parameters <italic>c</italic> (two-stage) and <italic>d</italic> (three-stage) were included to scale the overall magnitude of the top level reconstruction error.</p></sec></sec><sec id="s4-6"><title>Model parameter fits to neural data</title><p>In fitting the models to the observed neural dynamics, we mapped the summed activity in the hidden stage (<bold><italic>w</italic></bold>) to population averaged activity in pIT, and we mapped the summed activity in the output stage (<bold><italic>y</italic></bold>) to population averaged signals measured in aIT. To simulate error coding, we mapped the reconstruction errors <bold><italic>e<sub>1</sub></italic></bold> <italic>= <bold>w-</bold>B<sup>T</sup><bold>x</bold></italic> and <bold><italic>e<sub>3</sub></italic></bold> <italic>= y-C<sup>T</sup>z</italic> to activity in pIT and aIT, respectively. We applied a squaring nonlinearity to the model outputs as an approximation to rectification since recorded extracellular firing rates are non-negative (and linear rectification is not continuously differentiable). Analytically solving this system of dynamical <xref ref-type="disp-formula" rid="equ2 equ3 equ4 equ5 equ6 equ7 equ8 equ9 equ10 equ11 equ12 equ13 equ14 equ15">Equations (2)-(15)</xref> for a step input is precluded because of the higher order interaction terms (the roots of the determinant and hence the eigenvalues/eigenvectors of a 3 × 3 or larger matrix are not analytically determined, except for the purely feedforward model which only has first-order interactions), and in the case of the normalization models, there is an additional nonlinear dependence on the shunt term. Thus, we relied on computational methods (constrained nonlinear optimization) to fit the parameters of the dynamical systems to the neural data with a quadratic (sum of squares) loss function.</p><p>Parameter values were fit in a two-step procedure. In the first step, we fit only the difference in response between image classes (differential mode which is the selectivity profile over time, see <xref ref-type="fig" rid="fig5">Figure 5A</xref>, right data panel), and in the second step, we refined fits to capture an equally weighted average of the differential mode and the common mode (the common mode is the average across images of the response time course of visual drive). This two-step procedure was used to ensure that each model had the best chance of fitting the dynamics of selectivity (differential mode) as these selectivity profiles were the main phenomena of interest but were smaller in size (20% of response) compared to overall visual drive. In each step, fits were done using a large-scale algorithm (interior-point) to optimize coarsely, and the resulting solution was used as the initial condition for a medium-scale algorithm (sequential quadratic programming) for additional refinement. The lower and upper parameter bounds tested were: <italic>t<sub>0</sub></italic>=[50 70], σ=[0.5 25], <italic>τ</italic> =[0.5 1000], <italic>k<sub>l</sub></italic>,<italic>k<sub>s</sub></italic>,<italic>k<sub>td</sub></italic>=[0 1], <italic>a,b</italic>,<italic>c,d</italic>=[0 2], <italic>h</italic>=[0 20], <italic>sc</italic>=[0 100], <italic>th</italic>=[−20 20], and <italic>bi</italic>=[−1 1] which proved to be adequately liberal as parameter values converged to values that did not generally approach these boundaries. To avoid local minima, the algorithm was initialized to a number of randomly selected points (n = 50), and after fitting the differential mode, we took the top fits (n = 25) for each model class and used these as initializations in subsequent steps. The single best fitting instance of each model class is shown in the main figures.</p></sec><sec id="s4-7"><title>Data and code availability</title><p>Source data including all image stimuli and neural data are available online in accompanying files. Complete model code is also available in accompanying files online. All data analysis and computational modeling were done using custom scripts written in MATLAB.</p></sec><sec id="s4-8"><title>Statistics</title><p>Error bars represent standard errors of the mean obtained by bootstrap resampling (n = 1000). All statistical comparisons including those of means or correlation values were obtained by bootstrap resampling (n = 1000) producing p-values at a resolution of 0.001 so that the lowest p-value that can be reported is p = 0.000 given the resolution of this statistical analysis. All statistical tests were two-sided unless otherwise specified. Spearman’s rank correlation coefficient was used.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank J Deutsch, K Schmidt, and P Aparicio for help with MRI and animal care and B Andken and C Stawarz for help with experiment software.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were performed in compliance with National Institutes of Health guidelines and the standards of the MIT Committee on Animal Care (IACUC protocol #0111-003-14) and the American Physiological Society.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><object-id pub-id-type="doi">10.7554/eLife.42870.008</object-id><label>Source data 1.</label><caption><title>Source neural data for figures 1-5 and model code for figures 5-6.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-42870-data1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.42870.009</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-42870-transrepform-v2.pdf"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analyzed during this study are included in the supporting files for the manuscript. Source data files have been provided for Figures 1-5, and code for computational models in Figures 5 &amp; 6 is provided.</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackley</surname> <given-names>DH</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>A learning algorithm for boltzmann machines*</article-title><source>Cognitive Science</source><volume>9</volume><fpage>147</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog0901_7</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afraz</surname> <given-names>SR</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Microstimulation of inferotemporal cortex influences face categorization</article-title><source>Nature</source><volume>442</volume><fpage>692</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1038/nature04982</pub-id><pub-id pub-id-type="pmid">16878143</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aparicio</surname> <given-names>PL</given-names></name><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurophysiological organization of the middle face patch in macaque inferior temporal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>12729</fpage><lpage>12745</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0237-16.2016</pub-id><pub-id pub-id-type="pmid">27810930</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname> <given-names>SL</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamic shape synthesis in posterior inferotemporal cortex</article-title><source>Neuron</source><volume>49</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.11.026</pub-id><pub-id pub-id-type="pmid">16387636</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Pinto</surname> <given-names>N</given-names></name><name><surname>Ardila</surname> <given-names>D</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Linearity and normalization in simple cells of the macaque primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>8621</fpage><lpage>8644</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-21-08621.1997</pub-id><pub-id pub-id-type="pmid">9334433</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The code for facial identity in the primate brain</article-title><source>Cell</source><volume>169</volume><fpage>1013</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id><pub-id pub-id-type="pmid">28575666</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Yan</surname> <given-names>Y</given-names></name><name><surname>Gong</surname> <given-names>X</given-names></name><name><surname>Gilbert</surname> <given-names>CD</given-names></name><name><surname>Liang</surname> <given-names>H</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Incremental integration of global contours through interplay between visual cortical areas</article-title><source>Neuron</source><volume>82</volume><fpage>682</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.03.023</pub-id><pub-id pub-id-type="pmid">24811385</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Flexible gating of contextual influences in natural vision</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1648</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1038/nn.4128</pub-id><pub-id pub-id-type="pmid">26436902</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>DD</given-names></name><name><surname>Papanastassiou</surname> <given-names>AM</given-names></name><name><surname>Oreper</surname> <given-names>D</given-names></name><name><surname>Andken</surname> <given-names>BB</given-names></name><name><surname>Dicarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High-resolution three-dimensional microelectrode brain mapping using stereo microfocal X-ray imaging</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>2966</fpage><lpage>2976</lpage><pub-id pub-id-type="doi">10.1152/jn.90672.2008</pub-id><pub-id pub-id-type="pmid">18815345</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egeth</surname> <given-names>HE</given-names></name><name><surname>Yantis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Visual attention: control, representation, and time course</article-title><source>Annual Review of Psychology</source><volume>48</volume><fpage>269</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.48.1.269</pub-id><pub-id pub-id-type="pmid">9046562</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epshtein</surname> <given-names>B</given-names></name><name><surname>Lifshitz</surname> <given-names>I</given-names></name><name><surname>Ullman</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Image interpretation by a single bottom-up top-down cycle</article-title><source>PNAS</source><volume>105</volume><fpage>14298</fpage><lpage>14303</lpage><pub-id pub-id-type="doi">10.1073/pnas.0800968105</pub-id><pub-id pub-id-type="pmid">18796607</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A face feature space in the macaque temporal lobe</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1038/nn.2363</pub-id><pub-id pub-id-type="pmid">19668199</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimaldi</surname> <given-names>P</given-names></name><name><surname>Saleem</surname> <given-names>KS</given-names></name><name><surname>Tsao</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Anatomical connections of the functionally defined &quot;Face Patches&quot; in the macaque monkey</article-title><source>Neuron</source><volume>90</volume><fpage>1325</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.009</pub-id><pub-id pub-id-type="pmid">27263973</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname> <given-names>CP</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title><source>Science</source><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1126/science.1117593</pub-id><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>Papanastassiou</surname> <given-names>AM</given-names></name><name><surname>Andken</surname> <given-names>BB</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Towards large-scale, high resolution maps of object selectivity in inferior temporal cortex. Front. Neurosci. Conference Abstract: Computational and Systems Neuroscienc</article-title><source>Frontiers</source></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Precedence of the eye region in neural processing of faces</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>16666</fpage><lpage>16682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2391-12.2012</pub-id><pub-id pub-id-type="pmid">23175821</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>Papanastassiou</surname> <given-names>AM</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Large-scale, high-resolution neurophysiological maps underlying FMRI of macaque temporal lobe</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15207</fpage><lpage>15219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1248-13.2013</pub-id><pub-id pub-id-type="pmid">24048850</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>ImageNet Classification with Deep Convolutional Neural Networks</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>The Mit Press</publisher-name><fpage>1106</fpage><lpage>1114</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>TS</given-names></name><name><surname>Yang</surname> <given-names>CF</given-names></name><name><surname>Romero</surname> <given-names>RD</given-names></name><name><surname>Mumford</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural activity in early visual cortex reflects behavioral experience and higher-order perceptual saliency</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>589</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1038/nn0602-860</pub-id><pub-id pub-id-type="pmid">12021764</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>TS</given-names></name><name><surname>Mumford</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Hierarchical bayesian inference in the visual cortex</article-title><source>Journal of the Optical Society of America A</source><volume>20</volume><fpage>1434</fpage><pub-id pub-id-type="doi">10.1364/JOSAA.20.001434</pub-id><pub-id pub-id-type="pmid">12868647</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname> <given-names>DA</given-names></name><name><surname>Bondar</surname> <given-names>IV</given-names></name><name><surname>Giese</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title><source>Nature</source><volume>442</volume><fpage>572</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1038/nature04951</pub-id><pub-id pub-id-type="pmid">16862123</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Simple Learned Weighted Sums of Inferior Temporal Neuronal Firing Rates Accurately Predict Human Core Object Recognition Performance</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>13402</fpage><lpage>13418</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5181-14.2015</pub-id><pub-id pub-id-type="pmid">26424887</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>T</given-names></name><name><surname>Walker</surname> <given-names>C</given-names></name><name><surname>Cho</surname> <given-names>RY</given-names></name><name><surname>Olson</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Image familiarization sharpens response dynamics of neurons in inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1388</fpage><lpage>1394</lpage><pub-id pub-id-type="doi">10.1038/nn.3794</pub-id><pub-id pub-id-type="pmid">25151263</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Borzello</surname> <given-names>M</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Intelligent information loss: the coding of facial identity, head pose, and non-face information in the macaque face patch system</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>7069</fpage><lpage>7081</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3086-14.2015</pub-id><pub-id pub-id-type="pmid">25948258</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Patches with links: a unified system for processing faces in the macaque temporal lobe</article-title><source>Science</source><volume>320</volume><fpage>1355</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1126/science.1157436</pub-id><pub-id pub-id-type="pmid">18535247</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Crapse</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The effect of face patch microstimulation on perception of faces and objects</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>743</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.1038/nn.4527</pub-id><pub-id pub-id-type="pmid">28288127</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>MM</given-names></name><name><surname>Teder-Sälejärvi</surname> <given-names>W</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The time course of cortical facilitation during cued shifts of spatial attention</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>631</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1038/2865</pub-id><pub-id pub-id-type="pmid">10196572</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassi</surname> <given-names>JJ</given-names></name><name><surname>Gómez-Laberge</surname> <given-names>C</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Born</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Corticocortical feedback increases the spatial extent of normalization</article-title><source>Frontiers in Systems Neuroscience</source><volume>8</volume><pub-id pub-id-type="doi">10.3389/fnsys.2014.00105</pub-id><pub-id pub-id-type="pmid">24910596</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohayon</surname> <given-names>S</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>What makes a cell face selective? The importance of contrast</article-title><source>Neuron</source><volume>74</volume><fpage>567</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.024</pub-id><pub-id pub-id-type="pmid">22578507</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rifai</surname> <given-names>S</given-names></name><name><surname>Vincent</surname> <given-names>P</given-names></name><name><surname>Muller</surname> <given-names>X</given-names></name><name><surname>Glorot</surname> <given-names>X</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title><italic>Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</italic></article-title><source>ICML-11</source><conf-name>Proceedings of the 28th International Conference on Machine Learning</conf-name><fpage>833</fpage><lpage>840</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname> <given-names>DE</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name><name><surname>Williams</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadagopan</surname> <given-names>S</given-names></name><name><surname>Zarco</surname> <given-names>W</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A causal relationship between face-patch activity and face-detection behavior</article-title><source>eLife</source><volume>6</volume><elocation-id>e18558</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18558</pub-id><pub-id pub-id-type="pmid">28375078</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural signal statistics and sensory gain control</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1038/90526</pub-id><pub-id pub-id-type="pmid">11477428</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwiedrzik</surname> <given-names>CM</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-Level Prediction Signals in a Low-Level Area of the Macaque Face-Processing Hierarchy</article-title><source>Neuron</source><volume>96</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.007</pub-id><pub-id pub-id-type="pmid">28957679</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Pattern Analysis and Synthesis in Attractor Neural Networks</source><publisher-loc>A Multidisciplinary Perspective, Singapore</publisher-loc><publisher-name>Theoretical Aspects of Neural Computation</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugase</surname> <given-names>Y</given-names></name><name><surname>Yamane</surname> <given-names>S</given-names></name><name><surname>Ueno</surname> <given-names>S</given-names></name><name><surname>Kawano</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Global and fine information coded by single neurons in the temporal visual cortex</article-title><source>Nature</source><volume>400</volume><fpage>869</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1038/23703</pub-id><pub-id pub-id-type="pmid">10476965</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Comparing face patch systems in macaques and humans</article-title><source>PNAS</source><volume>105</volume><fpage>19514</fpage><lpage>19519</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809662105</pub-id><pub-id pub-id-type="pmid">19033466</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname> <given-names>R</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name><name><surname>Shapiro</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The slow time-course of visual attention</article-title><source>Cognitive Psychology</source><volume>30</volume><fpage>79</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1006/cogp.1996.0003</pub-id><pub-id pub-id-type="pmid">8660782</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Seibert</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Zeiler</surname> <given-names>MD</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stochastic pooling for regularization of deep convolutional neural networks</article-title><source>ArXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.3557">https://arxiv.org/abs/1301.3557</ext-link><date-in-citation iso-8601-date="2016-06-01">June 1, 2016</date-in-citation></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>NR</given-names></name><name><surname>von der Heydt</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Analysis of the context integration mechanisms underlying figure-ground organization in the visual cortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>6482</fpage><lpage>6496</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5168-09.2010</pub-id><pub-id pub-id-type="pmid">20463212</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42870.011</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Connor</surname><given-names>Ed</given-names></name><role>Reviewing Editor</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Connor</surname><given-names>Ed</given-names> </name><role>Reviewer</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors’ note: a previous version of this study was rejected after peer review, but the authors submitted for reconsideration. The first decision letter after peer review is shown below.]</p><p>Thank you for submitting your work entitled &quot;Neural dynamics at successive stages of the ventral visual stream are consistent with hierarchical error signals&quot; for consideration by eLife. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your submission has been rejected for publication in eLife at this time. However, we would be willing to consider a resubmission that addresses the suggestions and concerns outlined below.</p><p>Reviewers felt that the results have strong potential importance for understanding the role of feedback in ventral pathway, on a par with Schwiedrzik and Freiwald, 2017. Schwiedrzik and Freiwald show prediction-based modulation based on extensive training with arbitrary pairings of two successive faces. Your results show prediction-based modulation of a very different kind, based on mismatch between the driving stimulus (for this area, a contralateral eye) and surrounding stimulus patches that don't fit the expectation for a normal face. This is prediction based on spatial context rather than temporal training, and in that sense a more natural example that seems more relevant to normal ventral pathway processing and the idea that feedforward/feedback interactions progressively refine the estimate of what is present in the world. Both papers are important for being initial demonstrations that feedback in the ventral pathway may modulate signals based on high-level predictions, a longstanding theory.</p><p>The strongest concern was about the interpretation of the results as a switch in coding value from face to anti-face, because the neurons are still responding, at only a slightly different level, to an image with their primary driving stimulus, a contralateral eye, which can drive responses by itself. A small change in relative responses to very similar stimuli does not mean that coding polarity has suddenly flipped. Predictive coding theory does not involve neurons changing the meaning of their state value signals. The distinction made here between &quot;faces&quot; and &quot;anti-faces&quot; is an arbritrary boundary in a small set of stimuli all containing at least a contra-lateral eye, the overall shape of a face, and face-like texture. There is no demonstration that these neurons begin responding to actual non-face stimuli, i.e. other objects with completely different shapes and appearances (which were part of the stimulus set and could be compared). The example stimuli do not even exhibit the switch since they start out on the non-face side of the boundary. The d' between &quot;faces&quot; and &quot;anti-faces&quot; is only around 0.1 or 0.2.</p><p>Instead, the results have a simple interpretation based on the Issa and Dicarlo 2012 study of face responses in pIT. That study demonstrated that early, feedforward pIT face responses are driven by images of an eye, positioned near the contralateral top of a rounded outline. The relatively stronger responses to the &quot;non-face&quot; stimuli, which contain the contralateral eye but with missing or misplaced other face parts, make sense as a positive modulation of a state signal for the contralateral eye when the eye is unpredictable/surprising based on the spatial surround being inconsistent (jumbled or absent face features). This is analogous to the increase in responses to unexpected face identities/orientations observed by Schwiedrzik and Freiwald.</p><p>We suggest a resubmission based on this simpler interpretation. In addition, as elaborated in the separate reviews, a resubmission should address strong concerns about conclusions from the modeling analyses, especially the claim to support theories involving pure error signals, and claims to explain many other general ventral pathway phenomena in terms of this one observation.</p><p><italic>Reviewer #1:</italic> </p><p>Issa et al. present an analysis of response dynamics at three stages in the monkey face patch system. The stimuli are selected from the face part rearrangements used in the Issa and Dicarlo 2012 study of face responses in pIT. That study demonstrated that early, feedforward pIT face responses are driven primarily by images of an eye, positioned near the contralateral top of a rounded outline. Here, they compare responses in early (60-90 ms after onset) and late (100-130 ms after onset) response phases, for two types of stimuli (all of which contained at least one eye, at or near the optimum position for driving pIT responses), &quot;faces&quot; (which here means eight stimuli in which multiple other face parts are in their correct locations) and &quot;non-faces&quot; (which here means stimuli in which most other face parts besides the contralateral eye are missing or misplaced). All of these stimuli drove strong initial responses from pIT neurons, presumably because they all contained an eye at or near the critical location, consistent with Issa and Dicarlo 2012. Responses to the &quot;face&quot; stimuli declined by 18% in the 100-130 ms period relative to the 60-90 ms period. In contrast, responses to the &quot;non-face&quot; stimuli did not decline.</p><p>This result has a straightforward interpretation in terms of predictive coding theory, in which neural signals reflect, at least in part, deviations of feedforward stimulus signals from predictions based on temporal history, spatial surround information, or other factors. Retinal ganglion cells manifest both, responding either to increased or to decreased luminance at their receptive field center, relative to the previous luminance value at the receptive field center and relative to the luminance value in the spatial surround. Schwiedrzik and Freiwald, 2017, recently demonstrated predictive coding based on temporal history in face patch ML (probably corresponding to the pIT neurons in this new manuscript). Monkeys were passively exposed to many repeats of 9 specific pairings of face images, in which both identity and head orientation varied. After training, ML neurons were tested with both the trained pairings and with novel pairings of the same initial and successor stimuli. As expected, neurons exhibited different preferences for stimuli. Responses to preferred successor stimuli were about 17% higher when the preceding stimulus was switched and therefore predicted a different head orientation, face identity, or both. These three conditions evoked approximately equal deviations in the 120-210 ms period after onset, but at 300-440 ms differences in identity caused the largest deviations (around 20%) and differences in head orientation had no effect. This presumably reflects the more gradual evolution of identity information, which is represented in the most anterior face patch AM. The sensitivity of ML to identity errors, even when head orientation was identical, reflects the top-down origin of prediction signals.</p><p>The main result in this new Issa et al. manuscript seems to be a similar-sized prediction error based on surround information rather than temporal history. Later responses in pIT are about 18% lower when the information surrounding the eye stimulus is consistent with a face (thus no prediction error) than when the surrounding information is inconsistent with a face, so that the presence of the eye by itself becomes a more unexpected, unpredictable stimulus element. The appearance of this difference only after the initial pIT response (60-90 ms) is evidence that higher-level processing of the other, surrounding face elements was required to produce the prediction error difference. A similar pattern is observed for some cIT neurons, while aIT neurons maintain their selectivity for the face stimuli, providing the likely top-down source for prediction signals.</p><p>However, the authors interpret their phenomenon not in terms of prediction modulation of the feedforward signals for a contralateral eye stimulus from pIT but in terms of selectivity for faces in general, claiming that pIT neurons &quot;decreased their preference for faces, becoming anti-face preferring on average&quot;, which is diagrammed in Fig. 1B. This is confusing for a number of reasons. First, theories of prediction error coding do not involve neurons suddenly changing the basic meaning of their signals. Second, the &quot;anti-face&quot; preference is really a slightly differential response to the &quot;face&quot; stimuli vs. the &quot;non-face&quot; partially scrambled stimuli used here, all of which have the contralateral eye, which these authors have previously shown is the primary feedforward signal from pIT (Issa and Dicarlo, 2012). The main result is more naturally interpreted as a continuing response to the contralateral eye, modulated by the surrounding face-part information, in a way consistent with predictive coding. Slightly differential responses to the two groups of stimuli here (a d' of around 0.1 or 0.2) do not signify a reversal of preference for faces. There is no demonstration that pIT neurons suddenly respond more strongly to toasters and hammers than to faces. Third, there is no reason to think that the most informative tuning dimension for these neurons is a continuum between faces and other objects. Throughout the face patch system, neurons are tuned for the details of facial structure, and, consistent with this, Schwiedrzik and Freiwald report that error coding in ML/pIT occurs along dimensions like head orientation and identity, as one would expect. Thus the Fig. 1B diagram, and the logic throughout the text and the figures, which explain prediction errors in terms of reversing face selectivity, is confusing (i) because the face patch system does not simply discriminate faces from other objects, (ii) because wholesale tuning reversal is not expected based on prediction coding theory, and (iii) a small change in relative responses to very similar stimuli does not mean that coding polarity in general has suddenly flipped.</p><p>The second part of the paper is an extended analysis of predictive coding models. This part of the paper also suffers from the confusing interpretation that pIT neurons reverse their tuning polarity. The authors show that only models capable of computing errors between layers can fit their results, which makes sense. They claim to explain a number of previously observed phenomena with their model, including sublinear part integration, faster attenuation of signals for familiar stimuli, and ramp coding of face structure. These extrapolations to phenomena well beyond the scope of this paper seem qualitative, tacked on, and unconvincing. The authors also make a comparison of dynamics in superficial and deep neurons to network dynamics of &quot;state&quot; and &quot;error&quot; neurons, and claim that superficial, forward projecting neurons have longer dynamics, like state neurons, and thus state signals, not error signals, are propagated forward, consistent with error back propagation and not predictive coding. This also seems tenuous, speculative, and beyond the scope of this paper. If the authors wanted to explore this idea, they would need among other things to distinguish feedforward input layer 4 from deep and superficial layers and characterize differences in stimulus coding, error coding, and dynamics in all layers.</p><p>In summary, the basic error-related phenomenon makes sense in terms of the contralateral eye coding role for pIT neurons previously established by these authors, and this constitutes an important new finding that extends understanding of top-down feedback effects and their possible relation to error coding in ventral pathway vision. The phenomenon does not make very clear sense in terms of face/non-face coding. The manuscript would benefit from clarifying this interpretation, reducing the attempts to extrapolate the results so far through modeling, and doing more to situate the results with respect to the existing literature, especially the extremely relevant paper by Schwiedrzik and Freiwald, which receives only a glancing reference here.</p><p><italic>Reviewer #2:</italic> </p><p>This paper examines the firing rate dynamics of neurons in three hierarchically arranged face sensitive areas of the ventral macaque monkey cortex, areas and neurons proposed to be responsible for face vs. non-face detection behavior. The key empirical finding is that despite early selectivity for faces in neurons in lower areas, preference for faces decreases within several 10s of milliseconds, while at the same time face preference in the top area remains high. The authors interpret their finding as consistent with neural circuitry that computes an error signal measuring the difference between the initial input activity and subsequent feedback signal from the higher area(s). This interpretation is consistent with previous theories of predictive coding, in contrast to other theories that predict no decrease in the selectivity of earlier neurons. About half of the paper is then devoted to showing results from several classes of simplified dynamical models, and to comparing the pattern of model results with the empirical observations. The paper addresses a significant and deep question as to the computational functions of inter-area visual cortical circuitry. The empirical results are new, and the modeling has a number of novel features that should be of broad interest, such as the quantitative analysis of dynamic feedback networks that include both state and error units.</p><p>The scope of the paper, empirical and theoretical, results in a fairly long read. Some of length could benefit from reducing redundant descriptions of the classes of previous models within the paper. But the paper is also missing details in a few parts. The comments below primarily have to do with the need to fill in missing explanations in the data analysis, to better organize model descriptions in the early parts of the paper, and improve the organization.</p><p>- The Materials and methods goes into detail regarding the various stimulus variants. But other than the numbers of presentations and their durations, the details of the temporal presentation of stimulus types was not specified. The manuscript should better describe how the neural spiking data was analyzed with respect to the stimulus sequences, e.g. how the data in Figure 2 was determined. The lack of detail makes it difficult to evaluate possible interactions between stimulus type and presentation timing. It would also help to better describe how the relative timing in responses between the areas was determined, and its variability over sites.</p><p>- It seems odd that the laminar-related hypotheses first appear at the end of the computational section. It would be more consistent with the organization of the rest of the paper if the hypotheses were at the beginning (in the context of predictive coding, etc.). This section also seems isolated in that the reader is left hanging with the evidence that superficial units behave like state rather than error units. It would help to have some discussion of other literature, such as laminar analysis of fMRI data in human V1 that has been interpreted in terms of predictive coding.</p><p><italic>Reviewer #3:</italic> </p><p>The paper presents neurophysiological data from macaque suggesting a set of PIT and CIT neurons might be explicitly coding prediction error signals specifically to face configuration. The paper also investigated a set of models and argued that the PIT and CIT error responses can only be explained by a recurrent feedback model implementing predictive coding. Overall, the paper is reasonably well-written with interesting data and provocative claims but I have three concerns as to whether the claims are fully supported by the data. First, while the effect observed in PIT might indeed be mediated by feedback, horizontal (intra-areal interactIon) mechanisms cannot be ruled out empirically. This is because the error-computing &quot;model&quot; can be implemented within each cortical area as well. Some additional analysis of the data based on timing of the different effects in PIT and AIT might help argue for feedback over horizontal interaction. Second, the empirical observations seem to be also consistent with the simpler notion that PIT signals are coding face parts, and that these face part state variables were then amplified by the attention due to part-whole incongruence. Thus, the signals might not be pure error signals and the data might not be sufficient for arguing for the predictive coding model, which projects only error signals in feedforward connections, over other theories, which project attentional modulated state variables. Third, none of the PIT examples in Figure 2 showed preferred face configuration over non-face configurations in the early response window as Figure 3's graphs indicated and the text suggested. The reversal of neuronal preference from face in early responses to non-face in late responses was supposed to be key evidence in support of the idea of the error signals, and should be demonstrated in Figure 2. These concerns should be addressable by revising the claims appropriately, adding cautionary notes/caveats in the interpretation of the data, or maybe providing some additional analysis, graphs and clarification.</p><p>Elaboration:</p><p>The main claim was that many &quot;face-selective&quot; neurons in PIT (less so in CIT, and not in AIT) responded more to face images in normal configuration than to images of face components in unusual configuration (non-face stimuli) in the 60-100 ms window post-stimulus onset and then changed their preference to non-face in the 100-130 ms window. The evidence presented in the d' graphs in Figure 3 was rather compelling, and so were the simulation results of the models (Fig. 6 and 7). However, none of the PIT example cells or sites as shown in Figure 2 actually exhibited a preference for the normal face configurations in the early response. The examples showed that the neurons tended to prefer non-face images even in the early response window such preference might become more accentuated in the later part of the responses, which was different from the simulation results. It would be more compelling to provide some examples in Figure 2 that are consistent with the key claim.</p><p>Given the PIT neurons responded more to the non-faces from the beginning in Figure 2, it is important to confirm that these neurons actually were face or face part preferring or selective, by showing their responses to house, body parts and places as well, which were also tested. Assuming that these neurons indeed are face part sensitive, the data did suggest that the responses to face parts were stronger when they were part of an incoherent face configuration, rather than a part of a coherent face configuration. The stronger responses to the non-face images could be error signals between the expectation of the global configuration and local patterns but they might not be pure error signals, but attentional signals drawn by the errors to enhance the saliency of feature responses. It is not clear whether the neural responses could be considered as &quot;error signals&quot;, or simply the state variables modulated by the error signals.</p><p>The models do not seem to provide many new insights or predictions. By design, an error computing architecture will show error signals. It is obvious that simple lateral inhibition or normalization could not produce the effects, but I am not convinced that the observed effects cannot be explained by horizontal interaction within each area. The paper did not present experimental evidence establishing that the prediction error signals were mediated by feedback from AIT, rather than by horizontal connections within PIT. Short of reversible deactivating IT or computing conditional Granger causality using simultaneously recordings of AIT and PIT sites, any conclusion on feedback however can only be tentative based on current evidence on single unit recording. The authors might be able to provide some evidence based on relative timing of the responses in AIT and PIT to argue for this point. Figure 2's examples did suggest that AIT neurons' face preference might precede PIT's non-face preference, which might reflect a potential causal relationship. It would be prudent to point out that although simple lateral inhibition and normalization, as explored by the authors, might not explain the error signals, the prediction errors can easily be calculated within PIT with horizontal connections. After all, a significant number of the PIT neurons also preferred face over non-faces as in AIT. Besides, all the contextual modulation effects (end-stopping, surround suppression) that were cited in Rao and Ballard's paper could also be implemented with horizontal connections in V1. Thus, the modeling effort could only be used to argue that there were the error signals between global versus local representations were computed in PIT, it could not be used as a &quot;proof&quot; for the involvement of a feedback mechanism. So it might be prudent to be more cautious about the conclusions that can be drawn from the modeling effort.</p><p>Showing error signals (between face parts and global face configuration) in PIT but not AIT is interesting. This is consistent with earlier observation, for example Issa's earlier study, that PIT neurons are encoding face parts and AIT neurons are encoding the whole faces. If PIT neurons are encoding face parts, reflexive attention and vigilance will naturally be devoted to the neurons coding the parts due to the inconsistency between local parts and global configuration. Thus, it could be an error-induced attentional effect rather than the error signal itself.</p><p>The idea of predictive coding or residue coding was first proposed by Mumford (Biological Cybernetics 1993). In its purest form, Mumford's idea is a generalization of Barlow's sparse coding idea to the entire visual hierarchy, and is similar conceptually to Burt and Adelson's Laplacian pyramid, with different areas coding only the &quot;residue signals&quot;. Rao and Ballard proposed a Kalman filter implementation of the idea but with a significant modification (and retreat) - reintroducing the state variables into each area in addition to the residue signals. Thus, experimentally, it is difficult to distinguish the classical idea of interactive activation (McClelland and Rumelhart) or adaptive resonance (Grossberg) or hierarchical Bayes (Lee and Mumford) from Rao and Ballard's Kalman filter model because all these models require both state and error representations in each layer. Experimentally the only measurable difference to distinguish the &quot;predictive coding&quot; model from the other models is that its feedforward signals from one visual area to another area contain <italic>only</italic> the error signals in the predictive coding model, while the other models will project the &quot;state variables&quot; or beliefs, possibly modulated by attention. Thus, the data presented in this paper could not be used to distinguish whether the neural signals were the error (or residue signals or Bayesian surprises), or state variables or beliefs enhanced by attention. The data only showed that the incongruence between the local and the global representations lead to enhancement of the face part responses of the PIT neurons. The distinction is subtle but important. The existence of error related signals was well-known through the cortex, but they cannot be considered as conclusive evidence for supporting Mumford or the Rao-Ballard's hierarchical predictive coding theory.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42870.012</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the author responses to the first round of peer review follow.]</p><disp-quote content-type="editor-comment"><p>Thank you for submitting your work entitled &quot;Neural dynamics at successive stages of the ventral visual stream are consistent with hierarchical error signals&quot; for consideration by eLife. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your submission has been rejected for publication in eLife at this time. However, we would be willing to consider a resubmission that addresses the suggestions and concerns outlined below.</p><p>Reviewers felt that the results have strong potential importance for understanding the role of feedback in ventral pathway, on a par with Schwiedrzik and Freiwald, 2017. Schwiedrzik and Freiwald show prediction-based modulation based on extensive training with arbitrary pairings of two successive faces. Your results show prediction-based modulation of a very different kind, based on mismatch between the driving stimulus (for this area, a contralateral eye) and surrounding stimulus patches that don't fit the expectation for a normal face. This is prediction based on spatial context rather than temporal training, and in that sense a more natural example that seems more relevant to normal ventral pathway processing and the idea that feedforward/feedback interactions progressively refine the estimate of what is present in the world. Both papers are important for being initial demonstrations that feedback in the ventral pathway may modulate signals based on high-level predictions, a longstanding theory.</p></disp-quote><p>We thank the editors and reviewers for this positive comparison of our work to recently published work. In the Discussion, we have included a more direct comparison to the Neuron 2017 study highlighting points which make our study novel relative to this recent work.</p><disp-quote content-type="editor-comment"><p>The strongest concern was about the interpretation of the results as a switch in coding value from face to anti-face, because the neurons are still responding, at only a slightly different level, to an image with their primary driving stimulus, a contralateral eye, which can drive responses by itself. A small change in relative responses to very similar stimuli does not mean that coding polarity has suddenly flipped.</p></disp-quote><p>We have revised our phrasing to avoid implying that neurons prefer non-faces. Rather, we state throughout the text that pIT neuron responses are modulated by atypical face-part configurations. In other words, they are tuned to the local features (e.g. eye features), but signal an error in the configurational context with respect to that local feature preference. We also clarify that the observed dynamics reflect a relatively small change modulating the overall strong response for face features.</p><disp-quote content-type="editor-comment"><p>Predictive coding theory does not involve neurons changing the meaning of their state value signals. The distinction made here between &quot;faces&quot; and &quot;anti-faces&quot; is an arbritrary boundary in a small set of stimuli all containing at least a contra-lateral eye, the overall shape of a face, and face-like texture. There is no demonstration that these neurons begin responding to actual non-face stimuli, i.e. other objects with completely different shapes and appearances (which were part of the stimulus set and could be compared). The example stimuli do not even exhibit the switch since they start out on the non-face side of the boundary. The d' between &quot;faces&quot; and &quot;anti-faces&quot; is only around 0.1 or 0.2.</p><p>Instead, the results have a simple interpretation based on the Issa and Dicarlo 2012 study of face responses in pIT. That study demonstrated that early, feedforward pIT face responses are driven by images of an eye, positioned near the contralateral top of a rounded outline. The relatively stronger responses to the &quot;non-face&quot; stimuli, which contain the contralateral eye but with missing or misplaced other face parts, make sense as a positive modulation of a state signal for the contralateral eye when the eye is unpredictable/surprising based on the spatial surround being inconsistent (jumbled or absent face features). This is analogous to the increase in responses to unexpected face identities/orientations observed by Schwiedrzik and Freiwald.</p><p>We suggest a resubmission based on this simpler interpretation. In addition, as elaborated in the separate reviews, a resubmission should address strong concerns about conclusions from the modeling analyses, especially the claim to support theories involving pure error signals, and claims to explain many other general ventral pathway phenomena in terms of this one observation.</p></disp-quote><p>We appreciate the reviewer’s concerns that the modeling section extrapolates far from the data. We have significantly curtailed the modeling and removed the claims regarding phenomena outside the scope of the main dataset. As a result, the manuscript is significantly shorter, by ~25%, in text and figures. We hope this more concise manuscript reads more easily and now makes only the fully supported points.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] However, the authors interpret their phenomenon not in terms of prediction modulation of the feedforward signals for a contralateral eye stimulus from pIT but in terms of selectivity for faces in general, claiming that pIT neurons &quot;decreased their preference for faces, becoming anti-face preferring on average&quot;, which is diagrammed in Fig. 1B. This is confusing for a number of reasons. First, theories of prediction error coding do not involve neurons suddenly changing the basic meaning of their signals. Second, the &quot;anti-face&quot; preference is really a slightly differential response to the &quot;face&quot; stimuli vs. the &quot;non-face&quot; partially scrambled stimuli used here, all of which have the contralateral eye, which these authors have previously shown is the primary feedforward signal from pIT (Issa and Dicarlo, 2012). The main result is more naturally interpreted as a continuing response to the contralateral eye, modulated by the surrounding face-part information, in a way consistent with predictive coding. Slightly differential responses to the two groups of stimuli here (a d' of around 0.1 or 0.2) do not signify a reversal of preference for faces. There is no demonstration that pIT neurons suddenly respond more strongly to toasters and hammers than to faces. Third, there is no reason to think that the most informative tuning dimension for these neurons is a continuum between faces and other objects. Throughout the face patch system, neurons are tuned for the details of facial structure, and, consistent with this, Schwiedrzik and Freiwald report that error coding in ML/pIT occurs along dimensions like head orientation and identity, as one would expect. Thus the Fig. 1B diagram, and the logic throughout the text and the figures, which explain prediction errors in terms of reversing face selectivity, is confusing (i) because the face patch system does not simply discriminate faces from other objects, (ii) because wholesale tuning reversal is not expected based on prediction coding theory, and (iii) a small change in relative responses to very similar stimuli does not mean that coding polarity in general has suddenly flipped.</p></disp-quote><p>We have changed our phrasing throughout the text to indicate that pIT neurons are tuned to face parts but express a modulatory signal for a mismatched face configuration. We also remove any reference implying reversing selectivity for part configuration. Rather, we are simply claiming that there is a higher response for atypical configurations in the late response phase. We have updated the predictions Figure 1B accordingly.</p><disp-quote content-type="editor-comment"><p>The second part of the paper is an extended analysis of predictive coding models. This part of the paper also suffers from the confusing interpretation that pIT neurons reverse their tuning polarity. The authors show that only models capable of computing errors between layers can fit their results, which makes sense. They claim to explain a number of previously observed phenomena with their model, including sublinear part integration, faster attenuation of signals for familiar stimuli, and ramp coding of face structure. These extrapolations to phenomena well beyond the scope of this paper seem qualitative, tacked on, and unconvincing.</p></disp-quote><p>We agree with the reviewer and have removed these modeling analyses which greatly simplify the manuscript without losing the main point of the work.</p><disp-quote content-type="editor-comment"><p>The authors also make a comparison of dynamics in superficial and deep neurons to network dynamics of &quot;state&quot; and &quot;error&quot; neurons, and claim that superficial, forward projecting neurons have longer dynamics, like state neurons, and thus state signals, not error signals, are propagated forward, consistent with error back propagation and not predictive coding. This also seems tenuous, speculative, and beyond the scope of this paper. If the authors wanted to explore this idea, they would need among other things to distinguish feedforward input layer 4 from deep and superficial layers and characterize differences in stimulus coding, error coding, and dynamics in all layers.</p><p>In summary, the basic error-related phenomenon makes sense in terms of the contralateral eye coding role for pIT neurons previously established by these authors, and this constitutes an important new finding that extends understanding of top-down feedback effects and their possible relation to error coding in ventral pathway vision. The phenomenon does not make very clear sense in terms of face/non-face coding. The manuscript would benefit from clarifying this interpretation, reducing the attempts to extrapolate the results so far through modeling, and doing more to situate the results with respect to the existing literature, especially the extremely relevant paper by Schwiedrzik and Freiwald, which receives only a glancing reference here.</p></disp-quote><p>In addition to clarifying the nature of the neural phenomenon and shortening the modeling section, we relate our work to Schwiedrzik and Freiwald, 2017, highlighting how our study extends that work by recording from the putative areas providing prediction signals while also exposing a spatial prediction rather than a temporal prediction requiring behavioral training.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] The paper addresses a significant and deep question as to the computational functions of inter-area visual cortical circuitry. The empirical results are new, and the modeling has a number of novel features that should be of broad interest, such as the quantitative analysis of dynamic feedback networks that include both state and error units.</p><p>The scope of the paper, empirical and theoretical, results in a fairly long read. Some of length could benefit from reducing redundant descriptions of the classes of previous models within the paper.</p></disp-quote><p>We have simplified our explanation of the modeling section presenting a feedforward model as well as the predictive coding model only and removing the model prediction section. The paper is much shorter by ~25% in text and figure material.</p><disp-quote content-type="editor-comment"><p>But the paper is also missing details in a few parts. The comments below primarily have to do with the need to fill in missing explanations in the data analysis, to better organize model descriptions in the early parts of the paper, and improve the organization.</p><p>- The Materials and methods goes into detail regarding the various stimulus variants. But other than the numbers of presentations and their durations, the details of the temporal presentation of stimulus types was not specified. The manuscript should better describe how the neural spiking data was analyzed with respect to the stimulus sequences, e.g. how the data in Figure 2 was determined. The lack of detail makes it difficult to evaluate possible interactions between stimulus type and presentation timing. It would also help to better describe how the relative timing in responses between the areas was determined, and its variability over sites.</p></disp-quote><p>We thank the reviewer for this suggestion as it is important to discuss the order and timing of presentation in our stimuli for interpreting whether the results could be the result of attention to novel faces. We now discuss this in the Results and Materials and methods sections to clarify that stimuli (normal vs. novel faces) were randomly interleaved, minimizing any possibility of attention or priming signals to the novel facelike images.</p><disp-quote content-type="editor-comment"><p>- It seems odd that the laminar-related hypotheses first appear at the end of the computational section. It would be more consistent with the organization of the rest of the paper if the hypotheses were at the beginning (in the context of predictive coding, etc.). This section also seems isolated in that the reader is left hanging with the evidence that superficial units behave like state rather than error units. It would help to have some discussion of other literature, such as laminar analysis of fMRI data in human V1 that has been interpreted in terms of predictive coding.</p><p>Reviewer #3:</p><p>The paper presents neurophysiological data from macaque suggesting a set of PIT and CIT neurons might be explicitly coding prediction error signals specifically to face configuration. The paper also investigated a set of models and argued that the PIT and CIT error responses can only be explained by a recurrent feedback model implementing predictive coding. Overall, the paper is reasonably well-written with interesting data and provocative claims but I have three concerns as to whether the claims are fully supported by the data. First, while the effect observed in PIT might indeed be mediated by feedback, horizontal (intra-areal interactIon) mechanisms cannot be ruled out empirically. This is because the error-computing &quot;model&quot; can be implemented within each cortical area as well. Some additional analysis of the data based on timing of the different effects in PIT and AIT might help argue for feedback over horizontal interaction.</p></disp-quote><p>We have expanded on how the observed phenomenon may or may not constrain underlying mechanisms. First, we did provide analyses of how the timing of information in cIT and aIT relates to the late signals in pIT, and this analysis was consistent with a potential feedback interaction (early cIT/aIT response predicts late pIT response pattern). This was in the original manuscript but is highlighted more strongly since it is important for supporting a potential feedback mechanism. Second, in the Results and Discussion modeling sections, we make sure to state the limits of our observations for determining whether within area (lateral) recurrence or top-down (feedback) recurrence are the actual underlying mechanism and propose future causal studies to test these putative mechanisms for generating the observed neural phenomena.</p><disp-quote content-type="editor-comment"><p>Second, the empirical observations seem to be also consistent with the simpler notion that PIT signals are coding face parts, and that these face part state variables were then amplified by the attention due to part-whole incongruence. Thus, the signals might not be pure error signals and the data might not be sufficient for arguing for the predictive coding model, which projects only error signals in feedforward connections, over other theories, which project attentional modulated state variables.</p></disp-quote><p>The observed responses were unlikely to result from attention signals. The stimuli were randomly interleaved so that the subject had no prior expectations of the stimulus class (normal versus altered face-part configuration). Furthermore, the IT neural response dynamics were too rapid (&lt;100ms) for top-down attention signals to have developed. We cite a few studies suggesting attention cannot be evoked in tens of milliseconds. These factors argue against attentional processing of our stimuli and are now mentioned in the Results, Discussion, and Materials and methods sections - the dynamics are likely the result image-driven visual processing and not endogenous arousal or attention.</p><disp-quote content-type="editor-comment"><p>Third, none of the PIT examples in Figure 2 showed preferred face configuration over non-face configurations in the early response window as Figure 3's graphs indicated and the text suggested. The reversal of neuronal preference from face in early responses to non-face in late responses was supposed to be key evidence in support of the idea of the error signals, and should be demonstrated in Figure 2. These concerns should be addressable by revising the claims appropriately, adding cautionary notes/caveats in the interpretation of the data, or maybe providing some additional analysis, graphs and clarification.</p></disp-quote><p>We did not intend to imply that neurons reverse selectivity and attempted to avoid using the reversal language in our initial submission and have further refined our current resubmission. Rather, we simply show that a subset of neurons responded more strongly to the altered face part configurations in the late phase. We have added a statement to the Results section clarifying this main data claim.</p></body></sub-article></article>