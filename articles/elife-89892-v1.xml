<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89892</article-id><article-id pub-id-type="doi">10.7554/eLife.89892</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89892.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Ecology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Unsupervised discovery of family specific vocal usage in the Mongolian gerbil</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Peterson</surname><given-names>Ralph E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2692-5955</contrib-id><email>ralph.emilio.peterson@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Choudhri</surname><given-names>Aman</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Mitelut</surname><given-names>Catalin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0471-9816</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tanelus</surname><given-names>Aramis</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Capo-Battaglia</surname><given-names>Athena</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Williams</surname><given-names>Alex H</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Schneider</surname><given-names>David M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Sanes</surname><given-names>Dan H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3783-6165</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00sekdz59</institution-id><institution>Center for Computational Neuroscience, Flatiron Institute</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University, New York</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Psychology, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Neuroscience Institute, New York University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Biology, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Takahashi</surname><given-names>Daniel Y</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04wn09761</institution-id><institution>Federal University of Rio Grande do Norte</institution></institution-wrap><country>Brazil</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>12</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP89892</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-06-26"><day>26</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-03-13"><day>13</day><month>03</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.11.532197"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-08-31"><day>31</day><month>08</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89892.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-28"><day>28</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89892.2"/></event></pub-history><permissions><copyright-statement>© 2023, Peterson et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Peterson et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89892-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89892-figures-v1.pdf"/><abstract><p>In nature, animal vocalizations can provide crucial information about identity, including kinship and hierarchy. However, lab-based vocal behavior is typically studied during brief interactions between animals with no prior social relationship, and under environmental conditions with limited ethological relevance. Here, we address this gap by establishing long-term acoustic recordings from Mongolian gerbil families, a core social group that uses an array of sonic and ultrasonic vocalizations. Three separate gerbil families were transferred to an enlarged environment and continuous 20-day audio recordings were obtained. Using a variational autoencoder (VAE) to quantify 583,237 vocalizations, we show that gerbils exhibit a more elaborate vocal repertoire than has been previously reported and that vocal repertoire usage differs significantly by family. By performing gaussian mixture model clustering on the VAE latent space, we show that families preferentially use characteristic sets of vocal clusters and that these usage preferences remain stable over weeks. Furthermore, gerbils displayed family-specific transitions between vocal clusters. Since gerbils live naturally as extended families in complex underground burrows that are adjacent to other families, these results suggest the presence of a vocal dialect which could be exploited by animals to represent kinship. These findings position the Mongolian gerbil as a compelling animal model to study the neural basis of vocal communication and demonstrates the potential for using unsupervised machine learning with uninterrupted acoustic recordings to gain insights into naturalistic animal behavior.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Every time you speak, the sounds coming out of your mouth may carry more meaning that you may have intended; they may reveal, for example, which country, city or even neighborhood you may be coming from. Indeed, the vocal patterns that humans use to communicate differ from one population to the next, creating an array of languages, dialects and accents.</p><p>Such diversity has also been identified in various social species across the animal kingdom. Naked mole rats, for instance, which live underground in complex societies, exhibit different ‘dialects’ depending on their group of origin. Yet studying the vocal patterns of animals has remained difficult, especially for species inhabiting burrows or other environments difficult to access.</p><p>Aiming to bypass these limitations, Peterson et al. adopted a ‘naturalistic’ approach that allowed them to capture the vocal calls of three families of Mongolian gerbils living undisturbed in enclosures that mimic features of their natural environment. These animals spend their lives underground in tight-knit families, with multiple groups often being in close proximity. Researchers have speculated that individuals may rely on vocal cues to identify whether they are part of the same colony, as they are often too far from each other to rely on sight or smell.</p><p>Over half a million vocalizations obtained continuously through the course of 20 days were analyzed using an artificial intelligence technique known as unsupervised machine learning. The analyses helped add new types of calls to the gerbil vocal repertoire, but also highlighted its complexity. In particular, they revealed that the animals could combine individual vocal elements into complex sequences. More importantly, this approach showed that gerbil families have vocal dialects that are stable across weeks, with each group displaying a preference for certain call types (i.e. words) and certain sequential patterns (i.e. phrases).</p><p>These findings demonstrate the benefits of the approach developed by Peterson et al. for the study of animal vocalizations. Going forward, they also suggest that the Mongolian gerbil could be used as an animal model to study the neural basis of vocal communication.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>mongolian gerbil</kwd><kwd>vocal communication</kwd><kwd>bioacoustics</kwd><kwd>ethology</kwd><kwd>social behavior</kwd><kwd>auditory neuroscience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mongolian gerbil (Meriones unguiculatus)</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T90DA059110</award-id><principal-award-recipient><name><surname>Peterson</surname><given-names>Ralph E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5T32MH096331-10</award-id><principal-award-recipient><name><surname>Peterson</surname><given-names>Ralph E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01DC020279</award-id><principal-award-recipient><name><surname>Sanes</surname><given-names>Dan H</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01DC018802</award-id><principal-award-recipient><name><surname>Schneider</surname><given-names>David M</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000861</institution-id><institution>Burroughs Wellcome Fund</institution></institution-wrap></funding-source><award-id>Career Award at the Scientific Interface</award-id><principal-award-recipient><name><surname>Schneider</surname><given-names>David M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014185</institution-id><institution>Searle Scholars Program</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schneider</surname><given-names>David M</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Alfred P. Sloan Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schneider</surname><given-names>David M</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005270</institution-id><institution>McKnight Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schneider</surname><given-names>David M</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100003194</institution-id><institution>New York Stem Cell Foundation</institution></institution-wrap></funding-source><award-id>Robertson Neuroscience Investigator</award-id><principal-award-recipient><name><surname>Schneider</surname><given-names>David M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Gerbils exhibit stable, family-specific vocal dialects over weeks, suggesting vocal communication may play a key role in representing kinship in natural social groups.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The field of ethology contains rich descriptions of complex behavioral actions, including a wealth of species-specific vocal repertoires. However, natural observations are often incomplete due to limitations in physical access for experimenter observation or behavioral recording. This can be particularly severe for family behaviors which occur in protected or remote environments, such as burrows in the case of fossorial rodent species like naked mole-rats and Mongolian gerbils (<xref ref-type="bibr" rid="bib8">Brett, 1986</xref>; <xref ref-type="bibr" rid="bib72">Scheibler et al., 2006</xref>). Some of these limitations have been addressed with laboratory environments that partially recapitulate real-world features (<xref ref-type="bibr" rid="bib75">Shemesh and Chen, 2023</xref>). However, these studies generally focused on relatively short periods of data collection that consider single animals or dyads with no prior social relationship.</p><p>While our understanding of social aural communication is sparse, even for humans (<xref ref-type="bibr" rid="bib57">Pagel et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Mascaro et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Schindler et al., 2022</xref>), we know that many vocal cues are learned through social experience, and provide pivotal information about an animal’s identity. For example, a human infant’s ability to discriminate between foreign language phonemes can be preserved by exposure to a live foreign speaker, but not an audiovisual recording (<xref ref-type="bibr" rid="bib43">Kuhl et al., 2003</xref>). Evidence from swamp sparrows suggests the presence of culturally transmissible ‘dialects’ – a term borrowed from linguistics to denote a pattern of vocal behavior that is used by members of a social group (<xref ref-type="bibr" rid="bib44">Maler and Tamura, 1964</xref>). Our study adopts this operational definition of a vocal dialect. Even some rodents, such as the naked mole rat, learn colony-specific dialects based on early social experience (<xref ref-type="bibr" rid="bib6">Barker et al., 2021</xref>). The literature for social facilitation of vocal discrimination or production is particularly strong for zebra finches (<xref ref-type="bibr" rid="bib20">Eales, 1989</xref>; <xref ref-type="bibr" rid="bib18">Derégnaucourt et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Narula et al., 2018</xref>). Therefore, our study considers the possibility that there is a diversity of vocalizations within the gerbil social group that may harbor family specific information.</p><p>We chose to focus on families, a canonical social group that has been predominantly studied during brief and experimentally restricted social encounters (e.g. mating, pup retrieval, aggression) in relatively featureless environments. Our goal was to construct a complete gerbil family social-vocal soundscape during a significant period of development under undisturbed, environmentally enriched conditions. Unlike many laboratory rodents, gerbils form pair bonds and maintain a family structure across generations (<xref ref-type="bibr" rid="bib1">Ågren, 1984a</xref>). These families are composed of a founding adult pair, and up to 15 extended family members that live cooperatively in underground burrows (<xref ref-type="bibr" rid="bib3">Ågren et al., 1989a</xref>; <xref ref-type="bibr" rid="bib4">Ågren et al., 1989b</xref>; <xref ref-type="bibr" rid="bib49">Milne-Edwards, 1867</xref>; <xref ref-type="bibr" rid="bib71">Scheibler et al., 2004</xref>). Given the darkness and complexity of their burrow systems, gerbils are thought to rely heavily on their auditory system for social interactions. Sibling bonds established through adolescence facilitate social structure and minimize inbreeding (<xref ref-type="bibr" rid="bib2">Ågren, 1984b</xref>). Natural burrows are found in multi-family neighborhoods with strictly enforced territorial boundaries <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/en3qXx/RZSQS+oOvID+fB1qw">(</ext-link><xref ref-type="bibr" rid="bib72">Scheibler et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Ågren et al., 1989a</xref>; <xref ref-type="bibr" rid="bib4">Ågren et al., 1989b</xref>). Like prairie voles, gerbils act cooperatively to hoard food, maintain nests, defend their territory, and care for pups <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/en3qXx/HxMDE+X1jr">(</ext-link><xref ref-type="bibr" rid="bib22">Elwood, 1975</xref>; <xref ref-type="bibr" rid="bib32">Gromov, 2021</xref>). Therefore, gerbils display a range of rodent-typical behaviors (<xref ref-type="bibr" rid="bib38">Hurtado-Parrado et al., 2017</xref>), as well as complex family behaviors. Gerbils also display significant vocal communication in both the ultrasonic and sonic ranges (<xref ref-type="bibr" rid="bib65">Rübsamen et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Kobayasi and Riquimaroux, 2012</xref>) which is likely to be integral to social behaviors. Unlike many other rodent species, gerbils are able to hear within sonic ranges at sensitivities similar to humans (<xref ref-type="bibr" rid="bib66">Ryan, 1976</xref>). As a result, there is a rich, contemporary literature on the auditory perceptual skills, peripheral and central physiology, central anatomy, learning, and genomics in this species (<xref ref-type="bibr" rid="bib9">Budinger and Scheich, 2009</xref>; <xref ref-type="bibr" rid="bib10">Buran et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Happel et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">Myoga et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Pachitariu et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Sarro et al., 2015</xref>; <xref ref-type="bibr" rid="bib80">von Trapp et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Caras and Sanes, 2017</xref>; <xref ref-type="bibr" rid="bib15">Cheng et al., 2019</xref>; <xref ref-type="bibr" rid="bib86">Zorio et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Yao et al., 2020</xref>; <xref ref-type="bibr" rid="bib5">Amaro et al., 2021</xref>; <xref ref-type="bibr" rid="bib58">Paraouty et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Yao and Sanes, 2021</xref>; <xref ref-type="bibr" rid="bib69">Saldeitis et al., 2022</xref>; <xref ref-type="bibr" rid="bib59">Penikis and Sanes, 2023</xref>).</p><p>Here, we made continuous 20-day audio recordings from three separate gerbil families (2 parents, 4 pups) in an enlarged home cage that was isolated from other gerbils and humans. Specifically, we recorded audio over a period beginning at postnatal day 11–13 when auditory cortex is particularly sensitive to acoustic experience, and extending to postnatal day 31–32, the time when animals are typically weaned. Our goal was to acquire a descriptive dataset of the spectrotemporal structure of vocalizations emitted throughout daily family life, and without human intervention. Using emerging methods in unsupervised vocalization analysis, we quantitatively describe the spectrotemporal structure of vocalizations over multiple timescales and demonstrate that vocal repertoire usage differs between families.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Longitudinal familial audio recording</title><p>We obtained acoustic recordings (four microphones, 125 kHz sampling rate) from three separate gerbil families, each containing two adults and four pups (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Continuous recordings began at P11-13, lasted 20 days, and pups were weaned at P29 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). As shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, we extracted all sound events (yellow) using amplitude thresholding of acoustic power. To isolate vocalizations (blue) from non-vocal sounds (red), we computed the spectral flatness of each sound event and classified sounds with a threshold value of &lt;0.3 as vocalizations. A similar approach has previously been used in mice (<xref ref-type="bibr" rid="bib12">Castellucci et al., 2016</xref>), and we verified that a threshold value of 0.3 minimized the number of false positives (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Using this approach, 10,267,972 sound events were extracted, containing 583,237 vocalizations and 9,684,735 non-vocal sounds detected across the three families. Sound events were produced at an average rate of 6726+/-1260 times per hour (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), which reveals the rate of auditory object processing (<xref ref-type="bibr" rid="bib30">Griffiths and Warren, 2004</xref>) for gerbil families in an undisturbed setting. Vocalizations represent 6.99+/-3.07% of all sound events over the recording period (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) and were emitted at an average rate of 405+/-103 times per hour (<xref ref-type="fig" rid="fig1">Figure 1F</xref>), although this varied with time of day (see below).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Longitudinal familial audio recording.</title><p>(<bold>A</bold>) Recording apparatus. Four ultrasonic microphones sampled at 125 kHz continuously recorded a family in an enlarged environment. (<bold>B</bold>) Experiment timeline. Three gerbil families with the same family composition (2 adults, 4 pups) were recorded continuously for 20 days. (<bold>C</bold>) Extraction of sound events from raw audio using sound amplitude thresholding (Gray threshold = ‘th_2’, black threshold = ‘th_1’ and ‘th_3’; see Methods). Vocalizations (n=583,237) are separated from non-vocal sounds (n=9,684,735) using a threshold on spectral flatness (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> see Methods). (<bold>D</bold>) Summary of total sound event emission and average emission per hour. (<bold>E</bold>) Proportion of all sound events that are vocal or non-vocal sounds. (<bold>F</bold>) Summary of total vocalization emission and average emission per hour.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Vocalization extraction.</title><p>(<bold>A</bold>) Distribution of the spectral flatness of all sound events extracted. Vertical red line = 0.3. (<bold>B</bold>) False-positive percentage derived from human labeling of noise detected in randomly sampled 10x10 vocalization matrices. Random samples came from putative vocalizations with spectral flatness less than a moving threshold of 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4 (n=10 random samples per group). (<bold>C</bold>) Example random sample matrix of vocalizations with spectral flatness &lt;0.3. Four false positives observed in this grid.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig1-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Unsupervised discovery of the Mongolian gerbil vocal repertoire</title><p>To quantify the full array of vocalizations obtained from the three families, we trained a variational autoencoder (VAE) on vocalization spectrograms. The VAE learned a low-dimensional representation of latent acoustic features, thereby enabling analysis of such a large dataset with a larger representational capacity than standard acoustic features (<xref ref-type="bibr" rid="bib26">Goffinet et al., 2021</xref>). <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows a schematic of the VAE architecture used (<xref ref-type="bibr" rid="bib26">Goffinet et al., 2021</xref>), where spectrograms (top; 128x128 pixels) are reduced via a deep convolutional neural network ‘encoder’ to a latent vector (middle; 32-dimensional). A deep convolutional neural network ‘decoder’ then reconstructs a spectrogram (bottom) from the 32-dimensional latent representation. The encoder/decoder networks are jointly trained to minimize the discrepancy between the original and reconstructed spectrograms (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A, B</xref>), resulting in a low-dimensional latent representation, or ‘code’, which depicts each vocalization. To cluster vocalizations into distinct categories, we trained a Gaussian Mixture Model (GMM) on VAE latent representations. Using a combination of the elbow method on held-out log likelihood and established knowledge for how many vocal types gerbils emit (see Methods, GMM clustering), we selected a model with 70 vocal clusters as a parsimonious description of the data (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). <xref ref-type="fig" rid="fig2">Figure 2B</xref> shows a UMAP embedding of the VAE latents (center), used for visualization purposes only, which demonstrates that the gerbil vocal repertoire is more discrete than mouse, yet less discrete than zebra finch (<xref ref-type="bibr" rid="bib67">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Goffinet et al., 2021</xref>). Vocalizations occur as either single syllables bounded by silence (monosyllabic) or consist of combinations of single syllables without a silent interval (multisyllabic). Representative examples from 12 monosyllabic vocalization clusters are shown with their relative position in UMAP space, one of which appears similar in form to naked mole rat family specific chirp (blue box with asterisk; <xref ref-type="bibr" rid="bib6">Barker et al., 2021</xref>). Furthermore, monosyllabic vocalizations (56/70 vocal clusters) can be flexibly strung together to create multisyllabic or ‘composite’ vocalizations (9/70 of vocal clusters; <xref ref-type="bibr" rid="bib42">Kobayasi and Riquimaroux, 2012</xref>). The remaining five clusters contained a mixture of monosyllabic and multisyllabic vocalizations. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows 8 examples of multisyllabic vocalizations and their monosyllabic component boundaries, some of which have been reported previously (<xref ref-type="bibr" rid="bib42">Kobayasi and Riquimaroux, 2012</xref>) and some of which are newly characterized (white asterisks). To assess how family structure influences vocal repertoire usage, we compared vocal usage one day prior and one day after pup weaning, showing a drastic decrease in vocal emission (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). A large-magnitude vocal repertoire change is also observed, with the repertoire confined to a small region of vocal space following weaning (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B–D</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Unsupervised discovery of the Mongolian gerbil vocal repertoire.</title><p>Variational autoencoder and clustering. (<bold>A</bold>) Vocalization spectrograms (top) are input to a variational autoencoder (VAE) which encodes the spectrogram as a 32-D set of latent features (middle). The VAE learns latent features by minimizing the difference between original spectrograms and spectrograms reconstructed from the latent features by the VAE decoder (bottom). A gaussian mixture model (GMM) was trained on the latent features to cluster vocalizations into discrete categories. (<bold>B</bold>) Representative vocalizations from 12 distinct GMM clusters featuring monosyllabic vocalizations are shown surrounding a UMAP embedding of the latent features. Asterisk denotes vocal type not previously characterized. (<bold>C</bold>) Examples of multisyllabic vocalizations. White vertical lines indicate boundaries of monosyllabic elements. Asterisks denote multisyllabic vocal types not previously characterized.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>VAE training and GMM clustering.</title><p>(<bold>A</bold>) VAE reconstruction examples for different vocalization types. (<bold>B</bold>) VAE test and training loss show plateau in performance after a few epochs (model used in this study is epoch 50). (<bold>C</bold>) GMM held-out log likelihood as a function of the number of clusters used during model training. Seventy clusters were used in this study. (<bold>D</bold>) MMD<sup>2</sup> permutation comparisons. All family comparisons occur greater than expected by chance (p&lt;0.01, independent t-test). (<bold>E</bold>) Number of latent features used by VAE.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig2-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Family specific usage of vocal clusters</title><p>We next asked whether gerbil families display different vocalization usage patterns. First, we visualized the entire vocal repertoire usage of each family as a probability density heatmap and determined that vocal repertoire usage significantly differed between families (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>). Next, using GMM vocalization clusters, we compared the proportion usage of each vocal cluster for the three families, revealing specific vocal cluster differences between families (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). All families used each of the 70 vocal types (i.e. no cluster usage is 0), but each family relied more heavily on some clusters as compared to others. Importantly, this result is stable across a wide range of GMM clusters (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Family specific vocal usage.</title><p>(<bold>A</bold>) UMAP probability density plots (axes same as <xref ref-type="fig" rid="fig2">Figure 2B</xref>) show significant differences between family repertoires (p&lt;0.01, MMD permutation test on latent space; see Methods). (<bold>B</bold>) GMM vocal cluster usage by family. Clusters sorted by cumulative usage across all families. Families show distinct usage patterns of different vocal clusters. (<bold>C</bold>) Clusters are resorted by the usage difference between families. (<bold>D</bold>) Spectrogram examples from top differentially used clusters (left) and location of clusters in embedding space (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Pup removal biases vocal repertoire usage.</title><p>(<bold>A</bold>) Pup weaning causes a consistent reduction in vocal emission across families. (<bold>B</bold>) UMAP probability densities of the vocal repertoire pre and post pup weaning. Example vocalization from high-density post-weaning regions. (<bold>C</bold>). Difference in probability densities and total percent-change in repertoire pre-post pup weaning. (<bold>D</bold>) Quantification of day-to-day percent-change throughout the experiment shows that the percent-change magnitude observed in C is rare.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Acoustic features for GMM clusters.</title><p>Acoustic features computed on the top 100 most probable vocalizations from each GMM cluster. Mean values ± standard deviation shown. Details on acoustic feature calculation are described in the Methods section.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Sorting the GMM cluster labels by the pairwise difference in vocal type usage between the three families revealed which vocal types differed most (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Examples of top preferred vocal clusters for each family are shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, along with the position of those vocal clusters in UMAP embedding space. Families overexpress dissimilar vocal clusters relative to each other (e.g. clusters 4 and 8 in Family 2) and similar vocal clusters relative to each other (e.g. cluster 14 in Family 1 and cluster 1 in Family 3; cluster 9 in Family 1 and cluster 5 in Family 2).</p></sec><sec id="s2-4"><title>Vocal usage differences remain stable across days of development</title><p>It is possible that the observed vocal usage differences could result from varying developmental progression of vocal behavior or overexpression of certain vocal clusters during specific periods within the recording. To assess the potential effect of daily variation on family specific vocal usage, we visualized density maps of vocal usage across days for each of the families (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). There are two noteworthy trends: (1) the density map remains coarsely stable across days (rows) and (2) the maps look distinct across families on any given day (columns). This is a qualitative approximation for the repertoire’s stability, but does not take into account variation of call type usage (as defined by GMM clustering of the latent space). <xref ref-type="fig" rid="fig4">Figure 4B</xref>, shows the normalized usage of each cluster type over development for each family. Cluster usages during the period of ‘full family, shared recording days‘ (postnatal days beneath the purple bars) are stable across days within families – as is apparent by the horizontal striations in the plot – although each family maintains this stability through using a unique set of call types. This is addressed empirically in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, which shows clearly separable PCA projections of the cluster usages shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref> (purple days, concatenated into a 45 day x 70 cluster matrix). Finally, we computed the pairwise Maximum Mean Discrepancy (MMD) between latent distributions of vocalizations from individual recording days for each of the families (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). This shows that across-family repertoire differences are substantially larger than within-family differences. This is visualized in a multidimensional scaling projection of the MMD matrix in <xref ref-type="fig" rid="fig4">Figure 4E</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Vocal usage differences remain stable across days of development.</title><p>(<bold>A</bold>) UMAP probability density plots for each day of the recording, across families. Purple box indicated recording days that are shared across families. These days are used for subsequent analyses in <bold>C-E</bold>. (<bold>B</bold>) GMM vocal cluster usage per day. Usages are normalized on a per-day basis. A unique color is used for each cluster type. (<bold>C</bold>) PCA projection of daily usages within the purple (shared recording days) period showing that families use a unique subset of clusters stably across days. (<bold>D</bold>) Maximum Mean Discrepancy (MMD) distance between VAE latent distributions of vocalizations between days and across families. (<bold>E</bold>) Multidimensional scaling projection of MMD matrix from (<bold>D</bold>). Family vocal repertoires are distinct and remain so across days.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Family specific cluster usages do not depend on GMM cluster size.</title><p>(<bold>A</bold>) GMM cluster usages for each family over a range of GMM cluster sizes. (<bold>B</bold>) Quantification of pairwise cluster usage differences showing stability of family differences over all cluster sizes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig4-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Transition structure, but not emission structure, shows family specific differences</title><p>To assess whether temporal features also harbor family differences, we analyzed vocalization emission over a range of ethologically relevant timescales. First, we summed the total vocal emission for each hour of the day over the entire recording period, which revealed a diurnal activity pattern that was similar across the three families recorded (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We then analyzed a shorter time scale, the inter-vocalization-interval. The distribution of intervals between subsequent vocalizations is broad, with some vocalizations occurring rapidly after one another (e.g. within tens to hundreds of milliseconds) and others separated by many seconds. The majority of vocalizations occurred in bouts (58.5±0.9%), which we extracted using two criteria: (1) vocalizations within a bout display inter-vocalization-interval of &lt;2 s, and (2) a bout contains at least 5 vocalizations (based on <xref ref-type="bibr" rid="bib64">Rose et al., 2021</xref>). The distribution of bout durations, inter-vocalization-intervals, and vocalization durations for each family are highly overlapping and contain the same peaks (<xref ref-type="fig" rid="fig5">Figure 5B–D</xref>), suggesting that the temporal structure of vocal emission does not vary by family. Vocalization bouts show striking structure in vocal type sequencing (<xref ref-type="fig" rid="fig5">Figure 5E, F</xref>), therefore we next assessed whether vocal cluster sequencing varied by family. Vocal cluster transition matrices revealed a strong self-transition preference for all vocal clusters across families (<xref ref-type="fig" rid="fig5">Figure 5G</xref>); however, the proportion usage of different transitions (including self-transitions) drastically varied by family (<xref ref-type="fig" rid="fig5">Figure 5H</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Transition structure, but not emission structure, shows family specific differences.</title><p>(<bold>A</bold>) Vocalizations are emitted in a diurnal cycle. (<bold>B</bold>) Vocalizations consistently occur in seconds-long bouts across families. (<bold>C</bold>) Vocalization intervals (onset-to-onset) are consistent across families. (<bold>D</bold>) Vocalization durations are consistent across families. (<bold>E</bold>) Raw data examples of bouts. (<bold>F</bold>) Bouts typically occupy a similar area of vocal space. (<bold>G</bold>) Vocal cluster transition matrix. Vocalizations strongly favor self-transition. (<bold>H</bold>) Bigram probability graph. Self and other vocalization transition tendencies show family specific transitions (edges &gt; 0.001 usage shown).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Vocalization transitions are non-random and family specific.</title><p>(<bold>A</bold>) Vocal cluster transition matrix (same as <xref ref-type="fig" rid="fig5">Figure 5G</xref>). (<bold>B</bold>) Random transition matrix, computed after shuffling vocal cluster label sequence. (<bold>C</bold>) Transitions that occur greater than expected by chance (1000-iteration random shuffle with one-sample t-test and post hoc Benjamini-Hochberg multiple comparisons correction; see Methods). (<bold>D</bold>) Most common transitions (&gt;0.04% usage) from cluster 12 (roughly equally used across all families) to other clusters. Red lines indicate transitions that are shared across families, black lines indicate unique family specific transitions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89892-fig5-figsupp1-v1.tif"/></fig></fig-group><p>To determine whether differences in 1 gram structure contribute to differences in the transition (2 gram) structure, we performed a number of controls. Although subtle, vertical streaks are clearly present in shuffled transition matrices that correspond to 1 gram usages (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A, B</xref>). Given the shuffled data structure, we sought to determine whether the observed transition probabilities differed significantly from chance levels. We randomly shuffled label sequences 1000 times independently for each family to generate a null transition matrix distribution. Using these null distributions and the observed transition probabilities, we computed a p-value for each transition using a one-sample t-test and created a binary transition matrix indicating which transitions happen above chance levels (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, black pixels, <italic>P</italic> ≤ 0.05 after post hoc Benjamini-Hochberg multiple comparisons correction). As is made clear in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, most transitions for each family occur significantly above chance levels, despite the inherent 1 gram structure. Moreover, by looking at transitions from a highly usage cluster type used roughly the same proportion across families (cluster 12), we show that families arrange the same sets of vocal clusters into unique sequences (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D</xref>). We believe that this provides compelling evidence that the 1 gram structure does not change the interpretation of the main claim that transition structure varies by family.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Understanding the neural mechanisms that support natural behaviors depends upon our ability to quantify specific actions over a range of ethologically relevant contexts and timescales (<xref ref-type="bibr" rid="bib48">Miller et al., 2022</xref>). In principle, this requires continuous, undisturbed, and longitudinal recording that takes place in nature or a naturalistic context. This need has led to the emergence of powerful video tools for long-term monitoring and machine-learning based analyses (<xref ref-type="bibr" rid="bib16">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Pereira et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Shemesh and Chen, 2023</xref>). In contrast, most studies of natural behavior have not acquired and analyzed acoustic information over prolonged periods, or from a socially intact cohort. Therefore, to characterize vocal communication in a canonical social group, we obtained continuous audio recordings from three separate Mongolian gerbil families over a 20-day period (<xref ref-type="fig" rid="fig1">Figure 1</xref>). By expanding the recording duration, and permitting animals undisturbed interaction with their family unit, we sought to capture a more diverse vocal repertoire, and to determine whether vocal attributes were associated with family identity.</p><p>Capitalizing on advances in computational bioacoustics, which aid in the characterization of complex and high-dimensional vocal behavior (<xref ref-type="bibr" rid="bib67">Sainburg et al., 2020</xref>; <xref ref-type="bibr" rid="bib68">Sainburg and Gentner, 2021</xref>; <xref ref-type="bibr" rid="bib26">Goffinet et al., 2021</xref>), we extracted vocalization spectrograms and used a VAE to perform unsupervised analysis of a large number of familial gerbil vocalizations (n=583,237). At least one new vocal type and numerous multisyllabic vocal types were discovered using this approach (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Also, we provide evidence that family structure is necessary to elicit the full vocal repertoire (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). These findings underscore the advantage of a longitudinal naturalistic approach, and suggest that further elaborations (e.g. providing a larger-scale naturalistic environment) could reveal new aural communication behaviors.</p><p>Social vocalizations can convey pivotal information about an animal’s identity. For example, female macaques learn to recognize the vocalizations of their own offspring during the second postnatal week, and retain this ability for at least 6 months (<xref ref-type="bibr" rid="bib40">Jovanovic et al., 2000</xref>; <xref ref-type="bibr" rid="bib76">Shizawa et al., 2005</xref>). Similarly, kittens learn their mother’s vocalizations, and Australian sea lions can recall their mother’s voice up to 2 years after weaning (<xref ref-type="bibr" rid="bib63">Pitcher et al., 2010</xref>; <xref ref-type="bibr" rid="bib78">Szenczi et al., 2016</xref>). Furthermore, the meaning of vocal cues are often learned through long-term social experience. For example, when exposed to a chicken maternal call during development, socially reared mallard ducklings come to prefer it over their own species’ call (<xref ref-type="bibr" rid="bib28">Gottlieb, 1993</xref>). Similarly, wood ducklings must be exposed to sibling vocalizations in order to remain selectively responsive to its mother’s assembly call (<xref ref-type="bibr" rid="bib27">Gottlieb, 1983</xref>). Horseshoe bats, naked mole rats, and dolphins each model their calls based on early social experience (<xref ref-type="bibr" rid="bib39">Jones and Ransome, 1993</xref>; <xref ref-type="bibr" rid="bib24">Fripp et al., 2005</xref>; <xref ref-type="bibr" rid="bib23">Favaro et al., 2016</xref>; <xref ref-type="bibr" rid="bib6">Barker et al., 2021</xref>). Finally, rodent vocalizations can also harbor information about the individual identity and colony membership of the vocalizer (<xref ref-type="bibr" rid="bib6">Barker et al., 2021</xref>). In fact, research on song learning in zebra finches shows that a reward learning mechanism may support the transmission of vocal repertoires: exposure to a live singing tutor, but not song playback, selectively activates dopamine neurons in the juvenile periaqueductal gray which is thought to mediate learning (<xref ref-type="bibr" rid="bib79">Tanaka et al., 2018</xref>). Therefore, there is a compelling rationale for exploring the diversity of vocalizations within the gerbil family social group, and to pursue the underlying neural mechanisms in the future.</p><p>To address whether gerbils also exhibit family specific vocal features, we compared GMM-labeled vocal cluster usages across the three recorded families and showed differences in vocal type usage (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Although we chose 70 clusters for our analyses, the general finding was robust across a wide range of GMM clusters (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The differences in this study align with the definition of human vocal dialect, which is a regional or social variety of language that can differ in pronunciation, grammatical, semantic and/or language use differences (<xref ref-type="bibr" rid="bib34">Henry et al., 2015</xref>). This definition of dialect is inclusive of both pronunciation differences (e.g. a Bostonian’s characteristic pronunciation of ‘car’ as ‘cah’) and usage differences (e.g. a Bostonian’s preferential usage of the words ‘Go Red Sox’ vs. a New Yorker’s preferential usage of the words ‘Go Yankees’). In our case, vocal clusters can be rarely observed in some families yet highly overexpressed in others (e.g. analogous to language usage differences in humans), or highly expressed in both families, but contain subtle spectrotemporal variations (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, Family 1 cluster 11 vs. Family 3 clusters 2, 18, 30; for example analogous to pronunciation differences in humans). Like another fossorial species, the naked mole-rat, it is possible that gerbils may also possess the ability to acquire family specific vocal behavior through experience (<xref ref-type="bibr" rid="bib6">Barker et al., 2021</xref>). Unlike the naked mole-rat which showed the presence of a colony-specific vocal dialect in a single vocal type, the soft chirp, we show that fine spectrotemporal variations in multiple different gerbil vocal types could harbor dialects (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p>The described family differences collapse data from multiple days into a single comparison; however, it is possible that factors such as vocal development and/or high usage of particular vocal types during specific periods of the recording could explain family differences. Therefore, we took advantage of the longitudinal nature of our dataset to assess whether repertoire differences remain stable across time. First, we visualized vocal repertoire usage across days as either UMAP probability density maps (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) or daily GMM cluster usages (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Though qualitative, one can appreciate that family repertoire usage remains stable across days and appears to differ on a consistent daily basis across families. To formally quantify this, we first projected GMM cluster usages from <xref ref-type="fig" rid="fig4">Figure 4B</xref> into PC space and show that family GMM cluster usage patterns are highly separable, regardless of postnatal day (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). If families had used a more overlapping set of call types, then the projections would have appeared intermixed. Next, we performed a cluster-free analysis by computing the pairwise MMD distance between VAE latent distributions of vocalizations from each family and day (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). This analysis shows very low MMD values across days within a family (i.e. the repertoire is highly consistent with itself), and high MMD values across families/days (greater than would be expected by chance; see shuffle control in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>). The relative differences in this matrix are made clear in <xref ref-type="fig" rid="fig4">Figure 4E</xref>, which provides additional evidence that family vocal repertoires remain stable across days and are consistently different from other families. Taken together, we believe that this is compelling evidence that differences in vocal repertoires between families are not driven by dominating call types during specific phases in the recording period; rather, families consistently emit characteristic sets of call types across days. This opens up the possibility to assess repertoire differences over much shorter time periods (e.g. 24 hr) in future studies.</p><p>Vocalization emission statistics and behavioral syllable transition patterns can signify differences between groups of animals (<xref ref-type="bibr" rid="bib13">Castellucci et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Markowitz et al., 2018</xref>). Therefore, it is possible that vocal emission patterns or vocal cluster transition patterns may be family specific. To address this, we first compared vocalization emission rates over multiple ethologically relevant timescales, which revealed highly consistent emission patterns across families (<xref ref-type="fig" rid="fig5">Figure 5A–D</xref>). First, we observed that vocal emission follows a diurnal pattern, with peaks of activity in the morning and afternoon. This result complements prior work in gerbils showing diurnal activity patterns in gerbil groups for non-vocal behaviors (<xref ref-type="bibr" rid="bib62">Pietrewicz et al., 1982</xref>), but extends our understanding to vocal behavior. Vocalizations are rarely emitted in isolation. Rather, they are emitted in sequences (‘bouts’) with a modal duration of 4 s and a duration distribution that does not vary between families. These emission statistics are somewhat consistent with the common phoneme rate in humans (<xref ref-type="bibr" rid="bib21">Edwards and Chang, 2013</xref>; <xref ref-type="bibr" rid="bib19">Ding et al., 2017</xref>). Also, the distributions of inter-vocalization interval and vocalization duration did not differ between families. Taken together, the temporal emission structure is highly consistent across families and suggests that these features are likely not exploited for kinship identification. However, this does not rule out the possibility that the sequential organization of vocalizations could vary. Vocalization bouts (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, <xref ref-type="fig" rid="fig5">Figure 5E–F</xref>) show that temporal sequencing of vocalization clusters is non-random and has a compelling transition structure with potential to vary across families. To formally quantify this, we calculated vocalization transition matrices for each family, which revealed that all families strongly favor vocalization self-transitions (<xref ref-type="fig" rid="fig5">Figure 5G</xref>), although hinted that non self-transitions (off-diagonal) vary by family. To visualize this, we generated bigram transition graphs of highly expressed vocalization transitions, which provides evidence that vocalization transition structure varies by family (<xref ref-type="fig" rid="fig5">Figure 5H</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Importantly, families arrange the same sets of vocal clusters into unique sequences (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref>). There are limitations to this study that deserve consideration. First, a fully realized assessment of vocalization usage should be integrated with continuous sub-second synchronized videographic data from which one can extract animal pose estimation and behavioral categorization. For example, such data could allow us to control for the total number and type of social interactions, which may explain differences in the amount or usage of specific syllables. Second, although we used four microphones, it was not possible to localize the majority of vocalizations with sufficient spatial resolution (using Mouse Ultrasonic Source Estimation software; <xref ref-type="bibr" rid="bib53">Neunuebel et al., 2015</xref>). To properly address whether individual animals emit a unique vocalization repertoire, we will require significant advances in the field of computational bioacoustics. In anticipation of future research in this area, we have computed acoustic features of vocalizations in each of the GMM clusters as a reference (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>Although we were not able to attribute vocalizations to individual family members, we did seek to determine the importance of family structure by comparing audio recordings before and after removal of the pups at P30. The results show a clear effect of family integrity, and the sudden reduction of sonic calls following pup removal (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) could suggest that these vocalizations are produced selectively by pups. However, there is ample evidence that adult gerbils also produce sonic vocalizations. For example, a number of low-frequency call types are used by adults during a range of social interactions (<xref ref-type="bibr" rid="bib65">Rübsamen et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Furuyama et al., 2022</xref>), some of which are similar to a low-frequency call type used by pups (<xref ref-type="bibr" rid="bib77">Silberstein et al., 2023</xref>). Vocalization patterns of developing gerbils depend on isolation or staged interactions. Thus, when gerbil pups are recorded during isolation, ultrasonic vocalization rate declines and sonic vocalizations increase for animals that are in a high arousal state (<xref ref-type="bibr" rid="bib17">De Ghett, 1974</xref>; <xref ref-type="bibr" rid="bib77">Silberstein et al., 2023</xref>). As gerbils progress from juvenile to adolescent development (P17-55) a significant increase in ultrasonic vocalization rate is observed during dyadic social encounters, with a distinct change in usage pattern that depends upon the sex of each animal (<xref ref-type="bibr" rid="bib36">Holman and Seale, 1991</xref>; <xref ref-type="bibr" rid="bib37">Holman et al., 1995</xref>). The development of vocalization types has been assessed in another member of the Gerbillinae subfamily, called fat-tailed gerbils (Pachyuromys duprasi), during isolation and handling. Here, the number of ultrasonic vocalization syllable types increase from neonatal to adult animals (<xref ref-type="bibr" rid="bib84">Zaytseva et al., 2019</xref>), while some very low frequency sonic call types were rarely observed after P20 (<xref ref-type="bibr" rid="bib85">Zaytseva et al., 2020</xref>). By comparison, mouse syllable usage changes during development, but pups produced 10 of the 11 syllable types produced by adults (<xref ref-type="bibr" rid="bib31">Grimsley et al., 2011</xref>). In summary, our understanding of the maturation of vocalization usage remains limitted by our inability to obtain longitudinal data from individual animals within their natural social setting. For example, when recorded in their natural environment, chimpanzees display a prolonged maturation of vocalization complexity, such as the probability of a unique utterance in a sequence, with the greatest changes occuring when animals begin to experience non-kin social interactions (<xref ref-type="bibr" rid="bib7">Bortolato et al., 2023</xref>).</p><p>These results reveal that Mongolian gerbil families possess a rich repertoire of vocalizations used during day-to-day communication. Our findings indicate that long-term behavioral monitoring of a core social unit (i.e. the family) reveals richer vocal behavior than has previously been reported in the species. Leveraging unsupervised machine learning to quantify vocalizations, we reveal family-specific vocalization usage and transition structure. Taken together, these findings establish the Mongolian gerbil as a useful model organism for studying the neurobiology of vocal interactions in complex social groups.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental animals</title><p>Three gerbil families (<italic>Meriones unguiculatus</italic>, n=6 per family: 2 adults, 4 pups) were used in this study (Charles River). All procedures related to the maintenance and use of animals were approved by the University Animal Welfare Committee at New York University (protocol 2020–1112), and all experiments were performed in accordance with the relevant guidelines and regulations.</p></sec><sec id="s4-2"><title>Audio recording</title><p>Four ultrasonic microphones (Avisoft CM16/CMPA48AAF-5V) were synchronously recorded using a National Instruments multifunction data acquisition device (PCI-6143) via BNC connection with a National Instruments terminal block (BNC-2110). The recording was controlled with custom python scripts using the NI-DAQmx-python library version 0.5.7; (<ext-link ext-link-type="uri" xlink:href="https://github.com/ni/nidaqmx-python">https://github.com/ni/nidaqmx-python</ext-link>; <xref ref-type="bibr" rid="bib52">National Instruments, 2017</xref>) which wrote samples to disk at a 125 kHz sampling rate. In total, 13.084 TB of raw audio data were acquired across the three families. For further analyses, the four-channel microphone signals were averaged to create a single-channel high-fidelity audio signal.</p></sec><sec id="s4-3"><title>Audio segmentation</title><p>Audio was segmented by amplitude thresholding using the Autoencoded Vocal Analysis (AVA) python package (see <xref ref-type="bibr" rid="bib26">Goffinet et al., 2021</xref>). First, sound amplitude traces are calculated by computing spectrograms from raw audio, then summing each column of the spectrogram. The ‘get_onset_offsets’ function, which performs the segmenting, requires the selection of a number of parameters which affect segmenting performance. The following values were tuned via an interactive procedure which validated that the segmenting could detect low amplitude vocalizations and capture individual vocal units apparent by eye:</p><p><code xml:space="preserve">seg_params = {
'min_freq': 500 # minimum frequency
'max_freq': 62500, # maximum frequency
'nperseg': 512, # FFT'noverlap': 256, # FFT
'spec_min_val': –8, # minimum STFT log-modulus
'spec_max_val': –7.25, # maximum STFT log-modulus
'fs': 125000, # audio sample rate
'th_1': 2, # segmenting threshold 1
'th_2': 5, # segmenting threshold 2
'th_3': 2, # segmenting threshold 3
'min_dur':0.03, # minimum syllable duration (s)
'max_dur':0.3, # maximum syllable duration (s)
'smoothing_timescale': 0.007, # amplitude
'softmax': False, # apply softmax to the frequency bins to calculate amplitude
'temperature':0.5, # softmax temperature parameter
'algorithm': get_onsets_offsets
}</code></p><p>Sound onsets are detected when the amplitude exceeds 'th_3' (black dashed line, <xref ref-type="fig" rid="fig1">Figure 1C</xref>), and sound offset occurs when there is a subsequent local minimum for example amplitude less than 'th_2' (gray dashed line, <xref ref-type="fig" rid="fig1">Figure 1C</xref>), or 'th_1' (black dashed line, <xref ref-type="fig" rid="fig1">Figure 1C</xref>), whichever comes first. In this specific use case, th_2 (5) will always come before th_1 (2), therefore the gray dashed line will always be the offset. A subsequent onset will be marked if the sound amplitude crosses th_2 or th_3, whichever comes first. For example, the first sound event detected in <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows the sound amplitude rising above the black dashed line (th_3) and marks an onset. Subsequently, the amplitude trace falls below the gray dashed line (th_2) and an offset is marked. Finally, the amplitude rises above th_2 without dipping below th_3 and an onset for a new sound event is marked. Had the amplitude dipped below th_3, a new sound event onset would be marked when the amplitude trace subsequently exceeded th_3 (e.g. between sound event 2 and 3, <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The maximum and minimum syllable durations were selected based on published duration ranges of gerbil vocalizations (<xref ref-type="bibr" rid="bib65">Rübsamen et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Kobayasi and Riquimaroux, 2012</xref>).</p></sec><sec id="s4-4"><title>Vocalization extraction</title><p>We computed the spectral flatness of each detected sound event using the python package librosa (<ext-link ext-link-type="uri" xlink:href="https://github.com/librosa">https://github.com/librosa</ext-link>; <xref ref-type="bibr" rid="bib47">McFee et al., 2024</xref>). Consistent with prior literature (<xref ref-type="bibr" rid="bib12">Castellucci et al., 2016</xref>), we used a threshold on spectral flatness to separate putative vocal and non-vocal sounds. This threshold value was determined empirically, by calculating the false positive vocalization rate (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) of groups of randomly sampled vocalizations. For each spectral flatness value in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, 100 randomly sampled vocalization spectrograms less than the working threshold value were assembled into 10x10 grids and visually inspected for false positives (e.g. non-vocal sounds; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). This procedure was repeated 10 times for spectral flatness thresholds of 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, and 0.4. We quantified the false positive vocalization rate for each threshold value and selected 0.3, which had a 5.5+/-1.96% false positive rate.</p></sec><sec id="s4-5"><title>Variational autoencoder training</title><p>Extracted vocalizations were converted to 128x128 pixel spectrograms using the ‘process_sylls’ function from AVA with the following preprocessing parameters:</p><p><code xml:space="preserve">preprocess_params = {
'get_spec': get_spec, # spectrogram maker
'max_dur': 0.3, # maximum syllable duration'min_freq': 500, # minimum frequency
'max_freq': 62500, # maximum frequency
'nperseg': 512, # FFT
'noverlap': 256, # FFT
'spec_min_val': –8, # minimum log-spectrogram value
'spec_max_val': –5, # maximum log-spectrogram value
'fs': 125000, # audio sample rate
'mel': False, # frequency spacing, mel or linear
'time_stretch': True, # stretch short syllables?
'within_syll_normalize': False, # normalize spectrogram values on a
                       # spectrogram-by-spectrogram basis
'max_num_syllables': None, # maximum number of syllables per directory
'sylls_per_file': 100, # syllable per file
'real_preprocess_params': ('min_freq', 'max_freq', 
     'spec_min_val',’spec_max_val', 'max_dur'), # tunable parameters
'int_preprocess_params': ('nperseg','noverlap'), # tunable parameters
'binary_preprocess_params': ('time_stretch', 'mel',
      'within_syll_normalize'), # tunable parameters
}</code></p><p>A VAE was trained for 50 epochs using a model precision of 40. We removed additional false positive vocalizations by inspecting a 2D UMAP embedding of the VAE latent space and removing UMAP clusters containing non-vocal sounds from further analysis.</p></sec><sec id="s4-6"><title>Gaussian mixture model</title><p>GMMs were fit to cluster VAE latent feature vectors. To reduce computation time, we fit the model on 7 of 32 VAE latents (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E</xref>), as these explained 99.5% of the variance in the original feature space. The model was implemented in Stan (<ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/cmdstanpy">https://mc-stan.org/cmdstanpy</ext-link>), however similar clustering results were achieved using the scikit-learn Gaussian Mixture model class with a diagonal covariance matrix (<ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html">https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html</ext-link>). We fit the model using stochastic variational inference, an approximate Bayesian inference technique that recasts the task of learning a posterior distribution as an optimization problem and enables vast speedups (<xref ref-type="bibr" rid="bib35">Hoffman et al., 2013</xref>). GMMs typically assume that the whole population selects clusters with the same probabilities, however we modified this assumption to allow, although not enforce, the model to learn different cluster usage patterns for each family. Specifically, we used the following model:</p><p>Let <italic>D</italic> be the dimensionality of the VAE latents used (in our case, <italic>D</italic>=7) and <italic>K</italic> be the number of clusters. Denote our parameters by:</p><list list-type="simple"><list-item><p>Mixture means (<inline-formula><mml:math id="inf1"><mml:mi>β</mml:mi></mml:math></inline-formula>) for cluster <italic>j</italic>: <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></p></list-item><list-item><p>Mixture covariance matrix (<inline-formula><mml:math id="inf3"><mml:mi>Σ</mml:mi></mml:math></inline-formula>) for cluster <italic>j</italic>: <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, for <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></p></list-item><list-item><p>Cluster usage probabilities for cohort <inline-formula><mml:math id="inf6"><mml:mi>i</mml:mi></mml:math></inline-formula>: <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></inline-formula> with <inline-formula><mml:math id="inf8"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Cluster assignment for vocalization <italic>k</italic> of cohort <italic>i</italic>: <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula></p></list-item></list><p>We selected our hyperparameters according to Stan’s guidelines for weakly informative priors, yielding the model:</p><list list-type="simple"><list-item><p>Mixture means for cluster j: <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></p></list-item><list-item><p>Mixture standard deviations (<inline-formula><mml:math id="inf11"><mml:mi>σ</mml:mi></mml:math></inline-formula>) for cluster j: <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi><mml:mo>–</mml:mo><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></p></list-item><list-item><p>Cluster usage probabilities for cohort <inline-formula><mml:math id="inf13"><mml:mi>i</mml:mi></mml:math></inline-formula>: <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></p></list-item><list-item><p>Cluster assignment for vocalization k of cohort i: <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item></list><p>VAE feature embedding for vocalization k of cohort i: <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>To select the number of clusters, <italic>K</italic>, we held out 25% of our data, trained models with varying values for <italic>K</italic>, and calculated the log probability of seeing the held-out data under each model (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). Using the elbow method, we determined that ~70 clusters was a reasonable selection for K. Previous work documenting the Mongolian gerbil repertoire (<xref ref-type="bibr" rid="bib65">Rübsamen et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Kobayasi and Riquimaroux, 2012</xref>) has revealed ~12 vocalization types that vary with social context. It is likely that we are capturing these ~12 (plus a few more, as illustrated in <xref ref-type="fig" rid="fig2">Figure 2C</xref>) as well as individual or family-specific variations of some call types. Although the number of discrete call types is likely less than 70, it is plausible that variation due to vocalizer identity pushes some calls into unique clusters. This idea is supported by the fact that both naked mole rats and Mongolian gerbils have been shown to exhibit individual-specific variation in vocalizations, though only in single call types (Figure 1 from <xref ref-type="bibr" rid="bib6">Barker et al., 2021</xref>; Table I from <xref ref-type="bibr" rid="bib55">Nishiyama et al., 2011</xref>). Importantly, the core result is not affected by cluster size (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec><sec id="s4-7"><title>Maximum mean discrepancy permutation test</title><p>Clustering analyses are notoriously challenging (<xref ref-type="bibr" rid="bib41">Kleinberg, 2002</xref>). Thus, we performed a complementary analysis to investigate whether different gerbil families utilize different vocal repertoires. In particular, we pursued an approach that makes no assumptions about the number, character, or even existence of vocalization clusters.</p><p>Specifically, we used Maximum Mean Discrepancy (MMD) to quantify the difference between two latent distributions of vocalizations. This test considers two sets of observed data points (e.g. N vocalizations from Family 1 and N vocalizations from Family 2), which are assumed to be independent and identically distributed random variables from underlying probability distributions, and returns a distance metric corresponding to the equality of the two distributions (<xref ref-type="bibr" rid="bib29">Gretton et al., 2012</xref>). Lower values suggest distributions are more similar and higher values suggest distributions are more dissimilar. We investigated the null hypothesis that the gerbil families used the same vocal repertoire—that is the probability distribution over VAE latent space for each family was identical, corresponding to a MMD<sup>2</sup> distance of zero. To test this null hypothesis, we computed the MMD<sup>2</sup> distance between the empirical distributions of family pairs in batches of 1000 randomly sub-sampled vocalizations. This yielded a histogram of empirically observed MMD<sup>2</sup> distance values for each family pair, which we compared a null distribution generated by randomly permuting the family label attached to each vocalization. The empirically observed MMD<sup>2</sup> distances were much higher than the shuffle control, favoring the alternative hypothesis that gerbil families utilize distinct syllable usage statistics (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec><sec id="s4-8"><title>Transition analysis</title><p>Vocalization transition sequences were generated by concatenating vocal cluster labels chronologically for each family and calculating the number of transitions for all possible transition types. The resulting transition matrix was normalized such that each row sums to 1, thus reflecting the probability that vocalization <inline-formula><mml:math id="inf17"><mml:mi>i</mml:mi></mml:math></inline-formula> transitions to vocalization <inline-formula><mml:math id="inf18"><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, that is <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5G</xref>). The transition matrix used to generate the bigram probability graph in <xref ref-type="fig" rid="fig5">Figure 5H</xref> was normalized such that edge and node widths correspond to the probability of each vocalization pair, that is <inline-formula><mml:math id="inf20"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib74">Shannon, 1948</xref>).</p></sec><sec id="s4-9"><title>Acoustic feature calculations</title><p>First, raw audio from the most probable vocalization samples (n=100) from each vocal cluster were extracted. Next, using the VocalPy (<xref ref-type="bibr" rid="bib54">Nicholson, 2023</xref>) ‘similarity_features’ function (a python implementation of the Sound Analysis Pro Sound Analysis Tools library: <ext-link ext-link-type="uri" xlink:href="http://soundanalysispro.com/matlab-sat">http://soundanalysispro.com/matlab-sat</ext-link>), the following acoustic features were calculated: fundamental frequency (pitch), amplitude, entropy, frequency modulation, goodness of pitch. In addition to these features, spectral flatness was computed using librosa (<ext-link ext-link-type="uri" xlink:href="https://librosa.org/doc/latest/generated/librosa.feature.spectral_flatness.html">https://librosa.org/doc/latest/generated/librosa.feature.spectral_flatness.html</ext-link>), and duration was computed from the raw audio itself. Finally, start and stop frequencies were computed by taking the median fundamental frequency within the first third and last third (time) of each vocalization, respectively.</p><p>The following spectrogram parameters were used: nfft = 512, noverlap (a.k.a hop_length)=256, Fs = 125000, min_freq = 65, max_freq = 62,500. Features are computed on a spectrogram frame-by-frame basis. Single values for each vocalization were extracted by taking the median acoustic feature value across all spectrogram frames. The single exception to this was spectral flatness (to remain consistent with the spectral flatness calculation used for amplitude thresholding), which took the mean across all spectrogram frames and used the following spectrogram parameters: n_fft = 256, hop_length = 128, win_length = 256, center = False, power = 2.0.</p><p>Detailed description of the units associated with each feature are located here: <ext-link ext-link-type="uri" xlink:href="http://soundanalysispro.com/manual/chapter-4-the-song-features-of-sap2">http://soundanalysispro.com/manual/chapter-4-the-song-features-of-sap2</ext-link>. Code to compute acoustic features is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ralphpeterson/gerbil-vocal-dialects">https://github.com/ralphpeterson/gerbil-vocal-dialects</ext-link>, copy archived at <xref ref-type="bibr" rid="bib61">Peterson, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Methodology</p></fn><fn fn-type="con" id="con4"><p>Resources, Software, Methodology</p></fn><fn fn-type="con" id="con5"><p>Data curation</p></fn><fn fn-type="con" id="con6"><p>Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Three gerbil families (Meriones unguiculatus, n=6 per family: 2 adults, 4 pups) were used in this study (Charles River). All procedures related to the maintenance and use of animals were approved by the University Animal Welfare Committee at New York University, and all experiments were performed in accordance with the relevant guidelines and regulations. (protocol 2020-1112).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89892-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data analyzed in this study are freely available for download on Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.m905qfv68">https://doi.org/10.5061/dryad.m905qfv68</ext-link>). Code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ralphpeterson/gerbil-vocal-dialects">https://github.com/ralphpeterson/gerbil-vocal-dialects</ext-link>, copy archived at <xref ref-type="bibr" rid="bib61">Peterson, 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Choudhri</surname><given-names>A</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Tanelus</surname><given-names>A</given-names></name><name><surname>Capo-Battaglia</surname><given-names>A</given-names></name><name><surname>Wililams</surname><given-names>A</given-names></name><name><surname>Schneider</surname><given-names>A</given-names></name><name><surname>Sanes</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data for: Unsupervised discovery of family specific vocal usage in the Mongolian gerbil</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.m905qfv68</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Michael Long, Christine Constantinople, and members of the Sanes, Schneider, and Williams laboratories for helpful discussions and feedback on the study. We thank Nicholas Jourjine and David Nicholson for advice on acoustic feature calculations. This work was supported by National Institutes of Health Training Program in Computational Neuroscience T90DA059110 (REP), National Institutes of Health Training Program in Neuroscience 5T32MH096331 (REP), National Institute on Deafness and Other Communication Disorders at the National Institute of Health (R01DC020279 to DHS, R01DC018802 to DMS with supplement to REP, and R34DA059513 to AHW, DMS, and DHS), a Career Award at the Scientific Interface from the Burroughs Wellcome Fund (DMS), and fellowships from the Searle Scholars Program, the Alfred P Sloan Foundation, and the McKnight Foundation (DMS). DMS is a New York Stem Cell Foundation - Robertson Neuroscience Investigator.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ågren</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1984">1984a</year><article-title>Pair formation in the Mongolian gerbil</article-title><source>Animal Behaviour</source><volume>32</volume><fpage>528</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0003-3472(84)80291-2</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ågren</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1984">1984b</year><article-title>Incest avoidance and bonding between siblings in gerbils</article-title><source>Behavioral Ecology and Sociobiology</source><volume>14</volume><fpage>161</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1007/BF00299615</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ågren</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>Q</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1989">1989a</year><article-title>Ecology and social behaviour of Mongolian gerbils,Meriones unguiculatus, at Xilinhot, Inner Mongolia, China</article-title><source>Animal Behaviour</source><volume>37</volume><fpage>11</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(89)90002-X</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ågren</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>Q</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1989">1989b</year><article-title>Territoriality, cooperation and resource priority: hoarding in the Mongolian gerbil,Meriones unguiculatus</article-title><source>Animal Behaviour</source><volume>37</volume><fpage>28</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(89)90003-1</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amaro</surname><given-names>D</given-names></name><name><surname>Ferreiro</surname><given-names>DN</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Pecka</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Source identity shapes spatial preference in primary auditory cortex during active navigation</article-title><source>Current Biology</source><volume>31</volume><fpage>3875</fpage><lpage>3883</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.025</pub-id><pub-id pub-id-type="pmid">34192513</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barker</surname><given-names>AJ</given-names></name><name><surname>Veviurko</surname><given-names>G</given-names></name><name><surname>Bennett</surname><given-names>NC</given-names></name><name><surname>Hart</surname><given-names>DW</given-names></name><name><surname>Mograby</surname><given-names>L</given-names></name><name><surname>Lewin</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cultural transmission of vocal dialect in the naked mole-rat</article-title><source>Science</source><volume>371</volume><fpage>503</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1126/science.abc6588</pub-id><pub-id pub-id-type="pmid">33510025</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bortolato</surname><given-names>T</given-names></name><name><surname>Mundry</surname><given-names>R</given-names></name><name><surname>Wittig</surname><given-names>RM</given-names></name><name><surname>Girard-Buttoz</surname><given-names>C</given-names></name><name><surname>Crockford</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Slow development of vocal sequences through ontogeny in wild chimpanzees (<italic>Pan troglodytes verus</italic>)</article-title><source>Developmental Science</source><volume>26</volume><elocation-id>e13350</elocation-id><pub-id pub-id-type="doi">10.1111/desc.13350</pub-id><pub-id pub-id-type="pmid">36440660</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Brett</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Doctoral Dissertation: The Ecology and Behaviour of the Naked Mole-Rat, <italic>Heterocephalus glaber</italic> Ruppell (Rodenti: Bathyergidae)</article-title><publisher-name>University College London</publisher-name></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Budinger</surname><given-names>E</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Anatomical connections suitable for the direct processing of neuronal information of different modalities via the rodent primary auditory cortex</article-title><source>Hearing Research</source><volume>258</volume><fpage>16</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.04.021</pub-id><pub-id pub-id-type="pmid">19446016</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buran</surname><given-names>BN</given-names></name><name><surname>von Trapp</surname><given-names>G</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Behaviorally gated reduction of spontaneous discharge can improve detection thresholds in auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4076</fpage><lpage>4081</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4825-13.2014</pub-id><pub-id pub-id-type="pmid">24623785</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caras</surname><given-names>ML</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Top-down modulation of sensory cortex gates perceptual learning</article-title><source>PNAS</source><volume>114</volume><fpage>9972</fpage><lpage>9977</lpage><pub-id pub-id-type="doi">10.1073/pnas.1712305114</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname><given-names>GA</given-names></name><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Knockout of Foxp2 disrupts vocal development in mice</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>23305</elocation-id><pub-id pub-id-type="doi">10.1038/srep23305</pub-id><pub-id pub-id-type="pmid">26980647</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname><given-names>GA</given-names></name><name><surname>Calbick</surname><given-names>D</given-names></name><name><surname>McCormick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The temporal organization of mouse ultrasonic vocalizations</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0199929</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0199929</pub-id><pub-id pub-id-type="pmid">30376572</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Matheson</surname><given-names>LE</given-names></name><name><surname>Sakata</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mechanisms underlying the social enhancement of vocal learning in songbirds</article-title><source>PNAS</source><volume>113</volume><fpage>6641</fpage><lpage>6646</lpage><pub-id pub-id-type="doi">10.1073/pnas.1522306113</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Xian</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Klug</surname><given-names>A</given-names></name><name><surname>McCullagh</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Enhancement of de novo sequencing, assembly and annotation of the Mongolian gerbil genome with transcriptome sequencing and assembly from several different tissues</article-title><source>BMC Genomics</source><volume>20</volume><elocation-id>903</elocation-id><pub-id pub-id-type="doi">10.1186/s12864-019-6276-y</pub-id><pub-id pub-id-type="pmid">31775624</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Leifer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational neuroethology: a call to action</article-title><source>Neuron</source><volume>104</volume><fpage>11</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id><pub-id pub-id-type="pmid">31600508</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Ghett</surname><given-names>VJ</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Developmental changes in the rate of ultrasonic vocalization in the Mongolian gerbil</article-title><source>Developmental Psychobiology</source><volume>7</volume><fpage>267</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1002/dev.420070311</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derégnaucourt</surname><given-names>S</given-names></name><name><surname>Poirier</surname><given-names>C</given-names></name><name><surname>der Kant</surname><given-names>AV</given-names></name><name><surname>der Linden</surname><given-names>AV</given-names></name><name><surname>Gahr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Comparisons of different methods to train a young zebra finch (Taeniopygia guttata) to learn a song</article-title><source>Journal of Physiology, Paris</source><volume>107</volume><fpage>210</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.jphysparis.2012.08.003</pub-id><pub-id pub-id-type="pmid">22982543</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal modulations in speech and music</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>81</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.02.011</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eales</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>The influences of visual and vocal interaction on song learning in zebra finches</source><publisher-name>Animal Behaviour</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>E</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Syllabic (∼2-5 Hz) and fluctuation (∼1-10 Hz) ranges in speech and auditory processing</article-title><source>Hearing Research</source><volume>305</volume><fpage>113</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.08.017</pub-id><pub-id pub-id-type="pmid">24035819</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elwood</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Paternal and maternal behaviour in the Mongolian gerbil</article-title><source>Animal Behaviour</source><volume>23</volume><fpage>766</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(75)90104-9</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favaro</surname><given-names>L</given-names></name><name><surname>Neves</surname><given-names>S</given-names></name><name><surname>Furlati</surname><given-names>S</given-names></name><name><surname>Pessani</surname><given-names>D</given-names></name><name><surname>Martin</surname><given-names>V</given-names></name><name><surname>Janik</surname><given-names>VM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evidence suggests vocal production learning in a cross-fostered Risso’s dolphin (Grampus griseus)</article-title><source>Animal Cognition</source><volume>19</volume><fpage>847</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1007/s10071-016-0961-x</pub-id><pub-id pub-id-type="pmid">26874843</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fripp</surname><given-names>D</given-names></name><name><surname>Owen</surname><given-names>C</given-names></name><name><surname>Quintana-Rizzo</surname><given-names>E</given-names></name><name><surname>Shapiro</surname><given-names>A</given-names></name><name><surname>Buckstaff</surname><given-names>K</given-names></name><name><surname>Jankowski</surname><given-names>K</given-names></name><name><surname>Wells</surname><given-names>R</given-names></name><name><surname>Tyack</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Bottlenose dolphin (Tursiops truncatus) calves appear to model their signature whistles on the signature whistles of community members</article-title><source>Animal Cognition</source><volume>8</volume><fpage>17</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1007/s10071-004-0225-z</pub-id><pub-id pub-id-type="pmid">15221637</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furuyama</surname><given-names>T</given-names></name><name><surname>Shigeyama</surname><given-names>T</given-names></name><name><surname>Ono</surname><given-names>M</given-names></name><name><surname>Yamaki</surname><given-names>S</given-names></name><name><surname>Kobayasi</surname><given-names>KI</given-names></name><name><surname>Kato</surname><given-names>N</given-names></name><name><surname>Yamamoto</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Vocalization during agonistic encounter in Mongolian gerbils: Impact of sexual experience</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0272402</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0272402</pub-id><pub-id pub-id-type="pmid">35917294</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goffinet</surname><given-names>J</given-names></name><name><surname>Brudner</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title><source>eLife</source><volume>10</volume><elocation-id>e67855</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67855</pub-id><pub-id pub-id-type="pmid">33988503</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Development of species identification in ducklings: X. Perceptual specificity in the wood duck embryo requires sib stimulation for maintenance</article-title><source>Developmental Psychobiology</source><volume>16</volume><fpage>323</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1002/dev.420160407</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Social induction of malleability in ducklings: sensory basis and psychological mechanism</article-title><source>Animal Behaviour</source><volume>45</volume><fpage>707</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1006/anbe.1993.1085</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gretton</surname><given-names>A</given-names></name><name><surname>Borgwardt</surname><given-names>KM</given-names></name><name><surname>Rasch</surname><given-names>MJ</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A kernel two-sample test</article-title><source>The Journal of Machine Learning Research</source><volume>13</volume><fpage>723</fpage><lpage>773</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Warren</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What is an auditory object?</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>887</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1038/nrn1538</pub-id><pub-id pub-id-type="pmid">15496866</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimsley</surname><given-names>JMS</given-names></name><name><surname>Monaghan</surname><given-names>JJM</given-names></name><name><surname>Wenstrup</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Development of social vocalizations in mice</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e17460</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0017460</pub-id><pub-id pub-id-type="pmid">21408007</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gromov</surname><given-names>VS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Ecology and social behaviour of the Mongolian gerbil: a generalised review</article-title><source>Behaviour</source><volume>159</volume><fpage>403</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1163/1568539X-bja10128</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Happel</surname><given-names>MFK</given-names></name><name><surname>Niekisch</surname><given-names>H</given-names></name><name><surname>Castiblanco Rivera</surname><given-names>LL</given-names></name><name><surname>Ohl</surname><given-names>FW</given-names></name><name><surname>Deliano</surname><given-names>M</given-names></name><name><surname>Frischknecht</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Enhanced cognitive flexibility in reversal learning induced by removal of the extracellular matrix in auditory cortex</article-title><source>PNAS</source><volume>111</volume><fpage>2800</fpage><lpage>2805</lpage><pub-id pub-id-type="doi">10.1073/pnas.1310272111</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname><given-names>L</given-names></name><name><surname>Barbu</surname><given-names>S</given-names></name><name><surname>Lemasson</surname><given-names>A</given-names></name><name><surname>Hausberger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dialects in animals: evidence, development and potential functions</article-title><source>Animal Behavior and Cognition</source><volume>2</volume><fpage>132</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.12966/abc.05.03.2015</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Paisley</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stochastic variational inference</article-title><source>Journal of Machine Learning Research</source><volume>14</volume><fpage>1303</fpage><lpage>1347</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holman</surname><given-names>SD</given-names></name><name><surname>Seale</surname><given-names>WTC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Ontogeny of sexually dimorphic ultrasonic vocalizations in mongolian gerbils</article-title><source>Developmental Psychobiology</source><volume>24</volume><fpage>103</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1002/dev.420240204</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holman</surname><given-names>SD</given-names></name><name><surname>Seale</surname><given-names>WTC</given-names></name><name><surname>Hutchison</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Ultrasonic vocalizations in immature gerbils: emission rate and structural changes after neonatal exposure to androgen</article-title><source>Physiology &amp; Behavior</source><volume>57</volume><fpage>451</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(94)00237-Y</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurtado-Parrado</surname><given-names>C</given-names></name><name><surname>González-León</surname><given-names>C</given-names></name><name><surname>Arias-Higuera</surname><given-names>MA</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name><name><surname>Medina</surname><given-names>LG</given-names></name><name><surname>García-Muñoz</surname><given-names>L</given-names></name><name><surname>Sánchez</surname><given-names>C</given-names></name><name><surname>Cifuentes</surname><given-names>J</given-names></name><name><surname>Forigua</surname><given-names>JC</given-names></name><name><surname>Ortiz</surname><given-names>A</given-names></name><name><surname>Acevedo-Triana</surname><given-names>CA</given-names></name><name><surname>Rico</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Assessing Mongolian gerbil emotional behavior: effects of two shock intensities and response-independent shocks during an extended inhibitory-avoidance task</article-title><source>PeerJ</source><volume>5</volume><elocation-id>e4009</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.4009</pub-id><pub-id pub-id-type="pmid">29152417</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>G</given-names></name><name><surname>Ransome</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Echolocation calls of bats are influenced by maternal effects and change over a lifetime</article-title><source>Proceedings of the Royal Society of London. Series B</source><volume>252</volume><fpage>125</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1098/rspb.1993.0055</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jovanovic</surname><given-names>T</given-names></name><name><surname>Megna</surname><given-names>NL</given-names></name><name><surname>Maestripieri</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Early maternal recognition of offspring vocalizations in rhesus macaques (<italic>Macaca mulatta</italic>)</article-title><source>Primates; Journal of Primatology</source><volume>41</volume><fpage>421</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1007/BF02557653</pub-id><pub-id pub-id-type="pmid">30545206</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kleinberg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>An impossibility theorem for clustering</chapter-title><person-group person-group-type="editor"><name><surname>Ranzato</surname><given-names>M</given-names></name><name><surname>Beygelzimer</surname><given-names>A</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>NeurIPS</publisher-name><fpage>1</fpage><lpage>34</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayasi</surname><given-names>KI</given-names></name><name><surname>Riquimaroux</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Classification of vocalizations in the Mongolian gerbil, Meriones unguiculatus</article-title><source>The Journal of the Acoustical Society of America</source><volume>131</volume><fpage>1622</fpage><lpage>1631</lpage><pub-id pub-id-type="doi">10.1121/1.3672693</pub-id><pub-id pub-id-type="pmid">22352532</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Tsao</surname><given-names>FM</given-names></name><name><surname>Liu</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Foreign-language experience in infancy: Effects of short-term exposure and social interaction on phonetic learning</article-title><source>PNAS</source><volume>100</volume><fpage>9096</fpage><lpage>9101</lpage><pub-id pub-id-type="doi">10.1073/pnas.1532872100</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maler</surname><given-names>P</given-names></name><name><surname>Tamura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Culturally transmitted patterns of vocal behavior in sparrows</article-title><source>Science</source><volume>146</volume><fpage>1483</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.146.3650.1483</pub-id><pub-id pub-id-type="pmid">14208581</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Beron</surname><given-names>CC</given-names></name><name><surname>Neufeld</surname><given-names>SQ</given-names></name><name><surname>Robertson</surname><given-names>K</given-names></name><name><surname>Bhagat</surname><given-names>ND</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Peterson</surname><given-names>E</given-names></name><name><surname>Hyun</surname><given-names>M</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The striatum organizes 3D behavior via moment-to-moment action selection</article-title><source>Cell</source><volume>174</volume><fpage>44</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.04.019</pub-id><pub-id pub-id-type="pmid">29779950</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mascaro</surname><given-names>JS</given-names></name><name><surname>Rentscher</surname><given-names>KE</given-names></name><name><surname>Hackett</surname><given-names>PD</given-names></name><name><surname>Lori</surname><given-names>A</given-names></name><name><surname>Darcher</surname><given-names>A</given-names></name><name><surname>Rilling</surname><given-names>JK</given-names></name><name><surname>Mehl</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Preliminary evidence that androgen signaling is correlated with men’s everyday language</article-title><source>American Journal of Human Biology</source><volume>30</volume><elocation-id>e23136</elocation-id><pub-id pub-id-type="doi">10.1002/ajhb.23136</pub-id><pub-id pub-id-type="pmid">29752749</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>McFee</surname><given-names>B</given-names></name><name><surname>McVicar</surname><given-names>M</given-names></name><name><surname>Faronbi</surname><given-names>D</given-names></name><name><surname>Roman</surname><given-names>I</given-names></name><name><surname>Gover</surname><given-names>M</given-names></name><name><surname>Balke</surname><given-names>S</given-names></name><name><surname>Seyfarth</surname><given-names>S</given-names></name><name><surname>Malek</surname><given-names>A</given-names></name><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Lostanlen</surname><given-names>V</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Cwitkowitz</surname><given-names>F</given-names></name><name><surname>Zalkow</surname><given-names>F</given-names></name><name><surname>Nieto</surname><given-names>O</given-names></name><name><surname>Ellis</surname><given-names>D</given-names></name><name><surname>Mason</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Steers</surname><given-names>B</given-names></name><name><surname>Pimenta</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Librosa/librosa: 0.10.2.post1</data-title><version designator="Version 0.10.2.post1">Version 0.10.2.post1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.11192913">https://doi.org/10.5281/zenodo.11192913</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><name><surname>Gire</surname><given-names>D</given-names></name><name><surname>Hoke</surname><given-names>K</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Kelley</surname><given-names>D</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name><name><surname>Theunissen</surname><given-names>F</given-names></name><name><surname>Yartsev</surname><given-names>M</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Natural behavior is the language of the brain</article-title><source>Current Biology</source><volume>32</volume><fpage>R482</fpage><lpage>R493</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.03.031</pub-id><pub-id pub-id-type="pmid">35609550</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milne-Edwards</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1867">1867</year><article-title>Observations sur quelques mammiferes du nord de la Chine</article-title><source>Annales Des Science Naturelles</source><volume>7</volume><fpage>375</fpage><lpage>377</lpage></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myoga</surname><given-names>MH</given-names></name><name><surname>Lehnert</surname><given-names>S</given-names></name><name><surname>Leibold</surname><given-names>C</given-names></name><name><surname>Felmy</surname><given-names>F</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Glycinergic inhibition tunes coincidence detection in the auditory brainstem</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>3790</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms4790</pub-id><pub-id pub-id-type="pmid">24804642</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narula</surname><given-names>G</given-names></name><name><surname>Herbst</surname><given-names>JA</given-names></name><name><surname>Rychen</surname><given-names>J</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning auditory discriminations from observation is efficient but less robust than learning from experience</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3218</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05422-y</pub-id><pub-id pub-id-type="pmid">30104709</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><collab>National Instruments</collab></person-group><year iso-8601-date="2017">2017</year><data-title>Nidaqmx-python</data-title><version designator="0.5.7">0.5.7</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ni/nidaqmx-python">https://github.com/ni/nidaqmx-python</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neunuebel</surname><given-names>JP</given-names></name><name><surname>Taylor</surname><given-names>AL</given-names></name><name><surname>Arthur</surname><given-names>BJ</given-names></name><name><surname>Egnor</surname><given-names>SER</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Female mice ultrasonically interact with males during courtship displays</article-title><source>eLife</source><volume>4</volume><elocation-id>e06203</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06203</pub-id><pub-id pub-id-type="pmid">26020291</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>VocalPy: a core package for acoustic communication research in python</data-title><version designator="0.9.3">0.9.3</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7905426">https://doi.org/10.5281/zenodo.7905426</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishiyama</surname><given-names>K</given-names></name><name><surname>Kobayasi</surname><given-names>KI</given-names></name><name><surname>Riquimaroux</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Vocalization control in Mongolian gerbils (Meriones unguiculatus) during locomotion behavior</article-title><source>The Journal of the Acoustical Society of America</source><volume>130</volume><fpage>4148</fpage><lpage>4157</lpage><pub-id pub-id-type="doi">10.1121/1.3651815</pub-id><pub-id pub-id-type="pmid">22225069</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Lyamzin</surname><given-names>DR</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>State-dependent population coding in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>2058</fpage><lpage>2073</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3318-14.2015</pub-id><pub-id pub-id-type="pmid">25653363</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagel</surname><given-names>M</given-names></name><name><surname>Atkinson</surname><given-names>QD</given-names></name><name><surname>S. Calude</surname><given-names>A</given-names></name><name><surname>Meade</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultraconserved words point to deep language ancestry across Eurasia</article-title><source>PNAS</source><volume>110</volume><fpage>8471</fpage><lpage>8476</lpage><pub-id pub-id-type="doi">10.1073/pnas.1218726110</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paraouty</surname><given-names>N</given-names></name><name><surname>Rizzuto</surname><given-names>CR</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dopaminergic signaling supports auditory social learning</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>13117</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-92524-1</pub-id><pub-id pub-id-type="pmid">34162951</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penikis</surname><given-names>KB</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A redundant cortical code for speech envelope</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>93</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1616-21.2022</pub-id><pub-id pub-id-type="pmid">36379706</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Gerbil-vocal-dialects</data-title><version designator="swh:1:rev:e7f6d02be89d97dbaa44dad9b46e4accbad49006">swh:1:rev:e7f6d02be89d97dbaa44dad9b46e4accbad49006</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bb093292591a32933bb6600678cc6ee9db439ecc;origin=https://github.com/ralphpeterson/gerbil-vocal-dialects;visit=swh:1:snp:2406d9cf858599f0f9ad5504a95401e6d4537ecf;anchor=swh:1:rev:e7f6d02be89d97dbaa44dad9b46e4accbad49006">https://archive.softwareheritage.org/swh:1:dir:bb093292591a32933bb6600678cc6ee9db439ecc;origin=https://github.com/ralphpeterson/gerbil-vocal-dialects;visit=swh:1:snp:2406d9cf858599f0f9ad5504a95401e6d4537ecf;anchor=swh:1:rev:e7f6d02be89d97dbaa44dad9b46e4accbad49006</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietrewicz</surname><given-names>AT</given-names></name><name><surname>Hoff</surname><given-names>MP</given-names></name><name><surname>Higgins</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Activity rhythms in the Mongolian gerbil under natural light conditions</article-title><source>Physiology &amp; Behavior</source><volume>29</volume><fpage>377</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(82)90029-4</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>BJ</given-names></name><name><surname>Harcourt</surname><given-names>RG</given-names></name><name><surname>Charrier</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The memory remains: long-term vocal recognition in Australian sea lions</article-title><source>Animal Cognition</source><volume>13</volume><fpage>771</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1007/s10071-010-0322-0</pub-id><pub-id pub-id-type="pmid">20446102</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>MC</given-names></name><name><surname>Styr</surname><given-names>B</given-names></name><name><surname>Schmid</surname><given-names>TA</given-names></name><name><surname>Elie</surname><given-names>JE</given-names></name><name><surname>Yartsev</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cortical representation of group social communication in bats</article-title><source>Science</source><volume>374</volume><elocation-id>eaba9584</elocation-id><pub-id pub-id-type="doi">10.1126/science.aba9584</pub-id><pub-id pub-id-type="pmid">34672724</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rübsamen</surname><given-names>R</given-names></name><name><surname>Ter-Mikaelian</surname><given-names>M</given-names></name><name><surname>Yapa</surname><given-names>WB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Vocal behavior of the Mongolian gerbil in a seminatural enclosure</article-title><source>Behaviour</source><volume>149</volume><fpage>461</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1163/156853912X639778</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Hearing sensitivity of the mongolian gerbil, Meriones unguiculatis</article-title><source>The Journal of the Acoustical Society of America</source><volume>59</volume><fpage>1222</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1121/1.380961</pub-id><pub-id pub-id-type="pmid">956517</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>Thielk</surname><given-names>M</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008228</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008228</pub-id><pub-id pub-id-type="pmid">33057332</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Toward a computational neuroethology of vocal communication: from bioacoustics to neurophysiology, emerging tools and future directions</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>15</volume><elocation-id>811737</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2021.811737</pub-id><pub-id pub-id-type="pmid">34987365</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saldeitis</surname><given-names>K</given-names></name><name><surname>Jeschke</surname><given-names>M</given-names></name><name><surname>Michalek</surname><given-names>A</given-names></name><name><surname>Henschke</surname><given-names>JU</given-names></name><name><surname>Wetzel</surname><given-names>W</given-names></name><name><surname>Ohl</surname><given-names>FW</given-names></name><name><surname>Budinger</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Selective interruption of auditory interhemispheric cross talk impairs discrimination learning of frequency-modulated tone direction but not gap detection and discrimination</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>2025</fpage><lpage>2038</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0216-21.2022</pub-id><pub-id pub-id-type="pmid">35064004</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarro</surname><given-names>EC</given-names></name><name><surname>von Trapp</surname><given-names>G</given-names></name><name><surname>Mowery</surname><given-names>TM</given-names></name><name><surname>Kotak</surname><given-names>VC</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical synaptic inhibition declines during auditory learning</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>6318</fpage><lpage>6325</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4051-14.2015</pub-id><pub-id pub-id-type="pmid">25904785</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheibler</surname><given-names>E</given-names></name><name><surname>Weinandy</surname><given-names>R</given-names></name><name><surname>Gattermann</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Social categories in families of Mongolian gerbils</article-title><source>Physiology &amp; Behavior</source><volume>81</volume><fpage>455</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2004.02.011</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheibler</surname><given-names>E</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Weinandy</surname><given-names>R</given-names></name><name><surname>Gattermann</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Burrow systems of the Mongolian gerbil (meriones unguiculatus milne edwards, 1867)</article-title><source>Mammalian Biology</source><volume>71</volume><fpage>178</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.mambio.2005.11.007</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindler</surname><given-names>D</given-names></name><name><surname>Spors</surname><given-names>S</given-names></name><name><surname>Demiray</surname><given-names>B</given-names></name><name><surname>Krüger</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Automatic behavior assessment from uncontrolled everyday audio recordings by deep learning</article-title><source>Sensors</source><volume>22</volume><elocation-id>8617</elocation-id><pub-id pub-id-type="doi">10.3390/s22228617</pub-id><pub-id pub-id-type="pmid">36433214</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A mathematical theory of communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shemesh</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A paradigm shift in translational psychiatry through rodent neuroethology</article-title><source>Molecular Psychiatry</source><volume>28</volume><fpage>993</fpage><lpage>1003</lpage><pub-id pub-id-type="doi">10.1038/s41380-022-01913-z</pub-id><pub-id pub-id-type="pmid">36635579</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shizawa</surname><given-names>Y</given-names></name><name><surname>Nakamichi</surname><given-names>M</given-names></name><name><surname>Hinobayashi</surname><given-names>T</given-names></name><name><surname>Minami</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Playback experiment to test maternal responses of Japanese macaques (<italic>Macaca fuscata</italic>) to their own infant’s call when the infants were four to six months old</article-title><source>Behavioural Processes</source><volume>68</volume><fpage>41</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.beproc.2004.10.002</pub-id><pub-id pub-id-type="pmid">15639384</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silberstein</surname><given-names>Y</given-names></name><name><surname>Felmy</surname><given-names>F</given-names></name><name><surname>Scheumann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Encoding of arousal and physical characteristics in audible and ultrasonic vocalizations of mongolian gerbil pups testing common rules for mammals</article-title><source>Animals</source><volume>13</volume><elocation-id>2553</elocation-id><pub-id pub-id-type="doi">10.3390/ani13162553</pub-id><pub-id pub-id-type="pmid">37627344</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szenczi</surname><given-names>P</given-names></name><name><surname>Bánszegi</surname><given-names>O</given-names></name><name><surname>Urrutia</surname><given-names>A</given-names></name><name><surname>Faragó</surname><given-names>T</given-names></name><name><surname>Hudson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mother-offspring recognition in the domestic cat: Kittens recognize their own mother’s call</article-title><source>Developmental Psychobiology</source><volume>58</volume><fpage>568</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1002/dev.21402</pub-id><pub-id pub-id-type="pmid">26935009</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A mesocortical dopamine circuit enables the cultural transmission of vocal behaviour</article-title><source>Nature</source><volume>563</volume><fpage>117</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0636-7</pub-id><pub-id pub-id-type="pmid">30333629</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Trapp</surname><given-names>G</given-names></name><name><surname>Buran</surname><given-names>BN</given-names></name><name><surname>Sen</surname><given-names>K</given-names></name><name><surname>Semple</surname><given-names>MN</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A decline in response variability improves neural signal detection during auditory task performance</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>11097</fpage><lpage>11106</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1302-16.2016</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>JM</given-names></name><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Abraira</surname><given-names>VE</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping sub-second structure in mouse behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>JD</given-names></name><name><surname>Gimoto</surname><given-names>J</given-names></name><name><surname>Constantinople</surname><given-names>CM</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parietal cortex is required for the integration of acoustic evidence</article-title><source>Current Biology</source><volume>30</volume><fpage>3293</fpage><lpage>3303</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.06.017</pub-id><pub-id pub-id-type="pmid">32619478</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>JD</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Temporal encoding is required for categorization, but not discrimination</article-title><source>Cerebral Cortex</source><volume>31</volume><fpage>2886</fpage><lpage>2897</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhaa396</pub-id><pub-id pub-id-type="pmid">33429423</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaytseva</surname><given-names>AS</given-names></name><name><surname>Volodin</surname><given-names>IA</given-names></name><name><surname>Ilchenko</surname><given-names>OG</given-names></name><name><surname>Volodina</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ultrasonic vocalization of pup and adult fat-tailed gerbils (Pachyuromys duprasi)</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0219749</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0219749</pub-id><pub-id pub-id-type="pmid">31356642</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaytseva</surname><given-names>AS</given-names></name><name><surname>Volodin</surname><given-names>IA</given-names></name><name><surname>Ilchenko</surname><given-names>OG</given-names></name><name><surname>Volodina</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Audible calls and their ontogenetic relationship with ultrasonic vocalization in a rodent with a wide vocal range, the fat-tailed gerbil (Pachyuromys duprasi)</article-title><source>Behavioural Processes</source><volume>180</volume><elocation-id>104241</elocation-id><pub-id pub-id-type="doi">10.1016/j.beproc.2020.104241</pub-id><pub-id pub-id-type="pmid">32971224</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zorio</surname><given-names>DAR</given-names></name><name><surname>Monsma</surname><given-names>S</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name><name><surname>Golding</surname><given-names>NL</given-names></name><name><surname>Rubel</surname><given-names>EW</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>De novo sequencing and initial annotation of the Mongolian gerbil (Meriones unguiculatus) genome</article-title><source>Genomics</source><volume>111</volume><fpage>441</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.ygeno.2018.03.001</pub-id><pub-id pub-id-type="pmid">29526484</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89892.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Takahashi</surname><given-names>Daniel Y</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Federal University of Rio Grande do Norte</institution><country>Brazil</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study provides an experimental paradigm and state-of-the-art analysis method for studying the existence of call types and transition differences among Mongolian gerbil families in a naturalistic environment. The analyses are <bold>convincing</bold>, with a thorough treatment of the acoustic data and a demonstration of the robustness of the observed effect across days. The work will likely be of interest to the auditory neuroscience and neuroethology communities.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89892.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This research offers an in-depth exploration and quantification of social vocalization within three families of Mongolian gerbils. In an enlarged, semi-natural environment, the study continuously monitored two parent gerbils and their four pups from P14 to P34. Through dimensionality reduction and clustering, a diverse range of gerbil call types was identified. Interestingly, distinct sets of vocalizations were used by different families in their daily interactions, with unique transition structures exhibited across these families. The primary results of this study are compelling, although some elements could benefit from clarification</p><p>Strengths:</p><p>Three elements of this study warrant emphasis. Firstly, it bridges the gap between laboratory and natural environments. This approach offers the opportunity to examine natural social behavior within a controlled setting (such as specified family composition, diet, and life stages), maintaining the social relevance of the behavior. Secondly, it seeks to understand short-timescale behaviors, like vocalizations, within the broader context of daily and life-stage timescales. Lastly, the use of unsupervised learning precludes the injection of human bias, such as pre-defined call categories, allowing the discovery of the diversity of vocal outputs.</p><p>Comments on the revised version:</p><p>(1) The authors have clarified the possible types of differences in the vocalizations of different families and discussed the potential contribution of the adult-pup difference.</p><p>(2) The authors have added the analysis in Figure 4 about the developmental changes in call types.</p><p>(3) The authors have analyzed the additional information in the 2-gram structure of the calls as evidence to apply the transition matrices to compare the families.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89892.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Peterson et al., perform a series of behavioral experiments to study the repertoire and variance of Mongolian gerbil vocalizations across social groups (families). A key strength of the study is the use of a behavioral paradigm which allows for long term audio recordings under naturalistic conditions. This new experimental set-up results in the identification of additional vocalization types, not previously described the literature. In combination with state-of-the-art methods for vocalization analysis, the authors demonstrate that the distribution of sound types and the transitions between these sound types across three gerbil families is different. This is a highly compelling finding which suggests that individual families may develop distinct vocal repertories. One potential limitation of the study lies in the cluster analysis used for identifying distinct vocalization types. The authors use a Gaussian Mixed Model (GMM) trained on variational auto Encoder derived latent representation of vocalizations to classify recorded sounds into clusters. Through the analysis the authors identify 70 distinct clusters and demonstrate a differential usage of these sound clusters across families. While the authors acknowledge the inherent challenges in cluster analysis and provide additional analyses (i.e. maximum mean discrepancy, MMD), additional analysis would increase the strength of the conclusions. In particular, analysis with different cluster sizes would be valuable. An additional limitation of the study is that due to the methodology that is used, the authors can not provide any information about the bioacoustic features that contribute to differences in sound types across families which limits interpretations about how the animals may perceive and react to these sounds in an ethologically relevant manner.</p><p>The conclusions of this paper are well supported by data.</p><p>• Can the authors comment on the potential biological significance of the 70 sound clusters? Does each cluster represent a single sound type? How many vocal clusters can be attributed to a single individual? Similarly, can the authors comment on the intra-individual and inter-individual variability of the sound types within and across families?</p><p>• As a main conclusion of the paper rests on the different distribution of sound clusters across families, it is important to validate the robustness of these differences across different cluster parameters. Specifically, the authors state that &quot;we selected 70 clusters as the most parsimonious fit&quot;. Could the authors provide more details about how this was fit? Specifically, could the authors expand upon what is meant by &quot;prior domain knowledge about the number of vocal types...&quot;. If the authors chose a range of cluster values (i.e. 10, 30, 50, 90) does the significance of the results still hold?</p><p>• While VAEs are powerful tools for analyzing complex datasets in this case they are restricted to analysis of spectrogram images. Have the authors identified any acoustic differences (i.e. in pitch, frequency, other sound components) across families?</p><p>Following a revision of the manuscript the authors have taken many of these points under consideration and as a result have significantly improved the manuscript. Critically, they have now provided additional quantification that differences across family repertories are robust against cluster selection size.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89892.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this study, Peterson et al. longitudinally record and document the vocal repertoires of three Mongolian gerbil families. Using unsupervised learning techniques, they map the variability across these groups, finding that while overall statistics of, e.g., vocal emission rates and bout lengths are similar, families differed markedly in their distributions of syllable types and the transitions between these types within bouts. In addition, the large and rich data are likely to be valuable to others in the field.</p><p>Strengths:</p><p>- Extensive data collection across multiple days in multiple family groups.</p><p>- Thoughtful application of modern analysis techniques for analyzing vocal repertoires.</p><p>- Careful examination of the statistical structure of vocal behavior, with indications that these gerbils, like naked mole rats, may differ in repertoire across families.</p><p>- Estimation of the stability of the effects across days.</p><p>Weaknesses:</p><p>- The work is largely descriptive, documenting behavior rather than testing a specific hypothesis.</p><p>- The number of families (N=3) is somewhat limited, though the authors have taken some care to examine the robustness of the findings.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89892.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peterson</surname><given-names>Ralph</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Choudhri</surname><given-names>Aman</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Mitelut</surname><given-names>Catalin</given-names></name><role specific-use="author">Author</role><aff><institution>University of British Columbia</institution><addr-line><named-content content-type="city">Vancouver</named-content></addr-line><country>Canada</country></aff></contrib><contrib contrib-type="author"><name><surname>Tanelus</surname><given-names>Aramis</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Capo-Battaglia</surname><given-names>Athena</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Williams</surname><given-names>Alex H</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Schneider</surname><given-names>David</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Sanes</surname><given-names>Dan H</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>This research offers an in-depth exploration and quantification of social vocalization within three families of Mongolian gerbils. In an enlarged, semi-natural environment, the study continuously monitored two parent gerbils and their four pups from P14 to P34. Through dimensionality reduction and clustering, a diverse range of gerbil call types was identified. Interestingly, distinct sets of vocalizations were used by different families in their daily interactions, with unique transition structures exhibited across these families. The primary results of this study are compelling, although some elements could benefit from clarification</p><p>Strengths:</p><p>Three elements of this study warrant emphasis. Firstly, it bridges the gap between laboratory and natural environments. This approach offers the opportunity to examine natural social behavior within a controlled setting (such as specified family composition, diet, and life stages), maintaining the social relevance of the behavior. Secondly, it seeks to understand short-timescale behaviors, like vocalizations, within the broader context of daily and life-stage timescales. Lastly, the use of unsupervised learning precludes the injection of human bias, such as pre-defined call categories, allowing the discovery of the diversity of vocal outputs.</p><p>Weaknesses:</p><p>(1) While the notable differences in vocal clusters across families are convincing, the drivers of these differences remain unclear. Are they attributable to &quot;dialect,&quot; call usage, or specific vocalizing individuals (e.g., adults vs. pups)? Further investigation, via a literature review or additional observation, into acoustic differences between adult and pup calls is recommended. Moreover, a consistent post-weaning decrease in the bottom-left cluster (Fig. S3) invites interpretation: could this reflect drops in pup vocalization?</p></disp-quote><p>Thank you for bringing up this point of clarification. Without knowledge of individual vocalizers, we are unable to rigorously assess pronunciation differences between individuals, however we can get a clear proxy for dialect through observing usage differences between families. We’ve added the following text (blue) in the Discussion to help clarify:</p><p>“To address whether gerbils also exhibit family specific vocal features, we compared GMM-labeled vocal cluster usages across the three recorded families and showed differences in vocal type usage (Figure 3). The differences in this study align with the definition of human vocal dialect, which is a regional or social variety of language that can differ in pronunciation, grammatical, semantic and/or language use differences (Henry et al., 2015). This definition of dialect is inclusive of both pronunciation differences (e.g. a Bostonian’s characteristic pronunciation of “car” as “cah”) and usage differences (e.g. a Bostonian’s preferential usage of the words “Go Red Sox” vs. a New Yorker’s preferential usage of the words “Go Yankees”). In our case, vocal clusters can be rarely observed in some families yet highly over-expressed in others (e.g. analogous to language usage differences in humans), or highly expressed in both families, but contain subtle spectrotemporal variations (Figure 3D, Family 1 cluster 11 vs. Family 3 clusters 2, 18, 30; e.g. analogous to pronunciation differences in humans).”</p><p>Indeed, our recordings obtained after pup removal could suggest that adults may use fewer low frequency calls (bottom left cluster in UMAP). However, this dataset does not permit a proper assessment of post-weaning pup calls. In fact, our results and the literature shows that adults are likely to use low frequency calls, but only during social interactions with pups or other adults. For example, Furuyama et al. 2022 describe a number of low frequency call types used by adults in agonistic social interactions, which look similar to a low frequency call type used by pups described in Silberstein et al. 2023. Similarly, Ter-Mikaelian et al. 2012 (their Figure 6) recorded several types of sonic vocalizations during adult social interaction. To our knowledge, it has not been shown whether gerbil pups and adults produce distinct call types. It is a challenging problem to solve, as animals placed in isolation (i.e. an experimental condition for which the identity of the vocalizer is known) vocalize infrequently and of the limited number they might emit, they do not use the full range of vocalizations described in the literature (RP personal observations). To properly address this question, one would need to elicit full use of the vocal repertoire through free social interaction, then attribute calls to individual vocalizers via sound source localization and/or head-mounted microphones — we are currently pursuing both of these technical challenges, but this is outside the scope of this manuscript.</p><p>Although the literature reflects the limitations discussed above, we have added a brief paragraph to the Discussion (limitations section) that addresses the reviewer’s question about the development of vocalizations:</p><p>“Although we were not able to attribute vocalizations to individual family members, we did seek to determine the importance of family structure by comparing audio recordings before and after removal of the pups at P30. The results show a clear effect of family integrity, and the sudden reduction of sonic calls following pup removal (Figure S3) could suggest that these vocalizations are produced selectively by pups.</p><p>However, there is ample evidence that adult gerbils also produce sonic vocalizations. For example, a number of low frequency call types are used by adults during a range of social interactions (Ter-Mikaelian et al., 2012; Furuyama et al., 2022), some of which are similar to a low frequency call type used by pups (Silberstein et al., 2023). Vocalization patterns of developing gerbils depend on isolation or staged interactions. Thus, when gerbil pups are recorded during isolation, ultrasonic vocalization rate declines and sonic vocalizations increase for animals that are in a high arousal state (De Ghett 1974, Silberstein et al., 2023). As gerbils progress from juvenile to adolescent development (P17-55) a significant increase in ultrasonic vocalization rate is observed during dyadic social encounters, with a distinct change in usage pattern that depends upon the sex of each animal (Holman &amp; Seale 1991, Holman et al. 1995). The development of vocalization types has been assessed in another member of the Gerbillinae subfamily, called fat-tailed gerbils (Pachyuromys duprasi), during isolation and handling. Here, the number of ultrasonic vocalization syllable types increase from neonatal to adult animals (Zaytseva et al. 2019), while some very low frequency sonic call types were rarely observed after P20 (Zaytseva et al. 2020). By comparison, mouse syllable usage changes during development, but pups produced 10 of the 11 syllable types produced by adults (Grimsley et al. 2011). In summary, our understanding of the maturation of vocalization usage remains limited by our inability to obtain longitudinal data from individual animals within their natural social setting. For example, when recorded in their natural environment, chimpanzees display a prolonged maturation of vocalization complexity, such as the probability of a unique utterance in a sequence, with the greatest changes occuring when animals begin to experience non-kin social interactions (Bortolato et al. 2023).”</p><disp-quote content-type="editor-comment"><p>(2) Developmental progression, particularly during pre-weaning periods when pup vocal output remains unstable, might be another factor influencing cross-family vocal differences. Representing data from this non-stationary process as an overall density map could result in the loss of time-dependent information. For instance, were dominating call types consistently present throughout the recording period, or were they prominent only at specific times? Displaying the evolution of the density map would enhance understanding of this aspect.</p></disp-quote><p>This is a great suggestion. Thank you for bringing it up. To address this, we have added an additional figure (Figure 4) to the main text (Note that the former Figure 4 is now Figure 5). New text associated with this new figure was added to the Results and Discussion sections:</p><p>Results</p><p>“Vocal usage differences remain stable across days of development It is possible that the observed vocal usage differences could result from varying developmental progression of vocal behavior or overexpression of certain vocal types during specific periods within the recording. To assess the potential effect of daily variation on family specific vocal usage, we visualized density maps of vocal usage across days for each of the families (Figure 4A). There are two noteworthy trends: (1.) the density map remains coarsely stable across days (rows) and (2.) the maps look distinct across families on any given day (columns). This is a qualitative approximation for the repertoire’s stability, but does not take into account variation of call type usage (as defined by GMM clustering of the latent space). Figure 4B, shows the normalized usage of each cluster type over development for each family. Cluster usages during the period of “full family, shared recording days” (postnatal days beneath the purple bars) are stable across days within families – as is apparent by the horizontal striations in the plot – though each family maintains this stability through using a unique set of call types. This is addressed empirically in Figure 4C, which shows clearly separable PCA projections of the cluster usages shown in Figure 4B (purple days). Finally, we computed the pairwise Mean Max Discrepancy (MMD) between latent distributions of vocalizations from individual recording days for each of the families (Figure 4D). This shows that across-family repertoire differences are substantially larger than within-family differences. This is visualized in a multidimensional scaling projection of the MMD matrix in Figure 4E.”</p><p>Discussion</p><p>“The described family differences collapse data from multiple days into a single comparison, however it’s possible that factors such as vocal development and/or high usage of particular vocal types during specific periods of the recording could explain family differences. Therefore, we took advantage of the longitudinal nature of our dataset to assess whether repertoire differences remain stable across time. First, we visualized vocal repertoire usage across days as either UMAP probability density maps (Figure 4A) or daily GMM cluster usages (Figure 4B). Though qualitative, one can appreciate that family repertoire usage remains stable across days and appears to differ on a consistent daily basis across families. To formally quantify this, we first projected GMM cluster usages from Figure 4B into PC space and show that family GMM cluster usage patterns are highly separable, regardless of postnatal day (Figure 4C). If families had used a more overlapping set of call types, then the projections would have appeared intermixed. Next, we performed a cluster-free analysis by computing the pairwise MMD distance between VAE latent distributions of vocalizations from each family and day (Figure 4D). This analysis shows very low MMD values across days within a family (i.e. the repertoire is highly consistent with itself), and high MMD values across families/days (greater than would be expected by chance; see shuffle control in Figure S2D). The relative differences in this matrix are made clear in Figure 4E, which provides additional evidence that family vocal repertoires remain stable across days and are consistently different from other families. Taken together, we believe that this is compelling evidence that differences in vocal repertoires between families are not driven by dominating call types during specific phases in the recording period; rather, families consistently emit characteristic sets of call types across days. This opens up the possibility to assess repertoire differences over much shorter time periods (e.g. 24 hours) in future studies.”</p><disp-quote content-type="editor-comment"><p>(3) Family-specific vocalizations were credited to the transition structure, a finding that may seem obvious if the 1-gram (i.e., the proportion of call types) already differs. This result lacks depth unless it can be demonstrated that, firstly, the transition matrix provides a robust description of the data, and secondly, different families arrange the same set of syllables into unique sequences.</p></disp-quote><p>Thank you for these important suggestions. We agree that it is true that the 2-gram transition structure must vary based on the 1-gram structure. To determine whether this influences the interpretation of the finding, we have added Figure S5 and the following text in the Results section:</p><p>“To determine whether differences in 1-gram structure contribute to differences in the transition (2-gram) structure, we performed a number of controls. Although subtle, vertical streaks are clearly present in shuffled transition matrices that correspond to 1-gram usages (Figure S5A-B). Given the shuffled data structure, we sought to determine whether the observed transition probabilities differed significantly from chance levels. We randomly shuffled label sequences 1000 times independently for each family to generate a null transition matrix distribution. Using these null distributions and the observed transition probabilities, we computed a p-value for each transition using a one-sample t-test and created a binary transition matrix indicating which transitions happen above chance levels (Figure S5C, black pixels, p &lt;= 0.05 after post hoc Benjamini-Hochberg multiple comparisons correction). As is made clear in Figure S5C, most transitions for each family occur significantly above chance levels, despite the inherent 1-gram structure. Moreover, by looking at transitions from a highly usage cluster type used roughly the same proportion across families (cluster 12), we show that families arrange the same sets of vocal clusters into unique sequences (Figure S5D). We believe that this provides compelling evidence that the 1-gram structure does not change the interpretation of the main claim that transition structure varies by family. “””</p><p>To address your second point, we inspected frequent transitions from individual syllables to all other syllables using bigram transition probability graphs. This revealed a common trend that across all families, many shared and unshared transitions existed, suggesting that families use the same sets of syllables to make unique transition patterns. Figure S5D shows a single syllable example of the phenomenon, with red lines indicating the shared transition types between families and black showing transition patterns not shared between families (i.e. unique family-specific transitions, or lack thereof).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Peterson et al., perform a series of behavioral experiments to study the repertoire and variance of Mongolian gerbil vocalizations across social groups (families). A key strength of the study is the use of a behavioral paradigm which allows for long term audio recordings under naturalistic conditions. This experimental set-up results in the identification of additional vocalization types. In combination with state of the art methods for vocalization analysis, the authors demonstrate that the distribution of sound types and the transitions between these sound types across three gerbil families is different. This is a highly compelling finding which suggests that individual families may develop distinct vocal repertoires. One potential limitation of the study lies in the cluster analysis used for identifying distinct vocalization types. The authors use a Gaussian Mixed Model (GMM) trained on variational auto Encoder derived latent representation of vocalizations to classify recorded sounds into clusters. Through the analysis the authors identify 70 distinct clusters and demonstrate a differential usage of these sound clusters across families. While the authors acknowledge the inherent challenges in cluster analysis and provide additional analyses (i.e. maximum mean discrepancy, MMD), additional analysis would increase the strength of the conclusions. In particular, analysis with different cluster sizes would be valuable. An additional limitation of the study is that due to the methodology that is used, the authors can not provide any information about the bioacoustic features that contribute to differences in sound types across families which limits interpretations about how the animals may perceive and react to these sounds in an ethologically relevant manner.</p><p>The conclusions of this paper are well supported by data, but certain parts of the data analysis should be expanded and more fully explained.</p><p>• Can the authors comment on the potential biological significance of the 70 sound clusters? Does each cluster represent a single sound type? How many vocal clusters can be attributed to a single individual? Similarly, can the authors comment on the intra-individual and inter-individual variability of the sound types within and across families?</p></disp-quote><p>Previous work documenting the Mongolian gerbil repertoire (Ter-Mikaelian 2012, Kobayasi 2012) has revealed ~12 vocalization types that vary with social context. Our thinking is that we are capturing these ~12 (plus a few more, as illustrated in Figure 2C) as well as individual or family-specific variations of some call types. Although the number of discrete call types is likely less than 70, it’s plausible that variation due to vocalizer identity pushes some calls into unique clusters. This idea is supported by the fact that both naked mole rats and Mongolian gerbils have been shown to exhibit individual-specific variation in vocalizations, though only in single call types (Barker 2021, Figure 1; Nishiyama 2011, Table I). The current study is not ideal to test this prediction, as we cannot attribute each vocalization to individual family members. Using our 4-mic array, we attempted to apply established sound source localization techniques to assign vocalizations to individuals (Neunuebel 2015), but the technique failed, presumably due to high amounts of reverberation in the arena. We are currently developing a custom deep learning based sound localization algorithm, and had hoped to extract individual animal vocalizations from our data set (part of the reason why this manuscript has taken longer than expected to return!), but the performance is not yet satisfactory for large groups of animals. We have added text to the Methods sections with the context outlined above to further justify the use of ~70 clusters.</p><disp-quote content-type="editor-comment"><p>• As a main conclusion of the paper rests on the different distribution of sound clusters across families, it is important to validate the robustness of these differences across different cluster parameters. Specifically, the authors state that &quot;we selected 70 clusters as the most parsimonious fit&quot;. Could the authors provide more details about how this was fit? Specifically, could the authors expand upon what is meant by &quot;prior domain knowledge about the number of vocal types...&quot;. If the authors chose a range of cluster values (i.e. 10, 30, 50, 90) does the significance of the results still hold?</p></disp-quote><p>Thank you for the suggestion, this is an important point that we have addressed with new analyses in the revision (see GMM clustering methods and new Figure S4). The prior domain knowledge referenced is with respect to the information known about the Mongolian gerbil vocal types provided in the response above. We have made this more clear in the discussion.</p><p>We mainly based our selection of the number of clusters using the elbow method on GMM held-out log likelihood (Figure S2C). Around 70 clusters is when the likelihood begins to plateau, though it’s clear that there are a number of reasonable cluster sizes. To assess whether cluster size has an effect on interpretation of the family differences result, we added Figure S5, where we varied the number of GMM clusters used and compared cluster usage differences across families (Figure S4A). We quantified pairwise family differences in cluster usage by computing the sum of the absolute value of differential cluster usages, for each GMM cluster value (Figure S4B). We find that relative usage differences remain unchanged across the range of cluster values used, indicating that GMM cluster size does bias the finding.</p><disp-quote content-type="editor-comment"><p>• While VAEs are powerful tools for analyzing complex datasets in this case they are restricted to analysis of spectrogram images. Have the authors identified any acoustic differences (i.e. in pitch, frequency, and other sound components) across families?</p></disp-quote><p>Though it’s true that this VAE is limited to spectrograms, the VAE latent space has been shown to correspond to real acoustic features such as frequency and duration, and contain a higher representational capacity than traditional acoustic features (Goffinet 2021, Figure 2). Therefore, clustering of the latent space necessarily means that vocalizations with similar acoustic features are clustered together regardless of their family identity.</p><p>Despite this, your point is well taken that there could be systematic differences in certain acoustic features for specific call types. We are not able to ascertain this with the current dataset. This is addressed in Barker 2021 by recording a single call type (soft chirp) from individuals within and across families. Mongolian gerbils have been shown to exhibit individual differences in the initial, terminal, minimum, and maximum frequency of the ultrasonic up-frequency modulated call type (Figure 2, top right green; Nishiyama 2011, Figure 1A). Therefore it’s possible that family-specific differences exist for that particular call type. To assess whether other call types show family or individual differences, it’s necessary to either (1) elicit all call types from an animal in isolation or (2) determine vocalizer identity in social-vocal interactions. The problem with the former idea is that gerbils only produce up-frequency modulated USVs in isolation and there is no known way to elicit the full vocal repertoire in single animals. The latter idea would allow for full use of the vocal repertoire, but requires invasive techniques (e.g., skull-implanted microphones, or awake-behaving laryngeal nerve recordings) that permit assignment of vocalizations to individuals during a natural social interaction. We are actively exploring solutions to both problems.</p><p>It’s likely that future studies will look deeper into acoustic differences between individuals and families. Therefore, we have added acoustic feature quantification of vocalizations in each of the GMM clusters as a reference (Figure S6).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>In this study, Peterson et al. longitudinally record and document the vocal repertoires of three Mongolian gerbil families. Using unsupervised learning techniques, they map the variability across these groups, finding that while overall statistics of, e.g., vocal emission rates and bout lengths are similar, families differed markedly in their distributions of syllable types and the transitions between these types within bouts. In addition, the large and rich data are likely to be valuable to others in the field.</p><p>Strengths:</p><p>- Extensive data collection across multiple days in multiple family groups.</p><p>- Thoughtful application of modern analysis techniques for analyzing vocal repertoires. - Careful examination of the statistical structure of vocal behavior, with indications that these gerbils, like naked mole rats, may differ in repertoire across families.</p><p>Weaknesses:</p><p>- The work is largely descriptive, documenting behavior rather than testing a specific hypothesis.</p><p>- The number of families (N=3) is somewhat limited.</p></disp-quote><p>We agree that the number of families is relatively small. However, our new analysis of vocal repertoire by postnatal day (Figure 4) demonstrates that the finding is quite robust. A high sample-size study was outside the scope of this initial observational study given the difficulty of obtaining and processing longitudinal data of this scale. In light of new analyses in Figure 4, we are confident that future studies will not need so much data to characterize family-specific differences. A single 24-hour recording should be sufficient, making comparison of many more families relatively straightforward.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Several minor concerns:</p><p>(1) The three thresholds used for vocalization segmentation lack explanation.</p><p>Figure 1C's first vocal event appears to define the first gap via the gray threshold (th_2, as the trace does not cross the black line) and the second gap via the black threshold (th_1 or th_3). And this is not addressed in the Methods section.</p></disp-quote><p>Thank you for bringing this to our attention. We agree, this is presented in an unnecessarily complicated way. We have updated the methods section describing the thresholding procedure.</p><p>“Sound onsets are detected when the amplitude exceeds 'th_3' (black dashed line, Figure 1C), and sound offset occurs when there is a subsequent local minimum e.g., amplitude less than 'th_2' (gray dashed line, Figure 1C), or 'th_1' (black dashed line, Figure 1C), whichever comes first. In this specific use case, th_2 (5) will always come before th_1 (2), therefore the gray dashed line will always be the offset. A subsequent onset will be marked if the sound amplitude crosses th_2 or th_3, whichever comes first. For example, the first sound event detected in Figure 1C shows the sound amplitude rising above the black dashed line (th_3) and marks an onset. Subsequently, the amplitude trace falls below the gray dashed line (th_2) and an offset is marked. Finally, the amplitude rises above th_2 without dipping below th_3 and an onset for a new sound event is marked. Had the amplitude dipped below th_3, a new sound event onset would be marked when the amplitude trace subsequently exceeded th_3 (e.g. between sound event 2 and 3, Figure 1C). The maximum and minimum syllable durations were selected based on published duration ranges of gerbil vocalizations (Ter-Mikaelian et al. 2012, Kobayasi &amp; Riquimaroux, 2012).”</p><disp-quote content-type="editor-comment"><p>(2) The determination of multi-syllabic calls could be explained further. In Figure 1C, for instance, do syllables separated by short gaps (e.g., the first syllable and the rest of the first group, and the third group in this example) belong to the same call or different calls?</p></disp-quote><p>We have added an operational definition of mono vs. multisyllabic calls in the Results section:</p><p>“Vocalizations occur as either single syllables bounded by silence (monosyllabic) or consist of combinations of single syllables without a silent interval (multisyllabic).”</p><p>Under this definition, the examples you mentioned in Figure 1C are considered monosyllabic. One could reasonably expand the definition to include calls separated by less than X ms of silence for example, however we choose not to do that in this study. A deeper understanding of the phonation mechanisms for different gerbil vocalization types would be helpful to more rigorously determine the distinction between mono vs. multisyllabic vocalizations.</p><disp-quote content-type="editor-comment"><p>(3) Labeling the calls shown in Fig. 3D in the latent feature space would help highlight within-family diversity and between-family similarities.</p></disp-quote><p>Great suggestion. We have updated Figure 3 to include where in UMAP space each family’s preferred clusters are.</p><disp-quote content-type="editor-comment"><p>(4) In the introduction, the statement, &quot;Therefore, our study considers the possibility that there is a diversity of vocalizations within the gerbil family social group&quot; doesn't naturally follow from the previous example. This could be rephrased.</p></disp-quote><p>Agreed, thank you. We revised this section of the introduction to flow better.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>While outside the scope of the current study the authors may consider the following experiments and analysis for future studies:</p><p>• Do vocal repertories retain their family signatures across subsequent generations of pups? (i.e. if vocalizations are continually monitored during second or third litters of the same parents).</p><p>• Do the authors observe any long-term changes in family repertoires related to the developmental trajectory of the pups? Are there changes in individual pup vocal features or sound type usage throughout development?</p></disp-quote><p>Thank you for these great suggestions. Given that naked mole rats learn vocalizations through cultural transmission, it would be interesting to see whether other subterranean species with complex social structures (gerbils, voles, rats) have similar abilities. A straightforward way to assess this possibility could be as you suggest — are latent distributions of vocalizations from multi-generational families closer together than cross-family differences? If true, this would provide compelling evidence to investigate further.</p><p>We partially address your second suggestion in our response to Reviewer 1 and in Figure S4, which shows that the family repertoire remains stable throughout this particular period of development. This doesn’t rule out the possibility that there could be other phases of development that undergo more vocal change. Your final suggestion is an area that we are actively researching and eager to know the answer to. A follow-up question: could differences in pup vocal features contribute to differential care by parents?</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>In all, I found the paper clearly written and the figures easy to follow. One small suggestion:</p><p>Figure 1: I can't see the black and gray thresholds described in the caption very well. Perhaps a zoom-in to the first 0.15s or so of the normalized amplitude plot would better display these.</p></disp-quote><p>Agreed, thank you. We added a zoom-in to Figure 1.</p></body></sub-article></article>