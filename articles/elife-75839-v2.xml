<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75839</article-id><article-id pub-id-type="doi">10.7554/eLife.75839</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Audiovisual task switching rapidly modulates sound encoding in mouse auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-261921"><name><surname>Morrill</surname><given-names>Ryan J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8592-4549</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-264221"><name><surname>Bigelow</surname><given-names>James</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-264222"><name><surname>DeKloe</surname><given-names>Jefferson</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-59070"><name><surname>Hasenstaub</surname><given-names>Andrea R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3998-5073</contrib-id><email>andrea.hasenstaub@ucsf.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043mz5j54</institution-id><institution>Coleman Memorial Laboratory, University of California, San Francisco</institution></institution-wrap><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043mz5j54</institution-id><institution>Neuroscience Graduate Program, University of California, San Francisco</institution></institution-wrap><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043mz5j54</institution-id><institution>Department of Otolaryngology–Head and Neck Surgery, University of California, San Francisco</institution></institution-wrap><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>18</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75839</elocation-id><history><date date-type="received" iso-8601-date="2021-11-24"><day>24</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-08-17"><day>17</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-11-11"><day>11</day><month>11</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.11.09.467944"/></event></pub-history><permissions><copyright-statement>© 2022, Morrill et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Morrill et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75839-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75839-figures-v2.pdf"/><abstract><p>In everyday behavior, sensory systems are in constant competition for attentional resources, but the cellular and circuit-level mechanisms of modality-selective attention remain largely uninvestigated. We conducted translaminar recordings in mouse auditory cortex (AC) during an audiovisual (AV) attention shifting task. Attending to sound elements in an AV stream reduced both pre-stimulus and stimulus-evoked spiking activity, primarily in deep-layer neurons and neurons without spectrotemporal tuning. Despite reduced spiking, stimulus decoder accuracy was preserved, suggesting improved sound encoding efficiency. Similarly, task-irrelevant mapping stimuli during inter-trial intervals evoked fewer spikes without impairing stimulus encoding, indicating that attentional modulation generalized beyond training stimuli. Importantly, spiking reductions predicted trial-to-trial behavioral accuracy during auditory attention, but not visual attention. Together, these findings suggest auditory attention facilitates sound discrimination by filtering sound-irrelevant background activity in AC, and that the deepest cortical layers serve as a hub for integrating extramodal contextual information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>attention</kwd><kwd>cross-modal</kwd><kwd>multisensory</kwd><kwd>cortical layers</kwd><kwd>behavior</kwd><kwd>extracellular physiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS116598</award-id><principal-award-recipient><name><surname>Hasenstaub</surname><given-names>Andrea R</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01DC014101</award-id><principal-award-recipient><name><surname>Hasenstaub</surname><given-names>Andrea R</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>GFRP</award-id><principal-award-recipient><name><surname>Morrill</surname><given-names>Ryan J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Hearing Research Incorporated</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Hasenstaub</surname><given-names>Andrea R</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Klingenstein Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Hasenstaub</surname><given-names>Andrea R</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Coleman Memorial Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Hasenstaub</surname><given-names>Andrea R</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32DC016846</award-id><principal-award-recipient><name><surname>Bigelow</surname><given-names>James</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The brain processes the same multisensory stimulus differently when the way it sounds, as opposed to the way it looks, is useful for making a decision.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Information from one or another sensory pathway may become differentially relevant due to environmental changes. The brain must therefore continuously assign limited attentional resources to processing simultaneous information streams from each sensory modality. For example, hearing a siren while listening to music in the car might prompt an attentional shift away from the auditory stream, toward a visual search for emergency vehicles. On the other hand, a similar shift away from the music is unlikely while listening at home. In these cases, contextual cues support allocating attention to either the auditory domain or the visual domain, and the perceptual experience of the music is qualitatively different. How might sensory cortex differentially encode stimuli from an attended versus filtered modality?</p><p>Attentional selection operates cooperatively at many levels of sensory processing. Most effort has been devoted to understanding the neural mechanisms of feature-selective attention within a single modality (<xref ref-type="bibr" rid="bib26">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib37">Fritz et al., 2007</xref>). A major focus of this work has been characterizing transformations of stimulus representations in sensory cortical areas, due to their pivotal position between ascending sensory pathways and behavioral networks implementing top-down control (<xref ref-type="bibr" rid="bib56">Lamme et al., 1998</xref>; <xref ref-type="bibr" rid="bib92">Sutter and Shamma, 2011</xref>). These studies, largely from the visual domain, have shown that attention to a stimulus feature or space will often increase stimulus-evoked spiking responses and reduce thresholds for eliciting a response; likewise, responses to unattended stimuli are often decreased (<xref ref-type="bibr" rid="bib77">Reynolds and Chelazzi, 2004</xref>). On the other hand, fewer studies have examined how modality-selective attention affects encoding in sensory cortex. This mode of attention highlights behaviorally relevant sensory streams while filtering less relevant ones. Human fMRI studies have reported differential activation patterns in auditory and visual cortex (AC, VC) reflecting the attended modality (<xref ref-type="bibr" rid="bib47">Johnson and Zatorre, 2005</xref>; <xref ref-type="bibr" rid="bib73">Petkov et al., 2004</xref>; <xref ref-type="bibr" rid="bib90">Shomstein and Yantis, 2004</xref>; <xref ref-type="bibr" rid="bib97">Woodruff et al., 1996</xref>). Extending these findings, studies in primate AC and VC have reported entrainment local field potential (LFP) oscillations by modality-selective attention, which serves to modulate excitability and sharpen feature tuning within sensory cortex corresponding to the attended modality (<xref ref-type="bibr" rid="bib45">Hocherman et al., 1976</xref>; <xref ref-type="bibr" rid="bib54">Lakatos et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="bib69">O’Connell et al., 2014</xref>). Several findings suggest that these influences may differ among cortical layers and between inhibitory and excitatory neurons (<xref ref-type="bibr" rid="bib55">Lakatos et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">O’Connell et al., 2014</xref>).</p><p>Nevertheless, there are many open questions about the influence of modality-specific attention on stimulus encoding in sensory cortex. Importantly, potential interplay between ongoing activity and evoked responses during attentional selection, as well as their consequences for information and encoding efficiency, has not been examined. The degree to which influences of modality-specific attention may generalize beyond training stimuli has yet to be elucidated. Finally, how these influences may be differentially expressed in cell subpopulations defined by cortical depth or inhibitory/excitatory cell type similarly remains unknown.</p><p>In the present study, we addressed these open questions by examining single neuron activity and sensory responses in mouse AC during an audiovisual (AV) attention shifting task. AC integrates ascending auditory information with diverse input from frontal, cingulate, striatal, and non-auditory sensory areas to rapidly alter sensory processing in response to changing behavioral demands (<xref ref-type="bibr" rid="bib13">Budinger et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Budinger and Scheich, 2009</xref>; <xref ref-type="bibr" rid="bib72">Park et al., 2015</xref>; <xref ref-type="bibr" rid="bib79">Rodgers and DeWeese, 2014</xref>; <xref ref-type="bibr" rid="bib96">Winkowski et al., 2013</xref>). To isolate the influence of modality-selective attentional modulation, we compared responses to identical compound auditory-visual stimuli under different cued contexts requiring attention to the auditory or visual elements, thus holding constant other task-related variables such as arousal, attention, reward expectation, and motor activity (<xref ref-type="bibr" rid="bib83">Saderi et al., 2021</xref>). Because spike rate and information changes are dissociable (<xref ref-type="bibr" rid="bib9">Bigelow et al., 2019</xref>; <xref ref-type="bibr" rid="bib74">Phillips and Hasenstaub, 2016</xref>), we quantified both evoked spike rates and the mutual information (MI) between responses and stimuli. We also examined the generality of modality-specific attention by examining responses to task-irrelevant sounds presented between trials. Finally, we used translaminar probes and spike waveform morphology classification to capture possible attention-related differences in neurons among cortical layers and between putative inhibitory and excitatory cell classes.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>AV rule-switching in mice</title><p>We trained mice to perform an AV rule-switching task, in which they made decisions using auditory stimuli while ignoring simultaneously presented visual stimuli or vice versa. Trial presentation was self-paced in a virtual foraging environment wherein a visual track was advanced by forward locomotion on a spherical treadmill (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). A task-irrelevant random double sweep (RDS) sound was presented during inter-trial intervals (ITIs) for mapping auditory receptive fields in each attentional state (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Decision stimuli were presented after variable track length, consisting of 1 s auditory tone clouds (TCs; centered at 8 or 17 kHz) and/or visual drifting gratings (horizontal or vertical orientation; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). One of the decision stimuli for each modality was a rewarded target (A<sub>R</sub>, V<sub>R</sub>) and the other an unrewarded distractor (A<sub>U</sub>, V<sub>U</sub>). Lick responses following targets (hits) and distractors (false alarms [FAs]) produced water rewards and dark timeouts, respectively. Withholding licks for targets (misses) or distractors (correct rejects [CRs]) advanced the next trial. Each session began with a block of unimodal decision stimuli, which cued the attended modality of a subsequent AV block (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). A second unimodal block from the other modality was then presented, cueing the rule for a final AV block. Decision stimuli had identical physical properties but different behavioral significance between rules (e.g., licks following A<sub>R</sub>V<sub>U</sub> were rewarded in A-rule but punished in V-rule). Targets and distractor stimuli remained constant throughout training for each mouse and were approximately counterbalanced across animals. Block sequences (A-rule then V-rule, or vice versa) were also counterbalanced across sessions (<xref ref-type="fig" rid="fig1">Figure 1D</xref>.c).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Audiovisual rule-switching in mice.</title><p>(<bold>A</bold>) Virtual foraging environment: a head-fixed mouse runs on a floating spherical treadmill. Locomotion measured by treadmill movement controls auditory and visual stimulus presentation. A water spout in front of the mouse provides rewards. A lickometer records licks, which determines reward or punishment. (<bold>B</bold>) Trial sequence: during inter-trial intervals, a track of moving dots provides visual feedback for task progression while a random double sweep (RDS) auditory mapping stimulus is presented. Decision stimuli, either unimodal (auditory, A; visual, V) or bimodal (AV), are presented for 1 s. The choice window begins at decision stimulus onset, but trials with early licks (&lt;0.3 s post-stimulus onset) are removed from subsequent analysis. (<bold>C</bold>) Decision stimuli are tone clouds (TCs; 8 or 17 kHz centered) or drifting gratings (horizontal or vertical orientation). Each mouse is trained to lick for one auditory stimulus and one visual stimulus. Target/distractor stimulus identities were counterbalanced across mice for A- and V-rules. (<bold>D</bold>) Task sequences, attention cueing, and reward contingencies. (<bold>a–b</bold>) Behavioral sessions begin with a unimodal block, which cue the rule for the subsequent AV block. Water drops represent target stimuli, when mice have an opportunity for reward. (<bold>c</bold>) Each session used one of two possible task sequences. (<bold>d</bold>) Stimulus codes, for reference. (<bold>e</bold>) Contingencies for water reward, timeout punishment, or task continuation. (<bold>E</bold>) Example behavior session. (<bold>a</bold>) Hit rate (HR) and false alarm rate (FAR) across task blocks; trials and outcomes indicated by colored background bars. Mouse locomotion is shown below. (<bold>b</bold>) Stimulus onset-aligned lick rasters for example session, organized by rule and target/distractor. Note that errors are typically false alarms on trials with ‘conflict’ stimuli: A<sub>U</sub>V<sub>R</sub> in A-rule or A<sub>R</sub>V<sub>U</sub> in V-rule. (<bold>F</bold>) Performance for all sessions included in subsequent physiology analysis. (<bold>a</bold>) HR and FAR for all sessions organized by rule block; dashed lines indicate means. (<bold>b</bold>) Performance metrics, showing dual inclusion filters: 1. sensitivity index <italic>d’</italic> performance index &gt;1.5 for both A-rule and V-rule and 2. FAR<sub>conf</sub> &lt;0.5 for conflict stimuli, as a critical test of modality-selective attention. (<bold>c</bold>) <italic>d’</italic> is similar across task rules in unimodal and AV segments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig1-v2.tif"/></fig><p>We used two approaches to ensure that animals were engaged during both task rules. First, we restricted analysis to sessions in which discrimination was well above chance (<italic>d’</italic>&gt;1.5) for both rules, and for which FA rates were below 0.5 for the stimuli with reward valences that conflicted across rules (A<sub>U</sub>V<sub>R</sub> in the A-rule, A<sub>R</sub>V<sub>U</sub> in the V-rule; <xref ref-type="fig" rid="fig1">Figure 1F</xref>). Second, for a subset of sessions (<italic>n</italic>=14 sessions, 5 mice) we measured pupil size, a well-established correlate of arousal and behavioral performance (<xref ref-type="bibr" rid="bib12">Bradley et al., 2008</xref>; <xref ref-type="bibr" rid="bib63">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="bib76">Reimer et al., 2014</xref>). We used a computer vision algorithm to automate measurement of pupil size (pupil diameter/eye diameter) for each frame acquired by a CCD video camera (<xref ref-type="fig" rid="fig2">Figure 2A</xref>.a). To isolate pupil fluctuations reflecting general arousal, pupil size was measured during an ITI window designed to avoid pupil responses to decision stimulus onset, dark timeouts, and decreased locomotion events following reward administration (<xref ref-type="fig" rid="fig2">Figure 2</xref>.A.b–c, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Previous studies have reported that pupil size increases with task difficulty and engagement in humans, non-human primates, and rodents (<xref ref-type="bibr" rid="bib44">Hess and Polt, 1964</xref>; <xref ref-type="bibr" rid="bib48">Kawaguchi et al., 2018</xref>; <xref ref-type="bibr" rid="bib88">Schriver et al., 2018</xref>). We reasoned that comparison of pupil size across the rules would allow us to establish whether task demands differed between the rules. No difference in pupil size was observed between rules during bimodal blocks (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; A-rule bimodal: 0.29±0.05 norm. diameter ± SD, V-rule bimodal: 0.30±0.05; <italic>Z</italic>=–1.0, p=0.30, paired Wilcoxon signed-rank [WSR], Benjamini-Hochberg false discovery rate [FDR]-adjusted p<italic>-</italic>values). As expected, pupil diameters were significantly larger during the bimodal portion of the task, when visual stimuli were present and the task had increased in difficulty, compared to the auditory-only unimodal portion of the task (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; A-rule unimodal: 0.28±0.04, A-rule bimodal: 0.29±0.05; <italic>Z</italic>=–2.6, p=0.009). A trend toward smaller pupil size in the unimodal visual rule compared to bimodal rule was also noted, but the difference did not reach significance after multiple comparisons correction (V-rule unimodal: 0.29±0.04, V-rule bimodal: 0.30±0.05; <italic>Z</italic>=–2.0, p=0.062). Because pupil size also closely tracks locomotion (<xref ref-type="fig" rid="fig2">Figure 2A</xref>.b–c; <xref ref-type="bibr" rid="bib63">McGinley et al., 2015</xref>), we examined locomotion speed during the same ITI window (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Differences in locomotion speed were also not observed between rules (all p≥0.623; all |<italic>Z</italic>|≤1.29, paired WSR). Arousal and motor activity were thus comparable between rules, suggesting that differences in neuronal activity may be attributable to modality-selective attention.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Similar levels of arousal and movement during auditory and visual attention.</title><p>(<bold>A</bold>) Pupil size measurement. (<bold>a</bold>) Left eye pupil recorded via CCD camera during the task. Pupil circumference (light blue) is tracked using automated video analysis; size is measured as pupil diameter over visible eye diameter. (<bold>b</bold>) Example pupil video recorded during visual rule. Upper: annotated sample frames from times indicated by blue dashed lines. Lower: pupil width (green) and locomotion (gray) traces, with target stimuli and timeout punishments indicated. Large fluctuations of pupil size occur during timeouts due to drop in light level (hashed gray background). (<bold>c</bold>) Auditory rule from the same session. (<bold>B</bold>) Pupil size is measured during an inter-trial interval (ITI) window selected to capture engagement and arousal levels during each block and minimize influence from trial-related events such as rewards and timeouts. (<bold>a</bold>) Pupil size decreases during hit trials due to reward administration. Correct reject trials (CRs; bottom) show no such decrease in running speed. (<bold>b</bold>) Pupil size increases during timeout punishment when the recording chamber goes dark; ITI pupil size analysis window removes punishment-related fluctuations from analysis. (<bold>C</bold>) Pupil size is similar across V-rule bimodal and A-rule bimodal segments (pupillometry recorded in <italic>n</italic>=14 sessions, 5 mice), suggesting similar levels of arousal and task engagement across rules. Difference box plots: central line: median; box edges: 25th and 75th percentiles; whiskers: data points not considered outliers. (<bold>D</bold>) Min-max-normalized locomotion is also similar across rules.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Pupil size and locomotion compared between unimodal and bimodal blocks.</title><p>(<bold>A</bold>) Pupil size, measured as described in <xref ref-type="fig" rid="fig2">Figure 2</xref>, compared within rule between unimodal and bimodal task segments. (<bold>a</bold>) A-rule: unimodal A-only pupil size is smaller than bimodal AV pupil (A-rule unimodal: 0.28±0.04 mean norm, pupil width ± SD, A-rule bimodal: 0.29±0.05; <italic>Z</italic>=–2.6, p=0.009), possibly due to the presence of drifting grating visual stimulation or increased task difficulty in the bimodal segment. (<bold>b</bold>) V-rule: unimodal V-only pupil size trends toward smaller than that in bimodal AV pupil, but does not reach significance after multiple comparisons correction (V-rule unimodal: 0.29±0.04, V-rule bimodal: 0.30±0.05; <italic>Z</italic>=–2.0, p=0.062). (<bold>B</bold>) Min-max-normalized locomotion is similar across unimodal and bimodal task segments. (<bold>a</bold>) A-rule: locomotion during unimodal is comparable to locomotion during bimodal (A unimodal: 0.46±0.08, A bimodal: 0.46±0.05; <italic>Z</italic>=0.282, p=0.78). (<bold>b</bold>) V-rule: locomotion during unimodal is comparable to locomotion during bimodal (V uni: 0.46±0.06, V bimodal: 0.43±0.07; <italic>Z</italic>=1.224, p=0.33). All p<italic>-</italic>values false discovery rate (FDR) adjusted for <italic>n</italic>=3 comparisons (including A-rule bimodal vs. V-rule bimodal in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Conventions for difference box plots as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig2-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Single unit recording in AC</title><p>After mice learned the AV rule-switching task, a craniotomy was made over right AC, to allow for acute recordings during behavior using multichannel probes spanning the full cortical depth (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). In total, we recorded AC activity in 10 mice during 23 behavioral sessions meeting inclusion criteria. The putative cortical depth of each sorted single unit (SU) was assigned by calculating the fractional position of the channel with the largest waveform amplitude within the span of channels in AC, as estimated from spontaneous and tone-evoked recordings following the task (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). A separate set of experiments to visualize probe tracks with the fluorescent dye Di-I provided support for this depth estimation technique (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="bibr" rid="bib27">DiCarlo et al., 1996</xref>; <xref ref-type="bibr" rid="bib66">Morrill and Hasenstaub, 2018</xref>). We then divided the fractional depth values into superficial, middle, and deep groups, approximating the supragranular, granular, and infragranular laminae. We further divided SUs into narrow-spiking (NS, putative inhibitory; <italic>n</italic>=130, 18%) and broad-spiking (BS, predominantly excitatory; <italic>n</italic>=612, 82%) populations based on trough-peak time (<xref ref-type="fig" rid="fig3">Figure 3D</xref>; <xref ref-type="bibr" rid="bib9">Bigelow et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Cardin et al., 2007</xref>; <xref ref-type="bibr" rid="bib68">Nandy et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Phillips et al., 2017</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Single unit (SU) recording and depth estimation in auditory cortex.</title><p>(<bold>A</bold>) Translaminar probes were used to record activity in right auditory cortex (AC). (<bold>B</bold>) Physiological estimation of cortical depth. (<bold>a</bold>) Linear 64-channel probe captures all activity in layers of AC. (<bold>b</bold>) Example tone-evoked multi-unit (MU) sound responses by channel, providing a marker for the border of deep cortex and white matter (WM). Left: peristimulus time histogram (PSTH) plots showing mean tone response by time. Right: frequency response area (FRA) shows mean response during tone stimulus by frequency/attenuation. MU responses poorly estimate the upper cortical boundary due to low somatic spiking activity in the superficial cortex. (<bold>c</bold>) Local field potential (LFP; 1–300 Hz filtered) provides a marker for the upper cortex-pia boundary. Left: 10 s snippet of LFP by channel. Right: maximum LFP amplitude by channel, with putative pia location defined as the first deviation from probe-wise minimum LFP amplitude. (<bold>d</bold>) Channels are assigned cortical depths based on fractional division of cortex into ‘superficial’, ‘middle’, and ‘deep’, with fractions based on supragranular, granular, and infragranular anatomical divisions. (<bold>C</bold>) Histological confirmation of cortical depth estimation technique. (<bold>a</bold>) Coronal slice showing DI-I probe track (red) in right AC. Green: eYFP fluorescence from Ai32/Sst-Cre mouse strain. Blue: DAPI stain to visualize cell bodies. (<bold>b</bold>) Zoomed area indicated by dashed rectangle in a. (<bold>c</bold>) Probe overlay and WM/pia boundaries. Yellow arrows indicate locations of physiologically determined cortical span from B, showing close correspondence with Di-I probe track. (<bold>D</bold>) Sorted SU waveforms were divided into narrow-spiking (putative fast-spiking inhibitory) and broad-spiking (putative excitatory) based on a waveform trough-peak time boundary of 0.6 ms.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig3-v2.tif"/></fig></sec><sec id="s2-3"><title>Modality-selective attention modulates stimulus-evoked firing rates</title><p>To measure the effects of modality-selective attention on stimulus processing in AC, we began by comparing SU responses to bimodal decision stimuli across task rules. These responses reflected physically identical stimuli and similar levels of arousal and locomotion, as shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Activity patterns evoked by decision stimuli and modulatory effects of task rule were diverse (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). To capture a predominantly sensory-driven response component, we measured mean firing rates (FRs) during the first 300 ms post-stimulus onset (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), which preceded most lick responses (lick latency median: 611 ms; 5–95th percentiles: 289–1078 ms; 5.4% of licks &lt;300 ms, <italic>n</italic>=2852 total lick trials across dataset). Trials with licks earlier than 300 ms were excluded from analysis. We first compared A-rule and V-rule responses to the TC rewarded in the A-rule (A<sub>R</sub>*: A<sub>R</sub>V<sub>R</sub> and A<sub>R</sub>V<sub>U</sub> responses combined). Averaging across units, responses in the deep layers were suppressed in the A-rule relative to the V-rule for both NS and BS units (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; deep BS: p=2.8e-4, <italic>Z</italic>=4.1, median fold change [FC; A-rule/V-rule]: 0.89, <italic>n</italic>=333 SUs; deep NS: p=0.011, <italic>Z</italic>=2.9, median FC: 0.87, <italic>n</italic>=66; paired WSR, FDR-adjusted p<italic>-</italic>values; see <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1A</xref> for full stats). No significant group-level change was found in middle or superficial units. Consistent with group-level trends, individual units with significant FR decreases in the A-rule (p&lt;0.01, unpaired t-test) substantially outnumbered units with significant FR increases for all unit populations other than superficial and mid-depth BS units (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Net suppression of sound-evoked firing rates during auditory attention.</title><p>(<bold>A</bold>) Example single unit (SU) responses to physically identical audiovisual (AV) stimuli across task rules (A<sub>R</sub>*=A<sub>R</sub>V<sub>R</sub> and A<sub>R</sub>V<sub>U</sub> collapsed; A<sub>U</sub>*=A<sub>U</sub>V<sub>U</sub> and A<sub>U</sub>V<sub>R</sub> collapsed). (<bold>a</bold>) Response showing preference for A<sub>R</sub>* tone cloud, suppressed in A-rule relative to V-rule. (<bold>b</bold>) Preference for A<sub>U</sub>* tone cloud, suppressed in A-rule. (<bold>c</bold>) Moderately enhanced firing rate (FR) in A-rule. (<bold>B</bold>) Example SU responses to (<bold>a</bold>) A<sub>R</sub>* and (<bold>b</bold>) A<sub>U</sub>* tone clouds (TCs). Early sensory-driven response analysis window (0–0.3 s) shown in light blue. (<bold>C</bold>) Group data: responses to TCs rewarded in A-rule (A<sub>R</sub>*) between rules by unit type and depth. Scatter plots (left) show FR across rules. Red: broad-spiking (BS) units. Blue: narrow-spiking (NS) units. Outlined: significantly modulated units, paired t-test, Benjamini-Hochberg false discovery rate (FDR)-adjusted, q=0.01. Fold change histograms show A-rule FR divided by V-rule FR for all units; bins to the left of 1 (dashed line) indicate FR suppression in A-rule. Box plots above histograms: central line: median; box edges: 25th and 75th percentiles; whiskers: data points not considered outliers. Asterisks indicate FDR-adjusted (q=0.05, n=6 tests) p<italic>-</italic>values from paired Wilcoxon signed-rank tests of mean FRs across rules; no asterisk: not significant (p&gt;0.05). Right: fractions of significantly modulated units (inclusion as described above) over total. Darker colors indicate fractions with significantly suppressed FRs in A-rule; lighter colors indicate enhanced FRs in A-rule. (<bold>D</bold>) Responses to TCs unrewarded in A-rule (A<sub>U</sub>*). All conventions as in C. (<bold>E</bold>) Comparison of unit FR modulation by rule between A<sub>R</sub>* (abscissa) and A<sub>U</sub>* (ordinate). Top: BS units, bottom: NS units. Scatter plots (left) show all units with significant rule modulation for A<sub>R</sub>*, A<sub>U</sub>*, or both. Modulation values &lt;1 indicate suppressed FR response in A-rule. Note the increased density of units below 1 for BS and NS units. Right: counts of units by direction of FR rule modulation. Most units lie in quadrants with similar direction of modulation across stimuli, suggesting that attentional effects on FR are not frequency- or stimulus identity-dependent. (<bold>F</bold>) Example SU response to the onset of the random double sweep (RDS) mapping stimulus, showing analysis window for calculating FR (0–0.3 s, blue). (<bold>G</bold>) Group data for RDS FR modulation across rules by depth and BS/NS classifications. All conventions as in C.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Decision stimulus response firing rate (FR) across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig4-data1-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>Random double sweep (RDS) response firing rates (FR) across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig4-data2-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig4-v2.tif"/></fig><p>A similar pattern of attention-related modulation was observed for unrewarded stimuli in the A-rule (A<sub>U</sub>*: A<sub>U</sub>V<sub>R</sub> and A<sub>U</sub>V<sub>U</sub> responses combined). At the group level, superficial and middle unit responses did not differ significantly between conditions, whereas deep BS units were suppressed in the A-rule (<xref ref-type="fig" rid="fig4">Figure 4D</xref>; deep BS: p=2.0e-06, <italic>Z</italic>=5.11, median FC: 0.81, <italic>n</italic>=321; paired WSR, FDR-adjusted p<italic>-</italic>values; see <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1B</xref> for full stats). Relative fractions of units with significantly modulated FRs to A<sub>U</sub>* stimuli were similar to those described above for A<sub>R</sub>* stimuli (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, right), with the exception of the superficial group, in which slightly more unitshad increased FRs. We further found that most units showed the same direction of modulation for A<sub>R</sub>* and A<sub>U</sub>* stimuli (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), with a similar modulation sign observed for 78% of BS units (50% suppressed for both A<sub>R</sub>* and A<sub>U</sub>*, 28% enhanced for both) and 99% of NS units (68% suppressed for both, 31% enhanced for both). These findings suggest that modality-selective attention similarly influences FRs evoked by task-relevant target and distractor sounds with different acoustic properties and learned behavioral values.</p><p>To determine whether these attentional influences might generalize to task-irrelevant sounds, we examined responses to RDS sounds presented during the ITI. Using the same analysis window (300 ms post-stimulus onset, <xref ref-type="fig" rid="fig4">Figure 4F</xref>), we found that attention-related modulation of FR responses evoked by task-irrelevant sounds was highly similar to that observed for both types of decision stimuli: middle- and deep-layer BS and NS populations exhibited group-level FR suppression during the A-rule (<xref ref-type="fig" rid="fig4">Figure 4G</xref>), whereas superficial layer units showed no difference (middle BS: p=6.0e-3, <italic>Z</italic>=3.1, median FC: 0.85, <italic>n</italic>=112; middle NS: p=0.014, <italic>Z</italic>=2.7, median FC: 0.65, <italic>n</italic>=28; deep BS: p=1.2e-6, <italic>Z</italic>=5.2, median FC: 0.84, <italic>n</italic>=309; deep NS: p=0.021, <italic>Z</italic>=2.5, median FC: 0.80, <italic>n</italic>=64; paired WSR; see <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref> for full stats). Significantly modulated unit counts were again highly biased toward suppression in the A-rule, with pronounced differences in the middle and deep unit groups (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, right). Together, these results show that auditory-selective attention tends to reduce FR responses to sounds, regardless of their behavioral relevance, valence, or spectral content, and that these influences are strongest for deep-layer units.</p></sec><sec id="s2-4"><title>Modality-selective attention also modulates pre-stimulus FRs</title><p>Previous studies have found that modulation of ongoing activity in sensory cortex can influence subsequent sensory-evoked responses (<xref ref-type="bibr" rid="bib2">Arieli et al., 1996</xref>; <xref ref-type="bibr" rid="bib40">Haider and McCormick, 2009</xref>). Thus, the response suppression during auditory attention reported above may either reflect specific decreases in stimulus responsivity or general decreases in ongoing activity. To address these possibilities, we quantified FRs in a pre-stimulus window spanning 300 ms prior to decision stimulus onset in which no sounds were presented (<xref ref-type="fig" rid="fig5">Figure 5A</xref>.a). Although this window may include anticipatory modulation of activity (<xref ref-type="bibr" rid="bib22">Cox et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Egner et al., 2010</xref>; <xref ref-type="bibr" rid="bib86">Samuelsen et al., 2012</xref>), it nevertheless provides a measure of baseline activity for comparison with evoked responses. We observed significant group-level decreases in pre-stimulus FRs during the A-rule for units in the middle NS and deep BS groups, but no modulation of superficial units (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; middle NS: p=0.039, <italic>Z</italic>=2.48, median FC: 0.71, <italic>n</italic>=28; deep BS: p=4.2e-05, <italic>Z</italic>=4.49, median FC: 0.87, <italic>n</italic>=336; paired WSR, FDR-adjusted p<italic>-</italic>values; see <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref> for full stats). To test whether the reduction in pre-stimulus FR was sufficient to account for stimulus-evoked changes reported above, we recalculated FRs evoked by decision stimuli as FC from pre-stimulus FRs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>.b). Following this adjustment and after FDR correction, the middle- and deep-layer unit population responses no longer differed between rules (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). Together, these results suggest that group-level decreases in evoked FRs during A-rule are largely due to generalized suppression of ongoing AC activity.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Attention-related modulation of sound-evoked responses largely reflects pre-stimulus activity changes.</title><p>(<bold>A</bold>) Example pre-stimulus firing rate (FR) measurement, and normalization of post-stimulus response. (<bold>a</bold>) Raw FR by condition and stimulus. Pre-stimulus analysis window shown in blue (−0.3–0 s). (<bold>b</bold>) Normalized FRs (FR divided by mean pre-stimulus FR). (<bold>B</bold>) Group data: pre-stimulus onset FR compared across rules, with data organized by depth (S=superficial, M=middle, D=deep) and broad-spiking/narrow-spiking (BS/NS) (red/blue). Conventions as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Scatter plots (left) show individual units, with significantly modulated units outlined (paired t-test, Benjamini-Hochberg false discovery rate (FDR)-adjusted, q=0.01). Difference histograms show A-rule/V-rule for all units shown in scatters; fold change &lt;1 indicates suppression during the A-rule. As in <xref ref-type="fig" rid="fig4">Figure 4</xref>, asterisks represent p<italic>-</italic>values from FDR-adjusted paired Wilcoxon signed-rank tests on each group (q=0.05, n=6 tests). Absence of asterisk: not significant. (<bold>C</bold>) Group data: response as fold change, normalized by pre-stimulus FR. Conventions as in B and <xref ref-type="fig" rid="fig4">Figure 4</xref>. After accounting for pre-stimulus modulation, effects of rule on FR are abolished.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Pre-stimulus firing rate (FR) across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig5-data1-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Decision stimulus baseline-adjusted firing rate (FR) across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig5-data2-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig5-v2.tif"/></fig></sec><sec id="s2-5"><title>Attention-related suppression is driven by units without STRF tuning</title><p>We next sought to determine whether attention-related changes in stimulus response were related to the tuning preferences of units, a phenomenon termed ‘feature attention’ previously observed in both monkey VC (<xref ref-type="bibr" rid="bib62">Maunsell and Treue, 2006</xref>; <xref ref-type="bibr" rid="bib94">Treue and Martínez Trujillo, 1999</xref>) and AC (<xref ref-type="bibr" rid="bib24">Da Costa et al., 2013</xref>). The RDS mapping stimulus, which we have previously used to efficiently identify auditory response properties and AV interactions (<xref ref-type="bibr" rid="bib10">Bigelow et al., 2022</xref>), was used to generate spectrotemporal receptive fields (STRFs) through reverse correlation (<xref ref-type="fig" rid="fig6">Figure 6A</xref>; <xref ref-type="bibr" rid="bib1">Aertsen and Johannesma, 1981</xref>; <xref ref-type="bibr" rid="bib25">de Boer, 1968</xref>; <xref ref-type="bibr" rid="bib38">Gourévitch et al., 2015</xref>). Tuning for each STRF was measured through a trial-to-trial reliability metric, which we used to divide units into those with activity changes that were reliably evoked by defined spectral or temporal features (tuned STRFs, <italic>n</italic>=172; <xref ref-type="fig" rid="fig6">Figure 6C</xref>.a) and those without feature-evoked changes (untuned, <italic>n</italic>=409; <xref ref-type="fig" rid="fig6">Figure 6C</xref>.b). Spiking activity levels were higher in tuned units compared to untuned (<xref ref-type="fig" rid="fig6">Figure 6D</xref>.a). To control for possible activity level-dependent effects, we compared our population of tuned units to a randomly selected subset of untuned units which was matched for both sample size and FR to the tuned population (<xref ref-type="fig" rid="fig6">Figure 6D</xref>.b). We then examined attentional modulation of stimulus responses between the tuned and subsampled untuned groups. Responses to the rewarded TC (A<sub>R</sub>*), unrewarded TC (A<sub>U</sub>*), and the RDS mapping stimuli were significantly modulated by task rule in the untuned group, but not the tuned group (tuned: all p≥0.18, all |<italic>Z</italic>|≤1.72; untuned: all p≤0.023, all |<italic>Z</italic>|≥2.27; one-way WSR vs. modulation of 1 [equal across rules], FDR-adjusted p<italic>-</italic>values; <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1C, D</xref>). Nevertheless, comparisons across these tuned and untuned groups showed that the distributions did not significantly differ after multiple comparisons correction (all p≥0.12, all |<italic>Z</italic>|≤2.06; WSR).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Attentional modulation of spike rate is driven by neurons without spectrotemporal receptive field (STRF) tuning.</title><p>(<bold>A</bold>) STRFs for A-rule and V-rule were calculated from spikes during the inter-trial-interval random-double sweep (RDS) stimulus using standard reverse correlation methods. (<bold>B</bold>) STRF reliability as a measure for tuning. (<bold>a</bold>) Reliability was measured through correlations of randomly subsampled halves of all RDS presentations, repeated 1000 times. A p<italic>-</italic>value was calculated empirically through comparison of correlation value distributions from the actual STRF and a null STRF, generated from random circular shuffling of spike trains relative to stimulus. (<bold>b</bold>) Left: fraction of tuned and untuned units. Right: fractions of units with STRF tuning in both A- and V-rules (AV), A-rule only (A), or V-rule only (V). (<bold>C</bold>) Trial-to-trial reliability metric separates AC units into those with tuned STRFs (<bold>a</bold>) and untuned STRFs (<bold>b</bold>). (<bold>D</bold>) To control for activity level-driven effects, the larger group with untuned STRFs (<italic>n</italic>=345, 64 for broad-spiking [BS], narrow-spiking [NS]) is matched for sample size and firing rate to the group with tuned STRFs (<italic>n</italic>=121, 51 for BS, NS). (<bold>a</bold>) Mean firing rate (FR) during RDS mapping stimulus by tuned and untuned groups. (<bold>b</bold>) FR distribution-matched groups. (<bold>c</bold>) Tuned group contains a larger share of decision stimulus-responsive units compared with untuned (for both A<sub>R</sub>* and A<sub>U</sub>* tone clouds [TCs]). Stimulus responsiveness is defined as a significant FR difference between 0.3–0 s pre-stimulus window and 0–0.3 s post-stimulus window, paired t-test, Benjamini-Hochberg false discovery rate (FDR)-adjusted, q=0.01. (<bold>E</bold>) Untuned unit group is suppressed during auditory attention, while tuned unit group is not. (<bold>a</bold>) Attentional modulation of BS unit responses for task decision stimuli (left: A<sub>R</sub>*; right: A<sub>U</sub>*) and RDS mapping stimuli (right). Paired Wilcoxon signed-rank between mean FR in A-rule and V-rule, FDR-corrected at q=0.05 (<italic>n</italic>=3 comparisons per group). Asterisks indicated FDR-adjusted p<italic>-</italic>values. (<bold>b</bold>) NS, conventions as in <bold>a</bold>. Asterisks indicate significance: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001. (<bold>F</bold>) Measurement of best frequency (BF) from tuned STRF group, based on peaks of absolute values of significant time-frequency bins summed across time (–100 ms to 0 window). Significant time-frequency bins (p&lt;0.01) determined by comparison of observed STRF values with distribution of values from spike time-shuffled null STRF. (<bold>G</bold>) BFs of excitatory STRF fields show that AC units are preferentially tuned near the center frequency of the target (rewarded) TC. (<bold>H</bold>) Attentional modulation by BF of tuned units: tuned near A<sub>R</sub> (BF ± 0.5 octaves from TC center), A<sub>U</sub>, or tuned to frequency outside either band. Response modulation does not differ by BF tuning for any comparison (A<sub>R</sub>* or A<sub>U</sub>* response and BS or NS units; Kruskal-Wallis test; BS: all p&gt;0.11, NS: all p&gt;0.81, FDR-adjusted).</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Stimulus response modulation across rules by spectrotemporal receptive field (STRF) tuning.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig6-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig6-v2.tif"/></fig><p>An important caveat is that the RDS stimuli may not capture all units with some degree of tuning preference. As such, a conservative interpretation would be that group-level suppression during auditory attention is driven by units that do not exhibit strong tuning preferences. Additionally, both tuned and untuned populations contained units with significant evoked responses to the two TCs, although fractions of responsive units were higher in the tuned group (<xref ref-type="fig" rid="fig6">Figure 6D</xref>.c). This shows that an absence of STRF tuning does not imply that units were not responsive to the task stimuli.</p><p>For the tuned group, does frequency preference determine degree of attentional modulation? We measured the best frequency (BF) of the excitatory field in each tuned STRF (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). Consistent with previous work showing that task demands shape frequency representation in AC (<xref ref-type="bibr" rid="bib6">Atiani et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Fritz et al., 2005</xref>; <xref ref-type="bibr" rid="bib35">Fritz et al., 2003</xref>; <xref ref-type="bibr" rid="bib98">Yin et al., 2014</xref>), we found a strong BF preference for a 1-octave band around the center frequency of the rewarded TC (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). Furthermore, distributions of BFs measured during the A-rule and V-rule were strikingly similar. This suggests that in our task, AC had shifted its frequency representation in a manner that was not rule-dependent. To test whether modulation by rule was dependent on tuning, we next divided units by their BF, as measured from the A-rule STRF, into groups near center frequency of A<sub>R</sub> (±0.5 octaves), near A<sub>U</sub> or with a BF outside of either band. No difference between the tuning groups was observed for responses to A<sub>R</sub>* or A<sub>U</sub>* TCs (Kruskal-Wallis non-parametric ANOVA, all p&gt;0.12, all <italic>H</italic>&lt;5.5, FDR-adjusted p<italic>-</italic>values; <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1C, D</xref>), suggesting that frequency tuning does not determine suppression or enhancement by attention in this task.</p></sec><sec id="s2-6"><title>Attention to sound increases encoding efficiency in deep-layer BS units</title><p>Previous work has established that FR changes do not necessarily imply changes in the amount of information spikes carry about sensory stimuli. For instance, optogenetic activation of inhibitory interneurons can reduce FRs in AC without changing information, suggesting increased encoding efficiency (<xref ref-type="bibr" rid="bib74">Phillips and Hasenstaub, 2016</xref>). By contrast, locomotion reduces both FRs and information in AC (<xref ref-type="bibr" rid="bib9">Bigelow et al., 2019</xref>). To determine whether reduced FRs evoked by decision stimuli were accompanied by changes in information or encoding efficiency, we used a peristimulus time histogram (PSTH)-based neural pattern decoder to compare sound discrimination across attentional states (<xref ref-type="bibr" rid="bib34">Foffani and Moxon, 2004</xref>; <xref ref-type="bibr" rid="bib46">Hoglen et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Malone et al., 2007</xref>). For each unit, the decoder generates a single-trial test PSTH and then compares these to two or more template PSTHs from different stimulus response conditions, generated sans test trial (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). The test trial is assigned to the template that is closest in <italic>n-</italic>dimensional Euclidean space, reflecting <italic>n</italic> PSTH bins. This is repeated for all trials, generating new templates for each classifier run. After all trials have been classified, a confusion matrix is generated. From this, we calculated accuracy of classification, MI (bits), and encoding efficiency, a spike-rate-normalized MI (bits/spike). As in previous analyses, a 0–300 ms post-stimulus onset window was used in this method to restrict decoding to a predominantly sensory-driven component of the response. The binwidth for generating PSTHs was 30 ms (<xref ref-type="bibr" rid="bib46">Hoglen et al., 2018</xref>). Only trials with correct responses (hits and CRs) and units with a minimum stimulus response FR of 1 Hz to both stimuli used in the decoder comparison were included.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Auditory attention increases sound encoding efficiency in deep-layer broad-spiking units.</title><p>(<bold>A</bold>) Peristimulus time histogram (PSTH)-based spike train decoding analysis. Time-binned responses for each single trial (test trial; red) are compared to PSTHs (templates; green, purple) reflecting responses to each stimulus averaged across all other trials. Trials are classified as belonging to the template nearest to the test trial in <italic>n</italic>-dimensional Euclidean space (<italic>n</italic>=number of PSTH bins). A confusion matrix (right) reflecting predicted/actual outcomes for all trials is used to calculate accuracy, mutual information (MI; bits), and encoding efficiency (bits/spike). (<bold>B</bold>) Decoding accuracy of auditory stimulus identity, compared across attentional states. Decoder setup mimics task faced by mice in the A-rule: discrimination between A<sub>R</sub>* and A<sub>U</sub>* tone cloud identities. Results represent average of two decoder runs, which differ in their paired visual stimulus, but yield similar results: A<sub>R</sub>V<sub>R</sub> vs. A<sub>U</sub>V<sub>R</sub> and A<sub>R</sub>V<sub>U</sub> vs. A<sub>U</sub>V<sub>U</sub> (see <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> for separate presentation of these data). Conventions as in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. and elsewhere. Scatter plots represent decoder accuracy from individual units; dashed lines show chance level (50%). Histograms show raw unit counts for A-rule/V-rule fold change. S=superficial; M=middle; D=deep. No change in accuracy is observed across rules. (<bold>C</bold>) Stimulus-spike information efficiency (bits/spike, calculation shown in A) for PSTH-based decoding increases for deep broad-spiking units during auditory attention. Conventions as in <bold>B</bold>. (<bold>D</bold>) Measuring encoding changes across attentional states for inter-trial interval (ITI) mapping stimuli. (<bold>a</bold>) Example spectrotemporal receptive fields (STRFs) calculated from ITIs of A-rule and V-rule from the same single unit (SU). (<bold>b</bold>) Estimation of mutual information efficiency of ITI random double sweep (RDS) stimuli: the STRF is convolved with the windows of the RDS stimulus to define two distributions of relative STRF-stimulus similarity values: 1. <italic>P</italic>(<italic>x</italic>|spike), from time windows preceding a spike, and 2. <italic>P</italic>(<italic>x</italic>), a null distribution from non-overlapping time windows tiling the full stimulus duration. Information encoding efficiency is calculated as shown, reflecting the divergence between these distributions, which increases when spiking preferentially occurs during periods of higher stimulus-STRF similarity. Mutual information (MI) values are calculated from STRFs in A-rule and V-rule separately. (<bold>E</bold>) Example of spike train-STRF encoding efficiency from two SUs: low (<bold>a</bold>) and high (<bold>b</bold>) bits/spike examples. (<bold>F</bold>) Comparison of spike train-STRF encoding efficiency across rules, showing increased encoding efficiency in A-rule for deep broad-spiking units.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>Decoding of task rule from stimulus response peristimulus time histogram (PSTHs).</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig7-data1-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig7sdata2"><label>Figure 7—source data 2.</label><caption><title>Decoder accuracy by rule.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig7-data2-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig7sdata3"><label>Figure 7—source data 3.</label><caption><title>Comparison of decoding accuracy across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig7-data3-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig7sdata4"><label>Figure 7—source data 4.</label><caption><title>Mutual information (MI) efficiency across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig7-data4-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig7sdata5"><label>Figure 7—source data 5.</label><caption><title>Comparison of spectrotemporal receptive field (STRF) mutual information (MI) (bits/spike) across rules.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig7-data5-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Decoding of task rule from stimulus response peristimulus time histograms (PSTHs).</title><p>(<bold>A</bold>) Decoding of rule identity from PSTH responses using Euclidean distance-based classifier. Responses to each of the four audiovisual (AV) stimuli in the A-rule vs. responses to the same stimuli in the V-rule were decoded based on 0–0.3 s stimulus onset window as described in <xref ref-type="fig" rid="fig7">Figure 7</xref> and in text; PSTHs were constructed with 30 ms bins. Each dot represents single unit (SU) decoding accuracy (% correct); broad-spiking (BS) units in red, narrow-spiking (NS) in blue. Chance decoding is indicated by the dashed line. A minimum 1 Hz firing rate (FR) response to both stimuli in the decode was required for inclusion. Box plots as before: central line indicates median, box edges indicate 25th and 75th percentiles, whiskers extend to data points not considered outliers. A repeated-measures ANOVA showed no difference of stimulus or BS/NS group in decoding (stimulus [within-subject]: F(2.8,1050.6)=0.73, p=0.52; BS/NS [between-subject]: F(2.8,1050.6)=0.68, p=0.55; Greenhouse-Geisser corrected). (<bold>B</bold>) Decoding by depth group. Median decoding by stimulus indicated as colored horizontal box plots (as described above). Gray distributions represent decoding by depth, averaged across stimulus types. Left: BS units, right: NS. A two-way ANOVA shows a significant effect of depth (F(2,579) = 3.75, p=0.024), BS/NS (F(1,579) = 27.3, p ≈ 0), but not their interaction (F(2,579) = 0.61, p=0.55). Post hoc comparison by depth showed that decoding was significantly better in the deep compared to the superficial groups and the NS compared to BS unit groups. See <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref> for additional statistics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Decoding and information efficiency changes across rules are similar across visual stimulus pairings.</title><p>Decoding of auditory stimulus identity, compared across attentional states, similar to <xref ref-type="fig" rid="fig7">Figure 7A–C</xref> but showing decoder runs separated by paired visual stimuli. (<bold>A</bold>) Accuracy of discrimination between A<sub>R</sub> and A<sub>U</sub> stimuli across rules. (<bold>a</bold>) A<sub>R</sub>V<sub>R</sub> vs. A<sub>U</sub>V<sub>R</sub>: no difference across rules (all p≥0.36, all |<italic>Z</italic>|≤0.92, paired Wilcoxon signed rank (WSR) test, see <xref ref-type="supplementary-material" rid="fig7sdata3">Figure 7—source data 3</xref>). (<bold>b</bold>) A<sub>R</sub>V<sub>U</sub> vs. A<sub>U</sub>V<sub>U</sub>: no difference across rules (all p≥0.24, all |<italic>Z</italic>|≤1.17). Conventions as in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, and elsewhere. Scatter plots represent decoder accuracy from individual units; dashed lines show chance level (50%). Histograms show raw unit counts for A-rule/V-rule fold change. S=superficial; M=middle; D=deep. (<bold>B</bold>) Stimulus-spike information efficiency (bits/spike, calculation shown in A) for PSTH-based decoding increases for deep broad-spiking units during auditory attention. Conventions as in A. (<bold>a</bold>) A<sub>R</sub>V<sub>R</sub> vs. A<sub>U</sub>V<sub>R</sub>: deep group shows increased information efficiency in A-rule (deep BS: p=5.8e-4, <italic>Z</italic>=1.2, paired Wilcoxon signed-rank (WSR) test, p<italic>-</italic>values FDR-corrected; see <xref ref-type="supplementary-material" rid="fig7sdata4">Figure 7—source data 4</xref> for other groups) (<bold>b</bold>) A<sub>R</sub>V<sub>U</sub> vs. A<sub>U</sub>V<sub>U</sub>: deep BS group also shows increase in A-rule (p=0.036, <italic>Z</italic>=1.15).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig7-figsupp2-v2.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Deep auditory cortex (AC) A-rule information efficiency increases are driven by firing rate (FR) suppressed units.</title><p>(<bold>A</bold>) Analysis is restricted to unit group exhibiting information efficiency modulation: deep broad-spiking (BS) units (see <xref ref-type="fig" rid="fig7">Figure 7</xref>). (<bold>B</bold>) Decoder-based analyses are split into subpopulations showing increased stimulus responses in the A-rule (FR+; <italic>n</italic>=96, 39.7%) or decreased responses in the A-rule (FR-; <italic>n</italic>=146, 60.3%). (<bold>C</bold>) Restriction of analysis to FR+ units. (<bold>a</bold>) Example unit, decision stimulus-evoked peristimulus time histogram (PSTH) response during A-rule (orange) and V-rule (green). (<bold>b</bold>) PSTH-based decoder accuracy for FR+ group shows a significant increase in A-rule. Left, scatter showing accuracy across rules, points represent individual single units (SUs). Right, difference histograms. For succinctness, decoder accuracy shown is the mean of A<sub>R</sub>V<sub>R</sub> vs. A<sub>U</sub>V<sub>R</sub> and A<sub>R</sub>V<sub>U</sub> vs. A<sub>U</sub>V<sub>U</sub> decoding across rules. (<bold>c</bold>) No change is observed for mutual information (MI) efficiency across rules for FR+ deep BS units (conventions as in <bold>b</bold>). (<bold>D</bold>) Similar analyses for FR- units. (<bold>a</bold>) Example unit with reduced response in A-rule. (<bold>b</bold>) Decoder accuracy does not change across rules despite loss of spikes in the FR- subpopulation. (<bold>c</bold>) MI efficiency increases are driven by the FR- population, directly linking changes in information rate to changes in activity levels across attentional state. See text for statistics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig7-figsupp3-v2.tif"/></fig></fig-group><p>We found that task rule could be decoded at greater than chance levels from responses to all four AV stimuli, and at all depth and NS/BS groups, showing that attentional state modulates decision stimulus PSTH responses throughout AC (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>; <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref>). These comparisons suggest response modulation by task rule, but do not address how information processing changes <italic>across</italic> the rules. To test this, we used the decoder to compare accuracy in discriminating between responses to A<sub>R</sub>* (rewarded in A-rule) and responses to A<sub>U</sub>* (unrewarded in A-rule) bimodal stimuli across A-rule and V-rule conditions. This mimics the TC discrimination required by the mice during the A-rule. In both rules, classification accuracy for the auditory decision stimuli (A<sub>R</sub>*, A<sub>U</sub>*) was higher than chance for all depth and BS/NS groups (see scatter plots in <xref ref-type="fig" rid="fig7">Figure 7B</xref>; all p≤1.4e-05, all |<italic>Z</italic>|≥4.2, one-way WSR vs. chance [50%]; see <xref ref-type="supplementary-material" rid="fig7sdata2">Figure 7—source data 2A</xref> for stats). Sound classification accuracy (A<sub>R</sub>*, A<sub>U</sub>*) did not significantly differ <italic>across</italic> the A-rule and V-rule (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, A<sub>R</sub>* vs. A<sub>U</sub>* <italic>comparison across rules:</italic> all p≥0.17, all |<italic>Z</italic>|≤1.39, see <xref ref-type="supplementary-material" rid="fig7sdata3">Figure 7—source data 3A</xref> for full stats; paired WSR on decoder accuracy in A-rule vs. V-rule by depth and NS/BS groups). Despite a reduction in activity levels during auditory attention, there was no loss in decoder accuracy, suggesting a possible change in encoding efficiency.</p><p>Through analysis of all decoder runs, we found that classifier accuracy and raw information were indeed correlated with FR (accuracy: <italic>r</italic>(3001)=0.49, p=2.3e-180; MI: <italic>r</italic>(3001)=0.41, p=1.5e-123; Pearson’s correlation, all A<sub>R</sub>* vs. A<sub>U</sub>* decoder runs). Thus, normalizing information by mean joint per-trial spike rate for the two responses in each decode (bits/spike) provides insight into the efficiency with which spikes are used to represent stimuli. We found that this encoding efficiency measure increased by ~20% during the A-rule for deep-layer BS units (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, A<sub>R</sub>* vs. A<sub>U</sub>* <italic>comparison across rules:</italic> deep BS: p=2.9e-04, <italic>Z</italic>=–4.06, paired WSR, FDR-adjusted p<italic>-</italic>value; median FC: 1.19 [fold change: A-rule/V-rule]; V-rule: 0.15±0.13, A-rule: 0.19±0.19, mean bits/spike ± SDs, <italic>n</italic>=233; all other groups p≥0.40, all |<italic>Z</italic>|≤1.47; see <xref ref-type="supplementary-material" rid="fig7sdata4">Figure 7—source data 4A</xref> for full stats). No other unit subpopulations showed significant changes. Note that for clarity, the above results are presented as the mean of decoder comparisons A<sub>R</sub>V<sub>R</sub> vs. A<sub>U</sub>V<sub>R</sub> and A<sub>R</sub>V<sub>U</sub> vs. A<sub>U</sub>V<sub>U</sub>, thus collapsing visual stimulus identity. Analysis of these comparisons separately yields highly similar results (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>; <xref ref-type="supplementary-material" rid="fig7sdata2 fig7sdata3 fig7sdata4">Figure 7—source data 2–4</xref>), suggesting that visual stimulus identity does not contribute substantially to decoder accuracy or encoding efficiency at the level of group analysis.</p></sec><sec id="s2-7"><title>Receptive fields mapped during the ITI also show increased stimulus encoding efficiency</title><p>The analyses above revealed that auditory attention increased the per-spike encoding efficiency of task decision sounds. Does this effect of cross-modal attention switching generalize to encoding of sounds that were explicitly designed to be task-irrelevant? This helps determine whether attention observed here is specific to features of the auditory stream or broadly alters encoding of incoming auditory information. To address this, we tested whether information between STRFs derived from task-irrelevant ITI sounds and spike trains was modulated by attentional demands of the task. We restricted our analyses to only those units with STRFs passing the reliability criterion shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. To calculate STRF-spike train MI for each SU, we first calculated probability distributions of STRF-stimulus projection values for all stimulus time points (<italic>P(x</italic>)) and for those time points preceding a spike (<italic>P(x|spike</italic>); <xref ref-type="fig" rid="fig7">Figure 7D</xref>). Intuitively, these projection values reflect the similarity between a windowed stimulus segment at a given timepoint and the STRF. The divergence of the two projection distributions is captured in a spike-rate-normalized MI measure (bits/spike; encoding efficiency), which describes the reliability with which spikes are determined by stimulus features of the STRF (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). No differences in encoding efficiency between conditions were observed in the superficial or middle BS/NS groups, or the deep NS group. Instead, consistent with our earlier findings for decision stimuli, encoding efficiency showed a significant A-rule increase in the deep BS subpopulation (<xref ref-type="fig" rid="fig7">Figure 7F</xref>; deep BS: p=0.014, <italic>Z</italic>=–3.05, median FC: 1.25, <italic>n</italic>=50; paired WSR, FDR-adjusted p<italic>-</italic>value; FC: A-rule/V-rule; mean bits/spike ± SDs; all other groups p≥0.24, all |<italic>Z</italic>|≤1.66; see <xref ref-type="supplementary-material" rid="fig7sdata5">Figure 7—source data 5</xref> for full stats). This finding shows that during auditory attention, stimulus encoding is better described by a linear STRF filter and thus better tracks physical sound features. Furthermore, it suggests that increased encoding efficiency resulting from decreased spiking is a general effect of auditory attention in deep-layer BS units, regardless of the context-based behavioral relevance or learned valence of the sounds.</p></sec><sec id="s2-8"><title>Information encoding efficiency changes are driven by suppressed units</title><p>The increase in A-rule encoding efficiency and decrease in average FRs in deep AC led us to further explore the relationship between activity level and information changes. Specifically, we tested whether group-level information efficiency changes are driven by SUs with suppressed responses, and how the minority of units with increased A-rule FRs perform in the decoder. We therefore examined classifier accuracy and encoding efficiency for target and distractor (A<sub>R</sub>* vs. A<sub>U</sub>*) decoding separately for deep-layer BS units with increased and decreased FRs in the A-rule (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). We found that units with increased FRs (39%; <italic>n</italic>=96) exhibited a significant increase in A-rule decoding accuracy (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3C</xref>; p=0.0030, <italic>Z</italic>=–2.97, med. FC: 1.04, V-rule % correct: 66.4±15.5, A-rule: 69.5±15.5, <italic>n</italic>=96; paired WSR; FC: A-rule/V-rule; mean ± SDs), but no significant change in encoding efficiency (p=0.84, <italic>Z</italic>=0.2, V-rule bits/spike: 0.18±0.15, A-rule: 0.18±0.16). By contrast, units with suppressed FRs (60%; <italic>n</italic>=146) showed no significant change in decoding accuracy (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3D</xref>; p=0.44, <italic>Z</italic>=0.77, V-rule: 67.32±15.25, A-rule: 66.53±14.61), but a 44% increase in encoding efficiency (p=1.8e-07, <italic>Z</italic>=–5.22, median FC: 1.44, V-rule: 0.13±0.12, A-rule: 0.18±0.19; paired WSR). These results suggest that the minority of units that increase FRs in the A-rule perform marginally better at decoding the auditory stimulus, and that the units that decrease FRs drive the shift in encoding efficiency.</p></sec><sec id="s2-9"><title>Attention-related FR changes predict correct task performance</title><p>To ensure that mice were adequately engaged and attentive in the task, the analyses described above excluded any trials in which the incorrect behavioral response was made. However, an examination of these error trials, which may correlate with lapses in attention, could provide insight into the moment-to-moment behavioral relevance of the attentional effects described above. We have shown that attention to sound is marked by a net suppression of pre-stimulus and evoked FRs. We hypothesized that, if this attentional modulation is behaviorally meaningful, FRs preceding A-rule error trials may be more similar to sound-unattended V-rule trials than to A-rule correct trials. We addressed this possibility by comparing pre-stimulus FRs in error vs. correct trials (300 ms prior to stimulus onset; <xref ref-type="fig" rid="fig8">Figure 8B</xref>). Because misses were uncommon (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), we restricted our analysis to the comparison of FA and CR trials to allow for adequate sampling of each trial outcome. We included only behavior sessions with at least 10 FA and CR trials (A-rule and V-rule trials considered separately). This decreased unit sample sizes (<italic>n</italic>=234, 58 across all depth groups for BS, NS; min. group size = 9, 2 for BS, NS). Given the small sample of NS units and the likelihood of insufficient power, NS units were not included in this analysis. When considering BS units <italic>with increased FRs in the A-rule</italic>, we found no significant group-level difference between A-rule FA and CR trials at any cortical depth (<xref ref-type="fig" rid="fig8">Figure 8C</xref>; <xref ref-type="supplementary-material" rid="fig8sdata1">Figure 8—source data 1A</xref> for full stats; all p≥0.29, all |<italic>Z</italic>|≤1.43, paired WSR, FDR-adjusted p<italic>-</italic>values). However, deep cortical BS units <italic>with A-rule suppression</italic> showed significantly higher pre-stimulus FRs prior to A-rule FA trials than CR trials (deep BS [<italic>n</italic>=98]: mean FR difference between pre-stimulus FA and CR trials = 0.35 Hz, p=0.0098, <italic>Z</italic>=–3.15; paired WSR; other depth groups: p≥0.28, |<italic>Z</italic>|≤1.48; all p<italic>-</italic>values FDR-adjusted). This is unlikely to reflect a motor effect of higher FR before a lick, as it was specific to the A-rule: pre-stimulus FRs in A-rule-suppressed or A-rule-enhanced units did not differ between FA and CR trials in the V-rule (<xref ref-type="fig" rid="fig8">Figure 8C</xref>.c; <xref ref-type="supplementary-material" rid="fig8sdata1">Figure 8—source data 1B</xref>; paired WSR: all p≥0.68, all |<italic>Z</italic>|≤1.59, FDR-adjusted p<italic>-</italic>values). Together, these findings suggest that FR reductions typical of modality-selective attention directly relate to behavioral outcomes.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Attentionally suppressed units predict behavior performance during auditory attention.</title><p>(<bold>A</bold>) Summary of behavioral outcomes by session (<italic>n</italic>=23, 10 mice), organized by task stimulus. Bar sequence follows chronology of experiments. Error trials are predominantly false alarms (FAs). To allow sufficient trials for measurement of activity levels across behavioral outcomes, subsequent analysis focuses on analysis of FAs vs. correct rejects (CRs). (<bold>B</bold>) Example unit showing behavioral outcome- and rule-dependent firing rate (FR) modulation. Pre-stimulus FR analysis window (−0.3–0 s) shown in blue. (<bold>a</bold>) Pre-stimulus activity for A-rule FA trials (red) is elevated relative to CR trials (orange). (<bold>b</bold>) In the same unit, pre-stimulus activity is elevated in V-rule CR trials (green) relative to A-rule CR trials (orange). (<bold>C</bold>) Division of units into A-rule-suppressed and A-rule-enhanced groups reveals suppression of activity as a neural signature of correct task performance. (<bold>a</bold>) Broad-spiking (BS) units from sessions with ≥10 FA and CR trials are divided into A-rule suppressed and A-rule enhanced groups. (<bold>b</bold>) Deep units that are suppressed during auditory attention relative to visual show higher firing rates on A-rule error trials relative to correct trials (p=0.0098, paired Wilcoxon signed-rank test, false discovery rate [FDR]-adjusted for <italic>n</italic>=6 tests). Median of group indicated by black cross. No such trend exists for the A-rule-enhanced population. (<bold>c</bold>) Pre-stimulus activity does not predict V-rule behavioral outcomes in the same groups, suggesting that AC activity suppression is related to performance on sound but not visual stimulus discrimination (all p&gt;0.68, paired Wilcoxon signed-rank test, FDR-adjusted for <italic>n</italic>=6 tests).</p><p><supplementary-material id="fig8sdata1"><label>Figure 8—source data 1.</label><caption><title>Pre-stimulus firing rates (FRs) for false alarm vs correct reject trials.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-75839-fig8-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-fig8-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the present study, we recorded SU activity across AC layers in mice performing an AV rule-switching task. We compared responses evoked by identical stimuli under conditions of auditory or visual modality-selective attention. Attention to sound shifted AC stimulus representation by decreasing activity of untuned units and increasing encoding efficiency in the deep cortical laminae. Pre-stimulus activity was also reduced by auditory attention, which accounted for changes in stimulus-evoked responses. The effects of attention extended beyond the decision stimuli required to complete the task; responses to task-irrelevant receptive field mapping stimuli exhibited similar reductions in evoked activity and increases in encoding efficiency, suggesting that attention to sound induces a stimulus-general shift in processing. This attentional shift was behaviorally meaningful, with error trials in the A-rule predicted by higher FRs in the set of units that is suppressed under auditory attention. Taken together, these results show that attending to sound results in a general suppression of ongoing activity in AC, while retaining activity critical for sensory representation.</p><p>Attentional highlighting of behaviorally relevant signals may employ multiple mechanisms, including response enhancement or noise suppression. Feature selective attention studies have shown that FRs for neurons tuned to attended features are often increased, thereby increasing the reliability of the sensory cortical readout (<xref ref-type="bibr" rid="bib26">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib65">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib77">Reynolds and Chelazzi, 2004</xref>). Another mechanism which may act in tandem with response enhancement is the reduction of noise to improve encoding reliability. Noise reduction may act through decreased rates in pre-stimulus baseline activity (<xref ref-type="bibr" rid="bib16">Buran et al., 2014</xref>), reduced variance in single neuron rates (<xref ref-type="bibr" rid="bib64">Mitchell et al., 2007</xref>), or decreased correlations of noise across the population (<xref ref-type="bibr" rid="bib20">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib28">Downer et al., 2015</xref>). In the present study, we found no evidence for increased signal-to-noise ratio in the FR signal, as shown by the roughly equal stimulus response magnitudes across rules when normalizing for pre-stimulus rate. However, the timing of activity in AC is known to carry substantial information (<xref ref-type="bibr" rid="bib46">Hoglen et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Krishna and Semple, 2000</xref>; <xref ref-type="bibr" rid="bib61">Malone et al., 2007</xref>), which would not be captured by coarse rate estimations. By accounting for fine scale temporal patterns with a PSTH-based pattern classifier and analysis of stimulus-STRF selectivity, we show that decreased ongoing activity and a concomitant increase in encoding efficiency at the group level provides an additional mechanism for attentional noise reduction, perhaps refining the stimulus-encoding portion of the neural signal for readout in downstream brain areas.</p><p>Previous studies of behavioral state-dependent state changes in auditory processing have typically compared task-engaged and passive sound processing. While this paradigm does not specifically isolate the effects of attention due to confounds of arousal, attention, reward expectation, and motor activity (<xref ref-type="bibr" rid="bib83">Saderi et al., 2021</xref>), it has provided valuable insight into the dependence of sensory processing on task-engaged behavioral states. Consistent with our findings, this work has shown that AC stimulus-evoked spiking responses are predominantly suppressed during self-initiated task engagement when compared to passive listening (<xref ref-type="bibr" rid="bib7">Bagur et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Carcea et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Kuchibhotla et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Otazu et al., 2009</xref>). Activity levels preceding a stimulus may also decrease (<xref ref-type="bibr" rid="bib16">Buran et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Carcea et al., 2017</xref>), although some studies in AC do not show this effect (<xref ref-type="bibr" rid="bib70">Otazu et al., 2009</xref>). Reductions of pre-stimulus activity during task engagement have also been observed in rat gustatory cortex (<xref ref-type="bibr" rid="bib99">Yoshida and Katz, 2011</xref>) and monkey visual cortex (<xref ref-type="bibr" rid="bib11">Bisley and Goldberg, 2003</xref>; <xref ref-type="bibr" rid="bib22">Cox et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Herrington and Assad, 2010</xref>; <xref ref-type="bibr" rid="bib87">Sato and Schall, 2001</xref>).</p><p>Neuronal stimulus preferences relative to a target have been shown to determine the degree of attentional modulation, such that stimulus-evoked responses for attended features are generally enhanced but can also be suppressed for features outside of the receptive field (<xref ref-type="bibr" rid="bib77">Reynolds and Chelazzi, 2004</xref>). Here, we find that frequency preferences of units with STRF tuning do not appear to determine suppression or enhancement within the task, but critically we also find that the bulk of units with STRF tuning exhibit a preference for frequencies near the rewarded TC (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). This is consistent with a body of work from Shamma, Fritz, and colleagues showing that engagement in an auditory discrimination task rapidly shifts AC receptive fields to enhance frequency representation of behaviorally relevant stimuli (<xref ref-type="bibr" rid="bib6">Atiani et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Fritz et al., 2005</xref>; <xref ref-type="bibr" rid="bib35">Fritz et al., 2003</xref>; <xref ref-type="bibr" rid="bib98">Yin et al., 2014</xref>). In our task, mice were trained for multiple months prior to physiological recordings, and the TC frequencies of rewarded and unrewarded stimuli were held consistent for each animal. As such, spectral representation in the AC of our highly trained mice is biased toward task-relevant stimuli. Speculatively, it is possible that tuning-dependent attentional modulation may occur in earlier stages of task acquisition, but that the substantial reconfiguration of sound processing tailored to the task alters its expression after training. The distribution of preferred frequencies also does not shift between auditory and visual rules, suggesting that attending to visual stimuli does not place plasticity-inducing demands on AC frequency representation. Instead, we find that units without STRF tuning drive the reduction in neural activity during auditory attention. An important caveat is that our STRF-based approach is only one way to determine AC tuning, and other stimulus and analysis methods may reveal additional tuning preferences. Nevertheless, we believe that this method provides a useful classification for degree of tuning. This result is also consistent with our information theoretic analyses in that both suggest that attention to sound may selectively remove spikes that are minimally sound-driven.</p><p>As in previous studies, attention-related modulation was not uniformly expressed across cortical depths and neuron types. Changes in both FR and encoding efficiency were most prominent in deep-layer neurons. These findings extend several previous studies reporting larger effects of attention in infragranular LFP and multi-unit activity (MUA) (<xref ref-type="bibr" rid="bib69">O’Connell et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Zempeltzi et al., 2020</xref>). These physiological outcomes are consistent with anatomical work suggesting that top-down modulatory signals arrive primarily in the supragranular and infragranular layers (<xref ref-type="bibr" rid="bib32">Felleman and Van Essen, 1991</xref>). As the main cortical output layer, information shifts in the infragranular population would differentially influence subcortical sites and other cortical regions (<xref ref-type="bibr" rid="bib85">Salin and Bullier, 1995</xref>). One important caveat is that superficial AC is known to have lower spontaneous and evoked FRs than deeper cortex (e.g., <xref ref-type="fig" rid="fig4">Figure 4C</xref>; <xref ref-type="bibr" rid="bib19">Christianson et al., 2011</xref>; <xref ref-type="bibr" rid="bib84">Sakata and Harris, 2009</xref>), which may have made it more difficult for us to observe statistically significant attention-related effects. Furthermore, although we tried to minimize neural tissue damage through technical considerations such as using a slow probe insertion speed (<xref ref-type="bibr" rid="bib33">Fiáth et al., 2019</xref>), the superficial layers likely sustain the greatest level of damage when the probe is inserted to span the full cortical depth. Despite these factors, we were able to isolate a reasonably large sample size of responsive neurons in superficial cortex from successful behavior sessions (<italic>n</italic>=119 units, of which 57% were stimulus-responsive). Nevertheless, we cannot rule out whether the absence of observed attentional modulation at superficial depths may have been due to experimental limitations such as the comparatively small sample size. Future work employing imaging techniques to target superficial neurons may help resolve this.</p><p>Previous studies have reported larger effects of task engagement or attention in inhibitory interneurons (<xref ref-type="bibr" rid="bib51">Kuchibhotla et al., 2017</xref>; <xref ref-type="bibr" rid="bib64">Mitchell et al., 2007</xref>). As such, attention-related reduction of activity could be sustained by inhibitory network drive. Our approach of dividing activity into BS and NS did not suggest a general increase in NS activity during auditory attention. However, we observed heterogenous types of modulation; in many units, NS activity decreased during auditory attention, but in a smaller group, there was a significant increase. An important caveat is that the BS/NS distinction is an imperfect approximation of excitatory/inhibitory activity, with many inhibitory cell types presenting a BS waveform phenotype (e.g., somatostatin-positive interneurons; <xref ref-type="bibr" rid="bib58">Li et al., 2015</xref>). An alternative mechanism is that excitatory drive is decreased during auditory attention. These two proposed mechanisms – increased inhibitory tone and decreased excitatory drive – are not mutually exclusive.</p><p>Our findings suggest that attentional selection is achieved by removal of a noise background on which sound stimulus-encoding activity sits. This is in line with an influential theory of cortical attention that posits that spontaneous activity fluctuations partly reflect internal processes such as mental imagery or memory recall, in contrast with activity that arises from external sensory stimulation (<xref ref-type="bibr" rid="bib41">Harris and Thiele, 2011</xref>). In this model, attention suppresses internally generated spontaneous activity to favor the processing of behaviorally relevant external stimulation. The work presented here offers multiple pieces of evidence in favor of this theory. Auditory attention suppresses activity in untuned units, affecting both pre-stimulus and stimulus-evoked activity. This activity reduction does not alter stimulus-spike train decoding accuracy, but instead increases stimulus encoding efficiency and preserves stimulus representation.</p><p>In summary, we demonstrate a novel connection between attention-induced shifts in activity levels and stimulus encoding in early sensory cortex, which are directly related to behavioral outcomes. Previous research suggests that such effects reflect top-down control by executive networks comprising frontal, parietal, thalamic, and striatal areas (<xref ref-type="bibr" rid="bib21">Cools et al., 2004</xref>; <xref ref-type="bibr" rid="bib23">Crone et al., 2006</xref>; <xref ref-type="bibr" rid="bib59">Licata et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Rikhye et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Rougier et al., 2005</xref>; <xref ref-type="bibr" rid="bib93">Toth and Assad, 2002</xref>; <xref ref-type="bibr" rid="bib95">Wimmer et al., 2015</xref>). These networks may act as a context-dependent switch, routing attentional modulatory feedback to the sensory systems. In the present study, we provide evidence that such modulation specifically suppresses stimulus-irrelevant spiking, thus enhancing encoding efficiency in deep AC neurons.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>All experiments were approved by the Institutional Animal Care and Use Committee at the University of California, San Francisco. Twenty-seven C57BL/6 background male mice were surgically implanted with a headpost and began behavioral training, of which 10 completed the training and successfully performed the task during physiology recording sessions. All mice began the experiment between ages P56 and P84. Mice used in this report expressed optogenetic effectors in various subsets of interneurons, which we intended to use for optogenetic identification of cells (<xref ref-type="bibr" rid="bib60">Lima et al., 2009</xref>; analysis not included here). These mice were generated by crossing an interneuron subpopulation-specific Cre driver line (PV-Cre JAX Stock Nr. 012358; Sst-Cre: JAX Stock Nr. 013044) with either the Ai32 strain (JAX Stock Nr. 012569), expressing Cre-dependent eYFP-tagged channelrhodopsin-2, or the Ai40 strain (JAX Stock Nr. 021188), expressing Cre-dependent eGFP-tagged archaerhodopsin-3. Of the 10 behavior mice included in this report, 6 were Ai32/Sst-Cre, 3 were Ai32/PV-Cre, and 1 was Ai40/Sst-Cre. In most experiments (<italic>n</italic>=21 recordings), brief, low-level optogenetic pulses during the ITI of the task were used to identify opsin-expressing neurons (&lt;0.3 mW light; 5 light pulses of 10 ms duration, every ~1.5 min); these analyses are outside of the scope of this report. The optogenetic stimulation protocol was consistent through A- and V-rules of the task. Unit stimulus response FRs and behavioral response error rates were not statistically different between trials immediately after optogenetic pulses and stimulus-matched trials preceding the pulses.</p><p>All mice were housed in groups of 2–5 for the duration of the behavioral training until the craniotomy. Post-craniotomy and during physiology recordings, mice were housed singly (up to 6 days) to protect the surgical site. Mice were kept in a 12 hr/12 hr reversed dark/light cycle. All training occurred during the dark period, when mice show increased activity and behavioral task performance (<xref ref-type="bibr" rid="bib80">Roedel et al., 2006</xref>).</p></sec><sec id="s4-2"><title>AV rule-switching behavior task</title><p>Adult mice (&gt;P56) were trained on an AV go/no-go rule-switching behavior task. In this task, mice were positioned on a floating spherical treadmill in front of a monitor and a speaker, and an optical computer mouse recorded treadmill movement. Mice licked to receive a reward depending on auditory, visual, or AV stimulus presentation (‘decision’ stimuli, either ‘target’ or ‘distractor’), but the modality predictive of the reward changed partway through the behavioral session. Each session would start with a unimodal go/no-go block, in which a series of auditory (A<sub>R</sub>, A<sub>U</sub>; 17 or 8 kHz TC) or visual (V<sub>R</sub>, V<sub>U</sub>; upward or rightward moving gratings) stimuli was presented. After stimulus presentation, mice signaled choice by either licking a spout in front of the mouth or withholding licking. Licking at the target unimodal stimulus would trigger a water reward, while licking at the distractor would trigger a short dark timeout. After a fixed number of unimodal trials, the stimuli would become AV, but the rule for which stimulus modality predicted reward would carry over from the unimodal block. All four stimulus combinations (A<sub>R</sub>V<sub>R</sub>, A<sub>R</sub>V<sub>U</sub>, A<sub>U</sub>V<sub>R</sub>, A<sub>U</sub>V<sub>U</sub>) would be presented in the AV block, such that two AV combinations would be target stimuli and two would be distractor. Then, after completing a fixed number of trials in the AV block, the task using the rule of the opposite modality would begin; a unimodal block with the other modality would start, followed by a second AV block using the rule from the preceding unimodal block. For any mouse, the stimuli predictive of the reward in each rule was kept constant across days and training sessions (e.g., a 17 kHz TC would always predict a reward in the A-rule, and a rightward grating would always predict a reward in the V-rule).</p><p>The task was self-paced using a virtual foraging approach, in which mouse locomotion (measured through treadmill rotation) would cause a track of randomly placed dots on the screen to move downward. After a randomly varied virtual distance, a decision stimulus would be presented, at which point the mouse would lick or withhold licking to signal choice. For receptive field mapping during physiology experiments, an RDS stimulus was presented in-between decision stimuli, during the inter-trial track portion. Stimuli are detailed below.</p></sec><sec id="s4-3"><title>Behavior training and apparatus</title><p>Prior to any training, mice were surgically implanted with a stainless steel headplate, used both for head fixation during the task and for physiology recordings after the task was learned (surgical methods described below). Three days post-implant, mice began a water restriction protocol based on previously published guidelines (<xref ref-type="bibr" rid="bib39">Guo et al., 2014</xref>). Throughout the course of training, mice received a minimum water amount of 25 mL/kg/day, based on weight at time of surgical implant. After recovery from surgery, mice were given ~7 days to adjust to water restriction. Then, mice were head fixed and habituated to the floating treadmill for 15–30 min daily sessions with no stimulus presentation for 2–3 days. After mice appeared comfortable on the treadmill, a phased behavioral task training regimen began. Mice were trained once daily for ~6 days per week. On day 1, mice were introduced to an auditory-only (A-only) stimulus training version of the task in which A<sub>R</sub> (‘target’/‘rewarded’) or A<sub>U</sub> (‘distractor’/‘unrewarded’) stimuli were presented, and a reward would be automatically administered shortly after the onset of A<sub>R</sub>. Next, the mice were put on an operant version of the A-only task, which required licking any time after the onset of A<sub>R</sub> to receive a reward and withholding of licking during A<sub>U</sub> to avoid a dark timeout punishment. Mice achieved proficiency, defined as 2 or more consecutive days of sensitivity index <italic>d</italic>’&gt;1.5 (see <italic>Data analysis</italic> for calculation), on the A-only task after 11.0±4.7 days after start of training (median ± SD, <italic>n</italic>=10 successful mice). Then, a similar training structure was repeated for the visual task: V-only stimulus training with automatic rewards for V<sub>R</sub>, but not V<sub>U</sub>, followed by an operant version of the visual task requiring licks for rewards (median time to proficiency: 26.0±7.2 days after start). After learning the tasks for each modality separately, mice were introduced to an auditory-AV (A-AV) version, in which the rule from the auditory stimulus carried over to the AV block. This was intermixed with training days on a visual-AV (V-AV) version of the task. Number of training days on A-AV or V-AV were decided based on prior performance, with extra training given as needed. Mice were considered proficient at this stage after performing with <italic>d</italic>’&gt;1.5 on each rule (A-AV; V-AV) on 2 consecutive days (median time to proficiency: 40.0±15.8 days after start). Finally, the full rule-switching task was introduced (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), generally alternating between days of V-rule-first and the A-rule-first task sequences but allocating more training days to task orders as needed. Because physiology recordings were acute and strictly limited to 6 days after craniotomy, we set a greater threshold for expert-level performance on the full task before advancing to physiology: 3 consecutive days of <italic>d</italic>’&gt;2.5 (median time to expertise: 90.5±31.8 days). Care was taken to train each mouse at a roughly consistent time of day (no more than ~1–2 hr day-to-day variation). During expert-level task performance, mice typically completed 260–300 trials in a daily session (30 A-only; 100–120 A-AV; 30 V-only; 100–120 V-AV).</p><p>The behavior training setup was controlled by two computers: a behavior monitoring and reward control PC (OptiPlex 7040 MT, Dell) and a dedicated stimulus presentation machine running Mac OS X (Mac Mini, Apple). Stimulus presentation was controlled with MATLAB using custom software (<ext-link ext-link-type="uri" xlink:href="https://github.com/HasenstaubLab/AVtrainer-stim/tree/main/demo">https://github.com/HasenstaubLab/AVtrainer-stim/tree/main/demo</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:754af450b79f866e8a9ba8ae6d57a6fc041543f1;origin=https://github.com/HasenstaubLab/AVtrainer-stim;visit=swh:1:snp:a39333cd71789512ea8bbbb94d6d6a549bea79e8;anchor=swh:1:rev:737720f41fd5302b90fd5e60a10822270381818c;path=/demo/">swh:1:rev:737720f41fd5302b90fd5e60a10822270381818c;path=/demo/</ext-link>; <xref ref-type="bibr" rid="bib67">Morrill et al., 2022</xref>), and inter-machine communication used the ZeroMQ protocol. Auditory and visual stimuli were generated and presented using the Psychophysics Toolbox Version 3 (<xref ref-type="bibr" rid="bib49">Kleiner et al., 2007</xref>). Water rewards were administered using a programmable syringe pump (NE-500, New Era Pump Systems, Farmingdale, NY), positioned outside of the sound-attenuating recording chamber. Early in training, water reward volume was set at 0.01 mL per correct response, but over training the reward volume was gradually decreased to 0.006 mL to achieve greater trial counts. Licking events were recorded using a custom photobeam-based lickometer circuit based on plans provided by Evan Remington (Xiaoqin Wang Lab, Johns Hopkins University). Licks were registered when an IR photobeam positioned in front of the lick tube was broken, queried at a sample rate of 100 Hz by an Arduino Uno microcontroller (Arduino, LLC).</p></sec><sec id="s4-4"><title>In vivo awake recordings during behavior</title><p>Animals in this experiment underwent two surgeries: first, before training a surgery to implant a custom steel headplate over the temporal skull using dental cement was conducted. The animal was anesthetized using isoflurane and a headplate was implanted over AC, ~2.5 mm posterior to bregma and under the squamosal ridge, to allow for physiology recordings after achieving task expertise. When mice completed the training regimen outlined above, a craniotomy surgery was performed. The animal was again anesthetized using isoflurane and an elliptical opening (0.75 mm wide × 1.5 mm long) was made in the skull over AC using a dental drill. This opening was promptly covered with silicone elastomer (Kwik-Cast, World Precision Instruments), and the animal was allowed to recover overnight. The following day, the animal was affixed by its headplate over the treadmill inside of a sound-attenuating recording chamber, the silicone plug over the craniotomy was removed, and the craniotomy was flushed with saline. A silver chloride ground wire was placed into the craniotomy well at a safe distance from the exposed brain. A 64-channel linear probe (20 µm site spacing; Cambridge Neurotech, Cambridge, UK) was slowly inserted in the brain using a motorized microdrive (FHC, Bowdoin, ME) at an approximate rate of ~1 μm/s (<xref ref-type="bibr" rid="bib33">Fiáth et al., 2019</xref>). After reaching the desired depth, the brain was allowed to settle for 10 min, after which the water spout, lickometer, visual stimulus delivery monitor, and speaker were positioned in front of the mouse, and the behavior session commenced. Behavior sessions were sometimes stopped early and restarted due to poor performance. In approximately half of behavior-physiology sessions (13 of 23 successful recordings), the task was stopped due to low performance after the rule transition and restarted at the beginning (unimodal block) of the second rule. To control for possible effects of task order, attempts were made to counterbalance recordings from A-rule first (15) and V-rule first (8) behavior sessions.</p><p>After completion of the behavior task, the water spout and lickometer were removed, and a series of auditory and/or visual passive experiments were conducted in order to characterize the response properties of the recording site. All stimuli were presented with the auditory and visual stimulation apparatus described above. Following completion of these experiments, the probe was slowly removed, and the brain was covered with a thin layer of freshly mixed 2.5% agarose in saline, followed by a layer of silicone elastomer. The animal was returned to its home cage, and the following day the physiological recording process was repeated. Recordings were made for up to 6 days after the craniotomy. The neural signal acquisition system consisted of an Intan RHD2000 recording board and an RHD2164 amplifier (Intan Technologies), sampling at 30 kHz.</p></sec><sec id="s4-5"><title>Auditory and visual stimuli</title><p>In-task auditory decision stimuli were 1 s TCs, consisting of 50 ms tone pips overlapping by 25 ms, with frequencies in a 1-octave band around either 17 or 8 kHz. TCs were frozen for the duration of the task, so that each mouse always heard the same pip sequences, allowing for direct comparisons of sound-evoked neural responses across rules without concern that stimulus peculiarities may be driving observed differences. TCs were presented at 60 dB SPL. Visual decision stimuli consisted of a circular moving grating stimulus (33° diameter subtended visual space), which appeared at the center of the screen for 1 s (coincident with TC stimulus during bimodal presentation). Gratings moved either upward or rightward with a 4 Hz temporal frequency, 0.09 cycles/degree spatial frequency at 50% contrast. In-between decision stimulus presentations, an RDS stimulus was presented for receptive field mapping (<xref ref-type="bibr" rid="bib10">Bigelow et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Gourévitch et al., 2015</xref>). The RDS comprised two uncorrelated random sweeps that varied continuously and smoothly between 4 and 64 kHz, with a maximum sweep modulation frequency of 20 Hz. RDS stimuli were presented at 50 dB SPL.</p><p>After the behavior task, passive auditory search stimuli (pure tones, click trains) were presented to characterize response properties of the electrode channel. Click trains consisted of broadband 5 ms white noise pulses, presented at 20 Hz for 500 ms duration. Pure tone stimuli consisted of 100 ms tones of varied frequencies (4–64 kHz, 0.2 octave spacing) and sound attenuation levels (30–60 dB in 5 dB linear steps), with an interstimulus interval of 500 ms.</p><p>Auditory stimuli were presented from a free-field electrostatic speaker (ES1, Tucker-Davis Technologies) driven by an external soundcard (Quad-Capture or Octa-Capture, Roland) sampling at 192 kHz. Sound levels were calibrated using a Brüel &amp; Kjær model 2209 meter and a model 4939 microphone. Visual stimuli were presented on a 19-inch LCD monitor with a 60 Hz refresh rate (Asus VW199), positioned 25 cm in front of the mouse and centered horizontally and vertically on the eyes of the mouse. Monitor luminance was calibrated to 25 cd/m<sup>2</sup> for a gray screen, measured at approximate eye level for the mouse.</p></sec><sec id="s4-6"><title>Data analysis</title><sec id="s4-6-1"><title>Behavioral performance</title><p>Task performance was evaluated by calculation of the <italic>d’</italic> sensitivity index:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>H</italic> is hit rate and <italic>F</italic> is false alarm rate, and <italic>Z</italic> is the inverse normal transform. Because this transform is undefined for values of 0 or 1 and hit rates of 1 commonly occurred in this study, we employed the log-linear transformation, a standard method for correction of extreme proportions, for all calculations of <italic>d’</italic> (<xref ref-type="bibr" rid="bib42">Hautus, 1995</xref>). In this correction, a value of 0.5 is added to all elements of the 2×2 contingency table that defines performance such that:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>FA</italic> is the false alarm count and <italic>CR</italic> is the correct reject count. To ensure that mice properly transitioned between task rules, <italic>d’</italic> values were calculated separately for responses in the A-rule and the V-rule. Behavioral sessions during physiological recording with <italic>d’</italic>&lt;1.5 in either rule were excluded from analyses, as were any sessions with an FAR &gt;0.5 to stimuli with conflicting reward valances across rules: A<sub>U</sub>V<sub>R</sub> in A-rule or A<sub>R</sub>V<sub>U</sub> in V-rule (<italic>n</italic>=23 successful sessions, <italic>n</italic>=10 mice; 1 session excluded due to recording artifact, see below).</p></sec><sec id="s4-6-2"><title>Spike sorting and unit stability evaluation</title><p>Spikes were assigned to unit clusters using KiloSort2 (KS2; <xref ref-type="bibr" rid="bib71">Pachitariu et al., 2016</xref>). Clusters were first evaluated for isolation quality through the automated KS2 unit classification algorithm and then with a custom MATLAB interface. In this second step, clusters with non-neuronal waveforms or 2 ms refractory period violations &gt;0.5% were removed from analysis (<xref ref-type="bibr" rid="bib52">Laboy-Juárez et al., 2019</xref>; <xref ref-type="bibr" rid="bib91">Sukiban et al., 2019</xref>). To evaluate stability, activity for each unit was plotted for the recording duration as a raster and binned spike counts (2 min bins) and manually examined for periods with a substantial dropoff in FR (periods flagged for instability: 88 ± 10% [mean ± SD] decrease in FR from median activity level). Flagged unstable periods were marked and removed from analysis (101/742 SUs with flagged durations &gt;10% of recording time). One session meeting behavior performance criteria was excluded due to a high degree of electrical noise contamination.</p></sec><sec id="s4-6-3"><title>Classification of units by depth and waveform shape</title><p>Probes with electrode spans of 1260 µm were used, allowing for channels below and above AC. During recording, the probe was lowered to a point where several channels showed a prominent drop in field potential amplitude and spiking activity, indicating penetration into the white matter (<xref ref-type="bibr" rid="bib57">Land et al., 2013</xref>). After behavior sessions, a set of auditory and visual stimulation protocols was used to map response properties of each electrode site, and MUA responses were analyzed. Here, we define MUA as threshold crossings of 4.5 SD above a moving window threshold applied to each channel. Analysis of MUA was restricted to site characterization and is not included in the main results. We analyzed each tone or click PSTH for reliable responses, which we defined as trial-to-trial similarity of p&lt;0.01 (<xref ref-type="bibr" rid="bib31">Escabí et al., 2014</xref>). We designated the deepest channel with a reliable MUA sound response of any magnitude as the deep cortex-white matter border. Limited somatic spiking in the top layer of cortex prevented the use of MUA as a reliable marker for the superficial cortex-pia border (<xref ref-type="bibr" rid="bib89">Senzai et al., 2019</xref>), so we instead relied on an LFP-based measure. To define the top border of cortex, the maximum spontaneous LFP (1–300 Hz) amplitude of a 10 s snippet from each channel was plotted, and the channel at which LFP amplitude dropped off to the approximate probe-wise noise floor (i.e., minimum LFP amplitude) was considered the top channel in cortex (<xref ref-type="fig" rid="fig3">Figure 3B</xref>.c). These measures were confirmed histologically through Di-I probe marking experiments with a separate group of untrained mice; histology methods described below and elsewhere (<xref ref-type="bibr" rid="bib66">Morrill and Hasenstaub, 2018</xref>). Marking the top and bottom cortical borders generated a span of channels putatively within AC. This span was used to divide channels into superficial, middle, and deep groups, based on measurements of the fraction of cortex attributed to supragranular (layers 1–3), granular (layer 4), and infragranular (layers 5–6) in the mouse AC (Allen Institute Mouse Brain Atlas; <ext-link ext-link-type="uri" xlink:href="https://mouse.brain-map.org/">https://mouse.brain-map.org/</ext-link>). SUs were assigned the fractional depth of the channel on which the largest magnitude waveform was recorded.</p><p>Clusters were also classified into BS (putatively excitatory) and NS (putatively fast-spiking inhibitory) units on the basis of the bimodal distribution of waveform peak-trough durations (<xref ref-type="fig" rid="fig3">Figure 3D</xref><bold>;</bold> NS/BS transition boundary = 0.6 ms). From sessions with successful behavior, we recorded 742 SUs from all cortical depths, comprising 17.5% (130) NS units and 82.5% (612) BS units.</p></sec><sec id="s4-6-4"><title>FR analysis and trial filters</title><p>To compare FR responses to stimuli across task rules and to the receptive field mapping stimulus, we measured FR in the first 300 ms post-stimulus onset. Only units with nonzero FRs in both rules were included. To ensure that measurements were capturing periods of task engagement, all trials with incorrect responses (misses and FAs) were excluded from all decision-stimulus analyses, with the exception of those shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>. We also excluded trials with recorded licks earlier than the 300 ms post-stimulus onset, or in the 500 ms pre-stimulus onset. Given these filters, analyses were restricted to units present in the recording during at least 10 trials (correct behavioral choice and without ‘early licks’) for each stimulus type.</p></sec><sec id="s4-6-5"><title>PSTH-based Euclidean distance decoding</title><p>A PSTH-based decoder was used to compute the MI between spike trains and stimulus identity (<xref ref-type="fig" rid="fig7">Figure 7A</xref>; <xref ref-type="bibr" rid="bib34">Foffani and Moxon, 2004</xref>; <xref ref-type="bibr" rid="bib46">Hoglen et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Malone et al., 2007</xref>). In this method, two or more responses are compared by generating template PSTHs by removing one test trial. This test trial response is also binned into a single-trial PSTH, and then classified as belonging to the nearest template in <italic>n-</italic>dimensional Euclidean space, where <italic>n</italic> is the number of PSTH bins. More formally, the nearest template is that which minimizes the Euclidean norm between test and template vectors (PSTHs). This process is then repeated for all trials comprising the template PSTHs. Decoding accuracy is the percentage of trial responses that are correctly assigned to the stimuli that elicited them. MI is calculated from a confusion matrix of classifications as follows:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>X</italic> is the decoder prediction, <italic>Y</italic> is the actual, <italic>P(X<sub>i</sub>Y<sub>j</sub></italic>) represents the value of the (<italic>i</italic>, <italic>j</italic>) element of the confusion matrix, and <italic>P(X<sub>i</sub></italic>) and <italic>P(Y<sub>j</sub></italic>) are sums on the marginals. This yields a value of MI in bits. To measure encoding efficiency (bits/spike), we normalized MI by the joint mean spikes per trial of the responses submitted to the decoder (<xref ref-type="bibr" rid="bib9">Bigelow et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Buracas et al., 1998</xref>; <xref ref-type="bibr" rid="bib100">Zador, 1998</xref>).</p><p>For consistency with FR analyses, a time window of 0–300 ms, where stimulus onset is 0, was chosen for decoding analysis. A PSTH binwidth of 30 ms was chosen based on optimal binwidth calculations for mouse AC using the same decoding method (<xref ref-type="bibr" rid="bib46">Hoglen et al., 2018</xref>). To filter out units with low responsiveness to any of the stimuli in a given decoding analysis, we required a minimum FR of 1 Hz during the 0–300 ms window in both stimulus conditions. As such, unit sets may differ between each decoding analysis due to units that were responsive to one set of stimuli but unresponsive to others.</p></sec><sec id="s4-6-6"><title>STRF analysis</title><p>To test whether task rule modulates auditory receptive fields, we presented an RDS stimulus (described in <italic>Auditory and visual stimuli</italic>) in-between trials for durations of ~1–15 s, depending on rate of task progression. Different randomly generated RDS segments were presented in each ITI, and STRFs were generated separately for each rule. Because total RDS duration varied between the A-rule and the V-rule in a single session, we equated presentation time across rules by truncating the segments of the rule with greater RDS time (presentation time in each rule: 6.8±2.6 min [mean ± SD]; <italic>n</italic>=23 sessions). This ensured that different stimulus presentation times did not bias STRF estimation. The first 200 ms of RDS response was dropped from all STRF analyses to minimize bias from onset transients. SU activity during these short RDS segments was used to generate STRFs for each segment using standard reverse correlation techniques (<xref ref-type="bibr" rid="bib1">Aertsen and Johannesma, 1981</xref>; <xref ref-type="bibr" rid="bib25">de Boer, 1968</xref>; <xref ref-type="bibr" rid="bib38">Gourévitch et al., 2015</xref>). In brief, the spike-triggered average was calculated by summing all stimulus segments that preceded spikes using a window of 200 ms before and 50 ms after each spike. The choice of 200 ms prior to each spike reflects the upper limit of temporal integration times of auditory cortical neurons (<xref ref-type="bibr" rid="bib5">Atencio and Schreiner, 2013</xref>), and the 50 ms post-spike time was included to estimate acausal values, that is, those that would be expected by chance given the stimulus and spike train statistics (<xref ref-type="bibr" rid="bib38">Gourévitch et al., 2015</xref>). STRFs were transformed into units of FR (Hz) using standard methods discussed elsewhere (<xref ref-type="bibr" rid="bib82">Rutkowski et al., 2002</xref>). Units with poorly defined STRFs were filtered out using a trial-to-trial correlation metric (<xref ref-type="bibr" rid="bib31">Escabí et al., 2014</xref>): STRF segments were randomly divided into two halves, re-averaged separately, and a correlation value was calculated for the two STRFs. This process was then repeated 1000 times, and the mean of correlations defined the reliability value for each STRF. We compared the mean observed STRF reliability to a null distribution of reliabilities, generated by repeating the procedure on null STRFs made from circularly shuffled spike trains (preserving spike count and interspike interval but breaking the timing relationship between spikes and stimulus). A p-value was calculated as the fraction of the null STRF reliabilities greater than the mean observed STRF reliability, and STRFs with p&lt;0.05 in either rule were included in subsequent analyses. Any STRFs from units with greater than 10% of recording duration marked as unstable were removed from analysis.</p><p>MI between a spike train and an STRF was measured as the divergence of two distributions: one reflecting the similarity of the windowed stimulus segments (RDS) preceding a spike and the STRF, and the other reflecting the similarity of all possible windowed stimulus segments and the STRF, regardless of whether a spike occurred (<xref ref-type="fig" rid="fig7">Figure 7D</xref>; <xref ref-type="bibr" rid="bib3">Atencio et al., 2008</xref>; <xref ref-type="bibr" rid="bib4">Atencio and Schreiner, 2012</xref>; <xref ref-type="bibr" rid="bib30">Escabi and Schreiner, 2002</xref>). Stimulus-STRF similarity was defined as the inner product of the STRF and the stimulus segment of equivalent dimensions, with higher values reflecting closer matches between the STRF and stimulus. The distribution <italic>P(z|spike</italic>) was generated from <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>s</italic> represents all RDS stimulus segments that preceded a spike. Then the distribution <italic>P(z</italic>) was made from similarity calculations of all possible windowed RDS segments and the STRF. The mean <italic>μ</italic> and the standard deviation (SD) <italic>σ</italic> of <italic>P(z</italic>) were calculated, and the distributions were transformed into units of SD: <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, yielding distributions of <italic>P(x|spike</italic>) and <italic>P(x</italic>) expressed in units of SD.</p><p>Using the distributions described above, a spike count-normalized measure of MI between the calculated STRF and the spike train can be calculated as:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We used this value to compare how well STRFs from A-rule and V-rule ITIs predict a spike train, and thus whether activity in each attentional condition is well described by this canonical filter model.</p></sec></sec><sec id="s4-7"><title>Statistics</title><p>All statistical calculations were performed in MATLAB r2019a and its Statistics and Machine Learning Toolbox, V11.5. For group comparisons of SU responses across task rules, paired WSR tests were used, unless otherwise noted. Because tests were performed separately on each depth and spike waveform subpopulation, the Benjamini-Hochberg FDR procedure was used to correct for multiple comparisons, typically across <italic>n</italic>=6 comparisons (three depth groups, two spike waveform groups; <xref ref-type="bibr" rid="bib8">Benjamini and Hochberg, 1995</xref>). This method relies on controlling the Type I error rate (here, q=0.05), providing increased power over typical family-wise error rate controls. To determine if individual SUs were significantly modulated by rule, an unpaired Student’s t-test on FR was used with a threshold of p&lt;0.01. Descriptive statistics reported in text are mean ± standard deviation (SD), unless otherwise noted. Fractional change values between task rules are reported as the median of the A-rule/V-rule. All other statistical tests are described in Results. Sample sizes (<italic>n</italic>) are indicated for each comparison in Results or source data files.</p></sec><sec id="s4-8"><title>Histological verification of depth measurement</title><p>To test the accuracy of our depth estimation method based on physiological responses (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we presented the pure tone search stimuli described above to a separate set of untrained control mice while performing extracellular recordings (<italic>n</italic>=11 recordings from four mice; Ai32/Sst-Cre). Before insertion, the probe was painted with the fluorescent lipophilic dye Di-I (<xref ref-type="bibr" rid="bib27">DiCarlo et al., 1996</xref>; <xref ref-type="bibr" rid="bib66">Morrill and Hasenstaub, 2018</xref>). The depth measurement procedure based on physiological signals was carried out as described above, and then probe tracks from each recording were visualized as described previously (<xref ref-type="bibr" rid="bib66">Morrill and Hasenstaub, 2018</xref>). Briefly, after recordings, the animal was euthanized, and the brain was removed and placed into a solution of 4% PFA in PBS (0.1 m, pH 7.4) for 12 hr, followed by 30% sucrose in PBS solution for several days. The brain was then frozen and sliced using a sliding microtome (SM2000R, Leica Biosystems) and slices were imaged with a fluorescence microscope (BZ-X810, Keyence). Di-I probe markings showing cortical depth were consistent with physiological activity-based depth measurements described above (<xref ref-type="fig" rid="fig3">Figure 3B–C</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology, Project administration</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the Institutional Animal Care and Use Committee at the University of California, San Francisco.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-75839-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Physiology and behavior data supporting all figures in this manuscript have been submitted to Dryad with <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7272/Q6BV7DVM">https://doi.org/10.7272/Q6BV7DVM</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Morrill</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Audiovisual task switching rapidly modulates sound encoding in mouse auditory cortex</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.7272/Q6BV7DVM</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by National Institutes of Health grants R01DC014101 and NS116598 to ARH, the National Science Foundation GRFP to RJM, the Klingenstein Foundation to ARH, Hearing Research Inc to ARH, and the Coleman Memorial Fund to ARH.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aertsen</surname><given-names>AM</given-names></name><name><surname>Johannesma</surname><given-names>PI</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The spectro-temporal receptive field: A functional characteristic of auditory neurons</article-title><source>Biol Cybern</source><volume>42</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1007/BF00336731</pub-id><pub-id pub-id-type="pmid">7326288</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Sterkin</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dynamics of ongoing activity: explanation of the large variability in evoked cortical responses</article-title><source>Science</source><volume>273</volume><fpage>1868</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1126/science.273.5283.1868</pub-id><pub-id pub-id-type="pmid">8791593</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atencio</surname><given-names>CA</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cooperative nonlinearities in auditory cortical neurons</article-title><source>Neuron</source><volume>58</volume><fpage>956</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.026</pub-id><pub-id pub-id-type="pmid">18579084</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atencio</surname><given-names>CA</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spectrotemporal processing in spectral tuning modules of cat primary auditory cortex</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e31537</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0031537</pub-id><pub-id pub-id-type="pmid">22384036</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Atencio</surname><given-names>CA</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Stimulus Choices for Spike-Triggered Receptive Field Analysis. Handbook of Modern Techniques in Auditory Cortex</source><publisher-name>Nova Biomedical</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atiani</surname><given-names>S</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Task difficulty and performance induce diverse adaptive patterns in gain and shape of primary auditory cortical receptive fields</article-title><source>Neuron</source><volume>61</volume><fpage>467</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.027</pub-id><pub-id pub-id-type="pmid">19217382</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bagur</surname><given-names>S</given-names></name><name><surname>Averseng</surname><given-names>M</given-names></name><name><surname>Elgueda</surname><given-names>D</given-names></name><name><surname>David</surname><given-names>S</given-names></name><name><surname>Fritz</surname><given-names>J</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Go/no-go task engagement enhances population representation of target stimuli in primary auditory cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2529</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04839-9</pub-id><pub-id pub-id-type="pmid">29955046</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: A practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id><pub-id pub-id-type="pmid">8748093</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigelow</surname><given-names>J</given-names></name><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Dekloe</surname><given-names>J</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Movement and VIP interneuron activation differentially modulate encoding in mouse auditory cortex</article-title><source>ENeuro</source><volume>6</volume><elocation-id>ENEURO.0164-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0164-19.2019</pub-id><pub-id pub-id-type="pmid">31481397</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigelow</surname><given-names>J</given-names></name><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Olsen</surname><given-names>T</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual modulation of firing and spectrotemporal receptive fields in mouse auditory cortex</article-title><source>Current Research in Neurobiology</source><volume>3</volume><elocation-id>100040</elocation-id><pub-id pub-id-type="doi">10.1016/j.crneur.2022.100040</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bisley</surname><given-names>JW</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuronal activity in the lateral intraparietal area and spatial attention</article-title><source>Science</source><volume>299</volume><fpage>81</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1126/science.1077395</pub-id><pub-id pub-id-type="pmid">12511644</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>MM</given-names></name><name><surname>Miccoli</surname><given-names>L</given-names></name><name><surname>Escrig</surname><given-names>MA</given-names></name><name><surname>Lang</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The pupil as a measure of emotional arousal and autonomic activation</article-title><source>Psychophysiology</source><volume>45</volume><fpage>602</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00654.x</pub-id><pub-id pub-id-type="pmid">18282202</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Budinger</surname><given-names>E</given-names></name><name><surname>Laszcz</surname><given-names>A</given-names></name><name><surname>Lison</surname><given-names>H</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name><name><surname>Ohl</surname><given-names>FW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Non-sensory cortical and subcortical connections of the primary auditory cortex in mongolian gerbils: bottom-up and top-down processing of neuronal information via field AI</article-title><source>Brain Research</source><volume>1220</volume><fpage>2</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.07.084</pub-id><pub-id pub-id-type="pmid">17964556</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Budinger</surname><given-names>E</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Anatomical connections suitable for the direct processing of neuronal information of different modalities via the rodent primary auditory cortex</article-title><source>Hearing Research</source><volume>258</volume><fpage>16</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.04.021</pub-id><pub-id pub-id-type="pmid">19446016</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buracas</surname><given-names>GT</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Albright</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Efficient discrimination of temporal patterns by motion-sensitive neurons in primate visual cortex</article-title><source>Neuron</source><volume>20</volume><fpage>959</fpage><lpage>969</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80477-8</pub-id><pub-id pub-id-type="pmid">9620700</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buran</surname><given-names>BN</given-names></name><name><surname>von Trapp</surname><given-names>G</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Behaviorally gated reduction of spontaneous discharge can improve detection thresholds in auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4076</fpage><lpage>4081</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4825-13.2014</pub-id><pub-id pub-id-type="pmid">24623785</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carcea</surname><given-names>I</given-names></name><name><surname>Insanally</surname><given-names>MN</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamics of auditory cortical activity during behavioural engagement and auditory perception</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14412</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14412</pub-id><pub-id pub-id-type="pmid">28176787</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardin</surname><given-names>JA</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Stimulus feature selectivity in excitatory and inhibitory neurons in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>10333</fpage><lpage>10344</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1692-07.2007</pub-id><pub-id pub-id-type="pmid">17898205</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christianson</surname><given-names>GB</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Depth-dependent temporal response properties in core auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12837</fpage><lpage>12848</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2863-11.2011</pub-id><pub-id pub-id-type="pmid">21900562</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1594</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1038/nn.2439</pub-id><pub-id pub-id-type="pmid">19915566</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>Clark</surname><given-names>L</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Differential responses in human striatum and prefrontal cortex to changes in object and rule relevance</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4312-03.2004</pub-id><pub-id pub-id-type="pmid">14762131</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>MA</given-names></name><name><surname>Dougherty</surname><given-names>K</given-names></name><name><surname>Adams</surname><given-names>GK</given-names></name><name><surname>Reavis</surname><given-names>EA</given-names></name><name><surname>Westerberg</surname><given-names>JA</given-names></name><name><surname>Moore</surname><given-names>BS</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Maier</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spiking suppression precedes cued attentional enhancement of neural responses in primary visual cortex</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>77</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx305</pub-id><pub-id pub-id-type="pmid">29186348</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crone</surname><given-names>EA</given-names></name><name><surname>Wendelken</surname><given-names>C</given-names></name><name><surname>Donohue</surname><given-names>SE</given-names></name><name><surname>Bunge</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural evidence for dissociable components of task-switching</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>475</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi127</pub-id><pub-id pub-id-type="pmid">16000652</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Da Costa</surname><given-names>S</given-names></name><name><surname>van der Zwaag</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>LM</given-names></name><name><surname>Clarke</surname><given-names>S</given-names></name><name><surname>Saenz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tuning in to sound: frequency-selective attentional filter in human primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>1858</fpage><lpage>1863</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4405-12.2013</pub-id><pub-id pub-id-type="pmid">23365225</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Boer</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Reverse correlation I. - A heuristic introduction to the technique of triggered correlation with application to the analysis of compound systems</article-title><source>Proc Kon Ned Acad Wetensch</source><volume>71</volume><fpage>472</fpage><lpage>486</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Lane</surname><given-names>JW</given-names></name><name><surname>Hsiao</surname><given-names>SS</given-names></name><name><surname>Johnson</surname><given-names>KO</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Marking microelectrode penetrations with fluorescent dyes</article-title><source>Journal of Neuroscience Methods</source><volume>64</volume><fpage>75</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/0165-0270(95)00113-1</pub-id><pub-id pub-id-type="pmid">8869487</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downer</surname><given-names>JD</given-names></name><name><surname>Niwa</surname><given-names>M</given-names></name><name><surname>Sutter</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Task engagement selectively modulates neural correlations in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7565</fpage><lpage>7574</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4094-14.2015</pub-id><pub-id pub-id-type="pmid">25972181</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname><given-names>T</given-names></name><name><surname>Monti</surname><given-names>JM</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Expectation and surprise determine neural population responses in the ventral visual stream</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>16601</fpage><lpage>16608</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2770-10.2010</pub-id><pub-id pub-id-type="pmid">21147999</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escabi</surname><given-names>MA</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonlinear spectrotemporal sound analysis by neurons in the auditory midbrain</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>4114</fpage><lpage>4131</lpage><pub-id pub-id-type="pmid">12019330</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escabí</surname><given-names>MA</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Viventi</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>DH</given-names></name><name><surname>Higgins</surname><given-names>NC</given-names></name><name><surname>Storace</surname><given-names>DA</given-names></name><name><surname>Liu</surname><given-names>ASK</given-names></name><name><surname>Gifford</surname><given-names>AM</given-names></name><name><surname>Burke</surname><given-names>JF</given-names></name><name><surname>Campisi</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>YS</given-names></name><name><surname>Avrin</surname><given-names>AE</given-names></name><name><surname>Spiegel Jan</surname><given-names>V</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Rogers</surname><given-names>JA</given-names></name><name><surname>Litt</surname><given-names>B</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A high-density, high-channel count, multiplexed μecog array for auditory-cortex recordings</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1566</fpage><lpage>1583</lpage><pub-id pub-id-type="doi">10.1152/jn.00179.2013</pub-id><pub-id pub-id-type="pmid">24920021</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiáth</surname><given-names>R</given-names></name><name><surname>Márton</surname><given-names>AL</given-names></name><name><surname>Mátyás</surname><given-names>F</given-names></name><name><surname>Pinke</surname><given-names>D</given-names></name><name><surname>Márton</surname><given-names>G</given-names></name><name><surname>Tóth</surname><given-names>K</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Slow insertion of silicon probes improves the quality of acute neuronal recordings</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>111</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-36816-z</pub-id><pub-id pub-id-type="pmid">30643182</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foffani</surname><given-names>G</given-names></name><name><surname>Moxon</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>PSTH-based classification of sensory stimuli using ensembles of single neurons</article-title><source>Journal of Neuroscience Methods</source><volume>135</volume><fpage>107</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.12.011</pub-id><pub-id pub-id-type="pmid">15020095</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Klein</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>1216</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1038/nn1141</pub-id><pub-id pub-id-type="pmid">14583754</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Differential dynamic plasticity of A1 receptive fields during multiple spectral tasks</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>7623</fpage><lpage>7635</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1318-05.2005</pub-id><pub-id pub-id-type="pmid">16107649</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Auditory attention--focusing the searchlight on sound</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>437</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.011</pub-id><pub-id pub-id-type="pmid">17714933</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gourévitch</surname><given-names>B</given-names></name><name><surname>Occelli</surname><given-names>F</given-names></name><name><surname>Gaucher</surname><given-names>Q</given-names></name><name><surname>Aushana</surname><given-names>Y</given-names></name><name><surname>Edeline</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A new and fast characterization of multiple encoding properties of auditory neurons</article-title><source>Brain Topography</source><volume>28</volume><fpage>379</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1007/s10548-014-0375-5</pub-id><pub-id pub-id-type="pmid">24869676</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Hires</surname><given-names>SA</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Bonardi</surname><given-names>C</given-names></name><name><surname>Morandell</surname><given-names>K</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Peron</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>N</given-names></name><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Procedures for behavioral experiments in head-fixed mice</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e88678</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0088678</pub-id><pub-id pub-id-type="pmid">24520413</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haider</surname><given-names>B</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rapid neocortical dynamics: cellular and network mechanisms</article-title><source>Neuron</source><volume>62</volume><fpage>171</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.04.008</pub-id><pub-id pub-id-type="pmid">19409263</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical state and attention</article-title><source>Nature Reviews. Neuroscience</source><volume>12</volume><fpage>509</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1038/nrn3084</pub-id><pub-id pub-id-type="pmid">21829219</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hautus</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Corrections for extreme proportions and their biasing effects on estimated values ofd′</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>27</volume><fpage>46</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.3758/BF03203619</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrington</surname><given-names>TM</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Temporal sequence of attentional modulation in the lateral intraparietal area and middle temporal area during rapid covert shifts of attention</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>3287</fpage><lpage>3296</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6025-09.2010</pub-id><pub-id pub-id-type="pmid">20203188</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hess</surname><given-names>EH</given-names></name><name><surname>Polt</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Pupil size in relation to mental activity during simple problem-solving</article-title><source>Science</source><volume>143</volume><fpage>1190</fpage><lpage>1192</lpage><pub-id pub-id-type="doi">10.1126/science.143.3611.1190</pub-id><pub-id pub-id-type="pmid">17833905</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hocherman</surname><given-names>S</given-names></name><name><surname>Benson</surname><given-names>DA</given-names></name><name><surname>Goldstein</surname><given-names>MH</given-names></name><name><surname>Heffner</surname><given-names>HE</given-names></name><name><surname>Hienz</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Evoked unit activity in auditory cortex of monkeys performing a selective attention task</article-title><source>Brain Research</source><volume>117</volume><fpage>51</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(76)90555-2</pub-id><pub-id pub-id-type="pmid">825193</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoglen</surname><given-names>NEG</given-names></name><name><surname>Larimer</surname><given-names>P</given-names></name><name><surname>Phillips</surname><given-names>EAK</given-names></name><name><surname>Malone</surname><given-names>BJ</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Amplitude modulation coding in awake mice and squirrel monkeys</article-title><source>Journal of Neurophysiology</source><volume>119</volume><fpage>1753</fpage><lpage>1766</lpage><pub-id pub-id-type="doi">10.1152/jn.00101.2017</pub-id><pub-id pub-id-type="pmid">29364073</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>JA</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attention to simultaneous unrelated auditory and visual events: behavioral and neural correlates</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1609</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi039</pub-id><pub-id pub-id-type="pmid">15716469</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawaguchi</surname><given-names>K</given-names></name><name><surname>Clery</surname><given-names>S</given-names></name><name><surname>Pourriahi</surname><given-names>P</given-names></name><name><surname>Seillier</surname><given-names>L</given-names></name><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Nienborg</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Differentiating between models of perceptual decision making using pupil size inferred confidence</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>8874</fpage><lpage>8888</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0735-18.2018</pub-id><pub-id pub-id-type="pmid">30171092</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3?</article-title><source>The Perception Lecture</source><volume>1</volume><elocation-id>S101</elocation-id><pub-id pub-id-type="doi">10.1177/03010066070360S101</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishna</surname><given-names>BS</given-names></name><name><surname>Semple</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Auditory temporal processing: responses to sinusoidally amplitude-modulated tones in the inferior colliculus</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>255</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.1.255</pub-id><pub-id pub-id-type="pmid">10899201</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name><name><surname>Gill</surname><given-names>JV</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Field</surname><given-names>RE</given-names></name><name><surname>Sten</surname><given-names>TAH</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallel processing by cortical inhibition enables context-dependent behavior</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>62</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/nn.4436</pub-id><pub-id pub-id-type="pmid">27798631</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laboy-Juárez</surname><given-names>KJ</given-names></name><name><surname>Ahn</surname><given-names>S</given-names></name><name><surname>Feldman</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A normalized template matching method for improving spike detection in extracellular voltage recordings</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>12087</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-48456-y</pub-id><pub-id pub-id-type="pmid">31427615</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Karmos</surname><given-names>G</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection</article-title><source>Science</source><volume>320</volume><fpage>110</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1126/science.1154735</pub-id><pub-id pub-id-type="pmid">18388295</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>O’Connell</surname><given-names>MN</given-names></name><name><surname>Barczak</surname><given-names>A</given-names></name><name><surname>Mills</surname><given-names>A</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The leading sense: supramodal control of neurophysiological context by attention</article-title><source>Neuron</source><volume>64</volume><fpage>419</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.10.014</pub-id><pub-id pub-id-type="pmid">19914189</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Barczak</surname><given-names>A</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>McGinnis</surname><given-names>T</given-names></name><name><surname>Ross</surname><given-names>D</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>O’Connell</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Global dynamics of selective attention and its lapses in primary auditory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1707</fpage><lpage>1717</lpage><pub-id pub-id-type="doi">10.1038/nn.4386</pub-id><pub-id pub-id-type="pmid">27618311</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(98)80042-1</pub-id><pub-id pub-id-type="pmid">9751656</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>R</given-names></name><name><surname>Engler</surname><given-names>G</given-names></name><name><surname>Kral</surname><given-names>A</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Response properties of local field potentials and multiunit activity in the mouse visual cortex</article-title><source>Neuroscience</source><volume>254</volume><fpage>141</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2013.08.065</pub-id><pub-id pub-id-type="pmid">24035827</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>LY</given-names></name><name><surname>Xiong</surname><given-names>XR</given-names></name><name><surname>Ibrahim</surname><given-names>LA</given-names></name><name><surname>Yuan</surname><given-names>W</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Differential receptive field properties of parvalbumin and somatostatin inhibitory neurons in mouse auditory cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1782</fpage><lpage>1791</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht417</pub-id><pub-id pub-id-type="pmid">24425250</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Licata</surname><given-names>AM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Ryan</surname><given-names>MB</given-names></name><name><surname>Sheppard</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Posterior parietal cortex guides visual decisions in rats</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>4954</fpage><lpage>4966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0105-17.2017</pub-id><pub-id pub-id-type="pmid">28408414</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lima</surname><given-names>SQ</given-names></name><name><surname>Hromádka</surname><given-names>T</given-names></name><name><surname>Znamenskiy</surname><given-names>P</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>PINP: a new method of tagging neuronal populations for identification during in vivo electrophysiological recording</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e6099</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0006099</pub-id><pub-id pub-id-type="pmid">19584920</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malone</surname><given-names>BJ</given-names></name><name><surname>Scott</surname><given-names>BH</given-names></name><name><surname>Semple</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dynamic amplitude coding in the auditory cortex of awake rhesus macaques</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1451</fpage><lpage>1474</lpage><pub-id pub-id-type="doi">10.1152/jn.01203.2006</pub-id><pub-id pub-id-type="pmid">17615123</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JHR</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Feature-based attention in visual cortex</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>317</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.04.001</pub-id><pub-id pub-id-type="pmid">16697058</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical membrane potential signature of optimal states for sensory signal detection</article-title><source>Neuron</source><volume>87</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.038</pub-id><pub-id pub-id-type="pmid">26074005</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Sundberg</surname><given-names>KA</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differential attention-dependent response modulation across cell classes in macaque visual area V4</article-title><source>Neuron</source><volume>55</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.018</pub-id><pub-id pub-id-type="pmid">17610822</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Selective attention gates visual processing in the extrastriate cortex</article-title><source>Science</source><volume>229</volume><fpage>782</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1126/science.4023713</pub-id><pub-id pub-id-type="pmid">4023713</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual information present in infragranular layers of mouse auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2854</fpage><lpage>2862</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3102-17.2018</pub-id><pub-id pub-id-type="pmid">29440554</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Bigelow</surname><given-names>J</given-names></name><name><surname>DeKloe</surname><given-names>J</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>AVtrainer-stim</data-title><version designator="737720F">737720F</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/HasenstaubLab/AVtrainer-stim/tree/main/demo">https://github.com/HasenstaubLab/AVtrainer-stim/tree/main/demo</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nandy</surname><given-names>AS</given-names></name><name><surname>Nassi</surname><given-names>JJ</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Laminar organization of attentional modulation in macaque visual area V4</article-title><source>Neuron</source><volume>93</volume><fpage>235</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.11.029</pub-id><pub-id pub-id-type="pmid">27989456</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>MN</given-names></name><name><surname>Barczak</surname><given-names>A</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Layer specific sharpening of frequency tuning by selective attention in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>16496</fpage><lpage>16508</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2055-14.2014</pub-id><pub-id pub-id-type="pmid">25471586</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otazu</surname><given-names>GH</given-names></name><name><surname>Tai</surname><given-names>LH</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Engaging in an auditory task suppresses responses in auditory cortex</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>646</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1038/nn.2306</pub-id><pub-id pub-id-type="pmid">19363491</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Kenneth D.</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Kang</surname><given-names>X</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Yund</surname><given-names>EW</given-names></name><name><surname>Woods</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Attentional modulation of human auditory cortex</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>658</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1038/nn1256</pub-id><pub-id pub-id-type="pmid">15156150</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>EAK</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Asymmetric effects of activating and inactivating cortical interneurons</article-title><source>eLife</source><volume>5</volume><elocation-id>e18383</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18383</pub-id><pub-id pub-id-type="pmid">27719761</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>EAK</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Diverse effects of stimulus history in waking mouse auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>1376</fpage><lpage>1393</lpage><pub-id pub-id-type="doi">10.1152/jn.00094.2017</pub-id><pub-id pub-id-type="pmid">28566458</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Cadwell</surname><given-names>CR</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil fluctuations track fast switching of cortical states during quiet wakefulness</article-title><source>Neuron</source><volume>84</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.033</pub-id><pub-id pub-id-type="pmid">25374359</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>ATTENTIONAL modulation of visual processing</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>611</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131039</pub-id><pub-id pub-id-type="pmid">15217345</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rikhye</surname><given-names>RV</given-names></name><name><surname>Gilra</surname><given-names>A</given-names></name><name><surname>Halassa</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Thalamic regulation of switching between cortical representations enables cognitive flexibility</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1753</fpage><lpage>1763</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0269-z</pub-id><pub-id pub-id-type="pmid">30455456</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodgers</surname><given-names>CC</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural correlates of task switching in prefrontal cortex and primary auditory cortex in a novel stimulus selection task for rodents</article-title><source>Neuron</source><volume>82</volume><fpage>1157</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.031</pub-id><pub-id pub-id-type="pmid">24908492</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roedel</surname><given-names>A</given-names></name><name><surname>Storch</surname><given-names>C</given-names></name><name><surname>Holsboer</surname><given-names>F</given-names></name><name><surname>Ohl</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Effects of light or dark phase testing on behavioural and cognitive performance in DBA mice</article-title><source>Laboratory Animals</source><volume>40</volume><fpage>371</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1258/002367706778476343</pub-id><pub-id pub-id-type="pmid">17018208</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rougier</surname><given-names>NP</given-names></name><name><surname>Noelle</surname><given-names>DC</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Prefrontal cortex and flexible cognitive control: rules without symbols</article-title><source>PNAS</source><volume>102</volume><fpage>7338</fpage><lpage>7343</lpage><pub-id pub-id-type="doi">10.1073/pnas.0502455102</pub-id><pub-id pub-id-type="pmid">15883365</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutkowski</surname><given-names>RG</given-names></name><name><surname>Shackleton</surname><given-names>TM</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>Wallace</surname><given-names>MN</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Spectrotemporal receptive field properties of single units in the primary, dorsocaudal and ventrorostral auditory cortex of the guinea pig</article-title><source>Audiology &amp; Neuro-Otology</source><volume>7</volume><fpage>214</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1159/000063738</pub-id><pub-id pub-id-type="pmid">12097721</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saderi</surname><given-names>D</given-names></name><name><surname>Schwartz</surname><given-names>ZP</given-names></name><name><surname>Heller</surname><given-names>CR</given-names></name><name><surname>Pennington</surname><given-names>JR</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dissociation of task engagement and arousal effects in auditory cortex and midbrain</article-title><source>eLife</source><volume>10</volume><elocation-id>e60153</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.60153</pub-id><pub-id pub-id-type="pmid">33570493</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakata</surname><given-names>S</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Laminar structure of spontaneous and sensory-evoked population activity in auditory cortex</article-title><source>Neuron</source><volume>64</volume><fpage>404</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.020</pub-id><pub-id pub-id-type="pmid">19914188</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salin</surname><given-names>PA</given-names></name><name><surname>Bullier</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Corticocortical connections in the visual system: structure and function</article-title><source>Physiological Reviews</source><volume>75</volume><fpage>107</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1152/physrev.1995.75.1.107</pub-id><pub-id pub-id-type="pmid">7831395</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samuelsen</surname><given-names>CL</given-names></name><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effects of cue-triggered expectation on cortical processing of taste</article-title><source>Neuron</source><volume>74</volume><fpage>410</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.02.031</pub-id><pub-id pub-id-type="pmid">22542192</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>T</given-names></name><name><surname>Schall</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Pre-excitatory pause in frontal eye field responses</article-title><source>Experimental Brain Research</source><volume>139</volume><fpage>53</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1007/s002210100750</pub-id><pub-id pub-id-type="pmid">11482843</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schriver</surname><given-names>BJ</given-names></name><name><surname>Bagdasarov</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pupil-linked arousal modulates behavior in rats performing a whisker deflection direction discrimination task</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>1655</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1152/jn.00290.2018</pub-id><pub-id pub-id-type="pmid">29995602</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senzai</surname><given-names>Y</given-names></name><name><surname>Fernandez-Ruiz</surname><given-names>A</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Layer-specific physiological features and interlaminar interactions in the primary visual cortex of the mouse</article-title><source>Neuron</source><volume>101</volume><fpage>500</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.12.009</pub-id><pub-id pub-id-type="pmid">30635232</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shomstein</surname><given-names>S</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Control of attention shifts between vision and audition in human cortex</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>10702</fpage><lpage>10706</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2939-04.2004</pub-id><pub-id pub-id-type="pmid">15564587</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sukiban</surname><given-names>J</given-names></name><name><surname>Voges</surname><given-names>N</given-names></name><name><surname>Dembek</surname><given-names>TA</given-names></name><name><surname>Pauli</surname><given-names>R</given-names></name><name><surname>Visser-Vandewalle</surname><given-names>V</given-names></name><name><surname>Denker</surname><given-names>M</given-names></name><name><surname>Weber</surname><given-names>I</given-names></name><name><surname>Timmermann</surname><given-names>L</given-names></name><name><surname>Grün</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evaluation of spike sorting algorithms: application to human subthalamic nucleus recordings and simulations</article-title><source>Neuroscience</source><volume>414</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2019.07.005</pub-id><pub-id pub-id-type="pmid">31299347</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutter</surname><given-names>ML</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>The Relationship of Auditory Cortical Activity to Perception and BehaviorThe Auditory Cortex</source><publisher-name>Springer US</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4419-0074-6_29</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toth</surname><given-names>LJ</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dynamic coding of behaviourally relevant stimuli in parietal cortex</article-title><source>Nature</source><volume>415</volume><fpage>165</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1038/415165a</pub-id><pub-id pub-id-type="pmid">11805833</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Martínez Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>RD</given-names></name><name><surname>Schmitt</surname><given-names>LI</given-names></name><name><surname>Davidson</surname><given-names>TJ</given-names></name><name><surname>Nakajima</surname><given-names>M</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Halassa</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Thalamic control of sensory selection in divided attention</article-title><source>Nature</source><volume>526</volume><fpage>705</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nature15398</pub-id><pub-id pub-id-type="pmid">26503050</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkowski</surname><given-names>DE</given-names></name><name><surname>Bandyopadhyay</surname><given-names>S</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Kanold</surname><given-names>PO</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Frontal cortex activation causes rapid plasticity of auditory cortical processing</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18134</fpage><lpage>18148</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0180-13.2013</pub-id><pub-id pub-id-type="pmid">24227723</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodruff</surname><given-names>PW</given-names></name><name><surname>Benson</surname><given-names>RR</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Howard</surname><given-names>RJ</given-names></name><name><surname>Talavage</surname><given-names>T</given-names></name><name><surname>Belliveau</surname><given-names>J</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Modulation of auditory and visual cortex by selective attention is modality-dependent</article-title><source>Neuroreport</source><volume>7</volume><fpage>1909</fpage><lpage>1913</lpage><pub-id pub-id-type="doi">10.1097/00001756-199608120-00007</pub-id><pub-id pub-id-type="pmid">8905690</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rapid spectrotemporal plasticity in primary auditory cortex during behavior</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4396</fpage><lpage>4408</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2799-13.2014</pub-id><pub-id pub-id-type="pmid">24647959</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>T</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Control of prestimulus activity related to improved sensory coding within a discrimination task</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>4101</fpage><lpage>4112</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4380-10.2011</pub-id><pub-id pub-id-type="pmid">21411651</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zador</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Impact of synaptic unreliability on the information transmitted by spiking neurons</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>1219</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.3.1219</pub-id><pub-id pub-id-type="pmid">9497403</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zempeltzi</surname><given-names>MM</given-names></name><name><surname>Kisse</surname><given-names>M</given-names></name><name><surname>Brunk</surname><given-names>MGK</given-names></name><name><surname>Glemser</surname><given-names>C</given-names></name><name><surname>Aksit</surname><given-names>S</given-names></name><name><surname>Deane</surname><given-names>KE</given-names></name><name><surname>Maurya</surname><given-names>S</given-names></name><name><surname>Schneider</surname><given-names>L</given-names></name><name><surname>Ohl</surname><given-names>FW</given-names></name><name><surname>Deliano</surname><given-names>M</given-names></name><name><surname>Happel</surname><given-names>MFK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Task rule and choice are reflected by layer-specific processing in rodent auditory cortical microcircuits</article-title><source>Communications Biology</source><volume>3</volume><elocation-id>345</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-020-1073-3</pub-id><pub-id pub-id-type="pmid">32620808</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75839.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.11.09.467944" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.09.467944"/></front-stub><body><p>This is an important paper that is methodologically compelling, harnessing a complex behavioral task for modality-specific control of attention to provide new evidence that directed auditory attention produces a global decrease in auditory cortex firing rates without a loss of stimulus-related information. These findings build on previous results showing that task engagement or locomotion down regulates activity in auditory cortex. The manuscript is comprehensive and well-illustrated. It provides highly detailed analysis of the cortical activity modulations during attentional switching that will be valuable to others within and beyond the field of hearing research.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75839.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.11.09.467944">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.11.09.467944v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Audiovisual task switching rapidly modulates sound encoding in mouse auditory cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Brice Bathellier as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Andrew King as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The authors should analyze the modulation of auditory responses with respect to frequency tuning of the recorded neurons. Although the target stimuli are tone clouds. The frequency distribution of the cloud is very narrow. The authors should analyze the modulation according to best frequency (e.g. 6 or 8 bins of BFs including the average low and high target frequencies) as measured from the STRF (i.e for neurons that have one). Possibly inhibition is specific to the neurons whose BF is not one of the target, while neurons with BFs near the target tone cloud could be boosted. This is an important piece of information to connect the present study with some publications that show potentiation of the representation of the attended stimulus.</p><p>2) Related to point 1. The population of neurons in deep auditory cortex in which firing rates and information content increased were excluded from subsequent analysis with the authors choosing to focus on the 'efficiency' improvement in the suppressed population. It is important to have the analysis in figure 8, which seeks to link changes in firing rate to behaviour, repeated for this population. An alternative explanation (that is potentially supported by the observation in S1 that the firing rate changes are driven mostly by untuned neurons) is that during attention relevant neural populations are enhanced such that their activity is better read out and that homeostatic mechanisms act to suppress other, less informative, activity. It seems likely the authors have the data to rule this in or out.</p><p>3) The behavioral data should be presented in a way that better demonstrates how well the mice were selectively attending to the relevant modality. Figure 2 presents the overall performance (d') on this task, which seem robust. However, in figure 8 a more careful analysis of the false alarm rates to the AuVr stimulus (which in the attend-a condition is really the most critical condition to confirm that the animals are performing a selective attention task and not dividing their attention across modalities) look really very high (near 100% in some sessions). Information about the distribution of trial types was apparently not available. If they are not equiprobable (i.e. there are fewer of the conflict trials) then overall d' is a poor measure of selective attention. Rather than an overall d' criterion for performance one that assesses the ability of the animal to selectively attend – such as the false alarm rate on the AuVr attend-a and ArVu on the attend-v – would be more appropriate. In figure 8 it would be helpful to know how the data are ordered – by mouse? The conclusion the reader is meant to take away from figure 8 is that increased firing rate predicts lapses of attention. However enhanced firing could indicate higher arousal and therefore intention to lick. A comparison of correct reject trials and hit trials as well as the hits/fa analysis included would be meaningful here to see whether this is a (pre) motor effect or an attentional effect.</p><p>4) The paper relies heavily on Wilcoxon rank sum tests without any multiple comparisons correction. Often the p values are very small and therefore the lack of correction is inconsequential however there are exceptions. Specifically, this is problematic, especially for the pupillometry section where the AVv_v comparison would not survive multiple comparisons (unlike the Ava_a comparison its range includes zero). This begs the question as to whether the significant effect on pupil size simply comes about from the presence of visual stimulus rather than the AV conditions. There is also no justification provided for why this is a one-tailed test.</p><p>5) Please also improve captions according to reviewer 2 recommendation. Simplifying the figures according to reviewer 3's recommendations (supplementary figures can be used) may help extend the captions without making them too long.</p><p>6) The discussion should be extended according to recommendations of reviewer 2 to include observations from previous studies and of reviewer 3 to include a better discussion of the mechanisms and biological meaning of the present findings (see public review). For the mechanisms, it should be particular attention should be put on the origin of suppression (inhibition or excitation)? The implication of attentional effects before stimulus presentation should also be clarified (referee 3).</p><p>7) The authors should evaluate if suppression is more difficult to be picked up in supragranular layers, at the light of the repeated observation (also done by the group of Harris in the primary auditory cortex) that spiking frequencies are significantly lower (often reaching sparse levels, that is close to zero) there.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1/ The authors should analyze the modulation of auditory responses with respect to frequency tuning of the recorded neurons. Although the target stimuli are tone clouds. The frequency distribution of the cloud is very narrow. The authors should analyze the modulation according to best frequency (e.g. 6 or 8 bins of BFs including the average low and high target frequencies) as measured from the STRF (i.e for neurons that have one). Possibly inhibition is specific to the neurons whose BF is not one of the target, while neurons with BFs near the target tone cloud could be boosted. This is an important piece of information to connect the present study with some publications that show potentiation of the representation of the attended stimulus.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The Discussion is very short – other studies have found suppressive effects of firing rate alongside maintenance or enhancement of information encoded and that active listening accelerates processing (e.g. Otazu et al., Nat. neuro 2009, Dong et al., J.Neurosci 2013, Town et al., nat. comms. 2018, Kuchibhotla et al., Nat. neuro 2017), but many more show tuning changes. There isn't much attempt to relate the general suppressive effects to changes in tuning in the discussion (e.g. the Shamma lab studies looking at STRF changes due to attention), yet the authors perform relevant analysis in the supplemental materials e.g. S1E, that are not even mentioned in passing anywhere in the current manuscript.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I would like to highlight that I understand the concept of having a &quot;public&quot; review and an internal one, and I also see that the journal recommends to if possible avoid repetitions, but scientifically I find it not always easy to separate the two parts. With this I mean that I think Authors should address both parts, which also are highly interconnected.</p><p>The methodology used is rigorous and the authors document technically in a proper way their main findings. However, one crucial recommendation is to simplify the presentation of the main results by reducing the number of panels per figures to the ones conveying the essential take home message per figure (Figure titles are very clear).</p><p>– One important issue is about the layer specificity: how do the authors exclude that suppression could be more difficult to be picked up in supragranular layers, in the light of the repeated observation (also done by the group of Harris in the primary auditory cortex) that spiking frequencies are significantly lower (often reaching sparse levels, that is close to zero) there?</p><p>– Another important issue is the role of inhibitory intracortical mechanisms in the observed suppression of spiking. Please clarify in a more explicit way whether the attention-driven mechanisms could be sustained by increased inhibitory drive. In the requested and necessary simplification of data presentation please provide if there is evidence of such inhibitory subnetwork mechanisms. This should be discussed in the light that many interneuron types also do present an &quot;excitatory-regular spiking – like&quot; phenotype. Alternatively, hypothesis concerning possible withdrawal of excitatory drive (e.g. of thalamic origin) should be at least discussed.</p><p>– As highlighted above, one main concern is on the drawn conclusion that the reduced spike responses are related to modality-specific attentional mechanisms (as suggested by the fact that they correlate with behavioural accuracy during acoustic but not visual tasks) given that they are also observed during intertrial stimuli upon presentation of task irrelevant stimuli. How is the observation that cross-modal (visually-driven) attentional influences are not reported compatible with the above-suggested view? Please clarify.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75839.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors should analyze the modulation of auditory responses with respect to frequency tuning of the recorded neurons. Although the target stimuli are tone clouds. The frequency distribution of the cloud is very narrow. The authors should analyze the modulation according to best frequency (e.g. 6 or 8 bins of BFs including the average low and high target frequencies) as measured from the STRF (i.e for neurons that have one). Possibly inhibition is specific to the neurons whose BF is not one of the target, while neurons with BFs near the target tone cloud could be boosted. This is an important piece of information to connect the present study with some publications that show potentiation of the representation of the attended stimulus.</p></disp-quote><p>We have analyzed attentional modulation for neurons with and without significant STRF tuning and found that the suppression of activity during auditory attention is driven by units without tuned STRFs. Among tuned neurons, we did not find any difference in modulation between those with BFs near (± 0.5 octaves) the rewarded tone cloud, near the unrewarded tone cloud, or outside of either range. Importantly, we also found a strong tuning bias for frequencies near the rewarded tone cloud, consistent with the work of Shamma et al., suggesting that during the course of training, there was significant auditory cortical plasticity to better represent features of the task. In the Discussion, we note that “it is possible that tuning-dependent attentional modulation may occur in earlier stages of task acquisition, but that the substantial reconfiguration of sound processing tailored to the task alters its expression after training.” (Discussion lines 453 to 456). This result was included in Figure S1 in the originally submitted version of our manuscript but has now been moved to main Figure 6 and has been included in the Results (lines 230-273) and Discussion (lines 441-464) sections.</p><disp-quote content-type="editor-comment"><p>2) Related to point 1. The population of neurons in deep auditory cortex in which firing rates and information content increased were excluded from subsequent analysis with the authors choosing to focus on the 'efficiency' improvement in the suppressed population. It is important to have the analysis in figure 8, which seeks to link changes in firing rate to behaviour, repeated for this population. An alternative explanation (that is potentially supported by the observation in S1 that the firing rate changes are driven mostly by untuned neurons) is that during attention relevant neural populations are enhanced such that their activity is better read out and that homeostatic mechanisms act to suppress other, less informative, activity. It seems likely the authors have the data to rule this in or out.</p></disp-quote><p>The group of units with increased FRs is now presented in a new version of Figure 8. This also includes new behavioral filters and has multiple-comparisons corrections applied (see our responses to points 3 and 4 below). For the A-rule enhanced group, we observe no difference between pre-stimulus activity on correct versus error trials. In the suppressed group, increased activity is predictive of error trials, suggesting that this is a signature of attentional lapses. This is consistent with our findings that allocation of attention is associated with decreased AC activity. Importantly, in the V-rule, no significant changes between correct and error trials are observed for these groups, indicating that this effect is due to modality-specific attention rather than a global attentional or motor effect.</p><p>We agree with the interpretation that during attention, less informative activity is suppressed (see former Figure S1, now Figure 6), although we did not observe selective enhancement of more informative activity. This suggests an attentional mechanism which may broadly employ noise removal more than signal enhancement. The failure of this mechanism would manifest as increased activity during activity, and that is indeed what is reflected in Figure 8.</p><disp-quote content-type="editor-comment"><p>3) The behavioral data should be presented in a way that better demonstrates how well the mice were selectively attending to the relevant modality. Figure 2 presents the overall performance (d') on this task, which seem robust. However, in figure 8 a more careful analysis of the false alarm rates to the AuVr stimulus (which in the attend-a condition is really the most critical condition to confirm that the animals are performing a selective attention task and not dividing their attention across modalities) look really very high (near 100% in some sessions). Information about the distribution of trial types was apparently not available. If they are not equiprobable (i.e. there are fewer of the conflict trials) then overall d' is a poor measure of selective attention. Rather than an overall d' criterion for performance one that assesses the ability of the animal to selectively attend – such as the false alarm rate on the AuVr attend-a and ArVu on the attend-v – would be more appropriate. In figure 8 it would be helpful to know how the data are ordered – by mouse? The conclusion the reader is meant to take away from figure 8 is that increased firing rate predicts lapses of attention. However enhanced firing could indicate higher arousal and therefore intention to lick. A comparison of correct reject trials and hit trials as well as the hits/fa analysis included would be meaningful here to see whether this is a (pre) motor effect or an attentional effect.</p></disp-quote><p>We agree that the behavioral performance on the conflict stimuli (A<sub>U</sub>V<sub>R</sub> in A-rule and A<sub>R</sub>V<sub>U</sub> in V-rule) is the most critical for determining modality-selective allocation of attention in this task. As such, we have implemented an additional behavioral filter for inclusion of data. The previous sensitivity index-based filter required <italic>d'</italic>&gt;1.5 for both the A-rule and V-rule, calculated separately. We have also implemented a false alarm rate criteria such that the combined FAR for the two conflict distractor stimuli must be less than 0.5. Given the “go-bias” we observed in this task, in which FAs are more likely errors than misses, this specific metric likely provides a better test of modality-specific attention than <italic>d’</italic> over the entire rule, which includes many stimuli that are consistently at the ceiling for correct performance. Figure 1.F.b reflects these new inclusion criteria using color as an additional dimension on the scatter plot to represent FAR<sub>conflict</sub>. This has moderately reduced the number of included recordings to n = 23 recordings from 10 mice (down from n=27 from 12 mice). This change has not altered any of the main findings from the originally submitted manuscript.</p><p>We appreciate the concern that this effect could arise from a (pre) motor source. In Figure 8.C.c. we show that there is no difference between FA and CR pre-stimulus activity in the V-rule, which suggests that this effect is rule-specific and not the result of general motor-driven mechanisms. In addition, we have performed the requested comparison of pre-stimulus activity on hit vs correct reject trials (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). As in Figure 8 of the manuscript, this analysis examines activity over the -300 ms to 0 window relative to stimulus onset. Each dot in the scatters represents a SU, and subplot titles indicate depth grouping as well as p- and Z-values from a paired Willcoxon signed rank (Hit vs CR pre-stim FR). Consistent with the updated manuscript, all p-values are multiple comparisons corrected (see answer below for details). There is no significant difference between Hit and CR pre-stimulus activity in any depth group, suggesting that in our study, correct withholding versus correct licking is not meaningfully predicted by motor preparatory activity alone. This outcome may be expected given random ordering of target (hit) and distractor (CR) trials in our task. Pre-stimulus activity should only be highly predictive of these outcomes if measurable motor preparatory activity in this window outweighed stimulus identity in determining task performance. The high level of performance from trained animals in this task suggests that this is not the case. We have not included this analysis in the manuscript, as Figure 8.C.c may provide a more direct control for the observed effect being driven by motor-related activity.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-sa2-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>4) The paper relies heavily on Wilcoxon rank sum tests without any multiple comparisons correction. Often the p values are very small and therefore the lack of correction is inconsequential however there are exceptions. Specifically, this is problematic, especially for the pupillometry section where the AVv_v comparison would not survive multiple comparisons (unlike the Ava_a comparison its range includes zero). This begs the question as to whether the significant effect on pupil size simply comes about from the presence of visual stimulus rather than the AV conditions. There is also no justification provided for why this is a one-tailed test.</p></disp-quote><p>We agree that multiple comparisons correction is appropriate for many groups of tests in this study. To achieve this, the resubmission now includes false discovery rate (FDR) control with the Benjamini-Hochberg procedure (see Methods lines 816-821). FDR control has been applied to the outcomes of analyses in which the unit set was divided into depth and waveform-type groups and then tested for the same hypothesis. FDR-adjusted <italic>p­</italic>-values are presented in the text of the paper where noted. Original and FDR-adjusted <italic>p­</italic>-values are now shown in the supplemental statistics tables.</p><p>After FDR of the pupillometry tests mentioned above, we see no significant difference between the unimodal and bimodal block sections of the visual rule. The difference that we previously noted between the unimodal and bimodal blocks within a rule, while consistent with our hypothesis and previous reports (Benjamini and Hochberg 1995), did not meaningfully contribute to the narrative. For the sake of brevity and consistent with Reviewer 3’s recommendation to simplify the figures, they have been moved to supplementary materials (Figure 2 —figure supplement 1).</p><disp-quote content-type="editor-comment"><p>5) Please also improve captions according to reviewer 2 recommendation. Simplifying the figures according to reviewer 3's recommendations (supplementary figures can be used) may help extend the captions without making them too long.</p></disp-quote><p>Each figure caption has been expanded to increase clarity and reduce dependence on text for understanding. Additionally, to streamline the figures, we have reduced the number of figure panels that provide redundant information or null results that do not contribute to the narrative. Such figure panels have been moved to the supplementary materials (Figure 2 —figure supplement 1 and Figure 7 —figure supplement 2.)</p><disp-quote content-type="editor-comment"><p>6) The discussion should be extended according to recommendations of reviewer 2 to include observations from previous studies and of reviewer 3 to include a better discussion of the mechanisms and biological meaning of the present findings (see public review). For the mechanisms, it should be particular attention should be put on the origin of suppression (inhibition or excitation)? The implication of attentional effects before stimulus presentation should also be clarified (referee 3).</p></disp-quote><p>We have expanded the Discussion section to address the points raised by the reviewers. In particular, we have included a section on the relevance of the work by Shamma, Fritz and colleagues on plasticity driven by attention in the auditory cortex (lines 447 to 464). Our results in Figure 6G suggest a sustained alteration of AC spectral representation, which does not appear to exhibit rapid spectral plasticity, at least when comparing A-rule and V-rule responses.</p><p>We have also addressed possible origins of attention-related suppression (Lines 486 to 496, beginning with “Previous studies have reported larger effects of task engagement or attention in inhibitory interneurons…”). To summarize, our division of units into NS and BS does not suggest a general increase in NS activity during auditory attention that would provide clear evidence of enhanced inhibitory network activity; instead, for both NS and BS units, modulation is heterogenous, but group level trends show that activity suppression is more common. However, this division of units by waveform only provides a coarse handle for excitatory/inhibitory network activity. Inhibitory subnetworks only identifiable through molecular tools rather than waveform classification may be driving suppression. We also cannot rule out decreased excitatory drive as a possible mechanism.</p><p>Finally, we have attempted to emphasize that attentional effects on pre-stimulus activity and task-irrelevant responses are likely to reflect modality-wide attentional effects that result from a reduction in spontaneous rates. This finding is therefore a central feature to our interpretation of the data. This is the primary message of Figure 5 (“Attention-related modulation of sound-evoked responses largely reflects pre-stimulus activity changes”). This is also consistent with previous work showing that reductions in spontaneous activity are a feature of attention or task engagement in multiple modalities and multiple model organisms (Buran, von Trapp, and Sanes 2014; Carcea, Insanally, and Froemke 2017 Yoshida and Katz 2011; Cox et al., 2019; Sato and Schall 2001; Bisley and Goldberg 2003; Herrington and Assad 2010). These citations are included in the Discussion (lines 435-440).</p><disp-quote content-type="editor-comment"><p>7) The authors should evaluate if suppression is more difficult to be picked up in supragranular layers, at the light of the repeated observation (also done by the group of Harris in the primary auditory cortex) that spiking frequencies are significantly lower (often reaching sparse levels, that is close to zero) there.</p></disp-quote><p>To address the possibility that low firing rates have prevented discovery of attentional effects in the superficial group, we identified (by randomly selecting from our recorded deep units, without replacement) a subset of deep units for which the distribution of firing rates matches the distribution of firing rates among the superficial units, and then performed the firing rate modulation analyses on this subset of deep cells. The deep group exhibited the greatest modulation by attentional state, so analyzing a subpopulation of these units with spike rates and sample size matched to superficial should tell us whether an effect can be observed from the population of superficial units we recorded. The results are presented in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75839-sa2-fig2-v2.tif"/></fig><p>The top row shows the FR-matched subset of deep units (dark gray) selected to match the smaller population of superficial units (light gray). A<sub>R</sub> and A<sub>U</sub> responses are separated (L, R) for consistency with the paper. In the bottom row, modulation of the FR-matched subset is shown in colors (broad-spiking = red, narrow-spiking = blue), and is overlaid atop the modulation of the entire deep population (gray).In the FR-matched subset of deep units, we observe a significant effect of attention only for the A<sub>R</sub>* stimulus response. This shows that the sample size and firing rate distribution from the superficial group is sufficient to detect an effect in some but not all cases.</p><p>In our evaluation, this analysis suggests that it is possible to detect effects in populations of similar size and FR to our superficial group but does not rule out missed effects in superficial neurons, either due to biological or technical obstacles. We have added the following caveat to the discussion:</p><p>“One important caveat is that superficial AC is known to have lower spontaneous and evoked FRs than deeper cortex (Sakata and Harris 2009; Christianson, Sahani, and Linden 2011), which may have hindered the discovery of attention-related effects in our study. Furthermore, although we tried to minimize neural tissue damage through technical considerations such as using a slow probe insertion speed (Fiáth et al., 2019), the superficial layers likely sustain the greatest level of damage when the probe is inserted to span the full cortical depth. Despite these factors, we were able to isolate a reasonably large sample size of responsive neurons in superficial cortex from successful behavior sessions (n = 119 units, of which 57% were stimulus-responsive). Nevertheless, we cannot fully rule out that the absence of observed attentional modulation at superficial depths is not due to experimental limitations such as the comparatively small sample size. Future work employing imaging techniques to target superficial neurons may help resolve this.” Lines 473-485.</p><p>In addition to the compiled essential revisions, we have addressed the concern of Reviewer 2 regarding possible effects of low-level optogenetic pulses presented during inter-trial intervals. We have examined behavioral response error rate and stimulus response firing rates between trials immediately after optogenetic stimulation and stimulus-identity matched trials preceding the stimulation. No significant difference was observed in behavioral error rate post-opto vs. pre-opto: p = 0.21, Z = -1.27, error rates measured per recording, n = 21 recordings, Wilcoxon signed-rank test) or in response firing rate (p = 0.60, Z = 0.52,:</p><p>&quot;In most experiments (n = 21 recordings), brief, low-level optogenetic pulses during the inter-trial interval of the task were used to identify opsin-expressing neurons (&lt;0.3 mW light; 5 light pulses of 10 ms duration, every ~1.5 min); these analyses are outside of the scope of this report. The optogenetic stimulation protocol was consistent through A- and V-rules of the task. Unit stimulus response FRs and behavioral response error rates were similar between trials immediately after optogenetic pulses and stimulus-matched trials preceding the pulses.” Lines 530-536.</p><p>We again thank all of the reviewers for their thoughtful suggestions.***</p></body></sub-article></article>