<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53051</article-id><article-id pub-id-type="doi">10.7554/eLife.53051</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Transformation of a temporal speech cue to a spatial neural code in human auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-165018"><name><surname>Fox</surname><given-names>Neal P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0298-3664</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-147720"><name><surname>Leonard</surname><given-names>Matthew</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8530-880X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-165019"><name><surname>Sjerps</surname><given-names>Matthias J</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-44898"><name><surname>Chang</surname><given-names>Edward F</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2480-4700</contrib-id><email>edward.chang@ucsf.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Neurological Surgery, University of California, San Francisco</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Donders Institute for Brain, Cognition and Behaviour, Centre for Cognitive Neuroimaging, Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution>Max Planck Institute for Psycholinguistics</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff4"><label>4</label><institution>Weill Institute for Neurosciences, University of California, San Francisco</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role>Reviewing Editor</role><aff><institution>Washington University in St. Louis</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>25</day><month>08</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53051</elocation-id><history><date date-type="received" iso-8601-date="2019-10-25"><day>25</day><month>10</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-08-21"><day>21</day><month>08</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Fox et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Fox et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53051-v2.pdf"/><abstract><p>In speech, listeners extract continuously-varying spectrotemporal cues from the acoustic signal to perceive discrete phonetic categories. Spectral cues are spatially encoded in the amplitude of responses in phonetically-tuned neural populations in auditory cortex. It remains unknown whether similar neurophysiological mechanisms encode temporal cues like voice-onset time (VOT), which distinguishes sounds like /<italic>b</italic>/ and/<italic>p</italic>/. We used direct brain recordings in humans to investigate the neural encoding of temporal speech cues with a VOT continuum from /<italic>ba</italic>/ to /<italic>pa</italic>/. We found that distinct neural populations respond preferentially to VOTs from one phonetic category, and are also sensitive to sub-phonetic VOT differences within a population’s preferred category. In a simple neural network model, simulated populations tuned to detect either temporal gaps or coincidences between spectral cues captured encoding patterns observed in real neural data. These results demonstrate that a spatial/amplitude neural code underlies the cortical representation of both spectral and temporal speech cues.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech</kwd><kwd>electrocorticography</kwd><kwd>auditory cortex</kwd><kwd>temporal processing</kwd><kwd>categorical perception</kwd><kwd>voice-onset time (VOT)</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-DC012379</award-id><principal-award-recipient><name><surname>Chang</surname><given-names>Edward F</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32-DC015966</award-id><principal-award-recipient><name><surname>Fox</surname><given-names>Neal P</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>FP7-623072</award-id><principal-award-recipient><name><surname>Sjerps</surname><given-names>Matthias J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100003194</institution-id><institution>New York Stem Cell Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chang</surname><given-names>Edward F</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010246</institution-id><institution>William K. Bowes, Jr. Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chang</surname><given-names>Edward F</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chang</surname><given-names>Edward F</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010319</institution-id><institution>Shurl and Kay Curci Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chang</surname><given-names>Edward F</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The human brain has a spatial code for representing temporal phonetic distinctions in speech.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>During speech perception, listeners must extract acoustic cues from a continuous sensory signal and map them onto discrete phonetic categories, which are relevant for meaning (<xref ref-type="bibr" rid="bib107">Stevens, 2002</xref>; <xref ref-type="bibr" rid="bib54">Liberman et al., 1967</xref>). Many such cues to phonological identity are encoded within the fine temporal structure of speech (<xref ref-type="bibr" rid="bib95">Shannon et al., 1995</xref>; <xref ref-type="bibr" rid="bib92">Rosen, 1992</xref>; <xref ref-type="bibr" rid="bib40">Klatt, 1976</xref>). For example, voice-onset time (VOT), defined as the interval between a stop consonant’s release and the onset of vocal fold vibration (acoustically, the <italic>burst</italic> and the <italic>voicing</italic>), is a critical cue that listeners use to distinguish <italic>voiced</italic> (e.g., /<italic>b</italic>/, /<italic>d</italic>/, /<italic>g</italic>/) from <italic>voiceless</italic> (e.g., /<italic>p</italic>/, /<italic>t</italic>/, /<italic>k</italic>/) stop consonants in English (<xref ref-type="bibr" rid="bib52">Liberman et al., 1958</xref>; <xref ref-type="bibr" rid="bib57">Lisker and Abramson, 1964</xref>). When the burst and voicing are roughly coincident (short VOT; ~0 ms), listeners perceive a bilabial stop as a /<italic>b</italic>/, but when voicing follows the burst after a temporal gap (long VOT; ~50 ms), listeners hear a /<italic>p</italic>/.</p><p>Recent evidence from human electrocorticography (ECoG) has shown that information about a speech sound’s identity is encoded in the amplitude of neural activity at phonetically-tuned cortical sites in the superior temporal gyrus (STG) (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>). Distinct neural populations in this region respond selectively to different classes of phonemes that share certain spectral cues, such as the burst associated with stop consonants or the characteristic formant structure of vowels produced with specific vocal tract configurations. However, it is unclear whether phonetic categories distinguished by temporal cues (e.g., voiced vs. voiceless stops) are represented within an analogous spatial encoding scheme. If so, this would entail that local neural populations are tuned to detect not merely the presence of certain spectral cues (the burst and voicing), but also their timing relative to one another.</p><p>In addition to distinguishing phonetic categories, the exact VOT of a given utterance of a /<italic>b</italic>/ or a /<italic>p</italic>/ will vary considerably depending on numerous factors such as speech rate, phonetic context, and speaker accent (<xref ref-type="bibr" rid="bib72">Miller et al., 1986</xref>; <xref ref-type="bibr" rid="bib38">Kessinger and Blumstein, 1997</xref>; <xref ref-type="bibr" rid="bib39">Klatt, 1975</xref>; <xref ref-type="bibr" rid="bib58">Lisker and Abramson, 1967</xref>; <xref ref-type="bibr" rid="bib1">Allen et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Flege and Eefting, 1986</xref>; <xref ref-type="bibr" rid="bib28">Fox et al., 2015</xref>). Although only categorical phonetic identity (e.g., whether a particular VOT is more consistent with a /<italic>b</italic>/ or a /<italic>p</italic>/) is strictly necessary for understanding meaning, sensitivity to fine-grained sub-phonetic detail (e.g., whether a particular /<italic>p</italic>/ was pronounced with a 40 ms vs. a 50 ms VOT) is also crucial for robust speech perception, allowing listeners to flexibly adapt and to integrate multiple cues to phonetic identity online in noisy, unstable environments (<xref ref-type="bibr" rid="bib73">Miller and Volaitis, 1989</xref>; <xref ref-type="bibr" rid="bib12">Clayards et al., 2008</xref>; <xref ref-type="bibr" rid="bib42">Kleinschmidt and Jaeger, 2015</xref>; <xref ref-type="bibr" rid="bib69">McMurray and Jongman, 2011</xref>; <xref ref-type="bibr" rid="bib114">Toscano and McMurray, 2010</xref>; <xref ref-type="bibr" rid="bib30">Fox and Blumstein, 2016</xref>). However, the neurophysiological mechanisms that support listeners’ sensitivity (<xref ref-type="bibr" rid="bib47">Kuhl, 1991</xref>; <xref ref-type="bibr" rid="bib7">Carney, 1977</xref>; <xref ref-type="bibr" rid="bib87">Pisoni and Tash, 1974</xref>; <xref ref-type="bibr" rid="bib62">Massaro and Cohen, 1983</xref>; <xref ref-type="bibr" rid="bib2">Andruski et al., 1994</xref>; <xref ref-type="bibr" rid="bib68">McMurray et al., 2002</xref>; <xref ref-type="bibr" rid="bib93">Schouten et al., 2003</xref>) to such detailed speech representations are not known. We tested whether sub-phonetic information might be encoded in the neural response amplitude of the same acoustically-tuned neural populations that encode phonetic information in human auditory cortex.</p><p>To address these questions, we recorded neural activity directly from the cortex of seven human participants using high-density ECoG arrays while they listened to and categorized syllables along a VOT continuum from /<italic>ba</italic>/ (0 ms VOT) to /<italic>pa</italic>/ (50 ms VOT). We found that the amplitude of cortical responses in STG simultaneously encodes both phonetic and sub-phonetic information about a syllable’s initial VOT. In particular, spatially discrete neural populations respond preferentially to VOTs from one category (either /<italic>b</italic>/ or /<italic>p</italic>/). Furthermore, peak response amplitude is modulated by stimulus VOT within each population’s preferred – but not its non-preferred – voicing category (e.g., stronger response to 0 ms than to 10 ms VOT in voiced-selective [/<italic>b</italic>/-selective] neural populations). This same encoding scheme emerged in a computational neural network model simulating neuronal populations as leaky integrators tuned to detect either temporal coincidences or gaps between distinct spectral cues. Our results provide direct evidence that phonetic and sub-phonetic information carried by VOT are represented within spatially discrete, phonetically-tuned neural populations that integrate temporally-distributed spectral cues in speech. This represents a crucial step towards a unified model of cortical speech encoding, demonstrating that both spectral and temporal cues and both phonetic and sub-phonetic information are represented by a common (spatial) neural code.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Participants listened to and categorized speech sounds from a digitally synthesized continuum of consonant-vowel syllables that differed linearly only in their voice-onset time (VOT) from /<italic>ba</italic>/ (0 ms VOT) to /<italic>pa</italic>/ (50 ms VOT). This six-step continuum was constructed by manipulating only the relative timing of the spectral burst and the onset of voicing while holding all other acoustic properties of the stimuli constant (<xref ref-type="fig" rid="fig1">Figure 1A/B</xref>; see Materials and methods) (<xref ref-type="bibr" rid="bib41">Klatt, 1980</xref>). Analysis of participants’ identification behavior confirmed that stimuli with longer VOTs were more often labeled as /<italic>pa</italic>/ (mixed effects logistic regression: <italic>β</italic><sub>VOT</sub> = 0.19, <italic>t</italic> = 17.78, p=5.6*10<sup>−63</sup>; data for example participant in <xref ref-type="fig" rid="fig1">Figure 1C</xref>; data for all participants in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Moreover, and consistent with past work, listeners’ perception of the linear VOT continuum was sharply non-linear, a behavioral hallmark of categorical perception (<xref ref-type="bibr" rid="bib51">Liberman et al., 1957</xref>; <xref ref-type="bibr" rid="bib53">Liberman et al., 1961</xref>; <xref ref-type="bibr" rid="bib46">Kronrod et al., 2016</xref>). A psychophysical category boundary between 20 ms and 30 ms divided the continuum into stimuli most often perceived as voiced (/<italic>b</italic>/: 0 ms, 10 ms, 20 ms VOTs) or as voiceless (/<italic>p</italic>/: 30 ms, 40 ms, 50 ms VOTs).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Speech sound categories that are distinguished by a temporal cue are spatially encoded in the peak amplitude of neural activity in distinct neural populations.</title><p>(<bold>A</bold>) Stimuli varied only in voice-onset time (VOT), the duration between the onset of the burst (top) and the onset of voicing (bottom) (a.u. = arbitrary units). (<bold>B</bold>) Acoustic waveforms of the first 100 ms of the six synthesized stimuli. (<bold>C</bold>) Behavior for one example participant (mean ± bootstrap SE). Best-fit psychometric curve (mixed effects logistic regression) yields voicing category boundary between 20–30 ms (50% crossover point). (<bold>D</bold>) Neural responses in the same representative participant show selectivity for either voiceless or voiced VOTs at different electrodes. Electrode size indicates peak high-gamma (HG; z-scored) amplitude at all speech-responsive temporal lobe sites. Electrode color reflects strength and direction of selectivity (Spearman’s ρ between peak HG amplitude and VOT) at VOT-sensitive sites (p&lt;0.05). (<bold>E</bold>) Average HG responses (± SE) to voiced (0–20 ms VOTs; red) and voiceless (30–50 ms VOTs; blue) stimuli in two example electrodes from (<bold>D</bold>), aligned to stimulus onset (e1: voiceless-selective, V-; e2: voiced-selective, V+). Horizontal black bars indicate timepoints with category discriminability (p&lt;0.005). Grey boxes mark average peak window (± SD) across all VOT-sensitive electrodes (n = 49). (<bold>F</bold>) Population-based classification of voicing category (/p/ vs. /b/) during peak window (150–250 ms after stimulus onset). Chance is 50%. Boxes show interquartile range across all participants; whiskers extend to best- and worst-performing participants; horizontal bars show median performance. Asterisks indicate significantly better-than-chance classification across participants (p&lt;0.05; n.s. = not significant). Circles represent individual participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Identification behavior across all participants with behavioral data.</title><p>(<bold>A</bold>) Mean (± SE across participants; n = 4 of 7 participants) percent /pa/ responses for each voice-onset time (VOT) stimulus. Best-fit psychometric curve (mixed effects logistic regression) yields voicing category boundary at 21.0 ms (50% crossover point; see Materials and methods for details). (<bold>B</bold>) Behavior (mean ± bootstrap SE) for each individual participant (P1, P2, P6, P7). Total trials (n) listed for each participant (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Best-fit psychometric curves and category boundaries were computed using the mixed effects logistic regression across all participants, adjusted by the random intercept fit by the model for each participant. Voicing category boundaries were subject-dependent, with 3 of 4 participants’ occurring between 20–30 ms. P1 is representative participant in <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Locations of all speech-responsive and VOT-sensitive electrodes in each participant (P1–P7).</title><p>P1 is representative participant in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Electrode color reflects strength and direction of selectivity (Spearman’s ρ between peak HG amplitude and VOT) at subset of VOT-sensitive sites (p&lt;0.05) for either voiceless VOTs (/p/; blue) or voiced VOTs (/b/; red). Electrode size indicates peak high-gamma (HG; z-scored) amplitude at all speech-responsive temporal lobe sites. Maximum and minimum electrode size and selectivity was calculated per participant for visualization.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Analysis of evoked local field potentials reveals that some electrodes that encode VOT in their peak high-gamma amplitude also exhibit amplitude and/or temporal response features that are VOT-dependent.</title><p>(<bold>A</bold>) Grand average auditory evoked potential (AEP) to all VOT stimuli. Evoked local field potentials (negative up-going) were averaged over all VOT-sensitive STG electrodes for one representative participant (P1) (mean ± SE, computed across electrodes). Three peaks of the AEP were identified for analysis: 75–100 ms (P<sub>α</sub>), 100–150 ms (N<sub>α</sub>), and 150–250 ms (P<sub>β</sub>) after stimulus onset. (<bold>B</bold>) Correlation coefficients (Pearson’s r) quantifying association between VOT and latency (top) or amplitude (bottom) of each peak (P<sub>α</sub>: left; N<sub>α</sub>: middle; P<sub>β</sub>: right) for each VOT-sensitive electrode for which that peak could be reliably identified (see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref> and Materials and methods for details of this analysis). Horizontal bars represent bootstrapped estimate of correlation coefficient (mean and 95% CI) for each electrode (blue: voiceless-selective; red: voiced-selective; electrodes sorted by mean correlation value). Black bars around an electrode’s mean indicate that encoding of VOT by the designated parameter (latency or amplitude of a given peak) was significant (95% CI excluded r = 0; grey bars: not significant). Later peaks were reliably identified for fewer electrodes (P<sub>α</sub>: n = 32 of 49 electrodes; N<sub>α</sub>: n = 19; P<sub>β</sub>: n = 15).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig1-figsupp3-v2.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Complex and variable associations between VOT and amplitude/temporal features of auditory evoked local field potentials (AEPs) exist in responses of electrodes that robustly encode voicing in their peak high-gamma amplitude.</title><p>(<bold>A</bold> to <bold>D</bold>) Average high-gamma responses (± SE) to voiced (0–20 ms VOTs; red) and voiceless (30–50 ms VOTs; blue) stimuli in four representative VOT-sensitive STG electrodes, including two voiceless-selective (A: e1, C: e3) and two voiced-selective (B: e2, D: e4) electrodes, aligned to stimulus onset. Vertical bars indicate relative scaling of high-gamma (z-scored) in each panel. The two leftmost electrodes (e1, e2) correspond to e1 and e2 in main text (e.g., <xref ref-type="fig" rid="fig1">Figure 1E</xref>). (<bold>E</bold> to <bold>H</bold>) Average local field potentials (± SE) evoked by voiced/voiceless stimuli in the same four electrodes, aligned to stimulus onset. Vertical bars (negative-upgoing) indicate relative scaling of voltage in each panel. The three peaks of the AEP that were identified for analysis are labeled for each electrode (P<sub>α</sub>, N<sub>α</sub>, P<sub>β</sub>; see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). For a given electrode, peaks were omitted from this analysis if they could not be reliably identified across bootstrapped samples of trials from all six VOT conditions (e.g., P<sub>β</sub> for e4). See Materials and methods for details. (<bold>I</bold> to <bold>L</bold>) Average local field potentials evoked by each VOT stimulus (line color) in the same four electrodes, aligned to stimulus onset. (<bold>M</bold> to <bold>P</bold>) Mean latency (± bootstrap SE) of each AEP peak for each VOT stimulus for the same four electrodes. Mean bootstrapped correlation (Pearson’s r) between VOT and peak latency shown for each peak/electrode. (<bold>Q</bold> to <bold>T</bold>) Mean amplitude (± bootstrap SE) of each AEP peak for each VOT stimulus for the same four electrodes. Mean bootstrapped correlation (Pearson’s r) between VOT and peak amplitude shown for each peak/electrode. Note that negative correlations are visually represented as rising from left to right. Correlation coefficients comprised the source data for summary representations in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig1-figsupp4-v2.tif"/></fig></fig-group><sec id="s2-1"><title>Temporal cues to voicing category are encoded in spatially distinct neural populations</title><p>To investigate neural activity that differentiates the representation of speech sounds based on a temporal cue like VOT, we recorded high-density electrocorticography in seven participants while they listened to the VOT continuum. We examined high-gamma power (70–150 Hz) (<xref ref-type="bibr" rid="bib10">Chang, 2015</xref>; <xref ref-type="bibr" rid="bib14">Crone et al., 2001</xref>; <xref ref-type="bibr" rid="bib104">Steinschneider et al., 2008</xref>; <xref ref-type="bibr" rid="bib91">Ray and Maunsell, 2011</xref>), aligned to the acoustic onset of each trial (burst onset), at every speech-responsive electrode on the lateral surface of the temporal lobe of each patient (n = 346 electrodes; see Materials and methods for details of data acquisition, preprocessing, and electrode selection).</p><p>We used nonparametric correlation analysis (Spearman’sρ) to identify electrodes where the peak high-gamma amplitude was sensitive to stimulus VOT. Across all participants, we found 49 VOT-sensitive sites, primarily located over the lateral mid-to-posterior STG, bilaterally. Peak response amplitude at these VOT-sensitive electrodes reliably discriminated between voicing categories, exhibiting stronger responses to either voiced (/<italic>b</italic>/; VOT = 0–20 ms; n = 33) or voiceless (/<italic>p</italic>/; VOT = 30–50 ms; n = 16) stimuli (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; locations of all sites shown in <xref ref-type="fig" rid="fig2">Figures 2A</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). We observed that, within individual participants, electrodes spaced only 4 mm apart showed strong preferences for different voicing categories, and we did not observe any clear overall regional or hemispheric patterns in the prevalence or selectivity patterns of VOT-sensitive electrodes (see Materials and methods for additional information).</p><p>Robust category selectivity in voiceless-selective (V-) and voiced-selective (V+) neural populations emerged as early as 50–150 ms post-stimulus onset and often lasted for several hundred milliseconds (example electrodes in <xref ref-type="fig" rid="fig1">Figure 1E</xref>). Across all VOT-sensitive electrodes, voicing category selectivity was reliable whether a trial’s voicing category was defined based on the psychophysically-determined category boundary (0–20 ms vs. 30–50 ms VOTs; V- electrodes: <italic>z</italic> = 3.52, p=4.4×10<sup>−4</sup>; V+ electrodes: <italic>z</italic> = −5.01, p=5.4×10<sup>−7</sup>; Wilcoxon signed-rank tests) or based on the actual behavioral response recorded for each trial (V- electrodes: p=4.9×10<sup>−4</sup>; V+ electrodes: p=6.1×10<sup>−5</sup>; Wilcoxon signed-rank tests).</p><p>These results show that spatially distinct neural populations in auditory cortex are tuned to speech sound categories defined by a temporal cue. Critically, if individual neural populations only responded to spectral features (e.g., to the burst or to the onset of voicing), we would not have observed overall amplitude differences in their responses to /<italic>b</italic>/ versus /<italic>p</italic>/ categories.</p><p>Given this pattern of spatial tuning, we tested whether the voicing category of single trials could be reliably decoded from population neural activity across electrodes. For each participant, we trained a multivariate pattern classifier (linear discriminant analysis with leave-one-out cross validation) to predict trial-by-trial voicing category using high-gamma activity across all speech-responsive electrodes on the temporal lobe during the peak neural response (150–250 ms after stimulus onset; see Materials and methods). We found that, across participants, classification accuracy was significantly better than chance (Wilcoxon signed-rank test: p=0.016; <xref ref-type="fig" rid="fig1">Figure 1F</xref>, leftmost box plot), demonstrating that spatially and temporally distributed population neural activity during the peak response contains information that allows for decoding of a temporally-cued phonetic distinction in speech.</p></sec><sec id="s2-2"><title>Peak neural response amplitude robustly encodes voicing category</title><p>Next, we asked which features of the population neural response encode voicing category. Specifically, we evaluated three alternatives for how temporally-cued voicing category is encoded by high-gamma responses in cortex during the peak neural response: (1) the spatial pattern of peak response amplitude across electrodes, (2) the temporal patterns of evoked responses across electrodes during the peak response, or (3) both amplitude and timing of neural activity patterns. We tested these hypotheses by selectively corrupting amplitude and/or temporal neural features that were inputs for the classifier. As with the previous analyses, and following prior work on speech sound encoding (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>), these analyses (<xref ref-type="fig" rid="fig1">Figure 1F</xref>) focused on cortical high-gamma activity during the peak response window (150–250 ms after stimulus onset; but see <xref ref-type="fig" rid="fig3">Figure 3</xref> for analyses of an earlier time window).</p><p>To corrupt temporal information, we randomly jittered the exact timing of the neural response for each trial by shifting the 100 ms analysis window by up to ±50 ms. Because the uniform random jitter was applied independently to each trial, this procedure disrupts any temporal patterns during the peak neural response that might reliably distinguish trials of different voicing categories, such as precise (millisecond-resolution) timing of the peak response at an electrode or the dynamics of the evoked response during the peak window, including <italic>local</italic> temporal dynamics (during a single electrode’s peak response) or <italic>ensemble</italic> temporal dynamics (the relative timing of responses of spatially-distributed electrodes in the same participant). To corrupt amplitude information, we eliminated any condition-related differences in the peak response amplitude at every electrode. For each electrode, the evoked high-gamma response to all trials within a given voicing category were renormalized so that the average responses to both voicing categories had identical amplitudes at the peak, but could still vary reliably in the timing and dynamics during the peak window. These techniques allowed us to examine the relative contributions of temporal and amplitude information contained within the peak neural response window to the classification of voicing category (see Materials and methods for detailed description of this approach).</p><p>Across participants, we found that, when the classifiers had access to amplitude information but not timing information (+Amplitude/-Timing) during the peak response, performance was significantly better than chance (Wilcoxon signed-rank test: p=0.016; <xref ref-type="fig" rid="fig1">Figure 1F</xref>). Furthermore, despite the profound corruption of temporal information in the neural responses, classification accuracy was statistically comparable to the model that had access to both amplitude and timing information (+Amplitude/+Timing; Wilcoxon signed-rank test: p=0.69; <xref ref-type="fig" rid="fig1">Figure 1F</xref>), suggesting that amplitude information alone is sufficient for classifying a trial’s voicing category.</p><p>In contrast, when amplitude information was corrupted and only temporal patterns in the peak response window were reliable (-Amplitude/+Timing), classifier performance was not different from chance (Wilcoxon signed-rank test: p=0.69; <xref ref-type="fig" rid="fig1">Figure 1F</xref>) and was worse for every participant compared to the model with both types of information (Wilcoxon signed-rank test: p=0.016). Finally, we compared the model with only timing information to a model where both amplitude and timing information during the peak window were corrupted (-Amplitude/-Timing). We found that preserving timing information alone had no effect on classification performance compared to the most impoverished model (-Amplitude/-Timing; Wilcoxon signed-rank test: p=0.58; <xref ref-type="fig" rid="fig1">Figure 1F</xref>), which also failed to perform better than chance (Wilcoxon signed-rank test: p=0.94; <xref ref-type="fig" rid="fig1">Figure 1F</xref>). Together, these results constitute evidence for a spatial/amplitude code for speech categories that differ in a temporal cue. Thus, localized peak high-gamma response amplitude spatially encodes voicing of single trials in STG, analogous to other spectrally-cued phonetic features (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>). Note that, while spatial (and not temporal) patterns of high-gamma responses robustly encode voicing during this critical peak window, we later describe additional analyses that address possible temporal encoding patterns in the local field potential (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>) and in an earlier time window (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>The encoding of stop consonant voicing in the amplitude of evoked high-gamma responses in STG suggests that the representation of temporally-cued phonetic features may be explained within the same neural coding framework as the representation of spectrally-cued phonetic features. However, previous work on the cortical representation of voicing has identified a role for temporal information in the local field potential (LFP) (<xref ref-type="bibr" rid="bib101">Steinschneider et al., 1999</xref>; <xref ref-type="bibr" rid="bib106">Steinschneider et al., 2013</xref>), which is dominated by lower- frequencies (<xref ref-type="bibr" rid="bib6">Buzsáki et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Einevoll et al., 2013</xref>).</p><p>To link our results with this existing literature, we conducted a series of exploratory analyses of the neural responses to our stimuli using the raw voltage (LFP) signal. For each VOT-sensitive electrode (defined in the high-gamma analysis), we measured the correlations between VOT and peak latency and between VOT and peak amplitude for three peaks in the auditory evoked potential (AEP) occurring approximately 75–100 ms (<italic>P<sub>α</sub></italic>), 100–150 ms (<italic>N<sub>α</sub></italic>), and 150–250 ms (<italic>P<sub>β</sub></italic>) after stimulus onset (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>; <xref ref-type="bibr" rid="bib37">Howard et al., 2000</xref>; <xref ref-type="bibr" rid="bib79">Nourski et al., 2015</xref>). We found that some VOT-sensitive electrodes encoded VOT in the latency of these peaks (e.g., <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels E/I/M), replicating previous results (<xref ref-type="bibr" rid="bib105">Steinschneider et al., 2011</xref>). However, among electrodes that encode VOT in peak high-gamma amplitude, there exist many more electrodes that <italic>do not</italic> encode VOT in these temporal features of the AEP, and many that also encode VOT in the amplitude of these AEP peaks (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>). This further supports the prominent role that amplitude information plays in the neural representation of voicing and VOT, both in high-gamma and in the LFP. Therefore, subsequent analyses focus on the high-gamma amplitude. (For detailed descriptions of these LFP analyses and their results, see Methods and <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>).</p></sec><sec id="s2-3"><title>Peak response amplitude encodes sub-phonetic VOT information within preferred category</title><p>Next, we assessed whether VOT-sensitive neural populations (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), which reliably discriminate between phonetic categories (voiced vs. voiceless), also encoded within-category sub-phonetic detail in the peak response amplitude. Specifically, the cortical representation of stimuli from the same voicing category but with different VOTs (e.g., 30, 40, and 50 ms VOTs that all correspond to /<italic>p</italic>/) could be either categorical (i.e., all elicit the same peak response amplitude) or graded (i.e., peak response amplitude depends on within-category VOT).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Human auditory cortex encodes both phonetic (between-category) and sub-phonetic (within-category) information in peak response amplitude, which can be modeled by a simple neural network that implements temporal gap and coincidence detection.</title><p>(<bold>A</bold>) Spatial distribution of VOT-sensitive electrodes across all participants (on standardized brain). (<bold>B</bold>) Average (± SE) normalized HG response to each VOT across all voiceless-selective (V-) electrodes, aligned to stimulus onset. Line style denotes category membership of a given VOT (solid: preferred category; dashed: non-preferred category). Grey box marks average peak window (± SD) across all VOT-sensitive electrodes. (<bold>C</bold>) Average (± SE) normalized response to each VOT across all voiced-selective (V+) electrodes. (<bold>D</bold>) Average (± SE) peak response to each VOT stimulus for V- electrodes (left) and V+ electrodes (right) (see Materials and methods). (<bold>E</bold>) A simple neural network model (top) comprised of five leaky integrator nodes was implemented to examine computational mechanisms that could account for the spatial encoding of a temporal cue (VOT). Arrows and circle represent excitatory and inhibitory connections between nodes. See Materials and methods for details on model parameters. Postsynaptic potentials (PSPs) illustrate the internal dynamics of the gap detector (<sc>G</sc><sc>ap</sc>, middle) and coincidence detector (<sc>C</sc><sc>oinc</sc>, bottom) in response to simulated VOT stimuli (line color). Outputs (panels F/G) are triggered by suprathreshold instantaneous PSPs (ΣPSP≥θ, dark lines) but not by subthreshold PSPs (ΣPSP&lt;θ; semitransparent lines). (<bold>F</bold>) Model outputs (a.u. = arbitrary units) evoked by simulated VOT stimuli for <sc>G</sc><sc>ap</sc> (one cycle = 10 ms). Note that outputs for 0 ms and 10 ms VOTs are overlapping. No error bars shown because model simulations are deterministic. Grey box marks average peak window (across panels F/G); width matches peak window of real neural data (panels B/C). (<bold>G</bold>) Model outputs for <sc>C</sc><sc>oinc</sc> (<bold>H</bold>) Peak response to each simulated VOT stimulus for <sc>G</sc><sc>ap</sc> (left) and <sc>C</sc><sc>oinc</sc> (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Connection weights between model nodes.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig2-figsupp1-v2.tif"/></fig></fig-group><p>We examined the average responses to each of the six VOTs separately in the voiceless-selective electrodes (V-; <xref ref-type="fig" rid="fig2">Figure 2B</xref>) and the voiced-selective electrodes (V+; <xref ref-type="fig" rid="fig2">Figure 2C</xref>). We observed clear differences in activity evoked by different VOTs at the peak response (~200 ms after stimulus onset), even within the same voicing category, consistent with sensitivity to sub-phonetic detail (<xref ref-type="bibr" rid="bib4">Blumstein et al., 2005</xref>; <xref ref-type="bibr" rid="bib112">Toscano et al., 2010</xref>; <xref ref-type="bibr" rid="bib113">Toscano et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Frye et al., 2007</xref>). However, the discriminability of responses to within-category VOTs depended on the preferred voicing category of a given electrode.</p><p>To quantify this observation, at each electrode, we computed the rank-based correlation (Spearman’s ρ) between stimulus VOT and peak response amplitude separately for each voicing category (0–20 ms and 30–50 ms VOTs). This procedure resulted in two correlation coefficients for each VOT-sensitive site (ρ<italic><sub>0-20</sub>,</italic> ρ<italic><sub>30-50</sub></italic>) and corresponding test statistics reflecting the strength of within-category amplitude encoding of stimulus VOT in each voicing category. These test statistics (one per voicing category per VOT-sensitive electrode) then served as the input data for a series of signed-rank statistical tests to assess overall within-category encoding properties of groups of electrodes (e.g., of all V- electrodes) (see Methods for details). For example, consider V- electrodes, which exhibit stronger responses, overall, for voiceless stimuli (30–50 ms VOTs) compared to voiced stimuli (0–20 ms VOTs). Across V- electrodes, we found that voiceless stimuli with longer VOTs (i.e., closer to the preferred category’s 50 ms endpoint VOT) also elicit increasingly stronger responses (Wilcoxon signed-rank test: <italic>z</italic> = 3.52, p=4.4×10<sup>−4</sup>). At the same V- sites, however, within-category VOT does not reliably predict response amplitude among (non-preferred) voiced stimuli (Wilcoxon signed-rank test: <italic>z</italic> = −1.60, p=0.11; <xref ref-type="fig" rid="fig2">Figure 2B</xref>: differences among solid blue lines but not dashed red lines). Across all V- and V+ electrodes, peak high-gamma response amplitude encoded stimulus VOT within the preferred category (Wilcoxon signed-rank test: <italic>z</italic> = 6.02, p=1.7×10<sup>−9</sup>), but not the non-preferred category (Wilcoxon signed-rank test: <italic>z</italic> = 1.31, p=0.19). While V- electrodes encoded sub-phonetic VOT more robustly within the voiceless category than within the voiced category (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, left; Wilcoxon signed-rank test: <italic>z</italic> = 3.00, p=2.7×10<sup>−3</sup>), the opposite pattern emerged for V+ electrodes, which encoded sub-phonetic VOT more robustly within the voiced category than within the voiceless category (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, right; Wilcoxon signed-rank test: <italic>z</italic> = 3.78, p=1.6×10<sup>−4</sup>).</p><p>Together, these analyses revealed two key results: (1) VOT encoding in human STG is not purely categorical, but also (2) the relationship between response amplitude and VOT is not linear across the entire continuum (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). These results suggest that, even at the level of STG, the brain maintains information about the specific, sub-phonetic details of individual speech sounds. The asymmetrical pattern of within-category encoding suggests that individual neural populations in human auditory cortex encode information about both the category identity of a speech sound and its more fine-grained acoustic properties, or its <italic>category goodness</italic> (<xref ref-type="bibr" rid="bib47">Kuhl, 1991</xref>; <xref ref-type="bibr" rid="bib4">Blumstein et al., 2005</xref>; <xref ref-type="bibr" rid="bib74">Myers, 2007</xref>).</p></sec><sec id="s2-4"><title>A simple neural network model of VOT encoding in STG</title><p>Thus far, we have demonstrated that a temporal cue that distinguishes speech sounds is represented by a spatial/amplitude code (<xref ref-type="bibr" rid="bib26">Ferster and Spruston, 1995</xref>; <xref ref-type="bibr" rid="bib94">Shadlen and Newsome, 1994</xref>) in human STG. To understand how this could be implemented computationally in the brain, we built an architecturally minimalistic neural network (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, top). The network was designed to implement a small set of basic computations, motivated by well-established models of temporal processing (<xref ref-type="bibr" rid="bib5">Buonomano and Merzenich, 1995</xref>; <xref ref-type="bibr" rid="bib33">Gao and Wehr, 2015</xref>; <xref ref-type="bibr" rid="bib19">Eggermont, 2000</xref>; <xref ref-type="bibr" rid="bib8">Carr, 1993</xref>; <xref ref-type="bibr" rid="bib43">Konishi, 2003</xref>; <xref ref-type="bibr" rid="bib90">Rauschecker, 2014</xref>; <xref ref-type="bibr" rid="bib89">Rauschecker, 1998</xref>). Specifically, our model employs discrete integrator units that detect temporal gaps or coincidences between distinct spectral events by incorporating canonical neurophysiological mechanisms that allow current input to modulate a unit’s sensitivity to subsequent input in highly specific ways.</p><p>The entire model is comprised of just five localist units: a burst detector, a voicing detector, a gap detector (<bold><italic><sc>G</sc></italic></bold><sc>ap</sc>), a coincidence detector (<bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>), and an inhibitory unit. Conventional leaky integrator dynamics governed continuously varying activation values of each rectified linear unit within the model (<xref ref-type="bibr" rid="bib66">McClelland and Rumelhart, 1981</xref>; <xref ref-type="bibr" rid="bib64">McClelland et al., 2014</xref>), with the activity <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> of a given unit <inline-formula><mml:math id="inf2"><mml:mi>i</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf3"><mml:mi>t</mml:mi></mml:math></inline-formula> depending on its prior activity <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, the weighted sum of its excitatory and inhibitory inputs <inline-formula><mml:math id="inf5"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and unit-specific activation parameters (e.g., propagation threshold [<inline-formula><mml:math id="inf6"><mml:mi>θ</mml:mi></mml:math></inline-formula>], decay rate). To illustrate intuitively how time-dependent neuronal properties can give rise to spatially-localized temporal cue processing, model parameters and connection weights were set manually (see Methods for details; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). We presented the network with simplified inputs mimicking the spectral and temporal properties of the six VOT stimuli used in the ECoG experiment (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; see Materials and methods; <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Presentation of burst and voicing inputs triggered propagation of activation that spread through the network, and our analyses assessed how the resulting activation dynamics differed depending on VOT.</p><p>The simulated responses of <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> to VOTs of 0–50 ms are shown in <xref ref-type="fig" rid="fig2">Figure 2F/G</xref>. We observed striking qualitative similarities between <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> simulated outputs (<xref ref-type="fig" rid="fig2">Figure 2F</xref>) and the real neural responses of V- electrodes (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and between <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> outputs (<xref ref-type="fig" rid="fig2">Figure 2G</xref>) and the V+ electrodes (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). By design, voicing category is clearly distinguished in both <italic><sc>G</sc></italic><sc>ap</sc> and <italic><sc>C</sc></italic><sc>oinc</sc>, with <italic><sc>G</sc></italic><sc>ap</sc> responding more strongly to longer (voiceless) VOTs (30–50 ms), and <italic><sc>C</sc></italic><sc>oinc</sc> responding more strongly to shorter (voiced) VOTs (0–20 ms). This demonstrates that spatial encoding of temporal cues (gaps vs. coincidences) can arise naturally within a simple, biologically-inspired neural network (<xref ref-type="bibr" rid="bib5">Buonomano and Merzenich, 1995</xref>; <xref ref-type="bibr" rid="bib33">Gao and Wehr, 2015</xref>; <xref ref-type="bibr" rid="bib19">Eggermont, 2000</xref>; <xref ref-type="bibr" rid="bib8">Carr, 1993</xref>; <xref ref-type="bibr" rid="bib43">Konishi, 2003</xref>; <xref ref-type="bibr" rid="bib90">Rauschecker, 2014</xref>; <xref ref-type="bibr" rid="bib89">Rauschecker, 1998</xref>).</p><p>Perhaps more surprisingly, we also found that both <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> detector units exhibit sensitivity to within-category VOT distinctions (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). These partially graded activations mirror the pattern observed in the neural data (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), where V- electrodes and <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> units are only sensitive to differences among long (voiceless) VOTs, and V+ electrodes and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> units are only sensitive to differences among short (voiced) VOTs.</p><p>These relatively sophisticated dynamics are the natural result of well-established computational and physiological mechanisms. Within the model, the burst and voicing detector units are tuned to respond independently to distinct spectral cues in the simulated acoustic input. Hence, the relative timing of their responses, but not their amplitudes, differ as a function of VOT. Both the gap (<bold><italic><sc>G</sc></italic></bold><sc>ap</sc>) and the coincidence (<bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>) detector units receive excitatory input from both the burst and voicing detector units, but <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>. differ in how they integrate these inputs over time. Specifically, as described below, while initial excitatory input (from the burst detector) temporarily <italic>decreases</italic> the sensitivity of <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> to immediate subsequent excitatory input (from the voicing detector), the opposite is true of <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>.</p><p>In particular, prior work has shown that one computational implementation of gap detection involves configuration of a <italic>slow inhibitory postsynaptic potential</italic> (<italic>IPSP</italic>) microcircuit (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, middle) (<xref ref-type="bibr" rid="bib5">Buonomano and Merzenich, 1995</xref>; <xref ref-type="bibr" rid="bib33">Gao and Wehr, 2015</xref>; <xref ref-type="bibr" rid="bib17">Douglas and Martin, 1991</xref>; <xref ref-type="bibr" rid="bib67">McCormick, 1989</xref>). In our model, activity in the burst detector following burst onset elicits fast suprathreshold <italic>excitatory postsynaptic potentials</italic> (<italic>EPSPs</italic>) in both <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and the inhibitory unit, immediately followed by a longer-latency (‘slow’) IPSP in <bold><italic><sc>G</sc></italic></bold><sc>ap</sc>. This slow IPSP renders <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> temporarily insensitive to subsequent excitatory input from the voicing detector, meaning that voicing-induced excitation that arrives too soon (e.g., 10 ms) after the burst input, when inhibition is strongest, is not able to elicit a second suprathreshold EPSP in <bold><italic><sc>G</sc></italic></bold><sc>ap</sc>. Consequently, all short VOTs (below some threshold) elicit uniformly weak responses in <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> that reflect only the initial excitatory response to the burst (see, e.g., indistinguishable responses to 0 ms and 10 ms VOTs in <xref ref-type="fig" rid="fig2">Figure 2F</xref>). However, as <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> gradually recovers from the burst-induced slow IPSP, later-arriving voicing input (i.e., longer VOTs) tends to elicit suprathreshold responses that grow increasingly stronger with longer gaps, until <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> has reached its pre-IPSP (resting) baseline. In this way, our implementation of gap detection naturally captures three key patterns observed across V- electrodes (<xref ref-type="fig" rid="fig2">Figure 2H</xref>, left; <xref ref-type="fig" rid="fig2">Figure 2D</xref>, left): (1) amplitude encoding of a temporally cued category (selectivity for gaps over coincidences); (2) amplitude encoding of within-category differences in the preferred category (amplitude differences among gaps of different durations); and (3) no amplitude encoding of differences within the non-preferred category (uniformly lower amplitude responses to short VOTs of any duration).</p><p>In contrast, coincidence detection (<xref ref-type="bibr" rid="bib8">Carr, 1993</xref>; <xref ref-type="bibr" rid="bib43">Konishi, 2003</xref>; <xref ref-type="bibr" rid="bib90">Rauschecker, 2014</xref>; <xref ref-type="bibr" rid="bib61">Margoliash and Fortune, 1992</xref>; <xref ref-type="bibr" rid="bib85">Peña and Konishi, 2001</xref>; <xref ref-type="bibr" rid="bib86">Pena and Konishi, 2002</xref>; <xref ref-type="fig" rid="fig2">Figure 2E</xref>, bottom) emerges in the model because activity in the burst detector evokes only a subthreshold EPSP in <italic><sc>C</sc></italic><sc>oinc</sc>, temporarily increasing the sensitivity of <italic><sc>C</sc></italic><sc>oinc</sc> to immediate subsequent excitatory input (from the voicing detector). During this period of heightened sensitivity, voicing-induced excitatory input that arrives simultaneously or after short lags can elicit larger amplitude (additive) EPSPs than could voicing-induced excitatory input alone. Because the magnitude of the initial burst-induced EPSP gradually wanes, the summation of EPSPs (from the burst and voicing) is greatest (and hence elicits the strongest response) for coincident burst and voicing (0 ms VOT), and the magnitude of the <italic><sc>C</sc></italic><sc>oinc</sc> response to other voiced stimuli (e.g., 10–20 ms VOTs) becomes weaker as the lag between burst and voicing increases. Finally, in voiceless stimuli, since voicing arrives late enough after the burst (30+ ms) that there is no residual boost to the <italic><sc>C</sc></italic><sc>oinc</sc> baseline post-synaptic potential, elicited responses are entirely driven by a suprathreshold voicing-induced EPSP that reaches the same peak amplitude for all voiceless stimuli. Thus, our implementation of coincidence detection captures three key patterns observed in V+ electrodes (<xref ref-type="fig" rid="fig2">Figure 2H</xref>, right; <xref ref-type="fig" rid="fig2">Figure 2D</xref>, right): (1) amplitude encoding of a temporally cued category (selectivity for coincidences over gaps); (2) amplitude encoding of within-category differences in the preferred category (amplitude differences among stimuli with short VOTs); and (3) no amplitude encoding of differences within the non-preferred category (uniformly lower amplitude responses to long VOTs of any duration).</p><p>In summary, the neurophysiological dynamics underlying local STG encoding of VOT can be modeled using a simple, biologically-inspired neural network. The computational model captures both the between-category (phonetic) and within-category (sub-phonetic) properties of observed neural representations via well-established physiological mechanisms for gap and coincidence detection (<xref ref-type="bibr" rid="bib5">Buonomano and Merzenich, 1995</xref>; <xref ref-type="bibr" rid="bib33">Gao and Wehr, 2015</xref>; <xref ref-type="bibr" rid="bib19">Eggermont, 2000</xref>; <xref ref-type="bibr" rid="bib8">Carr, 1993</xref>; <xref ref-type="bibr" rid="bib43">Konishi, 2003</xref>; <xref ref-type="bibr" rid="bib90">Rauschecker, 2014</xref>; <xref ref-type="bibr" rid="bib89">Rauschecker, 1998</xref>).</p></sec><sec id="s2-5"><title>Mechanisms that explain local category selectivity also predict early temporal dynamics</title><p>Thus far, we have focused on the encoding of speech sounds that differ in VOT based on activity patterns around the peak of the evoked response. However, in comparing the real and simulated neural data (<xref ref-type="fig" rid="fig2">Figure 2</xref>), we also observed a qualitative resemblance with respect to the onset latencies of evoked responses. Specifically, the timing of the evoked neural responses (relative to burst onset) appeared to depend on stimulus VOT in V+ electrodes and in the coincidence detector (<italic><sc>C</sc></italic><sc>oinc</sc>) unit (<xref ref-type="fig" rid="fig2">Figure 2C/G</xref>), but not in V- electrodes or in the gap detector (<italic><sc>G</sc></italic><sc>ap</sc>) unit (<xref ref-type="fig" rid="fig2">Figure 2B/F</xref>). This pattern could suggest that early temporal dynamics of the evoked response contribute to the pattern of category selectivity observed at the peak.</p><p>We examined the neural activity evoked by each VOT stimulus in V- and V+ electrodes at the onset of the response, typically beginning approximately 75–125 ms after stimulus (burst) onset. In the same two example electrodes from <xref ref-type="fig" rid="fig1">Figure 1E</xref>, we observed clear differences in the relationship between response onset latency and VOT (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). To quantify the onset latency for each electrode to each VOT stimulus, we found the first timepoint after stimulus onset where the evoked high gamma response exceeded 50% of the electrode’s overall peak amplitude (grand mean across conditions). The rank correlation between VOT and response onset latency for e1 (a V- electrode) was substantially lower (Spearman’s ρ = 0.42) than for e2 (a V+ electrode; ρ = 0.89).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Early temporal dynamics of stimulus-evoked neural responses differ between voiceless-selective (V-) and voiced-selective (V+) electrodes.</title><p>(<bold>A</bold>) Normalized trial-averaged HG responses to each VOT stimulus (line color) in two example electrodes (e1 and e2; same electrodes shown in <xref ref-type="fig" rid="fig1">Figure 1D/E</xref>). The time window (x-axis) is relative to onset of the burst and precedes the peak response. Horizontal bars show estimates (bootstrapped mean ± SE) of response onset latency for each VOT (first timepoint exceeding 50% of electrode’s average peak HG). Mean bootstrapped rank-based correlation (Spearman’s ρ) between VOT and response onset latency shown for e1 (blue) and e2 (red). (<bold>B</bold>) Across all V- electrodes, the bootstrapped correlation coefficients did not differ significantly from 0, suggesting that onset latency was time-locked to the burst. In contrast, across all V+ electrodes, the bootstrapped correlation coefficients were reliably positive (longer latencies for longer VOTs), and greater than for V- electrodes. Circles represent individual electrodes (filled: example electrodes in <bold>A</bold>). Boxes show interquartile range; whiskers extend to maximum/minimum of each group (excluding two outlier V+ electrodes); vertical bars are medians. Asterisks indicate significance (p&lt;10<sup>−4</sup>; n.s. = not significant).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-fig3-v2.tif"/></fig><p>A bootstrapped rank-based correlation coefficient was computed for each V- and V+ electrode (1000 resamples; see Methods). We found that response onset latency was strongly associated with VOT for V+, but not V-, electrodes (Wilcoxon signed-rank tests: V+, p=1.6×10<sup>−6</sup>; V-, p=0.57), and this difference between the two electrode types was highly reliable (Mann-Whitney rank-sum test: p=1.7×10<sup>−5</sup>) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>The association between VOT and response latency also differed in <italic><sc>G</sc></italic><sc>ap</sc> versus <italic><sc>C</sc></italic><sc>oinc</sc> units in the model simulations (<xref ref-type="fig" rid="fig2">Figure 2F/G</xref>), with VOT-dependent response latencies emerging for <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>, but not <bold><italic><sc>G</sc></italic></bold><sc>ap</sc>. Closer examination of the model’s internal dynamics reveals how the same time-dependent mechanisms that give rise to peak amplitude encoding of VOT are also responsible for these early temporal dynamics. As described above, the category selectivity of <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> (voiceless) and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> (voiced) results from how each unit’s subsequent activity is modulated after detection of the burst. While the burst always elicits a fast suprathreshold response in <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> (irrespective of VOT), the <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> response to the burst alone is subthreshold (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, middle vs. bottom). Consequently, the initial <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> response is evoked by the burst of any VOT stimulus, so the response onset latency (when aligned to burst onset) does not depend on VOT (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Conversely, the earliest suprathreshold <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> response is triggered by the onset of voicing, so the response onset latency (relative to burst onset) is later for longer VOTs (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). Thus, the same well-established physiological mechanisms that give rise to peak amplitude encoding of temporally-cued voicing categories also predict the early temporal dynamics we observe in real neural data.</p><p>Finally, <xref ref-type="fig" rid="fig3">Figure 3</xref> shows that, unlike during the peak response window (150–250 ms after stimulus onset; <xref ref-type="fig" rid="fig1">Figure 1F</xref>), temporal information does encode VOT during an earlier window around the neural response onset in some neural populations. Indeed, both sub-phonetic and phonetic category-level information are carried by the onset latency of V+ electrodes, with evoked responses arising later at these sites for stimuli with progressively longer VOTs. Critically, the modeling results indicate that both the amplitude encoding patterns during the peak window and the temporal encoding patterns during the earlier onset window are captured by the same canonical neurophysiological mechanisms.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study investigated how voice-onset time (VOT), a temporal cue in speech, is represented in human auditory cortex. Using direct intracranial recordings, we found discrete neural populations located primarily on the bilateral posterior and middle STG that respond preferentially to either voiced sounds, where the onset of voicing is coincident with the burst or follows it after a short lag (20 ms or less), or voiceless sounds, where the onset of voicing follows the burst after a temporal gap of at least 30–50 ms.</p><p>Past work has also found that phonetic information about speech sounds is encoded in the amplitude of evoked neural responses at spatially localized cortical sites (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>). In that work, however, STG activity was shown to encode the spectral properties of speech sounds most robustly, such as whether a phoneme is a vowel or a consonant and whether a consonant’s spectrum is broadband (as in plosives, like /<italic>b</italic>/ and /<italic>p</italic>/) or is dominated by acoustic energy at high frequencies (as in fricatives, like /<italic>f</italic>/ and /<italic>s</italic>/).</p><p>The present results extend these earlier findings in a critical way, suggesting that the cortical representation of both spectral and temporal cues in speech follow a common spatial coding scheme. This result is also consistent with prior reports that neural response amplitude depends on VOT (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>), but such results have often involved natural speech stimuli where voicing categories varied along many other spectral acoustic dimensions besides the temporal cue (<xref ref-type="bibr" rid="bib56">Lisker, 1986</xref>; <xref ref-type="bibr" rid="bib98">Soli, 1983</xref>; <xref ref-type="bibr" rid="bib108">Stevens and Klatt, 1974</xref>; <xref ref-type="bibr" rid="bib109">Summerfield and Haggard, 1977</xref>). Here, the digitally synthesized VOT stimuli were tightly controlled to vary only in the relative timing of two invariant spectral cues (burst and voicing), thereby demonstrating that this temporal speech cue is encoded in the peak high-gamma response amplitude of spatially distinct neural populations in human STG.</p><p>While the present results clearly implicate a spatial/amplitude code in the cortical representation of VOT, other work has described VOT-dependent temporal response patterns that can also be used to encode voicing categories (<xref ref-type="bibr" rid="bib18">Eggermont, 1995</xref>; <xref ref-type="bibr" rid="bib21">Eggermont and Ponton, 2002</xref>; <xref ref-type="bibr" rid="bib55">Liégeois-Chauvel et al., 1999</xref>). For instance, Steinschneider and colleagues have observed neurons and neuronal populations in primate and human auditory cortices in which short VOTs elicit a single-peaked neural response, while longer VOTs elicit a double-peaked response (<xref ref-type="bibr" rid="bib101">Steinschneider et al., 1999</xref>; <xref ref-type="bibr" rid="bib106">Steinschneider et al., 2013</xref>; <xref ref-type="bibr" rid="bib105">Steinschneider et al., 2011</xref>; <xref ref-type="bibr" rid="bib103">Steinschneider et al., 2005</xref>; <xref ref-type="bibr" rid="bib99">Steinschneider et al., 1994</xref>; <xref ref-type="bibr" rid="bib100">Steinschneider et al., 1995</xref>; <xref ref-type="bibr" rid="bib102">Steinschneider et al., 2003</xref>). Under this ‘local’ temporal coding model, the precise temporal dynamics of the response evoked at a single cortical site could distinguish voiced from voiceless VOTs. Our examination of the timing and amplitude of three peaks in the auditory evoked local field potentials of VOT-sensitive electrodes confirmed that such patterns do appear in some electrodes (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>), clearly demonstrating that temporal and amplitude codes for VOT are not mutually exclusive (see also temporal encoding patterns in onset latencies of V+ electrodes; <xref ref-type="fig" rid="fig3">Figure 3</xref>). However, as with spectrally-defined phonetic contrasts (e.g., plosive vs. fricative; <xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>), it clear that the amplitude of the peak high-gamma (and, in many cases, of the LFP) response emerged as a robust representation of voicing category and of VOT.</p><p>VOT could also be encoded in the relative timing of responses in spatially-distributed, spectrally-tuned burst- and voicing-selective neural populations. Under this ‘ensemble’ temporal coding model (<xref ref-type="bibr" rid="bib111">Theunissen and Miller, 1995</xref>; <xref ref-type="bibr" rid="bib23">Engineer et al., 2008</xref>), the pattern of neural activity evoked by voiced VOTs (characterized by roughly coincident burst and voicing cues) would differ from the pattern evoked by voiceless VOTs in the precise temporal latency of the response in a vowel-selective neural population (a voicing detector) compared to the response in a plosive-selective neural population (a burst detector). However, the fact that we found cortical sites in every participant that exhibited robust category-dependent differences in their peak response amplitude rules out the possibility that at least these neural populations are merely responding to spectral cues in the burst or voicing alone.</p><p>Notably, if either (or both) of these models – a local or ensemble temporal code – was primarily responsible for the neural representation of VOT in the high-gamma range, then the selective corruption of temporal information in a classifier (<xref ref-type="fig" rid="fig1">Figure 1F</xref>) should have reduced neural decoding of voicing category to chance levels, while corrupting peak amplitude information should have had little or no effect. We found the opposite pattern of results: corrupting peak amplitude information had a devastating effect on the decoding of voicing category, while corrupting the fine temporal patterns that could have discriminated between voicing categories had no measurable impact on classifier performance. To be clear, our work does not rule out the possibility that local or ensemble temporal codes may also play a role in the cortical representation of VOT. However, it does highlight spatially-localized peak neural response amplitude as a robust code for VOT. Thus, in contrast to prior work theorizing parallel, but fundamentally different, coding schemes for spectrally- and temporally-cued phonetic features (<xref ref-type="bibr" rid="bib101">Steinschneider et al., 1999</xref>; <xref ref-type="bibr" rid="bib106">Steinschneider et al., 2013</xref>), we demonstrate evidence for a shared representation of both by high-gamma responses in the human superior temporal lobe.</p><p>In order to explicitly test potential computational and physiological mechanisms that could give rise to the observed spatial coding scheme, we implemented an architecturally simple neural network model. Although it is well known that spectral information is represented by a spatial neural code from the earliest stages of auditory transduction in the cochlea (<xref ref-type="bibr" rid="bib20">Eggermont, 2001</xref>; <xref ref-type="bibr" rid="bib82">Oxenham, 2018</xref>), the emergence of a spatial code for the representation of temporally-distributed cues in a transient acoustic signal poses a nontrivial computational problem. Our model highlights one parsimonious approach by which selectivity for either temporal gaps or coincidences could be implemented by biologically-inspired neurophysiological microcircuits (<xref ref-type="bibr" rid="bib5">Buonomano and Merzenich, 1995</xref>; <xref ref-type="bibr" rid="bib33">Gao and Wehr, 2015</xref>; <xref ref-type="bibr" rid="bib19">Eggermont, 2000</xref>; <xref ref-type="bibr" rid="bib8">Carr, 1993</xref>; <xref ref-type="bibr" rid="bib43">Konishi, 2003</xref>; <xref ref-type="bibr" rid="bib90">Rauschecker, 2014</xref>; <xref ref-type="bibr" rid="bib89">Rauschecker, 1998</xref>).</p><p>We found that, just like in the neural data, gap and coincidence detector units responded to simulated voiced (/<italic>b</italic>/) and voiceless (/<italic>p</italic>/) stimuli with different response amplitudes. As such, we need not invoke any specialized temporal code to account for the representation of temporally cued phonetic features. Rather, our results provide evidence implicating a common neural coding scheme in the neural representation of behaviorally relevant speech features, whether they are embedded within the instantaneous spectrum or the fine temporal structure of the speech signal. Recent ECoG evidence suggests an even more expansive view of the fundamental role of spatial coding in cortical speech representation (<xref ref-type="bibr" rid="bib115">Yi et al., 2019</xref>) in which different neural populations also encode pitch (<xref ref-type="bibr" rid="bib110">Tang et al., 2017</xref>) and key properties of the speech envelope such as onsets and auditory edges (<xref ref-type="bibr" rid="bib36">Hamilton et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Oganian and Chang, 2019</xref>).</p><p>Crucially, although the neural network was only designed to discriminate between categories (i.e., gaps vs. coincidences), we also observed graded amplitude differences in response to different VOTs (<xref ref-type="fig" rid="fig2">Figure 2H</xref>), but only in an electrode’s preferred category. These within-category patterns emerged naturally from the same computational properties that allowed the network to capture basic between-category encoding: (1) the relative responsiveness of each temporal integrator unit (<bold><italic><sc>G</sc></italic></bold><sc>ap</sc>, <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>) to its various inputs (burst, voicing, and inhibition); (2) the time-dependent properties inherent to neuronal activation dynamics (e.g., decay of postsynaptic potentials towards a unit’s resting activation level); and (3) the nonlinear transformation of postsynaptic inputs into response outputs (rectified linear activation function controlled by a unit’s propagation threshold).</p><p>This asymmetric within-category encoding scheme closely resembled the pattern observed in real neurophysiological data, where peak response amplitude to VOTs within the same voicing category only differed within a neural population’s preferred category (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). This result clearly demonstrates that human nonprimary auditory cortex maintains a robust, graded representation of VOT that includes the sub-phonetic details about how a particular speech token was pronounced (<xref ref-type="bibr" rid="bib4">Blumstein et al., 2005</xref>; <xref ref-type="bibr" rid="bib112">Toscano et al., 2010</xref>; <xref ref-type="bibr" rid="bib113">Toscano et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Frye et al., 2007</xref>). Even though sub-phonetic information is not strictly necessary for mapping sound to meaning in stable, noise-free listening environments, this fine-grained acoustic detail has demonstrable effects on listeners’ behavior (<xref ref-type="bibr" rid="bib47">Kuhl, 1991</xref>; <xref ref-type="bibr" rid="bib7">Carney, 1977</xref>; <xref ref-type="bibr" rid="bib87">Pisoni and Tash, 1974</xref>; <xref ref-type="bibr" rid="bib62">Massaro and Cohen, 1983</xref>; <xref ref-type="bibr" rid="bib2">Andruski et al., 1994</xref>; <xref ref-type="bibr" rid="bib68">McMurray et al., 2002</xref>; <xref ref-type="bibr" rid="bib93">Schouten et al., 2003</xref>), and modern theories of speech perception agree that perceptual learning (e.g., adaptation to accented speakers) and robust cue integration would be impossible if the perception of speech sounds were strictly categorical (<xref ref-type="bibr" rid="bib73">Miller and Volaitis, 1989</xref>; <xref ref-type="bibr" rid="bib12">Clayards et al., 2008</xref>; <xref ref-type="bibr" rid="bib42">Kleinschmidt and Jaeger, 2015</xref>; <xref ref-type="bibr" rid="bib69">McMurray and Jongman, 2011</xref>; <xref ref-type="bibr" rid="bib114">Toscano and McMurray, 2010</xref>; <xref ref-type="bibr" rid="bib65">McClelland and Elman, 1986</xref>; <xref ref-type="bibr" rid="bib78">Norris and McQueen, 2008</xref>; <xref ref-type="bibr" rid="bib77">Norris et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Magnuson et al., 2020</xref>). Crucially, these data suggest that the same spatial/amplitude code that is implicated in the representation of <italic>phonetic</italic> information (from spectral or temporal cues) can also accommodate the representation of <italic>sub-phonetic</italic> information in the speech signal.</p><p>The onset latency results (<xref ref-type="fig" rid="fig3">Figure 3</xref>) established an entirely novel correspondence between the real and simulated results that extended beyond the peak response window. Response onset latencies of V- electrodes were time-locked to the burst (<xref ref-type="fig" rid="fig2">Figures 2B</xref> and <xref ref-type="fig" rid="fig3">3</xref>), while responses of V+ electrodes were time-locked to voicing onset (<xref ref-type="fig" rid="fig2">Figures 2C</xref> and <xref ref-type="fig" rid="fig3">3</xref>). These highly reliable neurophysiological results neatly match specific predictions of our parsimonious model without the need to postulate additional mechanisms (<xref ref-type="fig" rid="fig2">Figure 2F/G</xref>).</p><p>The correspondence between simulated and real neural data in the onset latency results may also have implications for the question of whether the observed temporal integration is occurring locally in STG or is inherited from earlier levels of auditory processing (e.g., from midbrain or primary auditory cortex). The model’s gap and coincidence detectors (<bold><italic><sc>G</sc></italic></bold><sc>ap</sc>, <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>) are designed to directly simulate neural populations in the STG. Their inputs from the burst and voicing detectors are only spectrally processed, so in the model, the temporal onset latency dynamics (<xref ref-type="fig" rid="fig2">Figure 2F/G</xref>) first arise in <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>. As such, the fact that the model’s prediction is borne out in the neural data in STG (<xref ref-type="fig" rid="fig2">Figure 2B, C</xref> and <xref ref-type="fig" rid="fig3">3</xref>) is consistent with local temporal integration in STG. While these modeling results do not definitively rule out temporal integration at lower levels of the ascending auditory pathway, its potentially local emergence in high-order auditory cortex illustrates how even relatively simple computational models can be used to generate novel hypotheses, which can ultimately be tested in real neurophysiological data.</p><p>Overall, the results of these model simulations illustrate how the same network properties that transform temporal cues into a spatial code are also able to naturally explain at least three additional patterns observed within category-selective neural populations: (1) the graded encoding of VOT within a population’s preferred category; (2) the lack of graded encoding of VOT within a population’s non-preferred category; and (3) the early temporal dynamics of neural responses, which depend on a population’s category-selectivity. Thus, the model provides an explicit, mathematical account of multiple seemingly disparate observations about the neurophysiological data, all of which arise directly from a parsimonious implementation of gap- and coincidence-detection with well-established, theoretically-motivated neuronal circuits.</p><p>The model we present is just one of many possible architectures that could capture these interesting properties of the neural response. For example, mechanisms like temporal delay lines (<xref ref-type="bibr" rid="bib8">Carr, 1993</xref>; <xref ref-type="bibr" rid="bib90">Rauschecker, 2014</xref>) could also be used to implement gap detection. Broadly, we chose to implement a simple hand-tuned neural network model to maximize our ability to explore the detailed dynamics we observed in the neural data. Our approach follows a rich history of using these types of hand-tuned models to explain a wide array of cognitive and perceptual phenomena (including the perception of VOT in speech), as exemplified by the influential TRACE model of speech perception (<xref ref-type="bibr" rid="bib65">McClelland and Elman, 1986</xref>). An alternative approach to modeling VOT perception is to train a neural network to distinguish voiced from voiceless sounds based on distributed activation dynamics within biologically-grounded spectral processing maps (<xref ref-type="bibr" rid="bib15">Damper, 1994</xref>). Our model borrows aspects of these two approaches (hand-tuning; biological plausibility) and it extends this past work by directly modeling the time-dependent mechanisms that could give rise to continuously-varying neural responses in STG.</p><p>While the model captured several notable features of the neural data (including some for which it was not explicitly designed), we observed at least one inconsistency between the simulated and real neural responses. The model predicted VOT-dependence in the latency of the <italic>peak</italic> response in both <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> units (<xref ref-type="fig" rid="fig2">Figure 2F/G</xref>), but we did not find evidence for these fine-grained patterns in the high-gamma data (<xref ref-type="fig" rid="fig2">Figure 2B/C</xref>; see also lack of category-dependent temporal patterns during peak window: <xref ref-type="fig" rid="fig1">Figure 1F</xref>). However, it is unclear whether this is a false prediction of the model, or whether we did not observe the effect in the neural data because of, for example, poor signal-to-noise ratio for this effect. Regardless of whether the discrepancy arises from the model or the real data, it represents a gap in our mechanistic understanding of the processing of this phenomenon, and should therefore be a target for further research.</p><p>Although topographic functional organization is pervasive among many spatial neural coding schemes described in sensory neuroscience, including for the representation of spectral and temporal acoustic cues in audition (e.g., tonotopy in mammalian auditory cortex; <xref ref-type="bibr" rid="bib20">Eggermont, 2001</xref>; <xref ref-type="bibr" rid="bib82">Oxenham, 2018</xref> or chronotopy in bats; <xref ref-type="bibr" rid="bib45">Kössl et al., 2014</xref>; <xref ref-type="bibr" rid="bib88">Portfors and Wenstrup, 2001</xref>), this functional organization seems not to extend to the spatial code for speech on the lateral temporal cortex in humans. As with tuning for spectrally-cued phonetic features (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Hamilton et al., 2018</xref>) (e.g., plosives vs. fricatives), VOT-sensitive neural populations in the present study were scattered throughout posterior and middle superior temporal gyrus with no discernible topographical map of selectivity or evidence for lateralized asymmetries (<xref ref-type="bibr" rid="bib55">Liégeois-Chauvel et al., 1999</xref>; <xref ref-type="bibr" rid="bib116">Zatorre and Belin, 2001</xref>), although data limitations prevent us from ruling out this possibility entirely (for detailed results, see Material and methods).</p><p>Most of the present analyses focused on the high-gamma component of the neural response, but this work does not discount a potential role for lower-frequency oscillations in speech perception (<xref ref-type="bibr" rid="bib31">Fries, 2009</xref>; <xref ref-type="bibr" rid="bib34">Giraud and Poeppel, 2012</xref>) or in the perception of phonemes (<xref ref-type="bibr" rid="bib44">Kösem et al., 2018</xref>; <xref ref-type="bibr" rid="bib84">Peelle and Davis, 2012</xref>). Indeed, it is clear from the exploratory analyses of auditory evoked local field potentials (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>) that there do exist complex associations between VOT and the amplitude/temporal information carried in lower-frequency ranges. Future work should systematically investigate the relationship between high-gamma and other neural signals (such as the local field potential), their relative contributions to the perceptual experience of and neural representation of speech, and the importance of detailed temporal information in each (see, e.g., <xref ref-type="bibr" rid="bib79">Nourski et al., 2015</xref>).</p><p>Finally, it is critical to distinguish our results from studies describing neural correlates of categorical speech perception, per se (e.g., <xref ref-type="bibr" rid="bib9">Chang et al., 2010</xref>). Neural responses to different VOT tokens that are members of the same voicing category can only be considered truly categorical if the responses are indiscriminable (e.g., <xref ref-type="bibr" rid="bib51">Liberman et al., 1957</xref>; <xref ref-type="bibr" rid="bib59">Macmillan et al., 1977</xref>). In our results, acoustically distinct members of the same phonetic category <italic><underline>are</underline></italic> distinguishable in neural populations that are selective for that voicing category (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In light of this graded VOT representation, the present results are best interpreted as elucidating neural mechanisms of category perception, but not necessarily categorical perception, of voiced vs. voiceless stop consonants. While limited coverage beyond the superior temporal lobe precludes us from ruling out the influence of top-down categorical perception (<xref ref-type="bibr" rid="bib48">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib75">Myers et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Evans and Davis, 2015</xref>) (possibly originating in frontal cortex; <xref ref-type="bibr" rid="bib97">Sohoglu et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Leonard et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Cope et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Park et al., 2015</xref>) on our results, it is notable that the model we present (which does not posit top-down effects) suggests that top-down effects may not be a necessary condition for explaining the observed non-linear encoding patterns (see also <xref ref-type="bibr" rid="bib63">McClelland et al., 2006</xref>; <xref ref-type="bibr" rid="bib70">McQueen et al., 2006</xref>; <xref ref-type="bibr" rid="bib76">Norris et al., 2000</xref>; <xref ref-type="bibr" rid="bib65">McClelland and Elman, 1986</xref>; <xref ref-type="bibr" rid="bib78">Norris and McQueen, 2008</xref>).</p><p>In conclusion, the present results show that spatially-discrete neural populations in human auditory cortex are tuned to detect either gaps or coincidences between spectral cues, and these sites simultaneously represent both phonetic and sub-phonetic information carried by VOT, a temporal speech cue found in almost all languages (<xref ref-type="bibr" rid="bib57">Lisker and Abramson, 1964</xref>; <xref ref-type="bibr" rid="bib11">Cho and Ladefoged, 1999</xref>). This demonstrates a common (spatial) neural code in STG that accounts for the representation of behaviorally relevant phonetic features embedded within the spectral and temporal structure of speech. From a simple model that transforms a temporal cue into a spatial code, we observed complex dynamics that show how a highly variable, continuous sensory signal can give rise to partially abstract, discrete representations. In this way, our findings also add to a growing body of work highlighting the critical role of human STG as a sensory-perceptual computational hub in the human speech perception system (<xref ref-type="bibr" rid="bib115">Yi et al., 2019</xref>; <xref ref-type="bibr" rid="bib110">Tang et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Chang et al., 2010</xref>; <xref ref-type="bibr" rid="bib49">Leonard et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">DeWitt and Rauschecker, 2012</xref>; <xref ref-type="bibr" rid="bib80">Obleser and Eisner, 2009</xref>; <xref ref-type="bibr" rid="bib50">Leonard and Chang, 2014</xref>; <xref ref-type="bibr" rid="bib96">Sjerps et al., 2019</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data and code availability</title><p>All data and code associated with this study and necessary for replication of its results are available under a Creative Commons license at the associated Open Science Framework project page (<ext-link ext-link-type="uri" xlink:href="https://osf.io/9y7uh/">https://osf.io/9y7uh/</ext-link>) (<xref ref-type="bibr" rid="bib29">Fox et al., 2020</xref>).</p></sec><sec id="s4-2"><title>Participants</title><p>A total of seven human participants with self-reported normal hearing were implanted with high-density (128 or 256 electrodes; 4 mm pitch) multi-electrode cortical ECoG surface arrays as part of their clinical treatment for epilepsy. Placement of electrode arrays was determined based strictly on clinical criteria. For all patients who participated in this study, coverage included peri-Sylvian regions of the lateral left (n = 3) or right (n = 4) hemisphere, including the superior temporal gyrus (STG). All participants gave their written informed consent before the surgery and affirmed it at the start of each recording session. The study protocol was approved by the University of California San Francisco Committee on Human Research. Data from two additional participants were excluded from analyses because of excessive epileptiform activity (artifacts) during recording sessions.</p></sec><sec id="s4-3"><title>Imaging</title><p>Electrode positions (<xref ref-type="fig" rid="fig1">Figure 1D</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) were determined from post-surgical computed tomography (CT) scans and manually co-registered with the patient’s MRI. Details of electrode localization and warping to a standardized brain (MNI; <xref ref-type="fig" rid="fig2">Figure 2A</xref>) are described elsewhere (<xref ref-type="bibr" rid="bib35">Hamilton et al., 2017</xref>).</p></sec><sec id="s4-4"><title>Stimuli</title><p>Stimuli (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) were generated with a parallel/cascade Klatt-synthesizer KLSYN88a using a 20 kHz sampling frequency (5 ms frame width in parameter tracks). All stimulus parameters were identical across stimuli, with the exception of the time at which the amplitude of voicing began to increase (in 10 ms steps from 0 ms to 50 ms after burst onset; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The total duration of each stimulus was 300 ms regardless of VOT. The onset noise-burst was 2 ms in duration and had constant spectral properties across all stimuli. The dominant frequency ranges for the vowel were: F0 = 100 Hz; F1 = 736 Hz; F2 = 1221 Hz; F3 = 3241 Hz (consistent with a vocal tract length of 13.5 cm). Formant transitions always began at 30 ms. The vowel’s amplitude began ramping down 250 ms after stimulus onset. The stimuli are made available among this study’s supplementary materials and at the associated Open Science Framework page (<xref ref-type="bibr" rid="bib29">Fox et al., 2020</xref>).</p></sec><sec id="s4-5"><title>Behavioral procedure</title><p>During ECoG recording, the VOT stimuli were presented monaurally over free-field loudspeakers at a comfortably listening level via a custom MATLAB script (<xref ref-type="bibr" rid="bib29">Fox et al., 2020</xref>) in a blocked pseudorandom order. Four of seven participants simultaneously performed a behavioral task wherein they indicated on each trial whether they heard ‘ba’ or ‘pa’ using a touchscreen tablet (programmed using a custom MATLAB GUI). In these recording sessions, the onset of the next trial began 500 ms after a response was registered or 5 s after the end of the stimulus (if no response was registered). In sessions where participants chose to listen to the stimuli passively (instead of participating in the behavioral task), the onset of the next trial began approximately 1000 ms after the end of the previous trial. <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> reports number of trials per participant.</p></sec><sec id="s4-6"><title>Behavioral analysis</title><p>For the four participants who participated in the behavioral identification task, individual trials were excluded from behavioral analysis if a participant did not make a response or if the participant’s reaction time was more than three standard deviations from the participant’s mean reaction time.</p><p>Behavioral response data were submitted to mixed effects logistic regression with a fixed effect of VOT (coded as a continuous variable) and random intercepts for participants, allowing individual participants to vary in their voicing category boundary. Using the best-fit model estimates, we calculated the overall voicing category boundary across all participants (<inline-formula><mml:math id="inf7"><mml:mi>χ</mml:mi></mml:math></inline-formula> = 21.0ms; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, panel A) and in the each individual participant (after adjusting for random intercept fit for each participant; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, panel B, and <xref ref-type="fig" rid="fig1">Figure 1C</xref>) as follows (<xref ref-type="bibr" rid="bib25">Feldman et al., 2009</xref>), where <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the best-fit intercept and <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the best-fit effect of slope:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>χ</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p></sec><sec id="s4-7"><title>ECoG signal processing</title><sec id="s4-7-1"><title>Recording and preprocessing</title><p>Voltage fluctuations were recorded and amplified with a multichannel amplifier optically connected to a digital signal acquisition system (Tucker-Davis Technologies) sampling at approximately 3051.78 Hz. Line noise was removed via notch filtering (60 Hz and harmonics at 120 and 180 Hz) and the resulting time series for each session was visually inspected to exclude channels with excessive noise. Additionally, time segments with epileptiform activity were excluded. The time series data were then common-average referenced (CAR) to included electrodes either across an electrode’s row in a 16 × 16 channel grid or across the entire grid depending on the technical specifications of the amplifier used for a given participant.</p></sec><sec id="s4-7-2"><title>High-gamma extraction</title><p>The analytic amplitude of the high-gamma (HG; 70–150 Hz) frequency band was extracted by averaging across eight logarithmically-spaced bands with the Hilbert transform as described elsewhere (<xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>; <xref ref-type="bibr" rid="bib96">Sjerps et al., 2019</xref>). The HG signal was down-sampled to 400 Hz, providing temporal resolution to observe latency effects on the order of &lt;10 ms (the spacing of the VOTs of among the six experimental stimuli).</p></sec><sec id="s4-7-3"><title>Trial alignment and extraction</title><p>Trial epochs were defined as 500 ms before to 1000 ms after each stimulus onset. Trials were excluded for all channels if the epoch window contained any time segments that had been marked for exclusion during artifact rejection. The HG signal for each trial was z-scored based on the mean and standard deviation of a baseline window from 500 ms to 200 ms before stimulus onset. A 50 ms moving average boxcar filter was applied to the HG time series for each trial.</p></sec><sec id="s4-7-4"><title>Local field potential extraction</title><p>Data for analyses of auditory evoked local field potentials consisted of the same raw voltage fluctuations (local field potential), preprocessed with identical notch filtering, CAR, artifact/channel rejection, and down-sampling (to 400 Hz). Trial epochs (500 ms before to 1000 ms after each stimulus onset) were not z-scored.</p></sec></sec><sec id="s4-8"><title>Electrode selection</title><sec id="s4-8-1"><title>Speech-responsive electrodes</title><p>An electrode was included in our analyses if (1) it was anatomically located on the lateral temporal lobe (either superior or middle temporal gyrus), and (2) the electrode’s grand mean HG (across all trials and timepoints during a window 100–300 ms after stimulus onset) exceeded one standard deviation of the baseline window’s HG activity. Across all seven participants, 346 electrodes met these criteria (<italic>speech-responsive electrodes</italic>; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p></sec><sec id="s4-8-2"><title>Peak neural response</title><p>The timepoint at which each speech-responsive electrode reached its maximum HG amplitude (averaged across all trials, irrespective of condition) was identified as that electrode’s peak, which was used in the subsequent peak encoding analyses. Because we were focused on auditory-evoked activity in the temporal lobe, the search for an electrode’s peak was constrained between 0 and 500 ms after stimulus onset. Electrode size in <xref ref-type="fig" rid="fig1">Figure 1D</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> corresponds to this peak HG amplitude for each speech-responsive electrode.</p></sec><sec id="s4-8-3"><title>VOT-sensitive electrodes</title><p>To identify electrodes where the peak response depended on stimulus VOT (<italic>VOT-sensitive electrodes</italic>), we computed the nonparametric correlation coefficient (Spearman’s ρ) across trials between VOT and peak HG amplitude. Because nonparametric (rank-based) correlation analysis measures the monotonicity of the relationship between two variables, it represents an unbiased (‘model-free’) indicator of amplitude-based VOT encoding, whether the underlying monotonic relationship is categorical, linear, or follows some other monotonic function (<xref ref-type="bibr" rid="bib3">Bishara and Hittner, 2012</xref>). This procedure identified 49 VOT-sensitive electrodes across all seven participants (p&lt;0.05; <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Electrode color in <xref ref-type="fig" rid="fig1">Figure 1D</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> corresponds to the correlation coefficient at each electrode’s peak (min/max ρ = ±0.35), thresholded such that all speech-responsive electrodes with non-significant (p&gt;0.05) correlation coefficients appear as white.</p><p>This set of VOT-sensitive sites was then divided into two sub-populations based on the sign of each electrode’s correlation coefficient (ρ): voiced-selective (V+) electrodes (n = 33) had significant ρ&lt;0, indicating that shorter (more /<italic>b</italic>/-like; voiced) VOTs elicited stronger peak HG responses; voiceless-selective (V-) electrodes (n = 16) had significant ρ&gt;0, indicating that longer (more /<italic>p</italic>/-like; voiceless) VOTs elicited stronger peak HG responses.</p><p>Across VOT-sensitive electrodes, the mean peak occurred 198.8 ms after stimulus onset (SD = 42.3 ms). The semi-transparent grey boxes in <xref ref-type="fig" rid="fig1">Figures 1E</xref> and <xref ref-type="fig" rid="fig2">2B/C</xref> illustrate this peak window (mean peak ± 1 SD).</p></sec></sec><sec id="s4-9"><title>Analysis of VOT-sensitive electrodes</title><sec id="s4-9-1"><title>Encoding of voicing category</title><p>Electrodes that exhibit a monotonic relationship between VOT and peak HG amplitude should also be likely to exhibit a categorial distinction between shorter (voiced) and longer (voiceless) VOTs. We conducted two analyses that confirmed this expectation. In each analysis, we computed a nonparametric test statistic describing the discriminability of responses to voiced vs. voiceless stimuli at each electrode’s peak (<italic>z</italic>-statistic of Mann-Whitney rank-sum test) and then tested whether the population of test statistics for each group of electrodes (V- and V+) differed reliably from zero (Wilcoxon signed-rank tests). In the first analysis, voicing category was defined based on the psychophysically determined category boundary (voiced: 0–20 ms VOTs; voiceless: 30–50 ms VOTs), which allowed us to include all VOT-sensitive electrodes (n = 49) in the analysis, including electrodes from participants who did not complete the behavioral task (3/7 participants).</p><p>In the second analysis, a trial’s voicing category was determined based on the actual behavioral response recorded for each trial (irrespective of VOT), so this analysis was not dependent on the assumption that the VOT continuum can be divided into two categories based on the average boundary calculated across participants. This analysis examined the subset of trials with behavioral responses and the subset of VOT-sensitive electrodes found in the four participants with behavioral data (n = 27; 12 V- electrodes, 15 V+ electrodes) (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>Given the strong correspondence between the categorically defined VOT stimulus ranges (0–20 ms vs. 30–50 ms VOTs) and identification behavior (e.g., <xref ref-type="fig" rid="fig1">Figure 1C</xref>), the agreement between these results was expected.</p><p>Significance bars for the two example STG electrodes in one participant (e1 and e2; <xref ref-type="fig" rid="fig1">Figure 1E</xref>) we computed to illustrate the temporal dynamics of category selectivity. In these electrodes, we conducted the test of between-category encoding (Mann-Whitney rank-sum test; first analysis) at every timepoint during the trial epoch (in addition to the electrodes’ peaks). Bars plotted for each electrode in <xref ref-type="fig" rid="fig1">Figure 1E</xref> begin at the first timepoint after stimulus onset where the significance level reached p&lt;0.005 and ends at the first point thereafter where significance fails to reach that threshold (e1: 140 to 685 ms post onset; e2: 65 to 660 ms post onset).</p></sec><sec id="s4-9-2"><title>Encoding of VOT within voicing categories</title><p>Because VOT-sensitive electrodes were identified via nonparametric correlation analysis (Spearman’s ρ) across all VOTs, the monotonic relationship between VOT and peak HG amplitude at these sites could be driven by the observed phonetic (between-category) encoding of voicing without any robust sub-phonetic (within-category) encoding of VOT. To assess sub-phonetic encoding of VOT in the peak response amplitude of VOT-sensitive electrodes, we computed the rank-based correlation (Spearman’s ρ) between VOT and HG amplitude at each electrode’s peak separately for trials in each voicing category (0–20 ms vs. 30–50 ms VOTs). The statistical reliability of within-category encoding was summarized by computing a test-statistic (<italic>t</italic>) for every correlation coefficient (ρ<italic><sub>0-20</sub></italic> and ρ<italic><sub>30-50</sub></italic> for each VOT-sensitive electrode) as follows:<disp-formula id="equ2"><mml:math id="m2"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>ρ</mml:mi><mml:msqrt><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:msqrt></mml:mrow><mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula>where <italic>n</italic> is the number of trials with VOTs in a given voicing category. The resulting set of test statistics (one per voicing category per VOT-sensitive electrode) served as the basis for the following analyses of peak within-category encoding.</p><p>For each group of electrodes (V- and V+), we tested whether the encoding of VOT within each voicing category differed reliably from 0 (Wilcoxon signed-rank tests). We also conducted a Wilcoxon signed-rank test for each electrode group that compared the within-category correlation <italic>t</italic>-statistics for voiceless and voiced categories.</p><p>The above tests addressed the encoding properties of one electrode group at a time (either V- or V+ electrodes). Finally, a pair of Wilcoxon signed-rank tests combined across the full set of VOT-sensitive electrodes (n = 49) to summarize the within-category VOT encoding results within electrodes’ (1) preferred and (2) non-preferred categories. In order to conduct this ‘omnibus’ test, we multiplied the correlation <italic>t</italic>-statistics for all V+ electrodes (for tests within each category) by −1. This simple transformation had the consequence of ensuring that positive correlation statistics always indicate stronger peak HG responses to VOTs that were closer to the endpoint of an electrode’s preferred category.</p></sec><sec id="s4-9-3"><title>Visualizations of within-category VOT encoding</title><p>To visualize the pattern of within-category encoding of VOT in the peak HG amplitude of V- and V+ electrodes, we computed a normalized measure of the peak response amplitude to each VOT stimulus for each VOT-sensitive electrode. <xref ref-type="fig" rid="fig2">Figure 2B and C</xref> show the full time series of the average (± SE) evoked responses of V- and V+ electrodes to all six VOT stimuli. To show encoding patterns across electrodes with different peak amplitudes, each electrode’s activity was normalized by its peak HG (grand mean across all VOTs). <xref ref-type="fig" rid="fig2">Figure 2D</xref> shows the amplitude of the average response evoked by a given VOT at a given electrode’s peak relative to the average response evoked by the other VOT stimuli, or <italic>peak HG (% of max),</italic> averaged across electrodes in each group (V-, left; V+, right) and participants (± SE). For each electrode, the mean HG amplitude evoked by each VOT at the peak was scaled and normalized by subtracting the minimum across all VOTs and dividing by the maximum across all VOTs after scaling.</p></sec><sec id="s4-9-4"><title>Neural response latency</title><p>The normalized HG responses used for <xref ref-type="fig" rid="fig2">Figure 2B/C</xref> were also used for the analysis of onset latency effects (<xref ref-type="fig" rid="fig3">Figure 3</xref>): <italic>HG (normalized)</italic> (<xref ref-type="fig" rid="fig2">Figure 2B/C</xref>) and <italic>HG (% of peak)</italic> (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) are computationally equivalent. Neural response onset latency for an electrode was defined as the first timepoint at which its average response to a given VOT stimulus exceeded 50% of its peak HG (based on the peak of the grand average response across all VOTs). A bootstrapping with resampling procedure was employed to estimate the onset latencies of responses to different VOTs at each electrode and to assess any possible relationship between onset latency and VOT. During each sampling step in this procedure (1000 bootstrap samples), we computed the average time series of the normalized HG response to each VOT, the onset latency for the response to each VOT, and the nonparametric correlation (Spearman’s ρ) between onset latency and VOT. Wilcoxon signed-rank tests asked whether the population of bootstrapped correlation coefficient estimates for each electrode group reliably differed from zero. A Mann-Whitney rank-sum test compared the VOT-dependency of response onset latency between electrode groups. Color-coded horizontal bars below the neural data in <xref ref-type="fig" rid="fig3">Figure 3A</xref> show onset latency estimates (mean ± bootstrap standard error) for responses to each VOT at two example electrodes. All electrodes were included in the analyses, but the bootstrapped correlation coefficient estimates for two V+ electrodes that were outliers (&gt;3 SDs from median) were excluded from the visualized range of the box-plot’s whiskers in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p></sec><sec id="s4-9-5"><title>Population-based neural classification</title><p>For each participant, we trained a set of multivariate pattern classifiers (linear discriminant analysis with leave-one-out cross validation) to predict trial-by-trial voicing category (/<italic>b</italic>/: 0–20 ms VOTs vs. /<italic>p</italic>/: 30–50 ms VOTs) using HG activity across all speech-responsive electrodes on the temporal lobe during a time window around the peak neural response. The peak window was defined as beginning 150 ms and ending 250 ms after stimulus onset, selected based on the average and standard deviation of the peaks across all VOT-sensitive electrodes. We created four separate classifiers for each participant that allowed us to evaluate the contribution of amplitude and temporal structure to voicing category encoding (<xref ref-type="fig" rid="fig1">Figure 1F</xref>).</p><p>To corrupt the reliability of any spatially-localized amplitude information about whether the VOT stimulus presented to a participant on a given trial was a /<italic>b</italic>/ or a /<italic>p</italic>/, the neural responses at every electrode on every trial were normalized so that the average response to a/<italic>b</italic>/ and the average response to a/<italic>p</italic>/reached the same amplitude at each electrode’s peak. Specifically, for each electrode, we found its peak (timepoint where the grand average HG time series across all trials reached its maximum), calculated the mean HG amplitude across all trials for VOTs within each category at that peak, and divided the HG values for every timepoint in a trial’s time series by the peak HG amplitude for that trial’s category. This amplitude normalization procedure forces the average amplitude of the neural response across all trials of /<italic>b</italic>/ and of /<italic>p</italic>/ to be equal at each electrode’s peak, while still allowing for variation in the amplitude of any individual trial at the peak.</p><p>To corrupt the reliability of any timing information during the peak response window about whether the VOT stimulus presented to a participant on a given trial was a /<italic>b</italic>/ or a/<italic>p</italic>/, the timing of the neural response on every trial (across all electrodes) was randomly shifted in time so that the trial could begin up to 50 ms before or after the true start of the trial. Specifically, for each trial, a jitter value was drawn from a discrete (integer) uniform random distribution ranging between −20 to 20 (inclusive range) ECoG time samples (at 400 Hz, this corresponds to ±50 ms, with a mean jitter of 0 ms), and the HG time series for all electrodes on that trial was moved backward or forward in time by the number of samples dictated by the trial’s jitter value. This temporal jittering procedure has the effect of changing whether the peak response window for a given trial is actually drawn from 100 to 200 ms after stimulus onset, 200–300 ms after stimulus onset, or some other window in between.</p><p>Crucially, this procedure will misalign any reliable, category-dependent differences in peak timing or temporal dynamics within individual electrodes or temporal patterns or relationships that exist across distributed electrodes. For instance, the peak window overlaps with a window during which past work examining intracranial auditory evoked local field potentials found evidence of waveform shape differences between responses of single electrodes to voiced and voiceless stimuli (single- vs. double-peaked responses; see, e.g., Figure 10 of <xref ref-type="bibr" rid="bib105">Steinschneider et al., 2011</xref>). If similar temporal differences in waveform shape existed in the present high-gamma data, the temporal jittering procedure would detect a contribution of temporal information to decoding. Moreover, to the extent that the peak of a trial’s evoked high-gamma response occurs during or close to the peak window (either within one electrode [‘local’ temporal code] or across multiple electrodes in the same participant [‘ensemble’ temporal code]), the temporal jittering procedure would disrupt the reliability of this information to reveal the contribution of peak latency information to decoding accuracy. On the other hand, if the peak responses to stimuli from distinct voicing categories differ in the amplitude of the HG response at VOT-sensitive cortical sites, and if these differences persist throughout much of the peak window, then this temporal jittering procedure is unlikely to prevent the classifier from learning such differences.</p><p>For each participant, we trained one classifier where neither amplitude nor timing information were corrupted (+Amplitude/+Timing), one where only timing information was corrupted (+Amplitude/-Timing), one where only amplitude information was corrupted (-Amplitude/+Timing), and one where both were corrupted (-Amplitude/-Timing; here, amplitude normalization preceded temporal jittering). With each of these datasets, we then performed dimensionality reduction to minimize overfitting using spatiotemporal principal component analysis on the ECoG data for every electrode and all timepoints within the peak window (retaining PCs accounting for 90% of the variance across trials of all VOTs). Finally, training and testing of the linear discriminant analysis classifiers were conducted iteratively, holding out a single trial, training a classifier to predict voicing category using all other trials, and then predicting the voicing category of the held-out trial. For each participant and for each classifier, accuracy was the proportion of held-out trials that were correctly labeled. Wilcoxon signed-rank tests assessed and compared accuracy levels (across participants) achieved by the different models.</p></sec></sec><sec id="s4-10"><title>Computational neural network model</title><sec id="s4-10-1"><title>Overview of architecture and dynamics</title><p>A simple five-node, localist neural network (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) was hand-connected to illustrate how time-dependent properties of neuronal units and their interactions can transform a temporal cue into a spatial code (responses of different amplitudes to different VOTs at distinct model nodes). A gap detector received excitatory input from both a burst detector and voicing detector, as well as input from an inhibitory node that only received excitatory input from the burst detector. This represented an implementation of a slow inhibitory postsynaptic potential (slow IPSP) circuit (<xref ref-type="bibr" rid="bib5">Buonomano and Merzenich, 1995</xref>; <xref ref-type="bibr" rid="bib33">Gao and Wehr, 2015</xref>; <xref ref-type="bibr" rid="bib17">Douglas and Martin, 1991</xref>; <xref ref-type="bibr" rid="bib67">McCormick, 1989</xref>). A coincidence detector received excitatory input from the burst and voicing detectors.</p></sec><sec id="s4-10-2"><title>Network connectivity</title><p>Weights between units in this sparsely connected, feedforward network were set according to a minimalist approach. All excitatory connections from the burst detector (to the inhibitory node, the gap detector, and the coincidence detector) had identical weights. All excitatory connections from the voicing detector (to the gap detector and the coincidence detector) had identical weights (stronger than from burst detector). <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> indicates all nonzero connection weights between the network’s nodes, as illustrated in <xref ref-type="fig" rid="fig2">Figure 2E</xref>.</p></sec><sec id="s4-10-3"><title>Leaky-integrator dynamics</title><p>At the start of the model simulations, prior to the onset of any stimulus (<inline-formula><mml:math id="inf10"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), the activation level <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> of each node <inline-formula><mml:math id="inf12"><mml:mi>i</mml:mi></mml:math></inline-formula> was set to its resting level (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). Simulations ran for 100 cycles, with 1 cycle corresponding to 10ms. On each subsequent cycle (<inline-formula><mml:math id="inf14"><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>2,100</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>), activation levels of every node in the model were updated iteratively in two steps, as described in the following algorithm:</p><list list-type="order"><list-item><p>Decay: For every node <inline-formula><mml:math id="inf15"><mml:mi>i</mml:mi></mml:math></inline-formula> with prior activation level <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> that differs from <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> decays towards <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by its decay rate (<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) without overshooting <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Sum Inputs: For every node <inline-formula><mml:math id="inf22"><mml:mi>i</mml:mi></mml:math></inline-formula>, the total excitatory and inhibitory inputs are summed. This includes both model-external (clamped) inputs (i.e., from stimuli presented to the model) on the current cycle <inline-formula><mml:math id="inf23"><mml:mi>t</mml:mi></mml:math></inline-formula> and model-internal inputs from other nodes based on their activation level on the prior cycle <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>. Inputs from a presynaptic node <inline-formula><mml:math id="inf25"><mml:mi>j</mml:mi></mml:math></inline-formula> can only affect the postsynaptic node <inline-formula><mml:math id="inf26"><mml:mi>i</mml:mi></mml:math></inline-formula> if its prior activation <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> exceeds the presynaptic node’s propagation threshold (<inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). Summation of model-internal inputs within <inline-formula><mml:math id="inf29"><mml:mi>i</mml:mi></mml:math></inline-formula> is weighted by the connection weights from the various presynaptic nodes (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>): <inline-formula><mml:math id="inf30"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. The new activation level <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is bounded by the node’s minimum (<inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and maximum (<inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>Μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) activation levels, irrespective of the magnitude of the net effect of the inputs to a node.</p></list-item></list><p>All activation parameters for all nodes are listed in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. Minimum, maximum, and resting activation levels were identical across all units. Decay rates and propagation thresholds were identical across the burst and voicing detectors and the inhibitory node. The integrator units (gap and coincidence detectors) decayed more slowly than the other units, which could only affect other model nodes during one cycle. Activation levels in the coincidence detector had to reach a higher level (propagation threshold) to produce model outputs than in the gap detector, a difference which allowed the gap detector to register the fast suprathreshold response characteristic of slow IPSP circuits and allowed the coincidence detector to register a coincidence only when both burst and voicing were detected simultaneously or at a short lag.</p></sec><sec id="s4-10-4"><title>Model inputs</title><p>Two inputs were clamped onto the model in each simulation, representing the onset of the burst and of voicing (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The voicing input was only clamped onto the voicing detector at the onset of voicing. <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref> illustrates vectors describing each of the simulated VOT inputs.</p></sec><sec id="s4-10-5"><title>Sensitivity of model dynamics to variations in hand-tuned model parameters</title><p>Although most of the parameters of the model are theoretically uninteresting and were set to default levels (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>), analysis of parameter robustness for the model revealed four primary sensitivities based on the relative values set for certain specific parameters. (1) and (2) below involve the propagation thresholds [<inline-formula><mml:math id="inf34"><mml:mi>θ</mml:mi></mml:math></inline-formula>] of the temporal integrator units (<bold><italic><sc>G</sc></italic></bold><sc>ap</sc>, <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc>), which allow the model to achieve gap and coincidence detection. (3) and (4) below involve the rate of decay of activation [<inline-formula><mml:math id="inf35"><mml:mi>λ</mml:mi></mml:math></inline-formula>] of the temporal integrator units, which dictate where along the VOT continuum the boundary between voicing categories lies.</p><list list-type="order"><list-item><p>Propagation threshold [<inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>] of coincidence detector unit (<italic><sc>C</sc></italic><sc>oinc</sc>): In our model, coincidence detection is achieved by preventing the coincidence detector (<italic><sc>C</sc></italic><sc>oinc</sc>) from propagating an output in response to the burst until the voicing has arrived (hence responding with a higher-than-minimum peak amplitude only when the voicing is coincident with or arrives shortly after the burst). Thus, the propagation threshold for <italic><sc>C</sc></italic><sc>oinc</sc> (<inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>) must be <underline>greater than</underline> the connection weight from the burst-detector to (<italic><sc>C</sc></italic><sc>oinc</sc>). (<italic>W<sub>Burst→Coinc</sub></italic>).</p></list-item><list-item><p>Propagation threshold [<inline-formula><mml:math id="inf38"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>] of gap detector unit (<italic><sc>G</sc></italic><sc>ap</sc>): On the other hand, the propagation threshold for the gap detector [<bold><italic><sc>G</sc></italic></bold><sc>ap</sc>] (<inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) must be <underline>less than</underline> the connection weight from the burst-detector to <bold><italic><sc>G</sc></italic></bold><sc>ap (<italic>W<sub>Burst→Gap</sub></italic></sc>) to register the fast suprathreshold response characteristic of slow IPSP circuits.</p></list-item></list><p>The primary factor affecting the location of the boundary between voiced (short VOTs) and voiceless (long VOTs) categories is the time-dependent rate of decay of postsynaptic potentials in <bold><italic><sc>G</sc></italic></bold><sc>ap</sc> and <bold><italic><sc>C</sc></italic></bold><sc>oinc</sc> towards the unit’s resting activation level.</p><list list-type="order"><list-item><p>Rate of decay of activation [<inline-formula><mml:math id="inf40"><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:math></inline-formula>] in <italic><sc>C</sc></italic><sc>oinc</sc> in comparison to connection weights from inputs to <italic><sc>C</sc></italic><sc>oinc</sc>: For<italic> <sc>C</sc></italic><sc>oinc</sc>, the boundary is the VOT value after which there is no longer any additional boost to its peak amplitude from the initial burst, and this requires the decay rate of <italic><sc>C</sc></italic><sc>oinc</sc> (<inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>) and the connection weight from the burst-detector to <italic><sc>C</sc></italic><sc>oinc (<italic>W<sub>Burst→Coinc</sub></italic></sc>) to be in balance. Increasing <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> or decreasing <italic>W<sub>Burst→Coinc</sub></italic> (independently) will move the boundary earlier in time.</p></list-item><list-item><p>Rate of decay of activation [<inline-formula><mml:math id="inf43"><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:math></inline-formula>] in <italic><sc>G</sc></italic><sc>ap</sc> in comparison to connection weights from inputs to <italic><sc>G</sc></italic><sc>ap</sc>: Similarly, for<italic> <sc>G</sc></italic><sc>ap</sc>, the category boundary is the VOT value before which the remaining influence of the initial inhibition is still so strong that the arrival of voicing input cannot exceed <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Increasing <inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, decreasing <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mo>.</mml:mo><mml:mo>→</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, or increasing <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>→</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (independently) would each move the boundary earlier in time. All three of these parameters are in balance in these hand-tuned parameter settings.</p></list-item></list><p>It is critical to note that, for all of these cases where the hand-tuned parameter settings are in balance, the balance is required for the model to achieve gap and coincidence detection and/or to determine the position of the VOT boundary between categories. This was all the model was designed to do. No parameters were hand-tuned to achieve the other response properties (e.g., asymmetric within-category encoding, onset latency dynamics).</p></sec></sec><sec id="s4-11"><title>Analysis of auditory evoked local field potentials</title><sec id="s4-11-1"><title>Identification of key LFP peaks</title><p>We identified 3 peaks of the grand mean auditory evoked local field potential (AEP), which were consistent with AEP peaks previously described in the literature (<xref ref-type="bibr" rid="bib37">Howard et al., 2000</xref>; <xref ref-type="bibr" rid="bib79">Nourski et al., 2015</xref>): <italic>P<sub>α</sub></italic> (positive deflection approximately 75–100 ms after stimulus onset), <italic>N<sub>α</sub></italic> (negative deflection approximately 100–150 ms after stimulus onset), and <italic>P<sub>β</sub></italic> (positive deflection approximately 150–250 ms after stimulus onset) (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref> and <xref ref-type="fig" rid="fig1s4">4</xref>).</p></sec><sec id="s4-11-2"><title>Bootstrapping approach</title><p>For each VOT-sensitive electrode (speech-responsive electrodes whose peak high-gamma amplitude was correlated with VOT), a bootstrapping with resampling procedure was used to estimate the latencies and amplitudes of each peak of the AEP elicited by trials from each VOT condition. During each sampling step in this procedure (1000 bootstrap samples), we computed the average time series of the AEP for each VOT (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels I-L), the ECoG samples of the time series during each of three time-ranges with the maximum (for positive peaks) or minimum (for the negative peak) mean voltage values for each VOT, and six correlation coefficients (Pearson’s <italic>r</italic> between VOT and amplitude/latency for each peak; see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels M-T).</p></sec><sec id="s4-11-3"><title>Details of peak-finding</title><p> <italic>P<sub>α</sub></italic> was defined as the maximum mean voltage from 0 to 150 ms after stimulus onset, <italic>N<sub>α</sub></italic> was defined as the minimum mean voltage from 75 to 200 ms after stimulus onset, and <italic>P<sub>β</sub></italic> was defined as the maximum mean voltage from 150 to 250 ms after stimulus onset. To aid peak detection and enforce sequential ordering of the peaks, time ranges for the latter two peaks (<italic>N<sub>α</sub></italic>, <italic>P<sub>β</sub></italic>) were further constrained on a per-sample basis by setting the minimum bound of the search time range to be the time of the previous peak (i.e., the earliest possible times for <italic>N<sub>α</sub></italic> and <italic>P<sub>β</sub></italic> were <italic>P<sub>α</sub></italic> and <italic>N<sub>α</sub></italic>, respectively). For a given sample, if a peak occurred at either the earliest possible or latest possible time, it was assumed that the peak was either not prominent or did not occur during the defined time range for this electrode/VOT, so that sample was ignored in the analysis for that peak and any subsequent peaks. Because correlation coefficients for each peak were computed over just 6 VOTs in each sample, exclusion of a peak latency/amplitude value for one VOT condition resulted in exclusion of the all conditions for that peak for that sample. Finally, if more than 50% of the bootstrap samples were excluded for a given peak in a given electrode, no samples for that electrode/peak pair were not included in the analysis (see, e.g., <italic>P<sub>β</sub></italic> for e4 in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels H/P/T).</p></sec><sec id="s4-11-4"><title>Analysis of bootstrapped correlation estimates</title><p>For each remaining VOT-sensitive electrode/peak pair, we determined whether or not the latency and/or amplitude of the peak was significantly associated with VOT by evaluating whether the 95% confidence interval (95% CI) across all included bootstrapped estimates of the correlation coefficient excluded 0 (taking the highest density interval of the bootstrapped statistics) (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, panel B). These exploratory analyses did not undergo multiple comparison correction.</p></sec><sec id="s4-11-5"><title>Detailed results of analysis of AEPs</title><p>The exploratory analyses of correlations between VOT and the latency and/or amplitude of three peaks of the AEP in all VOT-sensitive electrodes revealed four overall conclusions:</p><list list-type="order"><list-item><p>Comparison of the AEPs evoked by different VOTs shows that there exist associations between stimulus VOT and the amplitude/temporal information in local field potential (LFP). Among electrodes that robustly encode voicing in their peak high-gamma amplitude (i.e., VOT-sensitive electrodes), these associations between VOT and LFP features are complex and highly variable (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>; <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>).</p></list-item><list-item><p>Replicating prior results regarding VOT encoding by AEPs (e.g., <xref ref-type="bibr" rid="bib105">Steinschneider et al., 2011</xref>), we find that some electrodes (e.g., e1 in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels E/I) exhibit temporal encoding of VOT in the latency of various peaks of the AEP. In some electrodes, the nature of this temporal code is straightforward (e.g., in e1, the latency of <italic>N<sub>α</sub></italic> is delayed by ~10 ms for every additional 10 ms of VOT duration; <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panel M), but – more often – the relationship between VOT and peak latency is less direct (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels N-P).</p></list-item><list-item><p>Among electrodes that encode VOT in their peak high-gamma amplitude, there exist many more electrodes that <italic>do not</italic> encode VOT in these temporal features of the AEP (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), supporting a prominent role for the peak high-gamma amplitude in the neural representation of voicing and of VOT.</p></list-item><list-item><p>Besides the timing of the various AEP peaks, there also exist many electrodes that encode VOT in the amplitude of those peaks (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). The encoding patterns are often visually similar to the encoding patterns observed in high-gamma (i.e., graded within the electrode’s preferred voicing category; see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels Q-S). However, there are also many electrodes that do encode VOT in their peak high-gamma amplitude but <italic>not</italic> in these amplitude features of the LFP (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, panel B; compare, e.g., <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>, panels D vs. H).</p></list-item></list></sec></sec><sec id="s4-12"><title>Supplementary analyses of spatial patterns of VOT effects</title><p>Of the 49 VOT-sensitive electrodes, 76% were located posterior to the lateral extent of the transverse temporal sulcus (defined as <italic>y</italic> ≥ 6 in MNI coordinate space based on projection of the sulcus onto the lateral STG in the left hemisphere). This is the same region that is densely populated with neural populations that are tuned for other phonetic features (e.g., manner of articulation; <xref ref-type="bibr" rid="bib71">Mesgarani et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Hamilton et al., 2018</xref>). Mann-Whitney rank-sum tests showed that there was no significant difference in the localization of voiceless-selective (V-) versus voiced-selective (V+) electrodes along either the anterior-posterior axis (<italic>y</italic>-dimension in MNI coordinate space; <italic>U</italic> = 342, <italic>z</italic> = −1.23, p=0.22) or the dorsal-ventral axis (<italic>z</italic>-dimension in MNI coordinate space; <italic>U</italic> = 414, <italic>z</italic> = 0.29, p=0.77).</p><p>Although no regional patterns were visually apparent, we tested for hemispheric differences in relative prevalence of VOT-sensitive sites or in voicing category selectivity. Of the seven participants (all of whom had unilateral coverage), four had right hemisphere coverage (57%), and these four patients contributed 28 of the 49 VOT-sensitive electrodes identified in this study (57%) (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Pearson’s <italic>χ</italic><sup>2</sup> tests confirmed there was no difference in the rate of VOT-sensitive sites (<italic>χ</italic><sup>2</sup>(1)=0.15, p=0.70) or in the proportion of VOT-sensitive sites that were selective for each category (<italic>χ</italic><sup>2</sup>(1)=1.74, p=0.19) as a function of hemisphere. Thus, consistent with past ECoG work examining spatial patterns of STG encoding for other phonetic features (e.g., <xref ref-type="bibr" rid="bib36">Hamilton et al., 2018</xref>) we found no evidence that the observed spatial/amplitude code reflected any topographical organization nor any lateralized asymmetries in the encoding of VOT, although data limitations prevent us from ruling out this possibility entirely.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are grateful to John Houde, who provided the stimuli used in this work, and to all members of the Chang Lab for helpful comments throughout this work. This work was supported by European Commission grant FP7-623072 (MJS); and NIH grants R01-DC012379 (EFC) and F32-DC015966 (NPF). EFC is a New York Stem Cell Foundation-Robertson Investigator. This research was also supported by The William K Bowes Foundation, the Howard Hughes Medical Institute, The New York Stem Cell Foundation and The Shurl and Kay Curci Foundation.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Project administration</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Project administration</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants gave their written informed consent before surgery and affirmed it at the start of each recording session. The study protocol was approved by the University of California, San Francisco Committee on Human Research. (Protocol number 10-03842: Task-evoked changes in the electrocorticogram in epilepsy patients undergoing invasive electrocorticography and cortical mapping for the surgical treatment of intractable seizures).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Table of experimental summary statistics for each participant.</title><p>Each participant had ECoG grid coverage of one hemisphere (Hem), either left (LH) or right (RH). Participants completed as many trials as they felt comfortable with. Number of trials per participant for ECoG analyses indicate trials remaining after artifact rejection. Some participants chose to listen passively to some or all blocks, so three participants have no trials for behavioral analyses. See Materials and methods for description of inclusion criteria for individual trials in ECoG and behavioral analyses. A subset of speech-responsive (SR) electrodes on the lateral surface of the temporal lobe had a peak amplitude that was sensitive to VOT, selectively responding to either voiceless (V-) or voiced (V+) stimuli. See Materials and methods for details on electrode selection.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53051-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Table of activation parameters for each model node.</title><p><inline-formula><mml:math id="inf48"><mml:mi>m</mml:mi></mml:math></inline-formula> = minimum activation level. <inline-formula><mml:math id="inf49"><mml:mi>Μ</mml:mi></mml:math></inline-formula> = maximum activation level. <inline-formula><mml:math id="inf50"><mml:mi>ρ</mml:mi></mml:math></inline-formula> = resting activation level. <inline-formula><mml:math id="inf51"><mml:mi>λ</mml:mi></mml:math></inline-formula> = decay rate. <inline-formula><mml:math id="inf52"><mml:mi>θ</mml:mi></mml:math></inline-formula> = propagation threshold.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53051-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Table illustrating timing of 6 simulated model inputs.</title><p>The table is sparse, meaning that inputs to both Burst and Voicing detector units are 0 whenever a cell is blank. Inputs are clamped onto either Burst or Voicing detector units (always with strength = 1) for a given simulated VOT stimulus during the cycles that are labeled with a B or a V.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53051-supp3-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-53051-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and code are available under a Creative Commons License at the project page on Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/9y7uh/">https://osf.io/9y7uh/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>NP</given-names></name><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Sjerps</surname><given-names>MJ</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Transformation of a temporal speech cue to a spatial neural code in human auditory cortex</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/9y7uh/">9y7uh</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname> <given-names>JS</given-names></name><name><surname>Miller</surname> <given-names>JL</given-names></name><name><surname>DeSteno</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Individual talker differences in voice-onset-time</article-title><source>The Journal of the Acoustical Society of America</source><volume>113</volume><fpage>544</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1121/1.1528172</pub-id><pub-id pub-id-type="pmid">12558290</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andruski</surname> <given-names>JE</given-names></name><name><surname>Blumstein</surname> <given-names>SE</given-names></name><name><surname>Burton</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The effect of subphonetic differences on lexical access</article-title><source>Cognition</source><volume>52</volume><fpage>163</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(94)90042-6</pub-id><pub-id pub-id-type="pmid">7956004</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishara</surname> <given-names>AJ</given-names></name><name><surname>Hittner</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Testing the significance of a correlation with nonnormal data: comparison of Pearson, Spearman, transformation, and resampling approaches</article-title><source>Psychological Methods</source><volume>17</volume><fpage>399</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1037/a0028087</pub-id><pub-id pub-id-type="pmid">22563845</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blumstein</surname> <given-names>SE</given-names></name><name><surname>Myers</surname> <given-names>EB</given-names></name><name><surname>Rissman</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The perception of voice onset time: an fMRI investigation of phonetic category structure</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>1353</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1162/0898929054985473</pub-id><pub-id pub-id-type="pmid">16197689</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname> <given-names>DV</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Temporal information transformed into a spatial code by a neural network with realistic properties</article-title><source>Science</source><volume>267</volume><fpage>1028</fpage><lpage>1030</lpage><pub-id pub-id-type="doi">10.1126/science.7863330</pub-id><pub-id pub-id-type="pmid">7863330</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The origin of extracellular fields and currents--EEG, ECoG, LFP and spikes</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>407</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nrn3241</pub-id><pub-id pub-id-type="pmid">22595786</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carney</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Noncategorical perception of stop consonants differing in VOT</article-title><source>The Journal of the Acoustical Society of America</source><volume>62</volume><fpage>961</fpage><lpage>970</lpage><pub-id pub-id-type="doi">10.1121/1.381590</pub-id><pub-id pub-id-type="pmid">908791</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Processing of temporal information in the brain</article-title><source>Annual Review of Neuroscience</source><volume>16</volume><fpage>223</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.16.030193.001255</pub-id><pub-id pub-id-type="pmid">8460892</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>EF</given-names></name><name><surname>Rieger</surname> <given-names>JW</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Berger</surname> <given-names>MS</given-names></name><name><surname>Barbaro</surname> <given-names>NM</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Categorical speech representation in human superior temporal gyrus</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1428</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1038/nn.2641</pub-id><pub-id pub-id-type="pmid">20890293</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Towards large-scale, human-based, mesoscopic neurotechnologies</article-title><source>Neuron</source><volume>86</volume><fpage>68</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.037</pub-id><pub-id pub-id-type="pmid">25856487</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname> <given-names>T</given-names></name><name><surname>Ladefoged</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Variation and universals in VOT: evidence from 18 languages</article-title><source>Journal of Phonetics</source><volume>27</volume><fpage>207</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1006/jpho.1999.0094</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayards</surname> <given-names>M</given-names></name><name><surname>Tanenhaus</surname> <given-names>MK</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Jacobs</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perception of speech reflects optimal use of probabilistic speech cues</article-title><source>Cognition</source><volume>108</volume><fpage>804</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2008.04.004</pub-id><pub-id pub-id-type="pmid">18582855</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname> <given-names>TE</given-names></name><name><surname>Sohoglu</surname> <given-names>E</given-names></name><name><surname>Sedley</surname> <given-names>W</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Jones</surname> <given-names>PS</given-names></name><name><surname>Wiggins</surname> <given-names>J</given-names></name><name><surname>Dawson</surname> <given-names>C</given-names></name><name><surname>Grube</surname> <given-names>M</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Rowe</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence for causal top-down frontal contributions to predictive processes in speech perception</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>2154</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01958-7</pub-id><pub-id pub-id-type="pmid">29255275</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Boatman</surname> <given-names>D</given-names></name><name><surname>Gordon</surname> <given-names>B</given-names></name><name><surname>Hao</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Induced electrocorticographic gamma activity during auditory perception</article-title><source>Clinical Neurophysiology</source><volume>112</volume><fpage>565</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(00)00545-9</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Damper</surname> <given-names>RI</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Connectionist models of categorical perception of speech</article-title><conf-name>Proceedings of ICSIPNN 1994 International Symposium on Speech, Image Processing and Neural Networks (Institute of Electrical and Electronics Engineers Inc)</conf-name><fpage>101</fpage><lpage>104</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeWitt</surname> <given-names>I</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Phoneme and word recognition in the auditory ventral stream</article-title><source>PNAS</source><volume>109</volume><fpage>E505</fpage><lpage>E514</lpage><pub-id pub-id-type="doi">10.1073/pnas.1113427109</pub-id><pub-id pub-id-type="pmid">22308358</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douglas</surname> <given-names>RJ</given-names></name><name><surname>Martin</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A functional microcircuit for cat visual cortex</article-title><source>The Journal of Physiology</source><volume>440</volume><fpage>735</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1991.sp018733</pub-id><pub-id pub-id-type="pmid">1666655</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Representation of a voice onset time continuum in primary auditory cortex of the cat</article-title><source>The Journal of the Acoustical Society of America</source><volume>98</volume><fpage>911</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1121/1.413517</pub-id><pub-id pub-id-type="pmid">7642830</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Neural responses in primary auditory cortex mimic psychophysical, across-frequency-channel, gap-detection thresholds</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>1453</fpage><lpage>1463</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.3.1453</pub-id><pub-id pub-id-type="pmid">10980018</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Between sound and perception: reviewing the search for a neural code</article-title><source>Hearing Research</source><volume>157</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(01)00259-3</pub-id><pub-id pub-id-type="pmid">11470183</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname> <given-names>JJ</given-names></name><name><surname>Ponton</surname> <given-names>CW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The neurophysiology of auditory perception: from single units to evoked potentials</article-title><source>Audiology and Neuro-Otology</source><volume>7</volume><fpage>71</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1159/000057656</pub-id><pub-id pub-id-type="pmid">12006736</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Einevoll</surname> <given-names>GT</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Modelling and analysis of local field potentials for studying the function of cortical circuits</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>770</fpage><lpage>785</lpage><pub-id pub-id-type="doi">10.1038/nrn3599</pub-id><pub-id pub-id-type="pmid">24135696</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engineer</surname> <given-names>CT</given-names></name><name><surname>Perez</surname> <given-names>CA</given-names></name><name><surname>Chen</surname> <given-names>YH</given-names></name><name><surname>Carraway</surname> <given-names>RS</given-names></name><name><surname>Reed</surname> <given-names>AC</given-names></name><name><surname>Shetake</surname> <given-names>JA</given-names></name><name><surname>Jakkamsetti</surname> <given-names>V</given-names></name><name><surname>Chang</surname> <given-names>KQ</given-names></name><name><surname>Kilgard</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical activity patterns predict speech discrimination ability</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>603</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1038/nn.2109</pub-id><pub-id pub-id-type="pmid">18425123</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical organization of auditory and motor representations in speech perception: evidence from searchlight similarity analysis</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4772</fpage><lpage>4788</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv136</pub-id><pub-id pub-id-type="pmid">26157026</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname> <given-names>NH</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Morgan</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The influence of categories on perception: explaining the perceptual magnet effect as optimal statistical inference</article-title><source>Psychological Review</source><volume>116</volume><fpage>752</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1037/a0017196</pub-id><pub-id pub-id-type="pmid">19839683</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferster</surname> <given-names>D</given-names></name><name><surname>Spruston</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Cracking the neuronal code</article-title><source>Science</source><volume>270</volume><fpage>756</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1126/science.270.5237.756</pub-id><pub-id pub-id-type="pmid">7481761</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flege</surname> <given-names>JE</given-names></name><name><surname>Eefting</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Linguistic and developmental effects on the production and perception of stop consonants</article-title><source>Phonetica</source><volume>43</volume><fpage>155</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1159/000261768</pub-id><pub-id pub-id-type="pmid">3797480</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>NP</given-names></name><name><surname>Reilly</surname> <given-names>M</given-names></name><name><surname>Blumstein</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Phonological neighborhood competition affects spoken word production irrespective of sentential context</article-title><source>Journal of Memory and Language</source><volume>83</volume><fpage>97</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2015.04.002</pub-id><pub-id pub-id-type="pmid">26124538</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>NP</given-names></name><name><surname>Leonard</surname> <given-names>M</given-names></name><name><surname>Sjerps</surname> <given-names>MJ</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Transformation of a temporal speech cue to a spatial neural code in human auditory cortex</article-title><source>Open Sci Framew</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/9y7uh/">https://osf.io/9y7uh/</ext-link><date-in-citation iso-8601-date="2020-09-01">September 1, 2020</date-in-citation></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>NP</given-names></name><name><surname>Blumstein</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Top-down effects of syntactic sentential context on phonetic processing</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>42</volume><fpage>730</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1037/a0039965</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuronal gamma-band synchronization as a fundamental process in cortical computation</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>209</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135603</pub-id><pub-id pub-id-type="pmid">19400723</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frye</surname> <given-names>RE</given-names></name><name><surname>Fisher</surname> <given-names>JM</given-names></name><name><surname>Coty</surname> <given-names>A</given-names></name><name><surname>Zarella</surname> <given-names>M</given-names></name><name><surname>Liederman</surname> <given-names>J</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Linear coding of voice onset time</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1476</fpage><lpage>1487</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.9.1476</pub-id><pub-id pub-id-type="pmid">17714009</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname> <given-names>X</given-names></name><name><surname>Wehr</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A coding transformation for temporally structured sounds within auditory cortical neurons</article-title><source>Neuron</source><volume>86</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.004</pub-id><pub-id pub-id-type="pmid">25819614</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Chang</surname> <given-names>DL</given-names></name><name><surname>Lee</surname> <given-names>MB</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Semi-automated anatomical labeling and Inter-subject warping of High-Density intracranial recording electrodes in electrocorticography</article-title><source>Frontiers in Neuroinformatics</source><volume>11</volume><elocation-id>62</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00062</pub-id><pub-id pub-id-type="pmid">29163118</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Edwards</surname> <given-names>E</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A spatial map of onset and sustained responses to speech in the human superior temporal gyrus</article-title><source>Current Biology</source><volume>28</volume><fpage>1860</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.033</pub-id><pub-id pub-id-type="pmid">29861132</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>MA</given-names></name><name><surname>Volkov</surname> <given-names>IO</given-names></name><name><surname>Mirsky</surname> <given-names>R</given-names></name><name><surname>Garell</surname> <given-names>PC</given-names></name><name><surname>Noh</surname> <given-names>MD</given-names></name><name><surname>Granner</surname> <given-names>M</given-names></name><name><surname>Damasio</surname> <given-names>H</given-names></name><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Reale</surname> <given-names>RA</given-names></name><name><surname>Hind</surname> <given-names>JE</given-names></name><name><surname>Brugge</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Auditory cortex on the human posterior superior temporal gyrus</article-title><source>The Journal of Comparative Neurology</source><volume>416</volume><fpage>79</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(20000103)416:1&lt;79::AID-CNE6&gt;3.0.CO;2-2</pub-id><pub-id pub-id-type="pmid">10578103</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kessinger</surname> <given-names>RH</given-names></name><name><surname>Blumstein</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Effects of speaking rate on voice-onset time in Thai, French, and English</article-title><source>Journal of Phonetics</source><volume>25</volume><fpage>143</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1006/jpho.1996.0039</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klatt</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Voice onset time, Frication, and aspiration in word-initial consonant clusters</article-title><source>Journal of Speech and Hearing Research</source><volume>18</volume><fpage>686</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1044/jshr.1804.686</pub-id><pub-id pub-id-type="pmid">1207100</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klatt</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Linguistic uses of segmental duration in english: acoustic and perceptual evidence</article-title><source>The Journal of the Acoustical Society of America</source><volume>59</volume><fpage>1208</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1121/1.380986</pub-id><pub-id pub-id-type="pmid">956516</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klatt</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Software for a cascade/parallel formant synthesizer</article-title><source>The Journal of the Acoustical Society of America</source><volume>67</volume><fpage>971</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1121/1.383940</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinschmidt</surname> <given-names>DF</given-names></name><name><surname>Jaeger</surname> <given-names>TF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Robust speech perception: recognize the familiar, generalize to the similar, and adapt to the novel</article-title><source>Psychological Review</source><volume>122</volume><fpage>148</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1037/a0038695</pub-id><pub-id pub-id-type="pmid">25844873</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konishi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Coding of auditory space</article-title><source>Annual Review of Neuroscience</source><volume>26</volume><fpage>31</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131123</pub-id><pub-id pub-id-type="pmid">14527266</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Takashima</surname> <given-names>A</given-names></name><name><surname>Meyer</surname> <given-names>A</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Hagoort</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural entrainment determines the words we hear</article-title><source>Current Biology</source><volume>28</volume><fpage>2867</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.023</pub-id><pub-id pub-id-type="pmid">30197083</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kössl</surname> <given-names>M</given-names></name><name><surname>Hechavarria</surname> <given-names>JC</given-names></name><name><surname>Voss</surname> <given-names>C</given-names></name><name><surname>Macias</surname> <given-names>S</given-names></name><name><surname>Mora</surname> <given-names>EC</given-names></name><name><surname>Vater</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural maps for target range in the auditory cortex of echolocating bats</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>68</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.08.016</pub-id><pub-id pub-id-type="pmid">24492081</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kronrod</surname> <given-names>Y</given-names></name><name><surname>Coppess</surname> <given-names>E</given-names></name><name><surname>Feldman</surname> <given-names>NH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A unified account of categorical effects in phonetic perception</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1681</fpage><lpage>1712</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1049-y</pub-id><pub-id pub-id-type="pmid">27220996</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname> <given-names>PK</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Human adults and human infants show a &quot;perceptual magnet effect&quot; for the prototypes of speech categories, monkeys do not</article-title><source>Perception &amp; Psychophysics</source><volume>50</volume><fpage>93</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.3758/BF03212211</pub-id><pub-id pub-id-type="pmid">1945741</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>YS</given-names></name><name><surname>Turkeltaub</surname> <given-names>P</given-names></name><name><surname>Granger</surname> <given-names>R</given-names></name><name><surname>Raizada</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Categorical speech processing in broca's area: an fMRI study using multivariate pattern-based analysis</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>3942</fpage><lpage>3948</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3814-11.2012</pub-id><pub-id pub-id-type="pmid">22423114</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Baud</surname> <given-names>MO</given-names></name><name><surname>Sjerps</surname> <given-names>MJ</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual restoration of masked speech in human cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13619</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13619</pub-id><pub-id pub-id-type="pmid">27996973</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic speech representations in the human temporal lobe</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.05.001</pub-id><pub-id pub-id-type="pmid">24906217</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname> <given-names>AM</given-names></name><name><surname>Harris</surname> <given-names>KS</given-names></name><name><surname>Hoffman</surname> <given-names>HS</given-names></name><name><surname>Griffith</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>The discrimination of speech sounds within and across phoneme boundaries</article-title><source>Journal of Experimental Psychology</source><volume>54</volume><fpage>358</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1037/h0044417</pub-id><pub-id pub-id-type="pmid">13481283</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname> <given-names>AM</given-names></name><name><surname>Delattre</surname> <given-names>PC</given-names></name><name><surname>Cooper</surname> <given-names>FS</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>Some cues for the distinction between voiced and voiceless stops in initial position</article-title><source>Language and Speech</source><volume>1</volume><fpage>153</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1177/002383095800100301</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname> <given-names>AM</given-names></name><name><surname>Harris</surname> <given-names>KS</given-names></name><name><surname>Kinney</surname> <given-names>JA</given-names></name><name><surname>Lane</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>The discrimination of relative onset-time of the components of certain speech and nonspeech patterns</article-title><source>Journal of Experimental Psychology</source><volume>61</volume><fpage>379</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1037/h0049038</pub-id><pub-id pub-id-type="pmid">13761868</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname> <given-names>AM</given-names></name><name><surname>Cooper</surname> <given-names>FS</given-names></name><name><surname>Shankweiler</surname> <given-names>DP</given-names></name><name><surname>Studdert-Kennedy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Perception of the speech code</article-title><source>Psychological Review</source><volume>74</volume><fpage>431</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1037/h0020279</pub-id><pub-id pub-id-type="pmid">4170865</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois-Chauvel</surname> <given-names>C</given-names></name><name><surname>de Graaf</surname> <given-names>JB</given-names></name><name><surname>Laguitton</surname> <given-names>V</given-names></name><name><surname>Chauvel</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Specialization of left auditory cortex for speech perception in man depends on temporal coding</article-title><source>Cerebral Cortex</source><volume>9</volume><fpage>484</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1093/cercor/9.5.484</pub-id><pub-id pub-id-type="pmid">10450893</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisker</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>&quot;Voicing&quot; in English: a catalogue of acoustic features signaling /b/ versus /p/ in trochees</article-title><source>Language and Speech</source><volume>29</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1177/002383098602900102</pub-id><pub-id pub-id-type="pmid">3657346</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisker</surname> <given-names>L</given-names></name><name><surname>Abramson</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>A Cross-Language study of voicing in initial stops: acoustical measurements</article-title><source>WORD</source><volume>20</volume><fpage>384</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1080/00437956.1964.11659830</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisker</surname> <given-names>L</given-names></name><name><surname>Abramson</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Some effects of context on voice onset time in english stops</article-title><source>Language and Speech</source><volume>10</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1177/002383096701000101</pub-id><pub-id pub-id-type="pmid">6044530</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macmillan</surname> <given-names>NA</given-names></name><name><surname>Kaplan</surname> <given-names>HL</given-names></name><name><surname>Creelman</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>The psychophysics of categorical perception</article-title><source>Psychological Review</source><volume>84</volume><fpage>452</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.84.5.452</pub-id><pub-id pub-id-type="pmid">905471</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnuson</surname> <given-names>JS</given-names></name><name><surname>You</surname> <given-names>H</given-names></name><name><surname>Luthra</surname> <given-names>S</given-names></name><name><surname>Li</surname> <given-names>M</given-names></name><name><surname>Nam</surname> <given-names>H</given-names></name><name><surname>Escabí</surname> <given-names>M</given-names></name><name><surname>Brown</surname> <given-names>K</given-names></name><name><surname>Allopenna</surname> <given-names>PD</given-names></name><name><surname>Theodore</surname> <given-names>RM</given-names></name><name><surname>Monto</surname> <given-names>N</given-names></name><name><surname>Rueckl</surname> <given-names>JG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>EARSHOT: a minimal neural network model of incremental human speech recognition</article-title><source>Cognitive Science</source><volume>44</volume><elocation-id>12823</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.12823</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margoliash</surname> <given-names>D</given-names></name><name><surname>Fortune</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Temporal and harmonic combination-sensitive neurons in the zebra Finch's HVc</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4309</fpage><lpage>4326</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-11-04309.1992</pub-id><pub-id pub-id-type="pmid">1432096</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massaro</surname> <given-names>DW</given-names></name><name><surname>Cohen</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Categorical or continuous speech perception: a new test</article-title><source>Speech Communication</source><volume>2</volume><fpage>15</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/0167-6393(83)90061-4</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Mirman</surname> <given-names>D</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Are there interactive processes in speech perception?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>363</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.06.007</pub-id><pub-id pub-id-type="pmid">16843037</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Mirman</surname> <given-names>D</given-names></name><name><surname>Bolger</surname> <given-names>DJ</given-names></name><name><surname>Khaitan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interactive activation and mutual constraint satisfaction in perception and cognition</article-title><source>Cognitive Science</source><volume>38</volume><fpage>1139</fpage><lpage>1189</lpage><pub-id pub-id-type="doi">10.1111/cogs.12146</pub-id><pub-id pub-id-type="pmid">25098813</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Elman</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The TRACE model of speech perception</article-title><source>Cognitive Psychology</source><volume>18</volume><fpage>1</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(86)90015-0</pub-id><pub-id pub-id-type="pmid">3753912</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Rumelhart</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An interactive activation model of context effects in letter perception</article-title><source>Psychological Review</source><volume>88</volume><fpage>375</fpage><lpage>407</lpage></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>GABA as an inhibitory neurotransmitter in human cerebral cortex</article-title><source>Journal of Neurophysiology</source><volume>62</volume><fpage>1018</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1152/jn.1989.62.5.1018</pub-id><pub-id pub-id-type="pmid">2573696</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMurray</surname> <given-names>B</given-names></name><name><surname>Tanenhaus</surname> <given-names>MK</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Gradient effects of within-category phonetic variation on lexical access</article-title><source>Cognition</source><volume>86</volume><fpage>B33</fpage><lpage>B42</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(02)00157-9</pub-id><pub-id pub-id-type="pmid">12435537</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMurray</surname> <given-names>B</given-names></name><name><surname>Jongman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What information is necessary for speech categorization? harnessing variability in the speech signal by integrating cues computed relative to expectations</article-title><source>Psychological Review</source><volume>118</volume><fpage>219</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1037/a0022325</pub-id><pub-id pub-id-type="pmid">21417542</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McQueen</surname> <given-names>JM</given-names></name><name><surname>Norris</surname> <given-names>D</given-names></name><name><surname>Cutler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Are there really interactive processes in speech perception?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><elocation-id>533</elocation-id><pub-id pub-id-type="doi">10.1016/j.tics.2006.10.004</pub-id><pub-id pub-id-type="pmid">17067845</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>JL</given-names></name><name><surname>Green</surname> <given-names>KP</given-names></name><name><surname>Reeves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Speaking rate and segments: a look at the relation between speech production and speech perception for the voicing contrast</article-title><source>Phonetica</source><volume>43</volume><fpage>106</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1159/000261764</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>JL</given-names></name><name><surname>Volaitis</surname> <given-names>LE</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Effect of speaking rate on the perceptual structure of a phonetic category</article-title><source>Perception &amp; Psychophysics</source><volume>46</volume><fpage>505</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.3758/BF03208147</pub-id><pub-id pub-id-type="pmid">2587179</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname> <given-names>EB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dissociable effects of phonetic competition and category typicality in a phonetic categorization task: an fMRI investigation</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>1463</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.11.005</pub-id><pub-id pub-id-type="pmid">17178420</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname> <given-names>EB</given-names></name><name><surname>Blumstein</surname> <given-names>SE</given-names></name><name><surname>Walsh</surname> <given-names>E</given-names></name><name><surname>Eliassen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Inferior frontal regions underlie the perception of phonetic category invariance</article-title><source>Psychological Science</source><volume>20</volume><fpage>895</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02380.x</pub-id><pub-id pub-id-type="pmid">19515116</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname> <given-names>D</given-names></name><name><surname>McQueen</surname> <given-names>JM</given-names></name><name><surname>Cutler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Merging information in speech recognition: feedback is never necessary</article-title><source>Behavioral and Brain Sciences</source><volume>23</volume><fpage>299</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1017/S0140525X00003241</pub-id><pub-id pub-id-type="pmid">11301575</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname> <given-names>D</given-names></name><name><surname>McQueen</surname> <given-names>JM</given-names></name><name><surname>Cutler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction, bayesian inference and feedback in speech recognition</article-title><source>Language, Cognition and Neuroscience</source><volume>31</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1081703</pub-id><pub-id pub-id-type="pmid">26740960</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname> <given-names>D</given-names></name><name><surname>McQueen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Shortlist B: a bayesian model of continuous speech recognition</article-title><source>Psychological Review</source><volume>115</volume><fpage>357</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.115.2.357</pub-id><pub-id pub-id-type="pmid">18426294</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname> <given-names>KV</given-names></name><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Rhone</surname> <given-names>AE</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Kawasaki</surname> <given-names>H</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name><name><surname>McMurray</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sound identification in human auditory cortex: differential contribution of local field potentials and high gamma power as revealed by direct intracranial recordings</article-title><source>Brain and Language</source><volume>148</volume><fpage>37</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2015.03.003</pub-id><pub-id pub-id-type="pmid">25819402</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname> <given-names>J</given-names></name><name><surname>Eisner</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Pre-lexical abstraction of speech in the auditory cortex</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>14</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.09.005</pub-id><pub-id pub-id-type="pmid">19070534</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oganian</surname> <given-names>Y</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A speech envelope landmark for syllable encoding in human superior temporal gyrus</article-title><source>Science Advances</source><volume>5</volume><elocation-id>eaay6279</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aay6279</pub-id><pub-id pub-id-type="pmid">31976369</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How we hear: the perception and neural coding of sound</article-title><source>Annual Review of Psychology</source><volume>69</volume><fpage>27</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122216-011635</pub-id><pub-id pub-id-type="pmid">29035691</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>H</given-names></name><name><surname>Ince</surname> <given-names>RA</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural oscillations carry speech rhythm through to comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peña</surname> <given-names>JL</given-names></name><name><surname>Konishi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Auditory spatial receptive fields created by multiplication</article-title><source>Science</source><volume>292</volume><fpage>249</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1126/science.1059201</pub-id><pub-id pub-id-type="pmid">11303092</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pena</surname> <given-names>JL</given-names></name><name><surname>Konishi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>From postsynaptic potentials to spikes in the genesis of auditory spatial receptive fields</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5652</fpage><lpage>5658</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-13-05652.2002</pub-id><pub-id pub-id-type="pmid">12097516</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisoni</surname> <given-names>DB</given-names></name><name><surname>Tash</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Reaction times to comparisons within and across phonetic categories</article-title><source>Perception &amp; Psychophysics</source><volume>15</volume><fpage>285</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.3758/BF03213946</pub-id><pub-id pub-id-type="pmid">23226881</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portfors</surname> <given-names>CV</given-names></name><name><surname>Wenstrup</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Topographical distribution of delay-tuned responses in the mustached bat inferior colliculus</article-title><source>Hearing Research</source><volume>151</volume><fpage>95</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(00)00214-8</pub-id><pub-id pub-id-type="pmid">11124455</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical processing of complex sounds</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>516</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80040-8</pub-id><pub-id pub-id-type="pmid">9751652</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Is there a tape recorder in your head? how the brain stores and retrieves musical melodies</article-title><source>Frontiers in Systems Neuroscience</source><volume>8</volume><elocation-id>149</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2014.00149</pub-id><pub-id pub-id-type="pmid">25221479</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1000610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000610</pub-id><pub-id pub-id-type="pmid">21532743</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Temporal information in speech: acoustic, auditory and linguistic aspects</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>336</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0070</pub-id><pub-id pub-id-type="pmid">1354376</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schouten</surname> <given-names>B</given-names></name><name><surname>Gerrits</surname> <given-names>E</given-names></name><name><surname>van Hessen</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The end of categorical perception as we know it</article-title><source>Speech Communication</source><volume>41</volume><fpage>71</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00094-8</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Noise, neural codes and cortical organization</article-title><source>Current Opinion in Neurobiology</source><volume>4</volume><fpage>569</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(94)90059-0</pub-id><pub-id pub-id-type="pmid">7812147</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>RV</given-names></name><name><surname>Zeng</surname> <given-names>FG</given-names></name><name><surname>Kamath</surname> <given-names>V</given-names></name><name><surname>Wygonski</surname> <given-names>J</given-names></name><name><surname>Ekelid</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjerps</surname> <given-names>MJ</given-names></name><name><surname>Fox</surname> <given-names>NP</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speaker-normalized sound representations in the human auditory cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2465</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10365-z</pub-id><pub-id pub-id-type="pmid">31165733</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname> <given-names>E</given-names></name><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive top-down integration of prior knowledge during speech perception</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>8443</fpage><lpage>8453</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5069-11.2012</pub-id><pub-id pub-id-type="pmid">22723684</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soli</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The role of spectral cues in discrimination of voice onset time differences</article-title><source>The Journal of the Acoustical Society of America</source><volume>73</volume><fpage>2150</fpage><lpage>2165</lpage><pub-id pub-id-type="doi">10.1121/1.389539</pub-id><pub-id pub-id-type="pmid">6875101</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Arezzo</surname> <given-names>JC</given-names></name><name><surname>Vaughan</surname> <given-names>HG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Speech-evoked activity in primary auditory cortex: effects of voice onset time</article-title><source>Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section</source><volume>92</volume><fpage>30</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/0168-5597(94)90005-1</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Arezzo</surname> <given-names>JC</given-names></name><name><surname>Vaughan</surname> <given-names>HG</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Physiologic correlates of the voice onset time boundary in primary auditory cortex (A1) of the awake monkey: temporal response patterns</article-title><source>Brain and Language</source><volume>48</volume><fpage>326</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1006/brln.1995.1015</pub-id><pub-id pub-id-type="pmid">7757450</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Volkov</surname> <given-names>IO</given-names></name><name><surname>Noh</surname> <given-names>MD</given-names></name><name><surname>Garell</surname> <given-names>PC</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Temporal encoding of the voice onset time phonetic parameter by field potentials recorded directly from human auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>2346</fpage><lpage>2357</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.5.2346</pub-id><pub-id pub-id-type="pmid">10561410</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Fishman</surname> <given-names>YI</given-names></name><name><surname>Arezzo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Representation of the voice onset time (VOT) speech parameter in population responses within primary auditory cortex of the awake monkey</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>307</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1121/1.1582449</pub-id><pub-id pub-id-type="pmid">12880043</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Volkov</surname> <given-names>IO</given-names></name><name><surname>Fishman</surname> <given-names>YI</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Arezzo</surname> <given-names>JC</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Intracortical responses in human and monkey primary auditory cortex support a temporal processing mechanism for encoding of the voice onset time phonetic parameter</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>170</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh120</pub-id><pub-id pub-id-type="pmid">15238437</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Fishman</surname> <given-names>YI</given-names></name><name><surname>Arezzo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spectrotemporal analysis of evoked and induced electroencephalographic responses in primary auditory cortex (A1) of the awake monkey</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>610</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm094</pub-id><pub-id pub-id-type="pmid">17586604</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Nourski</surname> <given-names>KV</given-names></name><name><surname>Kawasaki</surname> <given-names>H</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Brugge</surname> <given-names>JF</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Intracranial study of speech-elicited activity on the human posterolateral superior temporal gyrus</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>2332</fpage><lpage>2347</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr014</pub-id><pub-id pub-id-type="pmid">21368087</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Nourski</surname> <given-names>KV</given-names></name><name><surname>Fishman</surname> <given-names>YI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representation of speech in human auditory cortex: is it special?</article-title><source>Hearing Research</source><volume>305</volume><fpage>57</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.05.013</pub-id><pub-id pub-id-type="pmid">23792076</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname> <given-names>KN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Toward a model for lexical access based on acoustic landmarks and distinctive features</article-title><source>The Journal of the Acoustical Society of America</source><volume>111</volume><fpage>1872</fpage><lpage>1891</lpage><pub-id pub-id-type="doi">10.1121/1.1458026</pub-id><pub-id pub-id-type="pmid">12002871</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname> <given-names>KN</given-names></name><name><surname>Klatt</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Role of Formant transitions in the voiced-voiceless distinction for stops</article-title><source>The Journal of the Acoustical Society of America</source><volume>55</volume><fpage>653</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1121/1.1914578</pub-id><pub-id pub-id-type="pmid">4819867</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>Q</given-names></name><name><surname>Haggard</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>On the dissociation of spectral and temporal cues to the voicing distinction in initial stop consonants</article-title><source>The Journal of the Acoustical Society of America</source><volume>62</volume><fpage>435</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1121/1.381544</pub-id><pub-id pub-id-type="pmid">886081</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>C</given-names></name><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Intonational speech prosody encoding in the human auditory cortex</article-title><source>Science</source><volume>357</volume><fpage>797</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1126/science.aam8577</pub-id><pub-id pub-id-type="pmid">28839071</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname> <given-names>F</given-names></name><name><surname>Miller</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Temporal encoding in nervous systems: a rigorous definition</article-title><source>Journal of Computational Neuroscience</source><volume>2</volume><fpage>149</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1007/BF00961885</pub-id><pub-id pub-id-type="pmid">8521284</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toscano</surname> <given-names>JC</given-names></name><name><surname>McMurray</surname> <given-names>B</given-names></name><name><surname>Dennhardt</surname> <given-names>J</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Continuous perception and graded categorization: electrophysiological evidence for a linear relationship between the acoustic signal and perceptual encoding of speech</article-title><source>Psychological Science</source><volume>21</volume><fpage>1532</fpage><lpage>1540</lpage><pub-id pub-id-type="doi">10.1177/0956797610384142</pub-id><pub-id pub-id-type="pmid">20935168</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toscano</surname> <given-names>JC</given-names></name><name><surname>Anderson</surname> <given-names>ND</given-names></name><name><surname>Fabiani</surname> <given-names>M</given-names></name><name><surname>Gratton</surname> <given-names>G</given-names></name><name><surname>Garnsey</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The time-course of cortical responses to speech revealed by fast optical imaging</article-title><source>Brain and Language</source><volume>184</volume><fpage>32</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2018.06.006</pub-id><pub-id pub-id-type="pmid">29960165</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toscano</surname> <given-names>JC</given-names></name><name><surname>McMurray</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cue integration with categories: weighting acoustic cues in speech using unsupervised learning and distributional statistics</article-title><source>Cognitive Science</source><volume>34</volume><fpage>434</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1111/j.1551-6709.2009.01077.x</pub-id><pub-id pub-id-type="pmid">21339861</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname> <given-names>HG</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The encoding of speech sounds in the superior temporal gyrus</article-title><source>Neuron</source><volume>102</volume><fpage>1096</fpage><lpage>1110</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.023</pub-id><pub-id pub-id-type="pmid">31220442</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Belin</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Spectral and temporal processing in human auditory cortex</article-title><source>Cerebral Cortex</source><volume>11</volume><fpage>946</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1093/cercor/11.10.946</pub-id><pub-id pub-id-type="pmid">11549617</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53051.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role>Reviewing Editor</role><aff><institution>Washington University in St. Louis</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Wehr</surname><given-names>Michael</given-names> </name><role>Reviewer</role><aff><institution>University of Oregon</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>A major challenge for the auditory system is to interpret fine acoustic cues present in the speech signal in order to identify words. This work uses intercranial recordings from human listeners to better understand voice onset time, a key distinction in speech sounds, showing that individual electrodes in temporal lobe are sensitive to voice onset time differences. Complementing experimental work is an example model illustrating how voice onset time might be coded in a neural network.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Transformation of a temporal speech cue to a spatial neural code in human auditory cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Michael Wehr (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary</p><p>The authors report ECoG data from human listeners while listening to spoken syllables that varied in voice onset time (VOT). They found that VOT (a temporal cue) is encoded in the peak amplitude of high gamma activity in individual electrodes. A simple neural network is presented that qualitatively captures main features of the human data. The findings complement prior ECoG studies on the coding of basic speech properties in the temporal lobes.</p><p>Essential revisions</p><p>1) The rejection of a temporal code (or temporal contribution) for VOT representation was not entirely convincing. One concern is that the peak window may be too short to capture the temporal dynamics of the signal, so the fact that the classifier fails for temporal information could be artifactual. Examples of the types of dynamics that might temporally encode voicing would be onset latency, peak latency, or waveform shape. The 100 ms peak windows (150-250 ms) miss the onset, probably degrade peak latency information, and likely do not capture the waveform shape (e.g. single or double-peaked, fast or slow rise or decay times, etc). In other words, the HG amplitude appears to be mostly flat during this 100 ms window and thus cannot contain the timing information you wish to test for. Thus the classifier analysis seems almost designed to fail to decode timing information. A different (and possibly more straightforward) way to look at temporal information might be the following. Since you have already extracted the peak time and amplitude, and you want to know whether timing or amplitude convey information, why not just run a classifier on peak times, peak amplitudes, and both? This way instead of removing amplitude information by normalizing, or removing timing information by jittering, you can just directly ask whether the amplitude or timing values can be used to decode voicing. This could serve as a useful corroboration of the multivariate decoding results, or might instead reveal information in peak timing.</p><p>2) Although the inclusion of a model was a nice touch, the theoretical contribution of doing so was somewhat unclear. Are there other theoretical frameworks for understanding VOT representations that can be contrasted with the current one? Damper, 1994, is one that was identified by a reviewer (there may be others). Overall we had a difficult time discerning the theoretical advance gained from the model, and a clearer link to existing understandings (does it resolve a controversy?) or clearer way in which it might motivate further experimental approaches would be useful.</p><p>3) The focus in the current analysis is on high gamma oscillations. However, other work has suggested a role for low frequency oscillations in phoneme perception (Peelle and Davis 2012; Kösem et al., 2018). So, (a) what's the justification for focusing exclusively on high gamma, and (b) what is a framework for reconciling your high gamma responses with a potential role for lower frequencies?</p><p>4) The discussion of local or ensemble temporal coding and spatial coding would benefit from consideration of hierarchical organization and the construction of feature selectivity. If the observed spatial code is the result of some temporal-to-rate transformation, where might this occur and how does that relate to the types of feature selectivity seen in human and primate auditory cortex? As an analogy, your findings are reminiscent of call-echo sensitive cells in the bat. There, many cells in IC respond both to call and to echo (“double-peaked”), whereas other cells in IC respond only to the combination of call and an echo at a particular delay (“single-peaked”). The latter are not topographically organized in IC, but in the FM region of auditory cortex such cells form a topographic map of delay. Do you imagine that a similar hierarchical transformation is occurring in the human auditory system for the encoding of VOT? Where do your recordings and those of e.g. Steinschneider or Eggermont fit into this picture?</p><p>5) Please make the stimuli available as supplemental material.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Transformation of a temporal speech cue to a spatial neural code in human auditory cortex&quot; for further consideration by <italic>eLife</italic>. We apologize for the delay in returning a decision to you – due to COVID19-related disruptions to the workflows of many of our editors and reviewers, the process is taking longer than we would like. Thank you for your patience with us.</p><p>Your revised article has been evaluated by Barbara Shinn-Cunningham (Senior Editor) and a Reviewing Editor. We agree that the manuscript has been improved but there are some remaining issues that need to be addressed before acceptance. Specifically, the issue of the distinction between the hypotheses laid out in subsection “Peak neural response amplitude robustly encodes voicing category”.</p><p>The issues are nicely laid out by reviewer 3's comments, which we include unedited (so will not repeat here). It seems clear that the manuscript makes a valuable contribution and we have no remaining major issues with the analyses. However, it will be important to address the apparent contradictions in the results and conclusions in discriminating between the hypothesses laid out in subsection “Peak neural response amplitude robustly encodes voicing category”.</p><p>Of course, there is a chance we all missed something obvious – please let us know, and perhaps some clarification in the text would be helpful in this case.</p><p><italic>Reviewer #3:</italic></p><p>The authors have done a good job addressing the comments and the revised manuscript is responsive to most of the points raised. The additional interpretation of the model results, robustness, and context are welcome additions. The discussion of coding and hierarchical processing are also good. Yet there is a remaining issue that sticks out and that needs to be resolved. This is the question of whether the temporal patterns of neural responses encode VOT information. To be clear, I don't have a dog in this fight – I'm neutral about spatial or temporal codes. I'm just pointing out that the manuscript is internally conflicted on this point, and the revisions haven't resolved the issue.</p><p>As the authors spell out in the rebuttal, &quot;It is certainly the case that both sub-categorical and category-level information is carried by the onset latency of voiced-selective (V+) neural populations (Figure 3). However, this temporal information does not contribute to classification of voicing category (Figure 1F) because this information is not available during the peak window.&quot; Reading the reporting of the amplitude/timing decoding shown in Figure 1F, the take-home message from that is that peak amplitude, but not timing, contains VOT information. This message is wrong, because as shown in Figure 3 the onset latency encodes VOT information. So care must be taken to avoid leading readers towards that message.</p><p>Close reading of the Results section reporting Figure 1F reveals that the statements are accurate because they contain a clause such as &quot;in the peak response window,&quot; for example: &quot;In contrast, when amplitude information was corrupted and only temporal patterns in the peak response window were reliable (-Amplitude/+Timing), classifier performance was not different from chance.&quot; Even though this statement is accurate, I'd argue that it's misleading, especially because the set-up is to distinguish between 3 hypotheses: &quot;Specifically, we evaluated three alternatives for how temporally-cued voicing category is encoded by high-gamma responses in cortex: (1) the spatial pattern of peak response amplitude across electrodes, (2) the temporal patterns of evoked responses across electrodes, or (3) both amplitude and timing of neural activity patterns.&quot; At the end of this section, after looking at Figure 1F, the reader is left with hypothesis (1) as the take-home. But your data rule out (1) and instead demonstrate hypothesis (3), but not until Figure 3. I get the motivation that you want to show encoding by peak amplitude in order to compare with previous findings from your group. That's fine. But there's no need to rule out a temporal code to do this. If the take-home message from Figure 1 is that VOT information is encoded in peak amplitude, a spatial code, just say that, and drop the temporal jitter analysis, because it's misleading and unnecessary. Or else expand the window to include onsets, which based on Figure 3 should support VOT classification.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53051.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions</p><p>1) The rejection of a temporal code (or temporal contribution) for VOT representation was not entirely convincing. One concern is that the peak window may be too short to capture the temporal dynamics of the signal, so the fact that the classifier fails for temporal information could be artifactual. Examples of the types of dynamics that might temporally encode voicing would be onset latency, peak latency, or waveform shape. The 100 ms peak windows (150-250 ms) miss the onset, probably degrade peak latency information, and likely do not capture the waveform shape (e.g. single or double-peaked, fast or slow rise or decay times, etc). In other words, the HG amplitude appears to be mostly flat during this 100 ms window and thus cannot contain the timing information you wish to test for. Thus the classifier analysis seems almost designed to fail to decode timing information. A different (and possibly more straightforward) way to look at temporal information might be the following. Since you have already extracted the peak time and amplitude, and you want to know whether timing or amplitude convey information, why not just run a classifier on peak times, peak amplitudes, and both? This way instead of removing amplitude information by normalizing, or removing timing information by jittering, you can just directly ask whether the amplitude or timing values can be used to decode voicing. This could serve as a useful corroboration of the multivariate decoding results, or might instead reveal information in peak timing.</p></disp-quote><p>We understand the concerns embodied by this reviewer comment and appreciate the suggestions offered here. We have addressed them in four ways.</p><p>First, we have revised and clarified our claims to state that we are <italic>not</italic> ruling out a temporal code entirely, but instead are focusing on our key, novel result: the highly robust encoding of voicing in the peak neural response amplitude. For the reasons the reviewer mentioned, it is difficult, if not impossible, to completely rule out any possible role for a temporal code, since there are many possible ways in which such a code could manifest (including several that are discussed here).</p><p>What we find most important and striking are:</p><p>a) that the encoding of voicing category on single trials appears to depend strongly on the response amplitude during the peak window, but does not seem to depend greatly on the fine temporal dynamics during that window, and</p><p>b) that the signals that encode VOT in their amplitude (namely, the peaks of high gamma evoked responses) are the same as the signals that have previously been shown to encode other (primarily spectrally-cued) phonetic features (Mesgarani, Cheung, Johnson, and Chang, 2014).</p><p>We have made numerous modifications throughout the manuscript to clarify what our results do and do not show (Results and Discussion). Overall, we have clarified the conclusions we draw from the decoding analyses depicted in Figure 1F in order to focus on the above claims, which we believe constitute novel and important results.</p><p>Second, we have clarified the rationale for our original analysis approach. To that end, we address several specific points in the reviewer comment:</p><p>Peak window: The analysis of amplitude and temporal information was designed to be confined to the peak response window (150-250 ms after stimulus onset), a time window of interest to us based on prior work examining the encoding of spectrally cued phonetic features within the peak high-gamma responses of spatially discrete neural populations in human temporal lobe (Mesgarani et al., 2014). We do not believe that, <italic>a priori</italic>, this makes the temporal model likely to fail.</p><p>Waveform shape during peak: Although the amplitude may appear to be mostly flat during this window in the trial-averaged traces shown in Figure 1E, they are by no means flat on a single-trial basis (see <xref ref-type="fig" rid="respfig1">Author response image 1</xref>). Indeed, our peak window also overlaps almost entirely with a window during which past work (examining intracranial auditory evoked local field potentials) found evidence of waveform shape differences between voiced and voiceless stimuli (single- vs. double-peaked responses), prompting claims of temporal coding of VOT (see, e.g., Figure 10 of Steinschneider et al., 2011). In other words, there was no <italic>a priori</italic> reason to believe that the peak window we selected would not also contain temporal information in the form of waveform shape differences. If such reliable differences had existed in our high-gamma data, our method of corrupting temporal information (jittering) would have detected a contribution of temporal information to decoding, but it did not. Indeed, the fact that the trial-averaged waveforms appear to be relatively flat during this window (even though single-trial waveforms are not) is visual evidence that waveform shape is not a reliable cue to voicing category here.</p><p>Peak latency: To the extent that the peak of a trial’s evoked high-gamma response occurs during or close to the peak window, the contribution of peak latency information to decoding accuracy would also be captured by our approach, as the temporal jittering procedure would disrupt the reliability of this information. We address this issue directly in a new analysis described below (see Author response images 1 and 2).</p><p>Onset latency: If the only difference between the high-gamma responses elicited by different VOTs was when the response started (i.e., its onset latency), with all other aspects of the waveform’s shape remaining constant across conditions (i.e., a “phase shift”), there would also be reliable VOT-dependent differences in the responses’ temporal dynamics during the peak response window.</p><p>Other possible sources of temporal codes:</p><p>Onset latency: As is evident from our results in Figure 3, there are reliable VOT dependent temporal differences among response onset latencies in voiced selective electrodes that are apparently not reflected during the peak window (since decoding is not significantly affected by temporal jittering).</p><p>Outside of high-gamma range: Despite our focus on the high-gamma range, we recognize the importance of potential reliable temporal coding features carried by other components of the neural response, such as lower-frequency components. We address this possibility directly in new supplementary analyses (discussed in response to Essential Revision #3 in this letter; see Figure 1—figure supplements 3 and 4 in revised manuscript).</p><p>We have revised the manuscript to clarify the types of temporal information that would be corrupted by the temporal jittering approach (Results and Materials and methods). We also now emphasize that the results of our decoding analysis serve primarily to highlight the contribution of peak high-gamma amplitude to VOT representation (a novel result), but this analysis cannot elucidate whether other temporal properties of the neural response could also carry information about VOT (e.g., outside of the peak window or outside of the high-gamma range) (Results).</p><p>Third, we conducted the analysis suggested by the reviewer(s), obtaining results that ultimately support the same conclusion as our original decoding/classifier analyses. In <xref ref-type="fig" rid="respfig1">Author response image 1</xref>, we illustrate what the data look like for two representative electrodes (one voiceless-selective [e1] and one voiced-selective [e2]) by plotting the high-gamma traces elicited by each VOT stimulus on six individual trials (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>, left). For each trial, the peak high-gamma activity was identified. Next, we plot the peak latency and amplitude for every trial for each of the example electrodes in order to illustrate the clearer separation of VOT categories using the amplitude information (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>, right).</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Single electrodes demonstrate better separation of voicing category based on peak amplitude vs. peak latency.</title><p>left: High-gamma traces for six single trials (one per VOT condition, as indicated by line color; 0ms VOT = red; 50ms VOT = blue; example trials shown for visual simplicity) in each of two example VOT-sensitive electrodes (e1: voiceless-selective; e2: voiced-selective; same electrodes as shown in Figure 1 of the main text). Black dots indicate the peak high-gamma amplitude and latency for each trial. There is clear variation among single trials in the peak’s timing and amplitude. middle: The latency of the peak, tp, (in seconds) for each trial (n = 234 total trials; color of circles corresponds to trial’s voicing category: /b/ = red; /p/ = blue) projected into a 2-dimensional space, with the vertical and horizontal dimensions representing the two example electrodes (e1 vs. e2). Trials were selected such that peaks occurred between 0 and 0.5 seconds after stimulus onset. This panel illustrates the lack of a reliable difference between voicing categories based on the peak latency. right: The amplitude of the peak, HGz(tp), for each trial projected into the same 2-dimensional space illustrates the highly reliable difference between voicing categories based on peak amplitude.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-resp-fig1-v2.tif"/></fig><p>Per the reviewer’s suggestion, we quantified these results by building two separate classifiers (linear discriminant analysis with leave-one-out cross-validation) that used either peak latency or peak amplitude information separately. Consistent with our original result, we found that peak latency information alone did not lead to above-chance accuracy, while peak amplitude information performed significantly better than chance, and also significantly higher than the peak latency classifier (<xref ref-type="fig" rid="respfig2">Author response image 2</xref>).</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Peak amplitude outperforms peak latency in classifying single-trial voicing category.</title><p>For each participant, two classifier analyses were conducted to predict each trial’s voicing category using leave-one-out cross-validation. All speech-responsive electrodes for a given patient were included in both classifiers, but classifiers included only either temporal [Temp; peak latency = tp] or amplitude [Amp; peak amplitude = HGz(tp)] features. Across participants, only amplitude features performed better than chance (chance = 50%), and amplitude features performed significantly better than temporal features (ps &lt; 0.01; Wilcoxon signed-rank tests). Error bars represent standard error across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53051-resp-fig2-v2.tif"/></fig><p>As discussed above, we believe that our original decoding analysis constitutes a more general test of the hypothesis that peak amplitude information is a robust predictor of voicing category because peak latency information is just one type of temporal information that is included in our original decoding analysis, along with waveform shape, which could not be captured in this alternative analysis of single-trial peaks. Since both analyses ultimately point to the same conclusion, but the original analysis is more general, and since we have revised our claims to focus less on rejecting temporal codes than on illustrating the robustness of the amplitude information, we have opted to leave the original decoding analysis in the manuscript.</p><p>Please note, however, that if the Editors and reviewers feel it would help strengthen the manuscript, we are happy to include <xref ref-type="fig" rid="respfig1">Author response images 1</xref> and <xref ref-type="fig" rid="respfig2">2</xref> as figure supplements in the final manuscript.</p><p>Fourth, and in line with our renewed focus and recognition that it is not possible to completely reject every potential role for temporal information in VOT perception or representation, we present another new analysis that examines temporal coding features not contained within the high-gamma response. Because this new analysis is also responsive to other reviewer comments regarding contributions of lower-frequency information to VOT encoding, we discuss it in detail in response to Essential Revision #3 in this letter (see Figure 1—figure supplements 3 and 4 in revised manuscript). Regarding Essential Revision #1, though, the most relevant update to the manuscript is an acknowledgement that this result demonstrates that temporal and amplitude representations are not mutually exclusive (Discussion).</p><disp-quote content-type="editor-comment"><p>2) Although the inclusion of a model was a nice touch, the theoretical contribution of doing so was somewhat unclear. Are there other theoretical frameworks for understanding VOT representations that can be contrasted with the current one? Damper, 1994, is one that was identified by a reviewer (there may be others). Overall we had a difficult time discerning the theoretical advance gained from the model, and a clearer link to existing understandings (does it resolve a controversy?) or clearer way in which it might motivate further experimental approaches would be useful.</p></disp-quote><p>We appreciate this question, and are eager to clarify the role we believe the model plays in this study. We had two primary goals in including a computational model: (1) using simple, theoretically-motivated, and well-established computational mechanisms, we wanted to replicate as many of the key aspects of our data as possible <italic>in silico</italic>; and (2) we wanted to provide a mathematical description of our key result, namely that a temporal speech cue is encoded by a spatial (amplitude) code across different neural populations.</p><p>As with most computational modeling approaches, there were a large number of possible architectures and algorithms we could have chosen, including the one mentioned by the reviewers (Damper, 1994). Here, we were guided primarily by <italic>Occam’s Razor</italic>, seeking to implement an extremely simple model that could be linked directly to the scale of the neural data we have available (namely, population-level ECoG electrodes).</p><p>Motivated by previous literature (Buonomano and Merzenich, 1995; Carr, 1993; Gao and Wehr, 2015; Rauschecker, 2014), we sought to implement two types of computation that are well-established and reasonable hypotheses for the observation that some electrodes are voiceless-selective and others are voiced-selective. Specifically, the model demonstrates that the key findings regarding the encoding of a temporal speech cue in the amplitude of the peak neural response at spatially discrete neural populations emerge naturally from the time-dependent mechanisms of a simple neural network model with coincidence- and gap-detector circuits. If we had not actually implemented this model, we would have been forced to speculate about how these (or other) computations could underlie the observed data in the Discussion. Instead, we believe that the model allows us to go beyond pure speculation, both in providing an implemented mathematical explanation, and in providing a framework for generating explicit, testable hypotheses for future follow-up work. Thus, although the model we present was not specifically designed to resolve a controversy or distinguish between two particular competing hypotheses about VOT perception, we believe that these theoretical contributions are significant and stand on their own merits.</p><p>We also think it is important that the model we created captures aspects of the neural activity that were not explicitly designed into the model itself. This simple architecture that can achieve gap and coincidence detection also predicts the observed partially-graded within-category encoding of VOT. Additionally, the early temporal dynamics at these spatially localized cortical sites are also predicted by the model. This final point is particularly important because the early temporal dynamics were never considered when selecting electrodes for inclusion in the study, but were perfectly in line with the model’s predictions.</p><p>Together, we believe that these motivations and results warrant including the neural network model. It helps us achieve an important theoretical contribution by providing an explicit, testable account that connects multiple seemingly disparate observations about the neurophysiological data. In fact, all of the complex encoding properties we observed arise directly from a simple model designed to perform gap/coincidence detection by implementing theoretically-motivated, well-established neuronal circuits.</p><p>We have added text to the Discussion to clarify the motivation and theoretical contributions of the model. In addition, we agree with the reviewers that it is important to provide additional context for the specific model we chose; therefore, we have contextualized our model among other possible approaches (Damper, 1994; McClelland and Elman, 1986) within the Discussion.</p><disp-quote content-type="editor-comment"><p>3) The focus in the current analysis is on high gamma oscillations. However, other work has suggested a role for low frequency oscillations in phoneme perception (Peelle and Davis 2012; Kösem et al., 2018). So, (a) what's the justification for focusing exclusively on high gamma, and (b) what is a framework for reconciling your high gamma responses with a potential role for lower frequencies?</p></disp-quote><p>We thank the reviewer for bringing up these important points. We will respond to each of them separately:</p><p>a) Our primary goal was to examine neural responses within the temporal lobe to stimuli varying in their voice-onset time (VOT) using the same neural features previously used to illustrate a spatial (amplitude) code for other phonetic features, including manner and place of articulation of consonants (Mesgarani et al., 2014). In our view, this is critical because it seeks to unify two lines of prior research examining neurophysiological representations of phonetic features cued primarily by spectral acoustic information (e.g., manner/place of articulation) or primarily by temporal acoustic information (e.g., voicing). Up to now, there have been few attempts to address the central theoretical question of whether a common neural code for both exists. Some prior work (especially, but by no means exclusively, work examining auditory evoked local field potentials in primary auditory cortex) has posited that the neural code for VOT differs fundamentally from the code for spectrally-cued phonetic features, with only the latter relying on a spatial code (see, e.g., Steinschneider, Nourski, and Fishman, 2013; Steinschneider, Volkov, Noh, Garell, and Howard, 1999). Meanwhile, the above-referenced recent demonstration of a robust spatial code for spectrally-cued phonetic features focused specifically on the peak high-gamma response amplitude of neural populations in human superior temporal gyrus (Mesgarani et al., 2014).</p><p>Here, we explicitly tested the hypothesis that the same encoding scheme is used to represent phonetic features defined primarily by temporal information. To that end, we focused on stimulus-evoked activity in the high-gamma range of the neural response. Additionally, particularly for direct intracranial recordings, while there is a relatively clear link between high-gamma activity and neuronal firing, the underlying sources and single-/multi-unit activity that give rise to lower frequencies and oscillations are less well-understood. Therefore, while we do not deny the important roles of lower frequency activity, we believe we can make the clearest and most interpretable neurophysiological claims based on intracranially-recorded high-gamma.</p><p>b) Our work does not discount a potential role for low-frequency oscillations in speech perception or in the perception of phonemes. Indeed, our results are not inconsistent with the large body of work focused on phase-amplitude coupling between low frequency oscillations and gamma power (e.g., Fries, 2009; Giraud and Poeppel, 2012; though note that these frameworks typically refer to power in a lower gamma band than is used here). Specifically, it is possible that our perception of voicing based on VOT information (or of other temporally-cued phonetic features) may also depend on or interact with lowfrequency oscillations (Kösem et al., 2018; Peelle and Davis, 2012). These signals may, in fact, be coupled in their phase-amplitude relationship, and, according to the theoretical frameworks in that body of work, it may be the case that low frequency phase information modulates firing rates observed in higher frequency broadband activity.</p><p>In our opinion, a detailed examination of the relationship between low-frequency amplitude and/or phase and high-gamma power and their contributions to VOT encoding in speech is beyond the scope of the current manuscript, since our primary goal was to examine the encoding of VOT using a signal which has been shown to encode other phonetic features. However, we agree with the reviewer(s) that it is important to address these same questions using signals that have been used in directly related work (e.g., work by Steinschneider and Nourski).</p><p>To that end, we have now conducted an additional analysis of the neural responses to our stimuli using the raw voltage local field potential (LFP), which is dominated by lower frequency components. For every VOT-sensitive electrode identified in our study, we used a bootstrapping approach to analyze the correlation between VOT and the peak latency and amplitude of 3 peaks in the auditory evoked potential (AEP): <italic>P</italic><sub>a</sub>, <italic>N</italic><sub>a</sub>, <italic>P</italic><sub>b</sub> (Howard et al., 2000; Nourski et al., 2015). Detailed descriptions of these analyses and their results now appear in new subsections of Materials and methods.</p><p>We also summarize the results of these additional analyses in the main text (Results). Two new figure supplements (Figure 1—figure supplements 3 and 4) illustrate the following four conclusions:</p><p>1) Comparison of the AEPs evoked by different VOTs shows that there exist associations between stimulus VOT and the amplitude/temporal information in the LFP. Among electrodes that robustly encode voicing in their peak high-gamma amplitude (i.e., VOT-sensitive electrodes), these associations between VOT and LFP features are complex and highly variable (Figure 1—figure supplements 3 and 4).</p><p>2) Replicating prior results regarding VOT encoding by AEPs (e.g., Steinschneider et al., 2011), we find that some electrodes (e.g., e1 in Figure 1—figure supplement 4, panels E/I) exhibit temporal encoding of VOT in the latency of various peaks of the AEP. In some electrodes, the nature of this temporal code is straightforward (e.g., in e1, the latency of <italic>N</italic><sub>a</sub> is delayed by ~10ms for every additional 10ms of VOT duration; Figure 1—figure supplement 4, panel M), but – more often – the relationship between VOT and peak latency is less direct (Figure 1—figure supplement 4, panels N-P).</p><p>3) Among electrodes that encode VOT in their peak high-gamma amplitude, there exist many more electrodes that <italic>do not</italic> encode VOT in these temporal features of the AEP (Figure 1—figure supplement 3, panel B), supporting a prominent role for the peak high-gamma amplitude in the neural representation of voicing and of VOT.</p><p>4) Besides the timing of the various AEP peaks, there also exist many electrodes that encode VOT in the amplitude of those peaks (Figure 1—figure supplement 3, panel B). The encoding patterns are often visually similar to the encoding patterns observed in high-gamma (i.e., graded within the electrode’s preferred voicing category; see Figure 1—figure supplement 4, panels Q-S).</p><p>We feel that connecting our data to the previous literature with these additional analyses has substantially enhanced the contribution of our work. Besides these additional analyses, and in response to this and other reviewer comments, we have also updated the manuscript to clarify and emphasize the goal of our study (Results), and to acknowledge potential roles for low-frequency components of the neural response in the perceptual experience of speech and in its neurophysiological representation (Discussion), as discussed above.</p><p>Ultimately, we hope that the revised manuscript communicates that there is interesting and important information carried within lower frequencies (and, in some cases, by their temporal dynamics), while also emphasizing what we view as the significant theoretical contribution constituted by our robust, novel high-gamma data, which connect directly to previous findings regarding speech sound encoding (Discussion). In contrast to prior work theorizing parallel, but fundamentally different, coding schemes for spectrally- and temporally-cued phonetic features, we demonstrate evidence for a shared representation of both by high-gamma in the human superior temporal lobe.</p><disp-quote content-type="editor-comment"><p>4) The discussion of local or ensemble temporal coding and spatial coding would benefit from consideration of hierarchical organization and the construction of feature selectivity. If the observed spatial code is the result of some temporal-to-rate transformation, where might this occur and how does that relate to the types of feature selectivity seen in human and primate auditory cortex? As an analogy, your findings are reminiscent of call-echo sensitive cells in the bat. There, many cells in IC respond both to call and to echo (“double-peaked”), whereas other cells in IC respond only to the combination of call and an echo at a particular delay (“single-peaked”). The latter are not topographically organized in IC, but in the FM region of auditory cortex such cells form a topographic map of delay. Do you imagine that a similar hierarchical transformation is occurring in the human auditory system for the encoding of VOT? Where do your recordings and those of e.g. Steinschneider or Eggermont fit into this picture?</p></disp-quote><p>We thank the reviewers for raising this important question. We believe that this question highlights an important point: that temporal gap detection is a pervasive mechanism in neural processing of auditory stimuli and that coincidence and gap detection can (and likely does) arise at many levels of the nervous system.</p><p>Unfortunately, we do not think we can make strong claims about the hierarchical organization of this type of coding, since the recordings conducted as part of this study do not include data from either subcortical areas (like the inferior colliculus) or primary auditory cortex (Heschl’s gyrus). Therefore, while we agree with the reviewer that the nature of the hierarchical encoding of temporal cues is an important issue and would also link directly to other work in both animal models and humans, most of what we can say would be speculation.</p><p>That said, while we do not wish to speculate too much on these topics, we have addressed the important issues raised by this comment in three ways in the revised manuscript (Discussion).</p><p>First, our model actually suggests that temporal integration may be occurring locally. The inputs to the gap and coincidence detectors in the model are only spectrally processed (burst and voicing detectors), which arrive at various temporal latencies to the coincidence and gap detector units (which are meant to directly represent neural populations in the STG). As such, the model’s prediction of the within-category patterns (Figures 2B-D) and (especially) the temporal onset latency dynamics (Figure 3) are consistent with local temporal integration rather than gap and coincidence detection that is inherited from earlier levels of processing (e.g., from midbrain processing). However, we recognize that this is not a definitive interpretation, and, more importantly, even a finding that temporal integration is occurring locally in non-primary auditory cortex does not preclude that temporal integration could be simultaneously occurring at other (lower) levels of the ascending auditory pathway, including in IC. We have summarized this response in the Discussion.</p><p>Second, it is also worth noting that, contrary to the topographic map of delay described in the FM region of bats, neither the present study nor any others that we are aware of offer evidence of a topographic map of VOT encoding, nor of any other phonetic features, in human superior temporal gyrus. Therefore, the analogy to these animal models may be incomplete, and may require further direct work. This point is now summarized in the Discussion, and additional results regarding the lack of any discernible topographic organization are described in Materials and methods.</p><p>Third, although our discussion of hierarchical transformations in auditory representations is limited, we have tried to clarify how our results relate to past work (e.g., work by Steinschneider and Eggermont mentioned in the reviewer comment) by conducting and reporting new analyses of auditory evoked local field potentials, as described in our response to Essential Revision #3.</p><p>5) Please make the stimuli available as supplemental material.</p><p>We agree that this addition will enhance the contribution of our study, and have included the stimuli among the supplementary materials (see Materials and methods).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The authors have done a good job addressing the comments and the revised manuscript is responsive to most of the points raised. The additional interpretation of the model results, robustness, and context are welcome additions. The discussion of coding and hierarchical processing are also good. Yet there is a remaining issue that sticks out and that needs to be resolved. This is the question of whether the temporal patterns of neural responses encode VOT information. To be clear, I don't have a dog in this fight – I'm neutral about spatial or temporal codes. I'm just pointing out that the manuscript is internally conflicted on this point, and the revisions haven't resolved the issue.</p><p>As the authors spell out in the rebuttal, &quot;It is certainly the case that both sub-categorical and category-level information is carried by the onset latency of voiced-selective (V+) neural populations (Figure 3). However, this temporal information does not contribute to classification of voicing category (Figure 1F) because this information is not available during the peak window.&quot; Reading the reporting of the amplitude/timing decoding shown in Figure 1F, the take-home message from that is that peak amplitude, but not timing, contains VOT information. This message is wrong, because as shown in Figure 3 the onset latency encodes VOT information. So care must be taken to avoid leading readers towards that message.</p><p>Close reading of the Results section reporting Figure 1F reveals that the statements are accurate because they contain a clause such as &quot;in the peak response window,&quot; for example: &quot;In contrast, when amplitude information was corrupted and only temporal patterns in the peak response window were reliable (-Amplitude/+Timing), classifier performance was not different from chance.&quot; Even though this statement is accurate, I'd argue that it's misleading, especially because the set-up is to distinguish between 3 hypotheses: &quot;Specifically, we evaluated three alternatives for how temporally-cued voicing category is encoded by high-gamma responses in cortex: (1) the spatial pattern of peak response amplitude across electrodes, (2) the temporal patterns of evoked responses across electrodes, or (3) both amplitude and timing of neural activity patterns.&quot; At the end of this section, after looking at Figure 1F, the reader is left with hypothesis (1) as the take-home. But your data rule out (1) and instead demonstrate hypothesis (3), but not until Figure 3. I get the motivation that you want to show encoding by peak amplitude in order to compare with previous findings from your group. That's fine. But there's no need to rule out a temporal code to do this. If the take-home message from Figure 1 is that VOT information is encoded in peak amplitude, a spatial code, just say that, and drop the temporal jitter analysis, because it's misleading and unnecessary. Or else expand the window to include onsets, which based on Figure 3 should support VOT classification.</p></disp-quote><p>We thank reviewer 3 for these comments. The reviewer is correct that all claims referencing Figure 1 (including the panel in question – Figure 1F) are meant to apply only to the peak response window. Indeed, we believe that one of the primary contributions of this work is to show that peak high-gamma amplitude robustly encodes voicing category. Figure 1F shows that spatially distributed amplitude patterns are a robust code during the peak response window (150-250ms after stimulus onset) irrespective of whether or not timing information is corrupted.</p><p>As reviewer 3 acknowledges, and as we point out in the manuscript, this peak time window is of special interest because past work has shown that response amplitude of some neural populations throughout STG during this critical window constitute a spatial code for other phonetic properties of speech sounds (e.g., manner/place of articulation). Our primary goal was to test whether a temporally-cued phonetic distinction (voicing/VOT) might be represented within the same neural coding scheme, and our work shows that stop consonant voicing can, in fact, also be accounted for within this same theoretical framework.</p><p>We believe that the robustness of a spatial/amplitude code during this peak time window will be of great interest to readers of this paper, and so have opted not to remove these analyses. Instead, we have added clarifying language emphasizing that our results in Figure 1F refer only to the (critically interesting) peak neural response window (see revisions outlined below).</p><p>As reviewer 3 notes, subsequent analyses showed that sub-categorical and category level information is carried by the onset latency of voiced-selective (V+) neural populations (Figure 3). To better integrate the results in Figure 1 and Figure 3, we have also added text to point the reader to this secondary result and highlight the fact that it is in no way contradictory with our primary result (the spatial/amplitude code for voicing during the peak window).</p><p>In order to address this comment, we have made changes in several places in the manuscript:</p><p>1) Figure 1 caption’s title:</p><p>a) “Speech sound categories that are distinguished by a temporal cue are spatially encoded in the peak amplitude of neural activity in distinct neural populations.”</p><p>2) Motivation of classifier analyses shown in Figure 1F:</p><p>a) “As with the previous analyses, and following prior work on speech sound encoding, these analyses (Figure 1F) focused on cortical high-gamma activity during the peak response window (150-250ms after stimulus onset; but see Figure 3 for analyses of an earlier time window).”</p><p>3) Addition of language emphasizing that the analyses in Figure 1F apply only to the “peak response window”</p><p>4) Caveats pointing the reader to Figure 3 for evidence of temporal encoding patterns:</p><p>a) “Note that, while spatial (and not temporal) patterns of high-gamma responses robustly encode voicing during this critical peak window, we later describe additional analyses that address possible temporal encoding patterns in the local field potential (Figure 1—figure supplements 3 and 4) and in an earlier time window (Figure 3).” (Results)</p><p>b) “…clearly demonstrating that temporal and amplitude codes for VOT are not mutually exclusive (see also temporal encoding patterns in onset latencies of V+ electrodes; Figure 3)” (Discussion)</p><p>5) Clear interpretation of Figure 3 as evidence of temporal encoding pattern when looking outside of the peak response window:</p><p>a) “Finally, Figure 3 shows that, unlike during the peak response window (150250ms after stimulus onset; Figure 1F), temporal information does encode VOT during an earlier window around the neural response onset in some neural populations. Indeed, both sub-phonetic and phonetic category-level information are carried by the onset latency of V+ electrodes, with evoked responses arising later at these sites for stimuli with progressively longer VOTs. Critically, the modeling results indicate that both the amplitude encoding patterns during the peak window and the temporal encoding patterns during the earlier onset window are captured by the same canonical neurophysiological mechanisms.”</p></body></sub-article></article>