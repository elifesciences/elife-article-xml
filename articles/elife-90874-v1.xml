<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">90874</article-id><article-id pub-id-type="doi">10.7554/eLife.90874</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90874.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Jointly looking to the past and the future in visual working memory</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-326565"><name><surname>Liu</surname><given-names>Baiwei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7857-4456</contrib-id><email>b.liu@vu.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-326819"><name><surname>Alexopoulou</surname><given-names>Zampeta-Sofia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7304-0834</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-224556"><name><surname>van Ede</surname><given-names>Freek</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7434-1751</contrib-id><email>freek.van.ede@vu.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Institute for Brain and Behavior Amsterdam, Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Spering</surname><given-names>Miriam</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03rmrcq20</institution-id><institution>The University of British Columbia</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>15</day><month>05</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP90874</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-07-29"><day>29</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-02"><day>02</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.30.526235"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-11-28"><day>28</day><month>11</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90874.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-04-16"><day>16</day><month>04</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90874.2"/></event></pub-history><permissions><copyright-statement>© 2023, Liu et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Liu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-90874-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-90874-figures-v1.pdf"/><abstract><p>Working memory enables us to bridge past sensory information to upcoming future behaviour. Accordingly, by its very nature, working memory is concerned with two components: the past and the future. Yet, in conventional laboratory tasks, these two components are often conflated, such as when sensory information in working memory is encoded and tested at the same location. We developed a task in which we dissociated the past (encoded location) and future (to-be-tested location) attributes of visual contents in working memory. This enabled us to independently track the utilisation of past and future memory attributes through gaze, as observed during mnemonic selection. Our results reveal the joint consideration of past and future locations. This was prevalent even at the single-trial level of individual saccades that were jointly biased to the past and future. This uncovers the rich nature of working memory representations, whereby both past and future memory attributes are retained and can be accessed together when memory contents become relevant for behaviour.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>working memory</kwd><kwd>visual attention</kwd><kwd>gaze bias</kwd><kwd>dynamic situation</kwd><kwd>microsaccade</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>850636</award-id><principal-award-recipient><name><surname>van Ede</surname><given-names>Freek</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Sociale en Geesteswetenschappen, NWO</institution></institution-wrap></funding-source><award-id>14721</award-id><principal-award-recipient><name><surname>van Ede</surname><given-names>Freek</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When memorising dynamic visual objects, the brain codes for both the past and the anticipated future object location and co-activates both codes when selecting memories for guiding behaviour.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Working memory is a fundamental cognitive function that enables us to hold onto past sensory information in service of upcoming future behaviour (<xref ref-type="bibr" rid="bib3">D’Esposito and Postle, 2015</xref>; <xref ref-type="bibr" rid="bib39">van Ede and Nobre, 2023</xref>; <xref ref-type="bibr" rid="bib29">Rainer et al., 1999</xref>). Accordingly, by its very nature, working memory is concerned with two components: the past and the future.</p><p>In conventional laboratory tasks, past and future components are often conflated such as when sensory information in working memory is tested at the same location as where it was encoded. By contrast, in the dynamic situations we face every day, sensory information often disappears at one specific location (where it enters visual working memory) but becomes relevant at another location (as also in <xref ref-type="bibr" rid="bib1">Brincat et al., 2021</xref>; <xref ref-type="bibr" rid="bib6">Doherty et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Woodman et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Kahneman et al., 1992</xref>; <xref ref-type="bibr" rid="bib44">Zaksas et al., 2001</xref>). Imagine trying to capture a photograph of a precious bird species that you just saw disappear behind a building. Your working memory of the bird is likely to consider not only where you last saw the bird, but also where you expect it to re-appear to capture it on camera. Such situations raise an interesting, underexplored question: when past and future locations of memory contents are not the same, does the brain code internal representations with regard to past, future, or both?</p><p>To address this question, we developed a task in which we dissociated the encoding (past) and to-be-tested (future) locations associated with visual representations in working memory. This enabled us to experimentally isolate past and future memory attributes and to track their respective utilisation through spatial biases in gaze behaviour in healthy human volunteers.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Twenty-five human volunteers performed a working-memory task in which visual memory items were encoded and tested at different locations (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Participants memorised two coloured gratings with different orientations presented either vertically or horizontally. The crucial manipulation was that we always tested memory content in the orthogonal axis and at a predictable location (depending on the future rule that we varied across sessions; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for the four possible rules).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Directional gaze biases by past and future locations during mnemonic selection overlap in time.</title><p>(<bold>a</bold>) Task schematic. Participants memorised two oriented gratings with different colours presented either vertically or horizontally. Following a delay, a colour change of the central fixation dot prompted participants to select the colour-matching item from working memory to report its orientation later. After another delay, two test gratings appeared transiently and participants compared the cued memory grating to the relevant test grating (clockwise/counter-clockwise judgement) that was determined by the ‘future rule’ that was stable within each block. After a response, feedback (0: wrong, 1: correct) was presented at the side of the relevant test grating. Dash lines serve to explain the association between the encoding and test locations and were never presented in the actual experiment. (<bold>b–c</bold>) Time courses of gaze shift rates (number of saccades per second) for shifts toward and away from the encoded (panel b) and to-be-tested (panel c) locations. (<bold>d</bold>) Overlays and comparisons of the difference in gaze-shift rates (toward minus away) for the past (encoded) location and the future (to-be-tested) location. Horizontal lines indicate significant temporal clusters (cluster-based permutation test, p&lt;0.001). Data are presented as mean values with shading reflecting 95% confidence intervals, calculated across participants (n=25).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Four possible associations between encoding and test locations.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Early saccade biases by past and future memory attributes are predominantly driven by microsaccades.</title><p>Difference in gaze-shift rates toward minus away relative to past location (panel <bold>a</bold>) or future location (panel <bold>b</bold>), as a function of saccade size (y axes). For reference, dashed horizontal lines indicate 1° visual angle. Additionally, for each panel, we separately show the difference in gaze-shift rates (toward minus away) in time course at the top (collapsed over all depicted saccade sizes) and the difference in number of gaze-shift as a function of gaze-shift magnitude to the right (collapsed over all depicted times).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Trials with vertical or horizontal configurations show similar joint consideration of past and future memory attributes.</title><p>Conventions as in Main <xref ref-type="fig" rid="fig1">Figure 1b–d</xref>, separately for trials that encoded items horizontally and tested items vertically (panel <bold>a</bold>) and trials that encoded items vertically and tested items horizontally (panel <bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Gaze biases in an extended time window as a complement to <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>.</title><p>This extended analysis reveals that while the gaze bias towards the past location disappears around 600 ms after cue onset, the gaze bias towards the future location persists (panel <bold>a</bold>) and that while the early (joint) future bias occurs predominantly in the microsaccade range below 1-degree visual angle, the later bias to the future location incorporates larger eye movement that likely involve preparing for optimally perceiving the anticipated test stimulus (panel <bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig1-figsupp4-v1.tif"/></fig></fig-group><p>After a delay period, we cued the relevant memory item by changing the colour of the central fixation dot. At this stage, participants were required to select the colour-matching grating from working memory in order to compare it to the upcoming test stimulus (clockwise/counter-clockwise judgement). Crucially, we always presented two stimuli at the test-phase of which only one was relevant, as determined by the future rule. For example, in <xref ref-type="fig" rid="fig1">Figure 1a</xref>, after the green memory item is cued, the relevant test stimulus will be the right stimulus, given the future rule (top item tested on right) in this session. Note how the colour cue only ever informed the to-be-tested memory content but never directly informed the to-be-tested future location. The to-be-tested location was an attribute of the cued memory content but not of the cue itself.</p><p>Participants were able to perform this dynamic visual working-memory task, with an average accuracy of 70±2percent correct (mean ± SEM) and an average reaction time of 1218±125 ms.</p><sec id="s2-1"><title>Gaze reveals the use of both past and future memory attributes that are considered at overlapping time windows</title><p>To track the utilisation of past and/or future locations associated with working memory contents, we tracked spatial biases in gaze following the cue to select either memory item. Specifically, we focused on directional biases in saccades, that have previously been shown to be sensitive to selective spatial attention (<xref ref-type="bibr" rid="bib8">Engbert and Kliegl, 2003</xref>; <xref ref-type="bibr" rid="bib13">Hafed and Clark, 2002</xref>; <xref ref-type="bibr" rid="bib30">Rolfs, 2009</xref>; <xref ref-type="bibr" rid="bib17">Laubrock et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Yuval-Greenberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Shelchkova and Poletti, 2020</xref>; <xref ref-type="bibr" rid="bib10">Fernández et al., 2023</xref>; <xref ref-type="bibr" rid="bib20">Lowet et al., 2018</xref>), even when directed internally (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>; <xref ref-type="bibr" rid="bib38">van Ede et al., 2021</xref>; <xref ref-type="bibr" rid="bib36">van Ede et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">van Ede et al., 2020</xref>).</p><p>As shown in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, after cue onset, saccades became biased in the direction of the encoded (past) location of the selected memorized target, as demonstrated by significantly more gaze shifts toward vs. away from the encoded location of the target (<xref ref-type="fig" rid="fig1">Figure 1b</xref>; cluster p&lt;0.001), starting from around 200 ms after the cue. This is consistent with our prior demonstrations of directional eye-movement biases within the spatial layout of working memory (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>; <xref ref-type="bibr" rid="bib38">van Ede et al., 2021</xref>; <xref ref-type="bibr" rid="bib36">van Ede et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">van Ede et al., 2020</xref>). In our current task, this uniquely reveals how the retention of items in working memory continues to rely on their past encoded locations, even if items are known to become relevant (tested) at another location.</p><p>Having established that past (encoded) memory locations were still utilised by participants in our task (despite us never asking about memory-item locations), an interesting question becomes when the future memory attribute (to-be-tested location) would be considered after the selection cue. Intuitively, participants may select the relevant item at its past location, before considering the relevant future test location – which would yield a serial pattern of past-before-future. In contrast, as shown in <xref ref-type="fig" rid="fig1">Figure 1c and d</xref>, we found a similar saccade bias to the relevant future location (<xref ref-type="fig" rid="fig1">Figure 1c</xref>; cluster p&lt;0.001). Strikingly, this future bias also emerged early after the cue. An overlay of the spatial biases (toward vs. away) in the orthogonally manipulated past and future axes (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), revealed consideration of past and future locations at overlapping time windows. Gaze biases in both axes were driven predominantly by microsaccades (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) and occurred similarly in horizontal-to-vertical and vertical-to-horizontal trials (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Moreover, while the past bias was relatively transient, the future bias continued to increase in anticipation of the test stimulus and increasingly incorporated eye movements beyond the microsaccade range (see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref> for a more extended time range).</p><p>These data thus suggest the joint consideration – or ‘activation’ – of past and future memory attributes, at least when analysing past and future memory attributes separately. Below, we provide additional single-trial (single-saccade) evidence for this interpretation.</p></sec><sec id="s2-2"><title>Individual saccades reveal truly joint consideration of past and future memory attributes</title><p>In principle, the observed joint activation of the past and future locations associated with the cued memory content in the trial-averaged and participant-averaged data could result from two alternative scenarios with different interpretations. First, either the past or the future alone may be considered in different trials and/or participants, without past and future memory attributes ever being considered together. Alternatively, participants may truly consider both past and future memory attributes jointly at the single-trial, single-saccade level.</p><p>While it is notoriously hard to disentangle the single-trial interpretation of averaged data (<xref ref-type="bibr" rid="bib35">Stokes and Spaak, 2016</xref>), we were here able to do so by interrogating the individual saccade characteristics for which the two alternative scenarios make different predictions. In the first scenario, saccades should be biased to either the past or the future location, but there should be no dependency between them (i.e., a saccade may be biased to the past location regardless of the future location, and vice versa). In contrast, in the second scenario, with truly joint consideration of the past and future, there should be a clear dependency: it should be those saccades that are biased to the past that are also biased to the future. In other words, the future-biased saccades should predominantly be driven by the past-biased saccades, and vice versa.</p><p>To disentangle these alternatives at the single-trial level of individual saccades, we focused on the first saccades after the cue, in the 200–600 ms window. We previously identified these windows as the relevant window for microsaccade biases by internal selective attention (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>), and this is also where we observed the overlapping past and future biases in the average here (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). To facilitate visualisation and quantification, we rotated all detected saccades to match a consistent coordinate frame, in which the past location is represented horizontally (right = towards, left = away), and the future location vertically (top = toward, bottom = away; see <xref ref-type="fig" rid="fig2">Figure 2a</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Individual saccades are jointly biased to past and future memory attributes.</title><p>(<bold>a</bold>) The distribution of the direction of the first saccades we detected after cue onset relative to past (horizontal) and future (vertical) locations. Data from the different sessions were rotated to match a common coordinate frame. (<bold>b</bold>) The bias toward the past (x-axis) as a function of saccade direction with regard to the future (y-axis). (<bold>c</bold>) The bias toward the future (y-axis) as a function of saccade direction with regard to the past (x-axis). In (<bold>b–c</bold>), The bold black line indicates the significant temporal cluster (cluster-based permutation test, p&lt;0.001). Data are presented as mean values with shading indicating 95% confidence intervals, calculated across participants (n=25). (<bold>d</bold>) The percentage of identified first saccades toward or away from the future location as a function of whether the same saccades were also biased toward or away from the past location. Error bars in panel d indicate ±1 SEM calculated across participants (n=25).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Distribution of saccade directions relative to the future rule from encoding onset.</title><p>(<bold>Top</bold> panel) The spatial layouts in the four future rules. (<bold>Middle</bold> panel) Polar distributions of saccades during 0–1500ms after encoding onset (i.e. the period between encoding onset and cue onset). The purple quadrants represent the axis of the future rule and the gray quadrants the orthogonal axis. (<bold>Bottom</bold> panel) Time courses of saccades along the above two axes. We did not observe any sign of a bias along the axis of the future rule itself.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-fig2-figsupp1-v1.tif"/></fig></fig-group><p>As shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>, the majority of first-detected microsaccades in our window of interest were made toward the past (right &gt;left) and toward the future (top &gt;bottom), replicating our prior analyses. Critically, this visualisation and quantification enabled us to disentangle the two alternatives sketched above in which past and future saccades at the single-trial (individual-saccade) level were either independent (past bias regardless of future and future bias regardless of past) or dependent (joint past and future bias). Our data supported the latter.</p><p><xref ref-type="fig" rid="fig2">Figure 2b</xref> shows the bias toward the past as a function of saccade direction with regard to the future. Likewise, <xref ref-type="fig" rid="fig2">Figure 2c</xref>, shows the future bias as a function of whether saccades were also biased to the past. In both cases, we see a clear dependency: the past bias is particularly pronounced for saccades that also have a future bias (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and, vice versa, the future bias is most pronounced for saccades that also have a past bias (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). This is perhaps best appreciated by the binarised quantification of these same data in <xref ref-type="fig" rid="fig2">Figure 2d</xref>: showing the percentage of identified first saccades toward or away from the future location, as a function of whether the same saccades were also biased toward or away from the past location. We found a clear interaction (F(1, 24)=18.1, p&lt;0.001, partial η<sup>2</sup>=0.43), whereby the bias toward the future location was exclusively observed for those saccades that were also biased toward the past location. Indeed, follow-up t-tests revealed no difference in future bias for saccades that were away from the past (t(24) = 1.13, P<sub>Bonferroni</sub>=1, d=0.23), but a clear future effect for saccades that were toward the past (t(24)=4.65, P<sub>Bonferroni</sub>&lt;0.001, d=0.93). This would not be expected if single saccades cared exclusively about either the past or the future location. Instead, this provides single-trial-level support with the truly joint consideration of past and future memory attributes.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we brought the study of visual working memory into a dynamic context by experimentally dissociating (orthogonalising) the past (last-seen location) and future (to-be-tested location) attributes of visual memory contents, and independently tracking the utilisation of these two attributes through the gaze. Doing so, we unveil a novel, fundamental property of working memory – the joint availability and utilisation (i.e. selection) of past and future memory attributes. As such, our data provide key support for the proposal that memory is fundamentally future-oriented (<xref ref-type="bibr" rid="bib39">van Ede and Nobre, 2023</xref>; <xref ref-type="bibr" rid="bib29">Rainer et al., 1999</xref>; <xref ref-type="bibr" rid="bib23">Nobre and Stokes, 2019</xref>; <xref ref-type="bibr" rid="bib31">Schacter et al., 2007</xref>), while also reminding us that past memory attributes are not forgotten, even when these do not become relevant again.</p><p>Our finding of joint utilisation of past and future memory attributes emerged from at least two alternative scenarios of how the brain may deal with dynamic everyday working memory demands in which memory content is encoded at one location but needed at another. First, memory contents could have directly been remapped (<xref ref-type="bibr" rid="bib1">Brincat et al., 2021</xref>; <xref ref-type="bibr" rid="bib5">de Vries and van Ede, 2024</xref>; <xref ref-type="bibr" rid="bib14">Hayhoe and Ballard, 2005</xref>; <xref ref-type="bibr" rid="bib27">Pelz and Canosa, 2001</xref>) to their future-relevant location. However, in this case, one may have expected to exclusively find a future-directed gaze bias, unlike what we observed. Moreover, using a complementary analysis of saccade directions along the axis of the future rule (<xref ref-type="bibr" rid="bib5">de Vries and van Ede, 2024</xref>), we found no direct evidence for remapping in the period between encoding and cue (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Second, when dealing with multiple memory contents, contents could be stored at the past location at first and the future location could be considered only after relevant memory content has been selected (in which case the past bias should have <italic>preceded</italic> the future bias). In contrast, our data suggest that the brain simultaneously retains the copy of both past and future-relevant locations in working memory, and (re)activates each during mnemonic selection. Thus, while it is not surprising that the future location is considered (<xref ref-type="bibr" rid="bib14">Hayhoe and Ballard, 2005</xref>; <xref ref-type="bibr" rid="bib27">Pelz and Canosa, 2001</xref>; <xref ref-type="bibr" rid="bib22">Mennie et al., 2007</xref>), it is far less trivial that both past and future attributes would be retained and (re)activated together. This is our central contribution.</p><p>By capitalising on the discrete nature of saccades, we were able to demonstrate the truly joint consideration of past and future attributes, at the single-trial level. As such we were able to bypass a fundamental challenge of disentangling multiple potential single-trial interpretations when only having trial-average data available (for related discussions, see <xref ref-type="bibr" rid="bib35">Stokes and Spaak, 2016</xref>; <xref ref-type="bibr" rid="bib15">Jones, 2016</xref>; <xref ref-type="bibr" rid="bib24">Nobre and van Ede, 2020</xref>). For example, when only considering the temporally overlapping past and future signals at the trial-average level, it was impossible to tell whether these joint effects resulted from a mix of trials and/or participants that relied on either the past <italic>or</italic> the future. By considering the discrete events – the individual saccades at the single-trial level – we could demonstrate how biases to the past and the future co-existed at the single-saccade level, supporting a truly joint consideration of past and future.</p><p>In the current study, we tracked spatial attention using microsaccades (<xref ref-type="bibr" rid="bib8">Engbert and Kliegl, 2003</xref>; <xref ref-type="bibr" rid="bib13">Hafed and Clark, 2002</xref>; <xref ref-type="bibr" rid="bib30">Rolfs, 2009</xref>; <xref ref-type="bibr" rid="bib17">Laubrock et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Yuval-Greenberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Shelchkova and Poletti, 2020</xref>; <xref ref-type="bibr" rid="bib10">Fernández et al., 2023</xref>; <xref ref-type="bibr" rid="bib20">Lowet et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>; <xref ref-type="bibr" rid="bib38">van Ede et al., 2021</xref>; <xref ref-type="bibr" rid="bib36">van Ede et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">van Ede et al., 2020</xref>). Compared to the commonly used online indicator of spatial attention, such as electrophysiological measures, microsaccades have important features that make them a promising complementary tool for uncovering the mechanisms of spatial attention (<xref ref-type="bibr" rid="bib30">Rolfs, 2009</xref>; <xref ref-type="bibr" rid="bib9">Engbert, 2006</xref>), including when directed internally as we have shown here. First, as we have discussed above, microsaccades are discrete events, allowing to track spatial attention at the single-trial level. Second, microsaccades are not limited to tracking spatial attention toward the left or right visual field, as electrophysiology indicators often are. These two aspects were paramount to our demonstration of joint single-trial consideration of past and future memory attributes, and are likely to open additional doors for future investigations.</p><p>While the past gaze bias that we report here replicates our own prior studies (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>; <xref ref-type="bibr" rid="bib38">van Ede et al., 2021</xref>; <xref ref-type="bibr" rid="bib36">van Ede et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">van Ede et al., 2020</xref>), here we for the first time demonstrate a similar bias to the future-relevant memory location that is similarly driven by microsaccades. This signal may reflect either of two situations: the selection of a future copy of the cued memory content or anticipatory attention to the anticipated location of its associated test-stimulus. Either way, by the nature of our experimental design, this future signal should be considered a content-specific memory attribute for two reasons. First, the two memory contents were always associated with opposite testing locations, hence the observed bias to the relevant future location must be attributed specifically to the cued memory content. Second, we cued which memory item would become tested based on its colour, but the to-be-tested location was dependent on the item’s encoding location, regardless of its colour. Hence, consideration of the item’s future-relevant location must have been mediated by selecting the memory item itself, as it could not have proceeded via cue colour directly. Accordingly, our data reveal how this future feature can be accessed from memory together with the specific memory content that it is associated with, implying joint storage and utilisation of past and future memory attributes.</p><p>Building on the above, at face value, our task may appear like a study that simply combines two established tasks: tasks using retro-cues to study attention in working memory (e.g. <xref ref-type="bibr" rid="bib39">van Ede and Nobre, 2023</xref>; <xref ref-type="bibr" rid="bib12">Griffin and Nobre, 2003</xref>; <xref ref-type="bibr" rid="bib26">Panichello and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib34">Souza and Oberauer, 2016</xref>) and tasks using pre-cues to study the orienting of spatial attention to an upcoming external stimulus (e.g. <xref ref-type="bibr" rid="bib12">Griffin and Nobre, 2003</xref>; <xref ref-type="bibr" rid="bib26">Panichello and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib28">Posner, 1980</xref>; <xref ref-type="bibr" rid="bib41">Worden et al., 2000</xref>; <xref ref-type="bibr" rid="bib32">Schmidt et al., 2002</xref>). A critical difference with common pre-cue studies, however, is that the cue in our task never directly informed the relevant future location. Rather, as also stressed above, the future location was a feature of the cued memory item (according to the future rule), and not of the cue itself. Note how this type of scenario may not be uncommon in everyday life, such as in our opening example of a bird flying behind a building. Here too, the future relevant location is determined by the bird – i.e., the memory content – itself.</p><p>In our study, the past location of the memory items was technically irrelevant for the task and could thus, in principle, be dropped after encoding. One possibility is that participants remapped the two memory items to their future locations soon after encoding, and had started – but not finished – dropping the past location by the time the cue arrived. In such a scenario, the past signal is merely a residual trace of the memory items that serves no purpose but still pulls the gaze. Alternatively, however, the past locations may be utilised by the brain to help individuate/separate the two memory items. Moreover, by storing items with regard to multiple spatial frames (<xref ref-type="bibr" rid="bib7">Draschkow et al., 2022</xref>) – here with regard to both past and future visual locations – it is conceivable that memories may become more robust to decay and/or interference. Also, while in our task past locations were never probed, in everyday life it may be useful to remember where you last saw something before it disappeared behind an occluder. In future work, it will prove interesting to systematically vary the delay between encoding and cue to assess whether the reliance on the past location gradually dissipates with time (consistent with dropping an irrelevant feature), or whether the past trace remains preserved despite longer delays (consistent with preserving utility for working memory).</p><p>Our data complement other recent studies investigating visual working memory in more dynamic contexts (<xref ref-type="bibr" rid="bib1">Brincat et al., 2021</xref>; <xref ref-type="bibr" rid="bib38">van Ede et al., 2021</xref>; <xref ref-type="bibr" rid="bib7">Draschkow et al., 2022</xref>; <xref ref-type="bibr" rid="bib2">Chung et al., 2022</xref>; <xref ref-type="bibr" rid="bib4">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Xie et al., 2022</xref>), and showcase the rich nature of working memory representations. For example, akin to our finding of joint consideration of past and future locations in working memory, recent studies have uncovered the joint consideration of allocentric and egocentric spatial frames for working memory (<xref ref-type="bibr" rid="bib7">Draschkow et al., 2022</xref>; <xref ref-type="bibr" rid="bib11">Fiehler et al., 2014</xref>). While storing memory content at a single location (or with reference to a single frame) would appear more intuitive and efficient, our data reveal an intriguing alternative. We speculate that the joint retention of multiple spatial attributes may make memories more robust, as well as more flexible for serving continuously evolving demands during everyday behaviour.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Twenty-five healthy human volunteers participated in the study (age range: 20–27; 10 male and 15 female; 23 right-handed; 10 corrected-to-normal vision: five glasses and five lenses). Sample size of 25 was determined a-priori based on previous publications from the lab with similar experimental designs, and that relied on the same outcome measure (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>; <xref ref-type="bibr" rid="bib38">van Ede et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">van Ede et al., 2020</xref>). To achieve the intended sample size, three participants were replaced due to chance-level performance.</p></sec><sec id="s4-2"><title>Stimuli and procedure</title><p>Participants performed a visual working memory task in which we orthogonalised the encoding and to-be-tested location in order to track the utilisation of past and future working-memory attributes.</p><p>Participants were required to encode and maintain two visual items (tilted coloured gratings) in working memory to later report one of their orientations (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Each trial began with a brief (250 ms) encoding display in which two to-be-memorised gratings with different colours and orientations appeared vertically or horizontally on either side of the fixation dot, at 4degrees visual angle. After a retention delay of 1250 ms, the fixation dot changed colour for 1000 ms. This colour change served as a 100% valid retro-cue, prompting participants to select the colour-matching target memory item. The retro-cue was followed by another retention delay of 500 ms, before the test display. The test display always contained two black gratings with different orientations that were presented vertically or horizontally on either side of the fixation dot (again at four visual degrees to each side). Based on a rule that described in the following paragraph, one of the black gratings was relevant to the task (the test grating), while the other merely served as a filler. After seeing the test display, participants were required to compare the cued memory grating to the relevant test grating and report whether the memory grating should be turned clockwise (using the keyboard button ‘j’) or counterclockwise (using the keyboard button ‘f’) rotated to match the relevant test grating in the test display. After responding, feedback (‘0’ for wrong, or ‘1’ for correct) would be presented for 250 ms at the side of the relevant test grating, indicating the correctness of the response and reinforcing the future rule.</p><p>In the encoding display, the gratings were randomly assigned two distinct colours: green (RGB: 133, 194, 18) and purple (RGB: 197, 21, 234) and two distinct orientations ranging from 0° to 180° with a minimum difference of 20° between each other. During the test display, the gratings are always black (RGB: 64, 64, 64). The relevant grating was always rotated 20degrees in either a clockwise or counter-clockwise direction compared to the to-be-tested memory grating. The orientation of the irrelevant grating in the test display was chosen randomly.</p><p>The unique element of our task was that we dissociated the encoding and testing location by presenting the gratings in the test display on the orthogonal axis as where the items were presented on the encoding display. Relevant examples can be found in <xref ref-type="fig" rid="fig1">Figure 1a</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. For example, if the two memory items appeared vertically at the top and bottom at encoding, the test gratings would appear horizontally to the left and right in the test display (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><p>To to-be-tested location was always linked to the encoded location by virtue of a future rule. For counterbalancing reasons, we used four unique rules that were presented across four sessions (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) and that always remained stable within a session. In rule 1: the two memory items were encoded vertically on the top and bottom. If the top memory item was cued, the relevant test grating would be on the right, while if the bottom item would be cued, the relevant test grating would be on the left in the test display. In rule 2, the mapping was reversed: the two memory items would again be encoded vertically, but this time if the top memory item would be cued the relevant test grating is on the left, while if the bottom item is cued the relevant test grating is on the right. Rules 3 and 4 follow the same logic, except that now the memory items are presented horizontally, and the test gratings vertically. Before every session, participants were notified of the future rule that applied to the upcoming session.</p><p>To ensure the use of the future rule, we made it task-relevant by always presenting two test gratings in the test display of which only one was relevant: the one that matched the future rule (i.e. without applying the future rule, one would not know which test grating was to be used). It is for this reason that we consider the future location an attribute of the memory items, as each memory item was associated with its own unique test location. Note also how our colour cue never directly informed about the future rule or the to-be-tested location. The cue informed which memory item would become tested based on its colour, but the to-be-tested location was dependent on the cued item’s encoding location, regardless of its colour (with item colour and location varying randomly across trials). Accordingly, consideration of the item’s future-relevant location must have been mediated by selecting the memory item itself, as it could not proceed via the colour cue directly.</p><p>In total, the study consisted of four sessions, each contained five blocks of 32 trials each. At the start of each session, participants were notified of the session-specific future rule and then practiced the task with the current rule for 16 trials before starting the formal session. We did not include practice trials in our analyses. The study lasted approximately 70min per participant.</p></sec><sec id="s4-3"><title>Eye-tracking acquisition and pre-processing</title><p>Gaze was tracked from a single eye (right eye in all participants except one for which the left eye provided a better signal) using an EyeLink 1000 (SR Research) at a sampling rate of 1000Hz. The eye tracker camera was positioned on the table ∼5cm in front of the monitor and ∼65cm away from the eyes. Gaze position was tracked continuously along the horizontal and vertical axes. Before recording, the built-in calibration and validation protocols from the EyeLink software were used to calibrate the eye tracker.</p><p>After recording, the eye-tracking data were converted from the original.edf to the .asc format and analysed in Matlab using the Fieldtrip analysis toolbox (<xref ref-type="bibr" rid="bib25">Oostenveld et al., 2011</xref>) in combination with custom code. Blinks were marked by detecting 0 clusters in the eye-tracking data. All data from 100 ms before to 100 ms after the detected 0 clusters were set to NaN to eliminate residual blink artefacts. Finally, data were epoched from –1000 to+2000 ms relative to after the onset of the retro-cue.</p></sec><sec id="s4-4"><title>Gaze-shift detection</title><p>We focused our analysis on spatial biases in gaze shifts (saccades/microsaccades). To identify gaze shifts, we employed a velocity-based method that we established in our prior studies (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>; <xref ref-type="bibr" rid="bib19">Liu et al., 2022b</xref>), that builds on other velocity-based detection methods (e.g., <xref ref-type="bibr" rid="bib8">Engbert and Kliegl, 2003</xref>). First, gaze velocity was calculated by taking the Euclidean distance between temporally successive gaze-position values in the two-dimensional plane (horizontal and vertical gaze position). Velocity was smoothed with a Gaussian-weighted moving average filter with a 7ms sliding window (using the built-in function ‘smoothdata’ in MATLAB). When the velocity exceeded a trial-based threshold of five times the median velocity, we marked the first sample after the threshold crossing as the onset of a saccade. To avoid counting the same saccade multiple times, a minimum delay of 100 ms between successive saccades was imposed. Saccade magnitude and direction were calculated by estimating the difference between pre-saccade gaze position (−50–0 ms before saccade onset) vs. the post-saccade gaze position (50–100 ms after saccade onset).</p><p>To focus our analysis on (micro)saccades that were driven by attention, we here focused on the ‘start microsaccade’ defined as the saccade that moves the gaze away from the fixation dot (as opposed to saccades that bring the gaze back to fixation). We extracted start saccades using a method that we recently validated in another study (<xref ref-type="bibr" rid="bib19">Liu et al., 2022b</xref>). For each detected saccade, we estimated the pre- and post-saccade distance from the central fixation dot. If the post-saccade distance was larger than the pre-saccade distance, we defined the saccade as a start microsaccade. To minimise the contribution of gaze drift, we defined gaze positions associated with looking at the central fixation dot using the median gaze position in the fixation period from [-0.8 to –0.2 ms] relative to cue onset.</p><p>Time courses of gaze-shift rates (in Hz) were quantified using a sliding time window of 50 ms, advanced in steps of 1 ms. We additionally decomposed shift rates into a time-magnitude representation (as in <xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>), showing the time-resolved rate of attention-driven shifts (toward vs. away) per second, as a function of the saccade size (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). For magnitude sorting, we used successive magnitude bins of 0.2 visual degrees in steps of 0.04 visual degrees.</p></sec><sec id="s4-5"><title>Individual-shift level analysis</title><p>For our analysis at the single-trial, single-saccade level, we focused on the first start saccade observed during the 200–600 ms post-cue period that we have previously identified as the relevant time window for the effect of interest (<xref ref-type="bibr" rid="bib18">Liu et al., 2022a</xref>). For all ‘first saccades,’ we then looked at the spatial distribution of saccade directions relative to the relevant past (encoding) and future (to-be-tested) locations associated with the cued memory item. To facilitate visualisation and quantification, we rotated all detected saccades to a common coordinate system, in which the past location was represented horizontally (left=away, right=toward), and the future location vertically (top=toward, bottom = away).</p><p>To obtain the proportional distribution of saccades in this coordinate system (see <xref ref-type="fig" rid="fig2">Figure 2a</xref>), we used successive angular bins of 20degrees in steps of 1 angle degree and simply calculated the proportion of saccades in each angular bin (with angles being defined with reference to past and future locations).</p><p>To quantify whether saccades were jointly biased to the past and future (as opposed to either alone), we decomposed these distributions into the bias toward the past (toward vs. away from past) as a function of saccade direction with regard to the future (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and the bias toward the future (toward vs. away from future) as a function of saccade direction with regard to the past (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Finally, we binarised saccades as either going toward or away from the past and toward or away from the future (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). This enabled us to test whether saccades were jointly biased to the past and the future (which would predict an interaction whereby those same saccades that were biased to the past were also biased to the future).</p></sec><sec id="s4-6"><title>Statistical analysis</title><p>To evaluate the reliability of patterns in our gaze data, we used a cluster-based permutation approach (<xref ref-type="bibr" rid="bib21">Maris and Oostenveld, 2007</xref>). This method is ideal for evaluating patterns at multiple neighbouring points while circumventing the problem of multiple comparisons. We used this approach for all evaluations involving a series of data, such as along a time axis (<xref ref-type="fig" rid="fig1">Figure 1b–d</xref>; <xref ref-type="fig" rid="fig2">Figure 2b–c</xref>).</p><p>To create a permutation distribution, we randomly permuted the trial-average data at the participant level 10,000 times and identified the largest clusters found at each time. The p-values of the clusters in the original data were calculated as the proportion of permutations for which the size of the largest cluster after permutation was larger than the size of the observed cluster in the original, non-permuted data. We created the permutation distribution using Fieldtrip with default cluster settings (grouping adjacent same-signed data points that were significant in a mass univariate t-test at a two-sided alpha level of 0.05 and defining cluster size as the sum of all t-values in a cluster).</p><p>In addition, statistical evaluation for the single-trial, single-saccade analysis was performed on the binarised quantification of saccade proportions using a two-way repeated-measures ANOVA with the factors past (toward/away) and future (toward/away). ANOVA results were complemented with Bonferroni-corrected post-hoc t-tests. For measures of effect size, we used Partial eta squared (for our ANOVA) and Cohen’s d (for follow-up t-tests). p-values of follow-up t-tests were Bonferroni corrected.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Supervision, Validation, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation, Visualization</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Formal analysis, Supervision, Investigation, Visualization, Writing – original draft, Writing – review and editing, Funding acquisition, Methodology</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Experimental procedures were reviewed and approved by the local Ethics Committee at the Vrije Universiteit Amsterdam (reference code: VCWE-2020-155). Each participant provided written consent before participation and was reimbursed €10/hr.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-90874-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code availability. The corresponding data and code can be found at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/xukzs/">https://osf.io/xukzs/</ext-link> at OSF (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/XUKZS">https://doi.org/10.17605/OSF.IO/XUKZS</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Alexopoulou</surname><given-names>Z</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Jointly looking to the past and the future in visual working memory</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/XUKZS</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by an ERC Starting Grant from the European Research Council (MEMTICIPATION, 850636) and an NWO Vidi grant by the Dutch Research Council (grant number 14721) to F.v.E.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname><given-names>SL</given-names></name><name><surname>Donoghue</surname><given-names>JA</given-names></name><name><surname>Mahnke</surname><given-names>MK</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Interhemispheric transfer of working memories</article-title><source>Neuron</source><volume>109</volume><fpage>1055</fpage><lpage>1066</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.01.016</pub-id><pub-id pub-id-type="pmid">33561399</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>YH</given-names></name><name><surname>Schurgin</surname><given-names>M</given-names></name><name><surname>Brady</surname><given-names>TF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The role of motion in visual working memory for dynamic stimuli: More lagged but more precise representations of moving objects</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/cu3zg</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Esposito</surname><given-names>M</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cognitive neuroscience of working memory</article-title><source>Annual Review of Psychology</source><volume>66</volume><fpage>115</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010814-015031</pub-id><pub-id pub-id-type="pmid">25251486</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>IEJ</given-names></name><name><surname>Slagter</surname><given-names>HA</given-names></name><name><surname>Olivers</surname><given-names>CNL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Oscillatory control over representational states in working memory</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>150</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.11.006</pub-id><pub-id pub-id-type="pmid">31791896</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>E</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Microsaccades track location-based object rehearsal in visual working memory</article-title><source>eNeuro</source><volume>11</volume><elocation-id>ENEURO.0276-23.2023</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0276-23.2023</pub-id><pub-id pub-id-type="pmid">38176905</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doherty</surname><given-names>JR</given-names></name><name><surname>Rao</surname><given-names>A</given-names></name><name><surname>Mesulam</surname><given-names>MM</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Synergistic effect of combined temporal and spatial expectations on visual attention</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>8259</fpage><lpage>8266</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1821-05.2005</pub-id><pub-id pub-id-type="pmid">16148233</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multiple spatial frames for immersive working memory</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>536</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01245-y</pub-id><pub-id pub-id-type="pmid">35058640</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Microsaccades uncover the orientation of covert attention</article-title><source>Vision Research</source><volume>43</volume><fpage>1035</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(03)00084-1</pub-id><pub-id pub-id-type="pmid">12676246</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Microsaccades: a microcosm for research on oculomotor control, attention, and visual perception</article-title><source>Progress in Brain Research</source><volume>154</volume><fpage>177</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)54009-9</pub-id><pub-id pub-id-type="pmid">17010710</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernández</surname><given-names>A</given-names></name><name><surname>Hanning</surname><given-names>NM</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Transcranial magnetic stimulation to frontal but not occipital cortex disrupts endogenous attention</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2219635120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2219635120</pub-id><pub-id pub-id-type="pmid">36853947</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiehler</surname><given-names>K</given-names></name><name><surname>Wolf</surname><given-names>C</given-names></name><name><surname>Klinghammer</surname><given-names>M</given-names></name><name><surname>Blohm</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Integration of egocentric and allocentric information during memory-guided reaching to images of a natural environment</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>636</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00636</pub-id><pub-id pub-id-type="pmid">25202252</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffin</surname><given-names>IC</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Orienting attention to locations in internal representations</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>1176</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1162/089892903322598139</pub-id><pub-id pub-id-type="pmid">14709235</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafed</surname><given-names>ZM</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Microsaccades as an overt measure of covert attention shifts</article-title><source>Vision Research</source><volume>42</volume><fpage>2533</fpage><lpage>2545</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(02)00263-8</pub-id><pub-id pub-id-type="pmid">12445847</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>M</given-names></name><name><surname>Ballard</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye movements in natural behavior</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.02.009</pub-id><pub-id pub-id-type="pmid">15808501</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When brain rhythms aren’t “rhythmic”: implication for their mechanisms and meaning</article-title><source>Current Opinion in Neurobiology</source><volume>40</volume><fpage>72</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.06.010</pub-id><pub-id pub-id-type="pmid">27400290</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Treisman</surname><given-names>A</given-names></name><name><surname>Gibbs</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The reviewing of object files: object-specific integration of information</article-title><source>Cognitive Psychology</source><volume>24</volume><fpage>175</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(92)90007-o</pub-id><pub-id pub-id-type="pmid">1582172</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laubrock</surname><given-names>J</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Engbert</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>When do microsaccades follow spatial attention?</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>683</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.3758/APP.72.3.683</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Functional but not obligatory link between microsaccades and neural modulation by covert spatial attention</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>3503</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-31217-3</pub-id><pub-id pub-id-type="pmid">35715471</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Microsaccades Transiently Lateralise EEG Alpha Activity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.09.02.506318</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowet</surname><given-names>E</given-names></name><name><surname>Gomes</surname><given-names>B</given-names></name><name><surname>Srinivasan</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Schafer</surname><given-names>RJ</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Enhanced neural processing by covert attention only during microsaccades directed toward the attended stimulus</article-title><source>Neuron</source><volume>99</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.041</pub-id><pub-id pub-id-type="pmid">29937279</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mennie</surname><given-names>N</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name><name><surname>Sullivan</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Look-ahead fixations: anticipatory eye movements in natural tasks</article-title><source>Experimental Brain Research</source><volume>179</volume><fpage>427</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1007/s00221-006-0804-0</pub-id><pub-id pub-id-type="pmid">17171337</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Premembering experience: a hierarchy of time-scales for proactive attention</article-title><source>Neuron</source><volume>104</volume><fpage>132</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.08.030</pub-id><pub-id pub-id-type="pmid">31600510</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Under the mind’s hood: what we have learned by watching the brain at work</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>89</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0742-19.2019</pub-id><pub-id pub-id-type="pmid">31630115</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Fieldtrip: open source software for advanced analysis of meg, eeg, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shared mechanisms underlie the control of working memory and attention</article-title><source>Nature</source><volume>592</volume><fpage>601</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id><pub-id pub-id-type="pmid">33790467</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelz</surname><given-names>JB</given-names></name><name><surname>Canosa</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Oculomotor behavior and perceptual strategies in complex tasks</article-title><source>Vision Research</source><volume>41</volume><fpage>3587</fpage><lpage>3596</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00245-0</pub-id><pub-id pub-id-type="pmid">11718797</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Orienting of attention</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>32</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/00335558008248231</pub-id><pub-id pub-id-type="pmid">7367577</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Rao</surname><given-names>SC</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Prospective coding for objects in primate prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>5493</fpage><lpage>5505</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-13-05493.1999</pub-id><pub-id pub-id-type="pmid">10377358</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Microsaccades: small steps on a long way</article-title><source>Vision Research</source><volume>49</volume><fpage>2415</fpage><lpage>2441</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.08.010</pub-id><pub-id pub-id-type="pmid">19683016</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remembering the past to imagine the future: the prospective brain</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>657</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1038/nrn2213</pub-id><pub-id pub-id-type="pmid">17700624</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>BK</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Woodman</surname><given-names>GF</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Voluntazy and automatic attentional control of visual working memory</article-title><source>Perception &amp; Psychophysics</source><volume>64</volume><fpage>754</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.3758/bf03194742</pub-id><pub-id pub-id-type="pmid">12201334</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shelchkova</surname><given-names>N</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modulations of foveal vision associated with microsaccade preparation</article-title><source>PNAS</source><volume>117</volume><fpage>11178</fpage><lpage>11183</lpage><pub-id pub-id-type="doi">10.1073/pnas.1919832117</pub-id><pub-id pub-id-type="pmid">32358186</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souza</surname><given-names>AS</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>In search of the focus of attention in working memory: 13 years of the retro-cue effect</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>78</volume><fpage>1839</fpage><lpage>1860</lpage><pub-id pub-id-type="doi">10.3758/s13414-016-1108-5</pub-id><pub-id pub-id-type="pmid">27098647</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>M</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The importance of single-trial analyses in cognitive neuroscience</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>483</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.05.008</pub-id><pub-id pub-id-type="pmid">27237797</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Chekroud</surname><given-names>SR</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human gaze tracks attentional focusing in memorized visual space</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>462</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0549-y</pub-id><pub-id pub-id-type="pmid">31089296</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Board</surname><given-names>AG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Goal-directed and stimulus-driven selection of internal representations</article-title><source>PNAS</source><volume>117</volume><fpage>24590</fpage><lpage>24598</lpage><pub-id pub-id-type="doi">10.1073/pnas.2013432117</pub-id><pub-id pub-id-type="pmid">32929036</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Deden</surname><given-names>J</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Looking ahead in working memory to guide sequential behaviour</article-title><source>Current Biology</source><volume>31</volume><fpage>R779</fpage><lpage>R780</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.04.063</pub-id><pub-id pub-id-type="pmid">34157258</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Turning attention inside out: how working memory serves behavior</article-title><source>Annual Review of Psychology</source><volume>74</volume><fpage>137</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-021422-041757</pub-id><pub-id pub-id-type="pmid">35961038</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodman</surname><given-names>GF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Flexibility in visual working memory: Accurate change detection in the face of irrelevant variations in position</article-title><source>Visual Cognition</source><volume>20</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1080/13506285.2011.630694</pub-id><pub-id pub-id-type="pmid">22287933</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worden</surname><given-names>MS</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><name><surname>Simpson</surname><given-names>GV</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Anticipatory biasing of visuospatial attention indexed by retinotopically specific alpha-band electroencephalography increases over occipital cortex</article-title><source>The Journal of Neuroscience</source><volume>20</volume><elocation-id>RC63</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-j0002.2000</pub-id><pub-id pub-id-type="pmid">10704517</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Min</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title><source>Science</source><volume>375</volume><fpage>632</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1126/science.abm0204</pub-id><pub-id pub-id-type="pmid">35143322</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuval-Greenberg</surname><given-names>S</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spontaneous microsaccades reflect shifts in covert attention</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>13693</fpage><lpage>13700</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0582-14.2014</pub-id><pub-id pub-id-type="pmid">25297096</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaksas</surname><given-names>D</given-names></name><name><surname>Bisley</surname><given-names>JW</given-names></name><name><surname>Pasternak</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Motion information is spatially localized in a visual working-memory task</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>912</fpage><lpage>921</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.2.912</pub-id><pub-id pub-id-type="pmid">11495960</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90874.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Spering</surname><given-names>Miriam</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>The University of British Columbia</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study advances our understanding of how past and future information is jointly considered in visual working memory by studying gaze biases in a memory task that dissociates the locations during encoding and memory tests. The evidence supporting the conclusions is <bold>convincing</bold>, with state-of-the-art gaze analyses that build on a recent series of experiments introduced by the authors. This work will be of broad interest to vision scientists interested in the interplay of vision, eye movements, and memory.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90874.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this study, the authors offer a fresh perspective on how visual working memory operates. They delve into the link between anticipating future events and retaining previous visual information in memory. To achieve this, the authors build upon their recent series of experiments that investigated the interplay between gaze biases and visual working memory. In this study, they introduce an innovative twist to their fundamental task. Specifically, they disentangle the location where information is initially stored from the location where it will be tested in the future. Participants are tasked with learning a novel rule that dictates how the initial storage location relates to the eventual test location. The authors leverage participants' gaze patterns as an indicator of memory selection. Intriguingly, they observe that microsaccades are directed towards both the past encoding location and the anticipated future test location. This observation is noteworthy for several reasons. Firstly, participants' gaze is biased towards the past encoding location, even though that location lacks relevance to the memory test. Secondly, there's a simultaneous occurrence of an increased gaze bias towards both the past and future locations. To explore this temporal aspect further, the authors conduct a compelling analysis that reveals the joint consideration of past and future locations during memory maintenance. Notably, microsaccades biased towards the future test location also exhibit a bias towards the past encoding location. In summary, the authors present an innovative perspective on the adaptable nature of visual working memory. They illustrate how information relevant to the future is integrated with past information to guide behavior.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90874.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The manuscript by Liu et al. reports a task that is designed to examine the extent to which &quot;past&quot; and &quot;future&quot; information is encoded in working memory that combines a retrocue with rules that indicate the location of an upcoming test probe. An analysis of microsaccades on a fine temporal scale shows the extent to which shifts of attention track the location of the encoded item (past) and the location of the future item (test probe). The location of the encoded grating and test probe were always on orthogonal axes (horizontal, vertical) so that biases in microsaccades could be used to track shifts of attention to one or the other axis (or mixtures of the two). The overall goal here was then to (1) create a methodology that could tease apart memory for the past and future, respectively, (2) to look at the time-course attention to past/future, and (3) to test the extent to which microsaccades might jointly encode past and future memoranda. Finally, some remarks are made about the plausibility of various accounts of working memory encoding/maintenance based on the examination of these time-courses.</p><p>Strengths:</p><p>This research has several notable strengths. It has a clear statement of its aims, is lucidly presented, and uses a clever experimental design that neatly orthogonalized &quot;past&quot; and &quot;future&quot; as operationalized by the authors. Figure 1b-d shows fairly clearly that saccade directions have an early peak (around 300ms) for the past and a &quot;ramping&quot; up of saccades moving in the forward direction. This seems to be a nice demonstration that the method can measure shifts of attention at a fine temporal resolution and differentiate past from future oriented saccades due to the orthogonal cue approach. The second analysis shown in Figure 2, reveals a dependency in saccade direction such that saccades toward the probe future were more likely also to be toward the encoded location than away from the encoded direction. This suggests saccades are jointly biased by both locations &quot;in memory&quot;. The &quot;central contribution&quot; (as the authors characterize it) is that &quot;the brain simultaneously retains the copy of both past and future-relevant locations in working memory, and (re)activates each during mnemonic selection&quot;, and that: &quot;... while it is not surprising that the future location is considered, it is far less trivial that both past and future attributes would be retained and (re)activated together. This is our central contribution.&quot; The authors provide a nuanced analysis that offers persuasive evidence that past and future representations are jointly maintained in memory.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90874.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Baiwei</given-names></name><role specific-use="author">Author</role><aff><institution>VU Amsterdam</institution><addr-line><named-content content-type="city">amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Alexopoulou</surname><given-names>Zampeta-Sofia</given-names></name><role specific-use="author">Author</role><aff><institution>VU Amsterdam</institution><addr-line><named-content content-type="city">amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>van Ede</surname><given-names>Freek</given-names></name><role specific-use="author">Author</role><aff><institution>Vrije Universiteit Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This important study advances our understanding of how past and future information is jointly considered in visual working memory by studying gaze biases in a memory task that dissociates the locations during encoding and memory tests. The evidence supporting the conclusions is convincing, with state-of-the-art gaze analyses that build on a recent series of experiments introduced by the authors. This work, with further improvements incorporating the existing literature, will be of broad interest to vision scientists interested in the interplay of vision, eye movements, and memory.</p></disp-quote><p>We thank the Editors and the Reviewers for their enthusiasm and appreciation of our task, our findings, and our article. We also wish to thank the Reviewers for their constructive comments that we have embraced to improve our article. Please find below our point-by-point responses to this valuable feedback, where we also state relevant revisions that we have made to our article.</p><p>In addition, please note that we have now also made our data and code publicly available.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 1, Comments:</bold></p><p>In this study, the authors offer a fresh perspective on how visual working memory operates. They delve into the link between anticipating future events and retaining previous visual information in memory. To achieve this, the authors build upon their recent series of experiments that investigated the interplay between gaze biases and visual working memory. In this study, they introduce an innovative twist to their fundamental task. Specifically, they disentangle the location where information is initially stored from the location where it will be tested in the future. Participants are tasked with learning a novel rule that dictates how the initial storage location relates to the eventual test location. The authors leverage participants' gaze patterns as an indicator of memory selection. Intriguingly, they observe that microsaccades are directed toward both the past encoding location and the anticipated future test location. This observation is noteworthy for several reasons. Firstly, participants' gaze is biased towards the past encoding location, even though that location lacks relevance to the memory test. Secondly, there's a simultaneous occurrence of an increased gaze bias towards both the past and future locations. To explore this temporal aspect further, the authors conduct a compelling analysis that reveals the joint consideration of past and future locations during memory maintenance. Notably, microsaccades biased towards the future test location also exhibit a bias towards the past encoding location. In summary, the authors present an innovative perspective on the adaptable nature of visual working memory. They illustrate how information relevant to the future is integrated with past information to guide behavior.</p></disp-quote><p>Thank you for your enthusiasm for our article and findings as well as for your constructive suggestions for additional analyses that we respond to in detail below.</p><disp-quote content-type="editor-comment"><p>This short manuscript presents one experiment with straightforward analyses, clear visualizations, and a convincing interpretation. For their analysis, the authors focus on a single time window in the experimental trial (i.e., 0-1000 ms after retro cue onset). While this time window is most straightforward for the purpose of their study, other time windows are similarly interesting for characterizing the joint consideration of past and future information in memory. First, assessing the gaze biases in the delay period following the cue offset would allow the authors to determine whether the gaze bias towards the future location is sustained throughout the entire interval before the memory test onset. Presumably, the gaze bias towards the past location may not resurface during this delay period, but it is unclear how the bias towards the future location develops in that time window. Also, the disappearance of the retro cue constitutes a visual transient that may leave traces on the gaze biases which speaks again for assessing gaze biases also in the delay period following the cue offset.</p></disp-quote><p>Thank you for raising this important point. We initially focused on the time window during the cue given that our central focus was on gaze-biases associated with mnemonic item selection. By zooming in on this window, we could best visualize our main effects of interest: the joint selection (in time) of past and future memory attributes.</p><p>At the same time, we fully agree that examining the gaze biases over a more extended time window yields a more comprehensive view of our data. To this end, we have now also extended our analysis to include a wider time range that includes the period between cue offset (1000 ms after cue onset) and test onset (1500 ms after cue onset). We present these data below. Because we believe our future readers are likely to be interested in this as well, we have now added this complementary visualization as Supplementary Figure 4 (while preserving the focus in our main figure on the critical mnemonic selection period of interest).</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Supplementary Figure 4.</title><p>Gaze biases in extended time window as a complement to Figure 1 and Supplementary Figure 2. This extended analysis reveals that while the gaze bias towards the past location disappears around 600 ms after cue onset, the gaze bias towards the future location persists (panel a) and that while the early (joint) future bias occurs predominantly in the microsaccade range below 1 degree visual angle, the later bias to the future location incorporates larger eye movement that likely involve preparing for optimally perceiving the anticipated test stimulus (panel b).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-sa3-fig1-v1.tif"/></fig><p>This extended analysis reveals that while the gaze bias towards the past location disappears around 600 ms after cue onset (consistent with our prior reports of this bias), the gaze bias towards the future location persists. Moreover, as revealed by the data in panel b above, while the early (joint) future bias occurs predominantly in the microsaccade range below 1 degree visual angle, the later bias to the future location incorporates larger eye movement that likely involve preparing for optimally perceiving the anticipated test stimulus.</p><p>We now also call out these additional findings and figure in our article:</p><p>Page 2 (Results): “Gaze biases in both axes were driven predominantly by microsaccades (Supplementary Fig. 2) and occurred similarly in horizontal-to-vertical and vertical-tohorizontal trials (Supplementary Fig. 3). Moreover, while the past bias was relatively transient, the future bias continued to increase in anticipation of the of the test stimulus and increasingly incorporated eye-movements beyond the microsaccade range (see Supplementary Fig. 4 for a more extended time range)”.</p><disp-quote content-type="editor-comment"><p>Moreover, assessing the gaze bias before retro-cue onset allows the authors to further characterize the observed gaze biases in their study. More specifically, the authors could determine whether the future location is considered already during memory encoding and the subsequent delay period (i.e., before the onset of the retro cue). In a trial, participants encode two oriented gratings presented at opposite locations. The future rule indicates the test locations relative to the encoding locations. In their example (Figure 1a), the test locations are shifted clockwise relative to the encoding location. Thus, there are two pairs of relevant locations (each pair consists of one stimulus location and one potential test location) facing each other at opposite locations and therefore forming an axis (in the illustration the axis would go from bottom left to top right). As the future rule is already known to the participants before trial onset it is possible that participants use that information already during encoding. This could be tested by assessing whether more microsaccades are directed along the relevant axis as compared to the orthogonal axis. The authors should assess whether such a gaze bias exists already before retro cue onset and discuss the theoretical consequences for their main conclusions (e.g., is the future location only jointly used if the test location is implicitly revealed by the retro cue).</p></disp-quote><p>Thank you – this is another interesting point. We fully agree that additional analysis looking at the period prior to retrocue onset may also prove informative. In accordance with the suggested analysis, we have therefore now also analysed the distribution of saccade directions (including in the period from encoding to retrocue) as a function of the future rule (presented below, and now also included as Supplementary Fig. 5). Complementary recent work from our lab has shown how microsaccade directions can align to the axis of memory contents during retention (see de Vries &amp; van Ede, eNeuro, 2024). Based on this finding, one may predict that if participants retain the items in a remapped fashion, their microsaccades may align with the axis of the future rule, and this could potentially already happen prior to cue onset.</p><p>These complementary analyses show that saccade directions are predominantly influenced by the encoding locations rather than the test locations, as seen most clearly by the saccade distribution plots in the middle row of the figure below. To obtain time-courses, we categorized saccades as occurring along the axis of the future rule or along the orthogonal axis (bottom row of the figure below). Like the distribution plots, these time course plots also did not reveal any sign of a bias along the axis of the future rule itself.</p><p>Importantly, note how this does not argue against our main findings of joint selection of past and future memory attributes, as for that central analysis we focused on saccade biases that were specific to the selected memory item, whereas the analyses we present below focus on biases in the axes in which both memory items are defined; not only the cued/selected memory item.</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><caption><title>Supplementary Figure 5.</title><p>Distribution of saccade directions relative to the future rule from encoding onset. (Top panel) The spatial layouts in the four future rules. (Middle panel) Polar distributions of saccades during 0 to 1500 ms after encoding onset (i.e. the period between encoding onset and cue onset). The purple quadrants represent the axis of the future rule and the gray quadrants the orthogonal axis. (Bottom panel) Time courses of saccades along the above two axes. We did not observe any sign of a bias along the axis of the future rule itself.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90874-sa3-fig2-v1.tif"/></fig><p>We agree that these additional results are important to bring forward when we interpret our findings. Accordingly, we now mention these findings at the relevant section in our Discussion:</p><p>Page 5 (Discussion): “First, memory contents could have directly been remapped (cf. 4,24–26) to their future-relevant location. However, in this case, one may have expected to exclusively find a future-directed gaze bias, unlike what we observed. Moreover, using a complementary analysis of saccade directions along the axis of the future rule (cf. 24), we found no direct evidence for remapping in the period between encoding and cue (Supplementary Fig. 5)”.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 2, Comments:</bold></p><p>The manuscript by Liu et al. reports a task that is designed to examine the extent to which &quot;past&quot; and &quot;future&quot; information is encoded in working memory that combines a retro cue with rules that indicate the location of an upcoming test probe. An analysis of microsaccades on a fine temporal scale shows the extent to which shifts of attention track the location of the location of the encoded item (past) and the location of the future item (test probe). The location of the encoded grating of the test probe was always on orthogonal axes (horizontal, vertical) so that biases in microsaccades could be used to track shifts of attention to one or the other axis (or mixtures of the two). The overall goal here was then to (1) create a methodology that could tease apart memory for the past and future, respectively, (2) to look at the time-course attention to past/future, and (3) to test the extent to which microsaccades might jointly encode past and future memoranda. Finally, some remarks are made about the plausibility of various accounts of working memory encoding/maintenance based on the examination of these time courses.</p><p>Strengths:</p><p>This research has several notable strengths. It has a clear statement of its aims, is lucidly presented, and uses a clever experimental design that neatly orthogonalizes &quot;past&quot; and &quot;future&quot; as operationalized by the authors. Figure 1b-d shows fairly clearly that saccade directions have an early peak (around 300ms) for the past and a &quot;ramping&quot; up of saccades moving in the forward direction. This seems to be a nice demonstration the method can measure shifts of attention at a fine temporal resolution and differentiate past from future-oriented saccades due to the orthogonal cue approach. The second analysis shown in Figure 2, reveals a dependency in saccade direction such that saccades toward the probe future were more likely also to be toward the encoded location than away from the encoded direction. This suggests saccades are jointly biased by both locations &quot;in memory&quot;.</p></disp-quote><p>Thank you for your overall appreciation of our work and for highlighting the above strengths. We also thank you for your constructive comments and call for clarifications that we respond to below.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) The &quot;central contribution&quot; (as the authors characterize it) is that &quot;the brain simultaneously retains the copy of both past and future-relevant locations in working memory, and (re)activates each during mnemonic selection&quot;, and that: &quot;... while it is not surprising that the future location is considered, it is far less trivial that both past and future attributes would be retained and (re)activated together. This is our central contribution.&quot; However, to succeed at the task, participants must retain the content (grating orientation, past) and probe location (future) in working memory during the delay period. It is true that the location of the grating is functionally irrelevant once the cue is shown, but if we assume that features of a visual object are bound in memory, it is not surprising that location information of the encoded object would bias processing as indicated by microsaccades. Here the authors claim that joint representation of past and future is &quot;far less trivial&quot;, this needs to be evaluaed from the standpoint of prior empirical data on memory decay in such circumstances, or some reference to the time-course of the &quot;unbinding&quot; of features in an encoded object.</p></disp-quote><p>Thank you. We agree that our participants have to use the future rule – as otherwise they do not know to which test stimulus they should respond. This was a deliberate decision when designing the task. Critically, however, this does not require (nor imply) that participants have to incorporate and apply the rule to both memory items already prior to the selection cue. It is at least as conceivable that participants would initially retain the two items at their encoded (past) locations, then wait for the cue to select the target memory item, and only then consider the future location associated with the target memory item. After all, in every trial, there is only 1 relevant future location: the one associated with the cued memory item. The time-resolved nature of our gaze markers argues against such a scenario, by virtue of our observation of the joint (simultaneous) consideration of past and future memory attributes (as opposed to selection of past-before-future). These temporal dynamics are central to the insights provided by our study.</p><p>In our view, it is thus not obvious that the rule would be applied at encoding. In this sense, we do not assume that the future location is part of both memory objects from encoding, but rather ask whether this is the case – and, if so, whether the future location takes over the role of the past location, or whether past and future locations are retained jointly.</p><p>Our statements regarding what is “trivial” and what is “less trivial” regard exactly this point: it is trivial that the future is considered (after all, our task demanded it). However, it is less trivial that (1) the future location was already available at the time of initial item selection (as reflected in the simultaneous engagement of past and future locations), and (2) that in presence of the future location, the past location was still also present in the observed gaze biases.</p><p>Having said that, we agree that an interesting possibility is that participants remap both memory items to their future-relevant locations ahead of the cue, but that the past location is not yet fully “unbound” by the time of the cue. This may trigger a gaze bias not only to the new future location but also to the “sticky” (unbound) past location. We now acknowledge this possibility in our discussion (also in response to comment 3 below) where we also suggest how future work may be able to tap into this:</p><p>Page 6 (Discussion): “In our study, the past location of the memory items was technically irrelevant for the task and could thus, in principle, be dropped after encoding. One possibility is that participants remapped the two memory items to their future locations soon after encoding, and had started – but not finished – dropping the past location by the time the cue arrived. In such a scenario, the past signal is merely a residual trace of the memory items that serves no purpose but still pulls gaze. Alternatively, however, the past locations may be utilised by the brain to help individuate/separate the two memory items. Moreover, by storing items with regard to multiple spatial frames (cf. 37) – here with regard to both past and future visual locations – it is conceivable that memories may become more robust to decay and/or interference. Also, while in our task past locations were never probed, in everyday life it may be useful to remember where you last saw something before it disappeared behind an occluder. In future work, it will prove interesting to systematically vary to the delay between encoding and cue to assess whether the reliance on the past location gradually dissipates with time (consistent with dropping an irrelevant feature), or whether the past trace remains preserved despite longer delays (consistent with preserving utility for working memory).”</p><disp-quote content-type="editor-comment"><p>(2) The authors refer to &quot;future&quot; and &quot;past&quot; information in working memory and this makes sense at a surface level. However, once the retrocue is revealed, the &quot;rule&quot; is retrieved from long-term memory, and the feature (e.g. right/left, top/bottom) is maintained in memory like any other item representation. Consider the classic test of digit span. The digits are presented and then recalled. Are the digits of the past or future? The authors might say that one cannot know, because past and future are perfectly confounded. An alternative view is that some information in working memory is relevant and some is irrelevant. In the digit span task, all the digits are relevant. Relevant information is relevant precisely because it is thought be necessary in the future. Irrelevant information is irrelevant precisely because it is not thought to be needed in the immediate future. In the current study, the orientation of the grating is relevant, but its location is irrelevant; and the location of the test probe is also relevant.</p></disp-quote><p>Thank you for this stimulating reflection. We agree that in our set-up, past location is technically “task-irrelevant” while future location is certainly “task-relevant”. At the same time, the engagement of the past location suggests to us that the brain uses past location for the selection – presumably because the brain uses spatial location to help individuate/separate the items, even if encoded locations are never asked about. Therefore, whether something is relevant or irrelevant ultimately depends on how one defines relevance (past location may be relevant/useful for the brain even if technically irrelevant from the perspective of the task). In comparison, the use of “past” and “future” may be less ambiguous.</p><p>It is also worth noting how we interpret our findings in relation to demands on visual working memory, inspired by dynamic situations whereby visual stimuli may be last seen at one location but expected to re-appear at another, such as a bird disappearing behind a building (the example in our introduction). Thus, past for us does not refer to the memory item perse (like in the digit span analogue) but, rather, quite specifically to the past location of a dynamic visual stimulus in memory (which, in our experiment, was operationalised by the future rule, for convenience).</p><disp-quote content-type="editor-comment"><p>(3) It is not clear how the authors interpret the &quot;joint representation&quot; of past and future. Put aside &quot;future&quot; and &quot;past&quot; for a moment. If there are two elements in memory, both of which are associated with spatial bindings, the attentional focus might be a spatial average of the associated spatial indices. One might also view this as an interference effect, such that the location of the encoded location attracts spatial attention since it has not been fully deleted/removed from working memory. Again, for the impact of the encoded location to be exactly zero after the retrieval cue, requires zero interference or instantaneous decay of the bound location information. It would be helpful for the authors to expand their discussion to further explain how the results fit within a broader theoretical framework and how it fits with empirical data on how quickly an irrelevant feature of an object can be deleted from working memory.</p></disp-quote><p>Thank you also for this point (that is related to the two points above). As we stated in our reply to comment 1 above, we agree that one possibility is that the past location is merely “sticky” and pulls the task-relevant future bias toward the past location. If so, our time courses suggest that such “pulling” occurs only until approximately 600 ms after cue onset, as the past bias is only transient. An alternative interpretation is that the past location may not be merely a residual irrelevant trace, but actually be useful and used by the brain.</p><p>For example, the encoded (past) item locations provide a coordinate system in which to individuate/separate the two memory items. While the future locations also provide such a coordinate system, the brain may benefit from holding onto both coordinate systems at the same time, rendering our observation of joint selection in both frames. Indeed, in a recent VR experiment in which we had participants (rather than the items) rotate, we also found evidence for the joint use of two spatial frames, even if neither was technically required for the upcoming task (see Draschkow, Nobre, van Ede, Nature Human Behaviour, 2022). Though highly speculative at this stage, such reliance on multiple spatial frames may make our memories more robust to decay and/or interference. Moreover, while past location was never explicitly probed in our task, in daily life the past location may sometimes (unexpectedly) become relevant, hence it may be useful to hold onto it, just in case. Thus, considering the past location merely as an “irrelevant feature” (that takes time to delete) may not do sufficient justice to the potential roles of retaining past locations of dynamic visual objects held in working memory.</p><p>As also stated in response to comment 1 above, we now added these relevant considerations to our Discussion:</p><p>Page 5 (Discussion): “In our study, the past location of the memory items was technically irrelevant for the task and could thus, in principle, be dropped after encoding. One possibility is that participants remapped the two memory items to their future locations soon after encoding, and had started – but not finished – dropping the past location by the time the cue arrived. In such a scenario, the past signal is merely a residual trace of the memory items that serves no purpose but still pulls gaze. Alternatively, however, the past locations may be utilised by the brain to help individuate/separate the two memory items. Moreover, by storing items with regard to multiple spatial frames (cf. 37) – here with regard to both past and future visual locations – it is conceivable that memories may become more robust to decay and/or interference. Also, while in our task past locations were never probed, in everyday life it may be useful to remember where you last saw something before it disappeared behind an occluder. In future work, it will prove interesting to systematically vary to the delay between encoding and cue to assess whether the reliance on the past location gradually dissipates with time (consistent with dropping an irrelevant feature), or whether the past trace remains preserved despite longer delays (consistent with preserving utility for working memory).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3, Comments:</bold></p><p>This study utilizes saccade metrics to explore, what the authors term the &quot;past and future&quot; of working memory. The study features an original design: in each trial, two pairs of stimuli are presented, first a vertical pair and then a horizontal one. Between these two pairs comes the cue that points the participant to one target of the first pair and another of the second pair. The task is to compare the two cued targets. The design is novel and original but it can be split into two known tasks - the first is a classic working memory task (a post-cue informs participants which of two memorized items is the target), which the authors have used before; and the second is a classic spatial attention task (a pre-cue signal that attention should be oriented left or right), which was used by numerous other studies in the past. The combination of these two tasks in one design is novel and important, as it enables the examination of the dynamics and overlapping processes of these tasks, and this has a lot of merit. However, each task separately is not new. There are quite a few studies on working memory and microsaccades and many on spatial attention and microsaccades. I am concerned that the interpretation of &quot;past vs. future&quot; could mislead readers to think that this is a new field of research, when in fact it is the (nice) extension of an existing one. Since there are so many studies that examined pre-cues and post-cues relative to microsaccades, I expected the interpretation here to rely more heavily on the existing knowledge base in this field. I believe this would have provided a better context of these findings, which are not only on &quot;past&quot; vs. &quot;future&quot; but also on &quot;working memory&quot; vs. &quot;spatial attention&quot;.</p></disp-quote><p>Thank you for considering our findings novel and important, while at the same time reminding us of the parallels to prior tasks studying spatial attention in perception and working memory. We fully agree that our task likely engages both attention to the (past) memory item as well as spatial attention to the upcoming (future) test stimulus. At the same time, there is a critical difference in spatial attention for the future in our task compared with ample prior tasks engaging spatial cueing of attention for perception. In our task, the cue never directly cues the future location. Rather, it exclusively cues the relevant memory item. It is the memory item that is associated with the relevant future location, according to the future rule. This integration of the rule-based future location into the memory representation is distinct from classical spatial-attention tasks in which attention is cued directly to a specific location via, for example, a spatial cue such as an arrow.</p><p>Thus, if we wish to think about our task as engaging cueing of spatial attention for perception, we have to at least also invoke the process of cueing the relevant location via the appropriate memory item. We feel it is more parsimonious to think of this as attending to both the past and future location of a dynamic visual object in working memory.</p><p>If we return to our opening example, when we see a bird disappear behind a building, we can keep in working memory where we last saw it, while anticipating where it will re-appear to guide our external spatial attention. Here too, spatial attention is fully dependent on working-memory content (the bird itself) – mirroring the dynamic semng in our study. Thus, we believe our findings contribute a fresh perspective, while of course also extending established fields. We now contextualize our finding within the literature and clarify our unique contribution in our revised manuscript:</p><p>Page 5 (Discussion): “Building on the above, at face value, our task may appear like a study that simply combines two established tasks: tasks using retro-cues to study attention in working memory (e.g.,2,31-33) and tasks using pre-cues to study orienting of spatial attention to an upcoming external stimulus (e.g., 31,32,34–36). A critical difference with common pre-cue studies, however, is that the cue in our task never directly informed the relevant future location. Rather, as also stressed above, the future location was a feature of the cued memory item (according to the future rule), and not of the cue itself. Note how this type of scenario may not be uncommon in everyday life, such as in our opening example of a bird flying behind a building. Here too, the future relevant location is determined by the bird – i.e. the memory content – itself.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 2, Recommendations:</bold></p><p>It would be helpful to set up predictions based on existing working memory models. Otherwise, the claim that the joint coding of past/future is &quot;not trivial&quot; is simply asserted, rather than contradicting an existing model or prior empirical results. If the non-trivial aspect is simply the ability to demonstrate the joint coding empirical through a good experimental design, make it clear that this is the contribution. For example, it may be that prevailing models predict exactly this finding, but nobody has been able to demonstrate it cleanly, as the authors do here. So the non-triviality is not that the result contradicts working memory models, but rather relates to the methodological difficulty of revealing such an effect.</p></disp-quote><p>Thank you for your recommendation. First, please see our point-by-point responses to the individual comments above, where we also state relevant changes that we have made to our article, and where we clarify what we meant with “non trivial”. As we currently also state in our introduction, our work took as a starting point the framework that working memory is inherently about the past while being for the future (cf. van Ede &amp; Nobre, Annual Review of Psychology, 2023). By virtue of our unique task design, we were able to empirically demonstrate that visual contents in working memory are selected via both their past and their future-relevant locations – with past and future memory attributes being engaged together in time. With “not trivial” we merely intend to make clear that there are viable alternatives than the findings we observed. For example, past could have been replaced by the future, or it could have been that item selection (through its past location) was required before its future-relevant location could be considered (i.e. past-before-future, rather than joint selection as we reported). We outline these alternatives in the second paragraph of our Discussion:</p><p>Page 5 (Discussion): “Our finding of joint utilisation of past and future memory attributes emerged from at least two alternative scenarios of how the brain may deal with dynamic everyday working memory demands in which memory content is encoded at one location but needed at another.</p><p>First, [….]”</p><p>Our work was not motivated from a particular theoretical debate and did not aim to challenge ongoing debates in the working-memory literature, such as: slot vs. resource, active vs. silent coding, decay vs. interference, and so on. To our knowledge, none of these debates makes specific claims about the retention and selection of past and future visual memory attributes – despite this being an important question for understanding working memory in dynamics everyday semngs, as we hoped to make clear by our opening example.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3, Recommendations:</bold></p><p>I recommend that the present findings be more clearly interpreted in the context of previous findings on working memory and attention. The task design includes two components - the first (post-cue) is a classic working memory task and the second (the pre-cue) is a classic spatial attention design. Both components were thoroughly studied in the past and this previous knowledge should be better integrated into the present conclusions. I specifically feel uncomfortable with the interpretation of past vs. future. I find this framework to be misleading because it reads like this paper is on a topic that is completely new and never studied before, when in fact this is a study on the interaction between working memory and spatial attention. I recommend the authors minimize this past-future framing or be more explicit in explaining how this new framework relates to the more common terminology in the field and make sure that the findings are not presented in a vacuum, as another contribution to the vibrant field that they are part of.</p></disp-quote><p>Thank you for these recommendations. Please also see our point-by-point responses to the individual comments above. Here, we explained our logic behind using the terminology of past vs. future (in addition, see also our response to point 2 or reviewer 2). Here, we also stated relevant changes that we have made to our manuscript to explain how our findings complement – but are also distinct from – prior tasks that used pre-cues to direct spatial attention to an upcoming stimulus. As we explained above, in our task, the cue itself never contained information about the upcoming test location. Rather, the upcoming test location was a property of the memory item (given the future rule). Hence, we referred to this as a “future attribute” of the cued memory item, rather than as the “cued location” for external spatial attention. Still, we agree the future bias likely (also) reflects spatial allocation to the upcoming test array, and we explicitly acknowledge this in our discussion. For example:</p><p>Page 5 (Discussion): “This signal may reflect either of two situations: the selection of a future-copy of the cued memory content or anticipatory attention to its the anticipated location of its associated test-stimulus. Either way, by the nature of our experimental design, this future signal should be considered a content-specific memory attribute for two reasons. First, the two memory contents were always associated with opposite testing locations, hence the observed bias to the relevant future location must be attributed specifically to the cued memory content. Second, we cued which memory item would become tested based on its colour, but the to-be-tested location was dependent on the item’s encoding location, regardless of its colour. Hence, consideration of the item’s future-relevant location must have been mediated by selecting the memory item itself, as it could not have proceeded via cue colour directly.”</p><p>Page 6 (Discussion): “Building on the above, at face value, our task may appear like a study that simply combines two established tasks: tasks using retro-cues to study attention in working memory (e.g.,2,31-33) and tasks using pre-cues to study orienting of spatial attention to an upcoming external stimulus (e.g., 31,32,34–36). A critical difference with common pre-cue studies, however, is that the cue in our task never directly informed the relevant future location. Rather, as also stressed above, the future location was a feature of the cued memory item (according to the future rule), and not of the cue itself. Note how this type of scenario may not be uncommon in everyday life, such as in our opening example of a bird flying behind a building. Here too, the future relevant location is determined by the bird – i.e. the memory content – itself.”</p></body></sub-article></article>