<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">68344</article-id><article-id pub-id-type="doi">10.7554/eLife.68344</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Increasing stimulus similarity drives nonmonotonic representational change in hippocampus</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-230137"><name><surname>Wammes</surname><given-names>Jeffrey</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8923-5441</contrib-id><email>jeffrey.wammes@queensu.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-76997"><name><surname>Norman</surname><given-names>Kenneth A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5887-9682</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-30747"><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7519-3001</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Psychology, Queen’s University</institution><addr-line><named-content content-type="city">Kingston</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution>Department of Psychology, Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Princeton Neuroscience Institute, Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution>University of Toronto</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>01</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e68344</elocation-id><history><date date-type="received" iso-8601-date="2021-03-12"><day>12</day><month>03</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-08-09"><day>09</day><month>08</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-03-14"><day>14</day><month>03</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.03.13.435275"/></event></pub-history><permissions><copyright-statement>© 2022, Wammes et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Wammes et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-68344-v1.pdf"/><abstract><p>Studies of hippocampal learning have obtained seemingly contradictory results, with manipulations that increase coactivation of memories sometimes leading to differentiation of these memories, but sometimes not. These results could potentially be reconciled using the nonmonotonic plasticity hypothesis, which posits that representational change (memories moving apart or together) is a U-shaped function of the coactivation of these memories during learning. Testing this hypothesis requires manipulating coactivation over a wide enough range to reveal the full U-shape. To accomplish this, we used a novel neural network image synthesis procedure to create pairs of stimuli that varied parametrically in their similarity in high-level visual regions that provide input to the hippocampus. Sequences of these pairs were shown to human participants during high-resolution fMRI. As predicted, learning changed the representations of paired images in the dentate gyrus as a U-shaped function of image similarity, with neural differentiation occurring only for moderately similar images.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>plasticity</kwd><kwd>statistical learning</kwd><kwd>image synthesis</kwd><kwd>deep neural networks</kwd><kwd>model-based analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Post-doctoral fellowship</award-id><principal-award-recipient><name><surname>Wammes</surname><given-names>Jeffrey</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000155</institution-id><institution>Social Sciences and Humanities Research Council of Canada</institution></institution-wrap></funding-source><award-id>Banting Post-doctoral fellowship</award-id><principal-award-recipient><name><surname>Wammes</surname><given-names>Jeffrey</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 MH069456</award-id><principal-award-recipient><name><surname>Norman</surname><given-names>Kenneth A</given-names></name><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007631</institution-id><institution>Canadian Institute for Advanced Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Turk-Browne</surname><given-names>Nicholas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Hippocampal learning in dentate gyrus follows a U-shaped function, with moderate, but not high or low, overlap between representations leading to differentiation of neural patterns.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans constantly learn new facts, encounter new events, and see and hear new things. Successfully managing this incoming information requires accommodating the new with the old, reorganizing memory as we learn from experience. How does learning dynamically shape representations in the hippocampus? Our experiences are encoded in distributed representations (<xref ref-type="bibr" rid="bib42">Johnson et al., 2009</xref>; <xref ref-type="bibr" rid="bib67">Polyn et al., 2005</xref>), spanning populations of neurons that are partially reused across multiple memories, leading to overlap. As we learn, the overlapping neural populations representing different memories in the hippocampus can shift, leading to either <italic>integration</italic>, where memories become more similar to one other, or <italic>differentiation</italic>, where memories become more distinct from one another for reviews, see <xref ref-type="bibr" rid="bib9">Brunec et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Duncan and Schlichting, 2018</xref>; <xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>.</p><p>Whether memories integrate or differentiate depends on whether the synapses that are common across the different memories strengthen or weaken. Traditional Hebbian learning models hold that synaptic connections strengthen when the pre-synaptic neuron repeatedly stimulates the post-synaptic neuron, causing them to fire together (<xref ref-type="bibr" rid="bib10">Buonomano and Merzenich, 1998</xref>; <xref ref-type="bibr" rid="bib12">Caporale and Dan, 2008</xref>; <xref ref-type="bibr" rid="bib27">Feldman, 2009</xref>; <xref ref-type="bibr" rid="bib35">Hebb, 1949</xref>). In other words, coactivation of neurons leads to strengthened connections between these neurons. This logic can scale up to the level of many synapses among entire populations of neurons, comprising distributed representations. A greater degree of coactivation among representations will strengthen shared connections and lead to integration. Consistent with this view, arbitrary pairs of objects integrate in the hippocampus following repeated temporal or spatial co-occurrence (e.g. <xref ref-type="bibr" rid="bib20">Deuker et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>). Moreover, new information that builds a link between two previously disconnected events can lead the representations of the events to integrate (<xref ref-type="bibr" rid="bib16">Collin et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Milivojevic et al., 2015</xref>; <xref ref-type="bibr" rid="bib80">Tompary and Davachi, 2017</xref>). In other cases, however, coactivation produces the exact opposite outcome — differentiation. For example, hippocampal representations of two faces with similar associations (<xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>) and of two navigation events with similar routes (<xref ref-type="bibr" rid="bib13">Chanales et al., 2017</xref>) differentiate as a result of learning. Further complicating matters, some studies have found that the same experimental conditions can lead to integration in some subfields of the hippocampus and differentiation in other subfields (<xref ref-type="bibr" rid="bib21">Dimsdale-Zucker et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>).</p><p>Such findings challenge Hebbian learning as a complete or parsimonious account of hippocampal plasticity. They suggest a more complex relationship between coactivation and representational change than the linear positive relationship predicted by classic Hebbian learning. We recently argued (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>) that this complex pattern of data could potentially be explained by the nonmonotonic plasticity hypothesis (NMPH; <xref ref-type="bibr" rid="bib19">Detre et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Hulbert and Norman, 2015</xref>; <xref ref-type="bibr" rid="bib63">Newman and Norman, 2010</xref>; <xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>), which posits a ‘U-shaped’ pattern of representational change as a function of the degree to which two memories coactivate (<xref ref-type="fig" rid="fig1">Figure 1</xref>). According to the NMPH, low levels of coactivation between two memories will lead to no change in their overlap; high levels of coactivation will strengthen mutual connections and lead to integration; and moderate levels of coactivation (where one memory is strongly active and the unique parts of the other memory are only moderately active) will weaken mutual connections and lead to differentiation, thereby reducing competition between the memories for later retrieval attempts (<xref ref-type="bibr" rid="bib39">Hulbert and Norman, 2015</xref>; <xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>; <xref ref-type="bibr" rid="bib85">Wimber et al., 2015</xref>). The NMPH has been put forward as a learning mechanism that applies broadly across tasks in which memories compete, whether they have been linked based on incidental co-occurrence in time or through more intentional associative learning (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>). The NMPH can explain findings of differentiation in diverse paradigms (e.g. linking to a shared associate: <xref ref-type="bibr" rid="bib13">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>; retrieval practice: <xref ref-type="bibr" rid="bib39">Hulbert and Norman, 2015</xref>; statistical learning: <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>) by positing that these paradigms induced moderate coactivation of competing memories. Likewise, relying on the same parameter of coactivation, the NMPH can explain seemingly contradictory findings showing that shared associates (<xref ref-type="bibr" rid="bib16">Collin et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Milivojevic et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>) and co-occurring items (<xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Schapiro et al., 2016</xref>) can lead to integration by positing that — in these cases — the paradigms induced strong coactivation.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Explanation of why moderate levels of visual similarity lead to differentiation.</title><p>Inset (bottom left) depicts the hypothesized nonmonotonic relationship between coactivation of memories and representational change from pre- to post-learning in the hippocampus. Low coactivation leads to no representational change, moderate coactivation leads to differentiation, and high coactivation leads to integration. Network diagrams show activity patterns in high-level visual cortex and the hippocampus evoked by two stimuli (A and B) with a moderate level of visual similarity that are presented as a ‘pair’ in a statistical learning procedure (such that B is reliably presented after A). Note that the hippocampus is hierarchically organized into a layer of <italic>perceptual conjunction units</italic> that respond to conjunctions of visual features and a layer of <italic>context units</italic> that respond to other features of the experimental context (<xref ref-type="bibr" rid="bib57">McKenzie et al., 2014</xref>). Before statistical learning (left-hand column), the hippocampal representations of A and B share a context unit (because the items appeared in a highly similar experimental context) but do not share any perceptual conjunction units. The middle column (top) diagram shows network activity during statistical learning, when the B item is presented immediately following an A item; the key consequence of this sequencing is that there is residual activation of A’s representation in visual cortex when B is presented. The colored arrows are meant to indicate different sources of input converging on the unique part of each item’s hippocampal representation (in the perceptual conjunction layer) when the other item is presented: green = perceptual input from cortex due to shared features (this is proportional to the overlap in the visual cortex representations of these items); orange = recurrent input within the hippocampus; purple = input from residual activation of the unique features of the previously-presented item. The purple input is what is different between the pre-statistical-learning phase (where A is not reliably presented before B) and the statistical learning phase (where A is reliably presented before B). In this example, the orange and green sources of input are not (on their own) sufficient to activate the other item’s hippocampal representation during the pre-statistical-learning phase, but the combination of all three sources of input is enough to moderately activate A’s hippocampal representation when B is presented during the statistical learning phase. The middle column (bottom) diagram shows the learning that will occur as a result of this moderate activation, according to the NMPH: The connection between the (moderately activated) item-A hippocampal unit and the (strongly activated) hippocampal context unit is weakened (note that this is not the only learning predicted by the NMPH in this scenario, but it is the most relevant learning and hence is highlighted in the diagram). As a result of this weakening, when item A is presented after statistical learning (right-hand column, top), it does not activate the hippocampal context unit, but item B still does (right-hand column, bottom), resulting in an overall decrease in the overlap of the hippocampal representations of A and B from pre-to-post learning.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig1-v1.tif"/></fig><p>Importantly, although the NMPH is compatible with findings of both differentiation and integration across several paradigms with diverse task demands, the explanations above are post hoc and do not provide a principled test of the NMPH’s core claim that there is a continuous, U-shaped function relating the level of coactivation to representational change. If there were a way of knowing where on the x-axis of this function an experimental condition was located (note that the U-shaped curve in <xref ref-type="fig" rid="fig1">Figure 1</xref> has no units), we could make a priori predictions about the learning that should take place, but practically speaking this is impossible: A wide range of neural findings on <italic>metaplasticity</italic> (summarized by <xref ref-type="bibr" rid="bib6">Bear, 2003</xref>) suggest that the transition point on the U-shaped curve between synaptic weakening (leading to differentiation) and synaptic strengthening (leading to integration) can be shifted based on experience. In light of this constraint, <xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref> argue that the key to robustly testing the NMPH account of representational change is to obtain samples from the full x-axis of the U-shaped curve and to look for a graded transition where differentiation starts to emerge at higher levels of memory coactivation and then disappears for even higher levels of memory coactivation.</p><p>No existing study has demonstrated the full U-shaped pattern for representational change; that is what we set out to do here, using a visual statistical learning paradigm — specifically, we brought about coactivation using temporal co-occurrence between paired items, and we manipulated the degree of coactivation by varying the visual similarity of the items in a pair. <xref ref-type="fig" rid="fig1">Figure 1</xref> illustrates the NMPH’s predictions regarding how pairing two items (A and B) in a visual statistical learning paradigm (such that B reliably follows A) can affect the similarity of the hippocampal representations of A and B. The figure depicts a situation where items A and B have moderate visual similarity, and statistical learning leads to differentiation of their hippocampal representations (because item A’s hippocampal representation is moderately activated during the presentation of item B). Crucially, the figure illustrates that there are three factors that influence how strongly the hippocampal representation of item A coactivates with the hippocampal representation of item B during statistical learning: (1) overlap in the high-level visual cortex representations of items A and B; (2) recurrent input from overlapping features within the hippocampus; and (3) residual activation of item A’s representation in visual cortex (because item A was presented immediately before item B). Thus, if we want to parametrically vary the coactivation of the hippocampal representations (to span the full axis of <xref ref-type="fig" rid="fig1">Figure 1</xref> and test for a full ‘U’ shape), we need to vary at least one of these three factors. In our study, we chose to focus on the first factor (overlap in visual cortex). Specifically, by controlling the visual similarity of paired items (<xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>), we sought to manipulate overlap in visual cortex and (through this) parametrically vary the coactivation of memories in the hippocampus.</p><p>To accomplish this goal, we developed a novel approach for synthesizing image pairs using deep neural network (DNN) models of vision. These models provide a link from pictures to rich quantitative descriptions of visual features, which in turn approximate some key principles of how the visual system is organized (e.g. <xref ref-type="bibr" rid="bib14">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Op de Beeck et al., 2008</xref>; <xref ref-type="bibr" rid="bib31">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib44">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib48">Kriegeskorte, 2009</xref>; <xref ref-type="bibr" rid="bib49">Kriegeskorte, 2015</xref>; <xref ref-type="bibr" rid="bib51">Kubilius et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">Luo et al., 2016</xref>; <xref ref-type="bibr" rid="bib92">Zeiler and Fergus, 2014</xref>). Most critically, later DNN layers correspond most closely to higher order, object-selective visual areas (<xref ref-type="bibr" rid="bib25">Eickenberg et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib43">Jozwik et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Khaligh-Razavi and Kriegeskorte, 2014</xref>), and when neural networks are optimized to match human performance, their higher layers predict neural responses in higher-order visual cortex (<xref ref-type="bibr" rid="bib11">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib90">Yamins et al., 2014</xref>). We reasoned that synthesizing pairs of stimuli that parametrically varied in their feature overlap in the upper layers of a DNN (<xref ref-type="bibr" rid="bib79">Szegedy et al., 2015</xref>) would also parametrically vary their <italic>neural</italic> overlap in the high-level visual regions that provide input to the hippocampus.</p><p>Image pairs spanning the range of possible representational overlap values were synthesized according to the procedure shown in <xref ref-type="fig" rid="fig2">Figure 2A and B</xref> and embedded in a statistical learning paradigm (<xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>). During fMRI, participants were given a pre-learning templating run (where the images were presented in a random order, allowing us to record the neural activity evoked by each image separately), followed by six statistical learning runs (where the images where presented in a structured order, such that the first image in a pair was always followed by the second image), followed by a post-learning templating run (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We hypothesized that manipulating the visual similarity of the paired images would allow us to span the x-axis of <xref ref-type="fig" rid="fig1">Figure 1</xref> and reveal a full U-shaped curve going from no change to differentiation to integration.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Schematic of image synthesis algorithm, fMRI task design, and behavioral validation.</title><p>(<bold>A</bold>) Our image synthesis algorithm starts with two visual noise arrays that are updated through many iterations (only three are depicted here: i, ii, and iii), until the feature activations from selected neural network layers (shown in yellow) achieve an intended Pearson correlation (<bold>r</bold>) value. (<bold>B</bold>) The result of our image synthesis algorithm was eight image pairs, that ranged in similarity from completely unrelated (similarity level 1, intended <italic>r</italic> among higher-order features = 0) to almost identical (similarity level 8, intended <italic>r</italic> = 1.00). (<bold>C</bold>) An fMRI experiment was conducted with these images to measure neural similarity and representation change. Participants performed a monitoring task in which they viewed a sequence of images, one at a time, and identified infrequent (10% of trials) gray squares in the image. Unbeknownst to participants, the sequence of images in structured runs contained the pairs (i.e. the first pairmate was <italic>always</italic> followed by the second pairmate); the images in templating runs were pseudo-randomly ordered with no pairs, making it possible to record the neural activity evoked by each image separately. (<bold>D</bold>) A behavioral experiment was conducted to verify that these similarity levels were psychologically meaningful. Participants performed an arrangement task in which they dragged and dropped images in a workspace until the most visually similar images were closest together. From the final arrangements, pairwise Euclidean distances were calculated as a measure of perceived similarity. (<bold>E</bold>) Correlation between model similarity level and distance between images (in pixels) in the arrangement task. On the left, each point represents a pair of images, with distances averaged across participants. In the center, each trendline represents the relationship between similarity level and an individual participants’ distances. The rightmost plot shows the magnitude of the correlation for each participant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Schematic of synthesis algorithm, related to <xref ref-type="fig" rid="fig2">Figure 2A</xref>.</title><p>(<bold>A</bold>) In the endpoint selection phase, images were generated that maximally express each of the feature channels in a later layer of a deep neural network. These feature activations were then correlated across images and pairs of endpoint channels with the lowest possible correlation were selected. Gray circles represent feature channels and closer proximity denotes higher correlations. Colored circles highlight examples of distant, low-correlation channel pairs. (<bold>B</bold>) In the image initialization phase, paired channels served as endpoints for a weighted optimization procedure. For each endpoint pair, eight new pairs of images were created with the intention that the correlation among pairmates would span a correlation of 0 (top) to 1 (bottom) across the eight pairs. The image pairs began as simple visual noise arrays. Each image pairmate had its own endpoint, which was always maximally optimized. The other image pairmate’s endpoint was weighted according to the intended correlation. The pixels in the visual noise arrays were iteratively updated to meet this weighted goal, creating intermediate images which proceeded to the next phase. (<bold>C</bold>) In the correlation tuning phase, the feature activations for the two intermediate images of a pair were extracted from the 12 layers of interest. The correlation between these activations was computed at each iteration. The absolute difference between the intended correlation and the current iteration’s correlation was used to iteratively update the images to minimize this difference. For a single set of endpoints, image pairs were made at various intended correlations (eight shown here).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Model validation, related to <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>.</title><p>To ensure that our model-based synthesis approach was effective in constraining the shared features among image pairs, we fed the final image pairs (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) back through the neural network that generated them (GoogleNet/Inception; <xref ref-type="bibr" rid="bib79">Szegedy et al., 2015</xref>; middle), as well as through an additional architecture (VGG19; <xref ref-type="bibr" rid="bib76">Simonyan and Zisserman, 2014</xref>; right) to ensure that the similarity space produced was not dependent on the characteristics of one specific model. We extracted features from these networks for each of the generated images, and computed representational similarity matrices (RSMs) at the targeted layers to ensure that they reflected the intended similarity space. (<bold>A</bold>) Intended (left) and actual (center) correlations (<bold>r</bold>) of features across paired images as a function of similarity level and layer of the Inception model. We aimed for and achieved uniform correlations across pairs in lower and middle layers (2D0-4C), and linearly increasing correlations across pairs in higher layers (4D-5B, see panel B). Also shown are the actual feature correlations across layers of the alternate VGG19 model (right). In this alternate architecture (VGG19), the three highest model layers (B5P-FC2) mirrored the intended similarity for the highest model layers in the generating network (<italic>r</italic>(62) = 0.861, 0.849, 0.856 for the three highest model layers). The four low/middle layers (B1P-B4P) did as well (<italic>r</italic>(62) = 0.592, 0.542, 0.396, 0.455 for the four low/middle layers), but to a lesser extent (Steiger test <italic>p</italic>s &lt; .001), and with lower variability across pairs (<italic>SD</italic> = 0.050, 0.017, 0.025, 0.052 for the four low/middle layers; for comparison, for the three highest layers: 0.116, 0.164, 0.169). (<bold>B</bold>) Comparison of feature correlations in each of the targeted higher layers of Inception (4D-5B). In the four highest layers (4D-5B), across all eight pairs of each of the eight endpoint axes (64 pairs total), the intended and actual feature correlations were strongly associated (<italic>r</italic>(62) = 0.970, 0.983, 0.977, 0.985, respectively, for the four highest layers). Considering each endpoint axis separately, the minimum feature correlations across the eight pairs of that axis remained high (<italic>r</italic>(6) = 0.945, 0.965, 0.966, 0.967, respectively, for the four highest layers). In the lower and middle layers (2D0-4C), feature correlations did not vary across pairs. This cannot be quantified by relating intended and actual feature correlations, given the lack of variance in intended feature correlations across pairs. Instead, the average differences between any two pairs in these eight lower/middle layers were small (<italic>M</italic> = 0.012, 0.011, 0.012, 0.041, 0.025, 0.045, 0.043, 0.053 for the eight lower/middle layers; for comparison, for the four highest layers <italic>M</italic> = 0.14, 0.19, 0.22, 0.23). The standard deviations of the feature correlations across pairs in these layers were also low (<italic>SD</italic> = 0.010, 0.010, 0.011, 0.035, 0.022, 0.039, 0.039, 0.048 for the eight lower/middle layers; for comparison, for the highest four layers <italic>SD = 0.199, 0.244, 0.259, 0.243)</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We and others have previously hypothesized that nonmonotonic plasticity applies widely throughout the brain (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>), including sensory regions (e.g. <xref ref-type="bibr" rid="bib6">Bear, 2003</xref>). In this study, we focused on the hippocampus due to its well-established role in supporting learning effects over relatively short timescales (e.g. <xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>). Importantly, we hypothesized that, even if nonmonotonic plasticity occurs throughout the entire hippocampus, it might be easier to trace out the full predicted U-shape in some hippocampal subfields than in others. As discussed above, our hypothesis is that representational change is determined by the level of coactivation – detecting the U-shape requires sweeping across the full range of coactivation values, and it is particularly important to sample from the low-to-moderate range of coactivation values associated with the differentiation ‘dip’ in the U-shaped curve (i.e. the leftmost side of the inset in <xref ref-type="fig" rid="fig1">Figure 1</xref>). Prior work has shown that there is extensive variation in overall activity (sparsity) levels across hippocampal subfields, with CA2/3 and DG showing much sparser codes than CA1 (<xref ref-type="bibr" rid="bib4">Barnes et al., 1990</xref>; <xref ref-type="bibr" rid="bib23">Duncan and Schlichting, 2018</xref>). We hypothesized that regions with sparser levels of overall activity (DG, CA2/3) would show lower overall levels of coactivation and thus do a better job of sampling this differentiation dip, leading to a more robust estimate of the U-shape, compared to regions like CA1 that are less sparse and thus should show higher levels of coactivation (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>). Consistent with this idea, human fMRI studies have found that CA1 is relatively biased toward integration and CA2/3/DG are relatively biased toward differentiation (<xref ref-type="bibr" rid="bib21">Dimsdale-Zucker et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>). Zooming in on the regions that have shown differentiation in human fMRI (CA2/3/DG), we hypothesized that the U-shape would be most visible in DG, for two reasons: First, DG shows sparser activity than CA3 (<xref ref-type="bibr" rid="bib4">Barnes et al., 1990</xref>; <xref ref-type="bibr" rid="bib30">GoodSmith et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">West et al., 1991</xref>) and thus will do a better job of sampling the left side of the coactivation curve. Second, CA3 is known to show strong attractor dynamics (‘pattern completion’; <xref ref-type="bibr" rid="bib32">Guzowski et al., 2004</xref>; <xref ref-type="bibr" rid="bib58">McNaughton and Morris, 1987</xref>; <xref ref-type="bibr" rid="bib70">Rolls and Treves, 1998</xref>) that might make it difficult to observe moderate levels of coactivation. For example, rodent studies have demonstrated that, rather than coactivating representations of different locations, CA3 patterns tend to sharply flip between one pattern and the other (e.g. <xref ref-type="bibr" rid="bib53">Leutgeb et al., 2007</xref>; <xref ref-type="bibr" rid="bib83">Vazdarjanova and Guzowski, 2004</xref>). As discussed below, our hypothesis about DG was borne out in the data: Using synthesized image pairs varying in similarity, we observed the full U-shape (transitioning into and out of differentiation, as a function of similarity) in DG, thereby providing direct evidence that hippocampal plasticity is nonmonotonic.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Stimulus synthesis</title><sec id="s2-1-1"><title>Model validation</title><p>Before looking at the effects of statistical learning on hippocampal representations, we wanted to verify that our model-based synthesis approach was effective in creating graded levels of feature similarity in the targeted layers of the network (corresponding to high-level visual cortex): Specifically, our goal was to synthesize images that varied parametrically in their similarity in higher layers while not differing systematically in lower and middle layers of the network. To assess whether we were successful in meeting this goal, we fed the final image pairs (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) back through the neural network that generated them (GoogLeNet/Inception; <xref ref-type="bibr" rid="bib79">Szegedy et al., 2015</xref>), and computed the actual feature correlations at the targeted layers. We found that the intended and actual similarity levels of the images (in terms of model features) showed a close correspondence (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>): In the highest four layers (4D-5B), the intended and actual feature correlations were strongly associated (<italic>r</italic>(62) = .970, .983, .977, .985, respectively). In the lower and middle layers, feature correlations did not vary across pairs, as intended.</p></sec><sec id="s2-1-2"><title>Behavioral validation</title><p>Because deep neural networks can be influenced by visual features to which humans are insensitive (<xref ref-type="bibr" rid="bib64">Nguyen et al., 2015</xref>), we also sought to validate that the differences in similarity levels across image pairs were perceptually meaningful to human observers. We employed a behavioral task in which participants (<italic>n</italic> = 30) arranged sets of images (via dragging and dropping in a 2-D workspace; <xref ref-type="fig" rid="fig2">Figure 2D</xref>), with the instruction to place images that are visually similar close together and images that are visually dissimilar far apart (<xref ref-type="bibr" rid="bib50">Kriegeskorte and Mur, 2012</xref>). Participants completed at least 10 arrangement trials and the distances for each synthesized image pair were averaged across these trials. When further averaged across participants, perceptual distance was strongly negatively associated with the intended model similarity (<italic>r</italic>(62) = −0.813, p &lt; 0.0001; <xref ref-type="fig" rid="fig2">Figure 2E</xref>). In other words, image pairs at the highest similarity levels were placed closer to one another. In fact, every individual participant’s correlation was negative (mean <italic>r</italic> = −0.552, 95% CI = [−0.593–0.512]).</p></sec><sec id="s2-1-3"><title>Neural validation</title><p>Because we were synthesizing image pairs based on features from the highest model layers, we hypothesized that model similarity would be associated with representational similarity in high-level visual cortical regions such as lateral occipital (LO) and inferior temporal (IT) cortices. We also explored ventral temporal regions parahippocampal cortex (PHC) and fusiform gyrus (FG), and early visual regions V1 and V2. Based on separate viewing of the 16 synthesized images during the initial templating run (prior to statistical learning), we calculated an image-specific pattern of BOLD activity across voxels in each anatomical ROI. We then correlated these patterns across image pairs as a measure of neural similarity (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Model similarity level was positively associated with neural similarity in LO (mean <italic>r</italic> = 0.182, 95% CI = [0.083 0.279], randomization p = 0.007) and PHC (mean <italic>r</italic> = 0.125, 95% CI = [0.022 0.228], p = 0.029). No other region showed a significant positive relationship to model similarity (V2: mean <italic>r</italic> = −0.029, 95% CI = [−0.137 0.077], p = .674; IT: <italic>r</italic> = .070, 95% CI = [−0.057 0.197], p = 0.145; FG: <italic>r</italic> = 0.056, 95% CI = [−0.055 0.170], p = 0.171), including regions of the medial temporal lobe (perirhinal cortex, and entorhinal cortex; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>); V1 showed a negative relationship (mean <italic>r</italic> = −0.112, 95% CI = [−0.224–0.003], p = .038). The correspondence between the similarity of image pairs in the model and in LO and PHC is consistent with our use of the highest layers of a neural network model for visual object recognition in image synthesis. The fact that this correspondence was observed in LO and PHC but not in earlier visual areas further validates that similarity was based on high-level features.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Analysis of where in the brain representational similarity tracked model similarity, prior to statistical learning.</title><p>(<bold>A</bold>) Correlation of voxel activity patterns evoked by pairs of stimuli (before statistical learning) in different brain regions of interest, as a function of model similarity level (i.e. how similar the internal representations of stimuli were in the targeted layers of the model). Neural similarity was reliably positively associated with model similarity level only in LO and PHC. Shaded areas depict bootstrap resampled 95% confidence intervals at each model similarity level. (<bold>B</bold>) Searchlight analysis. Brain images depict coronal slices viewed from a posterior vantage point. Clusters in blue survived correction for family-wise error (FWE) at p &lt; 0.05 using the null distribution of maximum cluster mass. L = left hemisphere, R = right hemisphere, A = anterior, P = posterior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Similarity tracked model similarity prior to statistical learning, related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Correlation of voxel activity patterns evoked by pairs of stimuli (before statistical learning) in different brain regions of interest, as a function of model similarity level (i.e. how similar the internal representations of stimuli were in the targeted layers of the model). Shaded areas depict bootstrap resampled 95% confidence intervals at each model similarity level. The pattern of results was identical to that in the full sample: model similarity level was positively associated with neural similarity in LO (mean <italic>r</italic> = 0.163, 95% CI = [0.056 0.270], randomization p = 0.024) and PHC (mean <italic>r</italic> = 0.127, 95% CI = [0.016 0.239], p = 0.031). No other region showed a significant positive relationship to model similarity (V2: mean <italic>r</italic> = −0.046, 95% CI = [−0.163 0.069], p = 0.720; IT: <italic>r</italic> = 0.090, 95% CI = [−0.049 0.226], p = 0.095; FG: <italic>r</italic> = 0.058, 95% CI = [−0.059 0.179], p = 0.195); V1 showed a negative relationship (mean <italic>r</italic> = −0.141, 95% CI = [−0.261 −0.025], p = 0.020). We also repeated the searchlight analysis in the reduced sample size, which yielded a very similar statistical map to the full sample (<italic>r</italic> = 0.928), though the clusters reported for the full sample did not survive correction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Analyses of medial temporal cortex subregions, showing neural similarity (prior to statistical learning) as a function of model similarity, related to <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</title><p>Correlation of voxel activity patterns evoked by pairs of stimuli (before statistical learning) in different brain regions of interest, as a function of model similarity level (i.e. how similar the internal representations of stimuli were in the targeted layers of the model), in parahippocampal cortex (PHC; Duplicate figure and analysis from <xref ref-type="fig" rid="fig3">Figure 3A</xref>), perirhinal cortex (PRC) and entorhinal cortex (EC). Of these three regions, model similarity level was positively associated with neural similarity only in PHC (mean <italic>r</italic> = 0.126, 95% CI = [0.024 0.229], p = 0.028), not in PRC (<italic>r</italic> = 0.029, 95% CI = [−0.096 0.149], p = 0.339), or EC (<italic>r</italic> = 0.076, 95% CI = [−0.038 0.189], <italic>p</italic> = 0.154). Shaded area depicts bootstrap resampled 95% CIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig3-figsupp2-v1.tif"/></fig></fig-group><p>It is unlikely that any given model layer(s) will map perfectly and exclusively to a single anatomical region. Accordingly, although we targeted higher-order visual cortex (e.g. LO, IT), the layers we manipulated may have influenced representations in other regions, or alternatively, a subset of the voxels within a given anatomical ROI. To explore this possibility, we performed a searchlight analysis (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) testing where in the brain neural similarity was positively associated with model similarity. This revealed two large clusters of voxels (p &lt; 0.05 corrected): left ventral and dorsal LO extending into posterior FG (3722 voxels; peak <italic>t</italic>-value = 5.60; MNI coordinates of peak = −37.5,–72.0, −10.5; coordinates of center = −26.7,–71.7, 17.4) and right ventral and dorsal LO extending into occipital pole (3107 voxels; peak <italic>t</italic>-value = 4.82; coordinates of peak = 33.0, –88.5, 10.5; coordinates of center = 30.9, –86.5, 10.6). When this analysis was repeated with a reduced sample of the 36 participants who were also included in the subsequent representational change analyses, these clusters no longer emerged as statistically significant at a corrected threshold.</p></sec></sec><sec id="s2-2"><title>Representational change</title><sec id="s2-2-1"><title>Hippocampus</title><p>We hypothesized that learning-related representational change in the hippocampus, specifically in DG, would follow a nonmonotonic curve. That is, we predicted a cubic function wherein low levels of model similarity would yield no neural change, moderate levels of model similarity would dip toward neural differentiation, and high levels of model similarity would climb back toward neural integration (<xref ref-type="fig" rid="fig1">Figure 1</xref> inset). We predicted that this nonmonotonic pattern would be observed in the DG, and possibly CA2/3 subfields, given the predisposition of these subfields (especially DG) to sparse representations and pattern separation.</p><p>To test this hypothesis, we extracted spatial patterns of voxel activity associated with each image from separate runs that occurred before and after statistical learning (pre- and post-learning templating runs, respectively). In the templating runs, images were presented individually in a completely random order to evaluate how their representations were changed by learning. The response to each image was estimated in every voxel using a GLM. The voxels from each individual’s hippocampal subfield ROIs were extracted to form a pattern of activity for each image and subfield. We then calculated the pattern similarity between images in a pair using Pearson correlation, both before and after learning, and subtracted before-learning pattern similarity from after-learning pattern similarity to index the direction and amount of representational change. A separate representational change score was computed for each of the eight model similarity levels.</p><p>To test for the U-shaped curve predicted by the NMPH, we fit a theory-constrained cubic model to the series of representational change scores across model similarity levels (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Specifically, the leading coefficient was forced to be positive to ensure a dip, followed by a positive inflection — the characteristic shape of the NMPH (<xref ref-type="fig" rid="fig1">Figure 1</xref> inset). The predictions of this theory-constrained cubic model were reliably associated with representational change in DG (<italic>r</italic> = 0.134, 95% CI = [0.007 0.267], randomization p = 0.022). The fit was not reliable in CA2/3 (<italic>r</italic> = 0.082, 95% CI = [−0.027 0.191], <italic>p</italic> = 0.13), CA1 (<italic>r</italic> = 0.116, 95% CI = [−0.001 0.231], p = 0.10), or the hippocampus as a whole (<italic>r</italic> = 0.084, 95% CI = [−0.018 0.186], p = 0.15). Model fit was also not reliable in other regions of the medial temporal lobe (PHC, perirhinal cortex, and entorhinal cortex; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Interestingly, in an exploratory analysis, we found that the degree of model fit in DG was predicted by the extent to which visual representations in PRC tracked model similarity (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Analysis of representational change predicted by the nonmonotonic plasticity hypothesis.</title><p>(<bold>A</bold>) Difference in correlation of voxel activity patterns between paired images after minus before learning at each model similarity level, in the whole hippocampus (HC) and in hippocampal subfields CA1, CA2/3 and DG. Inset image shows an individual subject mask for the ROI in question, overlaid on their T2-weighted anatomical image. The nonmonotonic plasticity hypothesis reliably predicted representational change in DG. Shaded area depicts bootstrap resampled 95% CIs. (<bold>B</bold>) Searchlight analysis. Brain images depict coronal slices viewed from an anterior vantage point. Clusters in red survived correction for family-wise error (FWE) at p &lt; 0.05 using the null distribution of maximum cluster mass. L = left hemisphere, R = right hemisphere, A = anterior, P = posterior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Analyses of medial temporal cortex subregions, showing representational change as a function of model similarity, related to <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</title><p>Difference in correlation of voxel activity patterns between paired images after minus before learning at each model similarity level. Model fit was not reliable in PHC (<italic>r</italic> = 0.076, 95% CI = [−0.055 0.209], p = 0.18), PRC (<italic>r</italic> = −0.106, 95% CI = [−0.236 0.023], p = 0.79), or EC (<italic>r</italic> = −0.066, 95% CI = [−0.161 0.026], p = 0.72). Shaded area depicts bootstrap resampled 95% CIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Exploratory analyses of the association between visual similarity effects in visual regions of interest, and the observed nonmonotonic effect in DG, related to <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</title><p>(<bold>A</bold>) For each participant, we extracted the statistic for the correlation between model similarity level and representational similarity in visual regions of interest (Linear Effect), as well as the statistic for the correlation between observed representational change and predicted representational change based on the nonmonotonic model (Nonmonotonic Effect). We then probed for cortical regions where the Linear Effect was reliably associated with our observed Nonmonotonic Effect in DG. Based on bootstrap resampling participants 50000 times, the association was reliable only in PRC (mean <italic>r</italic> = 0.348, 95% CI = [0.026 0.629]). No other regions were reliably associated with the Nonmonotonic Effect in DG (V1: mean <italic>r</italic> = 0.085, 95% CI = [−0.152 0.317]; V2: mean <italic>r</italic> = 0.099, 95% CI = [−0.140 0.345]; LOC: mean <italic>r</italic> = 0.036, 95% CI = [−0.291 0.399]; Fus: mean <italic>r</italic> = 0.209, 95% CI = [−0.072 0.493]; PHC: mean <italic>r</italic> = 0.218, 95% CI = [−0.080 0.509]; IT: mean <italic>r</italic> = 0.150, 95% CI = [−0.162 0.458]; EC: mean <italic>r</italic> = 0.169, 95% CI = [−0.151 0.470]). (<bold>B</bold>) Scatterplot depicting the association between the Linear Effect in PRC and the Nonmonotonic Effect in DG. The dotted vertical gray line depicts the median Linear Effect in PRC (0.065). (<bold>C</bold>) To visualize the difference, plotted here is the difference in correlation of voxel activity patterns between paired images after minus before learning at each model similarity level in DG. The left and right plots depict participants whose Linear Effect in PRC was lower and higher than the median, respectively. Shaded area depicts bootstrap resampled 95% CIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68344-fig4-figsupp2-v1.tif"/></fig></fig-group><p>We followed up on the observed effect in DG and determined that there was reliable differentiation at model similarity levels 5 (<inline-formula><mml:math id="inf1"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula><italic>r</italic> = −0.093, 95% CI = [−0.177 −0.007], p &lt; 0.0001) and 6 (<inline-formula><mml:math id="inf2"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula><italic>r</italic> = −0.090, 95% CI = [−0.179 −0.004], p = 0.016). This trough in the center of the U-shaped curve was also reliably lower than the peaks preceding it at level 4 (<inline-formula><mml:math id="inf3"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula><italic>r</italic> = 0.129, 95% CI = [0.019 0.243], p = 0.015) and following it at level 8 (<inline-formula><mml:math id="inf4"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula><italic>r</italic> = 0.150, 95% CI = [0.025 .0271], p = 0.005). The curve showed a trend toward positive representational change, suggestive of integration, for model similarity level 8 (<inline-formula><mml:math id="inf5"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula><italic>r</italic> = 0.057, 95% CI = [−0.034 0.147], p = 0.078).</p></sec><sec id="s2-2-2"><title>Whole-brain searchlight</title><p>To determine whether nonmonotonic learning effects were specific to the hippocampus, we ran an exploratory searchlight analysis in which we repeated the above cubic model-fitting analysis over the whole brain (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). This analysis revealed two reliable clusters (p &lt; 0.05 corrected): right hippocampus extending into PHC and FG (832 voxels; peak <italic>t</italic>-value = 4.97; MNI coordinates of peak = 37.5,–7.5, −15.0; coordinates of center = 32.8,–18.7, −17.1) and anterior cingulate, extending into medial prefrontal cortex (1604 voxels; peak <italic>t</italic>-value = 5.43; coordinates of peak = −7.5, 28.5, 21.0; coordinates of center = −5.0, 28.5, 10.2).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We set out to determine how learning shapes representations in the hippocampus and found that the degree of overlap in visual features determined the nature of representational change in DG. The pattern of results was U-shaped: with low or high overlap, object representations did not reliably change with respect to one another, whereas with moderate overlap, they pushed apart from one another following learning. This is consistent with the predictions of the NMPH (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>) and related theories (e.g. <xref ref-type="bibr" rid="bib8">Bienenstock et al., 1982</xref>). Although previous studies have reported evidence consistent with the NMPH (e.g. manipulations that boost coactivation of hippocampal representations lead to differentiation; <xref ref-type="bibr" rid="bib13">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>), these studies generally compared only two or three conditions and their results can also be explained by competing hypotheses (e.g. a monotonic increase in differentiation with increasing shared activity). Crucially, the present study is the first to span coactivation continuously in order to reveal the full U-shape predicted by the NMPH, whereby differentiation emerges as coactivation grows from low to moderate and dissipates as coactivation continues from moderate to high.</p><p>To measure the impact of the degree of coactivation across a broad range of possible values, we developed a novel method of synthesizing experimental image pairs using DNN models. The intent of this approach was to precisely control the overlap among visual features at one or more layers of the model. In this case, we targeted higher layers of the model to indirectly control representational similarity in higher-order visual regions that provide input to the hippocampus. We found that the imposed visual feature relationships between images influenced human similarity judgments and were associated with parametric changes in neural similarity in higher-order visual cortex (i.e. LO, PHC). This is the first demonstration of the efficacy of stimulus synthesis in manipulating high-level representational similarity in targeted brain regions in humans. These results resonate with recent advances in stimulus synthesis designed to target individual neurons in primates (<xref ref-type="bibr" rid="bib5">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Ponce et al., 2019</xref>). Although fMRI does not allow for targeting of individual neurons, our findings show that it is feasible to use this method to target distribute representations in different visual cortical regions.</p><p>Our approach of manipulating the overlap of visual inputs to the hippocampus, rather than manipulating hippocampal codes directly, was a practical one, based on the fact that we have much better computational models of visual coding than hippocampal coding. Numerous studies have shown that, while hippocampal neurons are indeed ‘downstream’ from visual cortex, they additionally encode complex information from multiple sensory modalities (<xref ref-type="bibr" rid="bib52">Lavenex and Amaral, 2000</xref>), as well as information about reward (<xref ref-type="bibr" rid="bib86">Wimmer and Shohamy, 2012</xref>), social relevance (<xref ref-type="bibr" rid="bib65">Olson et al., 2007</xref>), context (<xref ref-type="bibr" rid="bib82">Turk-Browne et al., 2012</xref>), and time (<xref ref-type="bibr" rid="bib37">Hsieh et al., 2014</xref>; <xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>), to name a few. So, although we controlled visual inputs to the hippocampus, there were many additional non-visual inputs that were free to vary and could play a role in determining the overall relational structure of the representational space. Our work here demonstrates that controlling the visual features alone was sufficient to elicit non-monotonic learning effects, raising the possibility that controlling additional dimensions might yield greater differentiation (or integration). Future work could explore combining models of vision with hippocampal models and attempt to directly target hippocampal representations with image synthesis.</p><p>In generating our experimental stimuli, we deliberately avoided the semantic or conceptual information that comes with meaningful real-world stimuli. This choice was made for several reasons. First, it is highly unlikely that the feature correspondences of even a curated set of real-world stimuli could arrange themselves in a linearly increasing fashion in a targeted model layer, which was a requirement to test our hypotheses. Second, there are known top-down influences on visual representations (<xref ref-type="bibr" rid="bib29">Gilbert and Li, 2013</xref>), including the integration of conceptual information (<xref ref-type="bibr" rid="bib56">Martin et al., 2018</xref>), which would have undermined our intended visual similarity structure. Last, meaningless pictures and shapes are the most commonly used stimuli when studying visual statistical learning (e.g. <xref ref-type="bibr" rid="bib47">Kirkham et al., 2002</xref>; <xref ref-type="bibr" rid="bib55">Luo and Zhao, 2018</xref>; <xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>; <xref ref-type="bibr" rid="bib81">Turk-Browne et al., 2005</xref>), in part to avoid contamination from pre-existing stimulus relationships. Also, although our image pairs were not nameable objects per se, the DNN used to generate them was trained on real-world images (<xref ref-type="bibr" rid="bib18">Deng et al., 2009</xref>), meaning that they were composed from real object features. Nevertheless, the question remains whether our findings extend to meaningful real-world stimuli. Prior work has shown hippocampal differentiation in experimental conditions involving faces and scenes (<xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib45">Kim et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>) as well as complex navigation events (<xref ref-type="bibr" rid="bib13">Chanales et al., 2017</xref>). With this, and assuming one could find a way to impose precise differences in visual overlap as we have here, it seems likely that these effects would generalize. Future work may be able to address this issue more directly using recently developed generative methods (<xref ref-type="bibr" rid="bib78">Son et al., 2020</xref>) or cleverly designed stimuli that capture both conceptual and perceptual similarity (<xref ref-type="bibr" rid="bib56">Martin et al., 2018</xref>).</p><p>Importantly, our study allowed us to examine representational change in specific hippocampal subfields. We found that the differentiation ‘dip’ (creating the U shape) was reliable in DG. This fits with prior studies that found differentiation in a combined CA2/3/DG ROI (<xref ref-type="bibr" rid="bib21">Dimsdale-Zucker et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>), and greater sparsity and pattern separation in DG in particular (<xref ref-type="bibr" rid="bib7">Berron et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">GoodSmith et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Leutgeb et al., 2007</xref>), although note that the U-shaped pattern was trending but not significant in CA2/3 in our study. The clearer effects in DG may suggest that sparse coding (and the resulting low activation levels) is necessary to traverse the full spectrum of coactivation from low to moderate to high that can reveal nonmonotonic changes in representational similarity; regions with less sparsity (and higher baseline activation levels) may restrict coactivation to the moderate to high range, resulting in a bias toward integration and monotonic increases in representational similarity. Indeed, we had expected that CA1 might show integration effects due to its higher overall levels of activity (<xref ref-type="bibr" rid="bib4">Barnes et al., 1990</xref>), consistent with prior studies emphasizing a role for CA1 in memory integration (<xref ref-type="bibr" rid="bib9">Brunec et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Dimsdale-Zucker et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Duncan and Schlichting, 2018</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>; <xref ref-type="bibr" rid="bib74">Schlichting et al., 2014</xref>). One speculative possibility is that the hippocampus is affected by feature overlap in earlier stages of visual cortex in addition to later stages (e.g. <xref ref-type="bibr" rid="bib38">Huffman and Stark, 2017</xref>). Our paired stimuli were constructed to have high overlap at the top of the visual hierarchy but low overlap earlier on in the hierarchy; it is possible that allowing stimuli to have higher overlap throughout the visual hierarchy would lead to even greater coactivation in the hippocampus, resulting in integration.</p><p>Although we focused above on differences in sparsity when motivating our predictions about subfield-specific learning effects, there are numerous other factors besides sparsity that could affect coactivation and (through this) modulate learning. For example, the degree of coactivation during statistical learning will be affected by the amount of residual activity of the A item during the B item’s presentation in the statistical learning phase. In <xref ref-type="fig" rid="fig1">Figure 1</xref>, this residual activity is driven by sustained firing in cortex, but this could also be driven by sustained firing in hippocampus; subfields might differ in the degree to which activation of stimulus information is sustained over time (see, e.g. the literature on hippocampal time cells: <xref ref-type="bibr" rid="bib24">Eichenbaum, 2014</xref>; <xref ref-type="bibr" rid="bib36">Howard and Eichenbaum, 2013</xref>), and activation could be influenced by differences in the strength of attractor dynamics within subfields (e.g. <xref ref-type="bibr" rid="bib53">Leutgeb et al., 2007</xref>; <xref ref-type="bibr" rid="bib62">Neunuebel and Knierim, 2014</xref>). Also, in <xref ref-type="fig" rid="fig1">Figure 1</xref>, the learning responsible for differentiation was shown as happening between ‘perceptual conjunction’ neurons and ‘context’ neurons in the hippocampus. Subfields may vary in how strongly these item and context features are represented, in the stability/drift of the context representations (<xref ref-type="bibr" rid="bib22">DuBrow et al., 2017</xref>), and in the interconnectivity between item and context features (<xref ref-type="bibr" rid="bib88">Witter et al., 2000</xref>); it is also likely that some of the relevant plasticity between item and context features is happening across, in addition to within, subfields (<xref ref-type="bibr" rid="bib33">Hasselmo and Eichenbaum, 2005</xref>). For these reasons, exploring the predictions of the NMPH in the context of biologically detailed computational models of the hippocampus (e.g. <xref ref-type="bibr" rid="bib28">Frank et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Hasselmo and Wyble, 1997</xref>; <xref ref-type="bibr" rid="bib73">Schapiro et al., 2017</xref>) will help to sharpen predictions about what kinds of learning should occur in different parts of the hippocampus.</p><p>Although our results are broadly consistent with prior findings that increasing the coactivation of memories can lead to differentiation (<xref ref-type="bibr" rid="bib13">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>), they are notably inconsistent with results from <xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref>, who reported memory integration for arbitrarily paired images as a result of temporal co-occurrence; pairs in our study with comparable levels of visual similarity (roughly model similarity level 3) showed no evidence of integration. This difference between studies may relate to the fact that the visual sequences in <xref ref-type="bibr" rid="bib71">Schapiro et al., 2012</xref> contained a mix of strong and weak transition probabilities, whereas we used strong transition probabilities exclusively; moreover, our study had a higher baseline of visual feature overlap among pairs. Contextual and task-related factors (<xref ref-type="bibr" rid="bib9">Brunec et al., 2020</xref>), as well as the history of recent activation (<xref ref-type="bibr" rid="bib6">Bear, 2003</xref>), can bias the hippocampus toward integration or differentiation, similar to the remapping based on task context that occurs in rodent hippocampus (<xref ref-type="bibr" rid="bib3">Anderson and Jeffery, 2003</xref>; <xref ref-type="bibr" rid="bib15">Colgin et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">McKenzie et al., 2014</xref>). Speculatively, the overall higher degree of competition in our task — from stronger transition probabilities and higher baseline similarity — may have biased the hippocampus toward differentiation (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>).</p><p>Our design had several limitations. Prior work in this area has demonstrated brain-behavior relationships (<xref ref-type="bibr" rid="bib26">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Molitor et al., 2021</xref>), so it is clear that changes in representational overlap (i.e. either integration or differentiation) can bear on later behavioral performance. However, in the current work, our behavioral task was intentionally orthogonal to the dimensions of interest (i.e. unrelated to temporal co-occurrence and visual similarity), limiting our ability to draw conclusions about potential downstream effects on behavior. We believe that this presents a compelling target for follow-up research. Establishing a behavioral signature of both integration and differentiation in the context of nonmonotonic plasticity will not only clarify the brain-behavior relationship, but also allow for investigations in this domain without requiring brain data.</p><p>Finally, although analyzing representational overlap in templating runs before and after statistical learning afforded us the ability to quantify pre-to-post changes, our design precluded analysis of the <italic>emergence</italic> of representational change over time. That is, we could not establish whether integration or differentiation occurred early or late in statistical learning. This is because, during statistical learning runs, the onsets of paired images were almost perfectly correlated, meaning that it was not possible to distinguish the representation of one image from its pairmate. Future work could monitor the time course of representational change, either by interleaving additional templating runs throughout statistical learning (although this could interfere with the statistical learning process), or by exploiting methods with higher temporal resolution where the responses to stimuli presented close in time can more readily be disentangled.</p><sec id="s3-1"><title>Conclusion</title><p>Overall, these results highlight the complexity of learning rules in the hippocampus, showing that in DG, moderate levels of visual feature similarity lead to differentiation following a statistical learning paradigm, but higher and lower levels of visual similarity do not. From a theoretical perspective, these results provide the strongest evidence to date for the NMPH account of hippocampal plasticity. We expect that a similar U-shaped function relating coactivation and representational change will manifest in paradigms with different task demands and stimuli, but additional work is needed to provide empirical support for this claim about generality. From a methodological perspective, our results provide a proof-of-concept demonstration of how image synthesis, applied to neural network models of specific brain regions, can be used to test how representations in these regions shape learning. As neural network models continue to improve, we expect that this kind of model-based image synthesis will become an increasingly useful tool for studying neuroplasticity.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>For the fMRI study, we recruited 42 healthy young adults participants (18–35 years old, 25 females) with self-reported normal (or corrected to normal) visual acuity and good color vision. All participants provided informed consent to a protocol approved by the Yale IRB and were compensated for their time ($20 per hour). Five participants did not complete the task because of technical errors and/or time constraints, though their data could still be used for the visual templating analyses, as this only required the initial pre-learning templating run. One additional participant’s data quality precluded segmentation of hippocampal subfields. As such, our final sample for the learning task was 36 participants, with a total of 41 participants available for the visual templating analyses. See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the outcome of the visual templating analyses in a reduced sample containing only the 36 participants included in the representational change analyses.</p><p>For the behavioral validation study, we recruited 30 naive participants through Amazon Mechanical Turk (mTurk). All participants provided informed consent to a protocol approved by the Yale IRB, and were compensated for their time ($6 per hour).</p></sec><sec id="s4-2"><title>Stimulus synthesis</title><p>Image pairs were generated via a gradient descent optimization using features extracted from <italic>GoogLeNet</italic> a version of <italic>Inception</italic>; (<xref ref-type="bibr" rid="bib79">Szegedy et al., 2015</xref>), a deep neural network (DNN) architecture. This particular instantiation had been pretrained on ImageNet (<xref ref-type="bibr" rid="bib18">Deng et al., 2009</xref>), which contains over one million images of common objects. Accordingly, the learned features reflect information about the real-world features of naturalistic objects. Our approach drew heavily from <italic>Deepdream</italic> (<xref ref-type="bibr" rid="bib61">Mordvintsev et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Deepdreaming with tensorflow, 2021</xref>), an approach used to visualize the learned features of pretrained neural networks. Deepdream’s optimization uses gradient ascent to iteratively update input pixels such that activity in a given unit, layer, or collection of layers is maximized. Different from Deepdream, the core of our approach was controlling the correlation between the features of two images at a given layer <italic>j</italic>, as a means of controlling visual overlap at a targeted level of complexity. We prioritized image optimization over features in a subset of network layers: the early convolutional layers and the output layers of later inception modules (i.e. 12 total layers).</p><p>Because we were interested in targeting higher-order visual representations (e.g. in LO), our intention was to produce pairs of images whose higher-layer (top four layers) features were correlated with one another at a specified value, ranging from 0 (not at all similar) to 1 (almost exactly the same). As such, we produced pairs of images that fell along an axis between two ‘endpoints’, where the endpoints were pairs of images designed to have a correlation of 0. Each subsequent pair of increasing similarity can be thought of as sampling two new points by stepping inward along the axis from each side. Because it was our aim to have some specificity in the level of representation we were targeting, we sought to fix the feature correlations between all of the pairs in the lower and middle layers (i.e., the bottom eight layers) at 0.25. Altogether, our stimulus synthesis procedure was composed of three phases, described below: (1) endpoint channel selection, (2) image initialization, and (3) correlation tuning (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>The purpose of the endpoint channel selection phase was to select higher-layer feature channels that, when optimized, were maximally different from one another. To do this, we generated an optimized image (<xref ref-type="bibr" rid="bib17">Deepdreaming with tensorflow, 2021</xref>) that maximally expressed each of the 128 feature channels in layer mixed 4E (yielding 128 optimized images). We fed these images back through the network, and extracted the pattern of activation in the top four selected layers for each image. We then computed Pearson correlations among the extracted features and selected the 16 optimized channel images whose activation patterns were least inter-correlated with one another. These 16 channels were formed into eight pairs, which became the endpoint channels for a spectrum of image pairs (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>).</p><p>During image initialization, the endpoint channels served as the starting point for generating sets of image pairs with linearly increasing visual similarity. For every pair of endpoint channels, eight image pairs were synthesized, varying in intended higher-layer feature correlation from 0 to 1. These are also referred to as model similarity levels 1 through 8. Every AB image pair began with two randomly generated visual noise arrays, and an ‘endpoint’ was assigned to each — for example, channel 17 to image A and channel 85 to image B. If optimizing image A, channel 17 was always maximally optimized, while the weighting for the optimization of channel 85 depended on the intended correlation. For example, if the intended correlation was 0.14, channel 85 was weighted at 0.14. If optimizing image B for a correlation of 0.14, channel 85 was maximally optimized and channel 17’s optimization was weighted at 0.14. These two weighted channel optimizations were added together, and served as the cost function for gradient descent in this phase (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). On each iteration, the gradient of the cost function was computed with respect to the input pixels. In this way, the pixels were updated at each iteration, working toward this weighted image initialization objective. Eight different AB image pairs were optimized for each of the eight pairs of endpoints, for a total of 64 image pairs (128 images in total). After 200 iterations, the resulting images were fed forward into the correlation tuning phase.</p><p>The correlation tuning phase more directly and precisely targeted correlation values. On each iteration, the pattern of activations to image A and image B were extracted from every layer of interest, and a correlation was computed between image A’s pattern and image B’s pattern (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). The cost function for this phase was the squared difference between the current iteration’s correlation, and the intended correlation. Our aim was to equate the image pairs in terms of their similarity at lower layers, so the intended correlation for the first eight layers was always 0.25. For the highest four layers, the intended correlation varied from 0 to 1. Similar to the endpoint initialization phase, the gradient of the new cost function was computed with respect to the input pixels, and so the images iteratively stepped toward exhibiting the exact feature correlation properties specified. After 200 iterations, we were left with eight pairs of images, whose correlations (i.e. similarity in higher-order visual features) varied linearly from 0 to 1 (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>, right side).</p><p>Feature channel optimization tends to favor the expression of high frequency edges. To circumvent this, as in Deepdream (<xref ref-type="bibr" rid="bib61">Mordvintsev et al., 2015</xref>), we utilized Laplacian pyramid regularization to smooth the image and allow low-frequency features and richer color to be expressed. Also, we wanted to ensure that the final feature correlations were not driven by eccentric pixels in the image, and that the feature correlation was reasonably well represented in various parts of the images and at various scales. To accomplish this, we performed the entire 400 iteration optimization procedure three times, magnifying the image by 40% for each volley. We also computed the gradient over a subset of the input pixels, which were selected using a moving window slightly smaller than the image, in the center of the image. As a result of this procedure, pixels toward the outside of the image were not updated as often. We then cropped the images such that pixels that had been iterated over very few times were removed. Although this was intended to avoid feature correlations driven by the periphery, it had the added benefit of giving the images an irregular ragged edge, rather than a sharp square frame which can make the images appear more homogenous. However, this cropping procedure did remove some pixels that were contributing in part to the assigned correlation value. For this reason, the final inter-image correlations are closely related to but do not <italic>exactly</italic> match the assigned values.</p></sec><sec id="s4-3"><title>Model validation</title><p>We produced a large set of image pairs using the stimulus synthesis approach detailed above. For each of the eight sets of image endpoints, we generated a pair of images at each of eight model similarity levels, yielding a total of 128 (<inline-formula><mml:math id="inf6"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>) images. Each image was then fed back through the network, resulting in a pattern of activity across units in relevant layers that was correlated with the pattern from its pairmate. We also fed these images through a second commonly used DNN, VGG19 (<xref ref-type="bibr" rid="bib76">Simonyan and Zisserman, 2014</xref>), and applied the same procedure. This was done to verify that the feature overlaps we set out to establish were not specific to one model architecture. In the selected layers of both networks, we computed second-order correlations between the intended and actual image-pair correlations. In the network used to synthesize the image pairs, we averaged these second-order correlations in each of the top four target layers, to provide an estimate of the extent to which the synthesis procedure was effective. Despite varying in similarity in the target layers, the differences in similarity in the lower and middle layers (intended to be uniformly <italic>r</italic> = 0.25) should be minimal. To confirm this, we calculated differences between the actual correlations across image pairs, as well as the standard deviation of these differences (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Second-order correlations and standard deviations were also computed for each layer in VGG19, the alternate architecture (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p></sec><sec id="s4-4"><title>Behavioral validation</title><p>Online participants consented to participate and then were provided with task instructions (see next section). The most critical instruction was as follows: 'It will be your job to drag and drop those images into the arena, and arrange them so that the more visually similar items are placed closer together, and the more dissimilar items are placed farther apart'. After confirming that they understood, participants proceeded to the task. At the start of each trial, they clicked a button labeled ‘‘Start trial’’. They were then shown a black screen with a white outlined circle that defined the arena. The circle was surrounded by either 24 (20% of trials) or 26 images (80%), pseudo-randomly selected from the broader set of 128 images (eight pairs from each of 8 sets of endpoints; 64 pairs). Sets were selected such that there were no duplicates, the two images from a given target pair were always presented together, and every image was presented at least twice. Trials were self-paced, but could not last more than 5 min each. Warnings were provided in orange text and then red text when there was 60 and 30 s remaining, respectively. There was no minimum time, except that a trial could not be completed unless every image had been placed. This timing structure ensured that we would get at least 10 trials per subject in no longer than 50 min. On the right side of the arena, there was a button labeled ‘‘Click here when finished’’. If this button was clicked prior to placing all of the images, a warning appeared, ‘‘You have not placed all of the images’’. If all images had been placed, additional buttons appeared, giving the participant the option to confirm completion (‘‘Are you sure? Click here to confirm.’’) or return to sorting (‘‘…or here to go back’’). We used the coordinates of the final placement location of each image to compute pairwise Euclidean distances between images (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). We averaged across trials within participant to get one distance metric for each image pair for each participant. We then computed the Pearson correlation between the model similarity level (1 through 8) defined in the stimulus synthesis procedure and the distance between the images based on each participant’s placements. We also averaged Euclidian distances across participants for each of the 64 image pairs, and then computed a Pearson correlation between model similarity level and these group-averaged distances (<xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p></sec><sec id="s4-5"><title>Arrangement task instructions</title><sec id="s4-5-1"><title>Instructions (1/5)</title><p>Thank you for signing up!</p><p>In this experiment, you will be using the mouse to click and drag images around the screen. At the beginning, you will click the ‘Start trial’ button in the top left corner of your browser window, and a set of images will appear, surrounding the border of a white circular arena. It will be your job to drag and drop those images into the arena, and arrange them so that the more visually similar items are placed closer together, and the more dissimilar items are placed farther apart.</p></sec><sec id="s4-5-2"><title>Instructions (2/5)</title><p>You can move each image as many times as you’d like to make sure that the arrangement corresponds to the visual similarity. The images you will be viewing will be abstract in nature, and will not be animals, but the following example should clarify the instructions:</p><p>If you had moved images of a wolf, a coyote, and a husky into the arena, you might think that they look quite similar to one another, and therefore place them close together. However, if the next image you pulled in was a second image of a husky, you may need to adjust your previous placements so that the two huskies are closer together than a husky and a wolf.</p></sec><sec id="s4-5-3"><title>Instructions (3/5)</title><p>You may find that some clusters of similar items immediately pop out to you. Once you have a set of several of these somewhat similar items grouped together, you might notice more fine-grained differences between them. Please make sure that you take the time to tinker and fine-tune in these situations. The differences within these clusters is just as important.</p><p>If two images are exactly the same, they should be placed on top of one another, and if they are ALMOST exactly the same, feel free to overlap one image with the other. By that same logic, if images are totally dissimilar, place them quite far apart, as far as on opposite sides of the arena.</p><p>Depending on your screen resolution and zoom settings, the entire circle may not initially be in your field of view. This is okay. Feel free to scroll around and navigate the entire space as you arrange the images. However, it is important that the individual images are large enough that you can make out their details.</p></sec><sec id="s4-5-4"><title>Instructions (4/5)</title><p>There will be multiple trials to complete, each with its own set of images. You will have a maximum of 5 min to complete each trial. When there is one minute remaining, an orange message will appear in the center of the circle to inform you of this. You will receive another message, this time in red, when there are 30 s remaining. When time runs out, your arrangement will be saved, and the next trial will be prepared. If you are satisfied with your arrangement before 3 min has elapsed, you can submit it using the ‘Click here when finished’ button. This will bring up two buttons; one to confirm, and one to go back to sorting. You will not be able to submit your arrangement unless you have placed every image.</p><p>When the trial ends, whether it was because you submitted your response, or because time ran out, the screen will be reset, revealing a new set of images and empty arena.</p><p>The task will take just under an hour to complete, no matter how quickly or how slowly you complete each trial, so there is no benefit to rushing.</p></sec><sec id="s4-5-5"><title>Instructions (5/5)</title><p>We would like to thank you for taking the time to participate in our research. The information that you provide during this task is very valuable to us, and will be extremely helpful in developing our research. With this in mind, we ask that you really pay attention to the details of the images, and complete each trial and arrangement carefully and conscientiously.</p><p>[printed in red text] We would also like to remind you that because there is a set time limit, not a set number of trials, there is no benefit to rushing through the trials. In general, we have found that it tends to take 3.5 min at minimum to complete each trial accurately.</p></sec></sec><sec id="s4-6"><title>fMRI design</title><p>Each functional task run lasted 304.5 s and consisted of viewing a series of synthesized abstract images, one at a time. Images were presented for 1 s each. The first image onset occurred after 6 s, and each subsequent onset was presented after an ISI of 1, 3, or 5 s (40:40:20 ratio). There were eight pairs, meaning that there were 16 unique images. The order of image presentations was pseudo-randomly assigned in one of two ways. In the first and last (pre- and post-learning) templating runs, the 16 images were presented in a random order for the first 16 trials. For the next 16 trials, the 16 images were presented in a different random order, with the constraint that the same image not be presented twice in a row. This same procedure was repeated until there were 80 total trials (five presentations of each of image). In the six intervening statistical learning runs, image pairs were always presented intact and in the same A to B order. In these runs, the eight pairs were presented in a random order for the first 16 trials. For the next 16 trials, the eight pairs were presented in a different order, with the constraint that the same pair could not be presented twice in a row, and so on. Critically, the images appeared continuously without segmentation cues between pairs, such that participants had to learn transition probabilities in the sequence (i.e. for pair AB, the higher probability of A transitioning to B than B transitioning to any number of other images). One out of every 10 trials was randomly assigned to have a small, partially transparent grey patch overlaid on the image. The participants performed a cover task of pressing a button on a handheld button box when they saw the grey square. This task was designed to encourage participants to maintain attention on the images, but was completely orthogonal to the pair structure.</p></sec><sec id="s4-7"><title>Data acquisition</title><p>Data were acquired using a 3T Siemens Prisma scanner with a 64-channel head coil at the Yale Magnetic Resonance Research Center. We collected eight functional runs with a with a multiband echo-planar imaging (EPI) sequence (TR = 1500 ms; TE=32.6 ms; voxel size=1.5mm isotropic; FA=<inline-formula><mml:math id="inf7"><mml:mrow><mml:mn>71</mml:mn><mml:mi/><mml:mi mathvariant="normal">°</mml:mi></mml:mrow></mml:math></inline-formula>; multiband factor=6), yielding 90 axial slices. Each run contained 203 volumes. For field map correction, two spin-echo field map volumes (TR = 8000; TE=66) were acquired in opposite phase encoding directions. These otherwise matched the parameters of our functional acquisitions. We also collected a T1-weighted magnetization prepared rapid gradient echo (MPRAGE) image (TR = 2300 ms; TE=2.27 ms; voxel size=1 mm isotropic; FA=<inline-formula><mml:math id="inf8"><mml:mrow><mml:mn>8</mml:mn><mml:mi/><mml:mi mathvariant="normal">°</mml:mi></mml:mrow></mml:math></inline-formula>; 192 sagittal slices; GRAPPA acceleration factor=3), and a T2-weighted turbo spin-echo (TSE) image (TR = 11390 ms; TE=90 ms; voxel size=<inline-formula><mml:math id="inf9"><mml:mrow><mml:mn>0.44</mml:mn><mml:mo>×</mml:mo><mml:mn>0.44</mml:mn><mml:mo>×</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula> mm; FA=<inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>150</mml:mn><mml:mi/><mml:mi mathvariant="normal">°</mml:mi></mml:mrow></mml:math></inline-formula>; 54 coronal slices; perpendicular to the long axis of the hippocampus; distance factor=20%).</p></sec><sec id="s4-8"><title>fMRI preprocessing</title><p>For each functional run, preprocessing was performed using the FEAT tool in FSL (<xref ref-type="bibr" rid="bib89">Woolrich et al., 2001</xref>). Data were brain-extracted, corrected for slice timing, high-pass filtered (100 s cutoff), aligned to the middle functional volume of the run using MCFLIRT (<xref ref-type="bibr" rid="bib40">Jenkinson et al., 2002</xref>), and spatially smoothed (3 mm). FSL’s topup tool (<xref ref-type="bibr" rid="bib77">Smith et al., 2004</xref>), in conjunction with the two field maps, was used to estimate susceptibility-induced distortions. The output was converted to radians and used to perform fieldmap correction in FEAT. The functional runs were also aligned to both the participants’ T1-weighted anatomical image using boundary-based registration, and to MNI standard space with 12 degrees of freedom, using FLIRT (<xref ref-type="bibr" rid="bib41">Jenkinson and Smith, 2001</xref>). Analyses within a single run were conducted in native space. For comparisons across participants, analyses were conducted in standard space.</p></sec><sec id="s4-9"><title>Defining regions of interest</title><p>For each participant, their T1- and T2-weighted anatomical images were submitted to the automatic segmentation of hippocampal subfields (ASHS) software package (<xref ref-type="bibr" rid="bib91">Yushkevich et al., 2015</xref>), to derive participant-specific medial temporal lobe regions of interest. We used an atlas containing 51 manual segmentations of hippocampal subfields (<xref ref-type="bibr" rid="bib1">Aly and Turk-Browne, 2016a</xref>; <xref ref-type="bibr" rid="bib2">Aly and Turk-Browne, 2016b</xref>). The resulting automated segmentations were used to create masks for the CA1, CA2/3, and dentate gyrus (DG) subfields. For visual ROIs, freesurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>) was used to create masks for V1, V2, lateral occipital (LO) cortex, fusiform gyrus (FG), parahippocampal cortex (PHC), and inferior temporal (IT) cortex, for each participant.</p></sec><sec id="s4-10"><title>General linear model</title><p>For the pre- and post-learning templating runs, a regressor was developed for each of the 16 unique synthesized images. This was done by placing a delta function at each image onset and convolving this time course with the double-gamma hemodynamic response function. We then used these 16 regressors to fit a GLM to the time course of BOLD activity using FSL’s FILM tool, correcting for local autocorrelation (<xref ref-type="bibr" rid="bib89">Woolrich et al., 2001</xref>). This yielded parameter estimates for each of the 16 images, which were used for subsequent analyses.</p></sec><sec id="s4-11"><title>Stimulus synthesis validation analyses</title><p>The pre-learning templating run was analyzed to derive an estimate of the baseline representational similarity among the eight target pairs before any learning had taken place. For ROI analyses, the 16 parameter estimates output by the GLM (one per stimulus) were extracted for each voxel in a given ROI and vectorized to obtain the multivoxel pattern of activity for each stimulus. We then computed the Pearson correlation between the two vectors corresponding to the pairmates in each of the eight target image pairs. This yielded eight representational similarity values, one for each image pair. As established through model-based synthesis, each of the eight image pairs also had a corresponding model similarity level. We computed and Fisher transformed the second-order correlation of neural and model similarity across levels. In other words, this analysis tested whether the pattern of similarity built in to the image pairs through the DNN model corresponded to the representational similarity in a given brain region. We constructed 95% confidence intervals (CIs) for estimates of model-brain correspondence for each ROI by bootstrap resampling of participants 50,000 times. As an additional control, we compared the true group average correlation value to a noise distribution, wherein A and B images were paired randomly 50,000 times, obliterating any systematic similarity relationships among them. We did not, however, constrain the random pairing to exclude the true image pairings. This means that the control is especially conservative because some of the resulting shuffled pairs contained the true image pairings. This analysis was used to ensure that the true effect would not be likely to occur due to random noise. In addition to ROI analyses, we also conducted exploratory searchlight analyses over the whole brain. This involved repeating the same representational similarity analyses but over patterns defined from 125-voxel searchlights (radius = 2) centered on every brain voxel. The resulting whole-brain statistical maps were then Fisher transformed, concatenated, and tested for reliability at the group level using FSL’s randomise (<xref ref-type="bibr" rid="bib87">Winkler et al., 2014</xref>). We used a cluster-forming threshold of <italic>t</italic> = 2.33 (<italic>p</italic> &lt; .01, one-tailed) in cluster mass, and corrected for multiple comparisons using the null distribution of maximum cluster mass. Clusters that survived at (p &lt; 0.05) were retained.</p></sec><sec id="s4-12"><title>Representational change analyses</title><p>To test the NMPH, we measured whether learning-related changes in pairmate representational similarity (i.e. changes from pre- to post-statistical-learning) followed a U-shaped, cubic function of model similarity. This involved measuring the degree to which the image pairs at each level of similarity showed integration (i.e. increased representational similarity) or differentiation (i.e. decreased representational similarity). For ROI analyses, the 16 parameter estimates output by the GLM were extracted for each voxel in a given ROI and vectorized. This procedure was performed for both pre- and post-learning templating runs. In each run, Pearson correlations were computed between the vectors for each of the target image pairs, yielding eight representational similarity values. To measure learning-related changes in representational similarity, pre-learning values were subtracted from post-learning values. A positive value on this metric signifies integration, whereas a negative value signifies differentiation. Our predictions were not about any one model similarity level, but rather about a specific nonmonotonic relationship between model similarity and representational change. This hypothesis can be quantified using a cubic model, with the leading coefficient constrained to be positive. To test the efficacy of this model in each MTL ROI, we computed a cross-validated estimate of how well this model predicted the true data. Specifically, we fit the constrained cubic model to the data from all but one held-out participant. We then used the model to predict the held-out participant’s data, and computed a correlation between the predicted and actual data. This procedure was repeated such that every participant was held out once, and the resulting correlations were averaged into an estimate of the fit for that ROI. We then constructed 95% confidence intervals (CIs) for estimates of the model fit for each ROI by bootstrap resampling participants 50,000 times. To produce a noise distribution, we randomly re-paired A and B images 50,000 times and repeated the above analysis with our entire sample for each repairing. We compared the true group average model fit statistic to this noise distribution for each ROI. Lastly, we conducted an exploratory searchlight analysis, repeating the analysis above in 125-voxel searchlights (radius = 2) centered on every brain voxel. As in the model similarity searchlight, all subjects’ outputs were then Fisher transformed, concatenated, submitted to randomise, thresholded, and corrected for maximum cluster mass.</p></sec><sec id="s4-13"><title>Quantification and statistical analysis</title><p>No statistical methods were used to predetermine sample size, but we aimed to collect at least 36 participants for the primary learning analysis. For all analyses, we used bootstrap resampling methods to analyze our results non-parametrically. Details of these analyses, as well as exact results of statistical tests, 95% confidence intervals, and p values with respect to noise distributions, are reported alongside each analysis in our Results. Statistical significance was set at p &lt; 0.05 unless otherwise specified.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank L Rait for help with recruitment, scheduling, and data collection. This work was supported by NSERC PDF and SSHRC Banting PDF (JDW), NIH R01 MH069456 (KAN and NBT-B), and the Canadian Institute for Advanced Research (NBT-B).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants provided informed consent, and all research was conducted under a protocol approved by Yale University's Institutional Review Board (IRB Protocol ID: 2000022976).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-68344-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>fMRI data are available on Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.t4b8gtj38">https://doi.org/10.5061/dryad.t4b8gtj38</ext-link>). Analysis code and synthesized image stimuli are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/thelamplab/differint">https://github.com/thelamplab/differint</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:ed212e5bb9bbba8645a4fb3d4152db4030656202">https://archive.softwareheritage.org/swh:1:rev:ed212e5bb9bbba8645a4fb3d4152db4030656202</ext-link>). Further information and requests for resources should be directed to and will be fulfilled by the Lead Contact, Jeffrey Wammes (jeffrey.wammes@queensu.ca).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Wammes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Increasing stimulus similarity drives nonmonotonic representational change in hippocampus</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.t4b8gtj38</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aly</surname> <given-names>M</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Attention stabilizes representations in the human Hippocampus</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>783</fpage><lpage>796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv041</pub-id><pub-id pub-id-type="pmid">25766839</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aly</surname> <given-names>M</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Attention promotes episodic encoding by stabilizing hippocampal representations</article-title><source>PNAS</source><volume>113</volume><fpage>E420</fpage><lpage>E429</lpage><pub-id pub-id-type="doi">10.1073/pnas.1518931113</pub-id><pub-id pub-id-type="pmid">26755611</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>MI</given-names></name><name><surname>Jeffery</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Heterogeneous modulation of place cell firing by changes in context</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>8827</fpage><lpage>8835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-26-08827.2003</pub-id><pub-id pub-id-type="pmid">14523083</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Mizumori</surname> <given-names>SJ</given-names></name><name><surname>Leonard</surname> <given-names>BW</given-names></name><name><surname>Lin</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Comparison of spatial and temporal characteristics of neuronal activity in sequential stages of hippocampal processing</article-title><source>Progress in Brain Research</source><volume>83</volume><fpage>287</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/s0079-6123(08)61257-1</pub-id><pub-id pub-id-type="pmid">2392566</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname> <given-names>P</given-names></name><name><surname>Kar</surname> <given-names>K</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><volume>364</volume><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bear</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bidirectional synaptic plasticity: from theory to reality</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>358</volume><fpage>649</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1255</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berron</surname> <given-names>D</given-names></name><name><surname>Schütze</surname> <given-names>H</given-names></name><name><surname>Maass</surname> <given-names>A</given-names></name><name><surname>Cardenas-Blanco</surname> <given-names>A</given-names></name><name><surname>Kuijf</surname> <given-names>HJ</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Düzel</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Strong evidence for pattern separation in human dentate gyrus</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>7569</fpage><lpage>7579</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0518-16.2016</pub-id><pub-id pub-id-type="pmid">27445136</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienenstock</surname> <given-names>EL</given-names></name><name><surname>Cooper</surname> <given-names>LN</given-names></name><name><surname>Munro</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>32</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-01-00032.1982</pub-id><pub-id pub-id-type="pmid">7054394</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname> <given-names>IK</given-names></name><name><surname>Robin</surname> <given-names>J</given-names></name><name><surname>Olsen</surname> <given-names>RK</given-names></name><name><surname>Moscovitch</surname> <given-names>M</given-names></name><name><surname>Barense</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Integration and differentiation of hippocampal memory traces</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>118</volume><fpage>196</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.07.024</pub-id><pub-id pub-id-type="pmid">32712280</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname> <given-names>DV</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical plasticity: from synapses to maps</article-title><source>Annual Review of Neuroscience</source><volume>21</volume><fpage>149</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.21.1.149</pub-id><pub-id pub-id-type="pmid">9530495</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Pinto</surname> <given-names>N</given-names></name><name><surname>Ardila</surname> <given-names>D</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caporale</surname> <given-names>N</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spike Timing–Dependent Plasticity: A Hebbian Learning Rule</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>25</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125639</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanales</surname> <given-names>AJH</given-names></name><name><surname>Oza</surname> <given-names>A</given-names></name><name><surname>Favila</surname> <given-names>SE</given-names></name><name><surname>Kuhl</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Overlap among spatial memories triggers repulsion of hippocampal representations</article-title><source>Current Biology : CB</source><volume>27</volume><fpage>2307</fpage><lpage>2317</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.057</pub-id><pub-id pub-id-type="pmid">28736170</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colgin</surname> <given-names>LL</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Understanding memory through hippocampal remapping</article-title><source>Trends in Neurosciences</source><volume>31</volume><fpage>469</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.06.008</pub-id><pub-id pub-id-type="pmid">18687478</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collin</surname> <given-names>SH</given-names></name><name><surname>Milivojevic</surname> <given-names>B</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Memory hierarchies map onto the hippocampal long Axis in humans</article-title><source>Nature Neuroscience</source><volume>18</volume><elocation-id>1562</elocation-id><pub-id pub-id-type="doi">10.1038/nn.4138</pub-id><pub-id pub-id-type="pmid">26479587</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Deepdreaming with tensorflow</collab></person-group><year iso-8601-date="2021">2021</year><source>Github</source><version designator="01">01</version><ext-link ext-link-type="uri" xlink:href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Dong</surname> <given-names>W</given-names></name><name><surname>Socher</surname> <given-names>R</given-names></name><name><surname>Li</surname> <given-names>L-J</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imagenet: a Large-Scale hierarchical image database</article-title><conf-name>2009 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Detre</surname> <given-names>GJ</given-names></name><name><surname>Natarajan</surname> <given-names>A</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Moderate levels of activation lead to forgetting in the think/no-think paradigm</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>2371</fpage><lpage>2388</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.02.017</pub-id><pub-id pub-id-type="pmid">23499722</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuker</surname> <given-names>L</given-names></name><name><surname>Bellmund</surname> <given-names>JL</given-names></name><name><surname>Navarro Schröder</surname> <given-names>T</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An event map of memory space in the Hippocampus</article-title><source>eLife</source><volume>5</volume><elocation-id>e16534</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16534</pub-id><pub-id pub-id-type="pmid">27710766</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimsdale-Zucker</surname> <given-names>HR</given-names></name><name><surname>Ritchey</surname> <given-names>M</given-names></name><name><surname>Ekstrom</surname> <given-names>AD</given-names></name><name><surname>Yonelinas</surname> <given-names>AP</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CA1 and CA3 differentially support spontaneous retrieval of episodic contexts within human hippocampal subfields</article-title><source>Nature Communications</source><volume>9</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41467-017-02752-1</pub-id><pub-id pub-id-type="pmid">29348512</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuBrow</surname> <given-names>S</given-names></name><name><surname>Rouhani</surname> <given-names>N</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Does mental context drift or shift?</article-title><source>Current Opinion in Behavioral Sciences</source><volume>17</volume><fpage>141</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2017.08.003</pub-id><pub-id pub-id-type="pmid">29335678</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>KD</given-names></name><name><surname>Schlichting</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hippocampal representations as a function of time, subregion, and brain state</article-title><source>Neurobiology of Learning and Memory</source><volume>153</volume><fpage>40</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2018.03.006</pub-id><pub-id pub-id-type="pmid">29535044</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Time cells in the Hippocampus: a new dimension for mapping memories</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>732</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1038/nrn3827</pub-id><pub-id pub-id-type="pmid">25269553</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickenberg</surname> <given-names>M</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing it all: convolutional network layers map the function of the human visual system</article-title><source>NeuroImage</source><volume>152</volume><fpage>184</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.001</pub-id><pub-id pub-id-type="pmid">27777172</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname> <given-names>SE</given-names></name><name><surname>Chanales</surname> <given-names>AJ</given-names></name><name><surname>Kuhl</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-dependent hippocampal pattern differentiation prevents interference during subsequent learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11066</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11066</pub-id><pub-id pub-id-type="pmid">27925613</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Synaptic mechanisms for plasticity in neocortex</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>33</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135516</pub-id><pub-id pub-id-type="pmid">19400721</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>D</given-names></name><name><surname>Montemurro</surname> <given-names>MA</given-names></name><name><surname>Montaldi</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pattern separation underpins Expectation-Modulated memory</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3455</fpage><lpage>3464</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2047-19.2020</pub-id><pub-id pub-id-type="pmid">32161140</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname> <given-names>CD</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Top-down influences on visual processing</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>350</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id><pub-id pub-id-type="pmid">23595013</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>GoodSmith</surname> <given-names>D</given-names></name><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Wang</surname> <given-names>C</given-names></name><name><surname>Kim</surname> <given-names>SH</given-names></name><name><surname>Song</surname> <given-names>H</given-names></name><name><surname>Burgalossi</surname> <given-names>A</given-names></name><name><surname>Christian</surname> <given-names>KM</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial representations of granule cells and mossy cells of the dentate gyrus</article-title><source>Neuron</source><volume>93</volume><fpage>677</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.026</pub-id><pub-id pub-id-type="pmid">28132828</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzowski</surname> <given-names>JF</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Ensemble dynamics of hippocampal regions CA3 and CA1</article-title><source>Neuron</source><volume>44</volume><fpage>581</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.11.003</pub-id><pub-id pub-id-type="pmid">15541306</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname> <given-names>ME</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Hippocampal mechanisms for the context-dependent retrieval of episodes</article-title><source>Neural Networks</source><volume>18</volume><fpage>1172</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2005.08.007</pub-id><pub-id pub-id-type="pmid">16263240</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname> <given-names>ME</given-names></name><name><surname>Wyble</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Free recall and recognition in a network model of the Hippocampus: simulating effects of scopolamine on human memory function</article-title><source>Behavioural Brain Research</source><volume>89</volume><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/s0166-4328(97)00048-x</pub-id><pub-id pub-id-type="pmid">9475612</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname> <given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><source>The Organization of Behavior: A Neuropsychological Theory</source><publisher-loc>New York</publisher-loc><publisher-name>J. Wiley &amp; Sons, Inc</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>MW</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The Hippocampus, time, and memory across scales</article-title><source>Journal of Experimental Psychology: General</source><volume>142</volume><fpage>1211</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1037/a0033621</pub-id><pub-id pub-id-type="pmid">23915126</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsieh</surname> <given-names>LT</given-names></name><name><surname>Gruber</surname> <given-names>MJ</given-names></name><name><surname>Jenkins</surname> <given-names>LJ</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal activity patterns carry information about objects in temporal context</article-title><source>Neuron</source><volume>81</volume><fpage>1165</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.015</pub-id><pub-id pub-id-type="pmid">24607234</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huffman</surname> <given-names>DJ</given-names></name><name><surname>Stark</surname> <given-names>CEL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The influence of low-level stimulus features on the representation of contexts, items, and their mnemonic associations</article-title><source>NeuroImage</source><volume>155</volume><fpage>513</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.019</pub-id><pub-id pub-id-type="pmid">28400264</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulbert</surname> <given-names>JC</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural differentiation tracks improved recall of competing memories following interleaved study and retrieval practice</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3994</fpage><lpage>4008</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu284</pub-id><pub-id pub-id-type="pmid">25477369</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Bannister</surname> <given-names>P</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A global optimisation method for robust affine registration of brain images</article-title><source>Medical Image Analysis</source><volume>5</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/S1361-8415(01)00036-6</pub-id><pub-id pub-id-type="pmid">11516708</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>JD</given-names></name><name><surname>McDuff</surname> <given-names>SG</given-names></name><name><surname>Rugg</surname> <given-names>MD</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Recollection, familiarity, and cortical reinstatement: a multivoxel pattern analysis</article-title><source>Neuron</source><volume>63</volume><fpage>697</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.011</pub-id><pub-id pub-id-type="pmid">19755111</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jozwik</surname> <given-names>K</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep convolutional neural networks, features, and categories perform similarly at explaining primate High-Level visual representations</article-title><conf-name>2018 Conference on Cognitive Computational Neuroscience</conf-name><pub-id pub-id-type="doi">10.32470/CCN.2018.1232-0</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>G</given-names></name><name><surname>Lewis-Peacock</surname> <given-names>JA</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pruning of memories by context-based prediction error</article-title><source>PNAS</source><volume>111</volume><fpage>8997</fpage><lpage>9002</lpage><pub-id pub-id-type="doi">10.1073/pnas.1319438111</pub-id><pub-id pub-id-type="pmid">24889631</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>G</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural differentiation of incorrectly predicted memories</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>2022</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3272-16.2017</pub-id><pub-id pub-id-type="pmid">28115478</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkham</surname> <given-names>NZ</given-names></name><name><surname>Slemmer</surname> <given-names>JA</given-names></name><name><surname>Johnson</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual statistical learning in infancy: evidence for a domain general learning mechanism</article-title><source>Cognition</source><volume>83</volume><fpage>B35</fpage><lpage>B42</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(02)00004-5</pub-id><pub-id pub-id-type="pmid">11869728</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Relating population-code representations between man, monkey, and computational models</article-title><source>Frontiers in Neuroscience</source><volume>3</volume><fpage>363</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.035.2009</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inverse MDS: inferring dissimilarity structure from multiple item arrangements</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>245</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00245</pub-id><pub-id pub-id-type="pmid">22848204</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep neural networks as a computational model for human shape sensitivity</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004896</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id><pub-id pub-id-type="pmid">27124699</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavenex</surname> <given-names>P</given-names></name><name><surname>Amaral</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Hippocampal-neocortical interaction: a hierarchy of associativity</article-title><source>Hippocampus</source><volume>10</volume><fpage>420</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1002/1098-1063(2000)10:4&lt;420::AID-HIPO8&gt;3.0.CO;2-5</pub-id><pub-id pub-id-type="pmid">10985281</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname> <given-names>JK</given-names></name><name><surname>Leutgeb</surname> <given-names>S</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Pattern separation in the dentate gyrus and CA3 of the Hippocampus</article-title><source>Science</source><volume>315</volume><fpage>961</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1126/science.1135801</pub-id><pub-id pub-id-type="pmid">17303747</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>W</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Urtasun</surname> <given-names>R</given-names></name><name><surname>Zemel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Advances in Neural Information Processing Systems</source><publisher-loc>Massachusetts, United States</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>Y</given-names></name><name><surname>Zhao</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning creates novel object associations via transitive relations</article-title><source>Psychological Science</source><volume>29</volume><fpage>1207</fpage><lpage>1220</lpage><pub-id pub-id-type="doi">10.1177/0956797618762400</pub-id><pub-id pub-id-type="pmid">29787352</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>CB</given-names></name><name><surname>Douglas</surname> <given-names>D</given-names></name><name><surname>Newsome</surname> <given-names>RN</given-names></name><name><surname>Man</surname> <given-names>LL</given-names></name><name><surname>Barense</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title><source>eLife</source><volume>7</volume><elocation-id>e31873</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31873</pub-id><pub-id pub-id-type="pmid">29393853</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKenzie</surname> <given-names>S</given-names></name><name><surname>Frank</surname> <given-names>AJ</given-names></name><name><surname>Kinsky</surname> <given-names>NR</given-names></name><name><surname>Porter</surname> <given-names>B</given-names></name><name><surname>Rivière</surname> <given-names>PD</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal representation of related and opposing memories develop within distinct, hierarchically organized neural schemas</article-title><source>Neuron</source><volume>83</volume><fpage>202</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.019</pub-id><pub-id pub-id-type="pmid">24910078</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Morris</surname> <given-names>RGM</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title><source>Trends in Neurosciences</source><volume>10</volume><fpage>408</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(87)90011-7</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milivojevic</surname> <given-names>B</given-names></name><name><surname>Vicente-Grabovetsky</surname> <given-names>A</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Insight reconfigures hippocampal-prefrontal memories</article-title><source>Current Biology</source><volume>25</volume><fpage>821</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.033</pub-id><pub-id pub-id-type="pmid">25728693</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molitor</surname> <given-names>RJ</given-names></name><name><surname>Sherrill</surname> <given-names>KR</given-names></name><name><surname>Morton</surname> <given-names>NW</given-names></name><name><surname>Miller</surname> <given-names>AA</given-names></name><name><surname>Preston</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Memory reactivation during learning simultaneously promotes dentate gyrus/CA<sub>2,3</sub>Pattern Differentiation and CA<sub>1</sub>Memory Integration</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>726</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0394-20.2020</pub-id><pub-id pub-id-type="pmid">33239402</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mordvintsev</surname> <given-names>A</given-names></name><name><surname>Olah</surname> <given-names>C</given-names></name><name><surname>Tyka</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deepdream-a code example for visualizing neural networks</article-title><source>Google Research</source><volume>2</volume><elocation-id>25</elocation-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neunuebel</surname> <given-names>JP</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>CA3 retrieves coherent representations from degraded input: direct evidence for CA3 pattern completion and dentate gyrus pattern separation</article-title><source>Neuron</source><volume>81</volume><fpage>416</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.017</pub-id><pub-id pub-id-type="pmid">24462102</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newman</surname> <given-names>EL</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Moderate excitation leads to weakening of perceptual representations</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2760</fpage><lpage>2770</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq021</pub-id><pub-id pub-id-type="pmid">20181622</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>A</given-names></name><name><surname>Yosinski</surname> <given-names>J</given-names></name><name><surname>Clune</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks are easily fooled: high confidence predictions for unrecognizable images</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>427</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298640</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olson</surname> <given-names>IR</given-names></name><name><surname>Plotzker</surname> <given-names>A</given-names></name><name><surname>Ezzyat</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The enigmatic temporal pole: a review of findings on social and emotional processing</article-title><source>Brain</source><volume>130</volume><fpage>1718</fpage><lpage>1731</lpage><pub-id pub-id-type="doi">10.1093/brain/awm052</pub-id><pub-id pub-id-type="pmid">17392317</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name><name><surname>Torfs</surname> <given-names>K</given-names></name><name><surname>Wagemans</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perceived shape similarity among unfamiliar objects and the organization of the human object vision pathway</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>10111</fpage><lpage>10123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2511-08.2008</pub-id><pub-id pub-id-type="pmid">18829969</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polyn</surname> <given-names>SM</given-names></name><name><surname>Natu</surname> <given-names>VS</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Category-specific cortical activity precedes retrieval during memory search</article-title><source>Science</source><volume>310</volume><fpage>1963</fpage><lpage>1966</lpage><pub-id pub-id-type="doi">10.1126/science.1117645</pub-id><pub-id pub-id-type="pmid">16373577</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponce</surname> <given-names>CR</given-names></name><name><surname>Xiao</surname> <given-names>W</given-names></name><name><surname>Schade</surname> <given-names>PF</given-names></name><name><surname>Hartmann</surname> <given-names>TS</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evolving images for visual neurons using a deep generative network reveals coding principles and neuronal preferences</article-title><source>Cell</source><volume>177</volume><fpage>999</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.04.005</pub-id><pub-id pub-id-type="pmid">31051108</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritvo</surname> <given-names>VJH</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nonmonotonic plasticity: how memory retrieval drives learning</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>726</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.06.007</pub-id><pub-id pub-id-type="pmid">31358438</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Neural Networks and Brain Function</source><publisher-loc>Oxford</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname> <given-names>AC</given-names></name><name><surname>Kustner</surname> <given-names>LV</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Shaping of object representations in the human medial temporal lobe based on temporal regularities</article-title><source>Current Biology</source><volume>22</volume><fpage>1622</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.06.056</pub-id><pub-id pub-id-type="pmid">22885059</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname> <given-names>AC</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Statistical learning of temporal community structure in the Hippocampus</article-title><source>Hippocampus</source><volume>26</volume><fpage>3</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1002/hipo.22523</pub-id><pub-id pub-id-type="pmid">26332666</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname> <given-names>AC</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Complementary learning systems within the Hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160049</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0049</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlichting</surname> <given-names>ML</given-names></name><name><surname>Zeithamova</surname> <given-names>D</given-names></name><name><surname>Preston</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>CA1 subfield contributions to memory integration and inference</article-title><source>Hippocampus</source><volume>24</volume><fpage>1248</fpage><lpage>1260</lpage><pub-id pub-id-type="doi">10.1002/hipo.22310</pub-id><pub-id pub-id-type="pmid">24888442</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlichting</surname> <given-names>ML</given-names></name><name><surname>Mumford</surname> <given-names>JA</given-names></name><name><surname>Preston</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning-related representational changes reveal dissociable integration and separation signatures in the Hippocampus and prefrontal cortex</article-title><source>Nature Communications</source><volume>6</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/ncomms9151</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Bannister</surname> <given-names>PR</given-names></name><name><surname>De Luca</surname> <given-names>M</given-names></name><name><surname>Drobnjak</surname> <given-names>I</given-names></name><name><surname>Flitney</surname> <given-names>DE</given-names></name><name><surname>Niazy</surname> <given-names>RK</given-names></name><name><surname>Saunders</surname> <given-names>J</given-names></name><name><surname>Vickers</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>De Stefano</surname> <given-names>N</given-names></name><name><surname>Brady</surname> <given-names>JM</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Son</surname> <given-names>G</given-names></name><name><surname>Walther</surname> <given-names>DB</given-names></name><name><surname>Mack</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Scene wheels: measuring perception and memory of real-world scenes with a continuous stimulus space</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.10.09.333708</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>W</given-names></name><name><surname>Jia</surname> <given-names>Y</given-names></name><name><surname>Sermanet</surname> <given-names>P</given-names></name><name><surname>Reed</surname> <given-names>S</given-names></name><name><surname>Anguelov</surname> <given-names>D</given-names></name><name><surname>Rabinovich</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Going deeper with convolutions</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tompary</surname> <given-names>A</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Consolidation promotes the emergence of representational overlap in the Hippocampus and medial prefrontal cortex</article-title><source>Neuron</source><volume>96</volume><fpage>228</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.005</pub-id><pub-id pub-id-type="pmid">28957671</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Jungé</surname> <given-names>J</given-names></name><name><surname>Scholl</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The automaticity of visual statistical learning</article-title><source>Journal of Experimental Psychology: General</source><volume>134</volume><fpage>552</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.134.4.552</pub-id><pub-id pub-id-type="pmid">16316291</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Simon</surname> <given-names>MG</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Scene representations in parahippocampal cortex depend on temporal context</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>7202</fpage><lpage>7207</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0942-12.2012</pub-id><pub-id pub-id-type="pmid">22623664</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vazdarjanova</surname> <given-names>A</given-names></name><name><surname>Guzowski</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Differences in hippocampal neuronal population responses to modifications of an environmental context: evidence for distinct, yet complementary, functions of CA3 and CA1 ensembles</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>6489</fpage><lpage>6496</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0350-04.2004</pub-id><pub-id pub-id-type="pmid">15269259</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>West</surname> <given-names>MJ</given-names></name><name><surname>Slomianka</surname> <given-names>L</given-names></name><name><surname>Gundersen</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Unbiased stereological estimation of the total number of neurons in thesubdivisions of the rat Hippocampus using the optical fractionator</article-title><source>The Anatomical Record</source><volume>231</volume><fpage>482</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1002/ar.1092310411</pub-id><pub-id pub-id-type="pmid">1793176</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimber</surname> <given-names>M</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Charest</surname> <given-names>I</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Anderson</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Retrieval induces adaptive forgetting of competing memories via cortical pattern suppression</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>582</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/nn.3973</pub-id><pub-id pub-id-type="pmid">25774450</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Preference by association: how memory mechanisms in the Hippocampus bias decisions</article-title><source>Science</source><volume>338</volume><fpage>270</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1126/science.1223252</pub-id><pub-id pub-id-type="pmid">23066083</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname> <given-names>AM</given-names></name><name><surname>Ridgway</surname> <given-names>GR</given-names></name><name><surname>Webster</surname> <given-names>MA</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Permutation inference for the general linear model</article-title><source>NeuroImage</source><volume>92</volume><fpage>381</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.01.060</pub-id><pub-id pub-id-type="pmid">24530839</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witter</surname> <given-names>MP</given-names></name><name><surname>Wouterlood</surname> <given-names>FG</given-names></name><name><surname>Naber</surname> <given-names>PA</given-names></name><name><surname>Van Haeften</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Anatomical organization of the parahippocampal-hippocampal network</article-title><source>Annals of the New York Academy of Sciences</source><volume>911</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2000.tb06716.x</pub-id><pub-id pub-id-type="pmid">10911864</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Ripley</surname> <given-names>BD</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporal autocorrelation in Univariate linear modeling of FMRI data</article-title><source>NeuroImage</source><volume>14</volume><fpage>1370</fpage><lpage>1386</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0931</pub-id><pub-id pub-id-type="pmid">11707093</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Seibert</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yushkevich</surname> <given-names>PA</given-names></name><name><surname>Pluta</surname> <given-names>JB</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Xie</surname> <given-names>L</given-names></name><name><surname>Ding</surname> <given-names>SL</given-names></name><name><surname>Gertje</surname> <given-names>EC</given-names></name><name><surname>Mancuso</surname> <given-names>L</given-names></name><name><surname>Kliot</surname> <given-names>D</given-names></name><name><surname>Das</surname> <given-names>SR</given-names></name><name><surname>Wolk</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automated volumetry and regional thickness analysis of hippocampal subfields and medial temporal cortical structures in mild cognitive impairment</article-title><source>Human Brain Mapping</source><volume>36</volume><fpage>258</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1002/hbm.22627</pub-id><pub-id pub-id-type="pmid">25181316</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zeiler</surname> <given-names>MD</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visualizing and understanding convolutional networks</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>818</fpage><lpage>833</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10590-1_53</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68344.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution>University of Toronto</institution><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Brown</surname><given-names>Thackery</given-names> </name><role>Reviewer</role><aff><institution>Georgia Institute of Technology</institution></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.03.13.435275">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.03.13.435275v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper reports a timely, computationally-inspired fMRI analysis of how hippocampus-dependent memory handles overlap in the timing and visual characteristics of objects we encounter. The elegant experimental approach directly tests the predictions of a theoretical framework by parametrically manipulating visual overlap between associated stimuli. Results showed that within the dentate gyrus of the hippocampus, moderate levels of visual feature similarity led to differentiation following a statistical learning paradigm, but higher and lower levels of visual similarity did not. These findings speak to discrepancies in the field over how the hippocampus responds to similarity in memories and will be of broad interest to memory researchers and computational neuroscientists.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Increasing stimulus similarity drives nonmonotonic representational change in hippocampus&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Thackery Brown (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>1. Provide a clearer theoretical rationale for the focus on the dentate gyrus given extant rodent work, per the detailed comments provided by Reviewer 3.</p><p>2. Address the extent to which these findings are dependent on this particular learning paradigm.</p><p>3. Clarify the time course of these effects, in particular as they relate to interpreting the pre- to post-learning results.</p><p>4. Describe extent to which these neural patterns meaningfully relate to behaviour.</p><p>5. Describe the relationship between the effects in the hippocampus and early visual regions.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. Related to point 2 above – it would be helpful to show an analysis of how hippocampal representations change over time. I understand that time is a confound here – representations will drift further from the pre-learning template even with no further learning. However, since all stimuli were presented at all timepoints, adjacent run-to-run similarity values for these pairs could be calculated to establish whether the relative integration/differentiation occurs in a gradual way or as a step function. I leave it up to the authors to decide how to address this, or to provide an explanation for why such an analysis might not be feasible, but this would be helpful in interpreting the pre- to post-learning results.</p><p>2. Related to point 3 above – it would be helpful to provide any behavioural measures such as RT that might hint at whether (and how well) the participants learned the links between paired stimuli. Similarly, across the 6 statistical learning runs, did the authors observe RT facilitation for times when the grey square appeared on the second stimulus of a pair (vs. the first one)?</p><p>3. Perhaps I'm missing something, but it seems more parsimonious to remove participants who did not have statistical learning data from the templating analyses as well (p. 17). This way, the brain maps and plots are derived from data from the same individuals.</p><p>4. Figure 1 is somewhat difficult to follow. The logic is clear, but the lines depicting the information flow between the layers are somewhat difficult to keep in mind – perhaps it would be helpful if the hippocampal and visual layers were displayed in different colours. However, I leave it up to the authors to decide whether to make any changes, this could just be subjective preference. All other figures are exceptionally clear and easy to follow.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I am very enthusiastic about this work, and did find it quite well-motivated and designed in the broad strokes. I consider my concerns/comments to be &quot;moderate&quot; and am confident they can be addressed.</p><p>Specifics on the concerns and my suggestions:</p><p>Thinking about the DG findings, the authors present this as being the predicted locus of their non-monotonic representational relationship. But the justification for that is somewhat brief (limited to P.8) and to be honest not immediately intuitive to me. For one thing, in my view of the rodent literature the data might suggest DG favors differentiation for both low and moderate coactivation of memories on such a task, being highly sensitive to small changes. The authors cite the important Leutgeb 2007 work on this, but it's not clear to me that favors DG for the U-shaped pattern observed. The authors might also consider evidence juxtaposing CA1 and CA3 in a similar manner (Vazdarjanova and Guzowski 2004, J Neurosci). That work and the Leutgeb data would seem to favor CA3 having a &quot;thresholded&quot; representational relationship with contextual similarity, whil also suggesting CA1 may have higher discriminability for moderate levels of overlap. One suggestion is that pattern completion processes in the CA3 subcircuit may resist differention from DG up to an extent as contextual similarity drifts.</p><p>A related question on the theoretical side is how the network model is being conceptualized for subfields – for example, the subdivisions of the hippocampus can (ought to?) themselves be seen as layers in a neural network. Are the authors envisioning the context and perceptual conjunction cells as layers equally represented in each subfield tested, and that the transformation in that region alone is what differs? I ask because at least in broad terms the inputs to these regions from each other and the entorhinal cortex are different – DG and CA3 are predominantly targeted by entorhinal layer II, while CA1 is largely the target of entorhinal layer III, and it appears &quot;where&quot; and &quot;what&quot; cortical pathways are more segregated in CA1 than CA3 and DG (Witter et al., 2006 Annals of NYAS), and models of hippocampal circuit-level function often view item-context associations in inter-subregional terms (e.g., Hasselmo and Eichenbaum, 2005).</p><p>Thinking about the model –</p><p>One interesting assumption, if I read correctly, is that residual firing within the hippocampus during statistical learning (in the conjunction units) is &quot;externally driven&quot; – we see a &quot;moderate&quot; repeated activation of a unit in Figure 1 driven by sustained firing in visual cortex from the preceding trial and repeated firing of the same unit in the context layer. To my eye, this would seem to be a high overlap scenario for such a circuit – why does full context repetition and weak input from the cortex not drive strong activity in that item A cell (plasticity has indeed promoted a strong recurrent connection from prior learning between context and the perceptual layer)? Moreover, why do we assume sustained firing in the conjunctive codes in cortex but not those in the hippocampus?</p><p>On a related note, it seems allowing context to drift, as in a temporal context model, would help – the context is likely highly similar, but not identical, between adjacent events, which could somewhat attenuate the coactivation of the past event and facilitate the weakened connections with the perceptual conjunction layer.</p><p>Perhaps this is lost in the weeds, somewhat for the overall memory – similarity relationships observed, but I think it's an area where more justification and consideration could help tie the fMRI research here to data in rodents and/or motivate continued research into the circuit level of what is being observed.</p><p>Overall, I think a bit more on why the model was conceptualized the way it was and how the predictions (or, if more post-hoc – discover) of DG relate to our understanding of neural population connectivity and behavior from animal work would set the study up to promote even more hypothesis testing in this area.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68344.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>1. Provide a clearer theoretical rationale for the focus on the dentate gyrus given extant rodent work, per the detailed comments provided by Reviewer 3.</p></disp-quote><p>We thank the editor for the opportunity to better motivate our focus on the dentate gyrus. We have incorporated new sections in the Introduction to establish why we expected that the DG would be the most likely subfield to exhibit the predicted learning effects. This includes a thorough consideration of the existing rodent work, taking into account the papers helpfully recommended by Reviewer 3. A full accounting of these changes can be found in our response to Reviewer 3’s point #1.</p><disp-quote content-type="editor-comment"><p>2. Address the extent to which these findings are dependent on this particular learning paradigm.</p></disp-quote><p>Our current approach involves a statistical learning task, but non-monotonic plasticity is hypothesized to be a general mechanism that applies across several tasks and learning contexts, and there is evidence from other studies consistent with this idea. We have added sections in the Introduction and Discussion to clarify this point. These changes are described in more detail in our response to Reviewer 2’s point #1.</p><disp-quote content-type="editor-comment"><p>3. Clarify the time course of these effects, in particular as they relate to interpreting the pre- to post-learning results.</p></disp-quote><p>We agree that it would be ideal to be able to understand the time course over which these effects emerge. However, our pre/post design, with intervening statistical learning where transition probabilities were deterministic, precludes such an analysis. We have now explained why this analysis is not possible and included a new passage in the Discussion that highlights this limitation and provides suggestions for future work. These changes are described in detail in our response to Reviewer 2’s point #2.</p><disp-quote content-type="editor-comment"><p>4. Describe extent to which these neural patterns meaningfully relate to behaviour.</p></disp-quote><p>The behavioral cover task we used was orthogonal to the statistical learning and visual similarity manipulations. Grey squares appeared randomly and infrequently and were therefore not predictable. As a result, there is no reason to expect that statistical learning would yield any changes in detection performance. We have now added a section acknowledging this limitation, but pointing toward other work, with similar paradigms, where the same sort of learning resulted in measurable effects in behavior. These changes are described in detail in our response to Reviewer 2’s point #3.</p><disp-quote content-type="editor-comment"><p>5. Describe the relationship between the effects in the hippocampus and early visual regions.</p></disp-quote><p>We thank the editor and reviewer for the fantastic suggestion. In response to this comment, we ran a new analysis where we extracted the linear coefficients for each participant in the visual regions of interest (tracking visual similarity), and the cubic model fit coefficients in the representational change analysis from the hippocampus (tracking non-monotonic plasticity). We found an association between linearity in PRC and non-monotonicity in DG. This analysis is now referenced in the main text and in a figure supplement. These changes are described in detail in our response to Reviewer 2’s point #4.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. Related to point 2 above – it would be helpful to show an analysis of how hippocampal representations change over time. I understand that time is a confound here – representations will drift further from the pre-learning template even with no further learning. However, since all stimuli were presented at all timepoints, adjacent run-to-run similarity values for these pairs could be calculated to establish whether the relative integration/differentiation occurs in a gradual way or as a step function. I leave it up to the authors to decide how to address this, or to provide an explanation for why such an analysis might not be feasible, but this would be helpful in interpreting the pre- to post-learning results.</p></disp-quote><p>We agree that understanding the emergence of integration or differentiation effects over time would be fantastic, and we appreciate the thoughtful point that in principle it might be possible to compare representations across adjacent runs. However, any signal related to the smaller effect of differential visual representations would be drowned out by the massive confounding effects of temporal auto-correlation. We have summarized these points in our response to your Public Review Point #2, including an added paragraph which both highlights this opportunity for further work, and suggests potential designs or methods that would allow measuring the trajectory of learning.</p><disp-quote content-type="editor-comment"><p>2. Related to point 3 above – it would be helpful to provide any behavioural measures such as RT that might hint at whether (and how well) the participants learned the links between paired stimuli. Similarly, across the 6 statistical learning runs, did the authors observe RT facilitation for times when the grey square appeared on the second stimulus of a pair (vs. the first one)?</p></disp-quote><p>Although statistical learning can facilitate processing of the features of the second item in a pair (e.g., leading to RT facilitation on shape identification: Turk-Browne and Scholl, 2009; Zhao and Luo, 2017; arbitrary category learning: Rogers, Park and Vickery, 2021; and categorical judgments: Turk-Browne, Scholl, Johnson and Chun, 2010; Sherman and Turk- Browne, 2021), the gray square task did not require processing of the features of paired stimuli. Furthermore, the gray squares were inserted randomly in the trial sequence and thus were orthogonal to the pair structure, so there was no way to anticipate when a gray square would appear. As such, there is no reason to expect RT facilitation when the gray square appeared on the second stimulus of a pair compared to the first. Indeed, when we ran this analysis in response to this comment, there was a reduction in RT across runs, but both the main effect of pairmate (first, second) and the interaction of run by pairmate were not significant (<italic>p</italic>s &gt; 0.9). Having said that, we certainly agree about the value of linking representational change to behavior and have now acknowledged this important goal and limitation, as noted above. For what it’s worth, the structure of our task was closely modelled after the structure of Schapiro et al. (2012), in which there was some offline behavioral evidence of statistical learning in terms of familiarity judgments.</p><disp-quote content-type="editor-comment"><p>3. Perhaps I'm missing something, but it seems more parsimonious to remove participants who did not have statistical learning data from the templating analyses as well (p. 17). This way, the brain maps and plots are derived from data from the same individuals.</p></disp-quote><p>Since we had the templating data at our disposal, and the analysis of a given participant’s statistical learning data did not depend on others’ templating data, we opted to include the full sample for the visual analyses. However, we were accidentally inconsistent about this in the previous manuscript: the ROI data (Figure 3A) included only the 36 participants who were included in the learning analysis, whereas the searchlight data (Figure 3B) were reported for all 41 participants. We have now corrected this so both the ROI and searchlight analyses reflect our intention of reporting the full sample of 41 participants. This involved updating the ROI figure and analyses for subpanels 3A and 3B. To address the reviewer’s question, we also ran both analyses with just 36 participants. The pattern of results in the ROI analyses was identical across both approaches (see Figure 3—figure supplement 1). The searchlight analysis also yielded very similar results, with the statistical maps from n = 36 and n = 41 highly correlated across voxels/searchlights (r = 0.928). That said, the clusters reported for n = 41 did not survive correction with n = 36, perhaps reflecting the smaller sample size. We now refer to this in the main text and new supplementary figure.</p><p>See Neural validation: <bold>“</bold>When this analysis was repeated with a reduced sample of the 36 participants who were also included in thesubsequent representational change analyses, these clusters no longer emerged as statistically significant at a corrected threshold.”</p><p>See Participants: “As such, our final sample for the learning task was 36 participants, with a total of 41 participants available for the visual templating analyses. See Figure 3—figure supplement 1 for visual templating analyses in a reduced sample containing only the 36 participants included in the representational change analyses.”</p><disp-quote content-type="editor-comment"><p>4. Figure 1 is somewhat difficult to follow. The logic is clear, but the lines depicting the information flow between the layers are somewhat difficult to keep in mind – perhaps it would be helpful if the hippocampal and visual layers were displayed in different colours. However, I leave it up to the authors to decide whether to make any changes, this could just be subjective preference. All other figures are exceptionally clear and easy to follow.</p></disp-quote><p>We agree that the hippocampal and visual layers were difficult to discern and that the arrows could have been more prominent. As a result, we have updated the figure based on what the reviewer suggested: the hippocampal and visual layers are now distinguished by a unique background color and the arrows (and labels they correspond to) are larger and color-matched. We thank the reviewer for their suggestions and for the comments about the other figures.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I am very enthusiastic about this work, and did find it quite well-motivated and designed in the broad strokes. I consider my concerns/comments to be &quot;moderate&quot; and am confident they can be addressed.</p><p>Specifics on the concerns and my suggestions:</p><p>1. Thinking about the DG findings, the authors present this as being the predicted locus of their non-monotonic representational relationship. But the justification for that is somewhat brief (limited to P.8) and to be honest not immediately intuitive to me. For one thing, in my view of the rodent literature the data might suggest DG favors differentiation for both low and moderate coactivation of memories on such a task, being highly sensitive to small changes. The authors cite the important Leutgeb 2007 work on this, but it's not clear to me that favors DG for the U-shaped pattern observed. The authors might also consider evidence juxtaposing CA1 and CA3 in a similar manner (Vazdarjanova and Guzowski 2004, J Neurosci). That work and the Leutgeb data would seem to favor CA3 having a &quot;thresholded&quot; representational relationship with contextual similarity, whil also suggesting CA1 may have higher discriminability for moderate levels of overlap. One suggestion is that pattern completion processes in the CA3 subcircuit may resist differention from DG up to an extent as contextual similarity drifts.</p></disp-quote><p>We have now clarified our motivation for focusing on DG, drawing upon the very useful references and ideas provided by the reviewer. See Introduction: <bold>“</bold>We and others have previously hypothesized that nonmonotonic plasticity applies widely throughout the brain (Ritvo et al., 2019), including sensory regions (e.g., Bear, 2003). […] For example, rodent studies have demonstrated that, rather than coactivating representations of different locations, CA3 patterns tend to sharply flip between one pattern and the other (e.g., Leutgeb, Leutgeb, Moser, and Moser, 2007; Vazdarjanova and Guzowski, 2004).”</p><disp-quote content-type="editor-comment"><p>2. A related question on the theoretical side is how the network model is being conceptualized for subfields – for example, the subdivisions of the hippocampus can (ought to?) themselves be seen as layers in a neural network. Are the authors envisioning the context and perceptual conjunction cells as layers equally represented in each subfield tested, and that the transformation in that region alone is what differs? I ask because at least in broad terms the inputs to these regions from each other and the entorhinal cortex are different – DG and CA3 are predominantly targeted by entorhinal layer II, while CA1 is largely the target of entorhinal layer III, and it appears &quot;where&quot; and &quot;what&quot; cortical pathways are more segregated in CA1 than CA3 and DG (Witter et al., 2006 Annals of NYAS), and models of hippocampal circuit-level function often view item-context associations in inter-subregional terms (e.g., Hasselmo and Eichenbaum, 2005).</p></disp-quote><p>(see response after point 4)</p><disp-quote content-type="editor-comment"><p>3. Thinking about the model – One interesting assumption, if I read correctly, is that residual firing within the hippocampus during statistical learning (in the conjunction units) is &quot;externally driven&quot; – we see a &quot;moderate&quot; repeated activation of a unit in Figure 1 driven by sustained firing in visual cortex from the preceding trial and repeated firing of the same unit in the context layer. To my eye, this would seem to be a high overlap scenario for such a circuit – why does full context repetition and weak input from the cortex not drive strong activity in that item A cell (plasticity has indeed promoted a strong recurrent connection from prior learning between context and the perceptual layer)? Moreover, why do we assume sustained firing in the conjunctive codes in cortex but not those in the hippocampus?</p></disp-quote><p>(see response after point 4)</p><disp-quote content-type="editor-comment"><p>4. On a related note, it seems allowing context to drift, as in a temporal context model, would help – the context is likely highly similar, but not identical, between adjacent events, which could somewhat attenuate the coactivation of the past event and facilitate the weakened connections with the perceptual conjunction layer.</p></disp-quote><p>We thank the reviewer for the excellent points in comments 2, 3, and 4 above. We completely agree that the factors listed above (the distribution of perceptual conjunction and context neurons and their interconnectivity within and across subfields; the capacity of individual subfields for sustained activation; and temporal drift) can affect learning in our paradigm. We have added a new paragraph to address these points, reprinted from above for convenience.</p><p>See Discussion (p. 16, lines 318-336): “Although we focused above on differences in sparsity when motivating our predictions about subfield-specific learning effects, there are numerous other factors besides sparsity that could affect coactivation and (through this) modulate learning. […] For these reasons, exploring the predictions of the NMPH in the context of biologically detailed computational models of the hippocampus (e.g., Schapiro, Turk-Browne, Botvinick, and Norman, 2017; Frank, Montemurro, and Montaldi, 2020; Hasselmo and Wyble, 1997) will help to sharpen predictions about what kinds of learning should occur in different parts of the hippocampus.</p><disp-quote content-type="editor-comment"><p>5. Perhaps this is lost in the weeds, somewhat for the overall memory – similarity relationships observed, but I think it's an area where more justification and consideration could help tie the fMRI research here to data in rodents and/or motivate continued research into the circuit level of what is being observed. Overall, I think a bit more on why the model was conceptualized the way it was and how the predictions (or, if more post-hoc – discover) of DG relate to our understanding of neural population connectivity and behavior from animal work would set the study up to promote even more hypothesis testing in this area.</p></disp-quote><p>We thank the reviewer for their insightful comments; we believe that the resulting edits have substantially strengthened the paper.</p></body></sub-article></article>