<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63377</article-id><article-id pub-id-type="doi">10.7554/eLife.63377</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-208640"><name><surname>Bohnslav</surname><given-names>James P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9359-8907</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-208641"><name><surname>Wimalasena</surname><given-names>Nivanthika K</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-208642"><name><surname>Clausing</surname><given-names>Kelsey J</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-239473"><name><surname>Dai</surname><given-names>Yu Y</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-208643"><name><surname>Yarmolinsky</surname><given-names>David A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-208644"><name><surname>Cruz</surname><given-names>Tomás</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund11"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-239474"><name><surname>Kashlan</surname><given-names>Adam D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28781"><name><surname>Chiappe</surname><given-names>M Eugenia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1761-0457</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-195621"><name><surname>Orefice</surname><given-names>Lauren L</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-11675"><name><surname>Woolf</surname><given-names>Clifford J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-208332"><name><surname>Harvey</surname><given-names>Christopher D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9850-2268</contrib-id><email>harvey@hms.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund13"/><xref ref-type="other" rid="fund14"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Neurobiology, Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>F.M. Kirby Neurobiology Center, Boston Children’s Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Molecular Biology, Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Department of Genetics, Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Champalimaud Neuroscience Programme, Champalimaud Center for the Unknown</institution><addr-line><named-content content-type="city">Lisbon</named-content></addr-line><country>Portugal</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>02</day><month>09</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e63377</elocation-id><history><date date-type="received" iso-8601-date="2020-09-23"><day>23</day><month>09</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-09-01"><day>01</day><month>09</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-09-25"><day>25</day><month>09</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.09.24.312504"/></event></pub-history><permissions><copyright-statement>© 2021, Bohnslav et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Bohnslav et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63377-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-63377-figures-v2.pdf"/><abstract><p>Videos of animal behavior are used to quantify researcher-defined behaviors of interest to study neural function, gene mutations, and pharmacological therapies. Behaviors of interest are often scored manually, which is time-consuming, limited to few behaviors, and variable across researchers. We created DeepEthogram: software that uses supervised machine learning to convert raw video pixels into an ethogram, the behaviors of interest present in each video frame. DeepEthogram is designed to be general-purpose and applicable across species, behaviors, and video-recording hardware. It uses convolutional neural networks to compute motion, extract features from motion and images, and classify features into behaviors. Behaviors are classified with above 90% accuracy on single frames in videos of mice and flies, matching expert-level human performance. DeepEthogram accurately predicts rare behaviors, requires little training data, and generalizes across subjects. A graphical interface allows beginning-to-end analysis without end-user programming. DeepEthogram’s rapid, automatic, and reproducible labeling of researcher-defined behaviors of interest may accelerate and enhance supervised behavior analysis. Code is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/jbohnslav/deepethogram">https://github.com/jbohnslav/deepethogram</ext-link>.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>behavior analysis</kwd><kwd>deep learning</kwd><kwd>computer vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01MH107620</award-id><principal-award-recipient><name><surname>Harvey</surname><given-names>Christopher D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS089521</award-id><principal-award-recipient><name><surname>Harvey</surname><given-names>Christopher D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS108410</award-id><principal-award-recipient><name><surname>Harvey</surname><given-names>Christopher D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F31NS108450</award-id><principal-award-recipient><name><surname>Bohnslav</surname><given-names>James P</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R35NS105076</award-id><principal-award-recipient><name><surname>Woolf</surname><given-names>Clifford J</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01AT011447</award-id><principal-award-recipient><name><surname>Woolf</surname><given-names>Clifford J</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R00NS101057</award-id><principal-award-recipient><name><surname>Orefice</surname><given-names>Lauren L</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K99DE028360</award-id><principal-award-recipient><name><surname>Yarmolinsky</surname><given-names>David A</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-Stg-759782</award-id><principal-award-recipient><name><surname>Chiappe</surname><given-names>M Eugenia</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>GRFP</award-id><principal-award-recipient><name><surname>Wimalasena</surname><given-names>Nivanthika K</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006667</institution-id><institution>Ministry of Education</institution></institution-wrap></funding-source><award-id>PD/BD/105947/2014</award-id><principal-award-recipient><name><surname>Cruz</surname><given-names>Tomás</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006691</institution-id><institution>Harvard Medical School</institution></institution-wrap></funding-source><award-id>Dean's Innovation Award</award-id><principal-award-recipient><name><surname>Harvey</surname><given-names>Christopher D</given-names></name></principal-award-recipient></award-group><award-group id="fund13"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006691</institution-id><institution>Harvard Medical School</institution></institution-wrap></funding-source><award-id>Goldenson Research Award</award-id><principal-award-recipient><name><surname>Harvey</surname><given-names>Christopher D</given-names></name></principal-award-recipient></award-group><award-group id="fund14"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>DP1 MH125776</award-id><principal-award-recipient><name><surname>Harvey</surname><given-names>Christopher D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>DeepEthogram automatically classifies animal behavior videos into researcher-defined behaviors of interest, saving researcher time and enabling more detailed downstream analysis of behavior.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The analysis of animal behavior is a common approach in a wide range of biomedical research fields, including basic neuroscience research (<xref ref-type="bibr" rid="bib35">Krakauer et al., 2017</xref>), translational analysis of disease models, and development of therapeutics. For example, researchers study behavioral patterns of animals to investigate the effect of a gene mutation, understand the efficacy of potential pharmacological therapies, or uncover the neural underpinnings of behavior. In some cases, behavioral tests allow quantification of behavior through tracking an animal’s location in space, such as in the three-chamber assay, open-field arena, Morris water maze, and elevated plus maze (EPM) (<xref ref-type="bibr" rid="bib56">Pennington, 2019</xref>). Increasingly, researchers are finding that important details of behavior involve subtle actions that are hard to quantify, such as changes in the prevalence of grooming in models of anxiety (<xref ref-type="bibr" rid="bib54">Peça et al., 2011</xref>), licking a limb in models of pain (<xref ref-type="bibr" rid="bib7">Browne, 2017</xref>), and manipulation of food objects for fine sensorimotor control (<xref ref-type="bibr" rid="bib49">Neubarth, 2020</xref>; <xref ref-type="bibr" rid="bib64">Sauerbrei et al., 2020</xref>). In these cases, researchers often closely observe videos of animals and then develop a list of behaviors they want to measure. To quantify these observations, the most commonly used approach, to our knowledge, is for researchers to manually watch videos with a stopwatch to count the time each behavior of interest is exhibited (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This approach takes immense amounts of researcher time, often equal to or greater than the duration of the video per individual subject. Also, because this approach requires manual viewing, often only one or a small number of behaviors are studied at a time. In addition, researchers often do not label the video frames when specific behaviors occur, precluding subsequent analysis and review of behavior bouts, such as bout durations and the transition probability between behaviors. Furthermore, scoring of behaviors can vary greatly between researchers especially as new researchers are trained (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>) and can be subject to bias. Therefore, it would be a significant advance if a researcher could define a list of behaviors of interest, such as face grooming, tail grooming, limb licking, locomoting, rearing, and so on, and then use automated software to identify when and how frequently each of the behaviors occurred in a video.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>DeepEthogram overview.</title><p>(<bold>A</bold>) Workflows for supervised behavior labeling. Left: a common traditional approach based on manual labeling. Middle: workflow with DeepEthogram. Right: Schematic of expected scaling of user time for each workflow. (<bold>B</bold>) Ethogram schematic. Top: example images from Mouse-Ventral1 dataset. Bottom: ethogram with human labels. Dark colors indicate which behavior is present. Example shown is from Mouse-Ventral1 dataset. Images have been cropped, brightened, and converted to grayscale for clarity. (<bold>C</bold>) DeepEthogram-fast model schematic. Example images are from the Fly dataset. Left: a sequence of 11 frames is converted into 10 optic flows. Middle: the center frame and the stack of 10 optic flows are converted into 512-dimensional representations via deep convolutional neural networks (CNNs). Right: these features are converted into probabilities of each behavior via the sequence model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Optic flow.</title><p>(<bold>A</bold>) Example images from the Fly dataset on two consecutive frames. (<bold>B</bold>) Optic flow estimated with TinyMotionNet. Note that the image size is half the original due to the TinyMotionNet architecture. Displacements in the x dimension (left) and y dimension (right) between the frames in (<bold>A</bold>). (<bold>C</bold>) The reconstruction of frame 1 estimated by sampling frame 2 according to the optic flow calculation. The image was resized with bilinear interpolation before resampling. (<bold>D</bold>) Absolute error between frame 1 and the frame 1 reconstructed from optic flow. (<bold>E</bold>) Visualization of optic flow using arrow lengths to indicate the direction and magnitude flow. (<bold>F</bold>) Visualization of optic flow using coloring according the inset color scale. Left displacements are mapped to cyan, right displacements to red, and so on. Saturation indicates the magnitude of displacement.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Researchers are increasingly turning to computational approaches to quantify and analyze animal behavior (<xref ref-type="bibr" rid="bib13">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Perona, 2014</xref>; <xref ref-type="bibr" rid="bib23">Gomez-Marin et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Brown and de Bivort, 2017</xref>; <xref ref-type="bibr" rid="bib16">Egnor and Branson, 2016</xref>). The task of automatically classifying an animal’s actions into user-defined behaviors falls in the category of supervised machine learning. In computer vision, this task is called ‘action detection,’ ‘temporal action localization,’ ‘action recognition,’ or ‘action segmentation.’ This task is distinct from other emerging behavioral analysis methods based on unsupervised learning, in which machine learning models discover behavioral modules from the data, irrespective of researcher labels. Although unsupervised methods, such as Motion Sequencing (<xref ref-type="bibr" rid="bib73">Wiltschko, 2015</xref>; <xref ref-type="bibr" rid="bib74">Wiltschko et al., 2020</xref>), MotionMapper (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>), BehaveNet (<xref ref-type="bibr" rid="bib2">Batty, 2019</xref>), B-SOiD (<xref ref-type="bibr" rid="bib28">Hsu and Yttri, 2019</xref>), and others (<xref ref-type="bibr" rid="bib13">Datta et al., 2019</xref>), can discover behavioral modules not obvious to the researcher, their outputs are not designed to perfectly match up to behaviors of interest in cases in which researchers have strong prior knowledge about the specific behaviors relevant to their experiments.</p><p>Pioneering work, including JAABA (<xref ref-type="bibr" rid="bib31">Kabra et al., 2013</xref>), SimBA (<xref ref-type="bibr" rid="bib50">Nilsson, 2020</xref>), MARS (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>), Live Mouse Tracker (<xref ref-type="bibr" rid="bib14">de Chaumont et al., 2019</xref>), and others (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>; <xref ref-type="bibr" rid="bib12">Dankert et al., 2009</xref>; <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>), has made important progress toward the goal of supervised classification of behaviors. These methods track specific features of an animal’s body and use the time series of these features to classify whether a behavior is present at a given timepoint. In computer vision, this is known as ‘skeleton-based action detection.’ In JAABA, ellipses are fit to the outline of an animal’s body, and these ellipses are used to classify behaviors. SimBA classifies behaviors based on the positions of ‘keypoints’ on the animal’s body, such as limb joints. MARS takes a similar approach with a focus on social behaviors (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>). These approaches have become easier with recent pose estimation methods, including DeepLabCut (<xref ref-type="bibr" rid="bib44">Mathis, 2018</xref>; <xref ref-type="bibr" rid="bib47">Nath, 2019</xref>; <xref ref-type="bibr" rid="bib37">Lauer, 2021</xref>) and similar algorithms (<xref ref-type="bibr" rid="bib57">Pereira, 2018a</xref>; <xref ref-type="bibr" rid="bib24">Graving et al., 2019</xref>). Thus, these approaches utilize a pipeline with two major steps. First, researchers reduce a video to a set of user-defined features of interest (e.g., limb positions) using pose estimation software. Second, these pose estimates are used as inputs to classifiers that identify the behaviors of interest. This approach has the advantage that it provides information beyond whether a behavior of interest is present or absent at each timepoint. Because the parts of the animal’s body that contribute to the behavior are tracked, detailed analyses of movement and how these movements contribute to behaviors of interest can be performed.</p><p>Here, we took a different approach based on models that classify behaviors directly from the raw pixel values of videos. Drawing from extensive work in this area in computer vision (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>; <xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Simonyan and Zisserman, 2014</xref>), this approach has the potential to simplify the pipeline for classifying behaviors. It requires only one type of human annotation – labels for behaviors of interest – instead of labels for both pose keypoints and behaviors. In addition, this approach requires only a single model for behavior classification instead of models for pose estimation and behavior classification. Classification from raw pixels is in principle generally applicable to any dataset that has video frames and training data for the model in the form of frame-by-frame binary behavior labels. Some recent work has performed behavior classification from pixels but only focused on motor deficits (<xref ref-type="bibr" rid="bib63">Ryait, 2019</xref>) or one species and setup (<xref ref-type="bibr" rid="bib70">van Dam et al., 2020</xref>). Other recent work uses image and motion features, similar to the approaches we developed here, except with a focus on classifying the timepoint at which a behavior starts, instead of classifying every frame into one or more behaviors (<xref ref-type="bibr" rid="bib36">Kwak et al., 2019</xref>).</p><p>Our method, called DeepEthogram, is a modular pipeline for automatically classifying each frame of a video into a set of user-defined behaviors. It uses a supervised deep-learning model that, with minimal user-based training data, takes a video with <italic>T</italic> frames and a user-defined set of <italic>K</italic> behaviors and generates a binary [<italic>T</italic>, <italic>K</italic>] matrix (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>; <xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>). This matrix indicates whether each behavior is present or absent on each frame, which we term an ‘ethogram’: the set of behaviors that are present at a given timepoint (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We use convolutional neural networks (CNNs), specifically Hidden Two-Stream Networks (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) and Temporal Gaussian Mixture (TGM) networks (<xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>), to detect actions in videos, and we pretrained the networks on large open-source datasets (<xref ref-type="bibr" rid="bib15">Deng, 2008</xref>; <xref ref-type="bibr" rid="bib9">Carreira et al., 2019</xref>). Previous work has introduced the methods we use here (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>; <xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Simonyan and Zisserman, 2014</xref>), and we have adapted and extended these methods for application to biomedical research of animal behavior. We validated our approach’s performance on nine datasets from two species, with each dataset posing distinct challenges for behavior classification. DeepEthogram automatically classifies behaviors with high performance, often reaching levels obtained by expert human labelers. High performance is achieved with only a few minutes of positive example data and even when the behaviors occur at different locations in the behavioral arena and at distinct orientations of the animal. Importantly, specialized video recording hardware is not required, and the entire pipeline requires no programming by the end-user because we developed a graphical user interface (GUI) for annotating videos, training models, and generating predictions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Modeling approach</title><p>Our goal was to take a set of video frames as input and predict the probability that each behavior of interest occurs on a given frame. This task of automated behavior labeling presents several challenges that framed our solution. First, in many cases, the behavior of interest occurs in a relatively small number of video frames, and the accuracy must be judged based on correct identification of these low-frequency events. For example, if a behavior of interest is present in 5% of frames, an algorithm could guess that the behavior is ‘not present’ on every frame and still achieve 95% overall accuracy. Critically, however, it would achieve 0% accuracy on the frames that matter, and an algorithm does not know a priori which frames matter. Second, ideally a method should perform well after being trained on only small amounts of user-labeled video frames, including across different animals, and thus require little manual input. Third, a method should be able to identify the same behavior regardless of the position and orientation of the animal when the behavior occurs. Fourth, methods should require relatively low computational resources in case researchers do not have access to large compute clusters or top-level GPUs.</p><p>We modeled our approach after temporal action localization methods used in computer vision aimed to solve related problems (<xref ref-type="bibr" rid="bib76">Zeng, 2019</xref>; <xref ref-type="bibr" rid="bib75">Xie et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Chao, 2018</xref>; <xref ref-type="bibr" rid="bib17">El-Nouby and Taylor, 2018</xref>). The overall architecture of our solution includes (1) estimating motion (optic flow) from a small snippet of video frames, (2) compressing a snippet of optic flow and individual still images into a lower dimensional set of features, and (3) using a sequence of the compressed features to estimate the probability of each behavior at each frame in a video (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We implemented this architecture using large, deep CNNs. First, one CNN is used to generate optic flow from a set of images. We incorporate optic flow because some behaviors are only obvious by looking at the animal’s movements between frames, such as distinguishing standing still and walking. We call this CNN the <italic>flow generator</italic> (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). We then use the optic flow output of the flow generator as input to a second CNN to compress the large number of optic flow snippets across all the pixels into a small set of features called <italic>flow features</italic> (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Separately, we use a distinct CNN, which takes single video frames as input, to compress the large number of raw pixels into a small set of <italic>spatial features</italic>, which contain information about the values of pixels relative to one another spatially but lack temporal information (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We include single frames separately because some behaviors are obvious from a single still image, such as identifying licking just by seeing an extended tongue. Together, we call these latter two CNNs <italic>feature extractors</italic> because they compress the large number of raw pixels into a small set of features called a <italic>feature vector</italic> (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Each of these feature extractors is trained to produce a probability for each behavior on each frame based only on their input (optic flow or single frames). We then fuse the outputs of the two feature extractors by averaging (Materials and methods). To produce the final probabilities that each behavior was present on a given frame – a step called inference – we use a <italic>sequence model</italic>, which has a large temporal receptive field and thus utilizes long timescale information (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We use this sequence model because our CNNs only ‘look at’ either 1 frame (spatial) or about 11 frames (optic flow), but when labeling videos, humans know that sometimes the information present seconds ago can be useful for estimating the behavior of the current frame. The final output of DeepEthogram is a <inline-formula><mml:math id="inf1"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> matrix, in which each element is the probability of behavior <italic>k</italic> occurring at time <italic>t</italic>. We threshold these probabilities to get a binary prediction for each behavior at each timepoint, with the possibility that multiple behaviors can occur simultaneously (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>For the flow generator, we use the MotionNet (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) architecture to generate 10 optic flow frames from 11 images. For the feature extractors, we use the ResNet family of models (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Hara et al., 2018</xref>) to extract both flow features and spatial features. Finally, we use TGM (<xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>) models as the sequence model to perform the ultimate classification. Each of these models has many variants with a large range in the number of parameters and the associated computational demands. We therefore created three versions of DeepEthogram that use variants of these models, with the aim of trading off accuracy and speed: DeepEthogram-fast, DeepEthogram-medium, and DeepEthogram-slow. DeepEthogram-fast uses TinyMotionNet (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) for the flow generator and ResNet18 (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>) for the feature extractors. It has the fewest parameters, the fastest training of the flow generator and feature extractor models, the fastest inference time, and the smallest requirement for computational resources. As a tradeoff for this speed, DeepEthogram-fast tends to have slightly worse performance than the other versions (see below). In contrast, DeepEthogram-slow uses a novel architecture TinyMotionNet3D for its flow generator and 3D-ResNet34 (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Simonyan and Zisserman, 2014</xref>; <xref ref-type="bibr" rid="bib25">Hara et al., 2018</xref>) for its feature extractors. It has the most parameters, the slowest training and inference times, and the highest computational demands, but it has the capacity to produce the best performance. DeepEthogram-medium is intermediate and uses MotionNet (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) and ResNet50 (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>) for its flow generator and feature extractors. All versions of DeepEthogram use the same sequence model. All variants of the flow generators and feature extractors are pretrained on the Kinetics700 video dataset (<xref ref-type="bibr" rid="bib9">Carreira et al., 2019</xref>), so that model parameters do not have to be learned from scratch (Materials and methods). TGM networks represent the state of the art on various action detection benchmarks as of 2019 (<xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>). However, recent work based on multiple temporal resolutions (<xref ref-type="bibr" rid="bib20">Feichtenhofer et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Kahatapitiya and Ryoo, 2021</xref>), graph convolutional networks (<xref ref-type="bibr" rid="bib76">Zeng, 2019</xref>), and transformer architectures (<xref ref-type="bibr" rid="bib48">Nawhal and Mori, 2021</xref>) has exceeded this performance. We carefully chose DeepEthogram’s components based on their performance, parameter count, and hardware requirements. DeepEthogram as a whole and its component parts are not aimed to be the state of the art on standard computer vision temporal action localization datasets and instead are focused on practical application to biomedical research of animal behavior.</p><p>In practice, the first step in running DeepEthogram is to train the flow generator on a set of videos, which occurs without user input (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In parallel, a user must label each frame in a set of training videos for the presence of each behavior of interest. These labels are then used to train independently the spatial feature extractor and the flow feature extractor to produce separate estimates of the probability of each behavior. The extracted feature vectors for each frame are then saved and used to train the sequence models to produce the final predicted probability of each behavior at each frame. We chose to train the models in series, rather than all at once from end-to-end, due to a combination of concerns about backpropagating error across diverse models, overfitting with extremely large models, and computational capacity (Materials and methods).</p></sec><sec id="s2-2"><title>Diverse datasets to test DeepEthogram</title><p>To test the performance of our model, we used nine different neuroscience research datasets that span two species and present distinct challenges for computer vision approaches. Please see the examples in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s6">6</xref>, and <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video9">9</xref> that demonstrate the behaviors of interest and provide an intuition for the ease or difficulty of identifying and distinguishing particular behaviors.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Datasets and behaviors of interest.</title><p>(<bold>A</bold>) Left: raw example images from the Mouse-Ventral1 dataset for each of the behaviors of interest. Right: time spent on each behavior, based on human labels. Note that the times may add up to more than 100% across behaviors because multiple behaviors can occur on the same frame. Background is defined as when no other behaviors occur. (<bold>B–I</bold>) Similar to (<bold>A</bold>), except for the other datasets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Example images from the datasets, part 1.</title><p>(<bold>A</bold>) Examples from the Mouse-Ventral1 dataset. Each row is three consecutive frames of the indicated behavior. Right columns: optic flow computed by TinyMotionNet and visualized as in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>. (<bold>B</bold>) Similar to (<bold>A</bold>), except for the Mouse-Ventral2 dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Example images from the datasets, part 2.</title><p>(<bold>A</bold>) Examples from the Mouse-Openfield dataset. Each row is three consecutive frames of the indicated behavior. Right columns: optic flow computed by TinyMotionNet and visualized as in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>. (<bold>B</bold>) Similar to (<bold>A</bold>), except for the Fly dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Example images from the datasets, part 3.</title><p>Examples from the Mouse-Homecage dataset. Each row is three consecutive frames of the indicated behavior. Right columns: optic flow computed by TinyMotionNet and visualized as in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Example images from the datasets, part 4.</title><p>Examples from the Mouse-Social dataset. Each row is three consecutive frames of the indicated behavior. Right columns: optic flow computed by TinyMotionNet and visualized as in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Example images from the datasets, part 5.</title><p>Examples from the Sturman-EPM dataset. Each row is three consecutive frames of the indicated behavior. Right columns: optic flow computed by TinyMotionNet and visualized as in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>. All data from <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Example images from the datasets, part 6.</title><p>(<bold>A</bold>) Examples from the Sturman-FST dataset. Each row is three consecutive frames of the indicated behavior. Right columns: optic flow computed by TinyMotionNet and visualized as in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>. (<bold>B</bold>) Similar to (<bold>A</bold>), except for the Sturman-OFT dataset. All data from <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig2-figsupp6-v2.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video1.mp4"><label>Video 1.</label><caption><title>DeepEthogram example from the Mouse-Ventral1 dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video2.mp4"><label>Video 2.</label><caption><title>DeepEthogram example from the Mouse-Ventral2 dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video3.mp4"><label>Video 3.</label><caption><title>DeepEthogram example from the Mouse-Openfield dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video4.mp4"><label>Video 4.</label><caption><title>DeepEthogram example from the Mouse-Homecage dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video5.mp4"><label>Video 5.</label><caption><title>DeepEthogram example from the Mouse-Social dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video6" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video6.mp4"><label>Video 6.</label><caption><title>DeepEthogram example from the Sturman-EPM dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video7" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video7.mp4"><label>Video 7.</label><caption><title>DeepEthogram example from the Sturman-FST dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video8" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video8.mp4"><label>Video 8.</label><caption><title>DeepEthogram example from the Sturman-OFT dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><media id="video9" mime-subtype="mp4" mimetype="video" xlink:href="elife-63377-video9.mp4"><label>Video 9.</label><caption><title>DeepEthogram example from the Flies dataset.</title><p>Video is from the test set. Top: raw image. Title indicates frame number in video. Tick legends indicate pixels. Middle: human labels. Black box indicates the current frame. Bottom: DeepEthogram predictions from a trained DeepEthogram-medium model.</p></caption></media><p>We collected five datasets of mice in various behavioral arenas. The ‘Mouse-Ventral1’ and ‘Mouse-Ventral2’ datasets are bottom-up videos of a mouse in an open field and small chamber, respectively (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A, B</xref>, <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video2">2</xref>). The ‘Mouse-Openfield’ dataset includes commonly used top-down videos of a mouse in an open arena (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>, <xref ref-type="video" rid="video3">Video 3</xref>). The ‘Mouse-Homecage’ dataset are top-down videos of a mouse in its home cage with bedding, a hut, and two objects (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, <xref ref-type="video" rid="video4">Video 4</xref>). The ‘Mouse-Social’ dataset are top-down videos of two mice interacting in an open arena (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>, <xref ref-type="video" rid="video5">Video 5</xref>). We also tested three datasets from published work by <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref> that consist of mice in common behavior assays: the EPM, forced swim test (FST), and open field test (OFT) (<xref ref-type="fig" rid="fig2">Figure 2F–H</xref>, <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplements 5</xref>–<xref ref-type="fig" rid="fig2s6">6</xref>, <xref ref-type="video" rid="video6">Videos 6</xref>–<xref ref-type="video" rid="video8">8</xref>). Finally, we tested a different species in the ‘Fly’ dataset that includes side view videos of a <italic>Drosophila melanogaster</italic> and aims to identify a coordinated walking pattern (<xref ref-type="bibr" rid="bib22">Fujiwara et al., 2017</xref>; <xref ref-type="fig" rid="fig2">Figure 2D</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>, <xref ref-type="video" rid="video9">Video 9</xref>).</p><p>Collectively, these datasets include distinct view angles, a variety of illumination levels, and different resolutions and video qualities. They also pose various challenges for computer vision, including the subject occupying a small fraction of pixels (Mouse-Ventral1, Mouse-Openfield, Mouse-Homecage, Sturman-EPM, Sturman-OFT), complex backgrounds with non-uniform patterns (bedding and objects in Mouse-Homecage) or irrelevant motion (moving water in Sturman-FST), objects that occlude the subject (Mouse-Homecage), poor contrast of body parts (Mouse-Openfield, Sturman-EPM, Sturman-OFT), little motion from frame-to-frame (Fly, due to high video rate), and few training examples (Sturman-EPM, only five videos and only three that contain all behaviors). Furthermore, in most datasets, some behaviors of interest are rare and occur in only a few percent of the total video frames.</p><p>In each dataset, we labeled a behavior as present regardless of the location where it occurred and the orientation of the subject when it occurred. We did not note position or direction information, and we did not spatially crop the video frames or align the animal before training our model. In all datasets, we labeled the frames on which none of the behaviors of interest were present as ‘background,’ following the convention in computer vision. Each video in a dataset was recorded using a different individual mouse or fly, and thus training and testing the model across videos measured generalization across individual subjects. The video datasets and researcher annotations are available at the project website: <ext-link ext-link-type="uri" xlink:href="https://github.com/jbohnslav/deepethogram">https://github.com/jbohnslav/deepethogram</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d1e96891cb40f5b08b500a1de3ba92d372ac6b35;origin=https://github.com/jbohnslav/deepethogram;visit=swh:1:snp:533bf2df888313ada5443ac928c0d044941c726a;anchor=swh:1:rev:ffd7e6bd91f52c7d1dbb166d1fe8793a26c4cb01">swh:1:rev:ffd7e6bd91f52c7d1dbb166d1fe8793a26c4cb01</ext-link>), <xref ref-type="bibr" rid="bib4">Bohnslav, 2021</xref>.</p></sec><sec id="s2-3"><title>DeepEthogram achieves high performance approaching expert-level human performance</title><p>We split each dataset into three subsets: training, validation, and test (Materials and methods). The training set was used to update model parameters, such as the weights of the CNNs. The validation set was used to set appropriate hyperparameters, such as the thresholds used to turn the probabilities of each behavior into binary predictions about whether each behavior was present. The test set was used to report performance on new data not used in training the model. We generated five random splits of the data into training, validation, and test sets and averaged our results across these five splits, unless noted otherwise (Materials and methods). We computed three complementary metrics of model performance using the test set. First, we computed the accuracy, which is the fraction of elements of the <inline-formula><mml:math id="inf2"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> ethogram that were predicted correctly. We note that in theory accuracy could be high even if the model did not perform well on each behavior. For example, in the Mouse-Ventral2 dataset, some behaviors were incredibly rare, occurring in only ~2% of frames (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Thus, the model could in theory achieve ~98% accuracy simply by guessing that the behavior was absent on all frames. Therefore, we also computed the F1 score, a metric ranging from 0 (bad) to 1 (perfect) that takes into account the rates of false positives and false negatives. The F1 score is the geometric mean of the precision and recall of the model. Precision is the fraction of frames labeled by the model as a given behavior that are actually that behavior (true positives/(true positives + false positives)). Recall is the fraction of frames actually having a given behavior that are correctly labeled as that behavior by the model (true positives/(true positives + false negatives)). We report the F1 score in the main figures and show precision and recall performance in the figure supplements. Because the accuracy and F1 score depend on our choice of a threshold to turn the probability of a given behavior on a given frame into a binary prediction about the presence of that behavior, we also computed the area under the receiver operating curve (AUROC), which summarizes performance as a function of the threshold.</p><p>We first considered the entire ethogram, including all behaviors. DeepEthogram performed with greater than 85% accuracy on the test data for all datasets (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Overall metrics were calculated for each element of the ethogram. The model achieved high overall F1 scores, with high precision and recall (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). Similarly, high overall performance was observed with the AUROC measures (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>). These results indicate that the model was able to capture the overall patterns of behavior in videos.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>DeepEthogram performance.</title><p>All results are from the test sets only. (<bold>A</bold>) Overall accuracy for each model size and dataset. Error bars indicate mean ± SEM across five random splits of the data (three for Sturman-EPM). (<bold>B</bold>) Similar to (<bold>A</bold>), except for overall F1 score. (<bold>C</bold>) F1 score for DeepEthogram-medium for individual behaviors on the Mouse-Ventral1 dataset. Gray bars indicate shuffle (Materials and methods). *p≤0.05, **p≤0.01, ***p≤0.001, repeated measures ANOVA with a post-hoc Tukey’s honestly significant difference test. (<bold>D</bold>) Similar to (<bold>C</bold>), but for Mouse-Ventral2. Model and shuffle were compared with paired t-tests with Bonferroni correction. (<bold>E</bold>) Similar to (<bold>C</bold>), but for Mouse-Openfield. (<bold>F</bold>) Similar to (<bold>D</bold>), but for Mouse-Homecage. (<bold>G</bold>) Similar to (<bold>D</bold>), but for Mouse-Social. (<bold>H</bold>) Similar to (<bold>C</bold>), but for Sturman-EPM. (<bold>I</bold>) Similar to (<bold>C</bold>), but for Sturman-FST. (<bold>J</bold>) Similar to (<bold>C</bold>), but for Sturman-OFT. (<bold>K</bold>) Similar to (<bold>D</bold>), but for Fly dataset. (<bold>L</bold>) F1 score on individual behaviors (circles) for DeepEthogram-medium vs. human performance. Circles indicate the average performance across splits for behaviors in datasets with multiple human labels. Gray line: unity. Model vs. human performance: p=0.067, paired t-test. (<bold>M</bold>) Model F1 vs. the percent of frames in the training set with the given behavior. Each circle is one behavior for one split of the data. (<bold>N</bold>) Model accuracy on frames for which two human labelers agreed or disagreed. Paired t-tests with Bonferroni correction. (<bold>O</bold>) Similar to (<bold>N</bold>), but for F1. (<bold>P</bold>) Ethogram examples. Dark color indicates the behavior is present. Top: human labels. Bottom: DeepEthogram-medium predictions. The accuracy and F1 score for each behavior, and the overall accuracy and F1 scores are shown. Examples were chosen to be similar to the model’s average by behavior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>DeepEthogram performance, precision.</title><p>All results are from the test sets only. (<bold>A</bold>) Overall precision for each model size and dataset. Error bars indicate mean ± SEM across five random splits of the data (three for Sturman-EPM). (<bold>B</bold>) Precision for DeepEthogram-medium for individual behaviors on the Mouse-Ventral1 dataset. *p≤0.05, **p≤0.01, ***p≤0.001, repeated measures ANOVA with post-hoc Tukey’s honestly significant difference test. (<bold>C</bold>) Similar to (<bold>B</bold>), but for Mouse-Ventral2. Paired t-tests with Bonferroni correction. (<bold>D</bold>) Similar to (<bold>B</bold>), but for Mouse-Openfield. (<bold>E</bold>) Similar to (<bold>D</bold>), but for Mouse-Homecage. (<bold>F</bold>) Similar to (<bold>C</bold>), but for Mouse-Social. (<bold>G</bold>) Similar to (<bold>B</bold>), but for Sturman-EPM. (<bold>H</bold>) Similar to (<bold>B</bold>), but for Sturman-FST. (<bold>I</bold>) Similar to (<bold>B</bold>), but for Sturman-OFT. (<bold>J</bold>) Similar to (<bold>C</bold>), but for Fly dataset. (<bold>K</bold>) Precision on individual behaviors for DeepEthogram-medium vs. human performance. Circles are average performance across data splits for individual behaviors for all datasets with multiple human labels. Model performance vs. human performance: p=0.529, paired t-test. (<bold>L</bold>) Model precision vs. the percent of frames in the training set with the given behavior. Each point is for one behavior for one split of the data. (<bold>M</bold>) Model precision on frames for which two human labelers agreed or disagreed. Asterisks indicate p&lt;0.05, paired t-test with Bonferroni correction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>DeepEthogram performance, recall.</title><p>All results are from the test sets only. (<bold>A</bold>) Overall recall for each model size and dataset. Error bars indicate mean ± SEM across five random splits of the data (three for Sturman-EPM). (<bold>B</bold>) Recall for DeepEthogram-medium for individual behaviors on the Mouse-Ventral1 dataset. *p≤0.05, **p≤0.01, ***p≤0.001, repeated measures ANOVA with post-hoc Tukey’s honestly significant difference test. (<bold>C</bold>) Similar to (<bold>B</bold>), but for Mouse-Ventral2. Paired t-tests with Bonferroni correction. (<bold>D</bold>) Similar to (<bold>B</bold>), but for Mouse-Openfield. (<bold>E</bold>) Similar to (<bold>D</bold>), but for Mouse-Homecage. (<bold>F</bold>) Similar to (<bold>C</bold>), but for Mouse-Social. (<bold>G</bold>) Similar to (<bold>B</bold>), but for Sturman-EPM. (<bold>H</bold>) Similar to (<bold>B</bold>), but for Sturman-FST. (<bold>I</bold>) Similar to (<bold>B</bold>), but for Sturman-OFT. (<bold>J</bold>) Similar to (<bold>C</bold>), but for Fly dataset. (<bold>K</bold>) Recall on individual behaviors for DeepEthogram-medium vs. human performance. Shown is the average performance across splits for all datasets with multiple human labels. Circles are average performance across data splits for individual behaviors for all datasets with multiple human labels. Model performance vs. human performance: p&lt;0.035, paired t-test. (<bold>L</bold>) Model precision vs. the percent of frames in the training set with the given behavior. Each point is for one behavior for one split of the data. (<bold>M</bold>) Model recall on frames for which two human labelers agreed or disagreed. Asterisks indicate p&lt;0.05, paired t-test with Bonferroni correction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>DeepEthogram performance, area under the receiver operating characteristic curve (AUROC).</title><p>All results are from the test sets only. (<bold>A</bold>) Overall recall for each model size and dataset. Error bars indicate mean ± SEM across five random splits of the data (three for Sturman-EPM). (<bold>B</bold>) AUROC for DeepEthogram-medium for individual behaviors on the Mouse-Ventral1 dataset. *p≤0.05, **p≤0.01, ***p≤0.001, paired t-test with Bonferroni correction. (<bold>C</bold>) Similar to (<bold>B</bold>), but for Mouse-Ventral2. (<bold>D</bold>) Similar to (<bold>B</bold>), but for Mouse-Openfield. (<bold>E</bold>) Similar to (<bold>B</bold>), but for Mouse-Homecage. (<bold>F</bold>) Similar to (<bold>B</bold>), but for Mouse-Social. (<bold>G</bold>) Similar to (<bold>B</bold>), but for Sturman-EPM. (<bold>H</bold>) Similar to (<bold>B</bold>), but for Sturman-FST. (<bold>I</bold>) Similar to (<bold>B</bold>), but for Sturman-OFT. (<bold>J</bold>) Similar to (<bold>B</bold>), but for Fly dataset. (<bold>K</bold>) Model AUROC vs. the percent of frames in the training set with the given behavior. Each point is for one behavior for one split of the data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Ethogram examples for the Mouse-Ventral1 dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp4-v2.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Ethogram examples for the Mouse-Ventral2 dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp5-v2.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>Ethogram examples for the Mouse-Openfield dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp6-v2.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Ethogram examples for the Mouse-Homecage dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp7-v2.tif"/></fig><fig id="fig3s8" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 8.</label><caption><title>Ethogram examples for the Mouse-Social dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp8-v2.tif"/></fig><fig id="fig3s9" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 9.</label><caption><title>Ethogram examples for the Sturman-EPM dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp9-v2.tif"/></fig><fig id="fig3s10" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 10.</label><caption><title>Ethogram examples for the Sturman-FST dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp10-v2.tif"/></fig><fig id="fig3s11" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 11.</label><caption><title>Ethogram examples for the Sturman-OFT dataset.</title><p>(<bold>A</bold>) An example ethogram with above-average performance, showing the human labels, estimated probabilities for each behavior from DeepEthogram-medium, and the thresholded and postprocessed predictions, for data from the test set. The accuracy and F1 score for each behavior are shown, along with the overall accuracy and overall F1 score. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for approximately average performance and below-average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp11-v2.tif"/></fig><fig id="fig3s12" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 12.</label><caption><title>DeepEthogram exhibits position and heading invariance.</title><p>Nine randomly selected examples of the ‘face groom’ behavior from the Mouse-Openfield dataset. All examples were identified as ‘face groom’ by DeepEthogram-medium. The examples include different videos and different mice.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig3-figsupp12-v2.tif"/></fig></fig-group><p>We also analyzed the model’s performance for each individual behavior. The model achieved F1 scores of 0.7 or higher for many behaviors, even reaching F1 scores above 0.9 in some cases (<xref ref-type="fig" rid="fig3">Figure 3C–K</xref>). DeepEthogram’s performance significantly exceeded chance levels of performance on nearly all behaviors across datasets (<xref ref-type="fig" rid="fig3">Figure 3C–K</xref>). Given that F1 scores may not be intuitive to understand in terms of their values, we examined individual snippets of videos with a range of F1 scores and found that F1 scores similar to the means for our datasets were consistent with overall accurate predictions (<xref ref-type="fig" rid="fig3">Figure 3P</xref>, <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplements 4</xref>–<xref ref-type="fig" rid="fig3s11">11</xref>). We note that the F1 score is a demanding metric, and even occasional differences on single frames or a small number of frames can substantially decrease the F1 score. Relatedly, the model achieved high precision, recall, and AUROC values for individual behaviors (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B–J</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B–J</xref>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3B–J</xref>). The performance of the model depended on the frequency with which a behavior occurred (c.f. <xref ref-type="fig" rid="fig2">Figure 2</xref> right panels and <xref ref-type="fig" rid="fig3">Figure 3C–J</xref>). Strikingly, however, performance was relatively high even for behaviors that occurred rarely, that is, in less than 10% of video frames (<xref ref-type="fig" rid="fig3">Figure 3M</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1L</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2L</xref>, and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3K</xref>). The performance tended to be highest for DeepEthogram-slow and worst for DeepEthogram-fast, but the differences between model versions were generally small and varied across behaviors (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>). The high-performance values are, in our opinion, impressive given that they were calculated based on single-frame predictions for each behavior, and thus performance will be reduced if the model misses the onset or offset of a behavior bout by even a single frame. These high values suggest that the model not only correctly predicted which behaviors happened and when but also had the resolution to correctly predict the onset and offset of bouts.</p><p>To better understand the performance of DeepEthogram, we benchmarked the model by comparing its performance to the degree of agreement between expert human labelers. Multiple researchers with extensive experience in monitoring and analyzing mouse behavior videos independently labeled the same set of videos for the Mouse-Ventral1, Mouse-Ventral2, Mouse-Openfield, Mouse-Social, and Mouse-Homecage datasets, allowing us to measure the consistency across human experts. Also, Sturman et al. released the labels of each of three expert human labelers (<xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>). The Fly dataset has more than 3 million frames and thus was too large to label multiple times. Human-human performance was calculated by defining one labeler as the ‘ground truth’ and the other labelers as ‘predictions’ and then computing the same performance metrics as for DeepEthogram. In this way, ‘human accuracy’ is the same as the percentage of scores on which two humans agreed. Strikingly, the overall accuracy, F1 scores, precision, and recall for DeepEthogram approached that of expert human labelers (<xref ref-type="fig" rid="fig3">Figure 3A, B, C, E, H, I, J and L</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A,B,D,G,H,I,K</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). In many cases, DeepEthogram’s performance was statistically indistinguishable from human-level performance, and in the cases in which humans performed better, the difference in performance was generally small. Notably, the behaviors for which DeepEthogram had the lowest performance tended to be the behaviors for which humans had less agreement (lower human-human F1 score) (<xref ref-type="fig" rid="fig3">Figure 3L</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1K</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2K</xref>). Relatedly, DeepEthogram performed best on the frames in which the human labelers agreed and did more poorly in the frames in which humans disagreed (<xref ref-type="fig" rid="fig3">Figure 3N and O</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1M</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2M</xref>). Thus, there is a strong correlation between DeepEthogram and human performance, and the values for DeepEthogram’s performance approach those of expert human labelers.</p><p>The behavior with the worst model performance was ‘defecate’ from the Mouse-Openfield dataset (<xref ref-type="fig" rid="fig2">Figures 2C</xref> and <xref ref-type="fig" rid="fig3">3E</xref>). Notably, defecation was incredibly rare, occurring in only 0.1% of frames. Furthermore, the act of defecation was not actually visible from the videos. Rather, human labelers marked the ‘defecate’ behavior when new fecal matter appeared, which involved knowledge of the foreground and background, tracking objects, and inferring unseen behavior. This type of behavior is expected to be challenging for DeepEthogram because the model is based on images and local motion and thus will fail when the behavior cannot be directly observed visually.</p><p>The model was able to accurately predict the presence of a behavior even when that behavior happened in different locations in the environment and with different orientations of the animal (<xref ref-type="fig" rid="fig3s12">Figure 3—figure supplement 12</xref>). For example, the model predicted face grooming accurately both when the mouse was in the top-left quadrant of the chamber and facing north and when the mouse was in the bottom-right quadrant facing west. This result is particularly important for many analyses of behavior that are concerned with the behavior itself, rather than where that behavior happens.</p><p>One striking feature was DeepEthogram’s high performance even on rare behaviors. From our preliminary work building up to the model presented here, we found that simpler models performed well on behaviors that occurred frequently and performed poorly on the infrequent behaviors. Given that, in many datasets, the behaviors of interest are infrequent (<xref ref-type="fig" rid="fig2">Figure 2</xref>), we placed a major emphasis on performance in cases with large class imbalances, meaning when some behaviors only occurred in a small fraction of frames. In brief, we accounted for class imbalances in the initialization of the model parameters (Materials and methods). We also changed the cost function to weight errors on rare classes more heavily than errors on common classes. We used a form of regularization specific to transfer learning to reduce overfitting. Finally, we tuned the threshold for converting the model’s probability of a given behavior into a classification of whether that behavior was present. Without these added features, the model simply learned to ignore rare classes. We consider these steps toward identifying rare behaviors to be of major significance for effective application in common experimental datasets.</p></sec><sec id="s2-4"><title>DeepEthogram accurately predicts behavior bout statistics</title><p>Because DeepEthogram produces predictions on individual frames, it allows for subsequent analyses of behavior bouts, such as the number of bouts, the duration of bouts, and the transition probability from one behavior to another. These statistics of bouts are often not available if researchers only record the overall time spent on a behavior with a stopwatch, rather than providing frame-by-frame labels. We found a strong correspondence for the statistics of behavior bouts between the predictions of DeepEthogram and those from human labels. We first focused on results at the level of individual videos for the Mouse-Ventral1 dataset, comparing the model predictions and human labels for the percent of time spent on each behavior, the number of bouts per behavior, and the mean bout duration (<xref ref-type="fig" rid="fig4">Figure 4A–C</xref>). Note that the model was trained on the labels from Human 1. For the time spent on each behavior, the model predictions and human labels were statistically indistinguishable (one-way ANOVA, p&gt;0.05; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). For the number of bouts and bout duration, the model was statistically indistinguishable from the labels of Human 1, on which it was trained. Some differences were present between the model predictions and the other human labels not used for training (<xref ref-type="fig" rid="fig4">Figure 4B,C</xref>). However, the magnitude of these differences was within the range of differences between the multiple human labelers (<xref ref-type="fig" rid="fig4">Figure 4B,C</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>DeepEthogram performance on bout statistics.</title><p>All results from DeepEthogram-medium, test set only. (<bold>A–C</bold>) Comparison of model predictions and human labels on individual videos from the Mouse-Ventral1 dataset. Each point is one behavior from one video. Colors indicate video ID. Error bars: mean ± SEM (n = 18 videos). Asterisks indicate p&lt;0.05, one-way ANOVA with Tukey’s multiple comparison test. No asterisk indicates p&gt;0.05. (<bold>D–F</bold>) Comparison of model predictions and human labels on all behaviors for all datasets. Each circle is one behavior from one dataset, averaged across splits of the data. Gray line: unity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig4-v2.tif"/></fig><p>To summarize the performance of DeepEthogram on bout statistics for each behavior in all datasets, we averaged the time spent, number of bouts, and bout duration for each behavior across the five random splits of the data into train, validation, and test sets. This average provides a quantity similar to an average across multiple videos, and an average across multiple videos is likely how some end-users will report their results. The values from the model were highly similar to the those from the labels on which it was trained (Human 1 labels) for the time spent per behavior, the number of bouts, and the mean bout duration (<xref ref-type="fig" rid="fig4">Figure 4D–F</xref>). Together, these results show that DeepEthogram accurately predicts bout statistics that might be of interest to biologist end-users.</p></sec><sec id="s2-5"><title>DeepEthogram approaches expert-level human performance for bout statistics and transitions</title><p>Next, we benchmarked DeepEthogram’s performance on bout statistics by comparing its performance to the level of agreement between expert human labelers. We started by looking at the time spent on each behavior in single videos for the Mouse-Ventral1 and Sturman-OFT datasets. We compared the labels from Human 1 to the model predictions and to the labels from Humans 2 and 3 (<xref ref-type="fig" rid="fig5">Figure 5A,B</xref>). In general, there was strong agreement between the model and Human 1 and among human labelers (<xref ref-type="fig" rid="fig5">Figure 5A,B</xref>, left and middle). To directly compare model performance to human-human agreement, we plotted the absolute difference between the model and Human 1 versus the absolute difference between Human 1 and Humans 2 and 3 (<xref ref-type="fig" rid="fig5">Figure 5A,B</xref>, right). Model agreement was significantly worse than human-human agreement when considering individual videos. However, the magnitude of this difference was small, implying that discrepancies in behavior labels introduced by the model were only marginally larger than the variability between multiple human labelers.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Comparison of model performance to human performance on bout statistics.</title><p>All model data are from DeepEthogram-medium, test set data. r values indicate Pearson’s correlation coefficient. (<bold>A</bold>) Performance on Mouse-Ventral1 dataset for time spent. Each circle is one behavior from one video. Left: Human 1 vs. model. Middle: Human 1 vs. Humans 2 and 3. Both Humans 2 and 3 are shown on the y-axis. Right: absolute error between Human 1 and model vs. absolute error between Human 1 and each of Humans 2 and 3. Model difference vs. human difference: p&lt;0.001, paired t-test. (<bold>B</bold>) Similar to (<bold>A</bold>), but for Sturman-OFT dataset. Right: model difference vs. human difference: p&lt;0.001, paired t-test. (<bold>C–E</bold>) Performance on all datasets with multiple human labelers (Mouse-Ventral1, Mouse-Openfield, Sturman-OFT, Sturman-EPM, Sturman-FST). Each point is one behavior from one dataset, averaged across data splits. Performance for Humans 2 and 3 were averaged. Similar to <xref ref-type="fig" rid="fig4">Figure 4D–F</xref>, but only for datasets with multiple labelers. Left: Human 1 vs. model. Middle: Human 1 vs. Humans 2 and 3. Right: absolute error between Human 1 and model vs. absolute error between Human 1 and each of Humans 2 and 3. p&gt;0.05, paired t-test with Bonferroni correction, in (<bold>C–E</bold>) right panels. (<bold>F–H</bold>) Example transition matrices for Mouse-Ventral1 dataset. For humans and models, transition matrices were computed for each data split and averaged across splits.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Performance of keypoint-based behavior classification on the Mouse-Openfield dataset.</title><p>(<bold>A</bold>) Left: keypoints identified, labeled, and predicted using DeepLabCut. Right: example keypoint sequence predicted by DeepLabCut from a held-out video. (<bold>B</bold>) Example images from held-out videos showing good DeepLabCut performance. (<bold>C</bold>) Histograms of behavioral features derived from keypoints for each behavior. (<bold>D</bold>) Accuracy on the test set. Error bars: mean ± SEM, n = 5 data splits. *p≤0.05, **p≤0.01, ***p≤0.001, repeated measures ANOVA with post-hoc Tukey’s honestly significant difference test. Human vs. shuffle results not shown for clarity. (<bold>E</bold>) Similar to (<bold>A</bold>), but for F1. (<bold>F</bold>) Accuracy of keypoint-based behavioral classification vs. DeepEthogram. Each point is one behavior from one model type (colored as in <bold>D</bold>) and one data split. (<bold>G</bold>) similar to (<bold>F</bold>), but for F1. (<bold>H</bold>) Human vs. model time spent exhibiting each behavior. Each point is one behavior from one model type, averaged across data splits. (<bold>I</bold>) similar to (<bold>H</bold>), but for average bout number. (<bold>J</bold>) similar to (<bold>H</bold>), but for average frames per bout.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Comparison with unsupervised methods.</title><p>(<bold>A</bold>) B-SoID pipeline. (<bold>B</bold>) B-SoID behavioral space. Shown are a random sample of points that B-SoID labeled confidently (57% of total data). Left: colors are B-SoID cluster assignments. Right: colors (0–5) indicate human labeled behaviors. Note the overall lack of clustering of human-identified colors. (<bold>C</bold>) B-SoID classifier confusion matrix. X-axis: label predicted by the B-SoID classifier (Random forest). Note the good performance; the classifier successfully recaptures the HDBScan clustering, indicating the B-SoID is performing as expected. (<bold>D</bold>) Comparison between B-SoID cluster assignments and human labels. Left: each element is the proportion of B-soid clusters that correspond to the given human label. Rows sum to one. Right: each element is the proportion of human labels corresponding to the given B-SoID cluster. Columns sum to 1. Red outlines indicate the B-SoID cluster with maximum correspondence to the human label. Note the overall lack of a consistent structure between human-identified behaviors and B-SoID clusters. One exception: 74% of cluster six corresponds to the ‘rearing’ behavior. (<bold>E</bold>) Performance comparison between the unsupervised pipeline and DeepEthogram-fast. *p≤0.05, **p≤0.01, ***p≤0.001, repeated measures ANOVA with post-hoc Tukey’s honestly significant difference test. Human vs. shuffle results not shown for clarity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig5-figsupp2-v2.tif"/></fig></fig-group><p>To summarize our benchmarking of model performance on bout statistics for each behavior in all datasets with multiple human labelers, we again averaged the time spent, number of bouts, and bout duration for each behavior across the five random splits of the data to obtain a quantity similar to an average across multiple videos (<xref ref-type="fig" rid="fig5">Figure 5C–E</xref>). For time spent per behavior, number of bouts, and mean bout length, the human-model differences were similar, and not significantly different, in magnitude to the differences between humans (<xref ref-type="fig" rid="fig5">Figure 5C–E</xref>, right column). The transition probabilities between behaviors were also broadly similar between Human 1, Human 2, and the model (<xref ref-type="fig" rid="fig5">Figure 5F–H</xref>). Furthermore, model-human differences and human-human differences were significantly correlated (<xref ref-type="fig" rid="fig5">Figure 5C–E</xref>, right column), again showing that DeepEthogram models are more reliable for situations in which multiple human labelers agree (see <xref ref-type="fig" rid="fig3">Figure 3N–O</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1M</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2M</xref>).</p><p>Therefore, the results from <xref ref-type="fig" rid="fig5">Figure 5A,B</xref> indicate that the model predictions are noisier than human-human agreement on the level of individual videos. However, when averaged across multiple videos (<xref ref-type="fig" rid="fig5">Figure 5C–E</xref>), this noise averages out and results in similar levels of variability for the model and multiple human labelers. Given that DeepEthogram performed slightly worse on F1 scores relative to expert humans but performed similarly to humans on bout statistics, it is possible that for rare behaviors DeepEthogram misses a small number of bouts, which would minimally affect bout statistics but could decrease the overall F1 score.</p><p>Together, our results from <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref> indicate that DeepEthogram’s predictions match well the labels defined by expert human researchers. Further, these model predictions allow easy post-hoc analysis of additional statistics of behaviors, which may be challenging to obtain with traditional manual methods.</p></sec><sec id="s2-6"><title>Comparison to existing methods based on keypoint tracking</title><p>While DeepEthogram operates directly on the raw pixel values in the videos, other methods exist that first track body keypoints and then perform behavior classification based on these keypoints (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>; <xref ref-type="bibr" rid="bib31">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Nilsson, 2020</xref>; <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>). One such approach that is appealing due to its simplicity and clarity was developed by Sturman et al. and was shown to be superior to commercially available alternatives (<xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>). In their approach, DeepLabCut (<xref ref-type="bibr" rid="bib44">Mathis, 2018</xref>) is used to estimate keypoints and then a multilayer perceptron architecture is used to classify features of these keypoints into behaviors. We compared the performance of DeepEthogram and this alternate approach using our custom implementation of the Sturman et al. methods (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We focused our comparison on the Mouse-Openfield dataset, which is representative of videos used in a wide range of biological studies. We used DeepLabCut (<xref ref-type="bibr" rid="bib44">Mathis, 2018</xref>) to identify the position of the four paws, the base of the tail, the tip of the tail, and the nose. These keypoints could be used to distinguish behaviors. For example, the distance between the nose and the base of the tail was highest when the mouse was locomoting (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>). However, the accuracy and F1 scores for DeepEthogram generally exceeded those identified from classifiers based on features of these keypoints (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D–G</xref>). For bout statistics, the two methods performed similarly well (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1F–J</xref>). Thus, for at least one type of video and dataset, DeepEthogram outperformed an established approach.</p><p>There are several reasons why DeepEthogram might have done better on accuracy and F1 score. First, the videos tested were relatively low resolution, which restricted the number of keypoints on the mouse’s body that could be labeled. High-resolution videos with more keypoints may improve the keypoint-based classification approach. Second, our videos were recorded with a top-down view, which means that the paw positions were often occluded by the mouse’s body. A bottom-up or side view could allow for better identification of keypoints and may result in improved performance for the keypoint-based methods.</p><p>An alternative approach to DeepEthogram and other supervised classification pipelines could be to use an unsupervised behavior classification followed by human labeling of behavior clusters. In this approach, an unsupervised algorithm identifies behavior clusters without user input, and then the researcher identifies the cluster that most resembles their behavior of interest (e.g., ‘cluster 3 looks like face grooming’). The advantage of this approach is that it involves less researcher time due to the lack of supervised labeling. However, this approach is not designed to identify predefined behaviors of interest and thus, in principle, might not be well suited for the goal of supervised classification. We tested one such approach starting with the Mouse-Openfield dataset and DeepLabCut-generated keypoints (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A, C</xref>). We used B-SoID (<xref ref-type="bibr" rid="bib28">Hsu and Yttri, 2019</xref>), an unsupervised classification pipeline for animal behavior, which identified 11 behavior clusters for this dataset (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2A, B</xref>). These clusters were separable in a low-dimensional behavior space (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2B</xref>), and B-SoID’s fast approximation algorithm showed good performance (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2C</xref>). For every frame in our dataset, we had human labels, DeepEthogram predictions, and B-SoID cluster assignments. By looking at the joint distributions of B-SoID clusters and human labels, there appeared to be little correspondence (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2D</xref>). To assign human labels to B-SoID clusters, for each researcher-defined behavior, we picked the B-SoID cluster that had the highest overlap with the behavior of interest (red boxes, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2D</xref>, right). We then evaluated these ‘predictions’ compared to DeepEthogram. For most behaviors, DeepEthogram performed better than this alternative pipeline (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2E</xref>).</p><p>We note that the unsupervised clustering with post-hoc assignment of human labels is not the use for which B-SoID (<xref ref-type="bibr" rid="bib28">Hsu and Yttri, 2019</xref>) and other unsupervised algorithms (<xref ref-type="bibr" rid="bib73">Wiltschko, 2015</xref>; <xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>) were designed. Unsupervised approaches are designed to discover repeated behavior motifs directly from data, without humans predefining the behaviors of interest (<xref ref-type="bibr" rid="bib13">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Egnor and Branson, 2016</xref>), and B-SoID succeeded in this goal. However, if one’s goal is the automatic labeling of human-defined behaviors, our results show that DeepEthogram or other supervised machine learning approaches are better choices.</p></sec><sec id="s2-7"><title>DeepEthogram requires little training data to achieve high performance</title><p>We evaluated how much data a user must label to train a reliable model. We selected 1, 2, 4, 8, 12, or 16 random videos for training and used the remaining videos for evaluation. We only required that each training set had at least one frame of each behavior. We trained the feature extractors, extracted the features, and trained the sequence models for each split of the data into training, validation, and test sets. We repeated this process five times for each number of videos, resulting in 30 trained models per dataset. Given the large number of dataset variants for this analysis, to reduce overall computation time, we used DeepEthogram-fast and focused on only the Mouse-Ventral1, Mouse-Ventral2, and Fly datasets. Also, we trained the flow generator only once and kept it fixed for all experiments. For all but the rarest behaviors, the models performed at high levels even with only one labeled video in the training set (<xref ref-type="fig" rid="fig6">Figure 6A–C</xref>). For all the behaviors studied across datasets, the performance measured as accuracy or F1 score approached seemingly asymptotic levels after training on approximately 12 videos. Therefore, a training set of this size or less is likely sufficient for many cases.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>DeepEthogram performance as a function of training set size.</title><p>(<bold>A</bold>) Accuracy (top) and F1 score (bottom) for DeepEthogram-fast as a function of the number of videos in the training set for Mouse-Ventral1, shown for each behavior separately. The mean is shown across five random selections of training videos. (<bold>B, C</bold>) Similar to (<bold>A</bold>), except for the Mouse-Ventral2 dataset and Fly dataset. (<bold>D</bold>) Accuracy of DeepEthogram-fast as a function of the number of frames with the behavior of interest in the training set. Each point is one behavior for one random split of the data, across datasets. The black line shows the running average. For reference, 104 frames is ~5 min of behavior at 30 frames per second.(<bold>E</bold>) Similar to (<bold>D</bold>), except for F1 score. (<bold>F</bold>) Accuracy for the predictions of DeepEthogram-fast using the feature extractors only or using the sequence model. Each point is one behavior from one split of the data, across datasets, for the splits used in (<bold>D, E</bold>). (<bold>G</bold>) Similar to (<bold>F</bold>), except for F1 score.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig6-v2.tif"/></fig><p>We also analyzed the model’s performance as a function of the number of frames of a given behavior present in the training set. For each random split, dataset, and behavior, we had a wide range of the number of frames containing a behavior of interest. Combining all these splits, datasets, and behaviors together, we found that the model performed with more than 90% accuracy when trained with only 80 example frames of a given behavior and over 95% accuracy with only 100 positive example frames (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Furthermore, DeepEthogram achieved an F1 score of 0.7 with only 9000 positive example frames, which corresponds to about 5 min of example behavior at 30 frames per second (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, see <xref ref-type="fig" rid="fig3">Figure 3P</xref> for an example of ~0.7 F1 score). The total number of frames required to reach this number of positive example frames depends on how frequent the behavior is. If the behavior happens 50% of the time, then 18,000 total frames are required to reach 9000 positive example frames. Instead, if the behavior occurs 10% of the time, then 90,000 total frames are required. In addition, when the sequence model was used instead of using the predictions directly from the feature extractors, model performance was higher (<xref ref-type="fig" rid="fig6">Figure 6F,G</xref>) and required less training data (data not shown), emphasizing the importance of using long timescale information in the prediction of behaviors. Therefore, DeepEthogram models require little training to achieve high performance. As expected, as more training data are added, the performance of the model improves, but this rather light dependency on the amount of training data makes DeepEthogram amenable for even small-scale projects.</p></sec><sec id="s2-8"><title>DeepEthogram allows rapid inference time</title><p>A key aspect of the functionality of the software is the speed with which the models can be trained and predictions about behaviors made on new videos. Although the versions of DeepEthogram vary in speed, they are all fast enough to allow functionality in typical experimental pipelines. On modern computer hardware, the flow generator and feature extractors can be trained in approximately 24 hr. In many cases, these models only need to be trained once. Afterwards, performing inference to make predictions about the behaviors present on each frame can be performed at ~150 frames per second for videos at 256 × 256 resolution for DeepEthogram-fast, at 80 frames per second for DeepEthogram-medium, and 13 frames per second for DeepEthogram-slow (<xref ref-type="table" rid="table1">Table 1</xref>). Thus, for a standard 30 min video collected at 60 frames per second, inference could be finished in 12 min for DeepEthogram-fast or 2 hr for DeepEthogram-slow. Importantly, the training of the models and the inference involve zero user time because they do not require manual input or observation from the user. Furthermore, this speed is rapid enough to get results quickly after experiments to allow fast analysis and experimental iteration. However, the inference time is not fast enough for online or closed-loop experiments.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Inference speed.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="3" valign="top">Dataset</th><th align="left" rowspan="3" valign="top">Resolution</th><th align="left" colspan="6" valign="top">Inference time (FPS)</th></tr><tr><th align="left" colspan="3" valign="top">Titan RTX</th><th align="left" colspan="3" valign="top">Geforce 1080 Ti</th></tr><tr><th align="left" valign="top">DEG_f</th><th align="left" valign="top">DEG_m</th><th align="left" valign="top">DEG_s</th><th align="left" valign="top">DEG_f</th><th align="left" valign="top">DEG_m</th><th align="left" valign="top">DEG_s</th></tr></thead><tbody><tr><td align="left" valign="top">Mouse-Ventral1</td><td align="char" char="." valign="top">256 × 256</td><td align="char" char="." valign="top">235</td><td align="char" char="." valign="top">128</td><td align="char" char="." valign="top">34</td><td align="char" char="." valign="top">152</td><td align="char" char="." valign="top">76</td><td align="char" char="." valign="top">13</td></tr><tr><td align="left" valign="top">Mouse-Ventral2</td><td align="char" char="." valign="top">256 × 256</td><td align="char" char="." valign="top">249</td><td align="char" char="." valign="top">132</td><td align="char" char="." valign="top">34</td><td align="char" char="." valign="top">157</td><td align="char" char="." valign="top">79</td><td align="char" char="." valign="top">13</td></tr><tr><td align="left" valign="top">Mouse-Openfield</td><td align="char" char="." valign="top">256 × 256</td><td align="char" char="." valign="top">211</td><td align="char" char="." valign="top">117</td><td align="char" char="." valign="top">33</td><td align="char" char="." valign="top">141</td><td align="char" char="." valign="top">80</td><td align="char" char="." valign="top">13</td></tr><tr><td align="left" valign="top">Mouse-Homecage</td><td align="char" char="." valign="top">352 × 224</td><td align="char" char="." valign="top">204</td><td align="char" char="." valign="top">102</td><td align="char" char="." valign="top">28</td><td align="char" char="." valign="top">132</td><td align="char" char="." valign="top">70</td><td align="char" char="." valign="top">11</td></tr><tr><td align="left" valign="top">Mouse-Social</td><td align="char" char="." valign="top">224 × 224</td><td align="char" char="." valign="top">324</td><td align="char" char="." valign="top">155</td><td align="char" char="." valign="top">44</td><td align="char" char="." valign="top">204</td><td align="char" char="." valign="top">106</td><td align="char" char="." valign="top">17</td></tr><tr><td align="left" valign="top">Sturman-EPM</td><td align="char" char="." valign="top">256 × 256</td><td align="char" char="." valign="top">240</td><td align="char" char="." valign="top">123</td><td align="char" char="." valign="top">34</td><td align="char" char="." valign="top">157</td><td align="char" char="." valign="top">83</td><td align="char" char="." valign="top">13</td></tr><tr><td align="left" valign="top">Sturman-FST</td><td align="char" char="." valign="top">224 × 448</td><td align="char" char="." valign="top">157</td><td align="char" char="." valign="top">75</td><td align="char" char="." valign="top">21</td><td align="char" char="." valign="top">106</td><td align="char" char="." valign="top">51</td><td align="char" char="." valign="top">9</td></tr><tr><td align="left" valign="top">Sturman-OFT</td><td align="char" char="." valign="top">256 × 256</td><td align="char" char="." valign="top">250</td><td align="char" char="." valign="top">125</td><td align="char" char="." valign="top">34</td><td align="char" char="." valign="top">159</td><td align="char" char="." valign="top">84</td><td align="char" char="." valign="top">13</td></tr><tr><td align="left" valign="top">Flies</td><td align="char" char="." valign="top">128 × 192</td><td align="char" char="." valign="top">623</td><td align="char" char="." valign="top">294</td><td align="char" char="." valign="top">89</td><td align="char" char="." valign="top">378</td><td align="char" char="." valign="top">189</td><td align="char" char="." valign="top">33</td></tr></tbody></table></table-wrap></sec><sec id="s2-9"><title>A GUI for beginning-to-end management of experiments</title><p>We developed a GUI for labeling videos, training models, and running inference (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Our GUI is similar in behavior to those for BORIS (<xref ref-type="bibr" rid="bib21">Friard et al., 2016</xref>) and JAABA (<xref ref-type="bibr" rid="bib31">Kabra et al., 2013</xref>). To train DeepEthogram models, the user first defines which behaviors of interest they would like to detect in their videos. Next, the user imports a few videos into DeepEthogram, which automatically calculates video statistics and organizes them into a consistent file structure. Then the user clicks a button to train the <italic>flow generator</italic> model, which occurs without user time. While this model is training, the user can go through a set of videos frame-by-frame and label the presence or absence of all behaviors in these videos. Labeling is performed with simple keyboard or mouse clicks at the onset and offset of a given behavior while scrolling through a video in a viewing window. After a small number of videos have been labeled and the <italic>flow generator</italic> is trained, the user then clicks a button to train the <italic>feature extractors</italic>, which occurs without user input and saves the extracted features to disk. Finally, the <italic>sequence model</italic> can be trained automatically on these saved features by clicking another button. All these training steps could in many cases be performed once per project. With these trained models, the user can import new videos and click the <italic>predict</italic> button, which estimates the probability of each behavior on each frame. This GUI therefore presents a single interface for labeling videos, training models, and generating predictions on new videos. Importantly, this interface requires no programming by the end-user.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Graphical user interface.</title><p>(<bold>A</bold>) Example DeepEthogram window with training steps highlighted. (<bold>B</bold>) Example DeepEthogram window with inference steps highlighted.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63377-fig7-v2.tif"/></fig><p>The GUI also includes an option for users to manually check and edit the predictions output by the model. The user can load into the GUI a video and predictions made by the model. By scrolling through the video, the user can see the predicted behaviors for each frame and update the labels of the behavior manually. This allows users to validate the accuracy of the model and to fix errors should they occur. This process is expected to be fast because the large majority of frames are expected to be labeled correctly, based on our accuracy results, so the user can focus on the small number of frames associated with rare behaviors or behaviors that are challenging to detect automatically. Importantly, these new labels can then be used to retrain the models to obtain better performance on future experimental videos. Documentation for the GUI will be included on the project’s website.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We developed a method for automatically classifying each frame of a video into a set of user-defined behaviors. Our open-source software, called DeepEthogram, provides the code and user interface necessary to label videos and train DeepEthogram models. We show that modern computer vision methods for action detection based on pretrained deep neural networks can be readily applied to animal behavior datasets. DeepEthogram performed well on multiple datasets and generalized across videos and animals, even for identifying rare behaviors. Importantly, by design, CNNs ignore absolute spatial location and thus are able to identify behaviors even when animals are in different locations and orientations within a behavioral arena (<xref ref-type="fig" rid="fig3s12">Figure 3—figure supplement 12</xref>). We anticipate this software package will save researchers great amounts of time, will lead to more reproducible results by eliminating inter-researcher variability, and will enable experiments that may otherwise not be possible by increasing the number of experiments a lab can reasonably perform or the number of behaviors that can be investigated. DeepEthogram joins a growing community of open-source computer vision applications for biomedical research (<xref ref-type="bibr" rid="bib13">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Perona, 2014</xref>; <xref ref-type="bibr" rid="bib16">Egnor and Branson, 2016</xref>).</p><p>The models presented here performed well for all datasets tested. In general, we expect the models will perform well in cases in which there is a high degree of agreement between separate human labelers, as our results in <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref> indicate. As we have shown, the models do better with more training data. We anticipate that a common use of DeepEthogram will be to make automated predictions for each video frame followed by rapid and easy user-based checking and editing of the labels in the GUI for the small number of frames that may be inaccurately labeled. We note that these revised labels can then be used as additional training data to continually update the models and thus improve the performance on subsequent videos.</p><p>One of our goals for DeepEthogram was to make it general-purpose and applicable to all videos with behavior labels. DeepEthogram operates directly on the raw video pixels, which is advantageous because preprocessing is not required and the researcher does not need to make decisions about which features of the animal to track. Skeleton-based action recognition models, in which keypoints are used to predict behaviors, require a consistent skeleton as their input. A crucial step in skeleton-based action recognition is feature engineering, which means turning the x and y coordinates of keypoints (such as paws or joints) into features suitable for classification (such as the angle of specific joints). With different skeletons (such as mice, flies, or humans) or numbers of animals (one or more), these features must be carefully redesigned. Using raw pixel values as inputs to DeepEthogram allows for a general-purpose pipeline that can be applied to videos of all types, without the need to tailor preprocessing steps depending on the behavior of interest, species, number of animals, video angles, resolution, and maze geometries. However, DeepEthogram models are not expected to generalize across videos that differ substantially in any of these parameters. For example, models that detect grooming in top-down videos are unlikely to identify grooming in side-view videos.</p><p>Because DeepEthogram is a general-purpose pipeline, it will not perform as well as pipelines that are engineered for a specific task, arena, or species. For example, MARS was exquisitely engineered for social interactions between a black and white mouse (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>) and thus is expected to outperform DeepEthogram on videos of this type. Moreover, because DeepEthogram operates on raw pixels, it is possible that our models may perform more poorly on zoomed-out videos in which the animal is only a few pixels. Also, if the recording conditions change greatly, such as moving the camera or altering the arena background, it is likely that DeepEthogram will have to be retrained.</p><p>An alternate approach is to use innovative methods for estimating pose, including DeepLabCut (<xref ref-type="bibr" rid="bib44">Mathis, 2018</xref>; <xref ref-type="bibr" rid="bib47">Nath, 2019</xref>; <xref ref-type="bibr" rid="bib37">Lauer, 2021</xref>), LEAP (<xref ref-type="bibr" rid="bib58">Pereira, 2018b</xref>), and others (<xref ref-type="bibr" rid="bib24">Graving et al., 2019</xref>), followed by frame-by-frame classification of behaviors based on pose in a supervised (<xref ref-type="bibr" rid="bib66">Segalin, 2020</xref>; <xref ref-type="bibr" rid="bib50">Nilsson, 2020</xref>; <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>) or unsupervised (<xref ref-type="bibr" rid="bib28">Hsu and Yttri, 2019</xref>) way. Using pose for classification could make behavior classifiers faster to train, less susceptible to overfitting, and less demanding of computational resources. Using pose as an intermediate feature could allow the user to more easily assess model performance. Depending on the design, such skeleton-based action recognition could aid multi-animal experiments by more easily predicting behaviors separately for each animal, as JAABA does (<xref ref-type="bibr" rid="bib31">Kabra et al., 2013</xref>). While we demonstrated that DeepEthogram can accurately identify social interactions, it does not have the ability to track the identities of multiple mice and identify behaviors separately for each mouse. Furthermore, tracking keypoints on the animal gives valuable, human-understandable information for further analysis, such as time spent near the walls of an arena, distance traveled, and measures of velocity (<xref ref-type="bibr" rid="bib56">Pennington, 2019</xref>; <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>). In addition, specific aspects of an animal’s movements, such as limb positions and angles derived from keypoint tracking, can be directly related to each behavior of interest, providing an additional layer of interpretation and analysis of the behavior. DeepEthogram does not track parts of the animal’s body or position and velocity information, and instead it focuses only on the classification of human-defined behaviors. Finally, because DeepEthogram uses 11 frames at a time for inputs, as well as relatively large models, it is not easily applicable to real-time applications, such as the triggering of optogenetic stimulation based on ongoing behaviors.</p><p>DeepEthogram may prove to be especially useful when a large number of videos or behaviors need to be analyzed in a given project. These cases could include drug discovery projects or projects in which multiple genotypes need to be compared. Additionally, DeepEthogram could be used for standardized behavioral assays, such as those run frequently in a behavioral core facility or across many projects with standardized conditions. Importantly, whereas user time scales linearly with the number of videos for manual labeling of behaviors, user time for DeepEthogram is limited to only the labeling of initial videos for training the models and then can involve essentially no time on the user’s end for all subsequent movies. In our hands, it took approximately 1–3 hr for an expert researcher to label five behaviors in a 10 min movie from the Mouse-Openfield dataset. This large amount of time was necessary for researchers to scroll back and forth through a movie to mark behaviors that are challenging to identify by eye. If only approximately 10 human-labeled movies are needed for training the model, then only approximately 10–30 hr of user time would be required. Subsequently, tens of movies could be analyzed, across projects with similar recording conditions, without additional user time. DeepEthogram does require a fair amount of computer time (see Inference time above, Materials and methods); however, we believe that trading increasingly cheap and available computer time for valuable researcher effort is worthwhile. Notably, the use of DeepEthogram should make results more reproducible across studies and reduce variability imposed by inter-human labeling differences. Furthermore, in neuroscience experiments, DeepEthogram could aid identification of the starts and stops of behaviors to relate to neural activity measurements or manipulations.</p><p>Future extensions could continue to improve the accuracy and utility of DeepEthogram. First, DeepEthogram could be easily combined with an algorithm to track an animal’s location in an environment (<xref ref-type="bibr" rid="bib56">Pennington, 2019</xref>), thus allowing the identification of behaviors of interest and where those behaviors occur. Also, it would be interesting to use DeepEthogram’s optic flow snippets as inputs to unsupervised behavior pipelines, where they could help to uncover latent structure in animal behavior (<xref ref-type="bibr" rid="bib73">Wiltschko, 2015</xref>; <xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib2">Batty, 2019</xref>). In addition, while the use of CNNs for classification is standard practice in machine learning, recent works in temporal action detection use widely different sequence modeling approaches and loss functions (<xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>; <xref ref-type="bibr" rid="bib76">Zeng, 2019</xref>; <xref ref-type="bibr" rid="bib45">Monfort, 2020</xref>). Testing these different approaches in the DeepEthogram pipeline could further improve performance. Importantly, DeepEthogram was designed in a modular way to allow easy incorporation of new approaches as they become available. While inference is already fast, further development could improve inference speed by using low-precision weights, model quantization, or pruning. Furthermore, although our model is currently designed for temporal action localization, DeepEthogram could be extended by incorporating models for spatiotemporal action localization, in which there can be multiple actors (i.e., animals) performing different behaviors on each frame.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>DeepEthogram pipeline</title><p>Along with this publication, we are releasing open-source Python code for labeling videos, training all DeepEthogram models, and performing inference on new videos. The code, associated documentation, and files for the GUI can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/jbohnslav/deepethogram">https://github.com/jbohnslav/deepethogram</ext-link>.</p></sec><sec id="s4-2"><title>Implementation</title><p>We implemented DeepEthogram in the Python programming language (version 3.7 or later; <xref ref-type="bibr" rid="bib62">Rossum et al., 2010</xref>). We used PyTorch (<xref ref-type="bibr" rid="bib53">Paszke, 2018</xref>; version 1.4.0 or greater) for all deep-learning models. We used PyTorch Lightning for training (<xref ref-type="bibr" rid="bib18">Falcon, 2019</xref>). We used OpenCV (<xref ref-type="bibr" rid="bib5">Bradski, 2008</xref>) for image reading and writing. We use Kornia (<xref ref-type="bibr" rid="bib60">Riba et al., 2019</xref>) for GPU-based image augmentations. We used scikit-learn (<xref ref-type="bibr" rid="bib55">Pedregosa, 2021</xref>) for evaluation metrics, along with custom Python code. CNN diagram in <xref ref-type="fig" rid="fig1">Figure 1</xref> was generated using PlotNeuralNet (<xref ref-type="bibr" rid="bib29">Iqbal, 2018</xref>). Other figures were generated in Matplotlib (<xref ref-type="bibr" rid="bib10">Caswell, 2021</xref>). For training, we used one of the following Nvidia GPUs: GeForce 1080Ti, Titan RTX, Quadro RTX6000, or Quadro RTX8000. Inference speed was evaluated on a computer running Ubuntu 18.04, an AMD Ryzen Threadripper 2950 X CPU, an Nvidia Titan RTX, an Nvidia Geforce 1080Ti, a Samsung 970 Evo hard disk, and 128 GB DDR4 memory.</p></sec><sec id="s4-3"><title>Datasets</title><p>All experimental procedures were approved by the Institutional Animal Care and Use Committees at Boston Children’s Hospital (protocol numbers 17-06-3494R and 19-01-3809R) or Massachusetts General Hospital (protocol number 2018N000219) and were performed in compliance with the Guide for the Care and Use of Laboratory Animals.</p><p>For human-human comparison, we relabeled all videos for Mouse-Ventral1, Mouse-Ventral2, Mouse-Openfield, Mouse-Social, and Mouse-Homecage using the DeepEthogram GUI. Previous labels were not accessible during relabeling. Criteria for relabeling were written in detail by the original experimenters, and example labeled videos were viewed extensively before relabeling. Mouse-Ventral1 was labeled three times and the other datasets were labeled twice.</p><p>Videos and human annotations are available at the project website: <ext-link ext-link-type="uri" xlink:href="https://github.com/jbohnslav/deepethogram">https://github.com/jbohnslav/deepethogram</ext-link>.</p><sec id="s4-3-1"><title>Kinetics700</title><p>To pretrain our models for transfer to neuroscience datasets, we use the Kinetics700 (<xref ref-type="bibr" rid="bib9">Carreira et al., 2019</xref>) dataset. The training split of this dataset consisted of 538,523 videos and 141,677,361 frames. We first resized each video so that the short side was 256 pixels. During training, we randomly cropped 224 × 224 pixel images, and during validation, we used the center crop.</p></sec><sec id="s4-3-2"><title>Mouse-Ventral1</title><p>Recordings of voluntary behavior were acquired for 14 adult male C57BL/6J mice on the PalmReader device (Roberson et al., submitted). In brief, images were collected with infrared illumination and frustrated total internal reflectance (FTIR) illumination on alternate frames. The FTIR channel highlighted the parts of the mouse’s body that were in contact with the floor. We stacked these channels into an RGB frame: red corresponded to the FTIR image, green corresponded to the infrared image, and blue was the pixel-wise mean of the two. In particular, images were captured as a ventral view of mice placed within an opaque 18 cm long × 18 cm wide × 15 cm high chamber with a 5-mm-thick borosilicate glass floor using a Basler acA2000-50gmNIR GigE near-infrared camera at 25 frames per second. Animals were illuminated from below using nonvisible 850 nm near-infrared LED strips. All mice were habituated to investigator handling in short (~5 min) sessions and then habituated to the recording chamber in two sessions lasting 2 hr on separate days. On recording days, mice were habituated in a mock recording chamber for 45 min and then moved by an investigator to the recording chamber for 30 min. Each mouse was recorded in two of these sessions spaced 72 hr apart. The last 10 min of each recording was manually scored on a frame-by-frame basis for defined actions using a custom interface implemented in MATLAB. The 28 approximately 10 min videos totaled 419,846 frames (and labels) in the dataset. Data were recorded at 1000 × 1000 pixels and down-sampled to 250 × 250 pixels. We resized to 256 × 256 pixels using bilinear interpolation during training and inference.</p></sec><sec id="s4-3-3"><title>Mouse-Ventral2</title><p>Recordings of voluntary behavior were acquired for 16 adult male and female C57BL/6J mice. These data were collected on the iBob device. Briefly, the animals were enclosed in a device containing an opaque six-chambered plastic enclosure atop a glass floor. The box was dark and illuminated with only infrared light. Animals were habituated for 1 hr in the device before being removed to clean the enclosure. They were then habituated for another 30 min and recorded for 30 min. Recorded mice were either wild type or contained a genetic mutation predisposing them to dermatitis. Thus, scratching and licking behavior were scored. Up to six mice were imaged from below simultaneously and subsequently cropped to a resolution of 270 × 240 pixels. Images were resized to 256 × 256 pixels during training and inference. Data were collected at 30 frames per second. There were 16 approximately 30 min videos for a total of 863,232 frames.</p></sec><sec id="s4-3-4"><title>Mouse-Openfield</title><p>Videos for the Mouse-Openfield dataset were obtained from published studies (<xref ref-type="bibr" rid="bib52">Orefice, 2019</xref>; <xref ref-type="bibr" rid="bib51">Orefice, 2016</xref>) and unpublished work (Clausing et al., unpublished). Video recordings of voluntary behavior were acquired for 20 adult male mice.</p><p>All mice were exposed to a novel empty arena (40 cm × 40 cm × 40 cm) with opaque plexiglass walls. Animals were allowed to explore the arena for 10 min, under dim lighting. Videos were recorded via an overhead-mounted camera at either 30 or 60 frames per second. Videos were acquired with 2–4 mice simultaneously in separate arenas and cropped with a custom Python script such that each video contained the behavioral arena for a single animal. Prior to analysis, some videos were brightened in FIJI (<xref ref-type="bibr" rid="bib65">Schindelin, 2012</xref>), using empirically determined display range cutoffs that maximized the contrast between the mouse’s body and the walls of the arena. Twenty of the 10 min recordings were manually scored on a frame-by-frame basis for defined actions in the DeepEthogram interface. All data were labeled by an experimenter. The 20 approximately 10 min videos totaled 537,534 frames (and labels).</p></sec><sec id="s4-3-5"><title>Mouse-Homecage</title><p>Videos for the mouse home cage behavior dataset were obtained from unpublished studies (Clausing et al., unpublished). Video recordings of voluntary behavior were acquired for 12 adult male mice. All animals were group-housed in cages containing four total mice. On the day of testing, all mice except for the experimental mouse were temporarily removed from the home cage for home cage behavior testing. For these sessions, experimental mice remained alone in their home cages, which measured 28 cm × 16.5 cm × 12.5 cm and contained bedding and nesting material. For each session, two visually distinct novel wooden objects and one novel plastic igloo were placed into the experimental mouse’s home cage. Animals were allowed to interact with the igloo and objects for 10 min, under dim lighting. Videos were recorded via an overhead-mounted camera at 60 frames per second. Videos were acquired of two mice simultaneously in separate home cages. Following recordings, videos were cropped using a custom Python script such that each video contained the home cage for a single animal. Prior to analysis, all videos were brightened in FIJI48, using empirically determined display range cutoffs that maximized the contrast between each mouse’s body and the bedding and walls of the home cage. Twelve of the 10 min recordings were manually scored on a frame-by-frame basis for defined actions in the DeepEthogram interface. Data were labeled by two experimenters. The 12 approximately 10 min videos totaled 438,544 frames (and labels).</p></sec><sec id="s4-3-6"><title>Mouse-Social</title><p>Videos for the mouse reciprocal social interaction test dataset were obtained from unpublished studies (Clausing et al., unpublished; Dai et al., unpublished). Video recordings of voluntary behavior were acquired for 12 adult male mice. All mice were first habituated to a novel empty arena (40 cm × 40 cm × 40 cm) with opaque plexiglass walls for 10 min per day for two consecutive days prior to testing. For each test session, two sex-, weight-, and age-matched mice were placed into the same arena. Animals were allowed to explore the arena for 10 min under dim lighting. Videos were recorded via an overhead-mounted camera at 60 frames per second. Videos were acquired with 2–4 pairs of mice simultaneously in separate arenas. Following recordings, videos were cropped using a custom Python script, such that each video only contained the behavioral arena for two interacting animals. Prior to analysis, all videos were brightened in FIJI48 using empirically determined display range cutoffs that maximized the contrast between each mouse’s body and the walls of the arena. Twelve of the 10 min recordings were manually scored on a frame-by-frame basis for defined actions in the DeepEthogram interface. Data were labeled by two experimenters. The 12 approximately 10 minvideos totaled 438,544 frames (and labels).</p></sec><sec id="s4-3-7"><title>Sturman datasets</title><p>All Sturman datasets are from <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref>. For more details, please read their paper. Videos were downloaded from an online repository (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/3608658#.YFt8-f4pCEA">https://zenodo.org/record/3608658#.YFt8-f4pCEA</ext-link>). Labels were downloaded from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ETHZ-INS/DLCAnalyzer">https://github.com/ETHZ-INS/DLCAnalyzer</ext-link>, <xref ref-type="bibr" rid="bib42">Lukas von, 2021</xref>). We arbitrarily chose ‘Jin’ as the labeler for model training; the other labelers were used for human-human evaluation (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>).</p></sec><sec id="s4-3-8"><title>Sturman-EPM</title><p>This dataset consists of five videos. Only three contain one example of all behaviors. Therefore, we could only perform three random train-validation-test splits for this dataset (as our approach requires at least one example in each set). Images were resized to 256 × 256 during training and inference. Images were flipped up-down and left-right each with a probability of 0.5.</p></sec><sec id="s4-3-9"><title>Sturman-FST</title><p>This dataset consists of 10 recordings. Each recording has two videos, one top-down and one side view. To make this multiview dataset suitable for DeepEthogram, we closely cropped the mice in each view, resized each to 224 × 224, and concatenated them horizontally so that the final resolution was 448 × 224. We did not perform flipping augmentation.</p></sec><sec id="s4-3-10"><title>Sturman-OFT</title><p>This dataset consists of 20 videos. Images were resized to 256 × 256 for training and inference. Images were flipped up-down and left-right with probability 0.5 during training.</p></sec><sec id="s4-3-11"><title>Fly</title><p>Wild type DL adult male flies (<italic>D. melanogaster</italic>), 2–4 days post-eclosion were reared on a standard fly medium and kept on a 12 hr light-dark cycle at 25°. Flies were cold anesthetized and placed in a fly sarcophagus. We glued the fly head to its thorax and finally to a tungsten wire at an angle around 60° (UV cured glue, Bondic). The wire was placed in a micromanipulator used to position the fly on top of an air-suspended ball. Side-view images of the fly were collected at 200 Hz with a Basler A602f camera. Videos were down-sampled to 100 Hz. There were 19 approximately 30 min videos for a total of 3,419,943 labeled frames. Images were acquired at 168 × 100 pixels and up-sampled to 192 × 128 pixels during training and inference. Images were acquired in grayscale but converted to RGB (cv2.cvtColor, cv2.COLOR_GRAY2RGB) so that input channels were compatible with pretrained networks and other datasets.</p></sec></sec><sec id="s4-4"><title>Models</title><sec id="s4-4-1"><title>Overall setup</title><sec id="s4-4-1-1"><title>Problem statement</title><p>Our input features were a set of images with dimensions <inline-formula><mml:math id="inf3"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> , and our goal was to output the probability of each behavior on each frame, which is a matrix with dimensions <inline-formula><mml:math id="inf4"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> . <inline-formula><mml:math id="inf5"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of frames in a video. <inline-formula><mml:math id="inf6"><mml:mi>C</mml:mi></mml:math></inline-formula> is the number of input channels – in typical color images, this number is 3 for the red, green, and blue (RGB) channels. <inline-formula><mml:math id="inf7"><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:math></inline-formula> are the height and width of the images in pixels. <inline-formula><mml:math id="inf8"><mml:mi>K</mml:mi></mml:math></inline-formula> is the number of user-defined behaviors we aimed to estimate from our data.</p></sec></sec></sec><sec id="s4-5"><title>Training protocol</title><p>We used the ADAM optimizer (<xref ref-type="bibr" rid="bib33">Kingma and Ba, 2017</xref>) with an initial learning rate of 1 × 10<sup>–4</sup> for all models. When validation performance saturated for 5000 training steps, we decreased the learning rate by a factor of <inline-formula><mml:math id="inf9"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>10</mml:mn></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula> on the Kinetics700 dataset, or by a factor of 0.1 for neuroscience datasets (for speed), <inline-formula><mml:math id="inf10"><mml:mn>5</mml:mn><mml:mi>e</mml:mi><mml:mo>-</mml:mo><mml:mn>7</mml:mn><mml:mo>.</mml:mo></mml:math></inline-formula> For Kinetics700, we used the provided train and validation split. For neuroscience datasets, we randomly picked 60% of videos for training, 20% for validation, and 20% for test (with the exception of the subsampling experiments for <xref ref-type="fig" rid="fig5">Figure 5</xref>, wherein we only used training and validation sets to reduce overall training time). Our only restriction on random splitting was ensuring that at least one frame of each class was included in each split. We were limited to five random splits of the data for most experiments due to the computational and time demands of retraining models. We save both the final model weights and the best model weights; for inference, we load the best weights. Best is assessed by the minimum validation loss for flow generator models or the mean F1 across all non-background classes for feature extractor and sequence models.</p></sec><sec id="s4-6"><title>Stopping criterion</title><p>For Kinetics700 models, we stopped when the learning rate dropped below 5<italic>e</italic>-7. This required about 800,000 training steps. For neuroscience dataset flow generators, we stopped training at 10,000 training steps. For neuroscience dataset feature extractors, we stopped when the learning rate dropped below 5<italic>e</italic>-7, when 20,000 training steps were complete, or 24 hr elapsed, whichever came first. For sequence models, we stopped when the learning rate dropped below 5<italic>e</italic>-7, or when 100,000 training steps were complete, whichever came first.</p></sec><sec id="s4-7"><title>End-to-end training</title><p>We could, in theory, train the entire DeepEthogram pipeline end-to-end. However, we chose to train the flow generator, feature extractor, and then sequence models sequentially. By backpropagating the classification loss into the flow generator (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>), we risk increasing the overall number of parameters and overfitting. Furthermore, we designed the sequence models to have a large temporal receptive window. We therefore train on long sequences (see below). Very long sequences of raw video frames take large amounts of VRAM and exceed our computational limits. By illustration, to train on sequences of, for example, 180 frame snippets of 11 images, our tensor would be of shape [N × 33 × 180 × 256 × 256]. This corresponds to 24 GB of VRAM at a batch size of 16, just for the data and none of the neural activations or gradients, which is impractical. Therefore, we first extract features to disk and subsequently train sequence models.</p></sec><sec id="s4-8"><title>Augmentations</title><p>To improve the robustness and generalization of our models, we augmented the input images with random perturbations for all datasets during training. We used Kornia (<xref ref-type="bibr" rid="bib60">Riba et al., 2019</xref>) for GPU-based image augmentation to improve training speed. We perturbed the image brightness and contrast, rotated each image by up to 10°, and flipped horizontally and vertically (depending on the dataset). The input to the flow generator model is a set of 11 frames; the same augmentations were performed on each image in this stack. On Mouse-Ventral1 and Mouse-Ventral2, we also flipped images vertically with a probability of 0.5. We calculated the mean and standard deviation of the RGB input channels and standardized the input channel-wise.</p></sec><sec id="s4-9"><title>Pretraining + transfer learning</title><p>All flow generators and feature extractors were first trained to classify videos in the Kinetics700 dataset (see below). These weights were used to initialize models on neuroscience datasets. Sequence models were trained from scratch.</p></sec><sec id="s4-10"><title>Flow generators</title><p>For optic flow extraction, a common algorithm to use is TV-L1 (<xref ref-type="bibr" rid="bib8">Carreira and Zisserman, 2017</xref>). However, common implementations of this algorithm (<xref ref-type="bibr" rid="bib5">Bradski, 2008</xref>) require compilation of C++, which would introduce many dependencies and make installation more difficult. Furthermore, recent work (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) has shown that even simple neural-network-based optic flow estimators outperform TV-L1 for action detection. Therefore, we used CNN-based optic flow estimators. Furthermore, we found that saving optic flow as JPEG images, as is common, significantly degraded performance. Therefore, we computed optic flows from a stack of RGB images at runtime for both training and inference. This method is known as Hidden Two-Stream Networks (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>).</p><sec id="s4-10-1"><title>Architectures</title><p>For summary, see <xref ref-type="table" rid="table2">Table 2</xref>.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Model summary.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model name</th><th align="left" valign="top">Flow generator (parameters)</th><th align="left" valign="top">Feature extractor (parameters)</th><th align="left" valign="top">Sequence model (parameters)</th><th align="left" valign="top"># frames input to flow generator</th><th align="left" valign="top"># frames input to RGB feature extractor</th><th align="left" valign="top">Total parameters</th></tr></thead><tbody><tr><td align="left" valign="top">DeepEthogram-fast</td><td align="left" valign="top">TinyMotionNet (1.9M)</td><td align="left" valign="top">ResNet18 × 2 (22.4M)</td><td align="left" valign="top">TGM (250K)</td><td align="char" char="." valign="top">11</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">~24.5M</td></tr><tr><td align="left" valign="top">DeepEthogram-medium</td><td align="left" valign="top">MotionNet (45.8M)</td><td align="left" valign="top">ResNet50 × 2 (49.2M)</td><td align="left" valign="top">TGM (250K)</td><td align="char" char="." valign="top">11</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">~ 95.2M</td></tr><tr><td align="left" valign="top">DeepEthogram-slow</td><td align="left" valign="top">TinyMotionNet3D (0.4M)</td><td align="left" valign="top">ResNet3D-34 × 2 (127M)</td><td align="left" valign="top">TGM (250K)</td><td align="char" char="." valign="top">11</td><td align="char" char="." valign="top">11</td><td align="left" valign="top">~ 127.6M</td></tr></tbody></table></table-wrap><sec id="s4-10-1-1"><title>TinyMotionNet</title><p>For every timepoint, we extracted features based on one RGB image and up to 10 optic flow frames. Furthermore, for large datasets like Kinetics700 (<xref ref-type="bibr" rid="bib9">Carreira et al., 2019</xref>), it was time-consuming and required a large amount of disk space to extract and save optic flow frames. Therefore, we implemented TinyMotionNet (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) to extract 10 optic flow frames from 11 RGB images ‘on the fly,’ as we extracted features. TinyMotionNet is a small and fast optic flow model with 1.9 million parameters. Similar to a U-Net (<xref ref-type="bibr" rid="bib61">Ronneberger et al., 2015</xref>), it consists of a downward branch of convolutional layers of decreasing resolution and increasing depth. It is followed by an upward branch of increasing resolution. Units from the downward branch are concatenated to the upward branch. During training, estimated optic flows were output at 0.5, 0.25, and 0.125 of the original resolution.</p></sec><sec id="s4-10-1-2"><title>MotionNet</title><p>MotionNet is similar to TinyMotionNet except with more parameters and more feature maps per layer. During training, estimated optic flows were output at 0.5, 0.25, and 0.125 of the original resolution. See the original paper (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>) for more details.</p></sec><sec id="s4-10-1-3"><title>TinyMotionNet3D</title><p>This novel architecture is based on TinyMotionNet (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>), except we replaced all 2D convolutions with 3D convolutions. We maintained the height and width of the kernels. On the encoder and decoder branches, we used a temporal kernel size of 3, meaning that each filter spanned three images. On the last layer of the encoder and the <italic>iconv</italic> layers that connect the encoder and decoder branches, we used a temporal kernel of 2, meaning the kernels spanned two consecutive images. We aimed to have the model learn the displacement between two consecutive images (i.e., the optic flow). Due to the large memory requirements of 3D convolutional layers, we used 16, 32, and 64 filter maps per layer. For this architecture, we noticed large estimated flows in texture-less regions in neuroscience datasets after training on Kinetics. Therefore, we added a L1 sparsity penalty on the flows themselves (see ‘Loss functions,’ below).</p></sec><sec id="s4-10-1-4"><title>Modifications</title><p>For the above models, we deviated from the original paper. First, each time the flows were up-sampled by a factor of 2, we multiplied the values of the neural activations by 2. If the flow size increased from 0.25 to 0.5 of the original resolution, a flow estimate of 1 corresponds to four pixels and two pixels in the original image, respectively. To compensate for this distortion, we multiplied the up-sampled activations by 2. Secondly, when used in combination with the CNN feature extractors (see below), we did not compress the flow values to the discrete values between 0 and 255 (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>). In fact, we saw performance increases when keeping the continuous float32 values. Third, we did not backpropagate the classifier loss function into the flow generators as the neuroscience datasets likely did not have enough training examples to make this a sensible strategy. Finally, for MotionNet, we only output flows at three resolutions (rather than five) for consistency.</p></sec></sec></sec><sec id="s4-11"><title>Loss functions</title><p>In brief, we train flow generators to minimize reconstruction errors and minimize high-frequency components (to encourage smooth flow outputs).</p><sec id="s4-11-1"><title>MotionNet loss</title><p>For full details, see original paper (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>). For clarity, we reproduce the loss functions here. We estimate the current frame given the next frame and an estimated optic flow as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>0</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the current and next image. <inline-formula><mml:math id="inf12"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:math></inline-formula> are the indices of the given pixel in rows and columns. <inline-formula><mml:math id="inf13"><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the estimated x and y displacements between <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , which means <inline-formula><mml:math id="inf15"><mml:mi>V</mml:mi></mml:math></inline-formula> is the optic flow. We use Spatial Transformer Networks (<xref ref-type="bibr" rid="bib30">Jaderberg et al., 2015</xref>) to perform this sampling operation in a differentiable manner (PyTorch function <italic>torch.nn.functional.grid_sample</italic>).</p><p>The image loss is the error between the reconstructed <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>0</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and original <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> .<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>ρ</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf18"><mml:mi>ρ</mml:mi></mml:math></inline-formula> is the generalized Charbonnier penalty <inline-formula><mml:math id="inf19"><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow/></mml:msup></mml:math></inline-formula> , which reduces the influence of outliers compared to a simple L1 loss. Following (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>), we use <inline-formula><mml:math id="inf20"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> .</p><p>The structural similarity (SSIM, <xref ref-type="bibr" rid="bib71">Wang et al., 2004</xref>) loss encourages the reconstructed <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>0</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and original <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to be perceptually similar:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>∑</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>0</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The smoothness loss encourages smooth flow estimates by penalizing the <inline-formula><mml:math id="inf23"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mi>y</mml:mi></mml:math></inline-formula> gradients of the optic flow:<disp-formula id="equ4"><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>∇</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>∇</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>∇</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>∇</mml:mo><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>We set the Charbonnier <inline-formula><mml:math id="inf25"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>.</p><p>For the TinyMotionNet3D architecture only, we added a flow sparsity loss that penalizes unnecessary flows:<disp-formula id="equ5"><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s4-12"><title>Regularization</title><p>With millions of parameters and far fewer data points, it is likely that our models will overfit to the training data. Transfer learning (see above) ameliorates this problem somewhat, as does using dropout (see below). However, increasing dropout to very high levels reduces the representational space of the feature vector. To reduce overfitting, we used <inline-formula><mml:math id="inf26"><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:math></inline-formula> regularization (<xref ref-type="bibr" rid="bib38">Li et al., 2018</xref>). A common form of regularization is weight decay, in which the sum of squared weights is penalized. However, this simple term could cause the model to ‘forget’ its initial knowledge from transfer learning. Therefore, <inline-formula><mml:math id="inf27"><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:math></inline-formula> regularization uses the initial weights from transfer learning as the target.<disp-formula id="equ6"><mml:math id="m6"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mfenced close="‖" open="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mfenced close="‖" open="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>´</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></p><p>For details, see the L2-SP paper (<xref ref-type="bibr" rid="bib38">Li et al., 2018</xref>). <inline-formula><mml:math id="inf28"><mml:mi>w</mml:mi></mml:math></inline-formula> are all trainable parameters of the network, excluding biases and batch normalization parameters. <inline-formula><mml:math id="inf29"><mml:mi>α</mml:mi></mml:math></inline-formula> is a hyperparameter governing how much to decay weights towards their initial values (from transfer learning). <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are current model weights, and <inline-formula><mml:math id="inf31"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are their values from pre-training. <inline-formula><mml:math id="inf32"><mml:mi>β</mml:mi></mml:math></inline-formula> is a hyperparameter decaying new weights <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>´</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math></inline-formula> (such as the final linear readout layers in feature extractors) towards zero. For flow generator models, <inline-formula><mml:math id="inf34"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> . There are no new weights, so <inline-formula><mml:math id="inf35"><mml:mi>β</mml:mi></mml:math></inline-formula> is unused.</p><p>The final loss is the weighted sum of the previous components:<disp-formula id="equ7"><mml:math id="m7"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Following <xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>, we set <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. During training, the flow generator’s output flows at multiple resolutions. From largest to smallest, we set <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to be 0.01, 0.02, 0.04. For TinyMotionNet3D, we set <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to 0.05 and reduced <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> by a factor of 0.25.</p></sec><sec id="s4-13"><title>Feature extractors</title><p>The goal of the feature extractor was to model the probability that each behavior was present in the given frame of the video (or optic flow stack). We used two-stream CNNs (<xref ref-type="bibr" rid="bib77">Zhu et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Simonyan and Zisserman, 2014</xref>) to classify inputs from both RGB frames and optic flow frames. These CNNs reduced an input tensor from <inline-formula><mml:math id="inf40"><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> pixels to <inline-formula><mml:math id="inf41"><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> features. Our final fully connected layer estimated probabilities for each behavior, with output shape <inline-formula><mml:math id="inf42"><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> . Here, <inline-formula><mml:math id="inf43"><mml:mi>N</mml:mi></mml:math></inline-formula> is the batch size. We trained these CNNs on our labels, and then used the penultimate <inline-formula><mml:math id="inf44"><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> <italic>spatial features</italic> or <italic>flow features</italic> as inputs to our sequence models (below).</p><sec id="s4-13-1"><title>Architectures</title><p>For summary, see <xref ref-type="table" rid="table2">Table 2</xref>. We used the ResNet (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Hara et al., 2018</xref>) family of models for our feature extractors, one for the spatial stream and one for the flow stream. For DeepEthogram-fast, we used ResNet18 with ~11 million parameters. For DeepEthogram-medium, we used a ResNet50 with ~23 million parameters. We added dropout (<xref ref-type="bibr" rid="bib27">Hinton et al., 2012</xref>) layers before the final fully connected layer. For DeepEthogram-medium, we added an extra fully connected layer of shape <inline-formula><mml:math id="inf45"><mml:mfenced separators="|"><mml:mrow><mml:mn>2048,512</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> after the global average pooling layer to reduce the file size of stored features. For DeepEthogram-slow, we used a 3D ResNet34 (<xref ref-type="bibr" rid="bib25">Hara et al., 2018</xref>) with ~63 million parameters. For DeepEthogram-fast and DeepEthogram-medium, these models were pretrained on ImageNet (<xref ref-type="bibr" rid="bib15">Deng, 2008</xref>) with three input channels (RGB). We stacked 10 optic flow frames, for 20 input channels. To leverage ImageNet weights with this new number of channels, we used the mean weight across all three RGB channels and replicated it 20 times (<xref ref-type="bibr" rid="bib72">Wang et al., 2015</xref>). This was only performed when adapting ImageNet weights to Kinetics700 models to resolve the input-frame-number discrepancy; the user will never need to perform this averaging.</p></sec><sec id="s4-13-2"><title>Loss functions</title><p>Our problem is a multi-label classification task. Each timepoint can have multiple positive examples. For example, if a mouse is licking its forepaw and scratching itself with its hindlimb, both ‘lick’ and ‘scratch’ should be positive. Therefore, we used a focal binary loss (<xref ref-type="bibr" rid="bib41">Lin et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Marks, 2020</xref>). The focal loss is the binary cross-entropy loss, weighted by probability, to de-emphasize the loss for already well-classified examples and to encourage the model to ‘focus’ on misclassified examples. Combined with up-weighting rare, positive examples, the data loss function is<disp-formula id="equ8"><mml:math id="m8"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the ground truth label and was 1 if class <inline-formula><mml:math id="inf47"><mml:mi>k</mml:mi></mml:math></inline-formula> occurred at time <inline-formula><mml:math id="inf48"><mml:mi>t</mml:mi></mml:math></inline-formula>, or otherwise was 0. <inline-formula><mml:math id="inf49"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is our model output for class <inline-formula><mml:math id="inf50"><mml:mi>k</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf51"><mml:mi>t</mml:mi></mml:math></inline-formula>. Note that for the feature extractor we only considered one timepoint at a time, so <inline-formula><mml:math id="inf52"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. <inline-formula><mml:math id="inf53"><mml:mi>γ</mml:mi></mml:math></inline-formula> is a focal loss term (<xref ref-type="bibr" rid="bib41">Lin et al., 2018</xref>); if <inline-formula><mml:math id="inf54"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, this equation is simply the weighted binary cross-entropy loss. The larger the <inline-formula><mml:math id="inf55"><mml:mi>γ</mml:mi></mml:math></inline-formula>, the more the model down-weights correctly classified but insufficiently confident predictions. See the focal loss paper for more details (<xref ref-type="bibr" rid="bib41">Lin et al., 2018</xref>). We chose <inline-formula><mml:math id="inf56"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> for all feature extractor and sequence models for all datasets; see ‘Hyperparameter optimization’ section. We also use label smoothing (<xref ref-type="bibr" rid="bib46">Müller et al., 2019</xref>), so that the target was 0.05 if <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and 0.95 if <inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a weight given to positive examples – note that there was no corresponding weight in the second term when our ground truth is 0. Intuitively, if we had a very rare behavior, we wanted to penalize the model more for an error on positive examples because there were so few examples of the behavior. We calculated the weight as follows:<disp-formula id="equ9"><mml:math id="m9"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula></p><p>The numerator is the total number of positive examples in our training set, and the denominator is the total number of negative examples in our training set. <inline-formula><mml:math id="inf60"><mml:mi>β</mml:mi></mml:math></inline-formula> is a hyperparameter that we tuned manually. If <inline-formula><mml:math id="inf61"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, positive examples were weighted fully by their frequency in the training set. If <inline-formula><mml:math id="inf62"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, all training examples were weighted equally. By illustration, if only 1% of our training set had a positive example for a given behavior, with <inline-formula><mml:math id="inf63"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> our weight was 100 and with <inline-formula><mml:math id="inf64"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> this <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:math></inline-formula> We empirically found that with rare behaviors <inline-formula><mml:math id="inf66"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> drastically increased the levels of false positives, while with <inline-formula><mml:math id="inf67"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> many false negatives occurred. For all datasets, we set <inline-formula><mml:math id="inf68"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:math></inline-formula>. This <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> argument corresponds to <italic>pos_weight</italic> in <italic>torch.nn.BCEWithLogitsLoss</italic>.</p><p>We used L2-SP regularization as above. For feature extractors, we used <inline-formula><mml:math id="inf70"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> . See ‘Hyperparameter optimization’ section.</p><p>The final loss term is the sum of the data term and the regularization term:<disp-formula id="equ10"><mml:math id="m10"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p></sec><sec id="s4-13-3"><title>Bias initialization</title><p>To combat the effects of class imbalance, we set the bias parameters on the final layer to approximate the class imbalance (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data">https://www.tensorflow.org/tutorials/structured_data/imbalanced_data</ext-link>). For example, if we had 99 negative examples and 1 positive example, we wanted to set our initial biases such that the model guessed ‘positive’ around 1% of the time. Therefore, we initialized the bias term as the log ratio of positive examples to negative examples:<disp-formula id="equ11"><mml:math id="m11"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p></sec><sec id="s4-13-4"><title>Fusion</title><p>There are many ways to fuse together the outputs of the spatial and motion stream in two-stream CNNs (<xref ref-type="bibr" rid="bib19">Feichtenhofer et al., 2016</xref>). For simplicity, we used late, average fusion. We averaged together the K-dimensional output vectors of the CNNs before the sigmoid function:<disp-formula id="equ12"><mml:math id="m12"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></disp-formula></p></sec></sec><sec id="s4-14"><title>Inference time</title><p>To improve inference speed, we use a custom inference video pipeline that uses only sequential video reading, batched model predictions, and multiprocessed data loading. Inference speed time is strongly related to input resolution and GPU hardware. We report timing on both a Titan RTX graphics card and a GeForce 1080 Ti graphics card.</p></sec><sec id="s4-15"><title>Sequence models</title><sec id="s4-15-1"><title>Architecture</title><p>For summary, see <xref ref-type="table" rid="table2">Table 2</xref>. The goal of the sequence model was to have a wide temporal receptive field for classifying timepoints into behaviors. For human labelers, it is much easier to classify the behavior at time <inline-formula><mml:math id="inf72"><mml:mi>t</mml:mi></mml:math></inline-formula> by watching a short clip centered at <inline-formula><mml:math id="inf73"><mml:mi>t</mml:mi></mml:math></inline-formula> rather than viewing the static image. Therefore, we used a sequence model that takes as input a sequence of <italic>spatial features</italic> and <italic>flow features</italic> output by the feature extractors. Our criteria were to find a model that had a large temporal receptive field as context can be useful for classifying frames. However, we also wanted a model that had relatively few parameters as this model was trained from scratch on small neuroscience datasets. Therefore, we chose TGM (<xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>) models, which are designed for temporal action detection. Unless otherwise noted, we used the following hyperparameters:</p><list list-type="bullet"><list-item><p>Filter length:<inline-formula><mml:math id="inf74"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:math></inline-formula></p></list-item><list-item><p>Number of input layers:<inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item><list-item><p>Number of output layers:<inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:math></inline-formula></p></list-item><list-item><p>Number of TGM layers: 3</p></list-item><list-item><p>Input dropout: 0.5</p></list-item><list-item><p>Dropout of output features: 0.5</p></list-item><list-item><p>Input dimensionality (concatenation of flow and spatial):<inline-formula><mml:math id="inf77"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:math></inline-formula></p></list-item><list-item><p>Number of filters: 8</p></list-item><list-item><p>Sequence length: 180</p></list-item><list-item><p>Soft attention, not 1D convolution</p></list-item><list-item><p>We do not use super-events</p></list-item><list-item><p>For more details, see <xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>.</p></list-item></list><sec id="s4-15-1-1"><title>Modifications</title><p>TGM models use two main features to make the final prediction: the <inline-formula><mml:math id="inf78"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> input features (in our case, spatial and flow features from the feature extractors); and the <inline-formula><mml:math id="inf79"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> learned features output by the TGM layers. The original TGM model performed ‘early fusion’ by concatenating these two features into shape <inline-formula><mml:math id="inf80"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> before the 1D convolution layer. We found in low-data regimes that the model ignored the learned features, and therefore reduced to a simple 1D convolution. Therefore, we performed ‘late fusion’ – we used separate 1D convolutions on the input features and on the learned features. We averaged the output of these two layers (both <inline-formula><mml:math id="inf81"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> activations before the sigmoid function). Secondly, in the original TGM paper, the penultimate layer was a standard 1D convolutional layer with 512 output channels. We found that this dramatically increased the number of parameters without improving performance significantly. Therefore, we reduced output channels to 128. The total number of parameters was ~264,000.</p></sec></sec></sec><sec id="s4-16"><title>Loss function</title><p>The data loss term is the weighted, binary focal loss as for the feature extractor above. For the regularization loss, we used simple L2 regularization because we do not pretrain the sequence models.<disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mfenced close="‖" open="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>´</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></p><p>We used <inline-formula><mml:math id="inf82"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula> for all datasets.</p></sec><sec id="s4-17"><title>Keypoint-based classification</title><p>We compared pixel-based (DeepEthogram) and skeleton-based behavioral classification on the Mouse-Openfield dataset. Our goal was to replicate <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref> as closely as possible. We chose this dataset because videos with this resolution of mice in an open field arena are a common form of behavioral measurement in biology. We first used DeepLabCut (<xref ref-type="bibr" rid="bib44">Mathis, 2018</xref>) to label keypoints on the mouse and train pose estimation models. Due to the low resolution of the videos (200–300 pixels on each side), we could only reliably estimate seven keypoints: nose, left and right forepaw (if visible, or shoulder area), left and right hindpaw (if visible, or hip area), the base of the tail, and the tip of the tail. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref> for details. We labeled 1800 images and trained models using the DeepLabCut Colab notebook (ResNet50). Example performance on held-out data for unlabeled frames can be seen in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>. We used linear interpolation for keypoints with confidence below 0.9.</p><p>Using these seven keypoints for all videos, we computed a number of pose and behavioral features (Python, NumPy). As a check, we plotted the distribution of these features for each human-labeled behavior (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>). These features contained signal that can reliably discriminate behaviors. For example, the distance between the nose and the tailbase is larger during locomotion than during face grooming (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, left).</p><p>We attempted to replicate <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref> as closely as possible. However, due to technical considerations, using the exact codebase was not possible. Our dataset is multilabel, meaning that two behaviors can present, and be labeled, on a single frame. Therefore, we could not use cross-entropy loss. Our videos are lower resolution, and therefore we used seven keypoints instead of 10. We normalized pixels by the width and height of the arena. We computed the centroid as the mean of all paw locations. Due to the difference in keypoints we selected, we had to perform our own behavioral feature expansion. (See ‘Time-resolved skeleton representation,’ <xref ref-type="bibr" rid="bib69">Sturman et al., 2020</xref> supplementary methods.) We used the following features:</p><list list-type="bullet"><list-item><p>x and y coordinates of all keypoints in the arena</p></list-item><list-item><p>x and y coordinates after aligning relative to the body axis, such that the nose was to the right and the tailbase to the left</p></list-item><list-item><p>Angles between</p><list list-type="bullet"><list-item><p>tail and body axis</p></list-item><list-item><p>each paw and the body axis</p></list-item></list></list-item><list-item><p>Distances between</p><list list-type="bullet"><list-item><p>nose and tailbase</p></list-item><list-item><p>tail base and tip</p></list-item><list-item><p>left forepaw and left hindpaw, right forepaw and hindpaw, averaged</p></list-item><list-item><p>forepaw and nose</p></list-item><list-item><p>left and right forepaw</p></list-item><list-item><p>left and right hindpaw</p></list-item></list></list-item><list-item><p>The area of the body (polygon enclosed by the paws, nose, and tailbase)</p></list-item></list><p>This resulted in 44 behavioral features for each frame. Following Sturman et al., we used T-15 frames to T + 15 frames as input to our classifier, which is 1364 features in total (44 * 31). We also used the same model architecture as Sturman et al.: a multilayer perceptron with 1364 neurons in the input layer, two hidden layers with 256 and 128 neurons, respectively, and ReLU activations. For simplicity, we used Dropout (<xref ref-type="bibr" rid="bib68">Srivastava et al., 2014</xref>) with probability 0.35 between each layer.</p><p>To make the comparison as fair as possible, we implemented training tricks from DeepEthogram sequence models to train these keypoint-based models. These include the loss function (binary focal loss with up-weighting of rare behaviors), the stopping criterion (100 epochs or when learning rate reduces below 5e-7), learning rate scheduling based on validation F1 saturation, L2 regularization, thresholds optimized based on F1, postprocessing based on bout length statistics, and inference using the best weights during training (as opposed to the final weights).</p></sec><sec id="s4-18"><title>Unsupervised classification</title><p>To compare DeepEthogram to unsupervised classification, we used B-SoID (<xref ref-type="bibr" rid="bib28">Hsu and Yttri, 2019</xref>; version 2.0, downloaded January 18, 2021). We used the same DeepLabCut outputs as for the supervised classifiers. We used the Streamlit GUI for feature computation, UMAP embedding, model training, and classification. Our Mouse-Openfield dataset contained videos with a mixture of 30 and 60 frames-per-second videos. The B-SoID app assumed constant framerates; therefore, we down-sampled the poses from 60 Hz to 30 Hz, performed all embedding and classification, and then up-sampled classified behaviors back to 60 Hz using nearest-neighbor up-sampling (PyTorch, see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2A</xref>). B-SoID identified 11 clusters in UMAP space (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2B</xref>, left). To compare unsupervised with post-hoc assignment to DeepEthogram, we first computed a simple lookup table that mapped human annotations to B-SoID clusters by counting the frames on which they co-occurred (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2D</xref>). For each human label, we picked the B-SoID cluster with the maximum number of co-occurring labels; this defines a mapping between B-SoID clusters and human labels. We used this mapping to ‘predict’ human labels on the test set (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2E</xref>). We compared B-SoID to the DeepEthogram-fast model for a fair comparison as B-SoID inference is relatively fast.</p></sec><sec id="s4-19"><title>Hyperparameter optimization</title><p>There are many hyperparameters in DeepEthogram models that can dramatically affect performance. We optimized hyperparameters using Ray Tune (<xref ref-type="bibr" rid="bib40">Liaw, 2018</xref>), a software package for distributed, asynchronous model selection and training. Our target for hyperparameter optimization was the F1 score averaged over classes on the validation set, ignoring the background class. We used random search to select hyperparameters, and the Asynchronous Successive Halving algorithm (<xref ref-type="bibr" rid="bib39">Li, 2020</xref>) to terminate poor runs. We did not perform hyperparameter optimization on flow generator models. For feature extractors, we optimized the following hyperparameters: learning rate, <inline-formula><mml:math id="inf83"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:mi>β</mml:mi></mml:math></inline-formula> from the regularization loss, <inline-formula><mml:math id="inf85"><mml:mi>γ</mml:mi></mml:math></inline-formula> from the focal loss, <inline-formula><mml:math id="inf86"><mml:mi>β</mml:mi></mml:math></inline-formula> from the positive example weighting, whether or not to add a batch normalization layer after the final fully connected layer (<xref ref-type="bibr" rid="bib34">Kocaman et al., 2020</xref>), dropout probability, and label smoothing. For sequence models, we optimized learning rate, regularization <inline-formula><mml:math id="inf87"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf88"><mml:mi>γ</mml:mi></mml:math></inline-formula> from the focal loss, <inline-formula><mml:math id="inf89"><mml:mi>β</mml:mi></mml:math></inline-formula> from the positive example weighting, whether or not to add a batch normalization layer after the final fully connected layer (<xref ref-type="bibr" rid="bib34">Kocaman et al., 2020</xref>), input dropout probability, output dropout probability, filter length, number of layers, whether or not to use soft attention (<xref ref-type="bibr" rid="bib59">Piergiovanni and Ryoo, 2018</xref>), whether or not to add a nonlinear classification layer, and number of features in the nonlinear classification layer.</p><p>The absolute best performance could have been obtained by picking the best hyperparameters for each dataset, model size (DeepEthogram-f, DeepEthogram-m, or DeepEthogram-s), and split of the data. However, this would overstate performance for subsequent users that do not have the computational resources to perform such an optimization themselves. Therefore, we manually selected hyperparameters that had good performance on average across all datasets, models, and splits, and used the same parameters for all models. We will release the Ray Tune integration code required for users to optimize their own models, should they choose.</p><p>We performed this optimization on the O2 High Performance Compute Cluster, supported by the Research Computing Group, at Harvard Medical School. See <ext-link ext-link-type="uri" xlink:href="http://rc.hms.harvard.edu">http://rc.hms.harvard.edu</ext-link> for more information. Specifically, we used a cluster consisting of 8 RTX6000 GPUs.</p></sec><sec id="s4-20"><title>Postprocessing</title><p>The output of the feature extractor and sequence model is the probability of behavior <inline-formula><mml:math id="inf90"><mml:mi>k</mml:mi></mml:math></inline-formula> occurring on frame <inline-formula><mml:math id="inf91"><mml:mi>t</mml:mi></mml:math></inline-formula>: <inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> . To convert these probabilities into binary predictions, we thresholded the probabilities:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>We picked the threshold <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each behavior <inline-formula><mml:math id="inf94"><mml:mi>k</mml:mi></mml:math></inline-formula> that maximized the F1 score (below). We picked the threshold independently on the training and validation sets. On test data, we used the validation thresholds.</p><p>We found that these predictions overestimated the overall number of bouts. In particular, very short bouts were over-represented in model predictions. For each behavior <italic>k</italic>, we removed both ‘positive’ and ‘negative’ bouts (binary sequences of 1 s and 0 s, respectively) shorter than the first percentile of the bout length distribution in the training set.</p><p>Finally, we computed the ‘background’ class as the logical not of the other predictions.</p></sec><sec id="s4-21"><title>Evaluation and metrics</title><p>We used the following metrics: overall accuracy, F1 score, and the AUROC by class. Accuracy was defined as<disp-formula id="equ15"><mml:math id="m15"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf95"><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:math></inline-formula> is the number of true positives, <inline-formula><mml:math id="inf96"><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of true negatives, <inline-formula><mml:math id="inf97"><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:math></inline-formula> is the number of false positives, and <inline-formula><mml:math id="inf98"><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of false negatives. We reported overall accuracy, not accuracy for each class.</p><p>F1 score was defined as<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf99"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> . The above was implemented by <italic>sklearn.metrics.f1_score</italic> with argument <italic>average=’macro’</italic>.</p><p>AUROC was computed by taking the AUROC for each class and averaging the result. This was implemented by <italic>sklearn.metrics.roc_auc_score</italic> with argument <italic>average=’macro’</italic>.</p></sec><sec id="s4-22"><title>Shuffle</title><p>To compare model performance to random chance, we performed a shuffling procedure. For each model and random split, we randomly circularly permuted each video’s labels 100 times. This means that the distribution of labels was kept the same, but the correspondence between predictions and labels was broken. For each of these 100 repetitions, we computed all metrics and then averaged across repeats; this results in one chance value per split of the data (gray bars, <xref ref-type="fig" rid="fig3">Figure 3C–K</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B–J</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B–J</xref>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3B–J</xref>).</p></sec><sec id="s4-23"><title>Statistics</title><p>We randomly assigned input videos to train, validation, and test splits (see above). We then trained flow generator models, feature extractor models, performed inference, and trained sequence models. We repeated this process five times for all datasets. For Sturman-EPM, because only three videos had at least one behavior, we split this dataset three times. When evaluating DeepEthogram performance, this results in N = 5 samples. For each split of the data, the videos in each subset were different; the fully connected layers in the feature extractor were randomly initialized with different weights; and the sequence model was randomly initialized with different weights. When comparing the means of multiple groups (e.g., shuffle, DeepEthogram, and human performance for a single behavior), we used a one-way repeated measures ANOVA, with subjects being splits. If this was significant, we performed a post-hoc Tukey’s honestly significant difference test to compare means pairwise. For cases in which only two groups were being compared (e.g., model and shuffle without human performance), we performed paired t-tests with Bonferroni correction.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>none</p></fn><fn fn-type="COI-statement" id="conf2"><p>None</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Developed the software and analyzed the data. Performed video labeling, Formal analysis, Funding acquisition, Investigation, Methodology, Performed video labeling, Software, Visualization, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Funding acquisition, Investigation, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed video labeling, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed video labeling, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed video labeling, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Funding acquisition, Investigation, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed video labeling, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Investigation, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Performed experiments. Performed video labeling, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Investigation, Performed video labeling</p></fn><fn fn-type="con" id="con8"><p>Funding acquisition, Supervised experiments, Supervised experiments, Supervised experiments, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Funding acquisition, Supervised experiments, Supervised experiments, Supervised experiments, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Funding acquisition, Supervised experiments, Supervised experiments, Supervised experiments, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Funding acquisition, Methodology, Software, Supervised the software development and data analysis, Supervision, Writing - original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures were approved by the Institutional Animal Care and Use Committees at Boston Children's Hospital (protocol numbers 17-06-3494R and 19-01-3809R) or Massachusetts General Hospital (protocol number 2018N000219) and were performed in compliance with the Guide for the Care and Use of Laboratory Animals.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-63377-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Code is posted publicly on Github and linked in the paper. Video datasets and human annotations are publicly available and linked in the paper.</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>von Ziegler</surname><given-names>L</given-names></name><name><surname>Sturman</surname><given-names>O</given-names></name><name><surname>Bohacek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Videos for deeplabcut, noldus ethovision X14 and TSE multi conditioning systems comparisons</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.3608658</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Woolf lab members Rachel Moon for data acquisition and scoring and Victor Fattori for scoring, and David Roberson and Lee Barrett for designing and constructing the PalmReader and iBob mouse viewing platforms. We thank the Harvey lab for helpful discussions and feedback on the manuscript. We thank Sturman et al. 2020 for making their videos and human labels publicly available. This work was supported by NIH grants R01 MH107620 (CDH), R01 NS089521 (CDH), R01 NS108410 (CDH), DP1 MH125776 (CDH), F31 NS108450 (JPB), R35 NS105076 (CJW), R01 AT011447 (CJW), R00 NS101057 (LLO), K99 DE028360 (DAY), European Research Council grant ERC-Stg-759782 (EC), an NSF GRFP (NKW), FCT fellowship PD/BD/105947/2014 (TC), a Harvard Medical School Dean’s Innovation Award (CDH), and a Harvard Medical School Goldenson Research Award (CDH).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Toward a Science of Computational Ethology</article-title><source>Neuron</source><volume>84</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id><pub-id pub-id-type="pmid">25277452</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Batty</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><conf-name>Behavenet: Nonlinear embedding and bayesian neural decoding of behavioral videos</conf-name><article-title>Openreview</article-title></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Choi</surname><given-names>DM</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id><pub-id pub-id-type="pmid">25142523</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bohnslav</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Deepethogram</data-title><version designator="swh:1:rev:ffd7e6bd91f52c7d1dbb166d1fe8793a26c4cb01">swh:1:rev:ffd7e6bd91f52c7d1dbb166d1fe8793a26c4cb01</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:ffd7e6bd91f52c7d1dbb166d1fe8793a26c4cb01">https://archive.softwareheritage.org/swh:1:rev:ffd7e6bd91f52c7d1dbb166d1fe8793a26c4cb01</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Open Source Computer Vision Library</source><publisher-name>OpenCV</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>AE</given-names></name><name><surname>de Bivort</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ethology as a Physical Science</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/220855</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browne</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time-Resolved Fast Mammalian Behavior Reveals the Complexity of Protective Pain Responses</article-title><source>Cell Reports</source><volume>20</volume><fpage>89</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.06.024</pub-id><pub-id pub-id-type="pmid">28683326</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Carreira</surname><given-names>J</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><conf-name>Quo vadis, action recognition? A new model and the kinetics dataset</conf-name><article-title>IEEE conference on computer vision and pattern recognition</article-title></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Carreira</surname><given-names>J</given-names></name><name><surname>Noland</surname><given-names>E</given-names></name><name><surname>Hillier</surname><given-names>C</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A Short Note on the Kinetics-700 Human Action Dataset</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.06987">https://arxiv.org/abs/1907.06987</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Caswell</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Matplotlib/matplotlib: REL</data-title><version designator="V3.4.2">V3.4.2</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.592536">https://doi.org/10.5281/zenodo.592536</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>YW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rethinking the Faster R-CNN Architecture for Temporal Action Localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.07667">https://arxiv.org/abs/1804.07667</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dankert</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Hoopfer</surname><given-names>ED</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated monitoring and analysis of social behavior in <italic>Drosophila</italic></article-title><source>Nature Methods</source><volume>6</volume><fpage>297</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1310</pub-id><pub-id pub-id-type="pmid">19270697</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Leifer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational Neuroethology: A Call to Action</article-title><source>Neuron</source><volume>104</volume><fpage>11</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id><pub-id pub-id-type="pmid">31600508</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Chaumont</surname><given-names>F</given-names></name><name><surname>Ey</surname><given-names>E</given-names></name><name><surname>Torquet</surname><given-names>N</given-names></name><name><surname>Lagache</surname><given-names>T</given-names></name><name><surname>Dallongeville</surname><given-names>S</given-names></name><name><surname>Imbert</surname><given-names>A</given-names></name><name><surname>Legou</surname><given-names>T</given-names></name><name><surname>Le Sourd</surname><given-names>A-M</given-names></name><name><surname>Faure</surname><given-names>P</given-names></name><name><surname>Bourgeron</surname><given-names>T</given-names></name><name><surname>Olivo-Marin</surname><given-names>J-C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Real-time analysis of the behaviour of groups of mice via a depth-sensing camera and machine learning</article-title><source>Nature Biomedical Engineering</source><volume>3</volume><fpage>930</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1038/s41551-019-0396-1</pub-id><pub-id pub-id-type="pmid">31110290</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><conf-name>IMAGENET: A large-scale hierarchical image database</conf-name><article-title>IEEE Conference</article-title><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egnor</surname><given-names>SER</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational analysis of behavior</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>217</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013845</pub-id><pub-id pub-id-type="pmid">27090952</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>El-Nouby</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Real-Time End-to-End Action Detection with Two-Stream Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.08362">https://arxiv.org/abs/1802.08362</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Falcon</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Pytorch lightning</data-title><version designator="0.3">0.3</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://www.pytorchlightning.ai/">https://www.pytorchlightning.ai/</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Feichtenhofer</surname><given-names>C</given-names></name><name><surname>Pinz</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Convolutional Two-Stream Network Fusion for Video Action Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1604.06573">https://arxiv.org/abs/1604.06573</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Feichtenhofer</surname><given-names>C</given-names></name><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Malik</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title><italic>SlowFast Networks for Video Recognition</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.03982">https://arxiv.org/abs/1812.03982</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friard</surname><given-names>O</given-names></name><name><surname>Gamba</surname><given-names>M</given-names></name><name><surname>Fitzjohn</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>BORIS : a free, versatile open‐source event‐logging software for video/audio coding and live observations</article-title><source>Methods in Ecology and Evolution</source><volume>7</volume><fpage>1325</fpage><lpage>1330</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.12584</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujiwara</surname><given-names>T</given-names></name><name><surname>Cruz</surname><given-names>TL</given-names></name><name><surname>Bohnslav</surname><given-names>JP</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A faithful internal representation of walking movements in the <italic>Drosophila</italic> visual system</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>72</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1038/nn.4435</pub-id><pub-id pub-id-type="pmid">27798632</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Big behavioral data: Psychology, ethology and the foundations of neuroscience</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1455</fpage><lpage>1462</lpage><pub-id pub-id-type="doi">10.1038/nn.3812</pub-id><pub-id pub-id-type="pmid">25349912</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hara</surname><given-names>K</given-names></name><name><surname>Kataoka</surname><given-names>H</given-names></name><name><surname>Satoh</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Can spatiotemporal 3D CNNS retrace the history of 2D</article-title><source>CNNs and ImageNet</source><volume>10</volume><elocation-id>00685</elocation-id><pub-id pub-id-type="doi">10.1109/CVPR.2018.00685</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep Residual Learning for Image Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>B-SOID: An Open Source Unsupervised Algorithm for Discovery of Spontaneous Behaviors</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/770271</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Iqbal</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Harisiqbal88/plotneuralnet</data-title><version designator="v1.0.0">v1.0.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.2526396">https://doi.org/10.5281/zenodo.2526396</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jaderberg</surname><given-names>M</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title><italic>Spatial Transformer Networks</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.02025">https://arxiv.org/abs/1506.02025</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Rivera-Alba</surname><given-names>M</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: Interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kahatapitiya</surname><given-names>K</given-names></name><name><surname>Ryoo</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title><italic>Coarse-Fine Networks for Temporal Activity Detection in Videos</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.01302">https://arxiv.org/abs/2103.01302</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>Adam</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kocaman</surname><given-names>V</given-names></name><name><surname>Shir</surname><given-names>OM</given-names></name><name><surname>Bäck</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Improving Model Accuracy for Imbalanced Image Classification Tasks by Adding a Final Batch Normalization Layer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.06319">https://arxiv.org/abs/2011.06319</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: Correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kwak</surname><given-names>IS</given-names></name><name><surname>Kriegman</surname><given-names>D</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Detecting the Starting Frame of Actions in Video</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.03340">https://arxiv.org/abs/1906.03340</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-Animal Pose Estimation and Tracking with Deeplabcut</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.30.442096v1</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Grandvalet</surname><given-names>Y</given-names></name><name><surname>Davoine</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Explicit Inductive Bias for Transfer Learning with Convolutional Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.01483">https://arxiv.org/abs/1802.01483</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A System for Massively Parallel Hyperparameter Tuning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.05934">https://arxiv.org/abs/1810.05934</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Liaw</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tune</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.05118">https://arxiv.org/abs/1807.05118</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title><italic>Focal Loss for Dense Object Detection</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lukas von</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>DLC analyzer</data-title><version designator="7f12ca8">7f12ca8</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ETHZ-INS/DLCAnalyzer">https://github.com/ETHZ-INS/DLCAnalyzer</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SIPEC: The Deep-Learning Swiss Knife for Behavioral Data Analysis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.10.26.355115</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Monfort</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multi-Moments in Time</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1911.00232.pdf">https://arxiv.org/pdf/1911.00232.pdf</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>R</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>When Does Label Smoothing Help?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.02629">https://arxiv.org/abs/1906.02629</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>Nature Protocols</source><volume>14</volume><fpage>2152</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><pub-id pub-id-type="pmid">31227823</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nawhal</surname><given-names>M</given-names></name><name><surname>Mori</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Activity Graph Transformer for Temporal Action Localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2101.08540">https://arxiv.org/abs/2101.08540</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubarth</surname><given-names>NL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Meissner corpuscles and their spatially intermingled afferents underlie gentle touch perception</article-title><source>Science</source><volume>368</volume><elocation-id>eabb2751</elocation-id><pub-id pub-id-type="doi">10.1126/science.abb2751</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simple Behavioral Analysis (SIMBA) – an Open Source Toolkit for Computer Classification of Complex Social Behaviors in Experimental Animals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.19.049452</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orefice</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Peripheral Mechanosensory Neuron Dysfunction Underlies Tactile and Behavioral Deficits in Mouse Models of ASDs</article-title><source>Cell</source><volume>166</volume><fpage>299</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.05.033</pub-id><pub-id pub-id-type="pmid">27293187</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orefice</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Targeting Peripheral Somatosensory Neurons to Improve Tactile-Related Phenotypes in ASD Models</article-title><source>Cell</source><volume>178</volume><fpage>867</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.07.024</pub-id><pub-id pub-id-type="pmid">31398341</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pytorch: An Imperative Style, High-Performance Deep Learning Library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peça</surname><given-names>J</given-names></name><name><surname>Feliciano</surname><given-names>C</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Wells</surname><given-names>MF</given-names></name><name><surname>Venkatraman</surname><given-names>TN</given-names></name><name><surname>Lascola</surname><given-names>CD</given-names></name><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Shank3 mutant mice display autistic-like behaviours and striatal dysfunction</article-title><source>Nature</source><volume>472</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1038/nature09965</pub-id><pub-id pub-id-type="pmid">21423165</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Scikit-learn: Machine learning in Python</data-title><version designator="0.2">0.2</version><source>Mach. Learn. Python</source><ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>ZT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ezTrack: An open-source video analysis pipeline for the investigation of animal behavior</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>19979</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-56408-9</pub-id><pub-id pub-id-type="pmid">31882950</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018a</year><source>Leap Estimates Animal Pose</source><publisher-name>LEAP</publisher-name></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Fast Animal Pose Estimation Using Deep Neural Networks</article-title><source>Nature</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1101/331181</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Piergiovanni</surname><given-names>AJ</given-names></name><name><surname>Ryoo</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Temporal Gaussian Mixture Layer for Videos</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.06316">https://arxiv.org/abs/1803.06316</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Riba</surname><given-names>E</given-names></name><name><surname>Mishkin</surname><given-names>D</given-names></name><name><surname>Ponsa</surname><given-names>D</given-names></name><name><surname>Rublee</surname><given-names>E</given-names></name><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Kornia: An Open Source Differentiable Computer Vision Library for PyTorch</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1910.02190">https://arxiv.org/abs/1910.02190</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title><italic>U-Net: Convolutional Networks for Biomedical Image Segmentation</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rossum</surname><given-names>G</given-names></name><name><surname>Drake</surname><given-names>FL</given-names></name><name><surname>Van Rossum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><data-title>The Python language reference</data-title><source>Python Software Foundation</source><ext-link ext-link-type="uri" xlink:href="https://docs.python.org/3/reference/">https://docs.python.org/3/reference/</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryait</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Data-driven analyses of motor impairments in animal models of neurological disorders</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000516</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000516</pub-id><pub-id pub-id-type="pmid">31751328</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauerbrei</surname><given-names>BA</given-names></name><name><surname>Guo</surname><given-names>J-Z</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Mischiati</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Verma</surname><given-names>N</given-names></name><name><surname>Mensh</surname><given-names>B</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical pattern generation during dexterous movement is input-driven</article-title><source>Nature</source><volume>577</volume><fpage>386</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1869-9</pub-id><pub-id pub-id-type="pmid">31875851</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fiji: an open-source platform for biological-image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><pub-id pub-id-type="pmid">22743772</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Mouse Action Recognition System (MARS): A Software Pipeline for Automated Analysis of Social Behaviors in Mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.26.222299</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two-Stream Convolutional Networks for Action Recognition in Videos</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.2199">https://arxiv.org/abs/1406.2199</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sturman</surname><given-names>O</given-names></name><name><surname>von Ziegler</surname><given-names>L</given-names></name><name><surname>Schläppi</surname><given-names>C</given-names></name><name><surname>Akyol</surname><given-names>F</given-names></name><name><surname>Privitera</surname><given-names>M</given-names></name><name><surname>Slominski</surname><given-names>D</given-names></name><name><surname>Grimm</surname><given-names>C</given-names></name><name><surname>Thieren</surname><given-names>L</given-names></name><name><surname>Zerbi</surname><given-names>V</given-names></name><name><surname>Grewe</surname><given-names>B</given-names></name><name><surname>Bohacek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions</article-title><source>Neuropsychopharmacology</source><volume>45</volume><fpage>1942</fpage><lpage>1952</lpage><pub-id pub-id-type="doi">10.1038/s41386-020-0776-y</pub-id><pub-id pub-id-type="pmid">32711402</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Dam</surname><given-names>EA</given-names></name><name><surname>Noldus</surname><given-names>L</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning improves automated rodent behavior recognition within a specific experimental setup</article-title><source>Journal of Neuroscience Methods</source><volume>332</volume><elocation-id>S0165-0270(19)30393-0</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108536</pub-id><pub-id pub-id-type="pmid">31794777</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Sheikh</surname><given-names>HR</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Image Quality Assessment: From Error Visibility to Structural Similarity</article-title><source>IEEE Transactions on Image Processing</source><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/tip.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Xiong</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Qiao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title><italic>Towards Good Practices for Very Deep Two-Stream ConvNets</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1507.02159">https://arxiv.org/abs/1507.02159</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping Sub-Second Structure in Mouse Behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Tsukahara</surname><given-names>T</given-names></name><name><surname>Zeine</surname><given-names>A</given-names></name><name><surname>Anyoha</surname><given-names>R</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1433</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00706-3</pub-id><pub-id pub-id-type="pmid">32958923</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Patras</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exploring Feature Representation and Training Strategies in Temporal Action Localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.10608">https://arxiv.org/abs/1905.10608</ext-link></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Graph Convolutional Networks for Temporal Action Localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.03252">https://arxiv.org/abs/1909.03252</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Lan</surname><given-names>Z</given-names></name><name><surname>Newsam</surname><given-names>S</given-names></name><name><surname>Hauptmann</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hidden Two-Stream Convolutional Networks for Action Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.00389">https://arxiv.org/abs/1704.00389</ext-link></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63377.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewer</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Bohacek</surname><given-names>Johannes</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>DeepEthogram introduces a new tool to the neuroscience and behavior community that allow direct from-video-to-actions to be automatically identified. The authors comprehensively benchmark and provide data that demonstrates the tool's high utility in many common laboratory scenarios.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;DeepEthogram: a machine learning pipeline for supervised behavior classification from raw pixels&quot; for consideration by<italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Mackenzie Mathis as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Johannes Bohacek (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at eLife&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>Bohnslav et al., present a new toolkit and GUI for using video input to extract behavioral states (ethograms) using a set of established deep neural networks. They show their pipeline works on range of laboratory datasets, and provide metrics comparing network performance to humans. However, the reviewers all agreed there are several key revisions needed in order to support the main claims of the paper. These revolve around benchmarking, datasets, and a more careful handling of related work, limitations of such a software, and clarifying methods. We have collectively decided to send the individual reviews from each reviewer, and ask you address those (and perhaps combine where you see fit), but we urge you to focus in on the following points for your revision.</p><p>Datasets:</p><p>The reviewers each expressed concern over the simplicity of the datasets and the potentially limited scope of DeepEthogram in relation. For example, the authors claim these are difficult datasets, but in fact we feel they are not representative of the laboratory videos often collected: they have very static backgrounds, no animals have cables or other occluders. We would urge the authors to use other datasets, even those publically available, to more thoroughly benchmark performance in a broader collection of behaviors.</p><p>Benchmarking:</p><p>While DeepEthogram could be an important tool to the growing toolbox of deep learning tools for behavior, we felt that there are sufficiently other options available that the authors should directly compare performance. While we do appreciate that comparing to the &quot;gold standard&quot; of human-labeled data, the real challenge with such datasets is even humans tend not to agree on a semantic label. Here, the authors only use two humans for ground-truth annotation, but there is a concern of an outlier. Typically, 3 humans are used to overcome a bit of this limitation. Therefore, we suggest carefully benchmarking against humans (i.e., increase the number of ground truth annotations), and please see the individual reviewer comments with specific questions related to other published/available code bases where you can directly compare your pipelines performance.</p><p>Methods, Relation to other packages, and Limitations:</p><p>The reviewers raised several points where methods are unclear, or how an analysis was performed was not clear. In particular, we ask you to check reviewer #3's comments carefully regarding methods. Moreover, we think a more nuanced discussion about when to do some &quot;pre-processing&quot; (like pose estimation) would be beneficial vs. straight to an ethogram, and visa versa. In particular, it's worth nothing that often times having an intermediate bottleneck such as key points allows the user to more easily assess network performance (keypoints are a defined ground truth vs. semantic action labels).</p><p>In total, the reviews are certainly enthusiastic about this work, and do hope you find these suggestions helpful. We look forward to reading your revision.<italic>Reviewer #1:</italic></p><p>Bohnslav et al., present a new tool to quantify behavior actions directly from video. I think this is a nice addition to the growing body of work using video to analyze behavior. The paper is well written, clear for a general audience, and takes nice innovations in computer vision into life sciences and presents a usable tool for the community. I have a few critical points that I believe need addressed before publication, mostly revolving around benchmarking, but overall I am enthusiastic about this work being in<italic>eLife</italic>.</p><p>In the following sections I highlight areas I believe can be improved upon.</p><p>In relation to prior work: The authors should more explicitly state their contribution, and the field's contributions, to action recognition. The introduction mostly highlights limitations of unsupervised methods to perform behavioral analysis (which to note, produces the same outputs as this paper, i.e. an ethogram) and key point estimation alone, which of course is tackling a different problem. What I would like to see is a more careful consideration of the state-of-the-field in computer vision for action recognition, and clearly defining what the contribution is in this paper the cover letter alludes to them developing novel computer vision aspects of the package, but from the code base, etc, it seems they utilize (albeit nicely!) pre-existing works from ~3 years ago, begging the question if this is truly state-of-the-art performance. Moreover, and this does hurt novelty a bit, this is not the first report in life science of such a pipeline, so this should be clearly stated. I don't think it's required to compare this tool to every other tool available, but I do think discussing this in the introduction is of importance (but again, I am still enthusiastic for this being in<italic>eLife</italic>).</p><p>&quot;Our model operates directly on the raw pixel values of videos, and thus it is generally applicable to any case with video data and binary behavior labels and further does not require pre-specification of the body features of interest, such as keypoints on limbs or fitting the body with ellipses.&quot; – please include references to the many other papers that do this as well. For example, please see:</p><p>Data-driven analyses of motor impairments in animal models of neurological disorders https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000516</p><p>LSTM Self-Supervision for Detailed Behavior Analysis https://openaccess.thecvf.com/content_cvpr_2017/html/Brattoli_LSTM_Self-Supervision_for_CVPR_2017_paper.html</p><p>Facial expressions of emotion states and their neuronal correlates in mice https://science.sciencemag.org/content/368/6486/89/tab-figures-data (not deep learning, but similar workflow; also extract features as the authors here do, and gets good performance using old CV techniques)</p><p>Deep learning improves automated rodent behavior recognition within a specific experimental setup https://www.sciencedirect.com/science/article/pii/S0165027019303930</p><p>I think Figure 1A is a bit misleading, it's not clear anymore that manual annotation is the only or most common other alternative pipeline (discussed below in benchmarking)- many tools for automated analysis now exist, and tools like JAABA and MotionMapper have been around for 5+ years; I would rather like to see a comparison workflow to &quot;unsupervised methods,&quot; and/or keypoint estimation + classification with supervised or unsupervised means.</p><p>Lastly, they do not discuss key papers in life science for automated animal ethogram building, such as Live Mouse Tracker (https://livemousetracker.org/), BORIS and related Behatrix. Not only should these important papers be discussed, they should likely be benchmarked if the authors want to claim SOTA (see below).</p><p>Datasets: the authors claim they picked challenging datasets (&quot;Diverse and challenging datasets to test DeepEthogram&quot;), but I don't believe this is the case and they should tone down this statement. In fact, the datasets presented are rather easy to solve (the camera is orthogonal to the animal, i.e. top or bottom, or the animal's position is fixed, and the background is homogeneous, rarely the case even for laboratory experiments). I would urge them to use another more challenging dataset, and/or discuss the limitations of this work. For example, a mouse in a standard home cage with bedding, nests, huts, etc would pose more challenges, or they could report their performance on the Kinect700 dataset, which they pretrain on anyhow.</p><p>Benchmarking: The authors don't directly compare their work to that of other tools available in the field. Is their approach better (higher performance) than:</p><p>(1) unsupervised learning methods</p><p>(2) pose estimation plus classifiers or unsupervised clustering (as done in LEAP, DeepLabCut, B-SOiD, SIMBA, and the ETH DLC-Analyzer)</p><p>(3) tools that automate ethogram building, such as JAABA, BORIS/Behatrix.</p><p>Therefore, more results should be presented in relation to key works, and/or a more clear introduction on this topic should be presented.</p><p>– For example, they claim it's hard to match the resulting clusters from unsupervised learning to their &quot;label:&quot; i.e., &quot;their outputs can be challenging to match up to behaviors of interest in cases in which researchers have strong prior knowledge about the specific behaviors relevant to their experiments&quot;. But this is not really a fair statement; one can simply look at the clusters and post-hoc assign a label, which has been nicely done in MotionMapper, for example.</p><p>– In pose estimation, one gets an animal-centric lower dimensional representation, which can be mapped onto behavioral states (ethograms), or used for kinematic analysis if desired. However, there is the minimal number of key points needed to make a representation that can still be used for ethogram building. Is the raw-pixel input truly better than this for all behaviors? For example, on the simple datasets with black backgrounds presented in this work, the background pixels are useless, and don't hinder the analysis. However, if the background dynamically changed (camera is moving, or background changes (lighting, bedding etc)), then the classification task from raw pixels becomes much harder than the task of extracted keypoints to classification task. Therefore, I think the authors should do the following: (1) discuss this limitation clearly in the paper, and (2) if they want to claim their method has universally higher performance, they need to show this on both simple and more challenging data.</p><p>Moveover, the authors discuss 4 limitations of other approaches, but do not address them in their work, i.e.:</p><p>– &quot;First, the user must specify which features are key to the behavior (e.g. body position or limb position), but many behaviors are whole-body activities that could best be classified by full body data.&quot; – can they show an example where this is true? It seems from their data each action could be easily defined by kinematic actions of specific body parts a priori.</p><p>– &quot;Second, errors that occur in tracking these features in a video will result in poor input data to the classification of behaviors, potentially decreasing the accuracy of labeling.&quot; – but is poor video quality not an issue for your classification method? The apple-to-apple comparison here is having corrupted video data as &quot;bad&quot; inputs – of course any method will suffer with bad data input.</p><p>– &quot;Third, users might have to perform a pre-processing step between their raw videos and the input to these algorithms, increasing pipeline complexity and researcher time.&quot; – can they elaborate here? What preprocessing is needed for pose estimation, that is not needed for this, for example? (Both require manual labor, and given the time estimates, DEG takes longer to label than key point estimation due to the human needing to be able to look at video clips (see their own discussion)).</p><p>– &quot;Fourth, the selection of features often needs to be tailored to specific video angles, behaviors (e.g. social behaviors vs. individual mice), species, and maze environments, making the analysis pipelines often specialized to specific experiments.&quot; – this is absolutely true, but also a limitation to the authors work, where the classifiers are tailored, the video should be a fixed perspective, background static, etc. So again I don't see this as a major limitation that makes pose estimation a truly invalid option.</p><p>Benchmarking and Evaluation:</p><p>– &quot;We evaluated how many video frames a user must label to train a reliable model. We selected 1, 2, 4, 8, 12, or 16 random videos for training and used the remaining videos for evaluation. We only required that each training set had at least one frame of each behavior. We trained the feature extractors, extracted the features, and trained the sequence models for each split of the data.&quot; – it is not clear how many FRAMES are used here; please state in # of frames in Figure 5 and in the text (not just video #'s).</p><p>Related: &quot;Combining all these models together, we found that the model performed with more than 90% accuracy when trained with only 80 example frames&quot; This again is a bit misleading, as the user wants to know the total # of frames needed for your data, i.e. in this case this means that a human needs to annotate at least 80-100 frames per behavior, which for 5 states is ~500 frames; this should be made more explicit.</p><p>– &quot;We note that here we used DEG-fast due to the large numbers of splits of the data, and we anticipate that the more complex DEG-medium and DEG-slow models might even require less training data.&quot; – this would go against common assumptions in deep learning; the deeper the models, the more prone to overfitting you are with less data. Please revise, or show the data that this statement is true.</p><p>– &quot;Human-human performance was calculated by defining one labeler as the &quot;ground truth&quot; and the other labeler as &quot;predictions&quot;, and then computing the same performance metrics as for DEG. &quot; – this is a rather unconventional way to measure ground truth performance of humans. Shouldn't the humans be directly compared for % agreement and % disagreement on the behavioral state? (i.e., add a plot to the row that starts with G in figure 3).</p><p>To note, this is a limitation of such approaches, compared to pose-estimation, as humans can disagree on what a &quot;behavior&quot; is, whereas key points have a true GT, so I think it's a really important point that the authors address this head on (thanks!), and could be expanded in the discussion. Notably, MARS puts a lot of effort into measuring human performance, and perhaps this could be discussed in the context of this work as well.</p><p><italic>Reviewer #2:</italic></p><p>It was a pleasure reviewing the methodological manuscript describing DeepEthogram, a software developed for supervised behavioral classification. The software is intended to allow users to automate classification/quantification of complex animal behaviors using a set of supervised deep learning algorithms. The manuscript combines a few state-of-art neural networks into a pipeline to solve the problem of behavior classification in a supervised way. The pipeline uses well-established CNN to extract spatial features from each still frame of the videos that best predicts the user-provided behavior labels. In parallel, optical flow for each frame is estimated through another CNN, providing information about the &quot;instantaneous&quot; movement for each pixel. The optical flow &quot;image&quot; is then passed to another feature extractor that has the same architecture as the spatial feature extractor, and meaningful patterns of pixel-wise movements are extracted. Finally, the spatial feature stream and the optical flow feature stream are combined and fed into a temporal Gaussian mixture CNN, which can pool together information across long periods of time, mimicking human classifiers who can use previous frames to inform classification of behavior in current frame. The resulting pipeline provides a supervised classification algorithm that can directly operate on raw videos, while maintaining a relatively small computational demands on the hardware.</p><p>While I think something like DeepEthogram is needed in the field, I think the authors could do substantially more to validate that DeepEthogram is the ticket. In particular, I find the range of datasets validated in the manuscript poorly representative of the range of behavioral tracking circumstances that researchers routinely face. First, in all exemplar datasets, the animals are recorded in a completely empty environment. The animals are not interacting with any objects as they might in routine behavioral tests; there are no cables attached to them (which is routine for optogenetic studies, physiological recording studies, etc); they are alone (Can DeepEthogram classify social behaviors? the github page lists this as a typical use case); there isn't even cage bedding.</p><p>The authors also tout the time saving benefits of using deep ethogram. However, with their best performing implementation (DEG slow), with a state of the art computer, with a small video (256 x 256 pixels, width by height), the software runs at 15 frames per second (nearly 1/2 the speed of the raw video). My intuition is that this is on the slow side, given that many behaviors can be scored by human observers in near real time if the observer is using anything but a stopwatch. It would be nice to see benchmarks on larger videos that more accurately reflect the range of acquisition frames. If it is necessary for users to dramatically downsample videos, this should be made clear.</p><p>Specific comments:</p><p>– It would be nice to see if DeepEthogram is capable of accurately scoring a behavior across a range of backgrounds. For example, if the model is trained on a sideview recording of an animal grooming in its cage, can it accurately score an animal in an open field doing the same from an overhead view, or a side view? If the authors provided guidance on such issues to the reader this would be helpful.</p><p>– The authors should highlight that human scoring greatly outperforms DEG on a range of behaviors when comparing the individual F1 scores in Figure 3. Why aren't there any statistics for these comparisons?</p><p>– Some of the F1 scores for individual behaviors look very low (~0.5). It would be nice to know what chance performance is in these situations and if the software is performing above chance.</p><p>– I find it hard to understand the size of the data sets used in the analyses. For instance, what is 'one split of the data', referenced in Figure 3? Moreover, the authors state &quot;We selected 1, 2, 4, 8, 12, or 16 random videos for training and used the remaining videos for evaluation&quot; I have no idea what this means. What is the length and fps of the video?</p><p>– Are overall F1 scores in Figure 3 computed as the mean of the individual scores on each component F1 score, or the combination of all behaviors (such that it weights high frequency behaviors)? It's also difficult to understand what the individual points in Figure 4 (a-c) correspond to.</p><p>– The use of the names Mouse-1, Mouse-2 etc for experiments are confusing because it can appear that these experiments are only looking at single mice. I would change the nomenclature to highlight that these reflect experiments with multiple mice.</p><p>– It is not clear why the image has to be averaged across RGB channels and then replicated 20 times for the spatial stream. The author mentioned &quot;To leverage ImageNet weights with this new number of channels&quot;, and I assume this means the input to the spatial stream has to have same shape (number of weights) as the input to the flow stream. However why this is the case is not clear, especially considering two feature extractor networks are independently trained for spatial and flow streams. Lastly this might raise the question of whether there will be valuable information in the RGB channels separately that will be lost from the averaging operation (for example, certain part of an animal's body has different color than others but is equal-luminous).</p><p>– It is not intuitive why simple average pooling is sufficient for fusing the spatial and flow streams. It can be speculated that classification of certain behavior will benefit much more from optical flow features while other behaviors benefits from still image features. I'm curious to see whether an additional layer at the fusing stage that has behavior-specific weights could improve performance.</p><p>– Since computational demands is one of the major concern in this article, I'm wondering whether exploiting the sparse nature of the input images would further improve the performance of the algorithm. Often times the animal of interests only occupies a small number of pixels in the raw images, and some simple thresholding of the images, or even user-defined masking of the images, together with use of sparse data backends and operations should in theory significantly reduce the computational demands for both the spatial and flow feature extractor networks.</p><p><italic>Reviewer #3:</italic></p><p>The paper by Bohnslav et al., presents a software tool that integrates a supervised machine learning algorithm for detecting and quantifying behavior directly from raw video input. The manuscript is well-written, the results are clear. Strengths and weaknesses of the approach are discussed and the work is appropriately placed in the bigger context of ongoing research in the field. The algorithms demonstrate high performance and reach human-level accuracy for behavior recognition. The classifiers are embedded in an excellent user-friendly interface that eliminates the need of any programming skills on the end of the user. Labeled datasets can even be imported. We suggest additional analyses to strengthen the manuscript.</p><p>1) Although the presented metrics for accuracy and F1 are state of the art it would be useful to also report absolute numbers for some of the scored behaviors for each trial, because most behavioral neuroscience studies actually report behavior in absolute numbers and/or duration of individual behaviors (rears, face grooms, etc.). Correlation of human and DEG data should also be presented on this level. This will speak to many readers more directly than the accuracy and F1 statistics. For this, we would like to see a leave-one-out cross-validation or a k-fold cross-validation (ensure that each trial ends up exactly once in a cross validation set) that enables a final per-trial readout. This can be done with only one of the DEG types (e.g &quot;fast&quot;). The current randomization approach of 60/20/20% (train/validate/test) with a n of 3 repeats is insufficient, since it a) allows per-trial data for at most 60% of all files and b) is susceptible to artefacts due to random splits (i.e one abnormal trial can be over or under represented in the cross validation sets).</p><p>2) In line with comment 1) we propose to update Figure 4, which at the moment uses summed up data from multiple trials. We would rather like to see each trial represented by a single data-point in this figure (#bouts/#frames by behavior). As alternative to individual scatterplots, correlation-matrix-heatmaps could be used to compare different raters.</p><p>3) Direct benchmarking against existing datasets is necessary. With many algorithms being published these days, it is important to pick additional (published) datasets and test how well the classifiers perform on those videos. Their software package already allows import of labeled datasets, some are available online. For example, how well can DeepEthogram score…</p><p>a. grooming in comparison to Hsu and Yttri (REF #17) or van den Boom et al., (2017, J Neurosci Methods).</p><p>b. rearing in comparison to Sturman et al., (REF #21).</p><p>c. social interactions compared to (Segalin et al., (REF #7) or Nilsson et al., (REF #19)).</p><p>4) In the discussion on page 19 the authors state: &quot;Subsequently, tens to hundreds to thousands of movies could be analyzed, across projects and labs, without additional user-time, which would normally cost additionally hundreds to thousands of hours of time from researchers.&quot; This sentence suggests that a network trained on the e.g. the open field test in one lab can be transferred across labs. This key issue of &quot;model transferability&quot; should be tested. E.g. the authors could use the classifier from mouse#3 and test is on another available top-view recording dataset recorded in a different lab with different open-field setup (datasets are available online, e.g. REF #21).</p><p>5) Figure 5D/E: Trendline is questionable, we would advise to fit a sigmoid trendline, not an arbitrarily high order polynomial. Linear trend lines (such as shown in Figure 4) should include R<sup>2</sup>values on the plot or in the legend.</p><p>6) In the discussion, the authors do a very good job highlighting the limitations and advantages of their approach. The following limitations should however be expanded:</p><p>a. pose-estimation-based approaches (e.g. DLC) are going to be able to track multiple animals at the same time (thus allowing e.g. better read-outs of social interaction). It seems this feature cannot be incorporated in DeepEthogram.</p><p>b. Having only 2 human raters is a weakness that should briefly be addressed. Triplicates are useful for assessing outlier values, this could be mentioned in light of the fact that the F1 score of DeepEthogram occasionally outperforms the human raters (e.g. Figure 3C,E).</p><p>c. Traditional tracking measures such as time in zone, distance moved and velocity cannot be extracted with this approach. These parameters are still very informative and require a separate analysis with different tools (creating additional work).</p><p>d. The authors are correct that the additional time required for behavior analysis (due to the computationally demanding algorithms) is irrelevant for most labs. However, they should add (1) that the current system will not be able to perform behavior recognition in real time (thus preventing the use of closed-loop systems, which packages such as DLC have made possible) and (2) that the speed they discuss on page 16 is based on an advanced computer system (GPU, RAM) and will not be possible with a standard lab computer (or provide an estimate how long training would require if it is possible).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels&quot; for further consideration by<italic>eLife</italic>. Your revised article has been evaluated 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Johannes Bohacek (Reviewer #3).</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The reviewers all felt the manuscript was improved, and thank the authors for the additional datasets and analysis. We would just like to see two items before the publication is accepted fully.</p><p>(1) Both reviewer #1 and #2 note the new data is great, but lacks human ground truth. Both for comparison, and releasing the data for others to benchmark on, it would be please include the data. We also understand that obtaining ground truth from 3 persons is a large time commitment, but even if there is one person, this data should be included for all datasets shown in Figure 3.</p><p>(2) Please include links for the raw videos used in this work; it is essential for others to benchmark and use to validate the algorithm presented here (see Reviewer 3: &quot;raw videos used in this work (except the ones added during the revision) are – it appears – not accessible online&quot;).</p><p>Lastly, reviewer 3 notes that perhaps, still, some use-cases are best suited for DeepEthogram, while others more for pose-estimation plus other tools, but this of course cannot be exhaustively demonstrated here; at your discretion you might want to address in the discussion, but we leave that up to your judgement.<italic>Reviewer #1:</italic></p><p>I thank the authors for the revisions and clarifications, and I think the manuscript is much improved. Plus, the new datasets and comparisons to B-iOD and R-Analyzer (Sturman) are a good additions.</p><p>One note is that is not clear which datasets have ground truth data; namely, in the results 5 datasets they use for testing are introduced:</p><p>&quot;Mouse-Ventral1&quot;</p><p>&quot;Mouse-Ventral2&quot;</p><p>&quot;Mouse-Openfield&quot;</p><p>&quot;Mouse-Homecage&quot;</p><p>&quot;Mouse-Social&quot;</p><p>plus three datasets from published work by Sturman et al.,</p><p>and &quot;fly&quot;</p><p>Then it states that all datasets were labeled; yet, Figure 3 has no ground truth for &quot;Mouse-Ventral2&quot; , &quot;Mouse-Homecage&quot; , &quot;Mouse-Social&quot; or 'Fly&quot; -- please correct and include the ground truth. I do see that is says that only a subset of each of the 2 datasets in Figure 3 are labeled with 3 humans, but minimally then the rest (1 human?) should be included in Figure 3 (and be made open source for future benchmarking).</p><p>It appears from the discussion this was done (i.e., at least 1 human, as this is of course required for the supervised algorithm too):</p><p>&quot;In our hands, it took approximately 1-3 hours for an expert researcher to label five behaviors in a ten-minute movie from the Mouse-Openfield dataset&quot; and it appears that labeling is defined in the methods.</p><p><italic>Reviewer #2:</italic></p><p>The authors did a great job addressing our comments, especially with the additional validation work. My only concern is that some of the newly included datasets don't have human-labeled performance for comparison, hence making it hard to judge the actual performance of DeepEthogram. While I understand it is very time-consuming to obtain human labels, I think it will greatly improve the impact of the work if the model comparison can be bench-marked against ground truth. Especially it would be great to see the comparison to human label for the &quot;Mouse-Social&quot; and &quot;Mouse-Homecage&quot; datasets, which presumably represent a large proportion of use cases for DeepEthogram. Otherwise I think it looks good and I would support publication of this manuscript.</p><p><italic>Reviewer #3:</italic></p><p>The authors present a software solution (DeepEthogram) that performs supervised machine-learning analysis of behavior directly from raw videos files. DeepEthogram comes with a graphical user interface and performs behavior identification and quantification with high accuracy, requires modest amounts of pre-labeled training data, and demands manageable computational resources. It promises to be a versatile addition to the ever-growing compendium of open-source behavior analysis platforms and presents an interesting alternative to pose-estimation-based approaches for supervised behavior classification, under certain conditions.</p><p>The authors have generated a large amount of additional data and showcase the power of their approach in a wide variety of datasets including their own data as well as published datasets. DeepEthogram is clearly a powerful tool and the authors do an excellent job describing the advantages and disadvantages of their system and provide a nuanced comparison of point-tracking analyses vs. analyses based on raw videos (pixel data). Also their responses to the reviewers comments are very detailed, thoughtful and clear. The only major issue is that the raw videos used in this work (except the ones added during the revision) are – it appears – not accessible online. This problem must be solved, the videos are essential for reproducibility.</p><p>A minor caveat is that in order to compare DeepEthogram to existing supervised and unsupervised approaches, the authors have slightly skewed the odds in their favor by picking conditions that benefit their own algorithm. In the comparison with point-tracking data they use a low resolution top-view recording to label the paws of mice (which are obstructed most of the time from this angle). In the comparison with unsupervised clustering, they use the unsupervised approach for an application that it isn't really designed for (performed in response to reviewers requests). But the authors directly address these points in the text, and the comparisons are still valid and interesting and address the reviewers concerns.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63377.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>Bohnslav et al., present a new toolkit and GUI for using video input to extract behavioral states (ethograms) using a set of established deep neural networks. They show their pipeline works on range of laboratory datasets, and provide metrics comparing network performance to humans. However, the reviewers all agreed there are several key revisions needed in order to support the main claims of the paper. These revolve around benchmarking, datasets, and a more careful handling of related work, limitations of such a software, and clarifying methods. We have collectively decided to send the individual reviews from each reviewer, and ask you address those (and perhaps combine where you see fit), but we urge you to focus in on the following points for your revision.</p></disp-quote><p>We thank the reviewers for their feedback and constructive suggestions. We have worked hard to incorporate all the suggestions of each reviewer and feel that the manuscript and software are much improved as a result.</p><disp-quote content-type="editor-comment"><p>Datasets:</p><p>The reviewers each expressed concern over the simplicity of the datasets and the potentially limited scope of DeepEthogram in relation. For example, the authors claim these are difficult datasets, but in fact we feel they are not representative of the laboratory videos often collected: they have very static backgrounds, no animals have cables or other occluders. We would urge the authors to use other datasets, even those publically available, to more thoroughly benchmark performance in a broader collection of behaviors.</p></disp-quote><p>We thank the reviewers for the suggestion to add more datasets that cover a wider range of behavior settings. We have now added five datasets, including three publicly available datasets and two new datasets collected by us that specifically address these concerns. The new datasets we collected include a mouse in a homecage, which contains a complex background and occluders. The second dataset we added is a social interaction dataset that includes two mice and thus complex and dynamic settings. The three publicly available datasets we added feature commonly used behavioral paradigms: the open field test, the forced swim test, and the elevated plus maze. We thank Sturman et al., for making their videos and labels publicly available.</p><p>We now have nine datasets in total that span two species, multiple view angles (dorsal, ventral, side), individual and social settings, complex and static backgrounds, and multiple types of occluders (objects and other mice). We feel these datasets cover a wide range of common lab experiments and typical issues for behavior analysis. It is of course not possible to cover all types of videos, but we have made a sincere and substantial effort to demonstrate the efficacy of our software in a variety of settings.</p><p>We think the datasets that we include are representative of the laboratory videos often collected. To our knowledge, datasets for open field behavior are some of the most commonly collected in laboratory settings when one considers behavioral core facilities, biotech/pharmaceutical companies, as well as the fields of mouse disease models, mouse genetic mutations, and behavioral pharmacology. The same is true for the elevated plus maze, which is now included in the revision. A Pubmed search for “open field test” or “elevated plus maze” reveals more than 2500 papers for each in the past five years. We also note that all the datasets we include were not collected only for the purpose of testing our method; rather, they were collected for specific neuroscience research questions, indicating that they are at least reflective of the methods used in some fields, including the large fields of pain research, anxiety research, and autism research. The new additions of the datasets for the forced swim test, elevated plus maze, social interaction, and homecage behavior extend our tests of DeepEthogram to other commonly acquired videos. We agree that these videos do not cover all the possible types of videos that can be collected or that are common in a lab setting, but we are confident these videos cover a wide range of very commonly used behavioral tests and thus will be informative regarding the possible utility of DeepEthogram.</p><disp-quote content-type="editor-comment"><p>Benchmarking:</p><p>While DeepEthogram could be an important tool to the growing toolbox of deep learning tools for behavior, we felt that there are sufficiently other options available that the authors should directly compare performance. While we do appreciate that comparing to the &quot;gold standard&quot; of human-labeled data, the real challenge with such datasets is even humans tend not to agree on a semantic label. Here, the authors only use two humans for ground-truth annotation, but there is a concern of an outlier. Typically, 3 humans are used to overcome a bit of this limitation. Therefore, we suggest carefully benchmarking against humans (i.e., increase the number of ground truth annotations), and please see the individual reviewer comments with specific questions related to other published/available code bases where you can directly compare your pipelines performance.</p></disp-quote><p>We thank the reviewers for the recommendation to add more benchmarking. We have extended our benchmarking analysis in two important ways. First, we have added a third human labeler. The results are qualitatively similar to before with the third human labeler added. We have also included three datasets from Sturman et al., each of which has three labelers. Second, we have added a comparison to other recent approaches in the field, including the use of keypoint tracking followed by supervised classification into behaviors and the use of unsupervised behavior analysis followed by post-hoc labeling of machine-generated clusters. We find that DeepEthogram performs better than these alternate methods. We think the addition of this new benchmarking combined with the new datasets will provide the reader with an accurate measure of DeepEthogram’s performance.</p><disp-quote content-type="editor-comment"><p>Methods, Relation to other packages, and Limitations:</p><p>The reviewers raised several points where methods are unclear, or how an analysis was performed was not clear. In particular, we ask you to check reviewer #3's comments carefully regarding methods. Moreover, we think a more nuanced discussion about when to do some &quot;pre-processing&quot; (like pose estimation) would be beneficial vs. straight to an ethogram, and visa versa. In particular, it's worth nothing that often times having an intermediate bottleneck such as key points allows the user to more easily assess network performance (keypoints are a defined ground truth vs. semantic action labels).</p></disp-quote><p>We thank the reviewers for these suggestions. We have clarified the methods and analysis as suggested and detailed below. We have also extended the discussion of our method relative to others, including keypoint approaches, so that the advantages and disadvantages of our approach are clearer. These changes are described in detail below in response to individual reviewer comments.</p><disp-quote content-type="editor-comment"><p>In total, the reviews are certainly enthusiastic about this work, and do hope you find these suggestions helpful. We look forward to reading your revision.</p></disp-quote><p>We appreciate the time that the reviewers spent to provide detailed comments and constructive suggestions. The feedback has been valuable in helping us to improve the paper and the software.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>Bohnslav et al., present a new tool to quantify behavior actions directly from video. I think this is a nice addition to the growing body of work using video to analyze behavior. The paper is well written, clear for a general audience, and takes nice innovations in computer vision into life sciences and presents a usable tool for the community. I have a few critical points that I believe need addressed before publication, mostly revolving around benchmarking, but overall I am enthusiastic about this work being in eLife.</p></disp-quote><p>We thank the reviewer for this positive feedback and for their recommendations that have helped us to improve our work.</p><disp-quote content-type="editor-comment"><p>In the following sections I highlight areas I believe can be improved upon.</p><p>In relation to prior work: The authors should more explicitly state their contribution, and the field's contributions, to action recognition. The introduction mostly highlights limitations of unsupervised methods to perform behavioral analysis (which to note, produces the same outputs as this paper, i.e. an ethogram) and key point estimation alone, which of course is tackling a different problem. What I would like to see is a more careful consideration of the state-of-the-field in computer vision for action recognition, and clearly defining what the contribution is in this paper the cover letter alludes to them developing novel computer vision aspects of the package, but from the code base, etc, it seems they utilize (albeit nicely!) pre-existing works from ~3 years ago, begging the question if this is truly state-of-the-art performance. Moreover, and this does hurt novelty a bit, this is not the first report in life science of such a pipeline, so this should be clearly stated. I don't think it's required to compare this tool to every other tool available, but I do think discussing this in the introduction is of importance (but again, I am still enthusiastic for this being in eLife).</p></disp-quote><p>We have revised the introduction so that it now focuses on how DeepEthogram differs in design to previous methods used to classify animal behaviors. We try to provide the reader with an understanding of the differences in design and uses between methods that utilize unsupervised behavior classification, keypoints, and classification from raw pixel values. We have removed the text that focused on potential limitations of keypoint-based methods. We have also added sentences that highlight that the approach we take is built on existing methods that have addressed action detection in different settings, making it clear that our software is not built from scratch and is rather an extension and novel use case of earlier, pioneering work in a different field. We now directly state that our contribution is to extend and apply this previous work in the new setting of animal behavior and life sciences research.</p><disp-quote content-type="editor-comment"><p>&quot;Our model operates directly on the raw pixel values of videos, and thus it is generally applicable to any case with video data and binary behavior labels and further does not require pre-specification of the body features of interest, such as keypoints on limbs or fitting the body with ellipses.&quot; -- please include references to the many other papers that do this as well. For example, please see:</p><p>Data-driven analyses of motor impairments in animal models of neurological disorders https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000516</p><p>LSTM Self-Supervision for Detailed Behavior Analysis https://openaccess.thecvf.com/content_cvpr_2017/html/Brattoli_LSTM_Self-Supervision_for_CVPR_2017_paper.html</p><p>Facial expressions of emotion states and their neuronal correlates in mice https://science.sciencemag.org/content/368/6486/89/tab-figures-data (not deep learning, but similar workflow; also extract features as the authors here do, and gets good performance using old CV techniques)</p><p>Deep learning improves automated rodent behavior recognition within a specific experimental setup https://www.sciencedirect.com/science/article/pii/S0165027019303930</p></disp-quote><p>We have added new citations on this topic.</p><disp-quote content-type="editor-comment"><p>I think Figure 1A is a bit misleading, it's not clear anymore that manual annotation is the only or most common other alternative pipeline (discussed below in benchmarking)- many tools for automated analysis now exist, and tools like JAABA and MotionMapper have been around for 5+ years; I would rather like to see a comparison workflow to &quot;unsupervised methods,&quot; and/or keypoint estimation + classification with supervised or unsupervised means.</p></disp-quote><p>We have now added direct comparisons to the methods mentioned by the reviewer. In Supplementary Figures 20-21, we have direct comparisons to methods for keypoint estimation + classification and for unsupervised classification followed by post-hoc labeling. We show that DeepEthogram performs better than these approaches, at least for the dataset and behaviors we tested. In the discussion, we now provide an extended section on cases in which we expect DeepEthogram to perform better than other methods and cases in which we anticipate other methods, such as keypoint estimation followed by classification, may outperform DeepEthogram. We hope these new comparisons and new text help readers understand the differences between methods as well as the situations in which they should choose one versus the other.</p><p>To our knowledge, manual annotation is still very common, and we do not think automated analysis tools, such as JAABA and MotionMapper, are more commonly used than manual annotation. We have many colleagues in our department, departments at neighboring institutions, and behavior core facilities that solve the problem of behavior classification using manual annotation. In fact, we have had a hard time finding any colleagues who use JAABA and MotionMapper as their main methods. We asked our colleagues why they do not use these automated approaches, and the common response has been that they have been tried but that they do not work sufficiently well or they are challenging to implement. Thus, to our knowledge, unlike DeepLabCut for keypoint estimation, these methods have not caught on for behavior classification. There are of course labs that use the existing automated approaches for this problem, but our informal survey of colleagues and the literature indicates that manual annotation is still very common.</p><p>For these reasons, we think that mentioning the idea of manual annotation is valid, and following the reviewer’s suggestion, we now focus our introduction on describing the approaches taken by recent automated pipelines, with an emphasis on how our pipeline differs in design.</p><disp-quote content-type="editor-comment"><p>Lastly, they do not discuss key papers in life science for automated animal ethogram building, such as Live Mouse Tracker (https://livemousetracker.org/), BORIS and related Behatrix. Not only should these important papers be discussed, they should likely be benchmarked if the authors want to claim SOTA (see below).</p></disp-quote><p>We thank the reviewer for these references, and we have added citations to these papers. However, we feel that each of these references solves a different problem than the one DeepEthogram is meant to address. Live Mouse Tracker is an interesting approach. However, it uses specialized hardware (depth sensors and RFID monitoring), which means their approach cannot be applied using typical video recording hardware, and is specific to mouse social behaviors. DeepEthogram is meant to use typical lab hardware and to be general-purpose across species and behaviors. BORIS appears to be an excellent open-source GUI for labeling behaviors in videos. However, it does not appear to include automatic labeling or machine learning. Behatrix is software for generating transition matrices, calculating statistics from behavioral sequences, and generating plots. When combined with BORIS, it is software for data analysis of human-labeled behavioral videos, but it does not appear to perform automatic labeling of videos. We have therefore cited these papers, but we do not see an easy way to benchmark our work against these approaches given that they seem to address different goals. Instead, we have added benchmarking of our work relative to a keypoint estimation + classification approach and in comparison to an unsupervised clustering with post-hoc labeling approach.</p><disp-quote content-type="editor-comment"><p>Datasets: the authors claim they picked challenging datasets (&quot;Diverse and challenging datasets to test DeepEthogram&quot;), but I don't believe this is the case and they should tone down this statement. In fact, the datasets presented are rather easy to solve (the camera is orthogonal to the animal, i.e. top or bottom, or the animal's position is fixed, and the background is homogeneous, rarely the case even for laboratory experiments). I would urge them to use another more challenging dataset, and/or discuss the limitations of this work. For example, a mouse in a standard home cage with bedding, nests, huts, etc would pose more challenges, or they could report their performance on the Kinect700 dataset, which they pretrain on anyhow.</p></disp-quote><p>We have removed our statement about the datasets being challenging in general. We have now clarified the specific challenges that the datasets present to machine learning. In particular, we emphasize the major imbalance in the frequency of classes. Many of the classes of interest are only present in ~1-5% of the frames. In fact, this was the hardest problem for us to solve.</p><p>Following the reviewer’s suggestions, we have added new datasets that present the challenges mentioned, including complex backgrounds, occluders, and social interactions. This includes a mouse in the homecage with bedding, objects, and a hut, as suggested. We find that DeepEthogram performs well on these more challenging datasets.</p><disp-quote content-type="editor-comment"><p>Benchmarking: The authors don't directly compare their work to that of other tools available in the field. Is their approach better (higher performance) than:</p><p>(1) unsupervised learning methods</p></disp-quote><p>It is difficult to compare supervised learning methods to unsupervised ones. The objectives of the two approaches are different. In unsupervised methods, the goal is to identify behavior dimensions or clusters based on statistical regularities, whereas in supervised methods the goal is to label research-defined behaviors. The dimensions or clusters identified in unsupervised approaches are not designed to line up to researcher-defined behaviors of interest. In some cases, they may line up nicely, but in other cases it may be more challenging to identify a researcher’s behavior of interest in the output of an unsupervised algorithm.</p><p>Nevertheless, it is possible to try this approach and compare its performance to DeepEthogram (Supplementary Figure 21). We used DeepLabCut to identify keypoints followed by B-SOiD, an unsupervised method, to identify behavior clusters. We then labeled the B-SOiD clusters based on similarity to our pre-defined behaviors of interest. We verified that the DeepLabCut tracking worked well. Also, B-SOiD found statistically meaningful clusters that divided the DeepLabCut-based features into distinct parts of a low dimensional behavior space. However, in many cases, the correspondence of machine-generated clusters to researcher-defined behaviors of interest was poor.</p><p>We found that DeepEthogram’s performance was higher than that of the pipeline using unsupervised methods. For example, “rearing” was the behavior that the B-SOiD approach predicted most accurately, at 76% accuracy. In comparison, our worst model, DeepEthogram-fast, scored this behavior with 94% accuracy. The difference in performance was even greater for the other behaviors. Thus, a workflow in which one first generates unsupervised clusters and then treats them as labels for researcher-defined behaviors did not work as well as DeepEthogram.</p><disp-quote content-type="editor-comment"><p>(2) pose estimation plus classifiers or unsupervised clustering (as done in LEAP, DeepLabCut, B-SOiD, SIMBA, and the ETH DLC-Analyzer)</p></disp-quote><p>We benchmarked DeepEthogram relative to pose estimation plus classification using the classification architecture from Sturman et al., (ETH DLC-Analyzer) (Supplementary Figure 20). Note that we re-implemented the Sturman et al. approach for two reasons. First, our datasets can have more than one behavior per timepoint, which necessitates different activation and loss functions. Second, we wanted to ensure the same train/validation/test splits were used for both DeepEthogram and the ETH DLC-Analyzer classification architecture (the multilayer perceptron).</p><p>We found that DeepEthogram had higher accuracy and F1 scores than the ETH DLC-Analyzer on the dataset we tested. The two methods performed similarly on bout statistics, including the number of bouts and bout duration.</p><p>As mentioned above, we tested unsupervised clustering using a B-SOiD-based pipeline and found that DeepEthogram performed better.</p><p>We did not test other pipelines that are based on keypoint estimation plus classifiers, such as JAABA and SIMBA. The reason is that each pipeline takes a substantial effort to get set up and to verify that it is working properly. In a reasonable amount of time, it is therefore not feasible to test many different pipelines for benchmarking purposes. We therefore chose to focus on two pipelines with different approaches, namely the ETH DLC-Analyzer for keypoints + classification and the B-SOiD-based unsupervised approach.</p><disp-quote content-type="editor-comment"><p>(3) tools that automate ethogram building, such as JAABA, BORIS/Behatrix.</p></disp-quote><p>BORIS and Behatrix are excellent tools for annotated videos and creating statistics of behaviors. However, they do not provide automated labeling pipelines, to our knowledge. We therefore did not see how to benchmark our work relative to these approaches.</p><p>As mentioned above, it takes a great deal of time and effort to set up and validate each new pipeline. Given that JAABA is based on the idea of keypoint estimation followed by classification, we chose not to implement it and instead focused only on the ETH DLC-Analyzer for this approach. It would be ideal to rapidly benchmark DeepEthogram relative to many approaches, but we were unable to find a way to do so in a reasonable amount of time, especially because we need to ensure that each existing algorithm is operating properly.</p><disp-quote content-type="editor-comment"><p>Therefore, more results should be presented in relation to key works, and/or a more clear introduction on this topic should be presented.</p><p>– For example, they claim it's hard to match the resulting clusters from unsupervised learning to their &quot;label:&quot; i.e., &quot;their outputs can be challenging to match up to behaviors of interest in cases in which researchers have strong prior knowledge about the specific behaviors relevant to their experiments&quot;. But this is not really a fair statement; one can simply look at the clusters and post-hoc assign a label, which has been nicely done in MotionMapper, for example.</p></disp-quote><p>We have now supported this claim by trying out an unsupervised approach. We used the B-SOiD algorithm on DeepLabCut keypoints (Supplementary Figure 21). B-SOiD identified well separated behavior clusters. However, we found it challenging to line up these clusters to the researcher-defined behaviors of interest. In our Mouse-Openfield dataset, we found one cluster that looked like “rearing”, but it was not possible to find clusters that looked like the other behaviors. It is of course possible that in some cases the clusters and behaviors will line up nicely, but that was not apparent in our results. More generally, the goals of supervised and unsupervised clustering are not the same, so in principle there is no reason that post-hoc assignment of labels will work well. It is true that in theory this might work reasonably okay in some cases, but our results in Supplementary Figure 21 show that this approach does not work as well as DeepEthogram.</p><disp-quote content-type="editor-comment"><p>– In pose estimation, one gets an animal-centric lower dimensional representation, which can be mapped onto behavioral states (ethograms), or used for kinematic analysis if desired. However, there is the minimal number of key points needed to make a representation that can still be used for ethogram building. Is the raw-pixel input truly better than this for all behaviors? For example, on the simple datasets with black backgrounds presented in this work, the background pixels are useless, and don't hinder the analysis. However, if the background dynamically changed (camera is moving, or background changes (lighting, bedding etc)), then the classification task from raw pixels becomes much harder than the task of extracted keypoints to classification task. Therefore, I think the authors should do the following: (1) discuss this limitation clearly in the paper, and (2) if they want to claim their method has universally higher performance, they need to show this on both simple and more challenging data.</p></disp-quote><p>We have now added a paragraph in the discussion that mentions cases in which DeepEthogram is expected to perform poorly. This includes cases with, for example, moving cameras and changing lighting. We are not aware that these scenarios are common in laboratory settings, but nevertheless we mention conditions that will help DeepEthogram’s performance, including fixed camera angles, fixed lighting, and fixed backgrounds. In addition, we have added two datasets that have more complex or changing backgrounds. In the Mouse-Homecage dataset, there are bedding, objects, and a hut, which provide occluders and a complex background. In the Sturman-FST, the background is dynamic due to water movement as the animal swims. We also now include a social interaction dataset, which adds a different use case. In all these cases, DeepEthogram performs well.</p><p>We have expanded our discussion of the settings in which DeepEthogram may excel and those in which other methods, like keypoint-based methods, might be advantageous. In addition, we now mention that DeepEthogram is intended as a general framework that can be used without extensive fine tuning and customization for each experiment. As with all software focused on general purpose uses, we anticipate that DeepEthogram will perform worse than solutions that are custom for a problem at hand, such as custom pipelines for moving cameras or specific pose estimations. We now note this tradeoff between general purpose and customized solutions and discuss where DeepEthogram falls on this spectrum.</p><disp-quote content-type="editor-comment"><p>Moveover, the authors discuss 4 limitations of other approaches, but do not address them in their work, i.e.:</p><p>– &quot;First, the user must specify which features are key to the behavior (e.g. body position or limb position), but many behaviors are whole-body activities that could best be classified by full body data.&quot; – can they show an example where this is true? It seems from their data each action could be easily defined by kinematic actions of specific body parts a priori.</p></disp-quote><p>We have removed this sentence. Instead, in the introduction, we now focus on the design differences and general goals of the different approaches with less emphasis on potential limitations. In the discussion, we present a balanced discussion of cases in which DeepEthogram might be a good method to choose and cases in which other methods, such as those with keypoint estimation, might be better.</p><disp-quote content-type="editor-comment"><p>– &quot;Second, errors that occur in tracking these features in a video will result in poor input data to the classification of behaviors, potentially decreasing the accuracy of labeling.&quot; – but is poor video quality not an issue for your classification method? The apple-to-apple comparison here is having corrupted video data as &quot;bad&quot; inputs – of course any method will suffer with bad data input.</p></disp-quote><p>We have removed this sentence.</p><disp-quote content-type="editor-comment"><p>– &quot;Third, users might have to perform a pre-processing step between their raw videos and the input to these algorithms, increasing pipeline complexity and researcher time.&quot; – can they elaborate here? What preprocessing is needed for pose estimation, that is not needed for this, for example? (Both require manual labor, and given the time estimates, DEG takes longer to label than key point estimation due to the human needing to be able to look at video clips (see their own discussion)).</p></disp-quote><p>We have re-phrased this point in the introduction. The main point we wish to make is that a pipeline for behavior classification based on keypoints + classification requires two steps: keypoint estimation followed by behavior classification. This requires two stages of manual annotation, one for labeling the keypoints and one for labeling the behaviors for classification. If one wants to get frame-by-frame predictions for this type of pipeline, then one must have frame-by-frame labeling of training videos. Instead, DeepEthogram only requires the latter step. As the reviewer notes, the frame-by-frame behavior labeling is indeed the more time-consuming step, but it is required for both types of pipelines.</p><disp-quote content-type="editor-comment"><p>– &quot;Fourth, the selection of features often needs to be tailored to specific video angles, behaviors (e.g. social behaviors vs. individual mice), species, and maze environments, making the analysis pipelines often specialized to specific experiments.&quot; – this is absolutely true, but also a limitation to the authors work, where the classifiers are tailored, the video should be a fixed perspective, background static, etc. So again I don't see this as a major limitation that makes pose estimation a truly invalid option.</p></disp-quote><p>We have removed this sentence from the introduction and added this point to the discussion where we talk about the advantages and disadvantages of DeepEthogram. We disagree that DeepEthogram requires tailored classifiers. We have used the same model architecture for all nine datasets that involve different backgrounds, species, behaviors, and camera angles. It is true that DeepEthogram might not work in some cases, such as with moving cameras, and we now highlight the cases in which we expect DeepEthogram will fail. In general, though, we have used the same model throughout our study for a range of datasets.</p><disp-quote content-type="editor-comment"><p>Benchmarking and Evaluation:</p><p>– &quot;We evaluated how many video frames a user must label to train a reliable model. We selected 1, 2, 4, 8, 12, or 16 random videos for training and used the remaining videos for evaluation. We only required that each training set had at least one frame of each behavior. We trained the feature extractors, extracted the features, and trained the sequence models for each split of the data.&quot; – it is not clear how many FRAMES are used here; please state in # of frames in Figure 5 and in the text (not just video #'s).</p><p>Related: &quot;Combining all these models together, we found that the model performed with more than 90% accuracy when trained with only 80 example frames&quot; This again is a bit misleading, as the user wants to know the total # of frames needed for your data, i.e. in this case this means that a human needs to annotate at least 80-100 frames per behavior, which for 5 states is ~500 frames; this should be made more explicit.</p></disp-quote><p>We now report the number of frames to Figure 6 as suggested by the reviewer.</p><p>We choose to report the number of example frames because this is what is most relevant for the model, but we realize this is not the most relevant for the user. The number of actual frames (not just examples) varies depending on how frequent the behavior is. If 100 example frames are needed for a behavior, that could mean the user has to label 200 frames if the behavior occurs 50% of the time or 10,000 frames if the behavior occurs 1% of the time. We have now added sentences in the text to help the reader with this conversion. We do not think it is helpful to report the number of actual frames in the figure because then it is impossible for the user to extrapolate to their own use cases. Instead, if they know the number of example frames required and how frequent their behavior of interest is, they can calculate for themselves how many frames will need to be labeled. The new sentences we added help clarify this point and will help the reader calculate for their own cases.</p><disp-quote content-type="editor-comment"><p>– &quot;We note that here we used DEG-fast due to the large numbers of splits of the data, and we anticipate that the more complex DEG-medium and DEG-slow models might even require less training data.&quot; – this would go against common assumptions in deep learning; the deeper the models, the more prone to overfitting you are with less data. Please revise, or show the data that this statement is true.</p></disp-quote><p>We have removed this sentence.</p><disp-quote content-type="editor-comment"><p>– &quot;Human-human performance was calculated by defining one labeler as the &quot;ground truth&quot; and the other labeler as &quot;predictions&quot;, and then computing the same performance metrics as for DEG. &quot; – this is a rather unconventional way to measure ground truth performance of humans. Shouldn't the humans be directly compared for % agreement and % disagreement on the behavioral state? (i.e., add a plot to the row that starts with G in figure 3).</p></disp-quote><p>We agree that this description was confusing. We have now clarified in the text that the percent agreement between human labelers is identical to the accuracy measure we report.</p><disp-quote content-type="editor-comment"><p>To note, this is a limitation of such approaches, compared to pose-estimation, as humans can disagree on what a &quot;behavior&quot; is, whereas key points have a true GT, so I think it's a really important point that the authors address this head on (thanks!), and could be expanded in the discussion. Notably, MARS puts a lot of effort into measuring human performance, and perhaps this could be discussed in the context of this work as well.</p></disp-quote><p>We appreciate that the reviewer acknowledged the hard work of labeling many videos with multiple human labelers. We have now extended this effort to include a third human labeler for the Mouse-Ventral1 dataset. We have also added three publicly available datasets that each have three human labelers.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>It was a pleasure reviewing the methodological manuscript describing DeepEthogram, a software developed for supervised behavioral classification. The software is intended to allow users to automate classification/quantification of complex animal behaviors using a set of supervised deep learning algorithms. The manuscript combines a few state-of-art neural networks into a pipeline to solve the problem of behavior classification in a supervised way. The pipeline uses well-established CNN to extract spatial features from each still frame of the videos that best predicts the user-provided behavior labels. In parallel, optical flow for each frame is estimated through another CNN, providing information about the &quot;instantaneous&quot; movement for each pixel. The optical flow &quot;image&quot; is then passed to another feature extractor that has the same architecture as the spatial feature extractor, and meaningful patterns of pixel-wise movements are extracted. Finally, the spatial feature stream and the optical flow feature stream are combined and fed into a temporal Gaussian mixture CNN, which can pool together information across long periods of time, mimicking human classifiers who can use previous frames to inform classification of behavior in current frame. The resulting pipeline provides a supervised classification algorithm that can directly operate on raw videos, while maintaining a relatively small computational demands on the hardware.</p></disp-quote><p>Thank you for the positive feedback.</p><disp-quote content-type="editor-comment"><p>While I think something like DeepEthogram is needed in the field, I think the authors could do substantially more to validate that DeepEthogram is the ticket. In particular, I find the range of datasets validated in the manuscript poorly representative of the range of behavioral tracking circumstances that researchers routinely face. First, in all exemplar datasets, the animals are recorded in a completely empty environment. The animals are not interacting with any objects as they might in routine behavioral tests; there are no cables attached to them (which is routine for optogenetic studies, physiological recording studies, etc); they are alone (Can DeepEthogram classify social behaviors? the github page lists this as a typical use case); there isn't even cage bedding.</p></disp-quote><p>We thank the reviewer for this suggestion. We have now added five new datasets that address many of these concerns. We have added a dataset in which the mouse is in its homecage. In these videos, there is bedding, and the mouse interacts with multiple objects and is occluded by these objects and a hut. Second, we added a dataset for the elevated plus maze, in which the mouse interacts with the maze, exhibiting stretches and dips over the side of the maze. Third, we added a social interaction dataset with two mice that specifically tests the model’s performance on social behaviors. Fourth, we added a dataset for the forced swim test in which the mouse is swimming in a small pool. These datasets therefore have more complex backgrounds (e.g. bedding, objects, multiple mice), dynamic backgrounds (moving water in the forced swim test), and social interactions. Unfortunately, we did not have access to a dataset with cables, such as for optogenetics or physiology recordings, but we note that the Mouse-Homecage dataset includes occluders. We now have nine datasets in total, and DeepEthogram performed well across all these datasets.</p><p>We feel that the datasets we used are representative of videos that are common in many fields. For example, the open field assay and elevated plus maze are common assays that are typical in behavior core facilities, biotech/pharma companies, and many academic research labs. A Pubmed search for “open field test” or “elevated plus maze” each returns over 2500 papers in the past five years. These types of videos are not necessarily common in the fields of neurophysiology or systems neuroscience, but they are commonly used for phenotyping mutant mice, tests of mouse models of disease, and treatments of disease. However, we agree entirely that we have not tested all common types of behavior videos, including some mentioned by the reviewer. Unfortunately, each dataset takes a long time to test due to extensive labeling by humans for “ground-truth” data, and we did not have access to all types of datasets. In the revision, we made a sincere and substantial effort to extend the datasets tested. In addition, we have added more discussion of the cases in which we expect DeepEthogram to work well and the cases in which we expect it will be worse than other methods. We hope this discussion points readers to the best method for their specific use case.</p><disp-quote content-type="editor-comment"><p>The authors also tout the time saving benefits of using deep ethogram. However, with their best performing implementation (DEG slow), with a state of the art computer, with a small video (256 x 256 pixels, width by height), the software runs at 15 frames per second (nearly 1/2 the speed of the raw video). My intuition is that this is on the slow side, given that many behaviors can be scored by human observers in near real time if the observer is using anything but a stopwatch. It would be nice to see benchmarks on larger videos that more accurately reflect the range of acquisition frames. If it is necessary for users to dramatically downsample videos, this should be made clear.</p></disp-quote><p>We have now clarified in the text that the time savings are in terms of person-hours instead of actual time. A significant advantage is that DeepEthogram can run in the background on a computer with little to no human time (after it is trained), whereas manual labeling of each video continues to require human time with each video. In the case of the DeepEthogram-slow model, it is true that humans might be able to do it faster, but this would take human time, whereas the model can run while the researchers do other things. We think this is a critical difference and have highlighted this difference more clearly. Furthermore, we have made engineering improvements that have sped up inference time by nearly 300%.</p><p>We have added a table that measures inference time across the range of image sizes we used in this study. We note that all comparable methods in temporal action localization down-sample videos in resolution. For example, video classification networks commonly use 224 x 224 pixels as input. DeepEthogram works similarly for longer acquisitions (more frames per video). We had access to videos collected for specific neuroscience research questions and were restricted to these acquisition times, but the software will work similarly with all acquisition times.</p><disp-quote content-type="editor-comment"><p>Specific comments:</p><p>– It would be nice to see if DeepEthogram is capable of accurately scoring a behavior across a range of backgrounds. For example, if the model is trained on a sideview recording of an animal grooming in its cage, can it accurately score an animal in an open field doing the same from an overhead view, or a side view? If the authors provided guidance on such issues to the reader this would be helpful.</p></disp-quote><p>We have tested DeepEthogram in a range of different camera angles and backgrounds across the nine datasets we now include. Our results show that the model can work across this diversity of set ups. However, an important point is that it is unlikely that the model trained on a side view, for example, will work for videos recorded from above. Instead, the user would be required to label the top-view and side-view videos separately and train different models for these cases. We do not think this is a major limitation because often experimenters decide on a camera location and use that for an entire set of experiments. However, this is an important point for the user to understand. We have therefore added a sentence to the discussion describing this point, along with extended points about the cases in which DeepEthogram is expected to excel and when it might not work well.</p><disp-quote content-type="editor-comment"><p>– The authors should highlight that human scoring greatly outperforms DEG on a range of behaviors when comparing the individual F1 scores in Figure 3. Why aren't there any statistics for these comparisons?</p></disp-quote><p>Thank you for this recommendation. We have added statistics for comparing the F1 scores in Figure 3. These analyses reveal that human scoring outperforms DeepEthogram on some behaviors, but the performance of DeepEthogram and humans is statistically indistinguishable on the majority of behaviors tested. We have revised the main text to make it clear that human labelers do better than DeepEthogram in some cases.</p><p>We have also compared DeepEthogram to human performance on other measures of behavior, in particular the statistics of behavior bouts (duration and frequency). We expect that these metrics may be more commonly used by the end-user. In the case of bout statistics, DeepEthogram’s performance is worse than human performance at the level of single videos (Figure 5A-B). However, when averaged across videos, the performance of the model and humans is statistically indistinguishable (Figure 5C-E). The reason appears to be that the model’s predictions are more variable on a single-video level, but after averaging across videos, the noise is averaged out to reveal a similar mean to what is obtained by human labelers. These results can also be seen in Figure 4A-C, in which the difference between the bout statistics for the model and labels on which it was trained (Human 1) are similar in magnitude to the difference between human labelers.</p><p>We thank the reviewer again for this suggestion. The statistics along with portions of Figure 4-5 are new in response to this suggestion.</p><disp-quote content-type="editor-comment"><p>– Some of the F1 scores for individual behaviors look very low (~0.5). It would be nice to know what chance performance is in these situations and if the software is performing above chance.</p></disp-quote><p>We thank the reviewer for the suggestion to add chance level performance. We have added chance-level F1 scores by performing a shuffle of the actual human labels relative to the video frames. For nearly all behaviors, DeepEthogram’s performance is significantly higher than chance level performance by a substantial margin. We think the addition of the chance level performance helps to interpret the F1 scores, which are not always intuitive to understand in terms of how good or bad they are. We also note that the F1 score is a demanding metric. For example, if the model correctly predicts the presence of a grooming bout, but misses the onset and offset times by several frames, the F1 score will be substantially reduced. For this reason, we also report the bout statistics in Figures 4-5, which are closer to the metrics that end users might want to report and are easier to interpret.</p><disp-quote content-type="editor-comment"><p>– I find it hard to understand the size of the data sets used in the analyses. For instance, what is 'one split of the data', referenced in Figure 3? Moreover, the authors state &quot;We selected 1, 2, 4, 8, 12, or 16 random videos for training and used the remaining videos for evaluation&quot; I have no idea what this means. What is the length and fps of the video?</p></disp-quote><p>We thank the reviewer for this comment and agree that it was not clear in the original submission. We have updated the text and figure legends to clarify this point. A split of the data refers to a random splitting of the data into training, validation, and testing sets. We now describe this in the procedures for the model set up in the main text and clarify the meaning of “splits” when it is used later in the Results section. Videos refer to a single recorded behavior video. Videos can differ in terms of their duration and frame number. However, it is a commonly used division of the data for experimental recordings. We therefore include two sets of plots in Figure 6. One is based on the number of videos, and the other is based on the number of frames with positive examples of the behavior. The former might be more intuitive to some readers because it is a common division of the data, but it might be challenging to understand given that videos can differ in duration and frame rate. Instead, reporting the number of positive examples provides an exact metric that can be applied to any case once one calculates the frame rate and frequency of their behavior of interest. We have now included text that helps readers with this conversion.</p><disp-quote content-type="editor-comment"><p>– Are overall F1 scores in Figure 3 computed as the mean of the individual scores on each component F1 score, or the combination of all behaviors (such that it weights high frequency behaviors)? It's also difficult to understand what the individual points in Figure 4 (a-c) correspond to.</p></disp-quote><p>We have clarified the meaning of these values in text and figure legends. The overall F1 score (Figure 3A-B) reports the “combination” of all behaviors. We report each behavior individually below (Figure 3C-K). In the old Figure 4A-C, each dot represented a single behavior (e.g. “face grooming”). We have clarified the meaning of each marker in each figure in the legends.</p><disp-quote content-type="editor-comment"><p>– The use of the names Mouse-1, Mouse-2 etc for experiments are confusing because it can appear that these experiments are only looking at single mice. I would change the nomenclature to highlight that these reflect experiments with multiple mice.</p></disp-quote><p>We thank the reviewer for this suggestion. We have now updated the naming of each dataset to provide a more descriptive title.</p><disp-quote content-type="editor-comment"><p>– It is not clear why the image has to be averaged across RGB channels and then replicated 20 times for the spatial stream. The author mentioned &quot;To leverage ImageNet weights with this new number of channels&quot;, and I assume this means the input to the spatial stream has to have same shape (number of weights) as the input to the flow stream. However why this is the case is not clear, especially considering two feature extractor networks are independently trained for spatial and flow streams. Lastly this might raise the question of whether there will be valuable information in the RGB channels separately that will be lost from the averaging operation (for example, certain part of an animal's body has different color than others but is equal-luminous).</p></disp-quote><p>This averaging is only performed once, when loading ImageNet weights into our models. This occurs before training begins. The user never needs to do this because they will use our models that are pretrained on Kinetics700, rather than ImageNet.</p><p>For clarification, the ImageNet weights have 3 channels; red, green, and blue. Our flow classifier model has 20 channels; δ-X and δ-Y each for 10 frames. Therefore, we average across Red, Green, and Blue channels in the ImageNet weights and replicate this value 20 times. However, these weights are then changed by the training procedure, so there is no information lost due to averaging. We have updated the text to clarify this.</p><disp-quote content-type="editor-comment"><p>– It is not intuitive why simple average pooling is sufficient for fusing the spatial and flow streams. It can be speculated that classification of certain behavior will benefit much more from optical flow features while other behaviors benefits from still image features. I'm curious to see whether an additional layer at the fusing stage that has behavior-specific weights could improve performance.</p></disp-quote><p>We thank the reviewer for this interesting suggestion. In theory, this weighting of image features and flow features could happen implicitly in the fully connected layer prior to average pooling. Specifically, the biases on the units in the fully connected layer are per-behavior parameters for both the spatial and the flow stream that will affect which one is weighted more highly in the average fusion. We have also experimented with concatenation pooling, in which we fuse the two feature vectors together before one fully connected layer. However, this adds parameters and did not improve performance in our experiments. We have not experimented with more elaborate fusion schemes.</p><disp-quote content-type="editor-comment"><p>– Since computational demands is one of the major concern in this article, I'm wondering whether exploiting the sparse nature of the input images would further improve the performance of the algorithm. Often times the animal of interests only occupies a small number of pixels in the raw images, and some simple thresholding of the images, or even user-defined masking of the images, together with use of sparse data backends and operations should in theory significantly reduce the computational demands for both the spatial and flow feature extractor networks.</p></disp-quote><p>This is an interesting and insightful comment. For most of our datasets, the relevant information is indeed sparse, and the models could be made faster and perhaps more accurate if we could focus on the animal themselves. The difficulty is in developing a method that automatically focuses on the relevant pixels across datasets. For some datasets, thresholding would suffice, but for others more elaborate strategies would be required. Because we wanted DeepEthogram to be a general-purpose solution without requiring coding by the end user, we did not adopt any of these strategies. To make a general-purpose solution to focus on the relevant pixels, we could make a second version of DeepEthogram that implements models for spatiotemporal action detection, in which the user specifies one or more bounding boxes, each with their own behavior label. However, this would be a significant extension that we did not perform here.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The paper by Bohnslav et al., presents a software tool that integrates a supervised machine learning algorithm for detecting and quantifying behavior directly from raw video input. The manuscript is well-written, the results are clear. Strengths and weaknesses of the approach are discussed and the work is appropriately placed in the bigger context of ongoing research in the field. The algorithms demonstrate high performance and reach human-level accuracy for behavior recognition. The classifiers are embedded in an excellent user-friendly interface that eliminates the need of any programming skills on the end of the user. Labeled datasets can even be imported. We suggest additional analyses to strengthen the manuscript.</p></disp-quote><p>We thank the reviewer for this positive feedback and for their constructive suggestions.</p><disp-quote content-type="editor-comment"><p>Concerns:</p><p>1) Although the presented metrics for accuracy and F1 are state of the art it would be useful to also report absolute numbers for some of the scored behaviors for each trial, because most behavioral neuroscience studies actually report behavior in absolute numbers and/or duration of individual behaviors (rears, face grooms, etc.). Correlation of human and DEG data should also be presented on this level. This will speak to many readers more directly than the accuracy and F1 statistics. For this, we would like to see a leave-one-out cross-validation or a k-fold cross-validation (ensure that each trial ends up exactly once in a cross validation set) that enables a final per-trial readout. This can be done with only one of the DEG types (e.g &quot;fast&quot;). The current randomization approach of 60/20/20% (train/validate/test) with a n of 3 repeats is insufficient, since it a) allows per-trial data for at most 60% of all files and b) is susceptible to artefacts due to random splits (i.e one abnormal trial can be over or under represented in the cross validation sets).</p></disp-quote><p>We thank the reviewer for these suggestions. We have added multiple new analyses and plots that focus on the types of data mentioned. In the new Figure 4A-C, we show measures of bout statistics, with dots reporting values for individual videos in the test set. In addition, in Figure 5A-B and Figure 5F-H, we show measures of bout statistics benchmarked against human-human agreement. We agree with the reviewer that these bout statistics are likely more interpretable to most readers and likely closer to the metrics that end users will report and care about in their own studies. The remainders of Figures 4 and 5 provide summaries of these bout statistics for each behavior in each dataset. In these summaries, each dot represents a single behavior with values averaged across train/validation/test splits of the data, corresponding to a measure similar to averages across videos. We used these summaries to report the large number of comparisons in a compact manner.</p><p>We did not employ leave-one-out cross validation because we base our model’s thresholds and learning rate changes on the validation set. We therefore require three portions of the data. K-fold cross-validation is more complex with a split into three groups, and we also require that each of the train, validation, and test sets contains at least one example from each behavior, which might not happen with K-fold cross validation. However, to address your concerns, we repeated the random splitting 5 times for all datasets (except Sturman-EPM, which only has 3 videos with all behaviors). We found that empirically 68% of all videos are represented at least once in the test set. While this does add noise to our estimates of performance, it is not expected to add bias.</p><disp-quote content-type="editor-comment"><p>2) In line with comment 1) we propose to update Figure 4, which at the moment uses summed up data from multiple trials. We would rather like to see each trial represented by a single data-point in this figure (#bouts/#frames by behavior). As alternative to individual scatterplots, correlation-matrix-heatmaps could be used to compare different raters.</p></disp-quote><p>In response to this suggestion, we have completely revised and extended the old Figure 4, dividing it into two separate main figures (Figures 4 and 5). In Figure 4, we now report bout statistics first on an individual video basis (Figure 4A-C) and then averaged across train/validation/test splits of the data for each behavior as a summary (Figure 4D-F). In the new Figure 5, we benchmark the performance of the model on bout statistics by comparison to human performance for the datasets for which we have multiple human labelers. In the new Figure 5A-B, we examined this benchmarking on the basis of single videos. Then in Figure 5C-E, we report the averages across data splits for each behavior as a summary across all datasets and all behaviors. This analysis led to an interesting finding. On an individual video basis, humans are more accurate than DeepEthogram (Figure 5A-B). However, on averages for each behavior, the accuracy of the model and humans is statistically indistinguishable (Figure 5C-E). This occurs because the model predictions are noisier than humans on single videos, but when averaging across multiple videos, this noise is averaged out, resulting in similar values between the model and humans.</p><disp-quote content-type="editor-comment"><p>3) Direct benchmarking against existing datasets is necessary. With many algorithms being published these days, it is important to pick additional (published) datasets and test how well the classifiers perform on those videos. Their software package already allows import of labeled datasets, some are available online. For example, how well can DeepEthogram score…</p><p>a. grooming in comparison to Hsu and Yttri (REF #17) or van den Boom et al., (2017, J Neurosci Methods)</p><p>b. rearing in comparison to Sturman et al., (REF #21)</p><p>c. social interactions compared to (Segalin et al., (REF #7) or Nilsson et al., (REF #19))</p></disp-quote><p>We thank the reviewer for this recommendation. In response, we have done two things. First, we have tested DeepEthogram on publicly available datasets, in particular from Sturman et al. We greatly appreciate that Sturman et al., made their datasets and labels publicly available. We plan to do the same upon publication of this paper. This allowed us to confirm that DeepEthogram performs well on other datasets collected by different groups.</p><p>Second, we have benchmarked our method against two other approaches. First, we tried the method of Hsu and Yttri (B-SOiD) on the Mouse-Openfield dataset (Supplementary Figure 21). Specifically, we performed DeepLabCut to identify keypoints and then used B-SOiD on these keypoints to separate behaviors into clusters. The method worked well and created separable clusters in a low dimensional behavior space. We then tried to line up these clusters with researcher-defined behaviors of interest. However, it was challenging to find a good correspondence. As a result, DeepEthogram substantially outperformed this B-SOiD-based method. We think this is not surprising given that B-SOiD is designed for unsupervised behavior classification, whereas the problem we are trying to solve is supervised classification. While it is possible that clusters emerging from unsupervised methods may line up nicely with researcher-defined behaviors of interest, this does not have to happen. It therefore makes sense to utilize a supervised method, like DeepEthogram, for the problem of identifying pre-defined behaviors of interest. We now discuss this point and present these results in depth in the text.</p><p>We also compared the performance of DeepEthogram and the ETH DLC-Analyzer method from Sturman et al. Specifically, we used DeepLabCut to track body parts in our Mouse-Openfield dataset and then used the architecture from Sturman et al. to classify behaviors (the multilayer perceptron). Note that we had to re-implement the code for the Sturman et al. approach for two reasons. First, our datasets can have more than one behavior per timepoint, which necessitates different activation and loss functions. Second, we wanted to ensure the same train/validation/test splits were used for both DeepEthogram and the ETH DLC-Analyzer classification architecture (the multilayer perceptron). We also included several training tricks in this architecture based on things we had learned while developing DeepEthogram, aiming to make this the fairest comparison possible. We found that DeepEthogram had significantly better performance in terms of accuracy and F1 on some behaviors in this dataset. The accuracy of the bout statistics was similar between the two modeling approaches. We conclude that DeepEthogram can perform better than other methods, at least in some cases and on some metrics, and hope that these new benchmarking comparisons help lend credibility to the software.</p><p>We did not perform further benchmarking against other approaches. Such benchmarking takes substantial effort because it takes a lot of work to set up and validate each method. We agree with the reviewer that it would be valuable to have more extensive comparisons of methods, but in terms of timing and the multiple directions asked for by the reviewers (e.g. also new datasets), it was not feasible to try more methods. In addition, not all the datasets mentioned were available when we checked. For example, last we checked, the dataset for Segalin et al., is not publicly available, and the authors told us they would release the data when the paper is published. Also, to our knowledge, the method of Segalin et al., is specific to social interactions between a white and a black mouse, whereas the social interaction dataset that is available to us has two black mice.</p><disp-quote content-type="editor-comment"><p>4) In the discussion on page 19 the authors state: &quot;Subsequently, tens to hundreds to thousands of movies could be analyzed, across projects and labs, without additional user-time, which would normally cost additionally hundreds to thousands of hours of time from researchers.&quot; This sentence suggests that a network trained on the e.g. the open field test in one lab can be transferred across labs. This key issue of &quot;model transferability&quot; should be tested. E.g. the authors could use the classifier from mouse#3 and test is on another available top-view recording dataset recorded in a different lab with different open-field setup (datasets are available online, e.g. REF #21).</p></disp-quote><p>The original intent of this comment was that videos with similar recording conditions could be tested, such as in behavior core facilities with standardized set ups. We ran a simple test of generalization by training DeepEthogram on our Mouse-Openfield dataset and testing it on the Sturman-OFT dataset. We found that the model did not generalize well and speculate that this could be due to differences in resolution, background, or contrast. We have therefore removed this claim and added a note that DeepEthogram likely needs to be retrained if major aspects of the recordings change.</p><disp-quote content-type="editor-comment"><p>5) Figure 5D/E: Trendline is questionable, we would advise to fit a sigmoid trendline, not an arbitrarily high order polynomial. Linear trend lines (such as shown in Figure 4) should include R<sup>2</sup> values on the plot or in the legend.</p></disp-quote><p>We apologize for the lack of clarity in our descriptions. The lines mentioned are not trendlines. The line in the old Figure 5D-E is simply a moving average that is shown to aid visualization. Also, the lines in the scatterplots of the old Figure 4 are not linear trend lines; rather, they are unity lines to help the reader see if the points fall above or below unity. We now explain the meaning of these lines in the figure legends. We have also added correlation coefficients to the relevant plots.</p><disp-quote content-type="editor-comment"><p>6) In the discussion, the authors do a very good job highlighting the limitations and advantages of their approach. The following limitations should however be expanded:</p><p>a. pose-estimation-based approaches (e.g. DLC) are going to be able to track multiple animals at the same time (thus allowing e.g. better read-outs of social interaction). It seems this feature cannot be incorporated in DeepEthogram.</p></disp-quote><p>We thank the reviewer for noting our attempts to provide a balanced assessment of DeepEthogram, including its limitations. We have now expanded the discussion to even more clearly mention the cases in which DeepEthogram might not work well. We specifically mention that DeepEthogram cannot track multiple animals in its current implementation. We hope the new discussion material will help readers to identify when and when not to use DeepEthogram and when other methods might be preferable.</p><disp-quote content-type="editor-comment"><p>b. Having only 2 human raters is a weakness that should briefly be addressed. Triplicates are useful for assessing outlier values, this could be mentioned in light of the fact that the F1 score of DeepEthogram occasionally outperforms the human raters (e.g. Figure 3C,E).</p></disp-quote><p>We have added a third labeler for our Mouse-Ventral1 dataset. We have also included results from Sturman et al., each of which have 3 human labelers. Unfortunately, we were not able to add a third labeler for all datasets. For those in which we included a third labeler, the results are similar to using only two labelers. We think this is because all of our labelers are experts who spent a lot of time carefully labeling videos.</p><disp-quote content-type="editor-comment"><p>c. Traditional tracking measures such as time in zone, distance moved and velocity cannot be extracted with this approach. These parameters are still very informative and require a separate analysis with different tools (creating additional work).</p></disp-quote><p>We agree these measures are informative and not currently output from DeepEthogram. We have added a sentence in the discussion noting that DeepEthogram does not provide tracking details.</p><disp-quote content-type="editor-comment"><p>d. The authors are correct that the additional time required for behavior analysis (due to the computationally demanding algorithms) is irrelevant for most labs. However, they should add (1) that the current system will not be able to perform behavior recognition in real time (thus preventing the use of closed-loop systems, which packages such as DLC have made possible) and (2) that the speed they discuss on page 16 is based on an advanced computer system (GPU, RAM) and will not be possible with a standard lab computer (or provide an estimate how long training would require if it is possible).</p></disp-quote><p>We have added sentences in the discussion and results that state that the DeepEthogram is not suitable for closed-loop experiments and that the speed of inference depends on the computer system and might be substantially slower for typical desktop computers. We have also updated the inference evaluation by using a 1080Ti GPU, which is well within the budget of most labs. Furthermore, during the revision process, we significantly improved inference speed, so that DeepEthogram-fast runs inference at more than 150 frames per second on a 1080Ti GPU.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The reviewers all felt the manuscript was improved, and thank the authors for the additional datasets and analysis. We would just like to see two items before the publication is accepted fully.</p><p>(1) Both reviewer #1 and #2 note the new data is great, but lacks human ground truth. Both for comparison, and releasing the data for others to benchmark on, it would be please include the data. We also understand that obtaining ground truth from 3 persons is a large time commitment, but even if there is one person, this data should be included for all datasets shown in Figure 3.</p></disp-quote><p>We have performed addition labeling to get human levels of performance for benchmarking. For eight datasets, we now have labels from multiple researchers, which allows calculation of human-level performance. We have updated all the relevant figures, e.g. Figure 3. The results are consistent with those previously reported, in which DeepEthogram’s performance is similar to human-level performance in general. The only dataset for which we did not add another set of human labels is the Fly dataset. This dataset has more than 3 million video frames, and the initial labeling took months of work to label. Note that this dataset still has one set of human labels, which we use to train and test the model, but we are unable to plot a human-level of performance with only one set of human labels. We feel that comparing DeepEthogram’s performance to human-level performance for 8 datasets is sufficient and goes beyond, or is at least comparable to, the number of datasets typically reported in papers of this type.</p><disp-quote content-type="editor-comment"><p>(2) Please include links for the raw videos used in this work; it is essential for others to benchmark and use to validate the algorithm presented here (see Reviewer 3: &quot;raw videos used in this work (except the ones added during the revision) are – it appears – not accessible online&quot;).</p></disp-quote><p>We have posted a link to the raw videos and the human labels on the project website (https://github.com/jbohnslav/deepethogram). We have noted in the paper’s main text and methods that the videos and annotations are publicly available at the project website.</p><disp-quote content-type="editor-comment"><p>Lastly, reviewer 3 notes that perhaps, still, some use-cases are best suited for DeepEthogram, while others more for pose-estimation plus other tools, but this of course cannot be exhaustively demonstrated here; at your discretion you might want to address in the discussion, but we leave that up to your judgement.</p></disp-quote><p>We agree with Reviewer 3 on this point. After reviewing the manuscript text, we feel that we have already included a balanced and extensive discussion of the use-cases that are best suited for DeepEthogram versus pose estimation methods. We feel that the relevant points are mentioned and, for brevity, do not feel it would be helpful to add even more explanations as these points are already addressed extensively in both the Results section and the Discussion section.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>I thank the authors for the revisions and clarifications, and I think the manuscript is much improved. Plus, the new datasets and comparisons to B-iOD and R-Analyzer (Sturman) are a good additions.</p></disp-quote><p>Thank you for the positive feedback and the constructive comments. Your input has greatly improved the manuscript.</p><disp-quote content-type="editor-comment"><p>One note is that is not clear which datasets have ground truth data; namely, in the results 5 datasets they use for testing are introduced:</p><p>&quot;Mouse-Ventral1&quot;</p><p>&quot;Mouse-Ventral2&quot;</p><p>&quot;Mouse-Openfield&quot;</p><p>&quot;Mouse-Homecage&quot;</p><p>&quot;Mouse-Social&quot;</p><p>plus three datasets from published work by Sturman et al.,</p><p>and &quot;fly&quot;</p><p>Then it states that all datasets were labeled; yet, Figure 3 has no ground truth for &quot;Mouse-Ventral2&quot; , &quot;Mouse-Homecage&quot; , &quot;Mouse-Social&quot; or 'Fly&quot; -- please correct and include the ground truth. I do see that is says that only a subset of each of the 2 datasets in Figure 3 are labeled with 3 humans, but minimally then the rest (1 human?) should be included in Figure 3 (and be made open source for future benchmarking).</p><p>It appears from the discussion this was done (i.e., at least 1 human, as this is of course required for the supervised algorithm too):</p><p>&quot;In our hands, it took approximately 1-3 hours for an expert researcher to label five behaviors in a ten-minute movie from the Mouse-Openfield dataset&quot; and it appears that labeling is defined in the methods.</p></disp-quote><p>We apologize for the confusion on this point. All datasets had at least one set of human labels because this is required to train and test the model. To obtain a benchmark of human-level performance, multiple human labels are required so that the labels of one researcher can be compared to the labels of a second researcher. We have now performed additional annotation. Now 8 datasets have human-level performance reported (see updated Figure 3). The results show that the model achieves human-level performance in many cases.</p><p>The only dataset for which we do not have multiple human labelers is the Fly dataset. The Fly dataset has more than 3 million timepoints, and it took a graduate student over a month of full-time work to obtain the first set of labels. Given the size of this dataset, we did not add another set of labels.</p><p>We feel that having 8 datasets with human-level performance exceeds, or at least matches, what is typical for benchmarking of methods similar to DeepEthogram. We feel this number of datasets is sufficient to give the reader a good understanding of the model’s performance.</p><p>We have now made all the datasets and human labels available on the project website and hope they will be of use to other researchers.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The authors did a great job addressing our comments, especially with the additional validation work. My only concern is that some of the newly included datasets don't have human-labeled performance for comparison, hence making it hard to judge the actual performance of DeepEthogram. While I understand it is very time-consuming to obtain human labels, I think it will greatly improve the impact of the work if the model comparison can be bench-marked against ground truth. Especially it would be great to see the comparison to human label for the &quot;Mouse-Social&quot; and &quot;Mouse-Homecage&quot; datasets, which presumably represent a large proportion of use cases for DeepEthogram. Otherwise I think it looks good and I would support publication of this manuscript.</p></disp-quote><p>Thank you for the positive feedback and the valuable suggestions that helped us to extend and improve the paper. We have now added more human labels to obtain human-level performance for 8 datasets, including the Mouse-Social and Mouse-Homecage datasets. We have made the datasets and human labels publicly available at the project website, which we hope will assist other researchers.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The authors present a software solution (DeepEthogram) that performs supervised machine-learning analysis of behavior directly from raw videos files. DeepEthogram comes with a graphical user interface and performs behavior identification and quantification with high accuracy, requires modest amounts of pre-labeled training data, and demands manageable computational resources. It promises to be a versatile addition to the ever-growing compendium of open-source behavior analysis platforms and presents an interesting alternative to pose-estimation-based approaches for supervised behavior classification, under certain conditions.</p><p>The authors have generated a large amount of additional data and showcase the power of their approach in a wide variety of datasets including their own data as well as published datasets. DeepEthogram is clearly a powerful tool and the authors do an excellent job describing the advantages and disadvantages of their system and provide a nuanced comparison of point-tracking analyses vs. analyses based on raw videos (pixel data). Also their responses to the reviewers comments are very detailed, thoughtful and clear. The only major issue is that the raw videos used in this work (except the ones added during the revision) are – it appears – not accessible online. This problem must be solved, the videos are essential for reproducibility.</p></disp-quote><p>We thank the reviewer for the positive comments and constructive feedback that has greatly helped the paper. We have now made all the videos and human annotations available on the projection website: https://github.com/jbohnslav/deepethogram</p><disp-quote content-type="editor-comment"><p>A minor caveat is that in order to compare DeepEthogram to existing supervised and unsupervised approaches, the authors have slightly skewed the odds in their favor by picking conditions that benefit their own algorithm. In the comparison with point-tracking data they use a low resolution top-view recording to label the paws of mice (which are obstructed most of the time from this angle). In the comparison with unsupervised clustering, they use the unsupervised approach for an application that it isn't really designed for (performed in response to reviewers requests). But the authors directly address these points in the text, and the comparisons are still valid and interesting and address the reviewers concerns.</p></disp-quote><p>We agree with these points. In particular, we agree that the comparison of supervised and unsupervised methods is challenging and not an apples-to-apples comparison, but we included this comparison because other reviewers felt strongly that it was a necessary analysis for benchmarking the model’s performance. We include text that highlights the caveats mentioned by the reviewer and have tried to provide a thorough and balanced assessment of cases in which DeepEthogram might work best and cases in which other approaches may be preferred and will provide additional information. In the head-to-head comparisons, we have highlighted the caveats of the comparisons, including those mentioned by the reviewer. Our goal is for the reader to understand when and when not to use DeepEthogram and what type of information one can obtain from the software’s output. We hope our extensive discussion of these points will help the reader and the community.</p></body></sub-article></article>