<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">70777</article-id><article-id pub-id-type="doi">10.7554/eLife.70777</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Expansion and contraction of resource allocation in sensory bottlenecks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-241858"><name><surname>Edmondson</surname><given-names>Laura R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9886-1121</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-268692"><name><surname>Jiménez Rodríguez</surname><given-names>Alejandro</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-176293"><name><surname>Saal</surname><given-names>Hannes P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7544-0196</contrib-id><email>h.saal@sheffield.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Active Touch Laboratory, Department of Psychology, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Sheffield Robotics, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Department of Computer Science, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e70777</elocation-id><history><date date-type="received" iso-8601-date="2021-05-28"><day>28</day><month>05</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-07-29"><day>29</day><month>07</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-05-27"><day>27</day><month>05</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.05.26.445857"/></event></pub-history><permissions><copyright-statement>© 2022, Edmondson et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Edmondson et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-70777-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-70777-figures-v2.pdf"/><abstract><p>Topographic sensory representations often do not scale proportionally to the size of their input regions, with some expanded and others contracted. In vision, the foveal representation is magnified cortically, as are the fingertips in touch. What principles drive this allocation, and how should receptor density, for example, the high innervation of the fovea or the fingertips, and stimulus statistics, for example, the higher contact frequencies on the fingertips, contribute? Building on work in efficient coding, we address this problem using linear models that optimally decorrelate the sensory signals. We introduce a sensory bottleneck to impose constraints on resource allocation and derive the optimal neural allocation. We find that bottleneck width is a crucial factor in resource allocation, inducing either expansion or contraction. Both receptor density and stimulus statistics affect allocation and jointly determine convergence for wider bottlenecks. Furthermore, we show a close match between the predicted and empirical cortical allocations in a well-studied model system, the star-nosed mole. Overall, our results suggest that the strength of cortical magnification depends on resource limits.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>efficient coding</kwd><kwd>decorrelation</kwd><kwd>sensory magnification</kwd><kwd>receptor density</kwd><kwd>stimulus statistics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>209998/Z/17/Z</award-id><principal-award-recipient><name><surname>Saal</surname><given-names>Hannes P</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>HBP-SGA2 785907</award-id><principal-award-recipient><name><surname>Jiménez Rodríguez</surname><given-names>Alejandro</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A simple efficient coding model predicts complex trade-offs in resource allocation for sensory inputs with heterogeneous receptor densities and activation levels.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In many sensory systems, receptors are arranged spatially on a sensory sheet. The distribution of receptors is typically not uniform, but instead, densities can vary considerably. For example, in vision, cones are an order of magnitude more dense in the fovea than in the periphery (<xref ref-type="bibr" rid="bib43">Goodchild et al., 1996</xref>; <xref ref-type="bibr" rid="bib92">Wells-Gray et al., 2016</xref>). In the somatosensory system, mechanoreceptors are denser in the fingertips than the rest of the hand (<xref ref-type="bibr" rid="bib54">Johansson and Vallbo, 1979</xref>). Alongside the density of receptors, the statistics of the input stimuli can also vary. For example, the fingertips are much more likely to make contact with objects than the palm (<xref ref-type="bibr" rid="bib42">Gonzalez et al., 2014</xref>). Subsequent sensory areas are typically arranged topographically, such that neighbouring neurons map to nearby sensory input regions, for example, retinotopy in vision and somatotopy in touch. However, the size of individual cortical regions is often not proportional to the true physical size of the respective sensory input regions and, instead, representations might expand (often called magnification) or contract. For example, both the fovea and the fingertips exhibit expanded representations in early visual and somatosensory cortices, respectively, compared to their physical size (<xref ref-type="bibr" rid="bib6">Azzopardi and Cowey, 1993</xref>; <xref ref-type="bibr" rid="bib34">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="bib78">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="bib62">Martuzzi et al., 2014</xref>). What determines this cortical magnification? For somatotopy, it has been proposed that cortical topography might directly reflect the density of sensory receptors (<xref ref-type="bibr" rid="bib13">Catani, 2017</xref>). On the other hand, receptor density alone is a poor predictor of magnification (<xref ref-type="bibr" rid="bib23">Corniani and Saal, 2020</xref>) and work on plasticity has established that cortical regions can expand and contract dynamically depending on their usage, suggesting that expansion and contraction might be driven by the statistics of the sensory stimuli themselves (<xref ref-type="bibr" rid="bib22">Coq and Xerri, 1998</xref>; <xref ref-type="bibr" rid="bib63">Merzenich and Jenkins, 1993</xref>; <xref ref-type="bibr" rid="bib94">Xerri et al., 1996</xref>).</p><p>Here, we tackle this problem from a normative viewpoint, employing efficient coding theory, which has been widely used to model and predict sensory processing. Efficient coding theory includes a number of different approaches such as sparse coding, redundancy reduction and predictive coding, depending on the constraints of the proposed problem (<xref ref-type="bibr" rid="bib19">Chalk et al., 2018</xref>). Here, we focus on efficient coding via redundancy reduction (<xref ref-type="bibr" rid="bib7">Barlow, 1961</xref>), which suggests that neural populations are tuned to maximize the information present in the sensory input signals by removing redundant information (<xref ref-type="bibr" rid="bib3">Atick, 1992</xref>; <xref ref-type="bibr" rid="bib2">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="bib5">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib46">Graham and Field, 2009</xref>; <xref ref-type="bibr" rid="bib20">Chechik et al., 2006</xref>). Efficient coding models have been most prominent in vision (<xref ref-type="bibr" rid="bib57">Kersten, 1987</xref>; <xref ref-type="bibr" rid="bib56">Karklin and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib3">Atick, 1992</xref>; <xref ref-type="bibr" rid="bib2">Atick and Redlich, 1990</xref>; <xref ref-type="bibr" rid="bib27">Doi et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib66">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib67">Olshausen and Field, 2004</xref>; <xref ref-type="bibr" rid="bib8">Bell and Sejnowski, 1997</xref>) and audition (<xref ref-type="bibr" rid="bib82">Smith and Lewicki, 2006</xref>; <xref ref-type="bibr" rid="bib59">Lewicki, 2002</xref>). This prior work has mostly focused on predicting the response properties and receptive field structure of individual neurons. In contrast, here we ask how receptive fields—independent of their precise structure—should tile the sensory sheet when the receptors themselves differ in density and activation levels.</p><p>Some aspects of magnification in topographic representations have been qualitatively reproduced using self-organizing maps (<xref ref-type="bibr" rid="bib74">Ritter et al., 1992</xref>). However, these models generally lack a clear cost function and the magnification factor can be determined exactly only in rare cases, while a general expression is lacking (<xref ref-type="bibr" rid="bib73">Ritter and Schulten, 1986</xref>). Assuming a uniform density of output neurons, cortical maps may be optimizing for the spread of incoming information to be equally distributed over these neurons (<xref ref-type="bibr" rid="bib71">Plumbley, 1999</xref>).</p><p>In contrast to receptor density, there has been some work on how populations of neurons should encode non-uniform stimulus statistics using Fisher information (<xref ref-type="bibr" rid="bib37">Ganguli and Simoncelli, 2010</xref>; <xref ref-type="bibr" rid="bib38">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib39">Ganguli and Simoncelli, 2016</xref>; <xref ref-type="bibr" rid="bib97">Yerxa et al., 2020</xref>). This approach aims to approximate mutual information and is used to calculate optimal encoding in a neural population (<xref ref-type="bibr" rid="bib96">Yarrow et al., 2012</xref>) however, its use is restricted to specific conditions and assumptions (<xref ref-type="bibr" rid="bib9">Berens et al., 2011</xref>; <xref ref-type="bibr" rid="bib10">Bethge et al., 2002</xref>). Rather than receptive fields uniformly tiling the input space, the optimal population should be heterogeneous, with receptive fields placed more densely over high-probability inputs, at detriment to low-probability regions (<xref ref-type="bibr" rid="bib38">Ganguli and Simoncelli, 2014</xref>). Our approach differs from this prior work: rather than maximizing information between the neural population and the stimulus itself, we instead consider information between the neural population and an initial population of receptor neurons. This places a limit on the total amount of information that can be represented.</p><p>The need for information maximization is often motivated by resource constraints. These can take the form of an explicit bottleneck, where the number of receptor neurons is greater than the number of output neurons. This is the case in the early visual system, where photoreceptors in the retina are much more numerous than the retinal ganglion cells to which they project (<xref ref-type="bibr" rid="bib92">Wells-Gray et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Goodchild et al., 1996</xref>). Other sensory systems might lack such explicit bottlenecks, but still place limits on the amount of information that is represented at a higher-order processing stage.</p><p>How then should resource allocation change for different-sized bottlenecks, given varying densities of receptors and different stimulus statistics? Here, we derive optimal neural allocations for different bottlenecks, while systematically varying receptor density and stimulus statistics. A preliminary version of these results restricted to the effects of receptor density in a 1D space was previously presented as a conference paper (<xref ref-type="bibr" rid="bib32">Edmondson et al., 2019</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We restrict ourselves to linear models and only consider second-order statistics of the sensory signals, such that redundancy reduction simplifies to decorrelation (for examples from the visual literature, see <xref ref-type="bibr" rid="bib47">Hancock et al., 1992</xref>; <xref ref-type="bibr" rid="bib81">Simoncelli and Olshausen, 2001</xref>; <xref ref-type="bibr" rid="bib26">Doi and Lewicki, 2005</xref>). We also introduce a sensory bottleneck, such that the number of output neurons is smaller than the number of receptors. Specifically, <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf3"><mml:mi>m</mml:mi></mml:math></inline-formula>-dimensional output vector, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf5"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional input vector (with <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) containing sensory receptor responses, and <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> contains the receptive field. Assuming a low-noise regime, optimal decorrelation can be achieved if <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> takes the following form:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">Λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> contain the (top <inline-formula><mml:math id="inf11"><mml:mi>m</mml:mi></mml:math></inline-formula>) eigenvalues and eigenvectors, respectively, of the covariance matrix <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In the specific setup we consider here, receptors are placed in non-uniform densities across the sensory sheet, with some regions having a higher density than others. For example, in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, the digit tip is tiled more densely than the rest of the finger. The covariance of receptor activation decreases with distance (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Thus, directly neighbouring units in denser regions covary more strongly than those from less dense regions. The activation level of receptors can also vary between regions, which is modelled through scaling the covariance function (see Appendix 1 for further details). We initially focus on negative exponential covariance functions, for which the eigenvalues and the resulting allocation can be calculated analytically. The covariance between receptors <italic>x</italic><sub><italic>i</italic></sub> and <italic>x</italic><sub><italic>j</italic></sub> for two regions <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, differing in density and activation, can thus be expressed as<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>γ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Illustration of the resource allocation problem and solution outline.</title><p>(<bold>A</bold>) Abstract problem setup. When two regions vary in their receptor densities and activation, how should a shared resource be allocated between them when that resource is limited, as in a sensory ‘bottleneck’? The allocation between the two regions may vary depending on the width of the bottleneck, for example, for narrow bottlenecks it might be most efficient to allocate all resources to the higher density region 2. (<bold>B</bold>) Application of the problem to the tactile system. The density of touch receptors differs across regions of the hand (e.g. fingertip, shown in orange, versus finger phalanx, yellow). Different finger regions make contact with objects at different rates (dark blue versus light blue shading, darker colours indicating higher contact rates). (<bold>C</bold>) Sensory inputs are correlated according to a covariance function that decays with distance between receptors on the sensory sheet. This function is evaluated at different receptor distances depending on the density of sensory receptors (orange versus yellow dots at the bottom). Regions with higher probability of activation exhibit greater variance (dark versus light blue curves). (<bold>D</bold>) Decorrelation of sensory inputs in the presence of a bottleneck is achieved by calculating and retaining the eigenvectors and eigenvalues of the receptor covariance matrix. Here, this matrix is approximated as a block matrix, which allows calculating the eigenvalues for each region individually (dark blue versus light blue box). (<bold>E</bold>) The combined set of eigenvalues from all regions is then sorted; the region where each successive eigenvalue in the combined sorted set originates from determines where that output neuron’s receptive field will fall. (<bold>F</bold>) Counting which input regions successive eigenvalues belong to results in the allocation breakdown for different bottlenecks. For certain forms of the covariance function, this allocation can be calculated analytically.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig1-v2.tif"/></fig><p>where <inline-formula><mml:math id="inf13"><mml:mi>a</mml:mi></mml:math></inline-formula> denotes the receptor activation ratio and <inline-formula><mml:math id="inf14"><mml:mi>d</mml:mi></mml:math></inline-formula> the receptor density ratio between both regions. It can be seen that changing the density affects the decay of the covariance function and thereby ‘stretches’ the space, while changing the activation scales the function.</p><p>We approximate the regions as separate, such that no receptors are activated from both regions simultaneously. The covariance matrix across all receptors then forms a block matrix, where the covariance between separate regions is zero (<xref ref-type="fig" rid="fig1">Figure 1D</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, eigenvalues can be calculated separately from the covariance matrices of each region (inset panels in <xref ref-type="fig" rid="fig1">Figure 1D</xref>), and for negative exponential covariance functions, this can be done analytically (see ‘Methods’ for mathematical derivations):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>γ</mml:mi><mml:mi>a</mml:mi><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>o</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Finally, the resulting eigenvalues are ordered by magnitude for both regions combined (grey line in <xref ref-type="fig" rid="fig1">Figure 1E</xref>). A bottleneck is introduced by restricting the total number of output neurons. This is done by selecting from the combined set of ordered eigenvalues until the limit is reached. The proportion of eigenvalues originating from each input region determines its allocation for the chosen bottleneck width (see red dashed line in <xref ref-type="fig" rid="fig1">Figure 1E and F</xref>).</p><p>In the following, we will present results for the common case of 2D sensory sheets, while results for 1D receptor arrangements are summarized in Appendix 6. For ease of analysis, all cases discussed will assume two sensory input regions of equal size, which are differing in receptor density, input statistics, or both. In all our examples, receptors in the low-density (baseline) region are always spaced on a grid with a distance of 1.</p><sec id="s2-1"><title>Resource limits determine the amount of magnification</title><p>First, we investigated resource allocation in bottlenecks for heterogeneous density of receptors and heterogeneous stimulus statistics separately, whilst keeping the other factor uniform across the input regions.</p><sec id="s2-1-1"><title>Heterogeneous density</title><p>For two regions with different receptor densities, we found that the higher density region could either expand or contract relative to its input density, depending on the width of the bottleneck. Specifically, for narrow bottlenecks (smaller than approximately 10% of input neurons), the higher density region is either exclusively represented or its representation is expanded compared to a proportional density allocation (see example in <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Mathematically, this can be explained by a multiplicative scaling of the eigenvalue function for the higher density region (see illustration in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>). In contrast, for intermediate bottlenecks, low-density regions expand their representation, beyond what is expected for a proportional mapping (see dashed red line in <xref ref-type="fig" rid="fig2">Figure 2A</xref> denoting proportional allocation), leading to a contraction of the high-density region. For negative exponential covariance functions, this allocation converges to a fixed ratio at wider bottlenecks of <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf16"><mml:mi>d</mml:mi></mml:math></inline-formula> is the density ratio between both regions (dashed yellow line in <xref ref-type="fig" rid="fig2">Figure 2A</xref>; see ‘Methods’ for derivation), as neurons are allocated to either region at a fixed ratio (see inset in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>). Finally, for wide bottlenecks, all information arising from the low-density region has now been captured, and any additional output neurons will therefore be allocated to the high-density region only. The over-representation of the low-density region thus decays back to the original density ratio. The overall nonlinear effect of bottleneck width is present regardless of the ratio between the densities (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The spatial extent of the correlations over the sensory sheet (controlled by the decay constant γ in the covariance function, see ‘Methods’) determines allocation at narrow bottlenecks, and how fast the allocation converges, but does not affect the convergence limit itself (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). As γ increases, and therefore spatial correlations decrease, the convergence limit is approached only at increasingly wider bottlenecks. The extent of magnification thus heavily depends on the correlational structure of the stimuli for narrower bottlenecks, while receptor densities are more important for wider bottlenecks.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Optimal resource allocation for heterogeneous receptor densities or input statistics leads to complex trade-offs.</title><p>(<bold>A</bold>) Illustration of resource allocation for heterogeneous receptor density but homogeneous stimulus statistics over all bottleneck sizes. Orange denotes the lower density region and blue the higher density region, with a ratio of 1:3. Dotted lines show proportional representation according to receptor numbers (red) and convergence of the optimal allocation in the limit (yellow). Arrows indicate contraction (up) and expansion (down) of the higher density region representation. Inset demonstrates change in density between the regions. (<bold>B</bold>) Bottleneck allocation boundaries for different density ratios (given as low:high). The area below each line corresponds to the low-density representation, while the area above corresponds to the high-density representation, as in (<bold>A</bold>). (<bold>C</bold>) Effect of changing the extent of the spatial correlations (parameterized by the decay value γ, see ‘Methods’ for details and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for an illustration of the covariance function for different values of γ). Density ratio is set at 1:3 for all γ. Increasing γ leads to expansion of the higher density region for a larger initial portion of the bottleneck. (<bold>D–F</bold>) Same as in row above but for homogeneous density and heterogeneous receptor activation ratios. (<bold>D</bold>) Illustrative example with the blue region having higher receptor activation. Note that the representation of the higher activation region is expanded for all bottleneck widths. Inset demonstrates difference in activation between the regions. Larger, brighter coloured points indicate higher activation for that region compared to the other. (<bold>E</bold>) Allocation boundaries for different activation ratios. The representation of the higher activation regions is expanded for all bottlenecks. As activation ratio increases, the highly active region allocation is expanded for wider bottlenecks. (<bold>F</bold>) Changing the extent of spatial correlations (γ) has larger effects when the activation ratio is heterogeneous (set at 1:3 for all γ) compared to heterogeneous density (<bold>C</bold>). See <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for an equivalent figure considering 1D receptor arrangements.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Effect of different values of γ on the covariance function decay.</title><p>Smaller γ leads to a slower decay and therefore a larger extent of spatial correlations. When the sigma is large, receptors have less similar responses to their neighbours, and therefore co-vary less. Orange points denote the baseline spacing of receptors along one dimension. When receptor density is varied, the other region will have higher density (e.g. 1:3, indicated by blue dots).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Resource allocation for heterogeneous receptor densities and variations in input statistics in 1D.</title><p>(<bold>A</bold>) Examples of allocation for two input regions with differing receptor densities and activation (see insets) for different bottleneck widths, demonstrating complex trade-offs in resource allocation. Here, ratio for low:high density = 1:3, <italic>γ</italic>=1. Dotted lines show representation proportional to receptor density (red) and convergence in the limit (yellow). Arrows indicate expansion (down) and contraction (up) of the higher density (blue) region. Both expansion and contraction of the higher density region are seen, similar to the 2D case. (<bold>B</bold>). Allocation boundaries for different density ratios. Ratios are given as low:high density. (<bold>C</bold>) Effect of changing the decay γ parameter. Density ratio is set at 1:3 for all sigmas. (<bold>D–F</bold>), same as above but for density constant across the two regions and receptor activation manipulated. Activation ratio is set at 1:3 in (<bold>D, F</bold>). Note that in (<bold>D</bold>) the representation of the higher activation (blue) region is expanded for all bottlenecks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Illustration of eigenvalue sorting and resulting allocation.</title><p>Main panels: eigenvalues for heterogeneous density (<bold>A</bold>), heterogeneous activation (<bold>B</bold>), or combined (<bold>C</bold>). For each, the manipulated ratio is set as 1:2. For simplicity, the example considers 1D regions. Orange markers show eigenvalues from the baseline region, which is identical for all panels. Blue markers show eigenvalues from the second region with higher density, activation, or both, respectively. Markers connected by the grey line show the combined set of sorted eigenvalues from both regions. Insets: cumulative allocation for the set of eigenvalues marked by the red ellipse in the main panel. Horizontal orange lines indicate eigenvalues from and therefore allocation to the orange region, whilst vertical blue lines indicate allocation to the blue region. (<bold>A</bold>) Heterogeneous density. Increased receptor density in the blue region causes higher spatial correlations between neighbouring receptors and therefore larger eigenvalues, leading to a scaling of the eigenvalue curve. Increased density also leads to a larger number of receptors in the blue region, increasing the number of eigenvalues from this region and the total number of eigenvalues considered. (<bold>B</bold>) Heterogeneous activation. Increased variance in the blue region leads to a scaling of the eigenvalue curve. However, the number of receptors in the blue region is not affected, explaining differences observed compared to (<bold>A</bold>). (<bold>C</bold>) Heterogeneous density and activation. Effects from (<bold>A</bold>) and (<bold>B</bold>) are combined, leading to a double scaling of the eigenvalue curve, plus an increase in the total number of eigenvalues.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Limit on information rather than number of neurons.</title><p>Resource allocations where the bottleneck is expressed as variance explained. Because eigenvalues decrease dramatically in size, this re-expression results in a ’squashing’ of the allocation curve towards higher bottleneck sizes as the initial eigenvalues in the sorted set explain a much larger amount of the variance. (<bold>A</bold>) Examples with heterogeneous density. (<bold>B</bold>) Examples with heterogeneous activation. (<bold>C</bold>) Examples with heterogeneous density and activation. Dashed red line in each denotes allocation proportional to density.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig2-figsupp4-v2.tif"/></fig></fig-group></sec><sec id="s2-1-2"><title>Heterogeneous statistics</title><p>Aside from receptor densities, the stimulus statistics can also vary over the input space, leading to differences in receptor activation levels between the regions and their associated response variance. Overall, allocations for heterogeneous receptor activation are similar to those found with heterogeneous density. However, while the allocations are again a nonlinear function of bottleneck width, the representations are solely expanded for the higher activation region for all bottleneck widths (see example in <xref ref-type="fig" rid="fig2">Figure 2D</xref>). The extent of this expansion depends on the width of the bottleneck and is again more extreme for narrower bottlenecks (see <xref ref-type="fig" rid="fig2">Figure 2E</xref>). The convergence limit is <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf18"><mml:mi>a</mml:mi></mml:math></inline-formula> is the activation ratio, implying that the level of expansion and contraction in intermediate bottlenecks is more extreme than in the heterogeneous density case (see difference between red and yellow dashed lines in <xref ref-type="fig" rid="fig2">Figure 2A and D</xref>). Finally, the effect of spatial correlations is also more pronounced (<xref ref-type="fig" rid="fig2">Figure 2F</xref>).</p><p>In the cases described above, the bottleneck was constrained by the number of output neurons. Alternatively, the limiting factor might be set as the amount of information (total variance) captured. Doing so results in allocation curves that retain the nonlinear behaviour described here (see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> for illustrations). An additional consideration is the robustness of the results regarding small perturbations of the calculated eigenvalues. As allocation depends on the ranks of the eigenvalues only, low levels of noise are unlikely to affect the outcome for narrow bottlenecks, especially since eigenvalues are decaying rather steeply in this regime. On the other hand, allocation in wider bottlenecks is determined from small tail eigenvalues which are much more sensitive to noise (which is also evident when comparing the analytical solution to numerical ones). Allocation can therefore be expected to be somewhat less robust in those regimes.</p><p>In summary, we find that representations of different input regions can contract or expand, depending on the bottleneck width. This effect plays out similarly for differences in receptor density and receptor activation, however, with some crucial differences. Finally, for narrow bottlenecks, the spatial extent of the correlations across the sensory sheet becomes an important driver.</p></sec></sec><sec id="s2-2"><title>Interplay between stimulus statistics and receptor density</title><p>In sensory systems, such as in touch, both the density and input statistics vary across regions and will therefore jointly determine the resulting allocation. As a consequence, the convergence for intermediate bottlenecks will depend on both density and activation ratios, and can be calculated as <inline-formula><mml:math id="inf19"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <inline-formula><mml:math id="inf20"><mml:mi>a</mml:mi></mml:math></inline-formula> is the activation and <inline-formula><mml:math id="inf21"><mml:mi>d</mml:mi></mml:math></inline-formula> is the density ratio (see ‘Methods’ for derivation).</p><p>At narrow bottlenecks, the spread of possible allocations is much wider for varying the activation ratio (see left panel of <xref ref-type="fig" rid="fig3">Figure 3A</xref>) compared to varying the density ratio. Thus, for 2D sensory sheets, the activation ratio more strongly determines the possible allocation than does the density ratio. Specifically, this means that the allocation regime, that is, whether the allocation expands, contracts, or exhibits both behaviours across all bottleneck widths, is more dependent on relative receptor activation than densities (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Interactions between heterogeneous statistics and density.</title><p>(<bold>A</bold>) Allocations with both heterogeneous density and activation ratios. Expansion and contraction for a baseline region where relative density and activation is varied over the other region. All ratios given are baseline:other region. Left: fixed density ratio of 1:2, while activation ratio is varied between 7:1 and 1:7 (see colourbar). Purple dashed line indicates allocation proportional to density ratio. Right: fixed activation ratio of 1:2, while density ratio is varied between 7:1 and 1:7. The coloured markers indicate whether the baseline region is expanded (dark blue squares), contracted (white circles), or both (light blue stars) across all bottlenecks. (<bold>B</bold>) Possible expansion/contraction regimes for the baseline region based on combinations of density and activation ratios. Colours as shown by the markers in (<bold>A</bold>). Grey dashed lines show all possible allocation regimes for an example with either fixed density ratio (horizontal, corresponding plot in <bold>A</bold>, left) or activation ratio (vertical, <bold>A</bold>, right). When activation is fixed and density is varied, the allocations can be either expanded/both or contracted/both across the full bottleneck width. In contrast, when the density ratio is fixed and activation is varied, the allocation of a region could be any of the three regimes. The green ellipse highlights parameter combinations where activation and density ratios are correlated. See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for a comparison of how receptor density and activation interact between 1D and 2D receptor arrangements.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Comparison between 1D and 2D results for heterogeneous activation and density.</title><p>(<bold>A</bold>) Effect of changing both the density and activation ratios, and possible resource allocations for two regions. Plots show the same density ratio, 1:5, considering 1D (left) and 2D (right) regions. Allocation % is for baseline region (region 1). Each line denotes the variance ratios between region 1:region 2 (see colourbar). Solid lines denote contraction of baseline region representation, whilst dotted lines are expansion. The horizontal red dashed line shows the proportional density representation. Across both 1D and 2D, different patterns of expansion and contraction occur. The baseline region is more likely to either be fully expanded or contracted for the whole bottleneck in 2D, whereas in 1D both expansion and contraction occur (denoted by light blue star, which covers three activation ratios in 1D, but only 1:1 in 2D as indicated by the markers). In 1D, the same activation and density ratio leads to convergence at the proportional density, whereas for 2D, the convergence is always lower. (<bold>B</bold>) Comparison of the possible allocation schemes between 1D (left) and 2D (right). Dotted grey line denotes density ratio from (<bold>A</bold>). In 2D, the likelihood of both expansion and contraction (light blue region) of the representations decreases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig3-figsupp1-v2.tif"/></fig></fig-group><p>Finally, it is likely that regions with higher receptor densities will also show greater activation than lower density regions. For example, in touch, the regions on the hand with the highest receptor densities also are the most likely to make contact with objects (<xref ref-type="bibr" rid="bib42">Gonzalez et al., 2014</xref>). In these cases, both effects reinforce each other and drive the resulting allocation further from proportional allocation (see orange lines in <xref ref-type="fig" rid="fig3">Figure 3A</xref> and green ellipse in <xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p></sec><sec id="s2-3"><title>Resource limits determine the strength of plasticity under changes in stimulus statistics</title><p>Over most of the lifetime of an organism, the resources available for processing sensory information and the density of sensory receptors should be relatively constant. The stimulus statistics, on the other hand, can and will change, for example, when encountering a new environment or learning new skills. These changes in stimulus statistics should then affect sensory representations, mediated by a variety of plasticity mechanisms. For example, increased stimulation of a digit will lead to an expansion of that digit’s representation in somatosensory cortex (<xref ref-type="bibr" rid="bib53">Jenkins et al., 1990</xref>).</p><p>We asked how representations should adapt under the efficient coding framework and whether resource limits would affect the resulting changes. To answer this question, we calculated optimal allocations for different bottleneck widths, receptor densities, and stimulus statistics. We then introduced a change in stimulus statistics and re-calculated the resulting allocations (see illustration in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). As expected, we found that when increasing the receptor activation over a region (e.g. by increasing stimulation of the region), more neurons would be allocated to this region than to the other. Interestingly, however, this effect was dependent on the width of the bottleneck. The largest effects are seen for smaller bottlenecks and then diminish as the bottleneck size increases. <xref ref-type="fig" rid="fig4">Figure 4B</xref> demonstrates such allocation changes for three different bottleneck widths for a range of receptor densities and activation ratios. This suggests that plasticity should be relatively stronger under extreme resource constraints than in cases where limits on the information are generous.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Re-allocation to account for changes in stimulus statistics.</title><p>(<bold>A</bold>) Top left: illustration of problem setup. Increased stimulation is applied to the middle digit (yellow symbols), leading to changes in optimal allocations. Top right: optimal allocations for baseline (blue) and stimulation (yellow) conditions across all bottleneck widths. Stimulation of the middle finger increases its representation, but the relative magnitude of the effect depends on the bottleneck width. Bottom: changes in allocation of the middle digit for two bottleneck widths (indicated by dashed lines above). The increase is proportionally larger for the narrow compared to the wide bottleneck. (<bold>B</bold>) Change in allocation when receptor activation for an input region increases (left half) or decreases (right half). Drastic changes in cortical allocation are observed for narrow bottlenecks (green lines), while wider bottlenecks (red and purple lines) induce only moderate change. Solid lines denote equal receptor density across both regions, while dashed lines show examples with varying density.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig4-v2.tif"/></fig></sec><sec id="s2-4"><title>Generalization to other covariance functions</title><p>The results above relate to negative exponential covariance functions, where allocations can be solved analytically and it is unclear whether these findings generalize to other monotonically decreasing covariance functions and are therefore robust. Specifically, negative exponential covariances are relatively ‘rough’ and allocations might conceivably depend on the smoothness of the covariance function. To test directly how optimal allocations depend systematically on the smoothness of the covariance function, we numerically calculated allocations for three covariance functions of the Matérn class, whose smoothness depends on a single parameter ν. The negative exponential considered so far is a special case of the Matérn class when <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and we also considered <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (see ‘Methods’). As ν increases, the decay function becomes smoother, yielding larger correlations at short distances and smaller ones at farther distances (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Allocations for other monotonically decreasing covariance functions.</title><p>(<bold>A</bold>) Three covariance functions of different smoothness taken from the Matérn class, differing in the parameter ν (see ‘Methods’). (<bold>B</bold>) Examples of numerically determined allocation for the covariance functions shown in (<bold>A</bold>) for two scenarios. Left: density ratio 1:2, equal activation. Right: density ratio 1:6, activation ratio 1:3. The red dashed line shows the analytic solution for the negative exponential covariance (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) Estimated allocations at convergence for different density ratios (horizontal axis) and a fixed activation ratio of 1:2. Solid lines denote numerical convergence, and dashed lines refer to fitted functions (see main text).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig5-v2.tif"/></fig><p>We numerically calculated allocations for different density and activation ratios in 1D (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). We focused on smaller bottleneck widths, where solutions are numerically stable, and observed a close match between the numerical and analytical solutions for the negative exponential covariance function (see dashed lines in <xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>This analysis yielded two main findings. First, allocation curves are qualitatively similar across all tested covariance functions in that regions with higher density, activation, or both are systematically over-represented. Furthermore, this effect is more extreme at smaller bottleneck widths. Second, the resulting allocations depend systematically on the smoothness of the covariance function, such that smoother covariances yield less extreme differences in allocation. This effect is most obvious when considering the convergence points at larger bottlenecks: smoother covariance functions induce a more uniform allocation (closer to a 50:50 split) than does the negative exponential (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). We also noticed that the convergence points appeared to follow a simple function, namely, <inline-formula><mml:math id="inf26"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mroot><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:mroot></mml:mrow></mml:mfrac></mml:math></inline-formula> for <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf28"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mroot><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:mroot></mml:mrow></mml:mfrac></mml:math></inline-formula> for <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (see dashed lines in <xref ref-type="fig" rid="fig5">Figure 5C</xref>), and <inline-formula><mml:math id="inf30"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula> for <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (which is the result obtained analytically, see Appendix 6).</p><p>In summary, monotonically decreasing covariance functions result in qualitatively similar allocations to the negative exponential studied earlier. However, precise allocations and convergence are determined by the correlational structure: smoother covariance functions lead to less extreme differences in allocation.</p></sec><sec id="s2-5"><title>Predicting cortical magnification in the star-nosed mole</title><p>Finally, we investigated to what extent the procedure outlined in the previous sections might predict actual resource allocation in the brain. While our model is relatively simple (see ‘Discussion’), decorrelation has been found to be a strong driver in early sensory processing, including in vision (<xref ref-type="bibr" rid="bib4">Atick and Redlich, 1992</xref>; <xref ref-type="bibr" rid="bib45">Graham et al., 2006</xref>; <xref ref-type="bibr" rid="bib90">Vinje and Gallant, 2000</xref>), audition (<xref ref-type="bibr" rid="bib21">Clemens et al., 2011</xref>; <xref ref-type="bibr" rid="bib82">Smith and Lewicki, 2006</xref>), and touch (<xref ref-type="bibr" rid="bib61">Ly et al., 2012</xref>), and one might therefore expect the approach to at least yield qualitatively valid predictions. As currently available empirical data makes it difficult to test the impact of different bottleneck widths on the resulting allocation directly (see ‘Discussion’), we instead focused on another predicted outcome of the proposed model: the precise interaction between receptor density and receptor activation in driving resource allocation that we presented earlier. We picked the star-nosed mole as our model system because stimulus statistics, receptor densities, and cortical allocations have been precisely quantified. Moreover, the star-nosed mole displays considerable variation in all these parameters, presenting a good opportunity to put the model to the test.</p><p>The star-nosed mole is a mostly underground dwelling creature relying on active tactile sensing while foraging for prey (<xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>; <xref ref-type="bibr" rid="bib18">Catania, 2020</xref>). This process is facilitated by two sets of 11 appendages arranged in a star-like pattern that make up the mole’s nose (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Individual rays are tiled with tactile receptors known as Eimer’s organs that detect prey, which is then funnelled towards the mouth (<xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>; <xref ref-type="bibr" rid="bib76">Sawyer and Catania, 2016</xref>). The density of fibres innervating the Eimer’s organs differs across the rays, with rays closer to the mouth exhibiting higher densities (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The rays also vary in size and location with respect to the mouth. This affects their usage as the rays closest to the mouth encounter tactile stimuli much more frequently than other rays (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In the cortex, a clear topographic representation of the rays can be found (<xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>). However, the extent of the cortical ray representations is not proportional to the physical size of the rays. Ray 11, which sits closest to the mouth, is cortically magnified several fold and is considered the tactile equivalent of the visual fovea (<xref ref-type="bibr" rid="bib16">Catania and Remple, 2004</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Resource allocation in the star-nosed mole.</title><p>(<bold>A</bold>) Star-nosed moles have two sets of 11 tactile rays used for detecting and capturing prey. (<bold>B</bold>) Fibre innervation densities for each ray. (<bold>C</bold>) Typical usage percentages for each ray during foraging. Higher usage corresponds to greater average activation of receptors located on the corresponding ray. Typically prey is funnelled from distal rays towards ray 11, which is located next to the mouth. Ray outlines adapted from <xref ref-type="bibr" rid="bib17">Catania et al., 2011</xref>. (<bold>D</bold>) Left: explained variance (<inline-formula><mml:math id="inf32"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) between model predictions and cortical allocation for three different models: restricted to receptor density only (purple), restricted to receptor activation only (green), and a full model (pink) that incorporates accurate values for both factors. Results confirm previous findings that ray usage is a better predictor of cortical allocation than receptor densities alone. Additionally, we show that including both of these factors provides a marginal improvement to the fit, with the highest <inline-formula><mml:math id="inf33"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (86%). Right: predicted versus empirical cortical allocations for all rays. When including both density and activation parameters, the model provides a good fit to empirical measurements. Scatter plots for all models are available in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>. (<bold>A</bold>) Reproduced from Figure 1C in <xref ref-type="bibr" rid="bib17">Catania et al., 2011</xref>, copyright Kenneth Catania.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Additional figures for the star-nosed mole.</title><p>(<bold>A</bold>) Fits across all bottleneck sizes for each model. Lowest root-mean-square error (RMSE) is indicated for each model. Dashed line indicates lowest RMSE (excluded for densities-only model as RMSE is fairly constant across bottlenecks, see <bold>C</bold>). (<bold>B</bold>) Corresponding fits at each selected bottleneck size compared to cortical data. (<bold>C</bold>) RMSE values over all bottleneck sizes for each model. (<bold>D</bold>) Can any values be fit given the range of bottleneck sizes? Here, we test whether a good fit can be obtained for a series of ‘random models’, where the random values are selected for each ray’s density and variance within the range of parameters. The same fitting process is then applied, where the lowest RMSE fit to the cortical data is selected based on the range of possible bottlenecks. Note that <inline-formula><mml:math id="inf34"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> values are calculated with fixed intercept, resulting in some negative values. Most random models perform poorly, suggesting that the model cannot be fit successfully without using the precise parameter values from the empirical star-nosed mole data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-70777-fig6-figsupp1-v2.tif"/></fig></fig-group><p>Previous work by <xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref> found that cortical sizes are correlated more strongly with the different patterns of activation across the rays, rather than their innervation densities. Using the empirical data quantifying receptor densities and stimulus statistics from this study, we investigated whether the efficient coding model could predict typical cortical representation sizes for each ray (see ‘Methods’ for details) and whether innervation densities or usage would lead to more accurate allocation predictions. As the bottleneck size between periphery and cortex is unknown for the star-nosed mole, we calculated the optimal allocations over all possible bottleneck sizes. Using the model described above, we calculated allocations considering three different scenarios. The first, ‘density only’, included accurate afferent densities, but with activations set to the mean over all rays. The second model, ‘usage only’, included accurate activation ratios but mean receptor densities. Finally, the ‘full model’ included accurate values for both factors. We found that the empirical cortical representation sizes are most accurately predicted by the models that include receptor activation—the ‘receptor usage only’ and ‘full models’ (<xref ref-type="fig" rid="fig6">Figure 6D</xref>)—suggesting the star-nosed mole could be employing an efficient coding strategy based on decorrelation in the neural representation of somatosensory inputs.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We examined efficient population coding under limited numbers of output neurons in cases of non-uniform input receptor densities and stimulus statistics. Instead of focusing on the precise structure of the receptive field, we asked which coarse region of the sensory sheet the receptive field would fall on. We showed that the resulting allocations are nonlinear and depend crucially on the width of the bottleneck, rather than being proportional to the receptor densities or statistics. Specifically, narrow bottlenecks tend to favour expansion of a single region, whereas for larger bottlenecks, allocations converge to a constant ratio between the regions that is closer to a proportional representation. Whether, across all possible bottlenecks, allocations are always expanded, contracted, or show both expansion and contraction depends on the relative density and activation ratios, but receptor activation plays a bigger role. When allocation changes due to novel stimulus statistics, a larger fraction of output neurons will switch their receptive field to another region for narrow compared to wide bottlenecks. Finally, we demonstrated that in the star-nosed mole a model that includes both accurate innervation densities and contact statistics provides a better fit to the sizes of somatosensory cortical regions than considering each of these factors alone.</p><sec id="s3-1"><title>Comparison with previous approaches</title><p>A key feature of efficient coding population models is the non-uniform allocation of output neurons, whereby stimuli occurring at higher probabilities are represented by a greater number of neurons. A common approach is to use Fisher Information as a proxy for mutual information, enabling the calculation of optimal output neuron density and tuning curve placement given a distribution of sensory stimuli (<xref ref-type="bibr" rid="bib37">Ganguli and Simoncelli, 2010</xref>; <xref ref-type="bibr" rid="bib38">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib39">Ganguli and Simoncelli, 2016</xref>; <xref ref-type="bibr" rid="bib97">Yerxa et al., 2020</xref>). However, these approaches maximize information between the output population and the stimulus distribution itself, such that allocating additional neurons to a given region of the input space will always lead to an increase in the overall amount of information represented. In contrast, our approach assumes a finite number of input receptors in each region. We are thus asking a different question than previous research: once information about a sensory stimulus has been captured in a limited population of receptors, what is the most efficient way of representing this information? This framing implies that once all information from a given region has been fully captured in the output population, our method does not allocate further neurons to that region. Therefore, this also places a limit on the total size of the output population as this cannot exceed the total number of input receptors.</p><p>There has also been prior work on how bottlenecks affect sensory representations, though mostly focused on how different levels of compression affect receptive field structure. For example, <xref ref-type="bibr" rid="bib28">Doi and Lewicki, 2014</xref> predicted receptive fields of retinal ganglion cells at different eccentricities of the retina, which are subject to different convergence ratios. More recently, such direct effects on representations have also been studied in deep neural networks (<xref ref-type="bibr" rid="bib60">Lindsey et al., 2019</xref>). Finally, some approaches employ a different cost function than the mean-squared reconstruction error inherent to the principal component analysis (PCA) method used here. For example, the Information Bottleneck method (<xref ref-type="bibr" rid="bib86">Tishby et al., 2000</xref>; <xref ref-type="bibr" rid="bib87">Tishby and Zaslavsky, 2015</xref>) aims to find a low-dimensional representation that preserves information about a specific output variable, while compressing information available in the input. It is quite likely that the choice of cost function would affect the resulting allocations, a question that future research should pursue.</p></sec><sec id="s3-2"><title>Implications for sensory processing</title><p>The quantitative comparison of the model results with the cortical somatosensory representation in the star-nosed mole provided limited evidence that one of the model predictions—namely, that receptor density and activation statistics should jointly determine cortical organization—is borne out in a biological model system. Further direct tests of the theory are hampered by the lack of reliable quantitative empirical data. Nevertheless, the model makes a number of qualitative predictions that can be directly compared to available data or tested in future experiments.</p><p>First, there is additional evidence that both receptor density and stimulus statistics drive allocation in neural populations. Similar to the star-nosed model, the primate somatosensory system also exhibits non-uniform distributions of both stimulus statistics and receptor density. Both of these factors are also broadly correlated, for example, receptor densities are higher on the fingertips, which are also more likely to be in contact with objects (<xref ref-type="bibr" rid="bib42">Gonzalez et al., 2014</xref>). Importantly, while receptor densities alone can explain some of the magnification observed in the cortical somatosensory homunculus, they cannot account for this effect fully (<xref ref-type="bibr" rid="bib23">Corniani and Saal, 2020</xref>). Indeed, evidence from non-human primates shows that cortical magnification also critically depends on experience (<xref ref-type="bibr" rid="bib94">Xerri et al., 1996</xref>; <xref ref-type="bibr" rid="bib95">Xerri et al., 1999</xref>). Similar results have recently been obtained from the brainstem in mice (<xref ref-type="bibr" rid="bib58">Lehnert et al., 2021</xref>). Empirically reported differences in sizes of receptive fields between the digit tips and the palm have also been recreated from typical contact across the hand during grasping (<xref ref-type="bibr" rid="bib11">Bhand et al., 2011</xref>), suggesting an influence of statistics of hand use. Both mechanoreceptor densities (e.g. <xref ref-type="bibr" rid="bib89">Verendeev et al., 2015</xref>) and hand use statistics (<xref ref-type="bibr" rid="bib36">Fragaszy and Crast, 2016</xref>) differ across primates, forming the basis for a potential cross-species study. The model presented here demonstrates how both effects can be treated within a single framework driven by information maximization.</p><p>Second, it appears that allocation in sensory systems with severely constrained resources is qualitatively in agreement with model predictions. As our results demonstrated, magnification should be more extreme the tighter the bottleneck. The best characterized and most clearly established bottleneck in sensory processing is likely the optical nerve in vision. Given that the optic nerve serves as a narrow bottleneck (approximately 12–27%; assuming 0.71–1.54 million retinal ganglion cells with 80% Midget cells [<xref ref-type="bibr" rid="bib24">Curcio and Allen, 1990</xref>; <xref ref-type="bibr" rid="bib68">Perry et al., 1984</xref>] and 4.6 million cones [<xref ref-type="bibr" rid="bib25">Curcio et al., 1990</xref>]), and that the fovea contains a much higher density of cone receptors than the periphery (<xref ref-type="bibr" rid="bib92">Wells-Gray et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Curcio et al., 1990</xref>), the model would predict a large over-representation of the fovea, agreeing with experimental observations (see <xref ref-type="bibr" rid="bib32">Edmondson et al., 2019</xref>, for further qualitative evidence). In order to further test the proposed model, comparisons could be made between individuals to study variations within a population. Specifically, it would be expected that optic nerves containing a high number of fibres would devote proportionally fewer of them to the fovea than optic nerves containing smaller numbers of fibres (assuming equal receptor densities and numbers in the retina). These comparisons could be extended across species, taking advantage of the fact that photoreceptor densities and optic nerve fibre counts differ across many primate species (<xref ref-type="bibr" rid="bib35">Finlay et al., 2008</xref>). Along these lines, recent computational work has shown that the amount of neural resources allocated to the optic nerve can be expected to affect the structure of the receptive field itself (<xref ref-type="bibr" rid="bib60">Lindsey et al., 2019</xref>). Finally, the extent of cortical areas can be controlled by experimental interventions in animals (<xref ref-type="bibr" rid="bib51">Huffman et al., 1999</xref>), which would constitute a direct manipulation of the bottleneck.</p><sec id="s3-2-1"><title>Re-allocation during development, learning, and ageing</title><p>Changing receptor densities, stimulus statistics, or resource limits over the life span of an organism should lead to a dynamic re-allocation of the available resources. The most common case will be changes in stimulus statistics as both receptor densities and resources should be relatively stable. In such cases, representations should adapt to the new statistics. For example in touch, changing the nature of tactile inputs affects cortical representations (<xref ref-type="bibr" rid="bib22">Coq and Xerri, 1998</xref>; <xref ref-type="bibr" rid="bib63">Merzenich and Jenkins, 1993</xref>; <xref ref-type="bibr" rid="bib94">Xerri et al., 1996</xref>). Increasing statistics of contact over a region typically leads to expansion of that region in the cortex. Our method suggests that the precise level of expansion would be dependent on the bottleneck width with larger effects observed for narrower bottleneck sizes.</p><p>For the other cases, changes in fibre numbers during development and ageing might be interpreted as a change in resources. For example, the optic nerve undergoes a period of rapid fibre loss early during development (<xref ref-type="bibr" rid="bib77">Sefton et al., 1985</xref>; <xref ref-type="bibr" rid="bib72">Provis et al., 1985</xref>). Similarly, fibre counts in the optic nerve decrease during ageing (<xref ref-type="bibr" rid="bib29">Dolman et al., 1980</xref>; <xref ref-type="bibr" rid="bib55">Jonas et al., 1990</xref>; <xref ref-type="bibr" rid="bib75">Sandell and Peters, 2001</xref>). In this case, the model would predict a decrease in the size of peripheral representation in the bottleneck compared to the fovea. It is also possible that the receptor densities themselves may change. In touch, ageing leads to reductions in the densities of receptors in older adults (<xref ref-type="bibr" rid="bib40">García-Piqueras et al., 2019</xref>). In such cases, we have effectively increased the bottleneck width relative to our receptor population, which again should lead to the re-allocation of resources.</p></sec><sec id="s3-2-2"><title>Expansion and contraction along the sensory hierarchy</title><p>Magnification of specific sensory input regions can be observed throughout the sensory hierarchy, from the brainstem and thalamus up to cortical sensory areas. Often, the amount of expansion and contraction differs between areas. For example, the magnification of the fovea increases along the visual pathway from V1 to V4 (<xref ref-type="bibr" rid="bib49">Harvey and Dumoulin, 2006</xref>). Which of these representations might be best addressed by the model presented here? The main component of the model is decorrelation, which has been shown to be a driving principle for efficient coding in settings where noise is low (<xref ref-type="bibr" rid="bib19">Chalk et al., 2018</xref>). This is generally the case in low-level sensory processing, for example, in touch (<xref ref-type="bibr" rid="bib44">Goodwin and Wheat, 2004</xref>). Our results might therefore best match early sensory processing up to perhaps low-level cortical representations. Beyond this, it is likely that noise will be much higher (<xref ref-type="bibr" rid="bib79">Shadlen and Newsome, 1998</xref>) and for efficient codes to shift away from decorrelation (<xref ref-type="bibr" rid="bib50">Hermundstad et al., 2014</xref>). Furthermore, while distinct bottlenecks, whether on the number of neurons or the amount of information, are common in low-level processing, it is less clear whether such restrictions constrain cortical processing. Whether and how such different regimes should affect neural allocations remains an open question.</p></sec><sec id="s3-2-3"><title>Perceptual consequences</title><p>Does the allocation of output neurons lead to testable perceptual consequences? While we do not model neurons’ receptive fields directly, allocating more neurons to a given region would increase perceptual spatial acuity for that region. Indeed, cortical magnification and perceptual acuity are correlated in both vision (<xref ref-type="bibr" rid="bib30">Duncan and Boynton, 2003</xref>) and touch (<xref ref-type="bibr" rid="bib31">Duncan and Boynton, 2007</xref>). At the same time, the absolute limits on spatial acuity are determined by the density of receptors in each input region. A naive allocation scheme that assigns output neurons proportional to the density of receptors would therefore result in perceptual spatial acuity proportional to receptor distance. Instead, as our results have shown, the allocation should not be proportional in most cases. Specifically, for narrow bottlenecks we would expect relatively higher spatial acuity for regions with high receptor density than might be expected from a proportional allocation. Conversely, for wider bottlenecks this relationship should be reversed and spatial acuity should be better than expected for lower density regions. In agreement with these results, it has been found that in vision spatial resolution declines faster than expected with increasing eccentricity, suggesting a narrow bottleneck in the optic nerve (<xref ref-type="bibr" rid="bib1">Anderson et al., 1991</xref>).</p><p>A second consequence is that spatial acuity should be better in regions with higher activation probability even when receptor densities are equal. Indeed, spatial discrimination in touch improves with training or even just passive stimulation (<xref ref-type="bibr" rid="bib88">Van Boven et al., 2000</xref>; <xref ref-type="bibr" rid="bib41">Godde et al., 2000</xref>), up to a limit that is presumably related to receptor density (<xref ref-type="bibr" rid="bib93">Wong et al., 2013</xref>; <xref ref-type="bibr" rid="bib69">Peters et al., 2009</xref>). Assuming a fixed resource limit, training may offer improvements to some digits to the detriment of others. Whether this is indeed the case has to our knowledge not yet been empirically tested.</p><p>Finally, previous work has shown that non-uniform tuning curves across a population will lead to characteristic biases in perceptual tasks (<xref ref-type="bibr" rid="bib91">Wei and Stocker, 2015</xref>). While the original formulation assumed that this heterogeneous allocation of output neurons was driven by stimulus statistics alone, we have shown here that it can also be a consequence of receptor densities. Thus, perceptual biases might also be expected to arise from neural populations that efficiently represent sensory inputs sampled by non-uniform receptor populations.</p></sec></sec><sec id="s3-3"><title>Limitations and future work</title><p>We considered simple linear models based on decorrelation and demonstrated that even such seemingly straightforward models exhibit surprising complexity in how they manage trade-offs in resource allocation under constraints. Specifically, we found that output neurons were not generally allocated proportionally to input neurons or according to some other fixed rule. It therefore stands to reason that similarly complex trade-offs would manifest in more complex models, even though the precise allocations might differ. Nevertheless, since PCA is widely employed for decorrelation and dimensionality reduction, and therefore incorporated into many other algorithms, our results immediately generalize to several other methods. For example, it is straightforward to extend the model with additional constraints (e.g. response sparsity or power constraints) that would not affect the resulting allocations (see ‘Methods’ for details), and therefore the model presented here already covers a number of related models. This includes independent component analysis, which considers higher-order rather than second-order statistics, but relies on a whitened signal, which in the undercomplete (bottleneck) case is obtained via PCA (<xref ref-type="bibr" rid="bib52">Hyvärinen and Oja, 2000</xref>). Similarly, some models that do incorporate sensory noise and maximize reconstruction accuracy also use an undercomplete set of principal components to reduce the dimensionality of the sensory signal (<xref ref-type="bibr" rid="bib28">Doi and Lewicki, 2014</xref>). In both of these examples, the resulting receptive field structure will differ, but their allocation—where on the sensory sheet they will fall—will be governed by the same principles described earlier. However, we did not consider nonlinear models and previous work has demonstrated that response nonlinearities can make important contributions to decorrelation (<xref ref-type="bibr" rid="bib70">Pitkow and Meister, 2012</xref>). Additionally, the precise structure and origin of noise in nonlinear models have been demonstrated to affect efficient coding strategies (<xref ref-type="bibr" rid="bib12">Brinkman et al., 2016</xref>) and might therefore also influence the resulting allocations. Future work should explore resource allocation in such complex models.</p><p>We considered allocations for monotonically decreasing covariance functions. Choosing a negative exponential function and assuming that activations are uncorrelated across different input regions allowed us to derive an analytical solution. How justified are these assumptions? Many sensory systems will likely obey the intuitive notion that receptor correlations decrease with distance; however, there are notable exceptions, for example, in the auditory system (<xref ref-type="bibr" rid="bib85">Terashima and Okada, 2012</xref>). In touch, the sensory system we are mainly concerned with here, contact might often be made with several fingers (for humans) or rays (for the star-nosed mole) simultaneously, which would induce far-ranging non-monotonic correlations. Unfortunately there is little quantified evidence on the strength of these correlations, rendering their importance unclear. At least for the star-nosed mole, their prey is often small compared to the size of the rays, which move mostly independently, so cross-ray correlations might be low. Furthermore, the receptive fields of neurons in early somatosensory cortex of both humans and star-nosed moles are strongly localized to a single appendage and lie within cortical sub-regions that are clearly delineated from others (<xref ref-type="bibr" rid="bib84">Sur et al., 1980</xref>; <xref ref-type="bibr" rid="bib64">Nelson et al., 1980</xref>; <xref ref-type="bibr" rid="bib14">Catania and Kaas, 1995</xref>). If long-range non-monotonic correlations were strong, we would expect to find many multi-appendage receptive fields and blurry region boundaries. As this is not the case, it therefore stands to reason that either these correlations are not very strong or that there is some other process, perhaps during development, that prevents these correlations affecting the final allocation. Either way, our assumption of monotonically decreasing covariance functions appears to be a good first-order match. Still, the question of how to arrive at robust allocations for more complex covariance functions is an important one that should be considered in future research. While it is possible in principle to solve the allocation problem numerically for arbitrary covariance functions, in practice we noticed that the presence of small numerical errors can affect the sorting process and caution is therefore warranted, especially when considering non-monotonic functions.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>Our main goal was to derive a method for efficiently allocating output neurons to one of several input regions with different correlational response structure in the presence of constraints on the number of output neurons or amount of information being transmitted. In the following, we focus on the main rationale and equations, while specific proofs can be found in Appendix 1–6. First, we outline the framework for combined whitening and dimensionality reduction that is employed. Next, we demonstrate how this framework can be applied to multiple input regions with different statistics and densities of receptors, and how calculation of the eigenvalues of region-specific covariance matrices solves the problem of resource allocation. Finally, we demonstrate how the problem can be solved analytically for a certain choice of covariance function. The Python code implementing the equations that can be used to recreate the figures in this article is available on GitHub (<xref ref-type="bibr" rid="bib33">Edmondson, 2021</xref>).</p><sec id="s4-1"><title>Combined whitening and dimensionality reduction</title><p>We assume that receptors are arranged on a 2D sensory sheet. Correlations in the inputs occur as receptors that are nearby in space have more similar responses. Restricting ourselves to such second-order statistics and assuming that noise is negligible, information is maximized in such a setup by decorrelating the sensory inputs. Here, we decorrelate using a simple linear model. To model the bottleneck, we restrict the number of outputs to <inline-formula><mml:math id="inf35"><mml:mi>m</mml:mi></mml:math></inline-formula> &lt; <inline-formula><mml:math id="inf36"><mml:mi>n</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf37"><mml:mi>n</mml:mi></mml:math></inline-formula> is the total number of receptors.</p><p>If the inputs are represented as a matrix <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of dimensions <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf40"><mml:mi>z</mml:mi></mml:math></inline-formula> is the number of sensory input patterns), then our goal is to find an <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> dimensional matrix <inline-formula><mml:math id="inf42"><mml:mi mathvariant="bold-italic">W</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:math></inline-formula> is uncorrelated:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This is achieved by setting <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Solutions can then be expressed in terms of the diagonal matrix of eigenvalues, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and eigenvectors, <inline-formula><mml:math id="inf47"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula>, of the covariance matrix <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:msup><mml:mi mathvariant="bold">Λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Whitening filters are not uniquely determined and optimal decorrelation is obtained with any orthogonal matrix <inline-formula><mml:math id="inf49"><mml:mi mathvariant="bold-italic">P</mml:mi></mml:math></inline-formula>. Setting <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (plain whitening/PCA) leads to the form shown in the main results section (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Localized receptive fields are obtained by setting <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is known as zero-phase component analysis. In cases with a bottleneck, the solution involves solving an Orthogonal Procrustes problem (<xref ref-type="bibr" rid="bib28">Doi and Lewicki, 2014</xref>) to find <inline-formula><mml:math id="inf52"><mml:msup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, an <inline-formula><mml:math id="inf53"><mml:mi>m</mml:mi></mml:math></inline-formula>-dimensional orthogonal matrix (where <inline-formula><mml:math id="inf54"><mml:mi>m</mml:mi></mml:math></inline-formula> is the size of the bottleneck) which minimizes the reconstruction error of the inputs and a set of ideal local receptive fields <inline-formula><mml:math id="inf55"><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:msup><mml:mi mathvariant="bold">Λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf56"><mml:mrow><mml:mo>∥</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mo>∥</mml:mo><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the Frobenius norm, and <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are as above but retaining only those components with the <inline-formula><mml:math id="inf59"><mml:mi>m</mml:mi></mml:math></inline-formula> largest eigenvalues. As the optimally whitened solution is independent of <inline-formula><mml:math id="inf60"><mml:mi mathvariant="bold-italic">P</mml:mi></mml:math></inline-formula>, additional constraints on the solution can be enforced. For example, power constraints are often added, which either limit the total variance or equalize the variance across output neurons, and additional sparsity constraints can be placed on the output neuron’s activity or their receptive field weights (<xref ref-type="bibr" rid="bib27">Doi et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Doi and Lewicki, 2014</xref>). For example, to optimize the sparsity of the weight matrix, one would define an appropriate cost function (such as the <inline-formula><mml:math id="inf61"><mml:msup><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:math></inline-formula> norm of the weight matrix), then iteratively calculate the gradient with respect to <inline-formula><mml:math id="inf62"><mml:mi mathvariant="bold-italic">W</mml:mi></mml:math></inline-formula>, take an update step to arrive at a new <inline-formula><mml:math id="inf63"><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and determine <inline-formula><mml:math id="inf64"><mml:mi mathvariant="bold-italic">P</mml:mi></mml:math></inline-formula> as described in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> (see <xref ref-type="bibr" rid="bib28">Doi and Lewicki, 2014</xref>, for further details). Importantly for our problem, such additional constraints will affect the precise receptive field structure (through <inline-formula><mml:math id="inf65"><mml:mi mathvariant="bold-italic">P</mml:mi></mml:math></inline-formula>), but not the eigenvalues and eigenvectors included in <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. As we will see in the following section, our solution relies on the eigenvalues only, and we can therefore solve the allocation problem irrespective of the precise receptive field structure or additional constraints on the solution.</p></sec><sec id="s4-2"><title>Extension to multiple input regions</title><p>For our specific problem, we are interested in the case of multiple input regions with different correlational structure (i.e. due to differing receptor density or activation). To simplify the derivations, we approximate different input regions as independent, such that the overall covariance matrix will be a block diagonal matrix. The covariance <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for two input regions, <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, can then be expressed as follows:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This assumption turns out to be a reasonable approximation when region sizes are relatively big and correlations typically do not extend far across the sensory sheet (see <xref ref-type="bibr" rid="bib32">Edmondson et al., 2019</xref>, for a comparison between block and non-block region covariance matrices in 1D). Furthermore, in many sensory systems, the borders between regions of differing density tend to be relatively narrow. For example, in touch, the digits of the hand are spatially separated, and regions of differing densities, for example, between the digit tips and proximal phalanges, neighbour along the short rather than long axis of the digit. In the star-nosed mole, rays of different innervation densities are separated and neighbour only along their connection to the rest of the nose. However, the block matrix approximation might be problematic in cases with many very small adjacent regions with strong, far-ranging correlations.</p><p>The eigenvalues and eigenvectors of a block diagonal covariance matrix also follow the block diagonal form and can be calculated from the individual region covariances alone by a simple application of the Cauchy interlacing theorem. Thus, the corresponding eigenvalues and eigenvectors are<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold">Λ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mi mathvariant="bold">Λ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="bold">0</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Due to the imposed bottleneck, only the <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> largest eigenvalues from the combined set <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will be retained. If receptive fields are localized such that they are constrained to fall within a single input region, then an eigenvalue selected from <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">Λ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates that the receptive field of the corresponding output neuron will fall onto region <italic>R</italic><sub>1</sub>, and analogously for <italic>R</italic><sub>2</sub>. This fact holds independent of the structure of <inline-formula><mml:math id="inf72"><mml:mi mathvariant="bold-italic">P</mml:mi></mml:math></inline-formula> that is chosen in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> because in order to preserve decorrelation a given region cannot contain more output neurons than eigenvalues retained from this region. In the following, we show how the eigenvalues can be calculated analytically for certain covariance functions.</p></sec><sec id="s4-3"><title>Calculation of eigenvalues for negative exponential covariance functions</title><p>We model the covariance between receptors as a negative exponential function. The covariance matrix is then calculated as a function of distance between pairs of receptors (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). We will first go through the calculation of eigenvalues for the baseline region <italic>R</italic><sub>1</sub>, and then continue with the derivation for <italic>R</italic><sub>2</sub>, which exhibits different receptor density, receptor activation, or both.</p><p>For region <italic>R</italic><sub>1</sub> the covariance between receptors is calculated as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>x</italic><sub><italic>i</italic></sub> and <italic>x</italic><sub><italic>j</italic></sub> are the locations of the <italic>i</italic>th and <italic>j</italic>th receptors, and γ is the decay constant.</p><p>The corresponding eigenvalues for an exponential covariance function in the continuous domain can be calculated analytically. The eigenvalue–eigenvector problem is expressed as an integral homogeneous equation, such that for <italic>R</italic><sub>1</sub> we get<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the <italic>k</italic>th eigenfunction and <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> its corresponding eigenvalue. The domain length <inline-formula><mml:math id="inf75"><mml:mi>L</mml:mi></mml:math></inline-formula> is the input region size for one of the dimensions.</p><p>It can be shown that solutions to this problem can be related to the Laplacian operator (see Appendix 2 and Appendix 3 for proofs), such that<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>μ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> are the eigenvalues of the Laplacian operator.</p><p>The general solution for the Laplacian eigenvalue problem for a 2D rectangle with Dirichlet boundary conditions is (<xref ref-type="bibr" rid="bib83">Strauss, 2007</xref>)<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> are the sizes of the domain for each dimension.</p><p>To calculate the covariance for <italic>R</italic><sub>2</sub>, we need to take into account the potentially different receptor density and response variance for this region. Denoting the ratio of the response variances between both regions by <inline-formula><mml:math id="inf77"><mml:mi>a</mml:mi></mml:math></inline-formula>, and the ratio of receptor densities by <inline-formula><mml:math id="inf78"><mml:mi>d</mml:mi></mml:math></inline-formula>, the covariance for <italic>R</italic><sub>2</sub> can be expressed as<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>γ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It can be seen that <inline-formula><mml:math id="inf79"><mml:mi>a</mml:mi></mml:math></inline-formula> scales the overall covariance matrix (see also <xref ref-type="fig" rid="fig1">Figure 1C</xref>), while <inline-formula><mml:math id="inf80"><mml:mi>d</mml:mi></mml:math></inline-formula> changes the spatial extent of the correlations and thereby implicitly accounts for the different receptor density.</p><p>The calculation of the eigenvalues for <italic>R</italic><sub>2</sub> proceeds analogously. The eigenvalue–eigenvector problem is given as<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:msubsup><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the receptor density ratio <inline-formula><mml:math id="inf81"><mml:mi>d</mml:mi></mml:math></inline-formula> causes an implicit stretching of the space for the higher density region, the region length <inline-formula><mml:math id="inf82"><mml:mi>L</mml:mi></mml:math></inline-formula> needs to be adjusted in order to keep the effective size of the region constant. In 2D, each axis is therefore scaled by <inline-formula><mml:math id="inf83"><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:math></inline-formula>, resulting in an upper integration limit of <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-4"><title>Allocation in the bottleneck</title><p>Given a sensory system with limited representational capacity, different regions may be allocated different amounts of resources. Here, we calculate the allocations over different bottleneck widths for two regions, while the extension to multiple regions is given in Appendix 5. In the following, we assume 2D square regions of equal size for ease of analysis (see Appendix 6 for the equivalent solution in 1D). A single variable <inline-formula><mml:math id="inf85"><mml:mi>L</mml:mi></mml:math></inline-formula> is therefore used to denote the lengths of the squares. Following <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, the eigenvalues for regions <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub> can now be calculated as<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> enumerate different eigenvalues for regions <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, respectively.</p><p>In order to calculate how many output neurons are allocated to <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub> for different bottleneck widths, we will need to establish an ordering of the eigenvalues, such that for each pair <inline-formula><mml:math id="inf88"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> we can determine the sorted rank of the eigenvalues. In contrast to the 1D case (see Appendix 6), there is no natural ordering of the eigenvalues in two dimensions; however, a close approximation can be obtained by calculating the number of lattice points enclosed by a quarter circle with radius <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (see Appendix 4 for full details). Denoting this function as <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and setting <inline-formula><mml:math id="inf91"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf92"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we can then calculate the number of eigenvalues allocated to <italic>R</italic><sub>1</sub> as a function of the number of neurons allocated to <italic>R</italic><sub>2</sub>, by setting <inline-formula><mml:math id="inf93"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and solving for <inline-formula><mml:math id="inf94"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. This yields<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As we allocate more neurons to region <italic>R</italic><sub>1</sub>, the ratio <inline-formula><mml:math id="inf95"><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula> simplifies to <inline-formula><mml:math id="inf96"><mml:mrow><mml:mrow><mml:msub><mml:mo>lim</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. The fraction of neurons allocated to each region therefore depends on the size of the bottleneck and converges to <inline-formula><mml:math id="inf97"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="inf98"><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula> for <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, respectively.</p></sec><sec id="s4-5"><title>Alternative covariance functions</title><p>To test generalization to other covariance functions that decrease monotonically with receptor distance, we tested a number of functions from the Matérn class, in which the parameter ν controls the smoothness of the function. The negative exponential covariance function we employed in previous sections is equivalent to a Matérn function with <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Larger values of ν lead to progressively higher correlations for smaller distances and lower correlations for larger distances. Specifically, we tested a Matérn function with <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and with <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mpadded depth="-0.3em" height="+0.3em" voffset="0.3em"><mml:mn mathsize="70%">5</mml:mn></mml:mpadded><mml:mpadded lspace="-0.1em" width="-0.15em"><mml:mo stretchy="true">/</mml:mo></mml:mpadded><mml:mn mathsize="70%">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msqrt><mml:mn>5</mml:mn></mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>5</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mn>3</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:msqrt><mml:mn>5</mml:mn></mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>For simplicity and to obtain numerically stable solutions, all calculations were performed for 1D regions only. For all cases tests, the baseline region contained 500 receptors, while the number of receptors for the other region was determined by the density ratio. Eigenvalues were calculated from the covariance matrix using the <italic>eigh</italic> method of the <italic>numpy</italic> Python package (<xref ref-type="bibr" rid="bib48">Harris et al., 2006</xref>). Comparing the numerically obtained allocation for the negative exponential covariance function with its analytical solutions showed a close match for most bottleneck widths. However, numerical errors increased for wider bottlenecks, where eigenvalues became very small and their decay flattened out, affecting the sorting process. For the analyses here, we therefore restricted ourselves to bottleneck widths where a close match between the numerical and analytical solutions could be obtained. This range was sufficient to demonstrate allocation at narrow bottlenecks and estimate the convergence point for larger ones.</p></sec><sec id="s4-6"><title>Calculations for star-nosed mole</title><p>The 11 rays were approximated as 2D square regions with areas set to their reported sizes (<xref ref-type="bibr" rid="bib76">Sawyer and Catania, 2016</xref>). Receptor densities for each ray were calculated as the sensory fibre innervation per mm<sup>2</sup> (<xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>). Approximations of receptor activation on each ray were calculated from empirical data of prey foraging interactions recorded by <xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>. Contact statistics were converted to receptor activation probabilities with receptors following a Bernoulli distribution. Finally, activation variance was calculated as the variance of the Bernoulli distribution (see Appendix 1). The decay rate γ of the negative exponential covariance function was determined for each ray using a model of the typical extent of receptor activation during interaction with prey stimuli of varying sizes. Each ray interacts with varying prey sizes at different frequencies. For example, ray 11 is typically contacted by smaller stimuli more often than other rays. A 2D model of the rays was used to simulate average responses to each stimulus size. Each model ray was tiled with receptors, and circular stimuli of different sizes were then randomly placed over the ray. The radii and frequencies of each stimulus size were based on the prey model (<xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>). A ray receptor was marked as active if its coordinate position was within the bounds of the stimuli. Response covariance between receptors was then calculated and an exponential function was fit to find the γ decay parameter. See <xref ref-type="table" rid="table1">Table 1</xref> for the full set of parameters. The code implementing the receptor model is available on GitHub (<xref ref-type="bibr" rid="bib33">Edmondson, 2021</xref>). To determine allocations, eigenvalues associated with each ray were calculated analytically, resulting in allocations for each ray at all bottleneck widths. Three models were compared: first, a ‘density-only’ model, which includes accurate receptor density ratios, but receptor activation ratio remains uniform across all rays; second, an ‘activation-only’ model, which includes heterogeneous receptor activation ratios, but uniform receptor density ratios across all rays; finally, the ‘full model’ combines both accurate densities and receptor activation ratios. Model allocations for each ray were compared to the cortical allocation empirical data from <xref ref-type="bibr" rid="bib15">Catania and Kaas, 1997</xref>. As the bottleneck size for the star-nosed mole is unknown, the root-mean-square error (RMSE) was calculated for each model at all bottleneck widths. The bottleneck resulting in the lowest error was then selected for each. Allocations for rays at each bottleneck can be found in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>. For the ‘activation-only’ and ‘full’ models, the lowest RMSE values were for bottleneck widths of between 37 and 45%; for the ‘density-only’ model, the RMSE was similar over all bottlenecks widths (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref>). We also tested a ‘baseline model’ where densities and activation were randomly selected for each ray within the possible range of parameters. The aim was to determine how much explained variance within the cortical allocations was due to the selection of the best-fitting bottleneck and how much was due to the specific density and activation parameters. A total of 20 random models were run, and the average <inline-formula><mml:math id="inf102"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> was –0.09 (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1D</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Model parameters for the star-nosed mole.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Ray</th><th align="left" valign="bottom">1</th><th align="left" valign="bottom">2</th><th align="left" valign="bottom">3</th><th align="left" valign="bottom">4</th><th align="left" valign="bottom">5</th><th align="left" valign="bottom">6</th><th align="left" valign="bottom">7</th><th align="left" valign="bottom">8</th><th align="left" valign="bottom">9</th><th align="left" valign="bottom">10</th><th align="left" valign="bottom">11</th></tr></thead><tbody><tr><td align="left" valign="bottom">Size</td><td align="char" char="." valign="bottom">1.14</td><td align="char" char="." valign="bottom">1.21</td><td align="char" char="." valign="bottom">1.21</td><td align="char" char="." valign="bottom">1.17</td><td align="char" char="." valign="bottom">1.08</td><td align="char" char="." valign="bottom">1.02</td><td align="char" char="." valign="bottom">1.00</td><td align="char" char="." valign="bottom">1.13</td><td align="char" char="." valign="bottom">1.05</td><td align="char" char="." valign="bottom">0.87</td><td align="char" char="." valign="bottom">1.10</td></tr><tr><td align="left" valign="bottom">Density</td><td align="char" char="." valign="bottom">45.78</td><td align="char" char="." valign="bottom">47.14</td><td align="char" char="." valign="bottom">45.82</td><td align="char" char="." valign="bottom">45.69</td><td align="char" char="." valign="bottom">45.2</td><td align="char" char="." valign="bottom">46.91</td><td align="char" char="." valign="bottom">43.3</td><td align="char" char="." valign="bottom">44.01</td><td align="char" char="." valign="bottom">44.26</td><td align="char" char="." valign="bottom">47.92</td><td align="char" char="." valign="bottom">50.46</td></tr><tr><td align="left" valign="bottom">Activation</td><td align="char" char="." valign="bottom">0.03</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">0.04</td><td align="char" char="." valign="bottom">0.08</td><td align="char" char="." valign="bottom">0.11</td></tr><tr><td align="left" valign="bottom">γ</td><td align="char" char="." valign="bottom">0.99</td><td align="char" char="." valign="bottom">1.00</td><td align="char" char="." valign="bottom">1.02</td><td align="char" char="." valign="bottom">1.01</td><td align="char" char="." valign="bottom">1.00</td><td align="char" char="." valign="bottom">1.01</td><td align="char" char="." valign="bottom">0.99</td><td align="char" char="." valign="bottom">1.04</td><td align="char" char="." valign="bottom">1.10</td><td align="char" char="." valign="bottom">1.16</td><td align="char" char="." valign="bottom">1.27</td></tr></tbody></table></table-wrap></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Validation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Methodology, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-70777-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>No data was generated for this study. All equations and model parameters are included in the manuscript and supporting files. Additionally, code implementing the model equations has been made available on Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/lauraredmondson/expansion_contraction_sensory_bottlenecks">https://github.com/lauraredmondson/expansion_contraction_sensory_bottlenecks</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f4762d4e8991d225d00e0e197b0a06cef7f75986;origin=https://github.com/lauraredmondson/expansion_contraction_sensory_bottlenecks;visit=swh:1:snp:3306c00ba708467c7e0be64d6f8a42dcf75d1133;anchor=swh:1:rev:dd6de7c05ae9443d034361b042b053b4f40717f5">swh:1:rev:dd6de7c05ae9443d034361b042b053b4f40717f5</ext-link>) (see also Methods section in manuscript).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Wellcome Trust (209998/Z/17/Z) and the European Union Horizon 2020 programme as part of the Human Brain Project (HBP-SGA2, 785907).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>SJ</given-names></name><name><surname>Mullen</surname><given-names>KT</given-names></name><name><surname>Hess</surname><given-names>RF</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Human peripheral spatial resolution for achromatic and chromatic stimuli: limits imposed by optical and retinal factors</article-title><source>The Journal of Physiology</source><volume>442</volume><fpage>47</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1991.sp018781</pub-id><pub-id pub-id-type="pmid">1798037</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Redlich</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Towards a theory of early visual processing</article-title><source>Neural Computation</source><volume>2</volume><fpage>308</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1162/neco.1990.2.3.308</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Could information theory provide an ecological theory of sensory processing?</article-title><source>Network</source><volume>3</volume><fpage>213</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_3_2_009</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Redlich</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>What does the retina know about natural scenes?</article-title><source>Neural Computation</source><volume>4</volume><fpage>196</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.2.196</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><volume>61</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1037/h0054663</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azzopardi</surname><given-names>P</given-names></name><name><surname>Cowey</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Preferential representation of the fovea in the primary visual cortex</article-title><source>Nature</source><volume>361</volume><fpage>719</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1038/361719a0</pub-id><pub-id pub-id-type="pmid">7680108</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory Communication</source><volume>1</volume><fpage>217</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The “independent components” of natural scenes are edge filters</article-title><source>Vision Research</source><volume>37</volume><fpage>3327</fpage><lpage>3338</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00121-1</pub-id><pub-id pub-id-type="pmid">9425547</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Gerwinn</surname><given-names>S</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reassessing optimal neural population codes with neurometric functions</article-title><source>PNAS</source><volume>108</volume><fpage>4423</fpage><lpage>4428</lpage><pub-id pub-id-type="doi">10.1073/pnas.1015904108</pub-id><pub-id pub-id-type="pmid">21368193</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Rotermund</surname><given-names>D</given-names></name><name><surname>Pawelzik</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Optimal short-term population coding: when fisher information fails</article-title><source>Neural Computation</source><volume>14</volume><fpage>2317</fpage><lpage>2351</lpage><pub-id pub-id-type="doi">10.1162/08997660260293247</pub-id><pub-id pub-id-type="pmid">12396565</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhand</surname><given-names>M</given-names></name><name><surname>Mudur</surname><given-names>R</given-names></name><name><surname>Suresh</surname><given-names>B</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brinkman</surname><given-names>BAW</given-names></name><name><surname>Weber</surname><given-names>AI</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How do efficient coding strategies depend on origins of noise in neural circuits?</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005150</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005150</pub-id><pub-id pub-id-type="pmid">27741248</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A little man of some importance</article-title><source>Brain</source><volume>140</volume><fpage>3055</fpage><lpage>3061</lpage><pub-id pub-id-type="doi">10.1093/brain/awx270</pub-id><pub-id pub-id-type="pmid">29088352</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catania</surname><given-names>KC</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Organization of the somatosensory cortex of the star-nosed mole</article-title><source>The Journal of Comparative Neurology</source><volume>351</volume><fpage>549</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1002/cne.903510406</pub-id><pub-id pub-id-type="pmid">7721983</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catania</surname><given-names>KC</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Somatosensory fovea in the star-nosed mole</article-title><source>Journal of Comparative Neurology</source><volume>387</volume><fpage>215</fpage><lpage>233</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catania</surname><given-names>KC</given-names></name><name><surname>Remple</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Tactile foveation in the star-nosed mole</article-title><source>Brain, Behavior and Evolution</source><volume>63</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1159/000073755</pub-id><pub-id pub-id-type="pmid">15249650</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catania</surname><given-names>KC</given-names></name><name><surname>Leitch</surname><given-names>DB</given-names></name><name><surname>Gauthier</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A star in the brainstem reveals the first step of cortical magnification</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e22406</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0022406</pub-id><pub-id pub-id-type="pmid">21811600</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catania</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>All in the family - touch versus olfaction in moles</article-title><source>Anatomical Record</source><volume>303</volume><fpage>65</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1002/ar.24057</pub-id><pub-id pub-id-type="pmid">30614659</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalk</surname><given-names>M</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Toward a unified theory of efficient, predictive, and sparse coding</article-title><source>PNAS</source><volume>115</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1073/pnas.1711114115</pub-id><pub-id pub-id-type="pmid">29259111</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chechik</surname><given-names>G</given-names></name><name><surname>Anderson</surname><given-names>MJ</given-names></name><name><surname>Bar-Yosef</surname><given-names>O</given-names></name><name><surname>Young</surname><given-names>ED</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reduction of information redundancy in the ascending auditory pathway</article-title><source>Neuron</source><volume>51</volume><fpage>359</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.030</pub-id><pub-id pub-id-type="pmid">16880130</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clemens</surname><given-names>J</given-names></name><name><surname>Kutzki</surname><given-names>O</given-names></name><name><surname>Ronacher</surname><given-names>B</given-names></name><name><surname>Schreiber</surname><given-names>S</given-names></name><name><surname>Wohlgemuth</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Efficient transformation of an auditory population code in a small sensory system</article-title><source>PNAS</source><volume>108</volume><fpage>13812</fpage><lpage>13817</lpage><pub-id pub-id-type="doi">10.1073/pnas.1104506108</pub-id><pub-id pub-id-type="pmid">21825132</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coq</surname><given-names>JO</given-names></name><name><surname>Xerri</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Environmental enrichment alters organizational features of the forepaw representation in the primary somatosensory cortex of adult rats</article-title><source>Experimental Brain Research</source><volume>121</volume><fpage>191</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1007/s002210050452</pub-id><pub-id pub-id-type="pmid">9696389</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corniani</surname><given-names>G</given-names></name><name><surname>Saal</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Tactile innervation densities across the whole body</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>1229</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1152/jn.00313.2020</pub-id><pub-id pub-id-type="pmid">32965159</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname><given-names>CA</given-names></name><name><surname>Allen</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Topography of ganglion cells in human retina</article-title><source>The Journal of Comparative Neurology</source><volume>300</volume><fpage>5</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/cne.903000103</pub-id><pub-id pub-id-type="pmid">2229487</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname><given-names>CA</given-names></name><name><surname>Sloan</surname><given-names>KR</given-names></name><name><surname>Kalina</surname><given-names>RE</given-names></name><name><surname>Hendrickson</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Human photoreceptor topography</article-title><source>The Journal of Comparative Neurology</source><volume>292</volume><fpage>497</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1002/cne.902920402</pub-id><pub-id pub-id-type="pmid">2324310</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Doi</surname><given-names>E</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Relations between the Statistical Regularities of Natural Images and the Response Properties of the Early Visual System</source><publisher-name>Japanese Cognitive Science Society, SIG P&amp;P</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doi</surname><given-names>E</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Jepson</surname><given-names>LH</given-names></name><name><surname>Mathieson</surname><given-names>K</given-names></name><name><surname>Gunning</surname><given-names>DE</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient coding of spatial information in the primate retina</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>16256</fpage><lpage>16264</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4036-12.2012</pub-id><pub-id pub-id-type="pmid">23152609</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doi</surname><given-names>E.</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A simple model of optimal population coding for sensory systems</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003761</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003761</pub-id><pub-id pub-id-type="pmid">25121492</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolman</surname><given-names>CL</given-names></name><name><surname>McCormick</surname><given-names>AQ</given-names></name><name><surname>Drance</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Aging of the optic nerve</article-title><source>Archives of Ophthalmology</source><volume>98</volume><fpage>2053</fpage><lpage>2058</lpage><pub-id pub-id-type="doi">10.1001/archopht.1980.01020040905024</pub-id><pub-id pub-id-type="pmid">7436843</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>RO</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical magnification within human primary visual cortex correlates with acuity thresholds</article-title><source>Neuron</source><volume>38</volume><fpage>659</fpage><lpage>671</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00265-4</pub-id><pub-id pub-id-type="pmid">12765616</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>RO</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Tactile hyperacuity thresholds correlate with finger maps in primary somatosensory cortex (s1)</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2878</fpage><lpage>2891</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm015</pub-id><pub-id pub-id-type="pmid">17372277</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Edmondson</surname><given-names>LR</given-names></name><name><surname>Rodriguez</surname><given-names>AJ</given-names></name><name><surname>Saal</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nonlinear scaling of resource allocation in sensory bottlenecks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>7545</fpage><lpage>7554</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Edmondson</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Expansion_contraction_sensory_bottlenecks</data-title><version designator="5483347">5483347</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/lauraredmondson/expansion_contraction_sensory_bottlenecks">https://github.com/lauraredmondson/expansion_contraction_sensory_bottlenecks</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>SA</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Retinotopic organization in human visual cortex and the spatial precision of functional mri</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id><pub-id pub-id-type="pmid">9087826</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finlay</surname><given-names>BL</given-names></name><name><surname>Franco</surname><given-names>ECS</given-names></name><name><surname>Yamada</surname><given-names>ES</given-names></name><name><surname>Crowley</surname><given-names>JC</given-names></name><name><surname>Parsons</surname><given-names>M</given-names></name><name><surname>Muniz</surname><given-names>JAPC</given-names></name><name><surname>Silveira</surname><given-names>LCL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Number and topography of cones, rods and optic nerve axons in new and old world primates</article-title><source>Visual Neuroscience</source><volume>25</volume><fpage>289</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1017/S0952523808080371</pub-id><pub-id pub-id-type="pmid">18598400</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fragaszy</surname><given-names>DM</given-names></name><name><surname>Crast</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Functions of the hand in primates</chapter-title><person-group person-group-type="editor"><name><surname>Fragaszy</surname><given-names>DM</given-names></name></person-group><source>The Evolution of the Primate Hand</source><publisher-name>Springer</publisher-name><fpage>313</fpage><lpage>344</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>D</given-names></name><name><surname>Simoncelli</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Implicit encoding of prior probabilities in optimal neural populations</article-title><source>Advances in Neural Information Processing Systems</source><volume>23</volume><fpage>658</fpage><lpage>666</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>D</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Efficient sensory encoding and bayesian inference with heterogeneous neural populations</article-title><source>Neural Computation</source><volume>26</volume><fpage>2103</fpage><lpage>2134</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id><pub-id pub-id-type="pmid">25058702</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>D</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural and Perceptual Signatures of Efficient Sensory Coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.00058">https://arxiv.org/abs/1603.00058</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>García-Piqueras</surname><given-names>J</given-names></name><name><surname>García-Mesa</surname><given-names>Y</given-names></name><name><surname>Cárcaba</surname><given-names>L</given-names></name><name><surname>Feito</surname><given-names>J</given-names></name><name><surname>Torres-Parejo</surname><given-names>I</given-names></name><name><surname>Martín-Biedma</surname><given-names>B</given-names></name><name><surname>Cobo</surname><given-names>J</given-names></name><name><surname>García-Suárez</surname><given-names>O</given-names></name><name><surname>Vega</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ageing of the somatosensory system at the periphery: age-related changes in cutaneous mechanoreceptors</article-title><source>Journal of Anatomy</source><volume>234</volume><fpage>839</fpage><lpage>852</lpage><pub-id pub-id-type="doi">10.1111/joa.12983</pub-id><pub-id pub-id-type="pmid">30924930</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Godde</surname><given-names>B</given-names></name><name><surname>Stauffenberg</surname><given-names>B</given-names></name><name><surname>Spengler</surname><given-names>F</given-names></name><name><surname>Dinse</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Tactile coactivation-induced changes in spatial discrimination performance</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>1597</fpage><lpage>1604</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-04-01597.2000</pub-id><pub-id pub-id-type="pmid">10662849</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>F</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Bachta</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Analysis of hand contact areas and interaction capabilities during manipulation and exploration</article-title><source>IEEE Transactions on Haptics</source><volume>7</volume><fpage>415</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1109/TOH.2014.2321395</pub-id><pub-id pub-id-type="pmid">25532147</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodchild</surname><given-names>AK</given-names></name><name><surname>Ghosh</surname><given-names>KK</given-names></name><name><surname>Martin</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Comparison of photoreceptor spatial density and ganglion cell morphology in the retina of human, macaque monkey, cat, and the marmoset callithrix jacchus</article-title><source>The Journal of Comparative Neurology</source><volume>366</volume><fpage>55</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19960226)366:1&lt;55::AID-CNE5&gt;3.0.CO;2-J</pub-id><pub-id pub-id-type="pmid">8866846</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodwin</surname><given-names>AW</given-names></name><name><surname>Wheat</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sensory signals in neural populations underlying tactile perception and manipulation</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>53</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131032</pub-id><pub-id pub-id-type="pmid">15217326</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>DJ</given-names></name><name><surname>Chandler</surname><given-names>DM</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Can the theory of “whitening” explain the center-surround properties of retinal ganglion cell receptive fields?</article-title><source>Vision Research</source><volume>46</volume><fpage>2901</fpage><lpage>2913</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.03.008</pub-id><pub-id pub-id-type="pmid">16782164</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>D</given-names></name><name><surname>Field</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Natural images: coding efficiency</article-title><source>Encyclopedia of Neuroscience</source><volume>6</volume><fpage>19</fpage><lpage>27</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname><given-names>PJB</given-names></name><name><surname>Baddeley</surname><given-names>RJ</given-names></name><name><surname>Smith</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The principal components of natural images</article-title><source>Network</source><volume>3</volume><fpage>61</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_3_1_008</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Array programming with numpy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The relationship between cortical magnification factor and population receptive field size in human visual cortex: constancies in cortical architecture</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13604</fpage><lpage>13612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2572-11.2011</pub-id><pub-id pub-id-type="pmid">21940451</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Conte</surname><given-names>MM</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Variance predicts salience in central sensory processing</article-title><source>eLife</source><volume>3</volume><elocation-id>e03722</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03722</pub-id><pub-id pub-id-type="pmid">25396297</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huffman</surname><given-names>KJ</given-names></name><name><surname>Molnár</surname><given-names>Z</given-names></name><name><surname>Van Dellen</surname><given-names>A</given-names></name><name><surname>Kahn</surname><given-names>DM</given-names></name><name><surname>Blakemore</surname><given-names>C</given-names></name><name><surname>Krubitzer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Formation of cortical fields on a reduced cortical sheet</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>9939</fpage><lpage>9952</lpage><pub-id pub-id-type="pmid">10559402</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Oja</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Independent component analysis: algorithms and applications</article-title><source>Neural Networks</source><volume>13</volume><fpage>411</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(00)00026-5</pub-id><pub-id pub-id-type="pmid">10946390</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkins</surname><given-names>WM</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Ochs</surname><given-names>MT</given-names></name><name><surname>Allard</surname><given-names>T</given-names></name><name><surname>Guíc-Robles</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Functional reorganization of primary somatosensory cortex in adult owl monkeys after behaviorally controlled tactile stimulation</article-title><source>Journal of Neurophysiology</source><volume>63</volume><fpage>82</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1152/jn.1990.63.1.82</pub-id><pub-id pub-id-type="pmid">2299388</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname><given-names>RS</given-names></name><name><surname>Vallbo</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Tactile sensibility in the human hand: relative and absolute densities of four types of mechanoreceptive units in glabrous skin</article-title><source>The Journal of Physiology</source><volume>286</volume><fpage>283</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1979.sp012619</pub-id><pub-id pub-id-type="pmid">439026</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonas</surname><given-names>JB</given-names></name><name><surname>Müller-Bergh</surname><given-names>JA</given-names></name><name><surname>Schlötzer-Schrehardt</surname><given-names>UM</given-names></name><name><surname>Naumann</surname><given-names>GO</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Histomorphometry of the human optic nerve</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>31</volume><fpage>736</fpage><lpage>744</lpage><pub-id pub-id-type="pmid">2335441</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Karklin</surname><given-names>Y</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Efficient coding of natural images with a population of noisy linear-nonlinear neurons</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>999</fpage><lpage>1007</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Predictability and redundancy of natural images</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>4</volume><fpage>2395</fpage><lpage>2400</lpage><pub-id pub-id-type="doi">10.1364/josaa.4.002395</pub-id><pub-id pub-id-type="pmid">3430226</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lehnert</surname><given-names>BP</given-names></name><name><surname>Santiago</surname><given-names>C</given-names></name><name><surname>Huey</surname><given-names>EL</given-names></name><name><surname>Emanuel</surname><given-names>AJ</given-names></name><name><surname>Renauld</surname><given-names>S</given-names></name><name><surname>Africawala</surname><given-names>N</given-names></name><name><surname>Alkislar</surname><given-names>I</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Bai</surname><given-names>L</given-names></name><name><surname>Koutsioumpa</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>JT</given-names></name><name><surname>Magee</surname><given-names>AR</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Ginty</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mechanoreceptor Synapses in the Brainstem Shape the Central Representation of Touch</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.02.02.429463</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Efficient coding of natural sounds</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>356</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nn831</pub-id><pub-id pub-id-type="pmid">11896400</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Deny</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A Unified Theory Of Early Visual Representations From Retina To Cortex Through Anatomically Constrained Deep CNNs</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/511535</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ly</surname><given-names>C</given-names></name><name><surname>Middleton</surname><given-names>JW</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cellular and circuit mechanisms maintain low spike co-variability and enhance population coding in somatosensory cortex</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00007</pub-id><pub-id pub-id-type="pmid">22408615</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martuzzi</surname><given-names>R</given-names></name><name><surname>van der Zwaag</surname><given-names>W</given-names></name><name><surname>Farthouat</surname><given-names>J</given-names></name><name><surname>Gruetter</surname><given-names>R</given-names></name><name><surname>Blanke</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human finger somatotopy in areas 3b, 1, and 2: a 7t fmri study using a natural stimulus</article-title><source>Human Brain Mapping</source><volume>35</volume><fpage>213</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1002/hbm.22172</pub-id><pub-id pub-id-type="pmid">22965769</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Jenkins</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Reorganization of cortical representations of the hand following alterations of skin inputs induced by nerve injury, skin island transfers, and experience</article-title><source>Journal of Hand Therapy</source><volume>6</volume><fpage>89</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1016/s0894-1130(12)80290-0</pub-id><pub-id pub-id-type="pmid">8393727</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>RJ</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Representations of the body surface in postcentral parietal cortex of macaca fascicularis</article-title><source>The Journal of Comparative Neurology</source><volume>192</volume><fpage>611</fpage><lpage>643</lpage><pub-id pub-id-type="doi">10.1002/cne.901920402</pub-id><pub-id pub-id-type="pmid">7419747</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Wavelet-like receptive fields emerge from a network that learns sparse codes for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: A strategy employed by v1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sparse coding of sensory inputs</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>481</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.07.007</pub-id><pub-id pub-id-type="pmid">15321069</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>VH</given-names></name><name><surname>Oehler</surname><given-names>R</given-names></name><name><surname>Cowey</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Retinal ganglion cells that project to the dorsal lateral geniculate nucleus in the macaque monkey</article-title><source>Neuroscience</source><volume>12</volume><fpage>1101</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(84)90006-x</pub-id><pub-id pub-id-type="pmid">6483193</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>RM</given-names></name><name><surname>Hackeman</surname><given-names>E</given-names></name><name><surname>Goldreich</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Diminutive digits discern delicate details: fingertip size and the sex difference in tactile spatial acuity</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>15756</fpage><lpage>15761</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3684-09.2009</pub-id><pub-id pub-id-type="pmid">20016091</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decorrelation and efficient coding by retinal ganglion cells</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>628</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nn.3064</pub-id><pub-id pub-id-type="pmid">22406548</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plumbley</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Do cortical maps adapt to optimize information density?</article-title><source>Network</source><volume>10</volume><fpage>41</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1088/0954-898X/10/1/003</pub-id><pub-id pub-id-type="pmid">10372761</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provis</surname><given-names>JM</given-names></name><name><surname>van Driel</surname><given-names>D</given-names></name><name><surname>Billson</surname><given-names>FA</given-names></name><name><surname>Russell</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Human fetal optic nerve: overproduction and elimination of retinal axons during development</article-title><source>The Journal of Comparative Neurology</source><volume>238</volume><fpage>92</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1002/cne.902380108</pub-id><pub-id pub-id-type="pmid">4044906</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritter</surname><given-names>H</given-names></name><name><surname>Schulten</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>On the stationary state of kohonen’s self-organizing sensory mapping</article-title><source>Biological Cybernetics</source><volume>54</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1007/BF00320480</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ritter</surname><given-names>H</given-names></name><name><surname>Martinetz</surname><given-names>T</given-names></name><name><surname>Schulten</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Neural Computation and Self-Organizing Maps: An Introduction</source><publisher-name>Addison-Wesley</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sandell</surname><given-names>JH</given-names></name><name><surname>Peters</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Effects of age on nerve fibers in the rhesus monkey optic nerve</article-title><source>The Journal of Comparative Neurology</source><volume>429</volume><fpage>541</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1002/1096-9861(20010122)429:4&lt;541::aid-cne3&gt;3.0.co;2-5</pub-id><pub-id pub-id-type="pmid">11135234</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawyer</surname><given-names>EK</given-names></name><name><surname>Catania</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Somatosensory organ topography across the star of the star-nosed mole (condylura cristata)</article-title><source>The Journal of Comparative Neurology</source><volume>524</volume><fpage>917</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1002/cne.23943</pub-id><pub-id pub-id-type="pmid">26659700</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sefton</surname><given-names>AJ</given-names></name><name><surname>Horsburgh</surname><given-names>GM</given-names></name><name><surname>Lam</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>The development of the optic nerve in rodents</article-title><source>Australian and New Zealand Journal of Ophthalmology</source><volume>13</volume><fpage>135</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1111/j.1442-9071.1985.tb00414.x</pub-id><pub-id pub-id-type="pmid">4052262</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science</source><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1126/science.7754376</pub-id><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>3870</fpage><lpage>3896</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-10-03870.1998</pub-id><pub-id pub-id-type="pmid">9570816</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>GF</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Differential Equations with Applications and Historical Notes</source><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>EC</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient auditory coding</article-title><source>Nature</source><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strauss</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Partial Differential Equations: An Introduction</source><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sur</surname><given-names>M</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Magnification, receptive-field area, and “hypercolumn” size in areas 3b and 1 of somatosensory cortex in owl monkeys</article-title><source>Journal of Neurophysiology</source><volume>44</volume><fpage>295</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1152/jn.1980.44.2.295</pub-id><pub-id pub-id-type="pmid">7411189</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Terashima</surname><given-names>H</given-names></name><name><surname>Okada</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>The Topographic Unsupervised Learning of Natural Sounds in the Auditory Cortex, Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tishby</surname><given-names>N</given-names></name><name><surname>Pereira</surname><given-names>FC</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The Information Bottleneck Method</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/physics/0004057">https://arxiv.org/abs/physics/0004057</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tishby</surname><given-names>N</given-names></name><name><surname>Zaslavsky</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning and the information bottleneck principle</article-title><conf-name>2015 IEEE Information Theory Workshop (ITW)</conf-name><pub-id pub-id-type="doi">10.1109/ITW.2015.7133169</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Boven</surname><given-names>RW</given-names></name><name><surname>Hamilton</surname><given-names>RH</given-names></name><name><surname>Kauffman</surname><given-names>T</given-names></name><name><surname>Keenan</surname><given-names>JP</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Tactile spatial resolution in blind braille readers</article-title><source>Neurology</source><volume>54</volume><fpage>2230</fpage><lpage>2236</lpage><pub-id pub-id-type="doi">10.1212/wnl.54.12.2230</pub-id><pub-id pub-id-type="pmid">10881245</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verendeev</surname><given-names>A</given-names></name><name><surname>Thomas</surname><given-names>C</given-names></name><name><surname>McFarlin</surname><given-names>SC</given-names></name><name><surname>Hopkins</surname><given-names>WD</given-names></name><name><surname>Phillips</surname><given-names>KA</given-names></name><name><surname>Sherwood</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Comparative analysis of meissner’s corpuscles in the fingertips of primates</article-title><source>Journal of Anatomy</source><volume>227</volume><fpage>72</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1111/joa.12327</pub-id><pub-id pub-id-type="pmid">26053332</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title><source>Science</source><volume>287</volume><fpage>1273</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1126/science.287.5456.1273</pub-id><pub-id pub-id-type="pmid">10678835</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>XX</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A bayesian observer model constrained by efficient coding can explain “anti-bayesian” percepts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1038/nn.4105</pub-id><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wells-Gray</surname><given-names>EM</given-names></name><name><surname>Choi</surname><given-names>SS</given-names></name><name><surname>Bries</surname><given-names>A</given-names></name><name><surname>Doble</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Variation in rod and cone density from the fovea to the mid-periphery in healthy human retinas using adaptive optics scanning laser ophthalmoscopy</article-title><source>Eye</source><volume>30</volume><fpage>1135</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1038/eye.2016.107</pub-id><pub-id pub-id-type="pmid">27229708</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>M</given-names></name><name><surname>Peters</surname><given-names>RM</given-names></name><name><surname>Goldreich</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A physical constraint on perceptual learning: tactile spatial acuity improves with training to A limit set by finger size</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>9345</fpage><lpage>9352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0514-13.2013</pub-id><pub-id pub-id-type="pmid">23719803</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xerri</surname><given-names>C</given-names></name><name><surname>Coq</surname><given-names>JO</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Jenkins</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Experience-induced plasticity of cutaneous maps in the primary somatosensory cortex of adult monkeys and rats</article-title><source>Journal of Physiology, Paris</source><volume>90</volume><fpage>277</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1016/s0928-4257(97)81438-6</pub-id><pub-id pub-id-type="pmid">9116682</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xerri</surname><given-names>C</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Jenkins</surname><given-names>W</given-names></name><name><surname>Santucci</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Representational plasticity in cortical area 3b paralleling tactual-motor skill acquisition in adult monkeys</article-title><source>Cerebral Cortex</source><volume>9</volume><fpage>264</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1093/cercor/9.3.264</pub-id><pub-id pub-id-type="pmid">10355907</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>S</given-names></name><name><surname>Challis</surname><given-names>E</given-names></name><name><surname>Seriès</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fisher and shannon information in finite neural populations</article-title><source>Neural Computation</source><volume>24</volume><fpage>1740</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00292</pub-id><pub-id pub-id-type="pmid">22428594</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yerxa</surname><given-names>TE</given-names></name><name><surname>Kee</surname><given-names>E</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Cooper</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient sensory coding of multidimensional stimuli</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008146</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008146</pub-id><pub-id pub-id-type="pmid">32970679</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Stimulus statistics and response variance</title><p>Decorrelation works on second-order statistics and therefore stimulus statistics would only be taken into account by the model if they affect the covariance matrix. One way this can happen is through the extent of the spatial correlations (parameter γ in the covariance function). For example, in touch the size distribution of stimuli that would typically make contact with a given skin region might differ, leading to a different correlational structure. While we calculate allocations for different values of γ, we keep this value fixed across different input regions, for simplicity.</p><p>A more common case is that of receptors in one region being more active than receptors in another region. For example, in touch, the fingertips make contact with objects much more frequently than does the palm. An increased probability of making contact would translate into higher receptor response rates. In turn, higher response rates imply higher response variance, which would be directly reflected in the covariance matrix as a multiplicative scaling of the covariance function. For example, assuming that each receptor follows a simple Bernoulli distribution (either on or off, with a probability of <italic>p</italic> being on), then the response variance can be calculated as <inline-formula><mml:math id="inf103"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Assuming that the likelihood of any receptor being active is generally low, the variance scales almost linearly with receptor activation. Differences in activation between two regions are represented by the activation ratio <inline-formula><mml:math id="inf104"><mml:mi>a</mml:mi></mml:math></inline-formula> throughout the article.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Relationship between PCA and Laplacian eigenvalue problem</title><sec sec-type="appendix" id="s9-1"><title>Rationale</title><p>Let <inline-formula><mml:math id="inf105"><mml:mi mathvariant="normal">Ω</mml:mi></mml:math></inline-formula> be a region with a density of receptors ρ. In a 1D region <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the density can be expressed as <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> or number of receptors per unit length. Assuming an exponential decay of correlations, the covariance between receptors <inline-formula><mml:math id="inf108"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:mi>j</mml:mi></mml:math></inline-formula> is<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf110"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between receptors. Subsampling the space by taking a fraction <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> of the original receptors, <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the covariance for positions <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, becomes<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Therefore, we encode the changes in receptor density in a scaling of the exponential decay rate. For a given distribution of receptors, there is an induced partition of the interval <inline-formula><mml:math id="inf114"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, therefore, for a fixed <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the covariance in the <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> bin is approximately equal to the area of the exponential covered in that bin:<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Summing over all the bins, we arrive at the PCA problem:<disp-formula id="equ24"><label>(23)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The continuum limit is found formally as <inline-formula><mml:math id="inf117"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s9-2"><title>Derivation</title><p>In order to find the optimal assignment for a given receptor density, we are interested in solutions to the following equation which can be seen as a continuous version of the traditional PCA problem with an exponentially decaying covariance matrix:<disp-formula id="equ25"><label>(24)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where γ is the decay rate. We are interested in solutions <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℝ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, that is, twice differentiable solutions that satisfy appropriate boundary conditions.</p><p><bold>Theorem 1</bold> <italic>If</italic> <inline-formula><mml:math id="inf119"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> <italic>is a solution of</italic> <xref ref-type="disp-formula" rid="equ25">Equation 24</xref>, <italic>then it is an eigenfunction of the Laplacian operator with eigenvalues</italic>:<disp-formula id="equ26"><label>(25)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac><mml:mo>-</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><italic>That is, in one dimension, solutions</italic> <inline-formula><mml:math id="inf120"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> <italic>satisfy</italic><disp-formula id="equ27"><label>(26)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><italic>Proof</italic>: Differentiating <xref ref-type="disp-formula" rid="equ25">Equation 24</xref> twice using the Leibniz rule we obtain<disp-formula id="equ28"> <label>(27)</label><mml:math id="m28"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>x</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mtext>and</mml:mtext></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ29"><label>(28)</label><mml:math id="m29"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The second term on the right-hand side can be replaced using <xref ref-type="disp-formula" rid="equ25">Equation 24</xref> obtaining the desired result:<disp-formula id="equ30"> <label>(29)</label><mml:math id="m30"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mtext>or</mml:mtext></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ31"><label>(30)</label><mml:math id="m31"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac></mml:mstyle><mml:mo>-</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The previous is a sufficient condition on the solutions to <xref ref-type="disp-formula" rid="equ25">Equation 24</xref>. A necessary condition is given in the following theorem:</p><p><bold>Theorem 2</bold> <italic>A solution to</italic> <xref ref-type="disp-formula" rid="equ27">Equation 26</xref> <italic>is also solution to</italic> <xref ref-type="disp-formula" rid="equ25">Equation 24</xref> <italic>if it satisfies the following boundary conditions</italic>:<disp-formula id="equ32"><label>(31)</label><mml:math id="m32"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ33"><label>(32)</label><mml:math id="m33"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><italic>Proof</italic>: Assume <inline-formula><mml:math id="inf121"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is a solution to <xref ref-type="disp-formula" rid="equ27">Equation 26</xref>. We proceed by convolving <xref ref-type="disp-formula" rid="equ27">Equation 26</xref> on both sides with the kernel <inline-formula><mml:math id="inf122"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>:<disp-formula id="equ34"><label>(33)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Integrating by parts twice we get<disp-formula id="equ35"><label>(34)</label><mml:math id="m35"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Using <xref ref-type="disp-formula" rid="equ26">Equation 25</xref> and <xref ref-type="disp-formula" rid="equ33">Equation 32</xref>, we obtain<disp-formula id="equ36"><label>(35)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Repeating the procedure with the kernel <inline-formula><mml:math id="inf123"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>γ</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> in the interval <inline-formula><mml:math id="inf124"><mml:mrow><mml:mo mathvariant="normal" stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>L</mml:mi><mml:mo mathvariant="normal" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, yields<disp-formula id="equ37"><label>(36)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Adding <xref ref-type="disp-formula" rid="equ36">Equation 35</xref> and <xref ref-type="disp-formula" rid="equ37">Equation 36</xref> we recover <xref ref-type="disp-formula" rid="equ25">Equation 24</xref>, which finalizes the proof.</p></sec><sec sec-type="appendix" id="s9-3"><title>Scaling</title><p>In some instances of our problem, the exponential covariance will be scaled by the activation ratio, <italic>a</italic>. In general, the same reasoning applies to any linear combination of solutions; therefore, our results extend to that case. In particular, we have the following result:</p><p><bold>Theorem 3</bold> <italic>The eigenvalues of the scaled covariance matrix,</italic> <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo mathvariant="normal">′</mml:mo></mml:msup><mml:mo mathvariant="italic">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>y</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mi>C</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>y</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <italic>are</italic><disp-formula id="equ38"><label>(37)</label><mml:math id="m38"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p><italic>where λ is an eigenvalue of the original problem.</italic></p><p><italic>Proof</italic>: Let <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be a solution of <xref ref-type="disp-formula" rid="equ25">Equation 24</xref>. By linearity of the integral we have<disp-formula id="equ39"><label>(38)</label><mml:math id="m39"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>λ</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Solutions</title><p>In the previous section, we saw that solutions of the PCA problem (<xref ref-type="disp-formula" rid="equ25">Equation 24</xref>) and the Laplacian eigenvalue problem (<xref ref-type="disp-formula" rid="equ27">Equation 26</xref>) coincide if boundary conditions specified in <xref ref-type="disp-formula" rid="equ33">Equation 32</xref> and <xref ref-type="disp-formula" rid="equ32">Equation 31</xref> are met. Here, we show how these solutions relate to solutions of the boundary value problem of <xref ref-type="disp-formula" rid="equ27">Equation 26</xref> with <inline-formula><mml:math id="inf127"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, which correspond to the eigenmodes of an idealized vibrating string fixed at the extremes. Such modes are considerably simpler than the exact ones and, as we show, are sufficient for our analysis.</p></sec><sec sec-type="appendix" id="s11"><title>Eigenmodes of a vibrating string</title><p>Solving <xref ref-type="disp-formula" rid="equ27">Equation 26</xref>, for <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>μ</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, using standard methods (see <xref ref-type="bibr" rid="bib80">Simmons, 2016</xref>, p. 355 onwards), we find the general expression for the eigenfunctions:<disp-formula id="equ40"><label>(39)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with coefficients to be determined using the boundary conditions. The first boundary condition (<inline-formula><mml:math id="inf129"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) implies that <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The second boundary condition (<inline-formula><mml:math id="inf131"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) gives the equation <inline-formula><mml:math id="inf132"><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, which is satisfied for <inline-formula><mml:math id="inf133"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, or<disp-formula id="equ41"><label>(40)</label><mml:math id="m41"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the index of the eigenvalue. The value of <inline-formula><mml:math id="inf135"><mml:mi>A</mml:mi></mml:math></inline-formula> is arbitrary and is usually selected so the solution is normalized.</p></sec><sec sec-type="appendix" id="s12"><title>Exact eigenvalues</title><p>In order to find the exact analytical eigenvalues of <xref ref-type="disp-formula" rid="equ25">Equation 24</xref>, we again assume <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and a solution of the form given in <xref ref-type="disp-formula" rid="equ40">Equation 39</xref>. Using the first boundary condition (<xref ref-type="disp-formula" rid="equ32">Equation 31</xref>, <xref ref-type="disp-formula" rid="equ33">Equation 32</xref>) we get the following relationship:<disp-formula id="equ42"><label>(41)</label><mml:math id="m42"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>κ</mml:mi></mml:mfrac><mml:mi>B</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and with the second boundary condition, we obtain<disp-formula id="equ43"><label>(42)</label><mml:math id="m43"><mml:mrow><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>κ</mml:mi><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>;</mml:mo></mml:mrow></mml:math></disp-formula></p><p>replacing <xref ref-type="disp-formula" rid="equ42">Equation 41</xref>, we find the transcendental equation<disp-formula id="equ44"><label>(43)</label><mml:math id="m44"><mml:mrow><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>γ</mml:mi><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>κ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>;</mml:mo></mml:mrow></mml:math></disp-formula></p><p>whose solutions lead to the exact eigenvalues of the continuous PCA problem.</p></sec><sec sec-type="appendix" id="s13"><title>Relationship between exact and approximate eigenvalues</title><p>The main results presented in this study rely on the ordering of the eigenvalues only, rather than their precise magnitude. Any divergence between the exact and approximate eigenvalues is therefore relevant only if it affects this ordering. Furthermore, since allocation of eigenvalues to regions proceeds cumulatively as more eigenvalues are added, localized disturbances in a few eigenvalues will lead to only minor errors that are quickly corrected as more eigenvalues are added (since the correct allocation of eigenvalues to regions will be restored). In the following, we demonstrate that, comparing the ordering of the exact eigenvalues where the boundary conditions are derived from the integral equation (<xref ref-type="disp-formula" rid="equ44">Equation 43</xref>) with that of the approximate ones (<xref ref-type="disp-formula" rid="equ41">Equation 40</xref>), this order is altered only in a localized fashion that does not affect the analytical results of this article. To understand the changes in the ordering, consider the functions<disp-formula id="equ45"><label>(44)</label><mml:math id="m45"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ46"> <label>(45)</label><mml:math id="m46"><mml:mrow><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>whose intersection gives the solutions to <xref ref-type="disp-formula" rid="equ44">Equation 43</xref>. The function <inline-formula><mml:math id="inf137"><mml:mi>g</mml:mi></mml:math></inline-formula> has two hyperbolic-like branches, one to each side of the singularity <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula>. We can distinguish two cases for the said intersection:</p><list list-type="order"><list-item><p>For <inline-formula><mml:math id="inf139"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≫</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf140"><mml:mi>g</mml:mi></mml:math></inline-formula> approaches 0 and there is clearly only one intersection with <inline-formula><mml:math id="inf141"><mml:mi>f</mml:mi></mml:math></inline-formula>, which happens to be increasingly close to the approximate eigenvalues. Moreover, the function is also monotonic in this regime. This implies that the order is preserved.</p></list-item><list-item><p>For <inline-formula><mml:math id="inf142"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula>, that is, close to the singularity, an additional root is inserted as <inline-formula><mml:math id="inf143"><mml:mi>f</mml:mi></mml:math></inline-formula> cuts both branches of <inline-formula><mml:math id="inf144"><mml:mi>g</mml:mi></mml:math></inline-formula> and the order is disturbed.</p></list-item></list><p>Altogether, this implies that, for the cases studied in this article that consider <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, only the first two eigenvalues might differ from the order given by the approximate solution. In general, in the intervals <inline-formula><mml:math id="inf146"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> there will be one root for both equations except in the one closest to the singularity which will contain two. Moreover, from <xref ref-type="disp-formula" rid="equ26">Equation 25</xref> we see that, for two regions with densities <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> and Laplacian eigenvalues <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>κ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> implies <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. This inequality is satisfied by the exact eigenvalues, again, in all intervals except the one in which the singularity lies (as can be confirmed by expanding <inline-formula><mml:math id="inf153"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> around the approximate eigenvalues and solving for κ). We have compared the exact and approximate ordering for two regions for a number of (manageable) cases and have found the relationship described above to hold.</p></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s14"><title>Ordering in the 2D square case</title><p>For rectangle regions, the ordering can be solved by calculating the number of lattice points enclosed by a quarter ellipse (<xref ref-type="bibr" rid="bib83">Strauss, 2007</xref>). Here, we use square regions and therefore the solution is the number of points enclosed in a quarter circle. The Gauss circle problem determines the number of integer lattice points which lie within a circle with radius <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, with its centre at the origin:<disp-formula id="equ47"><label>(46)</label><mml:math id="m47"><mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>≤</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The number of lattice points within the circle is approximately equal to its area. The number of points within a square region can be approximated by calculating the area of the upper quarter of the circle (positive values only):<disp-formula id="equ48"><label>(47)</label><mml:math id="m48"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The number of eigenvalues in each region is therefore the area of the intersection of the circle and region.</p><p>For each region we calculate the number of lattice points enclosed by a quarter circle; for <italic>R</italic><sub>1</sub> we set the radius equal to <inline-formula><mml:math id="inf155"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and for <italic>R</italic><sub>2</sub> to <inline-formula><mml:math id="inf156"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (the solution of <xref ref-type="disp-formula" rid="equ18">Equation 18</xref>), where <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>. This number is approximated as the area of the quarter circle. For values of <inline-formula><mml:math id="inf158"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf159"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> greater than the total number of eigenvalues in each dimension (<inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>), the approximation diverges from the true ordering as the area of the quarter circle becomes larger than the area of the lattice (region). In this case, a correction term is added:<disp-formula id="equ49"><label>(48)</label><mml:math id="m49"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>k</mml:mi><mml:msqrt><mml:mi>p</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:msqrt><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>k</mml:mi><mml:msqrt><mml:mi>p</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf161"><mml:mi>p</mml:mi></mml:math></inline-formula> is either <inline-formula><mml:math id="inf162"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf163"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> for <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub> respectively, and <italic>k</italic> is the total number of eigenvalues in each region. Assuming a region size of <inline-formula><mml:math id="inf164"><mml:mi>L</mml:mi></mml:math></inline-formula>, with each receptor spaced one unit apart, <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> for <italic>R</italic><sub>1</sub>, and <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for <italic>R</italic><sub>2</sub>.</p></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s15"><title>Allocation for multiple regions in 2D</title><p>For more than two regions, density and activation ratios for each additional region are calculated relative to a chosen baseline region. This leads to the following general form for calculation of the eigenvalues of any region <italic>x</italic>:<disp-formula id="equ50"> <label>(49)</label><mml:math id="m50"><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mtext>:</mml:mtext></mml:mrow></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:msqrt></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:msqrt></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:msqrt></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf167"><mml:mi>a</mml:mi></mml:math></inline-formula> is the region activation scaling ratio, <italic>d</italic><sub><italic>b</italic></sub> is the density of the baseline region, and <italic>d</italic><sub><italic>x</italic></sub> the density of region <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> enumerate different eigenvalues for region <inline-formula><mml:math id="inf169"><mml:mi>x</mml:mi></mml:math></inline-formula>.</p></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s16"><title>Allocation for the 1D case</title><p>The 1D case for changes in density has previously been addressed in <xref ref-type="bibr" rid="bib32">Edmondson et al., 2019</xref>. Here, we extend this to include changes in activation. For two regions <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, we can calculate their eigenvalues as<disp-formula id="equ51"><label>(50)</label><mml:math id="m51"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo rspace="7.5pt">:</mml:mo><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ52"> <label>(51)</label><mml:math id="m52"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo rspace="7.5pt">:</mml:mo><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>d</italic> is the ratio of higher and lower densities, <inline-formula><mml:math id="inf170"><mml:mi>a</mml:mi></mml:math></inline-formula> is the ratio of receptor activation, <italic>L</italic> is the length of the region, and <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>ℕ</mml:mi></mml:mrow></mml:math></inline-formula> denote successive eigenvalues for regions <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, respectively.</p><p>To calculate how many output neurons are allocated to region <italic>R</italic><sub>2</sub> as a function of the number of neurons allocated to region <italic>R</italic><sub>1</sub>, we set <inline-formula><mml:math id="inf172"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and solve for <inline-formula><mml:math id="inf173"><mml:mi>m</mml:mi></mml:math></inline-formula>. This yields<disp-formula id="equ53"><label>(52)</label><mml:math id="m53"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt><mml:mi>π</mml:mi></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It becomes apparent that for <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, that is, the first neuron allocated to region <italic>R</italic><sub>1</sub>, we have already assigned <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:msqrt><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt><mml:mi>π</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> neurons to region <italic>R</italic><sub>2</sub>. As we allocate more neurons to region <italic>R</italic><sub>1</sub>, the ratio <inline-formula><mml:math id="inf176"><mml:mfrac><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mfrac></mml:math></inline-formula> simplifies to: <inline-formula><mml:math id="inf177"><mml:mrow><mml:mrow><mml:msub><mml:mo>lim</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mfrac><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. The fraction of neurons allocated to each region therefore depends on the size of the bottleneck and converges to <inline-formula><mml:math id="inf178"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="inf179"><mml:mfrac><mml:msqrt><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula> for <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>, respectively.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70777.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.05.26.445857" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.26.445857"/></front-stub><body><p>The article develops a mathematical approach to study the allocation of cortical area to sensory representations in the presence of resource constraints. The theory is applied to study sensory representations in the somatosensory system. This problem is largely unexplored, the results are novel, and can be of interest to experimental and theoretical neuroscientists.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70777.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Park</surname><given-names>Memming</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.26.445857">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.05.26.445857v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Expansion and contraction of resource allocation in sensory bottlenecks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Memming Park (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>While the reviewers found the work interesting and engaging, they agreed that some major revisions would greatly strengthen the manuscript. All have consulted with each other and the reviewing editor to compile these major prompts. Please also see the recommended, slightly more minor, revisions below as well as the public reviews provided by each reviewer.</p><p>1) Clarifying assumptions made</p><p>The paper should explicitly state the assumptions of the model, and discuss their consequences much earlier, and in more depth.</p><p>Currently, the Introduction and first paragraphs of the results equate efficient coding with decorrelation (e.g. paragraphs 3 and 4 of the introduction and opening sentences of the Results). The authors address the role of noise only briefly in the Discussion. This could be expanded. &quot;Efficient coding&quot; is a broad conceptual framework, and not a single algorithm such as decorrelation or even redundancy reduction. Currently, the manuscript makes the impression that the two are the same.</p><p>Other, unstated assumptions of the model (e.g. constraints on the total variance of the bottleneck, and the assumption that output activity is equalized across all neurons) should be made more explicit. How are those assumptions connected to biology? Is there evidence that cortical neurons in the somatosensory system are equally active? Even if there is not – this should be addressed explicitly.</p><p>Also, the analytical results are derived with a number of assumptions about the correlation structure of the stimuli and the structure of the bottleneck. Please clarify these assumptions as well. Importantly, please more clearly define the cost function. Is it simply the squared error loss between receptors and representation?</p><p>2) Putting the math more front-and-center</p><p>The fact that all results are derived analytically is a clear strength of the manuscript, but the authors have chosen to refrain from presenting any math in the Results section. This is very unfortunate, because this makes the formal arguments more fuzzy than need be. Please find a compromise between a technical and very high-level paper, and include the essential formal steps of the arguments in the main text.</p><p>3) Expanding the simulation examples provided, increasing the scope of the results</p><p>Along these lines, and looping back to prompt 1 and clarifying the assumptions in the model, please include one or two examples with more complicated input statistics or system architecture. For example, it is frequently the case that a narrow sensory bottleneck is followed by an expansion (e.g. retina -&gt; optic nerve -&gt; V1) – could the theory say something about resource allocation in such scenario?</p><p>In another example – what if the correlation structure is more complex? E.g. in the auditory nerve correlations between sensors can spatially non-local, due to the presence of harmonic sounds (e.g. Terashima and Okada, NeurIPS 2012). These are just suggestions of more complex scenarios which the authors could consider. These additional results do not have to be obtained analytically, and could be optimized through simulation. Such additions could greatly strengthen the current paper, broaden its scope, and increase its impact.</p><p>To show that those results are robust for small variations in the stimulus distribution, please provide numerical experiments with a different covariance function. Perhaps try a Matérn covariance function with nu=3/2 which is slightly smoother than the OU covariance. The experiments are numerically difficult, so this prompt only requires showing a case in a regime where things are numerically stable (this should be possible since the authors have done something similar in the NeurIPS 2019 paper). We would like to see that the presented results are not qualitatively very different.</p><p>4) Provide more details on the star-nosed mole data and predictions from the model</p><p>Please report in more detail how the predictions are generated for the data from the star-nosed mole. This could be done in the main text. For example, one could plot predictions for different sizes of the bottleneck. As a field, we should not be afraid of showing the range of predictions for different parameter values (e.g. for different bottleneck sizes), and demonstrating where they match the data, and where they do not – this does not necessarily weaken the theory. In this particular case – the theory could inform us about not-yet-measured properties of the system, such as the size of the bottleneck. Moreover, this will be more explicit way to report these results.</p><p>Furthermore, to model the stimulus distribution for star-nosed mole case, the authors simulated a noisy sensory system to convert contact statistics (assuming a circular object) to receptor activation statistics. Star-nose mole parameters (Appendix table) are in the code provided, but it doesn't seem like code for the conversion from the contact statistics to the receptor activation statistics is included. Please share this code, since the text in the manuscript is missing much of the necessary detail for the reproduction of this work.</p><p>Recommendations for the authors:</p><p>A) page 2 and Discussion: The authors discuss the use of Fisher Information and other measures for their goal. The use of Fisher Information for optimal population coding can be dangerous especially when taking N to infinity, as then the convergence conditions of 1/FI to the MSE are not met (see Berens et al. 2012). The crucial quantity for convergence is the SNR. Thus, the stated reason is not a reason not to use Fisher Information, although it's fine for the authors to choose not to use it. The logic should be improved in the text, though.</p><p>B) The claim that full model has better MSE is not very convincing. The difference between using density only and receptor only makes sense. But the full model has an additional degree of freedom, so naturally it should be better then either if tested on the same data that it was fit to. Unfortunately, the dataset the authors have is only 11 points, so there's no easy way of splitting them to show that this is effect is real. Hence, it's hard to make a strong claim about the tiny change in MSE. Please weaken the claim. Also, Figure 5D scatter plot for the full model is very similar to that of the receptor usage model (I ran your code). Please provide the corresponding plots explicitly in the supplement instead of the code only for transparency.</p><p>C) The model makes no distinction between the density of sensors in different regions and the exponential decay of the covariance functions. This is not a problem from the perspective of theoretical predictions, but perhaps it could be explained better (maybe with additional illustrations?).</p><p>D) It wasn't clear what the authors mean by &quot;second-order&quot;. One assumes this means that the input statistics is Gaussian, but perhaps this is wrong. Did the authors mean that they are only analyzing the covariance functions, i.e. second-order statistics? Or was it that the neurons' response entropy were quantified by their variance? Please clarify.</p><p>E) In the text, 'activation' and 'activation ratio' are used interchangeably. It would be better to stick with the ratio since it's really only the relative activation of the second population that matters.</p><p>E) The 'σ' as 'decay constant' should be explicitly mentioned in the main text in addition to the methods section. At first it seems to be the length-scale, but it turned out to be inverse length-scale which was confusing, since σ is often used to denote the inverse length-scale. (Figures1C, 2C)</p><p>F) Please add the following citations to other theoretical work, which considered efficient encoding of somatosensory stimuli and the allocation of cortical space :</p><p>a) Bhand, M., Mudur, R., Suresh, B., Saxe, A., and Ng, A. (2011). Unsupervised learning models of primary cortical receptive fields and receptive field plasticity. Advances in neural information processing systems, 24, 1971-1979.</p><p>b) Plumbley, M. D. (1999). Do cortical maps adapt to optimize information density?. Network: Computation in Neural Systems, 10(1), 41.</p><p>G) page 2: In the discussion about bottlenecks in the retina, recent work by Lindsey et al. (2019, arxiv 1901:00945) should be cited.</p><p>H) page 5: &quot;Decorrelation has been found to be a…&quot; -&gt; Please add citations to this claim</p><p>Figures:</p><p>I) Figure 1D: This figure is quite confusing. The decaying covariance is plotted for a 1-D case, the axes are unlabelled, and it takes a while to figure out that these two block matrices do form a single covariance matrix (even-though they correspond to different regions). Please revise this figure.</p><p>J) Figure 2D: small boxes with receptor location is misleading. It shows increased number of receptors when it's the activation ratio that's different. Please revise this figure.</p><p>K) Figure 3: A/B-labels seem to be too large; the meaning of the boxes of different colors and the lines was unclear to me. What determines e.g. the line length? The two panels in A should be clearly labeled with what quantity is varied.</p><p>L) Figure 4: What are the dashed lines? Is it correct that 4B is a simple restatement of the fact that the lines in 3A left are more spread for smaller bottlenecks? Interpreting this as a sign of more plasticity is an overstatement.</p><p>M) Figure 5: Please normalize the RMSE in Figure 5D with respect to the power of the signal. Otherwise the numbers aren't that meaningful, unless some clarifying point is missing. (Or, why not just report r^2?) Please also provide RMSE curves for all different bottleneck sizes and discuss how plausible the optimal values are. Given the right panel, it looks like the RMSE is mainly driven by ray 11, the outlier. How stable is the inferred conclusion if ray 11 is left out? Also, the authors seem to use RMSE to compare percentages. Could the authors comment on the suitability for that loss function?</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Expansion and contraction of resource allocation in sensory bottlenecks&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>1. Regarding low-noise regime, it only appears in the discussion and it does not warn the readers that the numerous tiny tail eigenvalues are very sensitive to noise. This affects the robustness of the analysis and should be clearly stated in the text and appear after the first interpretation of the bottleneck results (Figure 2 and 2-supplement 4).</p><p>2. It is not stated clearly that decorrelation is an information maximizing transform only in absence of noise, which is a key assumption of the model. In the Methods (lines 493-494) it is written that adding the noise does not affect the eigendecomposition (and, in consequence, resource allocation), but if that is the case – it is an important result which should be highlighted and explored further – a sensory coding transformation which is independent of the noise level seems like an important finding. Please revise the text around this point.</p><p>3. It is misleading to claim that the analysis &quot;maximizes information&quot; without qualification in the abstract since this work only deals with variances. Please rephrase.</p><p>4. In Line 66, please add a qualifier to the statement that Fisher Information can be used to approximate Mutual Information. This is only true under certain assumptions in certain cases and specifically for optimal coding, the relationship is not straightforward (Berens et al. 2012, PNAS; Bethge et al. Neural Comp 2002).</p><p>5. lines 87-88: &quot;We consider linear models which maximize information by decorrelating the sensory inputs, or, equivalently, minimize the mean-squared reconstruction error […]&quot; – in a general case these two goals are not equivalent. The following citations referencing work by Doi and colleagues do not support this claim. This should be revised.</p><p>6. In Figure 1, it is hard to parse which colors are kept to mean the same thing across panels and which change meaning. An additional schematic of the model in this figure would help with clarity of the presentation.</p><p>7. In Equation 1, is P used at all? If not simplify. The explanation of P sounds complicated without really saying anything specific. Give an example of constraints that could be incorporated.</p><p>8. The assumption that the covariance between regions is zero is quite a strong one and seems central to the paper. This should be justified or at least discussed how realistic it is. In the finger tip example, one of course typically touches objects not with one finger but with multiple ones.</p><p>9. The brackets in Figure 3 were found to be very confusing, please revise.</p><p>10. The covariance functions in Figure 5 are not very different, it is not surprising that these result in very similar outcomes. Comment on this or consider revising.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70777.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>While the reviewers found the work interesting and engaging, they agreed that some major revisions would greatly strengthen the manuscript. All have consulted with each other and the reviewing editor to compile these prompts.</p></disp-quote><p>We would like to thank the reviewers and editors for their clear and insightful feedback. We have addressed the comments in a revised version. In brief, the major changes are as follows:</p><list list-type="order"><list-item><p>We have re-organised the manuscript as suggested, and clarified the model and its assumptions.Specifically, we now make clear that our results are not restricted to a particular decorrelation model but cover a wider class of models. This broadens the applicability of the findings, but also clarifies the essential assumptions the model rests on.</p></list-item><list-item><p>We have included numerical results for additional covariance functions; these demonstrate that theresults hold qualitatively and are robust to small changes in the covariance structure, but also shed some light on how properties of the covariance structure affect the resulting allocation, which we believe will be insightful to the reader.</p></list-item><list-item><p>Additional results have been provided for the star-nosed mole example and the language has beentoned down. Throughout the manuscript, many minor improvements have been made and some unwieldy sections of the mathematical appendix have been re-organised. Visualisation has also been improved.</p></list-item></list><p>Overall, we believe that the manuscript is much improved and think that the reviewers will agree.</p><disp-quote content-type="editor-comment"><p>1) Clarifying assumptions made</p><p>The paper should explicitly state the assumptions of the model, and discuss their consequences much earlier, and in more depth.</p><p>Currently, the Introduction and first paragraphs of the results equate efficient coding with decorrelation (e.g. paragraphs 3 and 4 of the introduction and opening sentences of the Results). The authors address the role of noise only briefly in the Discussion. This could be expanded. &quot;Efficient coding&quot; is a broad conceptual framework, and not a single algorithm such as decorrelation or even redundancy reduction. Currently, the manuscript makes the impression that the two are the same.</p></disp-quote><p>We have clarified the broader meaning of ‘efficient coding’ in the manuscript and now explain in more detail how decorrelation/redundancy reduction fits into this framework and why we have chosen to focus on it for this paper. We have also updated our discussion on the role of noise (specifically in the light of model assumptions, see next comment).</p><disp-quote content-type="editor-comment"><p>Other, unstated assumptions of the model (e.g. constraints on the total variance of the bottleneck, and the assumption that output activity is equalized across all neurons) should be made more explicit. How are those assumptions connected to biology? Is there evidence that cortical neurons in the somatosensory system are equally active? Even if there is not – this should be addressed explicitly.</p><p>Also, the analytical results are derived with a number of assumptions about the correlation structure of the stimuli and the structure of the bottleneck. Please clarify these assumptions as well. Importantly, please more clearly define the cost function. Is it simply the squared error loss between receptors and representation?</p></disp-quote><p>Thank you for prompting us to clarify our assumptions. We realized that in our original submission, we did not make sufficiently clear which assumptions were strictly necessary for determining receptive field locations (i.e. allocation) and which were choices that need to be made in order to calculate precise receptive fields, but which do not affect the resulting allocation, and where therefore different choices might be made. This is now made clearer both at the start of the Results section (where we now present some key equations, see below) and in the Methods section.</p><p>In brief, the framework we employ (developed by Doi and Lewicki in several papers) allows calculation of linear receptive fields in the undercomplete case, enforcing decorrelation of the signals. Dimensionality reduction and decorrelation are achieved via PCA by calculating eigenvectors and eigenvalues. Since decorrelation is not unique, the additional degrees of freedom can be used to enforce additional constraints (via the matrix P in Equation 6). Doi and Lewicki showed how this could be done using an iterative procedure for optimizing P (Equation 7) for a number of constraints e.g. power constraints on individual or all output neurons, sparsity constraints, and even included some simple forms of noise. Crucially, the decorrelation part (eigenvectors) is unaffected by any of these constraints (as they only act through P), and it is this component that determines which region receptive fields will fall onto. Any additional constraints enforced through P will affect the shape and structure of the receptive fields, but not the region they fall onto. Additional constraints, such as power constraints or response sparsity, can thus be added to the model without affecting the results.</p><disp-quote content-type="editor-comment"><p>2) Putting the math more front-and-center</p><p>The fact that all results are derived analytically is a clear strength of the manuscript, but the authors have chosen to refrain from presenting any math in the Results section. This is very unfortunate, because this makes the formal arguments more fuzzy than need be. Please find a compromise between a technical and very high-level paper, and include the essential formal steps of the arguments in the main text.</p></disp-quote><p>We now present the main equations associated with the efficient coding approach as well as the main steps of the derivation at the start of the Results section, aligned with the presentation of the problem and solution strategy presented in figure 1.</p><disp-quote content-type="editor-comment"><p>3) Expanding the simulation examples provided, increasing the scope of the results</p><p>Along these lines, and looping back to prompt 1 and clarifying the assumptions in the model, please include one or two examples with more complicated input statistics or system architecture. For example, it is frequently the case that a narrow sensory bottleneck is followed by an expansion (e.g. retina -&gt; optic nerve -&gt; V1) – could the theory say something about resource allocation in such scenario?</p></disp-quote><p>We have included numerical calculations for other covariance functions (see next comment). The model assumes a bottleneck, either as a limit on the number of neurons or on the information, and therefore will not directly be able to answer what the allocation should look like in an overcomplete scenario (such as the expansion from optic nerve to V1). The model might still apply if a subsequent stage places a clear limit on the amount of information rather than the number of neurons (see Figure2—figure supplement 4), however empirical data is lacking to answer such questions. We have expanded our discussion on this issue and now make clearer the limits and assumptions of the current model.</p><disp-quote content-type="editor-comment"><p>In another example – what if the correlation structure is more complex? E.g. in the auditory nerve correlations between sensors can spatially non-local, due to the presence of harmonic sounds (e.g. Terashima and Okada, NeurIPS 2012). These are just suggestions of more complex scenarios which the authors could consider. These additional results do not have to be obtained analytically, and could be optimized through simulation. Such additions could greatly strengthen the current paper, broaden its scope, and increase its impact.</p><p>To show that those results are robust for small variations in the stimulus distribution, please provide numerical experiments with a different covariance function. Perhaps try a Matérn covariance function with nu=3/2 which is slightly smoother than the OU covariance. The experiments are numerically difficult, so this prompt only requires showing a case in a regime where things are numerically stable (this should be possible since the authors have done something similar in the NeurIPS 2019 paper). We would like to see that the presented results are not qualitatively very different.</p></disp-quote><p>We thank the reviewers for these suggestions, which have prompted us to test how well our findings generalize beyond negative exponential covariance functions. To do this we have focused on Matérn covariance functions, as suggested, of which the negative exponential can be considered a special case, while other instances exhibit a smoother covariance structure with higher correlation at small distances but smaller correlation at larger distances compared to the negative exponential. Therefore this class provides a natural starting point to test robustness and it turns out that the results prove rather insightful.</p><p>First, in numerical simulations, we found that our results held qualitatively across a range of Matérn parameter values. Allocations were more extreme at narrow compared to wide bottlenecks and regions with higher density and/or activations ended up over-represented. Thus, it appears the results obtained for negative exponential covariance functions do generalize to other monotonically decreasing covariance functions.</p><p>Second, we noted that precise allocations, and specifically the convergence points, differ across Matern functions and that they do so in a systematic fashion. Specifically, the smoother the covariance, the less extreme the over-representation. Moreover, the convergence points for the different Matérn functions appear to follow a simple mathematical function. Altogether these results suggest that the precise allocations for monotonically decreasing covariance functions are determined by their smoothness. We have included these findings as a new section in the Results (“Generalisation to other covariance functions”) and also added a new figure.</p><p>We also considered more complex covariance functions. Thank you for pointing out a specific example from the auditory system, which we weren’t aware of. Our approach requires that covariance functions decay with distance (to ensure that the approximation of the covariance matrix with a block matrix holds, which allows us to consider regions separately), however, they need not do so monotonically. We therefore tested a damped sinusoid covariance function numerically, whose shape resembles the complex harmonic covariance structure observed in the auditory system. Results suggested that in contrast to monotonically decreasing covariance functions, where allocations converge monotonically towards a limit, this more complex covariance function induced complex nonlinear allocations at different bottleneck widths. Unfortunately, it was difficult to obtain numerically stable solutions. These instabilities arise when the eigenvalue function flattens out and small numerical errors then affect the sorting of eigenvalues between the two regions. For monotonically decreasing covariance functions, this issue only arises close to convergence, however more complex covariance functions contain more such regions and we could not find a method to satisfactorily address this issue. We therefore elected not to include these results in the current manuscript, but have included additional text in the discussion instead.</p><disp-quote content-type="editor-comment"><p>4) Provide more details on the star-nosed mole data and predictions from the model</p><p>Please report in more detail how the predictions are generated for the data from the star-nosed mole. This could be done in the main text. For example, one could plot predictions for different sizes of the bottleneck. As a field, we should not be afraid of showing the range of predictions for different parameter values (e.g. for different bottleneck sizes), and demonstrating where they match the data, and where they do not – this does not necessarily weaken the theory. In this particular case – the theory could inform us about not-yet-measured properties of the system, such as the size of the bottleneck. Moreover, this will be more explicit way to report these results.</p><p>Furthermore, to model the stimulus distribution for star-nosed mole case, the authors simulated a noisy sensory system to convert contact statistics (assuming a circular object) to receptor activation statistics. Star-nose mole parameters (Appendix table) are in the code provided, but it doesn't seem like code for the conversion from the contact statistics to the receptor activation statistics is included. Please share this code, since the text in the manuscript is missing much of the necessary detail for the reproduction of this work.</p></disp-quote><p>We have included a supplementary figure with the predictions for all bottleneck sizes and have run some additional tests to verify that a good fit cannot be obtained by simply varying the bottleneck width if arbitrary densities and activation statistics are chosen. We have also clarified in the text the simulations of the decay parameter and calculation for bottleneck allocations. We have additionally switched from RMSE to R^2 and the results should now be easier to interpret. Finally, the code to convert the contact statistics and calculate the decay parameters for each ray has been added to the Github repository linked in the paper.</p><disp-quote content-type="editor-comment"><p>Recommendations for the authors:</p><p>A) page 2 and Discussion: The authors discuss the use of Fisher Information and other measures for their goal. The use of Fisher Information for optimal population coding can be dangerous especially when taking N to infinity, as then the convergence conditions of 1/FI to the MSE are not met (see Berens et al. 2012). The crucial quantity for convergence is the SNR. Thus, the stated reason is not a reason not to use Fisher Information, although it's fine for the authors to choose not to use it. The logic should be improved in the text, though.</p></disp-quote><p>Thank you for the clarification. We have removed the reasoning about population sizes from the manuscript.</p><disp-quote content-type="editor-comment"><p>B) The claim that full model has better MSE is not very convincing. The difference between using density only and receptor only makes sense. But the full model has an additional degree of freedom, so naturally it should be better then either if tested on the same data that it was fit to. Unfortunately, the dataset the authors have is only 11 points, so there's no easy way of splitting them to show that this is effect is real. Hence, it's hard to make a strong claim about the tiny change in MSE. Please weaken the claim. Also, Figure 5D scatter plot for the full model is very similar to that of the receptor usage model (I ran your code). Please provide the corresponding plots explicitly in the supplement instead of the code only for transparency.</p></disp-quote><p>All models have the same number of variable parameters- density of each ray and activation. In the models where one of these is fixed, we set the fixed value to the mean of the values for all the rays. To test whether the range of bottleneck sizes results in the possibility to fit any given values for the rays, we included a further random simulation in the supplementary information. Here we randomise the values for the density and activation of each ray within the possible range of values for each. We find that with this randomisation of the values the model performs poorly on fitting even with a range of bottleneck sizes. This suggests that the model can only be fitted when using the empirically measured values.</p><disp-quote content-type="editor-comment"><p>C) The model makes no distinction between the density of sensors in different regions and the exponential decay of the covariance functions. This is not a problem from the perspective of theoretical predictions, but perhaps it could be explained better (maybe with additional illustrations?).</p></disp-quote><p>We have improved the text clarifying this relationship in the Results section. We now also make clear that in all examples shown receptors in the low-density region are spaced with a distance of 1, and thus the covariance decay is relative to this spacing.</p><disp-quote content-type="editor-comment"><p>D) It wasn't clear what the authors mean by &quot;second-order&quot;. One assumes this means that the input statistics is Gaussian, but perhaps this is wrong. Did the authors mean that they are only analyzing the covariance functions, i.e. second-order statistics? Or was it that the neurons' response entropy were quantified by their variance? Please clarify.</p></disp-quote><p>Upon reflection, we have removed the term “second-order” from the text, as it was redundant and confusing. Instead, the term “decorrelation” that we already use implies that we are acting on second-order statistics. We don’t require the stimulus distribution itself to be Gaussian, or in fact the processing to only consider second-order statistics, but merely that decorrelation is a necessary step. If it is, this requires calculating the eigenvalues and eigenvectors of the covariance matrix and then the rest of our argument follows.</p><disp-quote content-type="editor-comment"><p>E) In the text, 'activation' and 'activation ratio' are used interchangeably. It would be better to stick with the ratio since it's really only the relative activation of the second population that matters.</p></disp-quote><p>We have clarified in the text what we mean by activation and activation ratio. Where we mean the activation ratio the text has been amended to clearly state this.</p><disp-quote content-type="editor-comment"><p>E) The 'σ' as 'decay constant' should be explicitly mentioned in the main text in addition to the methods section. At first it seems to be the length-scale, but it turned out to be inverse length-scale which was confusing, since σ is often used to denote the inverse length-scale. (Figures1C, 2C)</p></disp-quote><p>To avoid confusion, we have renamed the σ parameter to γ and have clarified its meaning in the text.</p><disp-quote content-type="editor-comment"><p>F) Please add the following citations to other theoretical work, which considered efficient encoding of somatosensory stimuli and the allocation of cortical space :</p><p>a) Bhand, M., Mudur, R., Suresh, B., Saxe, A., and Ng, A. (2011). Unsupervised learning models of primary cortical receptive fields and receptive field plasticity. Advances in neural information processing systems, 24, 1971-1979.</p><p>b) Plumbley, M. D. (1999). Do cortical maps adapt to optimize information density?. Network: Computation in Neural Systems, 10(1), 41.</p></disp-quote><p>Thank you for these additional references. We have added relevant points from these in the introduction relating to the optimal representation of information and magnification in cortical maps (Plumbley) and in the discussion on the influence of receptor density and statistics (Bhand).</p><disp-quote content-type="editor-comment"><p>G) page 2: In the discussion about bottlenecks in the retina, recent work by Lindsey et al. (2019, arxiv 1901:00945) should be cited.</p></disp-quote><p>This work was cited elsewhere in the paper, but indeed should also be mentioned in this context and we have now added the reference.</p><disp-quote content-type="editor-comment"><p>H) page 5: &quot;Decorrelation has been found to be a…&quot; -&gt; Please add citations to this claim</p></disp-quote><p>We have added a number of citations to support this claim.</p><disp-quote content-type="editor-comment"><p>Figures:</p><p>I) Figure 1D: This figure is quite confusing. The decaying covariance is plotted for a 1-D case, the axes are unlabelled, and it takes a while to figure out that these two block matrices do form a single covariance matrix (even-though they correspond to different regions). Please revise this figure.</p></disp-quote><p>We have updated this figure and believe the new version to be much clearer. We have switched to a 2D version for the covariance so that it aligns with the example introduced in the earlier panels. The fact that we are considering a single covariance matrix with block structure should also be more visible now.</p><disp-quote content-type="editor-comment"><p>J) Figure 2D: small boxes with receptor location is misleading. It shows increased number of receptors when it's the activation ratio that's different. Please revise this figure.</p></disp-quote><p>We have updated the figure and this should be more clearly indicated now.</p><disp-quote content-type="editor-comment"><p>K) Figure 3: A/B-labels seem to be too large; the meaning of the boxes of different colors and the lines was unclear to me. What determines e.g. the line length? The two panels in A should be clearly labeled with what quantity is varied.</p></disp-quote><p>Thank you for pointing out this issue. The original boxes and lines were supposed to indicate groupings of examples that showed similar behaviour. These indicators have been now replaced with symbols resembling brackets which should be more easily interpretable. We have also clarified the content of these plots with a fuller explanation in the figure caption. The size of the labels has been corrected.</p><disp-quote content-type="editor-comment"><p>L) Figure 4: What are the dashed lines? Is it correct that 4B is a simple restatement of the fact that the lines in 3A left are more spread for smaller bottlenecks? Interpreting this as a sign of more plasticity is an overstatement.</p></disp-quote><p>The dashed lines in panel A denote the two example bottlenecks for which allocations are visualized directly below. This has now been clarified in the figure legend.</p><disp-quote content-type="editor-comment"><p>M) Figure 5: Please normalize the RMSE in Figure 5D with respect to the power of the signal. Otherwise the numbers aren't that meaningful, unless some clarifying point is missing. (Or, why not just report r^2?) Please also provide RMSE curves for all different bottleneck sizes and discuss how plausible the optimal values are. Given the right panel, it looks like the RMSE is mainly driven by ray 11, the outlier. How stable is the inferred conclusion if ray 11 is left out? Also, the authors seem to use RMSE to compare percentages. Could the authors comment on the suitability for that loss function?</p></disp-quote><p>The plot in former figure 5D (now figure 6) has been changed to report the R^2 between allocations in model and cortical percentages from the empirical data at the best fitting bottleneck. We have provided the fits across a range of bottlenecks in figure 6—figure supplement 1.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>1. Regarding low-noise regime, it only appears in the discussion and it does not warn the readers that the numerous tiny tail eigenvalues are very sensitive to noise. This affects the robustness of the analysis and should be clearly stated in the text and appear after the first interpretation of the bottleneck results (Figure 2 and 2-supplement 4).</p></disp-quote><p>We have made the assumption of low noise and its implication clearer by making several changes to the text. In addition to the text in the discussion, we now make clear at the start of the Results section that we are assuming a low-noise regime. As requested, we have also added a note about the potential effects of noise after presenting the initial results in Figure 2:</p><p>“An additional consideration is the robustness of the results regarding small perturbations of the calculated eigenvalues. As allocation depends on the ranks of the eigenvalues only, low levels of noise are unlikely to affect the outcome for narrow bottlenecks, especially since eigenvalues are decaying rather steeply in this regime. On the other hand, allocation in wider bottlenecks is determined from small tail eigenvalues which are much more sensitive to noise (which is also evident when comparing the analytical solution to numerical ones). Allocation can therefore be expected to be somewhat less robust in those regimes.”</p><disp-quote content-type="editor-comment"><p>2. It is not stated clearly that decorrelation is an information maximizing transform only in absence of noise, which is a key assumption of the model. In the Methods (lines 493-494) it is written that adding the noise does not affect the eigendecomposition (and, in consequence, resource allocation), but if that is the case – it is an important result which should be highlighted and explored further – a sensory coding transformation which is independent of the noise level seems like an important finding. Please revise the text around this point.</p></disp-quote><p>We have edited the text in the Results section to clarify that decorrelation maximises information only in the absence of noise. We have removed the text in the Methods section regarding the effect of noise on the eigendecomposition. The idea was that under some specific noise models, receptive fields would be affected but not their location, thus rendering the resulting allocation the same. However, this issue is not the focus of the paper, not fully worked out and in any case has no bearing on the results of the paper, and removing it will avoid confusing the reader.</p><disp-quote content-type="editor-comment"><p>3. It is misleading to claim that the analysis &quot;maximizes information&quot; without qualification in the abstract since this work only deals with variances. Please rephrase.</p></disp-quote><p>We have removed the phrase ‘maximize information’ from the abstract:</p><p>“Building on work in efficient coding, we address this problem using linear models that optimally decorrelate the sensory signals.”</p><p>See also point 5 below, which concerns the wording in the main manuscript text.</p><disp-quote content-type="editor-comment"><p>4. In Line 66, please add a qualifier to the statement that Fisher Information can be used to approximate Mutual Information. This is only true under certain assumptions in certain cases and specifically for optimal coding, the relationship is not straightforward (Berens et al. 2012, PNAS; Bethge et al. Neural Comp 2002).</p></disp-quote><p>We have added a note to this section (with the suggested references) that explains this important caveat:</p><p>“In contrast to receptor density, there has been some work on how populations of neurons should encode non-uniform stimulus statistics using Fisher information (Ganguli and Simoncelli, 2010, 2014, 2016; Yerxa et al., 2020). This approach aims to approximate mutual information and is used to calculate optimal encoding in a neural population (Yarrow et al., 2012), however its use is restricted to specific conditions and assumptions (Berens et al., 2011; Bethge et al., 2002).”</p><disp-quote content-type="editor-comment"><p>5. lines 87-88: &quot;We consider linear models which maximize information by decorrelating the sensory inputs, or, equivalently, minimize the mean-squared reconstruction error […]&quot; – in a general case these two goals are not equivalent. The following citations referencing work by Doi and colleagues do not support this claim. This should be revised.</p></disp-quote><p>We have revised the wording in this section:</p><p>“We restrict ourselves to linear models and only consider second-order statistics of the sensory signals, such that redundancy reduction simplifies to decorrelation (see Hancock et al. 1992, Simoncell and Olshausen 2001, Doi and Lewicki 2005, for examples from the visual literature).”</p><p>We have also ensured that the wording in the Methods section is appropriate:</p><p>“We assume that receptors are arranged on a two-dimensional sensory sheet. Correlations in the inputs occur as receptors that are nearby in space have more similar responses. Restricting ourselves to such second-order statistics and assuming that noise is negligible, information is maximised in such a setup by decorrelating the sensory inputs. Here we decorrelate using a simple linear model.”</p><disp-quote content-type="editor-comment"><p>6. In Figure 1, it is hard to parse which colors are kept to mean the same thing across panels and which change meaning. An additional schematic of the model in this figure would help with clarity of the presentation.</p></disp-quote><p>We have redesigned Figure 1 and made two major changes:</p><disp-quote content-type="editor-comment"><p>7. In Equation 1, is P used at all? If not simplify. The explanation of P sounds complicated without really saying anything specific. Give an example of constraints that could be incorporated.</p></disp-quote><p>We have removed mention of <italic>P</italic> from the Results section, as it is not needed to understand the main arguments and its choice does not affect allocation. We have kept it in the Methods section, to acknowledge that whitening is not unique and because it can be used to develop more complex models that enforce further constraints, but still allow a straightforward determination of the allocation. We now give a specific example of how such a constraint (in this case sparsity) could be enforced:</p><p>“As the optimally whitened solution is independent of P, additional constraints on the solution can be enforced. For example, power constraints are often added, which either limit the total variance or equalize the variance across output neurons, and additional sparsity constraints can be placed on the output neuron’s activity or their receptive field weights (Doi et al., 2012; Doi and Lewicki, 2014). For example, to optimize the sparsity of the weight matrix, one would define an appropriate cost function (such as the L1 norm of the weight matrix), then iteratively calculate the gradient with respect to W , take an update step to arrive at a new W_opt, and determine P as described above in Equation 7 (see Doi and Lewicki, 2014, for further details). Importantly for our problem, such additional constraints will affect the precise receptive field structure (through P), but not the eigenvalues and eigenvectors […].”</p><disp-quote content-type="editor-comment"><p>8. The assumption that the covariance between regions is zero is quite a strong one and seems central to the paper. This should be justified or at least discussed how realistic it is. In the finger tip example, one of course typically touches objects not with one finger but with multiple ones.</p></disp-quote><p>If covariance functions are monotonically decreasing and they are doing so fast enough that the extent of correlations is smaller than the size of the different regions, then the block matrix approximation is very accurate (as we have shown previously numerically). Of course, covariance functions might NOT decrease monotonically and, as pointed out here, one might expect this to be the case in touch where multi-finger contact would induce long-ranging and nonlinear correlations.</p><p>We have responded to this point by expanding the corresponding paragraph in the ‘Limitations’ section of the Discussion:</p><p>“We considered allocations for monotonically decreasing covariance functions. Choosing a negative exponential function and assuming that activations are uncorrelated across different input regions allowed us to derive an analytical solution. How justified are these assumptions? Many sensory systems will likely obey the intuitive notion that receptor correlations decrease with distance; however, there are notable exceptions, for example in the auditory system (Terashima and Okada, 2012). In touch, the sensory system we are mainly concerned with here, contact might often be made with several fingers (for humans) or rays (for the star-nosed mole) simultaneously, which might induce far-ranging non-monotonic correlations. Unfortunately there is little quantified evidence on the strength of these correlations, rendering their importance unclear. At least for the star-nosed mole, their prey is often small compared to the size of the rays, which move mostly independently, so cross-ray correlations might be low. Furthermore, the receptive fields of neurons in early somatosensory cortex of both humans and star-nosed moles are strongly localized to a single appendage and lie within cortical sub-regions that are clearly delineated from others (Sur et al., 1980; Nelson et al., 1980; Catania and Kaas, 1995). If long-range non-monotonic correlations were strong, we would expect to find many multi-appendage receptive fields and blurry region boundaries. As this is not the case, it therefore stands to reason that either these correlations are not very strong or that there is some other process, perhaps during development, that prevents these correlations affecting the final allocation. Either way, our assumption of monotonically decreasing covariance functions appears to be a good first-order match. That said, the question of how to arrive at robust allocations for more complex covariance functions is an important one that should be considered in future research. While it is possible in principle to solve the allocation problem numerically for arbitrary covariance functions, in practice we noticed that the presence of small numerical errors can affect the sorting process and caution is therefore warranted, especially when considering non-monotonic functions.”</p><disp-quote content-type="editor-comment"><p>9. The brackets in Figure 3 were found to be very confusing, please revise.</p></disp-quote><p>We have changed the brackets to markers whose colors match those in panel B.</p><disp-quote content-type="editor-comment"><p>10. The covariance functions in Figure 5 are not very different, it is not surprising that these result in very similar outcomes. Comment on this or consider revising.</p></disp-quote><p>We agree that the results are not particularly surprising, however they demonstrate the robustness of the method (i.e. the results are not an artefact of assuming a negative exponential covariance function) and provide some additional, insightful context on how the particular covariance structure affects the resulting allocation, tested via a systematic change in the smoothness of the function. Last but not least, the covariance functions tested were specifically requested by reviewers in the previous revision round. We have now clarified the motivation behind including these functions and what we can learn from them at the start of the relevant section:</p><p>“The results above relate to negative exponential covariance functions, where allocations can be solved analytically and it is unclear whether these findings generalize to other monotonically decreasing covariance functions and are therefore robust. Specifically, negative exponential covariances are relatively 'rough' and allocations might conceivably depend on the smoothness of the covariance function. To test directly how optimal allocations depend systematically on the smoothness of the covariance function, we numerically calculated allocations for three covariance functions […]”</p></body></sub-article></article>